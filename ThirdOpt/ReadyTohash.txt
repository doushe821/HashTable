In
Praise
of
Digital
Design
and
Computer
Architecture
Harris
and
Harris
have
taken
the
popular
pedagogy
from
Computer
Organization
and
Design
to
the
next
level
of
refinement
showing
in
detail
how
to
build
a
MIPS
microprocessor
in
both
Verilog
and
VHDL
Given
the
exciting
opportunity
that
students
have
to
run
large
digital
designs
on
modern
FGPAs
the
approach
the
authors
take
in
this
book
is
both
informative
and
enlightening
David
A
Patterson
University
of
California
Berkeley
Digital
Design
and
Computer
Architecture
brings
a
fresh
perspective
to
an
old
discipline
Many
textbooks
tend
to
resemble
overgrown
shrubs
but
Harris
and
Harris
have
managed
to
prune
away
the
deadwood
while
preserving
the
fundamentals
and
presenting
them
in
a
contemporary
context
In
doing
so
they
offer
a
text
that
will
benefit
students
interested
in
designing
solutions
for
tomorrow
s
challenges
Jim
Frenzel
University
of
Idaho
Harris
and
Harris
have
a
pleasant
and
informative
writing
style
Their
treatment
of
the
material
is
at
a
good
level
for
introducing
students
to
computer
engineering
with
plenty
of
helpful
diagrams
Combinational
circuits
microarchitecture
and
memory
systems
are
handled
particularly
well
James
Pinter
Lucke
Claremont
McKenna
College
Harris
and
Harris
have
written
a
book
that
is
very
clear
and
easy
to
understand
The
exercises
are
well
designed
and
the
real
world
examples
are
a
nice
touch
The
lengthy
and
confusing
explanations
often
found
in
similar
textbooks
are
not
seen
here
It
s
obvious
that
the
authors
have
devoted
a
great
deal
of
time
and
effort
to
create
an
accessible
text
I
strongly
recommend
Digital
Design
and
Computer
Architecture
Peiyi
Zhao
Chapman
University
Harris
and
Harris
have
created
the
first
book
that
successfully
combines
digital
system
design
with
computer
architecture
Digital
Design
and
Computer
Architecture
is
a
much
welcomed
text
that
extensively
explores
digital
systems
designs
and
explains
the
MIPS
architecture
in
fantastic
detail
I
highly
recommend
this
book
James
E
Stine
Jr
Oklahoma
State
University
Digital
Design
and
Computer
Architecture
is
a
brilliant
book
Harris
and
Harris
seamlessly
tie
together
all
the
important
elements
in
microprocessor
design
transistors
circuits
logic
gates
finite
state
machines
memories
arithmetic
units
and
conclude
with
computer
architecture
This
text
is
an
excellent
guide
for
understanding
how
complex
systems
can
be
flawlessly
designed
Jaeha
Kim
Rambus
Inc
Digital
Design
and
Computer
Architecture
is
a
very
well
written
book
that
will
appeal
to
both
young
engineers
who
are
learning
these
subjects
for
the
first
time
and
also
to
the
experienced
engineers
who
want
to
use
this
book
as
a
reference
I
highly
recommend
it
A
Utku
Diril
Nvidia
Corporation
Digital
Design
and
Computer
Architecture
About
the
Authors
David
Money
Harris
is
an
associate
professor
of
engineering
at
Harvey
Mudd
College
He
received
his
Ph
D
in
electrical
engineering
from
Stanford
University
and
his
M
Eng
in
electrical
engineering
and
computer
science
from
MIT
Before
attending
Stanford
he
worked
at
Intel
as
a
logic
and
circuit
designer
on
the
Itanium
and
Pentium
II
processors
Since
then
he
has
consulted
at
Sun
Microsystems
Hewlett
Packard
Evans
Sutherland
and
other
design
companies
David
s
passions
include
teaching
building
chips
and
exploring
the
outdoors
When
he
is
not
at
work
he
can
usually
be
found
hiking
mountaineering
or
rock
climbing
He
particularly
enjoys
hiking
with
his
son
Abraham
who
was
born
at
the
start
of
this
book
project
David
holds
about
a
dozen
patents
and
is
the
author
of
three
other
textbooks
on
chip
design
as
well
as
two
guidebooks
to
the
Southern
California
mountains
Sarah
L
Harris
is
an
assistant
professor
of
engineering
at
Harvey
Mudd
College
She
received
her
Ph
D
and
M
S
in
electrical
engineering
from
Stanford
University
Before
attending
Stanford
she
received
a
B
S
in
electrical
and
computer
engineering
from
Brigham
Young
University
Sarah
has
also
worked
with
Hewlett
Packard
the
San
Diego
Supercomputer
Center
Nvidia
and
Microsoft
Research
in
Beijing
Sarah
loves
teaching
exploring
and
developing
new
technologies
traveling
wind
surfing
rock
climbing
and
playing
the
guitar
Her
recent
exploits
include
researching
sketching
interfaces
for
digital
circuit
design
acting
as
a
science
correspondent
for
a
National
Public
Radio
affiliate
and
learning
how
to
kite
surf
She
speaks
four
languages
and
looks
forward
to
adding
a
few
more
to
the
list
in
the
near
future
Digital
Design
and
Computer
Architecture
David
Money
Harris
Sarah
L
Harris
AMSTERDAM
BOSTON
HEIDELBERG
LONDON
NEW
YORK
OXFORD
PARIS
SAN
DIEGO
SAN
FRANCISCO
SINGAPORE
SYDNEY
TOKYO
Morgan
Kaufmann
Publishers
is
an
imprint
of
Elsevier
Publisher
Denise
E
M
Penrose
Senior
Developmental
Editor
Nate
McFadden
Publishing
Services
Manager
George
Morrison
Project
Manager
Marilyn
E
Rash
Assistant
Editor
Mary
E
James
Editorial
Assistant
Kimberlee
Honjo
Cover
and
Editorial
Illustrations
Duane
Bibby
Interior
Design
Frances
Baca
Design
Composition
Integra
Technical
Illustrations
Harris
and
Harris
Integra
Production
Services
Graphic
World
Inc
Interior
Printer
Courier
Westford
Cover
Printer
Phoenix
Color
Corp
Morgan
Kaufmann
Publishers
is
an
imprint
of
Elsevier
Sansome
Street
Suite
San
Francisco
CA
This
book
is
printed
on
acid
free
paper
by
Elsevier
Inc
All
rights
reserved
Designations
used
by
companies
to
distinguish
their
products
are
often
claimed
as
trademarks
or
registered
trademarks
In
all
instances
in
which
Morgan
Kaufmann
Publishers
is
aware
of
a
claim
the
product
names
appear
in
initial
capital
or
all
capital
letters
Readers
however
should
contact
the
appropriate
companies
for
more
complete
information
regarding
trademarks
and
registration
Permissions
may
be
sought
directly
from
Elsevier
s
Science
Technology
Rights
Department
in
Oxford
UK
phone
fax
e
mail
permissions
elsevier
co
uk
You
may
also
complete
your
request
on
line
via
the
Elsevier
homepage
http
elsevier
com
by
selecting
Customer
Support
and
then
Obtaining
Permissions
Library
of
Congress
Cataloging
in
Publication
Data
Harris
David
Money
Digital
design
and
computer
architecture
David
Money
Harris
and
Sarah
L
Harris
st
ed
p
cm
Includes
bibliographical
references
and
index
ISBN
alk
paper
ISBN
Digital
electronics
Logic
design
Computer
architecture
I
Harris
Sarah
L
II
Title
TK
D
H
dc
For
information
on
all
Morgan
Kaufmann
publications
visit
our
Web
site
at
www
mkp
com
Printed
in
the
United
States
of
America
Pr
To
my
family
Jennifer
and
Abraham
DMH
To
my
sisters
Lara
and
Jenny
SLH
Contents
Preface
xvii
Features
xviii
Online
Supplements
xix
How
to
Use
the
Software
Tools
in
a
Course
xix
Labs
xx
Bugs
xxi
Acknowledgments
xxi
Chapter
From
Zero
to
One
The
Game
Plan
The
Art
of
Managing
Complexity
Abstraction
Discipline
The
Three
Y
s
The
Digital
Abstraction
Number
Systems
Decimal
Numbers
Binary
Numbers
Hexadecimal
Numbers
Bytes
Nibbles
and
All
That
Jazz
Binary
Addition
Signed
Binary
Numbers
Logic
Gates
NOT
Gate
Buffer
AND
Gate
OR
Gate
Other
Two
Input
Gates
Multiple
Input
Gates
Beneath
the
Digital
Abstraction
Supply
Voltage
Logic
Levels
Noise
Margins
DC
Transfer
Characteristics
The
Static
Discipline
ix
CMOS
Transistors
Semiconductors
Diodes
Capacitors
nMOS
and
pMOS
Transistors
CMOS
NOT
Gate
Other
CMOS
Logic
Gates
Transmission
Gates
Pseudo
nMOS
Logic
Power
Consumption
Summary
and
a
Look
Ahead
Exercises
Interview
Questions
Chapter
Combinational
Logic
Design
Introduction
Boolean
Equations
Terminology
Sum
of
Products
Form
Product
of
Sums
Form
Boolean
Algebra
Axioms
Theorems
of
One
Variable
Theorems
of
Several
Variables
The
Truth
Behind
It
All
Simplifying
Equations
From
Logic
to
Gates
Multilevel
Combinational
Logic
Hardware
Reduction
Bubble
Pushing
X
s
and
Z
s
Oh
My
Illegal
Value
X
Floating
Value
Z
Karnaugh
Maps
Circular
Thinking
Logic
Minimization
with
K
Maps
Don
t
Cares
The
Big
Picture
Combinational
Building
Blocks
Multiplexers
Decoders
Timing
Propagation
and
Contamination
Delay
Glitches
x
CONTENTS
Summary
Exercises
Interview
Questions
Chapter
Sequential
Logic
Design
Introduction
Latches
and
Flip
Flops
SR
Latch
D
Latch
D
Flip
Flop
Register
Enabled
Flip
Flop
Resettable
Flip
Flop
Transistor
Level
Latch
and
Flip
Flop
Designs
Putting
It
All
Together
Synchronous
Logic
Design
Some
Problematic
Circuits
Synchronous
Sequential
Circuits
Synchronous
and
Asynchronous
Circuits
Finite
State
Machines
FSM
Design
Example
State
Encodings
Moore
and
Mealy
Machines
Factoring
State
Machines
FSM
Review
Timing
of
Sequential
Logic
The
Dynamic
Discipline
System
Timing
Clock
Skew
Metastability
Synchronizers
Derivation
of
Resolution
Time
Parallelism
Summary
Exercises
Interview
Questions
Chapter
Hardware
Description
Languages
Introduction
Modules
Language
Origins
Simulation
and
Synthesis
CONTENTS
xi
Combinational
Logic
Bitwise
Operators
Comments
and
White
Space
Reduction
Operators
Conditional
Assignment
Internal
Variables
Precedence
Numbers
Z
s
and
X
s
Bit
Swizzling
Delays
VHDL
Libraries
and
Types
Structural
Modeling
Sequential
Logic
Registers
Resettable
Registers
Enabled
Registers
Multiple
Registers
Latches
More
Combinational
Logic
Case
Statements
If
Statements
Verilog
casez
Blocking
and
Nonblocking
Assignments
Finite
State
Machines
Parameterized
Modules
Testbenches
Summary
Exercises
Interview
Questions
Chapter
Digital
Building
Blocks
Introduction
Arithmetic
Circuits
Addition
Subtraction
Comparators
ALU
Shifters
and
Rotators
Multiplication
Division
Further
Reading
xii
CONTENTS
Number
Systems
Fixed
Point
Number
Systems
Floating
Point
Number
Systems
Sequential
Building
Blocks
Counters
Shift
Registers
Memory
Arrays
Overview
Dynamic
Random
Access
Memory
Static
Random
Access
Memory
Area
and
Delay
Register
Files
Read
Only
Memory
Logic
Using
Memory
Arrays
Memory
HDL
Logic
Arrays
Programmable
Logic
Array
Field
Programmable
Gate
Array
Array
Implementations
Summary
Exercises
Interview
Questions
Chapter
Architecture
Introduction
Assembly
Language
Instructions
Operands
Registers
Memory
and
Constants
Machine
Language
R
type
Instructions
I
type
Instructions
J
type
Instructions
Interpreting
Machine
Language
Code
The
Power
of
the
Stored
Program
Programming
Arithmetic
Logical
Instructions
Branching
Conditional
Statements
Getting
Loopy
Arrays
Procedure
Calls
Addressing
Modes
CONTENTS
xiii
Lights
Camera
Action
Compiling
Assembling
and
Loading
The
Memory
Map
Translating
and
Starting
a
Program
Odds
and
Ends
Pseudoinstructions
Exceptions
Signed
and
Unsigned
Instructions
Floating
Point
Instructions
Real
World
Perspective
IA
Architecture
IA
Registers
IA
Operands
Status
Flags
IA
Instructions
IA
Instruction
Encoding
Other
IA
Peculiarities
The
Big
Picture
Summary
Exercises
Interview
Questions
Chapter
Microarchitecture
Introduction
Architectural
State
and
Instruction
Set
Design
Process
MIPS
Microarchitectures
Performance
Analysis
Single
Cycle
Processor
Single
Cycle
Datapath
Single
Cycle
Control
More
Instructions
Performance
Analysis
Multicycle
Processor
Multicycle
Datapath
Multicycle
Control
More
Instructions
Performance
Analysis
Pipelined
Processor
Pipelined
Datapath
Pipelined
Control
Hazards
More
Instructions
Performance
Analysis
xiv
CONTENTS
HDL
Representation
Single
Cycle
Processor
Generic
Building
Blocks
Testbench
Exceptions
Advanced
Microarchitecture
Deep
Pipelines
Branch
Prediction
Superscalar
Processor
Out
of
Order
Processor
Register
Renaming
Single
Instruction
Multiple
Data
Multithreading
Multiprocessors
Real
World
Perspective
IA
Microarchitecture
Summary
Exercises
Interview
Questions
Chapter
Memory
Systems
Introduction
Memory
System
Performance
Analysis
Caches
What
Data
Is
Held
in
the
Cache
How
Is
the
Data
Found
What
Data
Is
Replaced
Advanced
Cache
Design
The
Evolution
of
MIPS
Caches
Virtual
Memory
Address
Translation
The
Page
Table
The
Translation
Lookaside
Buffer
Memory
Protection
Replacement
Policies
Multilevel
Page
Tables
Memory
Mapped
I
O
Real
World
Perspective
IA
Memory
and
I
O
Systems
IA
Cache
Systems
IA
Virtual
Memory
IA
Programmed
I
O
Summary
Exercises
Interview
Questions
CONTENTS
xv
Appendix
A
Digital
System
Implementation
A
Introduction
A
xx
Logic
A
Logic
Gates
A
Other
Functions
A
Programmable
Logic
A
PROMs
A
PLAs
A
FPGAs
A
Application
Specific
Integrated
Circuits
A
Data
Sheets
A
Logic
Families
A
Packaging
and
Assembly
A
Transmission
lines
A
Matched
Termination
A
Open
Termination
A
Short
Termination
A
Mismatched
Termination
A
When
to
Use
Transmission
Line
Models
A
Proper
Transmission
Line
Terminations
A
Derivation
of
Z
A
Derivation
of
the
Reflection
Coefficient
A
Putting
It
All
Together
A
Economics
Appendix
B
MIPS
Instructions
Further
Reading
Index
xvi
CONTENTS
xvii
Preface
Why
publish
yet
another
book
on
digital
design
and
computer
architecture
There
are
dozens
of
good
books
in
print
on
digital
design
There
are
also
several
good
books
about
computer
architecture
especially
the
classic
texts
of
Patterson
and
Hennessy
This
book
is
unique
in
its
treatment
in
that
it
presents
digital
logic
design
from
the
perspective
of
computer
architecture
starting
at
the
beginning
with
s
and
s
and
leading
students
through
the
design
of
a
MIPS
microprocessor
We
have
used
several
editions
of
Patterson
and
Hennessy
s
Computer
Organization
and
Design
COD
for
many
years
at
Harvey
Mudd
College
We
particularly
like
their
coverage
of
the
MIPS
architecture
and
microarchitecture
because
MIPS
is
a
commercially
successful
microprocessor
architecture
yet
it
is
simple
enough
to
clearly
explain
and
build
in
an
introductory
class
Because
our
class
has
no
prerequisites
the
first
half
of
the
semester
is
dedicated
to
digital
design
which
is
not
covered
by
COD
Other
universities
have
indicated
a
need
for
a
book
that
combines
digital
design
and
computer
architecture
We
have
undertaken
to
prepare
such
a
book
We
believe
that
building
a
microprocessor
is
a
special
rite
of
passage
for
engineering
and
computer
science
students
The
inner
workings
of
a
processor
seem
almost
magical
to
the
uninitiated
yet
prove
to
be
straightforward
when
carefully
explained
Digital
design
in
itself
is
a
powerful
and
exciting
subject
Assembly
language
programming
unveils
the
inner
language
spoken
by
the
processor
Microarchitecture
is
the
link
that
brings
it
all
together
This
book
is
suitable
for
a
rapid
paced
single
semester
introduction
to
digital
design
and
computer
architecture
or
for
a
two
quarter
or
two
semester
sequence
giving
more
time
to
digest
the
material
and
experiment
in
the
lab
The
only
prerequisite
is
basic
familiarity
with
a
high
level
programming
language
such
as
C
C
or
Java
The
material
is
usually
taught
at
the
sophomore
or
junior
year
level
but
may
also
be
accessible
to
bright
freshmen
who
have
some
programming
experience
FEATURES
This
book
offers
a
number
of
special
features
Side
by
Side
Coverage
of
Verilog
and
VHDL
Hardware
description
languages
HDLs
are
at
the
center
of
modern
digital
design
practices
Unfortunately
designers
are
evenly
split
between
the
two
dominant
languages
Verilog
and
VHDL
This
book
introduces
HDLs
in
Chapter
as
soon
as
combinational
and
sequential
logic
design
has
been
covered
HDLs
are
then
used
in
Chapters
and
to
design
larger
building
blocks
and
entire
processors
Nevertheless
Chapter
can
be
skipped
and
the
later
chapters
are
still
accessible
for
courses
that
choose
not
to
cover
HDLs
This
book
is
unique
in
its
side
by
side
presentation
of
Verilog
and
VHDL
enabling
the
reader
to
quickly
compare
and
contrast
the
two
languages
Chapter
describes
principles
applying
to
both
HDLs
then
provides
language
specific
syntax
and
examples
in
adjacent
columns
This
side
by
side
treatment
makes
it
easy
for
an
instructor
to
choose
either
HDL
and
for
the
reader
to
transition
from
one
to
the
other
either
in
a
class
or
in
professional
practice
Classic
MIPS
Architecture
and
Microarchitecture
Chapters
and
focus
on
the
MIPS
architecture
adapted
from
the
treatment
of
Patterson
and
Hennessy
MIPS
is
an
ideal
architecture
because
it
is
a
real
architecture
shipped
in
millions
of
products
yearly
yet
it
is
streamlined
and
easy
to
learn
Moreover
hundreds
of
universities
around
the
world
have
developed
pedagogy
labs
and
tools
around
the
MIPS
architecture
Real
World
Perspectives
Chapters
and
illustrate
the
architecture
microarchitecture
and
memory
hierarchy
of
Intel
IA
processors
These
real
world
perspective
chapters
show
how
the
concepts
in
the
chapter
relate
to
the
chips
found
in
most
PCs
Accessible
Overview
of
Advanced
Microarchitecture
Chapter
includes
an
overview
of
modern
high
performance
microarchitectural
features
including
branch
prediction
superscalar
and
out
oforder
operation
multithreading
and
multicore
processors
The
treatment
is
accessible
to
a
student
in
a
first
course
and
shows
how
the
microarchitectures
in
the
book
can
be
extended
to
modern
processors
End
of
Chapter
Exercises
and
Interview
Questions
The
best
way
to
learn
digital
design
is
to
do
it
Each
chapter
ends
with
numerous
exercises
to
practice
the
material
The
exercises
are
followed
by
a
set
of
interview
questions
that
our
industrial
colleagues
have
asked
students
applying
for
work
in
the
field
These
questions
provide
a
helpful
xviii
PREFACE
glimpse
into
the
types
of
problems
job
applicants
will
typically
encounter
during
the
interview
process
Exercise
solutions
are
available
via
the
book
s
companion
and
instructor
Web
pages
For
more
details
see
the
next
section
Online
Supplements
ONLINE
SUPPLEMENTS
Supplementary
materials
are
available
online
at
textbooks
elsevier
com
This
companion
site
accessible
to
all
readers
includes
Solutions
to
odd
numbered
exercises
Links
to
professional
strength
computer
aided
design
CAD
tools
from
Xilinx
and
Synplicity
Link
to
PCSPIM
a
Windows
based
MIPS
simulator
Hardware
description
language
HDL
code
for
the
MIPS
processor
Xilinx
Project
Navigator
helpful
hints
Lecture
slides
in
PowerPoint
PPT
format
Sample
course
and
lab
materials
List
of
errata
The
instructor
site
linked
to
the
companion
site
and
accessible
to
adopters
who
register
at
textbooks
elsevier
com
includes
Solutions
to
even
numbered
exercises
Links
to
professional
strength
computer
aided
design
CAD
tools
from
Xilinx
and
Synplicity
Instructors
from
qualified
universities
can
access
free
Synplicity
tools
for
use
in
their
classroom
and
laboratories
More
details
are
available
at
the
instructor
site
Figures
from
the
text
in
JPG
and
PPT
formats
Additional
details
on
using
the
Xilinx
Synplicity
and
PCSPIM
tools
in
your
course
are
provided
in
the
next
section
Details
on
the
sample
lab
materials
are
also
provided
here
HOW
TO
USE
THE
SOFTWARE
TOOLS
IN
A
COURSE
Xilinx
ISE
WebPACK
Xilinx
ISE
WebPACK
is
a
free
version
of
the
professional
strength
Xilinx
ISE
Foundation
FPGA
design
tools
It
allows
students
to
enter
their
digital
designs
in
schematic
or
using
either
the
Verilog
or
VHDL
hardware
description
language
HDL
After
entering
the
design
students
can
PREFACE
xix
Prelims
qxd
simulate
their
circuits
using
ModelSim
MXE
III
Starter
which
is
included
in
the
Xilinx
WebPACK
Xilinx
WebPACK
also
includes
XST
a
logic
synthesis
tool
supporting
both
Verilog
and
VHDL
The
difference
between
WebPACK
and
Foundation
is
that
WebPACK
supports
a
subset
of
the
most
common
Xilinx
FPGAs
The
difference
between
ModelSim
MXE
III
Starter
and
ModelSim
commercial
versions
is
that
Starter
degrades
performance
for
simulations
with
more
than
lines
of
HDL
Synplify
Pro
Synplify
Pro
is
a
high
performance
sophisticated
logic
synthesis
engine
for
FPGA
and
CPLD
designs
Synplify
Pro
also
contains
HDL
Analyst
a
graphical
interface
tool
that
generates
schematic
views
of
the
HDL
source
code
We
have
found
that
this
is
immensely
useful
in
the
learning
and
debugging
process
Synplicity
has
generously
agreed
to
donate
Synplify
Pro
to
qualified
universities
and
will
provide
as
many
licenses
as
needed
to
fill
university
labs
Instructors
should
visit
the
instructor
Web
page
for
this
text
for
more
information
on
how
to
request
Synplify
Pro
licenses
For
additional
information
on
Synplicity
and
its
other
software
visit
www
synplicity
com
university
PCSPIM
PCSPIM
also
called
simply
SPIM
is
a
Windows
based
MIPS
simulator
that
runs
MIPS
assembly
code
Students
enter
their
MIPS
assembly
code
into
a
text
file
and
run
it
using
PCSPIM
PCSPIM
displays
the
instructions
memory
and
register
values
Links
to
the
user
s
manual
and
an
example
file
are
available
at
the
companion
site
textbooks
elsevier
com
LABS
The
companion
site
includes
links
to
a
series
of
labs
that
cover
topics
from
digital
design
through
computer
architecture
The
labs
teach
students
how
to
use
the
Xilinx
WebPACK
or
Foundation
tools
to
enter
simulate
synthesize
and
implement
their
designs
The
labs
also
include
topics
on
assembly
language
programming
using
the
PCSPIM
simulator
After
synthesis
students
can
implement
their
designs
using
the
Digilent
Spartan
Starter
Board
or
the
XUP
Virtex
Pro
V
Pro
Board
Both
of
these
powerful
and
competitively
priced
boards
are
available
from
www
digilentinc
com
The
boards
contain
FPGAs
that
can
be
programmed
to
implement
student
designs
We
provide
labs
that
describe
how
to
implement
a
selection
of
designs
using
Digilent
s
Spartan
Board
using
xx
PREFACE
PREFACE
xxi
WebPACK
Unfortunately
Xilinx
WebPACK
does
not
support
the
huge
FPGA
on
the
V
Pro
board
Qualified
universities
may
contact
the
Xilinx
University
Program
to
request
a
donation
of
the
full
Foundation
tools
To
run
the
labs
students
will
need
to
download
and
install
the
Xilinx
WebPACK
PCSPIM
and
possibly
Synplify
Pro
Instructors
may
also
choose
to
install
the
tools
on
lab
machines
The
labs
include
instructions
on
how
to
implement
the
projects
on
the
Digilent
s
Spartan
Starter
Board
The
implementation
step
may
be
skipped
but
we
have
found
it
of
great
value
The
labs
will
also
work
with
the
XST
synthesis
tool
but
we
recommend
using
Synplify
Pro
because
the
schematics
it
produces
give
students
invaluable
feedback
We
have
tested
the
labs
on
Windows
but
the
tools
are
also
available
for
Linux
BUGS
As
all
experienced
programmers
know
any
program
of
significant
complexity
undoubtedly
contains
bugs
So
too
do
books
We
have
taken
great
care
to
find
and
squash
the
bugs
in
this
book
However
some
errors
undoubtedly
do
remain
We
will
maintain
a
list
of
errata
on
the
book
s
Web
page
Please
send
your
bug
reports
to
ddcabugs
onehotlogic
com
The
first
person
to
report
a
substantive
bug
with
a
fix
that
we
use
in
a
future
printing
will
be
rewarded
with
a
bounty
Be
sure
to
include
your
mailing
address
ACKNOWLEDGMENTS
First
and
foremost
we
thank
David
Patterson
and
John
Hennessy
for
their
pioneering
MIPS
microarchitectures
described
in
their
Computer
Organization
and
Design
textbook
We
have
taught
from
various
editions
of
their
book
for
many
years
We
appreciate
their
gracious
support
of
this
book
and
their
permission
to
build
on
their
microarchitectures
Duane
Bibby
our
favorite
cartoonist
labored
long
and
hard
to
illustrate
the
fun
and
adventure
of
digital
design
We
also
appreciate
the
enthusiasm
of
Denise
Penrose
Nate
McFadden
and
the
rest
of
the
team
at
Morgan
Kaufmann
who
made
this
book
happen
Jeff
Somers
at
Graphic
World
Publishing
Services
has
ably
guided
the
book
through
production
Numerous
reviewers
have
substantially
improved
the
book
They
include
John
Barr
Ithaca
College
Jack
V
Briner
Charleston
Southern
University
Andrew
C
Brown
SK
Communications
Carl
Baumgaertner
Harvey
Mudd
College
A
Utku
Diril
Nvidia
Corporation
Jim
Frenzel
University
of
Idaho
Jaeha
Kim
Rambus
Inc
Phillip
King
xxii
PREFACE
ShotSpotter
Inc
James
Pinter
Lucke
Claremont
McKenna
College
Amir
Roth
Z
Jerry
Shi
University
of
Connecticut
James
E
Stine
Oklahoma
State
University
Luke
Teyssier
Peiyi
Zhao
Chapman
University
and
an
anonymous
reviewer
Simon
Moore
was
a
wonderful
host
during
David
s
sabbatical
visit
to
Cambridge
University
where
major
sections
of
this
book
were
written
We
also
appreciate
the
students
in
our
course
at
Harvey
Mudd
College
who
have
given
us
helpful
feedback
on
drafts
of
this
textbook
Of
special
note
are
Casey
Schilling
Alice
Clifton
Chris
Acon
and
Stephen
Brawner
I
David
particularly
thank
my
wife
Jennifer
who
gave
birth
to
our
son
Abraham
at
the
beginning
of
the
project
I
appreciate
her
patience
and
loving
support
through
yet
another
project
at
a
busy
time
in
our
lives
The
Game
Plan
The
Art
of
Managing
Complexity
The
Digital
Abstraction
Number
Systems
Logic
Gates
Beneath
the
Digital
Abstraction
CMOS
Transistors
Power
Consumption
Summary
and
a
Look
Ahead
Exercises
Interview
Questions
From
Zero
to
One
THE
GAME
PLAN
Microprocessors
have
revolutionized
our
world
during
the
past
three
decades
A
laptop
computer
today
has
far
more
capability
than
a
room
sized
mainframe
of
yesteryear
A
luxury
automobile
contains
about
microprocessors
Advances
in
microprocessors
have
made
cell
phones
and
the
Internet
possible
have
vastly
improved
medicine
and
have
transformed
how
war
is
waged
Worldwide
semiconductor
industry
sales
have
grown
from
US
billion
in
to
billion
in
and
microprocessors
are
a
major
segment
of
these
sales
We
believe
that
microprocessors
are
not
only
technically
economically
and
socially
important
but
are
also
an
intrinsically
fascinating
human
invention
By
the
time
you
finish
reading
this
book
you
will
know
how
to
design
and
build
your
own
microprocessor
The
skills
you
learn
along
the
way
will
prepare
you
to
design
many
other
digital
systems
We
assume
that
you
have
a
basic
familiarity
with
electricity
some
prior
programming
experience
and
a
genuine
interest
in
understanding
what
goes
on
under
the
hood
of
a
computer
This
book
focuses
on
the
design
of
digital
systems
which
operate
on
s
and
s
We
begin
with
digital
logic
gates
that
accept
s
and
s
as
inputs
and
produce
s
and
s
as
outputs
We
then
explore
how
to
combine
logic
gates
into
more
complicated
modules
such
as
adders
and
memories
Then
we
shift
gears
to
programming
in
assembly
language
the
native
tongue
of
the
microprocessor
Finally
we
put
gates
together
to
build
a
microprocessor
that
runs
these
assembly
language
programs
A
great
advantage
of
digital
systems
is
that
the
building
blocks
are
quite
simple
just
s
and
s
They
do
not
require
grungy
mathematics
or
a
profound
knowledge
of
physics
Instead
the
designer
s
challenge
is
to
combine
these
simple
blocks
into
complicated
systems
A
microprocessor
may
be
the
first
system
that
you
build
that
is
too
complex
to
fit
in
your
head
all
at
once
One
of
the
major
themes
weaved
through
this
book
is
how
to
manage
complexity
THE
ART
OF
MANAGING
COMPLEXITY
One
of
the
characteristics
that
separates
an
engineer
or
computer
scientist
from
a
layperson
is
a
systematic
approach
to
managing
complexity
Modern
digital
systems
are
built
from
millions
or
billions
of
transistors
No
human
being
could
understand
these
systems
by
writing
equations
describing
the
movement
of
electrons
in
each
transistor
and
solving
all
of
the
equations
simultaneously
You
will
need
to
learn
to
manage
complexity
to
understand
how
to
build
a
microprocessor
without
getting
mired
in
a
morass
of
detail
Abstraction
The
critical
technique
for
managing
complexity
is
abstraction
hiding
details
when
they
are
not
important
A
system
can
be
viewed
from
many
different
levels
of
abstraction
For
example
American
politicians
abstract
the
world
into
cities
counties
states
and
countries
A
county
contains
multiple
cities
and
a
state
contains
many
counties
When
a
politician
is
running
for
president
the
politician
is
mostly
interested
in
how
the
state
as
a
whole
will
vote
rather
than
how
each
county
votes
so
the
state
is
the
most
useful
level
of
abstraction
On
the
other
hand
the
Census
Bureau
measures
the
population
of
every
city
so
the
agency
must
consider
the
details
of
a
lower
level
of
abstraction
Figure
illustrates
levels
of
abstraction
for
an
electronic
computer
system
along
with
typical
building
blocks
at
each
level
At
the
lowest
level
of
abstraction
is
the
physics
the
motion
of
electrons
The
behavior
of
electrons
is
described
by
quantum
mechanics
and
Maxwell
s
equations
Our
system
is
constructed
from
electronic
devices
such
as
transistors
or
vacuum
tubes
once
upon
a
time
These
devices
have
well
defined
connection
points
called
terminals
and
can
be
modeled
by
the
relationship
between
voltage
and
current
as
measured
at
each
terminal
By
abstracting
to
this
device
level
we
can
ignore
the
individual
electrons
The
next
level
of
abstraction
is
analog
circuits
in
which
devices
are
assembled
to
create
components
such
as
amplifiers
Analog
circuits
input
and
output
a
continuous
range
of
voltages
Digital
circuits
such
as
logic
gates
restrict
the
voltages
to
discrete
ranges
which
we
will
use
to
indicate
and
In
logic
design
we
build
more
complex
structures
such
as
adders
or
memories
from
digital
circuits
Microarchitecture
links
the
logic
and
architecture
levels
of
abstraction
The
architecture
level
of
abstraction
describes
a
computer
from
the
programmer
s
perspective
For
example
the
Intel
IA
architecture
used
by
microprocessors
in
most
personal
computers
PCs
is
defined
by
a
set
of
CHAPTER
ONE
From
Zero
to
One
Physics
Devices
Analog
Circuits
Digital
Circuits
Logic
Microarchitecture
Architecture
Operating
Systems
Application
Software
Electrons
Transistors
Diodes
Amplifiers
Filters
AND
gates
NOT
gates
Adders
Memories
Datapaths
Controllers
Instructions
Registers
Device
Drivers
Programs
Figure
Levels
of
abstraction
for
electronic
computing
system
instructions
and
registers
memory
for
temporarily
storing
variables
that
the
programmer
is
allowed
to
use
Microarchitecture
involves
combining
logic
elements
to
execute
the
instructions
defined
by
the
architecture
A
particular
architecture
can
be
implemented
by
one
of
many
different
microarchitectures
with
different
price
performance
power
trade
offs
For
example
the
Intel
Core
Duo
the
Intel
and
the
AMD
Athlon
all
implement
the
IA
architecture
with
different
microarchitectures
Moving
into
the
software
realm
the
operating
system
handles
lowlevel
details
such
as
accessing
a
hard
drive
or
managing
memory
Finally
the
application
software
uses
these
facilities
provided
by
the
operating
system
to
solve
a
problem
for
the
user
Thanks
to
the
power
of
abstraction
your
grandmother
can
surf
the
Web
without
any
regard
for
the
quantum
vibrations
of
electrons
or
the
organization
of
the
memory
in
her
computer
This
book
focuses
on
the
levels
of
abstraction
from
digital
circuits
through
computer
architecture
When
you
are
working
at
one
level
of
abstraction
it
is
good
to
know
something
about
the
levels
of
abstraction
immediately
above
and
below
where
you
are
working
For
example
a
computer
scientist
cannot
fully
optimize
code
without
understanding
the
architecture
for
which
the
program
is
being
written
A
device
engineer
cannot
make
wise
trade
offs
in
transistor
design
without
understanding
the
circuits
in
which
the
transistors
will
be
used
We
hope
that
by
the
time
you
finish
reading
this
book
you
can
pick
the
level
of
abstraction
appropriate
to
solving
your
problem
and
evaluate
the
impact
of
your
design
choices
on
other
levels
of
abstraction
Discipline
Discipline
is
the
act
of
intentionally
restricting
your
design
choices
so
that
you
can
work
more
productively
at
a
higher
level
of
abstraction
Using
interchangeable
parts
is
a
familiar
application
of
discipline
One
of
the
first
examples
of
interchangeable
parts
was
in
flintlock
rifle
manufacturing
Until
the
early
th
century
rifles
were
individually
crafted
by
hand
Components
purchased
from
many
different
craftsmen
were
carefully
filed
and
fit
together
by
a
highly
skilled
gunmaker
The
discipline
of
interchangeable
parts
revolutionized
the
industry
By
limiting
the
components
to
a
standardized
set
with
well
defined
tolerances
rifles
could
be
assembled
and
repaired
much
faster
and
with
less
skill
The
gunmaker
no
longer
concerned
himself
with
lower
levels
of
abstraction
such
as
the
specific
shape
of
an
individual
barrel
or
gunstock
In
the
context
of
this
book
the
digital
discipline
will
be
very
important
Digital
circuits
use
discrete
voltages
whereas
analog
circuits
use
continuous
voltages
Therefore
digital
circuits
are
a
subset
of
analog
circuits
and
in
some
sense
must
be
capable
of
less
than
the
broader
class
of
analog
circuits
However
digital
circuits
are
much
simpler
to
design
By
limiting
The
Art
of
Managing
Complexity
ourselves
to
digital
circuits
we
can
easily
combine
components
into
sophisticated
systems
that
ultimately
outperform
those
built
from
analog
components
in
many
applications
For
example
digital
televisions
compact
disks
CDs
and
cell
phones
are
replacing
their
analog
predecessors
The
Three
Y
s
In
addition
to
abstraction
and
discipline
designers
use
the
three
y
s
to
manage
complexity
hierarchy
modularity
and
regularity
These
principles
apply
to
both
software
and
hardware
systems
Hierarchy
involves
dividing
a
system
into
modules
then
further
subdividing
each
of
these
modules
until
the
pieces
are
easy
to
understand
Modularity
states
that
the
modules
have
well
defined
functions
and
interfaces
so
that
they
connect
together
easily
without
unanticipated
side
effects
Regularity
seeks
uniformity
among
the
modules
Common
modules
are
reused
many
times
reducing
the
number
of
distinct
modules
that
must
be
designed
To
illustrate
these
y
s
we
return
to
the
example
of
rifle
manufacturing
A
flintlock
rifle
was
one
of
the
most
intricate
objects
in
common
use
in
the
early
th
century
Using
the
principle
of
hierarchy
we
can
break
it
into
components
shown
in
Figure
the
lock
stock
and
barrel
The
barrel
is
the
long
metal
tube
through
which
the
bullet
is
fired
The
lock
is
the
firing
mechanism
And
the
stock
is
the
wooden
body
that
holds
the
parts
together
and
provides
a
secure
grip
for
the
user
In
turn
the
lock
contains
the
trigger
hammer
flint
frizzen
and
pan
Each
of
these
components
could
be
hierarchically
described
in
further
detail
Modularity
teaches
that
each
component
should
have
a
well
defined
function
and
interface
A
function
of
the
stock
is
to
mount
the
barrel
and
lock
Its
interface
consists
of
its
length
and
the
location
of
its
mounting
pins
In
a
modular
rifle
design
stocks
from
many
different
manufacturers
can
be
used
with
a
particular
barrel
as
long
as
the
stock
and
barrel
are
of
the
correct
length
and
have
the
proper
mounting
mechanism
A
function
of
the
barrel
is
to
impart
spin
to
the
bullet
so
that
it
travels
more
accurately
Modularity
dictates
that
there
should
be
no
side
effects
the
design
of
the
stock
should
not
impede
the
function
of
the
barrel
Regularity
teaches
that
interchangeable
parts
are
a
good
idea
With
regularity
a
damaged
barrel
can
be
replaced
by
an
identical
part
The
barrels
can
be
efficiently
built
on
an
assembly
line
instead
of
being
painstakingly
hand
crafted
We
will
return
to
these
principles
of
hierarchy
modularity
and
regularity
throughout
the
book
CHAPTER
ONE
From
Zero
to
One
Captain
Meriwether
Lewis
of
the
Lewis
and
Clark
Expedition
was
one
of
the
early
advocates
of
interchangeable
parts
for
rifles
In
he
explained
The
guns
of
Drewyer
and
Sergt
Pryor
were
both
out
of
order
The
first
was
repared
with
a
new
lock
the
old
one
having
become
unfit
for
use
the
second
had
the
cock
screw
broken
which
was
replaced
by
a
duplicate
which
had
been
prepared
for
the
lock
at
Harpers
Ferry
where
she
was
manufactured
But
for
the
precaution
taken
in
bringing
on
those
extra
locks
and
parts
of
locks
in
addition
to
the
ingenuity
of
John
Shields
most
of
our
guns
would
at
this
moment
been
entirely
unfit
for
use
but
fortunately
for
us
I
have
it
in
my
power
here
to
record
that
they
are
all
in
good
order
See
Elliott
Coues
ed
The
History
of
the
Lewis
and
Clark
Expedition
vols
New
York
Harper
reprint
vols
New
York
Dover
Cha
THE
DIGITAL
ABSTRACTION
Most
physical
variables
are
continuous
For
example
the
voltage
on
a
wire
the
frequency
of
an
oscillation
or
the
position
of
a
mass
are
all
continuous
quantities
Digital
systems
on
the
other
hand
represent
information
with
discrete
valued
variables
that
is
variables
with
a
finite
number
of
distinct
values
An
early
digital
system
using
variables
with
ten
discrete
values
was
Charles
Babbage
s
Analytical
Engine
Babbage
labored
from
to
designing
and
attempting
to
build
this
mechanical
computer
The
Analytical
Engine
used
gears
with
ten
positions
labeled
through
much
like
a
mechanical
odometer
in
a
car
Figure
shows
a
prototype
The
Digital
Abstraction
Barrel
Stoc
Lock
Expanded
view
of
Lock
k
Flint
Cock
Pan
Spring
String
Figure
Flintlock
rifle
with
a
close
up
view
of
the
lock
Image
by
Euroams
Italia
www
euroarms
net
And
we
thought
graduate
school
was
long
Charles
Babbage
Attended
Cambridge
University
and
married
Georgiana
Whitmore
in
Invented
the
Analytical
Engine
the
world
s
first
mechanical
computer
Also
invented
the
cowcatcher
and
the
universal
postage
rate
Interested
in
lock
picking
but
abhorred
street
musicians
image
courtesy
of
Fourmilab
Switzerland
www
fourmilab
ch
of
the
Analytical
Engine
in
which
each
row
processes
one
digit
Babbage
chose
rows
of
gears
so
the
machine
has
digit
precision
Unlike
Babbage
s
machine
most
electronic
computers
use
a
binary
two
valued
representation
in
which
a
high
voltage
indicates
a
and
a
low
voltage
indicates
a
because
it
is
easier
to
distinguish
between
two
voltages
than
ten
The
amount
of
information
D
in
a
discrete
valued
variable
with
N
distinct
states
is
measured
in
units
of
bits
as
D
log
N
bits
A
binary
variable
conveys
log
bit
of
information
Indeed
the
word
bit
is
short
for
binary
digit
Each
of
Babbage
s
gears
carried
log
bits
of
information
because
it
could
be
in
one
of
unique
positions
A
continuous
signal
theoretically
contains
an
infinite
amount
of
information
because
it
can
take
on
an
infinite
number
of
values
In
practice
noise
and
measurement
error
limit
the
information
to
only
to
bits
for
most
continuous
signals
If
the
measurement
must
be
made
rapidly
the
information
content
is
lower
e
g
bits
This
book
focuses
on
digital
circuits
using
binary
variables
s
and
s
George
Boole
developed
a
system
of
logic
operating
on
binary
variables
that
is
now
known
as
Boolean
logic
Each
of
Boole
s
variables
could
be
TRUE
or
FALSE
Electronic
computers
commonly
use
a
positive
voltage
to
represent
and
zero
volts
to
represent
In
this
book
we
will
use
the
terms
TRUE
and
HIGH
synonymously
Similarly
we
will
use
FALSE
and
LOW
interchangeably
The
beauty
of
the
digital
abstraction
is
that
digital
designers
can
focus
on
s
and
s
ignoring
whether
the
Boolean
variables
are
physically
represented
with
specific
voltages
rotating
gears
or
even
hydraulic
CHAPTER
ONE
From
Zero
to
One
Figure
Babbage
s
Analytical
Engine
under
construction
at
the
time
of
his
death
in
image
courtesy
of
Science
Museum
Science
and
Society
Picture
Library
George
Boole
Born
to
working
class
parents
and
unable
to
afford
a
formal
education
Boole
taught
himself
mathematics
and
joined
the
faculty
of
Queen
s
College
in
Ireland
He
wrote
An
Investigation
of
the
Laws
of
Thought
which
introduced
binary
variables
and
the
three
fundamental
logic
operations
AND
OR
and
NOT
image
courtesy
of
xxx
Chap
fluid
levels
A
computer
programmer
can
work
without
needing
to
know
the
intimate
details
of
the
computer
hardware
On
the
other
hand
understanding
the
details
of
the
hardware
allows
the
programmer
to
optimize
the
software
better
for
that
specific
computer
An
individual
bit
doesn
t
carry
much
information
In
the
next
section
we
examine
how
groups
of
bits
can
be
used
to
represent
numbers
In
later
chapters
we
will
also
use
groups
of
bits
to
represent
letters
and
programs
NUMBER
SYSTEMS
You
are
accustomed
to
working
with
decimal
numbers
In
digital
systems
consisting
of
s
and
s
binary
or
hexadecimal
numbers
are
often
more
convenient
This
section
introduces
the
various
number
systems
that
will
be
used
throughout
the
rest
of
the
book
Decimal
Numbers
In
elementary
school
you
learned
to
count
and
do
arithmetic
in
decimal
Just
as
you
probably
have
ten
fingers
there
are
ten
decimal
digits
Decimal
digits
are
joined
together
to
form
longer
decimal
numbers
Each
column
of
a
decimal
number
has
ten
times
the
weight
of
the
previous
column
From
right
to
left
the
column
weights
are
and
so
on
Decimal
numbers
are
referred
to
as
base
The
base
is
indicated
by
a
subscript
after
the
number
to
prevent
confusion
when
working
in
more
than
one
base
For
example
Figure
shows
how
the
decimal
number
is
written
as
the
sum
of
each
of
its
digits
multiplied
by
the
weight
of
the
corresponding
column
An
N
digit
decimal
number
represents
one
of
N
possibilities
N
This
is
called
the
range
of
the
number
For
example
a
three
digit
decimal
number
represents
one
of
possibilities
in
the
range
of
to
Binary
Numbers
Bits
represent
one
of
two
values
or
and
are
joined
together
to
form
binary
numbers
Each
column
of
a
binary
number
has
twice
the
weight
Number
Systems
nine
thousands
s
column
s
column
s
column
seven
hundreds
four
tens
two
ones
s
column
Figure
Representation
of
a
decimal
number
C
of
the
previous
column
so
binary
numbers
are
base
In
binary
the
column
weights
again
from
right
to
left
are
and
so
on
If
you
work
with
binary
numbers
often
you
ll
save
time
if
you
remember
these
powers
of
two
up
to
An
N
bit
binary
number
represents
one
of
N
possibilities
N
Table
shows
and
bit
binary
numbers
and
their
decimal
equivalents
Example
BINARY
TO
DECIMAL
CONVERSION
Convert
the
binary
number
to
decimal
Solution
Figure
shows
the
conversion
CHAPTER
ONE
From
Zero
to
One
Bit
Bit
Bit
Bit
Binary
Binnary
Binary
Binary
Decimal
Numbers
Numbers
Numbers
Numbers
Equivalents
Table
Binary
numbers
and
their
decimal
equivalent
C
Example
DECIMAL
TO
BINARY
CONVERSION
Convert
the
decimal
number
to
binary
Solution
Determine
whether
each
column
of
the
binary
result
has
a
or
a
We
can
do
this
starting
at
either
the
left
or
the
right
column
Working
from
the
left
start
with
the
largest
power
of
less
than
the
number
in
this
case
so
there
is
a
in
the
s
column
leaving
so
there
is
a
in
the
s
column
so
there
is
a
in
the
s
column
leaving
so
there
is
a
in
the
s
column
so
there
is
a
in
the
s
column
leaving
Thus
there
must
be
s
in
the
s
and
s
column
Putting
this
all
together
Working
from
the
right
repeatedly
divide
the
number
by
The
remainder
goes
in
each
column
so
goes
in
the
s
column
so
goes
in
the
s
column
with
a
remainder
of
going
in
the
s
column
so
goes
in
the
s
column
with
a
remainder
of
going
in
the
s
column
so
goes
in
the
s
column
Finally
with
a
remainder
of
going
in
the
s
column
Again
Hexadecimal
Numbers
Writing
long
binary
numbers
becomes
tedious
and
prone
to
error
A
group
of
four
bits
represents
one
of
possibilities
Hence
it
is
sometimes
more
convenient
to
work
in
base
called
hexadecimal
Hexadecimal
numbers
use
the
digits
to
along
with
the
letters
A
to
F
as
shown
in
Table
Columns
in
base
have
weights
of
or
or
and
so
on
Example
HEXADECIMAL
TO
BINARY
AND
DECIMAL
CONVERSION
Convert
the
hexadecimal
number
ED
to
binary
and
to
decimal
Solution
Conversion
between
hexadecimal
and
binary
is
easy
because
each
hexadecimal
digit
directly
corresponds
to
four
binary
digits
E
and
D
so
ED
Conversion
to
decimal
requires
the
arithmetic
shown
in
Figure
Number
Systems
one
sixteen
s
column
no
eight
one
four
one
two
no
one
s
column
s
column
s
column
s
column
Figure
Conversion
of
a
binary
number
to
decimal
Hexadecimal
a
term
coined
by
IBM
in
derives
from
the
Greek
hexi
six
and
Latin
decem
ten
A
more
proper
term
would
use
the
Latin
sexa
six
but
sexidecimal
sounded
too
risqu
Chapter
qxd
Example
BINARY
TO
HEXADECIMAL
CONVERSION
Convert
the
binary
number
to
hexadecimal
Solution
Again
conversion
is
easy
Start
reading
from
the
right
The
four
least
significant
bits
are
A
The
next
bits
are
Hence
A
CHAPTER
ONE
From
Zero
to
One
Table
Hexadecimal
number
system
Hexadecimal
Digit
Decimal
Equivalent
Binary
Equivalent
A
B
C
D
E
F
ED
E
D
two
two
hundred
fifty
six
s
s
column
fourteen
sixteens
thirteen
ones
s
column
s
column
Figure
Conversion
of
hexadecimal
number
to
decimal
Cha
Example
DECIMAL
TO
HEXADECIMAL
AND
BINARY
CONVERSION
Convert
the
decimal
number
to
hexadecimal
and
binary
Solution
Like
decimal
to
binary
conversion
decimal
to
hexadecimal
conversion
can
be
done
from
the
left
or
the
right
Working
from
the
left
start
with
the
largest
power
of
less
than
the
number
in
this
case
goes
into
once
so
there
is
a
in
the
s
column
leaving
goes
into
four
times
so
there
is
a
in
the
s
column
leaving
D
so
there
is
a
D
in
the
s
column
In
summary
D
Now
it
is
easy
to
convert
from
hexadecimal
to
binary
as
in
Example
D
Working
from
the
right
repeatedly
divide
the
number
by
The
remainder
goes
in
each
column
with
a
remainder
of
D
going
in
the
s
column
with
a
remainder
of
going
in
the
s
column
with
a
remainder
of
going
in
the
s
column
Again
the
result
is
D
Bytes
Nibbles
and
All
That
Jazz
A
group
of
eight
bits
is
called
a
byte
It
represents
one
of
possibilities
The
size
of
objects
stored
in
computer
memories
is
customarily
measured
in
bytes
rather
than
bits
A
group
of
four
bits
or
half
a
byte
is
called
a
nibble
It
represents
one
of
possibilities
One
hexadecimal
digit
stores
one
nibble
and
two
hexadecimal
digits
store
one
full
byte
Nibbles
are
no
longer
a
commonly
used
unit
but
the
term
is
cute
Microprocessors
handle
data
in
chunks
called
words
The
size
of
a
word
depends
on
the
architecture
of
the
microprocessor
When
this
chapter
was
written
in
most
computers
had
bit
processors
indicating
that
they
operate
on
bit
words
At
the
time
computers
handling
bit
words
were
on
the
verge
of
becoming
widely
available
Simpler
microprocessors
especially
those
used
in
gadgets
such
as
toasters
use
or
bit
words
Within
a
group
of
bits
the
bit
in
the
s
column
is
called
the
least
significant
bit
lsb
and
the
bit
at
the
other
end
is
called
the
most
significant
bit
msb
as
shown
in
Figure
a
for
a
bit
binary
number
Similarly
within
a
word
the
bytes
are
identified
as
least
significant
byte
LSB
through
most
significant
byte
MSB
as
shown
in
Figure
b
for
a
four
byte
number
written
with
eight
hexadecimal
digits
Number
Systems
A
microprocessor
is
a
processor
built
on
a
single
chip
Until
the
s
processors
were
too
complicated
to
fit
on
one
chip
so
mainframe
processors
were
built
from
boards
containing
many
chips
Intel
introduced
the
first
bit
microprocessor
called
the
in
Now
even
the
most
sophisticated
supercomputers
are
built
using
microprocessors
We
will
use
the
terms
microprocessor
and
processor
interchangeably
throughout
this
book
Chapter
qx
By
handy
coincidence
Hence
the
term
kilo
Greek
for
thousand
indicates
For
example
bytes
is
one
kilobyte
KB
Similarly
mega
million
indicates
and
giga
billion
indicates
If
you
know
thousand
million
billion
and
remember
the
powers
of
two
up
to
it
is
easy
to
estimate
any
power
of
two
in
your
head
Example
ESTIMATING
POWERS
OF
TWO
Find
the
approximate
value
of
without
using
a
calculator
Solution
Split
the
exponent
into
a
multiple
of
ten
and
the
remainder
million
So
million
Technically
but
million
is
close
enough
for
marketing
purposes
bytes
is
called
a
kilobyte
KB
bits
is
called
a
kilobit
Kb
or
Kbit
Similarly
MB
Mb
GB
and
Gb
are
used
for
millions
and
billions
of
bytes
and
bits
Memory
capacity
is
usually
measured
in
bytes
Communication
speed
is
usually
measured
in
bits
sec
For
example
the
maximum
speed
of
a
dial
up
modem
is
usually
Kbits
sec
Binary
Addition
Binary
addition
is
much
like
decimal
addition
but
easier
as
shown
in
Figure
As
in
decimal
addition
if
the
sum
of
two
numbers
is
greater
than
what
fits
in
a
single
digit
we
carry
a
into
the
next
column
Figure
compares
addition
of
decimal
and
binary
numbers
In
the
right
most
column
of
Figure
a
which
cannot
fit
in
a
single
digit
because
it
is
greater
than
So
we
record
the
s
digit
and
carry
the
s
digit
over
to
the
next
column
Likewise
in
binary
if
the
sum
of
two
numbers
is
greater
than
we
carry
the
s
digit
over
to
the
next
column
For
example
in
the
right
most
column
of
Figure
b
the
sum
CHAPTER
ONE
From
Zero
to
One
least
significant
bit
most
significant
bit
a
b
DEAFDAD
least
significant
byte
most
significant
byte
carries
a
b
Figure
Least
and
most
significant
bits
and
bytes
Figure
Addition
examples
showing
carries
a
decimal
b
binary
Chapter
qx
cannot
fit
in
a
single
binary
digit
So
we
record
the
s
digit
and
carry
the
s
digit
of
the
result
to
the
next
column
In
the
second
column
the
sum
is
Again
we
record
the
s
digit
and
carry
the
s
digit
to
the
next
column
For
obvious
reasons
the
bit
that
is
carried
over
to
the
neighboring
column
is
called
the
carry
bit
Example
BINARY
ADDITION
Compute
Solution
Figure
shows
that
the
sum
is
The
carries
are
indicated
in
blue
We
can
check
our
work
by
repeating
the
computation
in
decimal
The
sum
is
Digital
systems
usually
operate
on
a
fixed
number
of
digits
Addition
is
said
to
overflow
if
the
result
is
too
big
to
fit
in
the
available
digits
A
bit
number
for
example
has
the
range
bit
binary
addition
overflows
if
the
result
exceeds
The
fifth
bit
is
discarded
producing
an
incorrect
result
in
the
remaining
four
bits
Overflow
can
be
detected
by
checking
for
a
carry
out
of
the
most
significant
column
Example
ADDITION
WITH
OVERFLOW
Compute
Does
overflow
occur
Solution
Figure
shows
the
sum
is
This
result
overflows
the
range
of
a
bit
binary
number
If
it
must
be
stored
as
four
bits
the
most
significant
bit
is
discarded
leaving
the
incorrect
result
of
If
the
computation
had
been
done
using
numbers
with
five
or
more
bits
the
result
would
have
been
correct
Signed
Binary
Numbers
So
far
we
have
considered
only
unsigned
binary
numbers
that
represent
positive
quantities
We
will
often
want
to
represent
both
positive
and
negative
numbers
requiring
a
different
binary
number
system
Several
schemes
exist
to
represent
signed
binary
numbers
the
two
most
widely
employed
are
called
sign
magnitude
and
two
s
complement
Sign
Magnitude
Numbers
Sign
magnitude
numbers
are
intuitively
appealing
because
they
match
our
custom
of
writing
negative
numbers
with
a
minus
sign
followed
by
the
magnitude
An
N
bit
sign
magnitude
number
uses
the
most
significant
bit
Number
Systems
Figure
Binary
addition
example
Figure
Binary
addition
example
with
overflow
Chapter
as
the
sign
and
the
remaining
N
bits
as
the
magnitude
absolute
value
A
sign
bit
of
indicates
positive
and
a
sign
bit
of
indicates
negative
Example
SIGN
MAGNITUDE
NUMBERS
Write
and
as
bit
sign
magnitude
numbers
Solution
Both
numbers
have
a
magnitude
of
Thus
and
Unfortunately
ordinary
binary
addition
does
not
work
for
sign
magnitude
numbers
For
example
using
ordinary
addition
on
gives
which
is
nonsense
An
N
bit
sign
magnitude
number
spans
the
range
N
N
Sign
magnitude
numbers
are
slightly
odd
in
that
both
and
exist
Both
indicate
zero
As
you
may
expect
it
can
be
troublesome
to
have
two
different
representations
for
the
same
number
Two
s
Complement
Numbers
Two
s
complement
numbers
are
identical
to
unsigned
binary
numbers
except
that
the
most
significant
bit
position
has
a
weight
of
N
instead
of
N
They
overcome
the
shortcomings
of
sign
magnitude
numbers
zero
has
a
single
representation
and
ordinary
addition
works
In
two
s
complement
representation
zero
is
written
as
all
zeros
The
most
positive
number
has
a
in
the
most
significant
position
and
s
elsewhere
N
The
most
negative
number
has
a
in
the
most
significant
position
and
s
elsewhere
N
And
is
written
as
all
ones
Notice
that
positive
numbers
have
a
in
the
most
significant
position
and
negative
numbers
have
a
in
this
position
so
the
most
significant
bit
can
be
viewed
as
the
sign
bit
However
the
remaining
bits
are
interpreted
differently
for
two
s
complement
numbers
than
for
sign
magnitude
numbers
The
sign
of
a
two
s
complement
number
is
reversed
in
a
process
called
taking
the
two
s
complement
The
process
consists
of
inverting
all
of
the
bits
in
the
number
then
adding
to
the
least
significant
bit
position
This
is
useful
to
find
the
representation
of
a
negative
number
or
to
determine
the
magnitude
of
a
negative
number
Example
TWO
S
COMPLEMENT
REPRESENTATION
OF
A
NEGATIVE
NUMBER
Find
the
representation
of
as
a
bit
two
s
complement
number
CHAPTER
ONE
From
Zero
to
One
The
billion
Ariane
rocket
launched
on
June
veered
off
course
seconds
after
launch
broke
up
and
exploded
The
failure
was
caused
when
the
computer
controlling
the
rocket
overflowed
its
bit
range
and
crashed
The
code
had
been
extensively
tested
on
the
Ariane
rocket
However
the
Ariane
had
a
faster
engine
that
produced
larger
values
for
the
control
computer
leading
to
the
overflow
Photograph
courtesy
ESA
CNES
ARIANESPACEService
Optique
CS
Chapter
qxd
Solution
Start
with
To
get
invert
the
bits
and
add
Inverting
produces
So
is
Example
VALUE
OF
NEGATIVE
TWO
S
COMPLEMENT
NUMBERS
Find
the
decimal
value
of
the
two
s
complement
number
Solution
has
a
leading
so
it
must
be
negative
To
find
its
magnitude
invert
the
bits
and
add
Inverting
Hence
Two
s
complement
numbers
have
the
compelling
advantage
that
addition
works
properly
for
both
positive
and
negative
numbers
Recall
that
when
adding
N
bit
numbers
the
carry
out
of
the
Nth
bit
i
e
the
N
th
result
bit
is
discarded
Example
ADDING
TWO
S
COMPLEMENT
NUMBERS
Compute
a
and
b
using
two
s
complement
numbers
Solution
a
b
The
fifth
bit
is
discarded
leaving
the
correct
bit
result
Subtraction
is
performed
by
taking
the
two
s
complement
of
the
second
number
then
adding
Example
SUBTRACTING
TWO
S
COMPLEMENT
NUMBERS
Compute
a
and
b
using
bit
two
s
complement
numbers
Solution
a
Take
its
two
s
complement
to
obtain
Now
add
Note
that
the
carry
out
of
the
most
significant
position
is
discarded
because
the
result
is
stored
in
four
bits
b
Take
the
two
s
complement
of
to
obtain
Now
add
The
two
s
complement
of
is
found
by
inverting
all
the
bits
producing
and
adding
which
produces
all
s
disregarding
the
carry
out
of
the
most
significant
bit
position
Hence
zero
is
always
represented
with
all
s
Unlike
the
sign
magnitude
system
the
two
s
complement
system
has
no
separate
Zero
is
considered
positive
because
its
sign
bit
is
Number
Systems
Chapter
qxd
AM
Page
Like
unsigned
numbers
N
bit
two
s
complement
numbers
represent
one
of
N
possible
values
However
the
values
are
split
between
positive
and
negative
numbers
For
example
a
bit
unsigned
number
represents
values
to
A
bit
two
s
complement
number
also
represents
values
to
In
general
the
range
of
an
N
bit
two
s
complement
number
spans
N
N
It
should
make
sense
that
there
is
one
more
negative
number
than
positive
number
because
there
is
no
The
most
negative
number
N
is
sometimes
called
the
weird
number
Its
two
s
complement
is
found
by
inverting
the
bits
producing
and
adding
which
produces
the
weird
number
again
Hence
this
negative
number
has
no
positive
counterpart
Adding
two
N
bit
positive
numbers
or
negative
numbers
may
cause
overflow
if
the
result
is
greater
than
N
or
less
than
N
Adding
a
positive
number
to
a
negative
number
never
causes
overflow
Unlike
unsigned
numbers
a
carry
out
of
the
most
significant
column
does
not
indicate
overflow
Instead
overflow
occurs
if
the
two
numbers
being
added
have
the
same
sign
bit
and
the
result
has
the
opposite
sign
bit
Example
ADDING
TWO
S
COMPLEMENT
NUMBERS
WITH
OVERFLOW
Compute
a
using
bit
two
s
complement
numbers
Does
the
result
overflow
Solution
a
The
result
overflows
the
range
of
bit
positive
two
s
complement
numbers
producing
an
incorrect
negative
result
If
the
computation
had
been
done
using
five
or
more
bits
the
result
would
have
been
correct
When
a
two
s
complement
number
is
extended
to
more
bits
the
sign
bit
must
be
copied
into
the
most
significant
bit
positions
This
process
is
called
sign
extension
For
example
the
numbers
and
are
written
as
bit
two
s
complement
numbers
and
respectively
They
are
sign
extended
to
seven
bits
by
copying
the
sign
bit
into
the
three
new
upper
bits
to
form
and
respectively
Comparison
of
Number
Systems
The
three
most
commonly
used
binary
number
systems
are
unsigned
two
s
complement
and
sign
magnitude
Table
compares
the
range
of
N
bit
numbers
in
each
of
these
three
systems
Two
s
complement
numbers
are
convenient
because
they
represent
both
positive
and
negative
integers
and
because
ordinary
addition
works
for
all
numbers
CHAPTER
ONE
From
Zero
to
One
Chapter
qxd
Subtraction
is
performed
by
negating
the
second
number
i
e
taking
the
two
s
complement
and
then
adding
Unless
stated
otherwise
assume
that
all
signed
binary
numbers
use
two
s
complement
representation
Figure
shows
a
number
line
indicating
the
values
of
bit
numbers
in
each
system
Unsigned
numbers
span
the
range
in
regular
binary
order
Two
s
complement
numbers
span
the
range
The
nonnegative
numbers
share
the
same
encodings
as
unsigned
numbers
The
negative
numbers
are
encoded
such
that
a
larger
unsigned
binary
value
represents
a
number
closer
to
Notice
that
the
weird
number
represents
and
has
no
positive
counterpart
Sign
magnitude
numbers
span
the
range
The
most
significant
bit
is
the
sign
bit
The
positive
numbers
share
the
same
encodings
as
unsigned
numbers
The
negative
numbers
are
symmetric
but
have
the
sign
bit
set
is
represented
by
both
and
Thus
N
bit
sign
magnitude
numbers
represent
only
N
integers
because
of
the
two
representations
for
LOGIC
GATES
Now
that
we
know
how
to
use
binary
variables
to
represent
information
we
explore
digital
systems
that
perform
operations
on
these
binary
variables
Logic
gates
are
simple
digital
circuits
that
take
one
or
more
binary
inputs
and
produce
a
binary
output
Logic
gates
are
drawn
with
a
symbol
showing
the
input
or
inputs
and
the
output
Inputs
are
Logic
Gates
Two
s
Complement
Sign
Magnitude
Unsigned
Table
Range
of
N
bit
numbers
System
Range
Unsigned
N
Sign
Magnitude
N
N
Two
s
Complement
N
N
Figure
Number
line
and
bit
binary
encodings
Chapter
qxd
usually
drawn
on
the
left
or
top
and
outputs
on
the
right
or
bottom
Digital
designers
typically
use
letters
near
the
beginning
of
the
alphabet
for
gate
inputs
and
the
letter
Y
for
the
gate
output
The
relationship
between
the
inputs
and
the
output
can
be
described
with
a
truth
table
or
a
Boolean
equation
A
truth
table
lists
inputs
on
the
left
and
the
corresponding
output
on
the
right
It
has
one
row
for
each
possible
combination
of
inputs
A
Boolean
equation
is
a
mathematical
expression
using
binary
variables
NOT
Gate
A
NOT
gate
has
one
input
A
and
one
output
Y
as
shown
in
Figure
The
NOT
gate
s
output
is
the
inverse
of
its
input
If
A
is
FALSE
then
Y
is
TRUE
If
A
is
TRUE
then
Y
is
FALSE
This
relationship
is
summarized
by
the
truth
table
and
Boolean
equation
in
the
figure
The
line
over
A
in
the
Boolean
equation
is
pronounced
NOT
so
Y
A
is
read
Y
equals
NOT
A
The
NOT
gate
is
also
called
an
inverter
Other
texts
use
a
variety
of
notations
for
NOT
including
Y
A
Y
A
Y
A
or
Y
A
We
will
use
Y
A
exclusively
but
don
t
be
puzzled
if
you
encounter
another
notation
elsewhere
Buffer
The
other
one
input
logic
gate
is
called
a
buffer
and
is
shown
in
Figure
It
simply
copies
the
input
to
the
output
From
the
logical
point
of
view
a
buffer
is
no
different
from
a
wire
so
it
might
seem
useless
However
from
the
analog
point
of
view
the
buffer
might
have
desirable
characteristics
such
as
the
ability
to
deliver
large
amounts
of
current
to
a
motor
or
the
ability
to
quickly
send
its
output
to
many
gates
This
is
an
example
of
why
we
need
to
consider
multiple
levels
of
abstraction
to
fully
understand
a
system
the
digital
abstraction
hides
the
real
purpose
of
a
buffer
The
triangle
symbol
indicates
a
buffer
A
circle
on
the
output
is
called
a
bubble
and
indicates
inversion
as
was
seen
in
the
NOT
gate
symbol
of
Figure
AND
Gate
Two
input
logic
gates
are
more
interesting
The
AND
gate
shown
in
Figure
produces
a
TRUE
output
Y
if
and
only
if
both
A
and
B
are
TRUE
Otherwise
the
output
is
FALSE
By
convention
the
inputs
are
listed
in
the
order
as
if
you
were
counting
in
binary
The
Boolean
equation
for
an
AND
gate
can
be
written
in
several
ways
Y
A
B
Y
AB
or
Y
A
B
The
symbol
is
pronounced
intersection
and
is
preferred
by
logicians
We
prefer
Y
AB
read
Y
equals
A
and
B
because
we
are
lazy
CHAPTER
ONE
From
Zero
to
One
NOT
Y
A
A
Y
A
Y
BUF
Y
A
A
Y
A
Y
AND
Y
AB
ABY
A
B
Y
Figure
NOT
gate
Figure
AND
gate
Figure
Buffer
According
to
Larry
Wall
inventor
of
the
Perl
programming
language
the
three
principal
virtues
of
a
programmer
are
Laziness
Impatience
and
Hubris
Chapter
q
OR
Gate
The
OR
gate
shown
in
Figure
produces
a
TRUE
output
Y
if
either
A
or
B
or
both
are
TRUE
The
Boolean
equation
for
an
OR
gate
is
written
as
Y
A
B
or
Y
A
B
The
symbol
is
pronounced
union
and
is
preferred
by
logicians
Digital
designers
normally
use
the
notation
Y
A
B
is
pronounced
Y
equals
A
or
B
Other
Two
Input
Gates
Figure
shows
other
common
two
input
logic
gates
XOR
exclusive
OR
pronounced
ex
OR
is
TRUE
if
A
or
B
but
not
both
are
TRUE
Any
gate
can
be
followed
by
a
bubble
to
invert
its
operation
The
NAND
gate
performs
NOT
AND
Its
output
is
TRUE
unless
both
inputs
are
TRUE
The
NOR
gate
performs
NOT
OR
Its
output
is
TRUE
if
neither
A
nor
B
is
TRUE
Example
XNOR
GATE
Figure
shows
the
symbol
and
Boolean
equation
for
a
two
input
XNOR
gate
that
performs
the
inverse
of
an
XOR
Complete
the
truth
table
Solution
Figure
shows
the
truth
table
The
XNOR
output
is
TRUE
if
both
inputs
are
FALSE
or
both
inputs
are
TRUE
The
two
input
XNOR
gate
is
sometimes
called
an
equality
gate
because
its
output
is
TRUE
when
the
inputs
are
equal
Multiple
Input
Gates
Many
Boolean
functions
of
three
or
more
inputs
exist
The
most
common
are
AND
OR
XOR
NAND
NOR
and
XNOR
An
N
input
AND
gate
produces
a
TRUE
output
when
all
N
inputs
are
TRUE
An
N
input
OR
gate
produces
a
TRUE
output
when
at
least
one
input
is
TRUE
Logic
Gates
OR
Y
A
B
ABY
A
B
Y
Figure
OR
gate
XOR
NAND
NOR
Y
A
B
Y
AB
Y
A
B
ABY
ABY
ABY
A
B
Y
A
B
Y
A
B
Y
Figure
More
two
input
logic
gates
A
silly
way
to
remember
the
OR
symbol
is
that
it
s
input
side
is
curved
like
Pacman
s
mouth
so
the
gate
is
hungry
and
willing
to
eat
any
TRUE
inputs
it
can
find
Figure
XNOR
gate
XNOR
Y
A
B
ABY
A
B
Y
Chapt
An
N
input
XOR
gate
is
sometimes
called
a
parity
gate
and
produces
a
TRUE
output
if
an
odd
number
of
inputs
are
TRUE
As
with
two
input
gates
the
input
combinations
in
the
truth
table
are
listed
in
counting
order
Example
THREE
INPUT
NOR
GATE
Figure
shows
the
symbol
and
Boolean
equation
for
a
three
input
NOR
gate
Complete
the
truth
table
Solution
Figure
shows
the
truth
table
The
output
is
TRUE
only
if
none
of
the
inputs
are
TRUE
Example
FOUR
INPUT
AND
GATE
Figure
shows
the
symbol
and
Boolean
equation
for
a
four
input
AND
gate
Create
a
truth
table
Solution
Figure
shows
the
truth
table
The
output
is
TRUE
only
if
all
of
the
inputs
are
TRUE
BENEATH
THE
DIGITAL
ABSTRACTION
A
digital
system
uses
discrete
valued
variables
However
the
variables
are
represented
by
continuous
physical
quantities
such
as
the
voltage
on
a
wire
the
position
of
a
gear
or
the
level
of
fluid
in
a
cylinder
Hence
the
designer
must
choose
a
way
to
relate
the
continuous
value
to
the
discrete
value
For
example
consider
representing
a
binary
signal
A
with
a
voltage
on
a
wire
Let
volts
V
indicate
A
and
V
indicate
A
Any
real
system
must
tolerate
some
noise
so
V
probably
ought
to
be
interpreted
as
A
as
well
But
what
about
V
Or
V
Or
V
Supply
Voltage
Suppose
the
lowest
voltage
in
the
system
is
V
also
called
ground
or
GND
The
highest
voltage
in
the
system
comes
from
the
power
supply
and
is
usually
called
VDD
In
s
and
s
technology
VDD
was
generally
V
As
chips
have
progressed
to
smaller
transistors
VDD
has
dropped
to
V
V
V
V
V
or
even
lower
to
save
power
and
avoid
overloading
the
transistors
Logic
Levels
The
mapping
of
a
continuous
variable
onto
a
discrete
binary
variable
is
done
by
defining
logic
levels
as
shown
in
Figure
The
first
gate
is
called
the
driver
and
the
second
gate
is
called
the
receiver
The
output
of
CHAPTER
ONE
From
Zero
to
One
Figure
Three
input
NOR
gate
Figure
Three
input
NOR
truth
table
Figure
Four
input
AND
gate
NOR
Y
A
B
C
BCY
A
B
Y
C
A
BCY
A
AND
Y
ABCD
A
B
C
Y
D
Figure
XNOR
truth
table
ABY
Cha
the
driver
is
connected
to
the
input
of
the
receiver
The
driver
produces
a
LOW
output
in
the
range
of
to
VOL
or
a
HIGH
output
in
the
range
of
VOH
to
VDD
If
the
receiver
gets
an
input
in
the
range
of
to
VIL
it
will
consider
the
input
to
be
LOW
If
the
receiver
gets
an
input
in
the
range
of
VIH
to
VDD
it
will
consider
the
input
to
be
HIGH
If
for
some
reason
such
as
noise
or
faulty
components
the
receiver
s
input
should
fall
in
the
forbidden
zone
between
VIL
and
VIH
the
behavior
of
the
gate
is
unpredictable
VOH
VOL
VIH
and
VIL
are
called
the
output
and
input
high
and
low
logic
levels
Noise
Margins
If
the
output
of
the
driver
is
to
be
correctly
interpreted
at
the
input
of
the
receiver
we
must
choose
VOL
VIL
and
VOH
VIH
Thus
even
if
the
output
of
the
driver
is
contaminated
by
some
noise
the
input
of
the
receiver
will
still
detect
the
correct
logic
level
The
noise
margin
is
the
amount
of
noise
that
could
be
added
to
a
worst
case
output
such
that
the
signal
can
still
be
interpreted
as
a
valid
input
As
can
be
seen
in
Figure
the
low
and
high
noise
margins
are
respectively
NML
VIL
VOL
NMH
VOH
VIH
Example
Consider
the
inverter
circuit
of
Figure
VO
is
the
output
voltage
of
inverter
I
and
VI
is
the
input
voltage
of
inverter
I
Both
inverters
have
the
following
characteristics
VDD
V
VIL
V
VIH
V
VOL
V
and
VOH
V
What
are
the
inverter
low
and
high
noise
margins
Can
the
circuit
tolerate
V
of
noise
between
VO
and
VI
Solution
The
inverter
noise
margins
are
NML
VIL
VOL
V
V
V
NMH
VOH
VIH
V
V
V
The
circuit
can
tolerate
V
of
noise
when
the
output
is
LOW
NML
V
but
not
when
the
output
is
HIGH
NMH
V
For
example
suppose
the
driver
I
outputs
its
worst
case
HIGH
value
VO
VOH
V
If
noise
causes
the
voltage
to
droop
by
V
before
reaching
the
input
of
the
receiver
VI
V
V
V
This
is
less
than
the
acceptable
input
HIGH
value
VIH
V
so
the
receiver
may
not
sense
a
proper
HIGH
input
DC
Transfer
Characteristics
To
understand
the
limits
of
the
digital
abstraction
we
must
delve
into
the
analog
behavior
of
a
gate
The
DC
transfer
characteristics
of
a
gate
describe
the
output
voltage
as
a
function
of
the
input
voltage
when
the
Beneath
the
Digital
Abstraction
Figure
Four
input
AND
truth
table
BDY
C
A
VDD
stands
for
the
voltage
on
the
drain
of
a
metal
oxidesemiconductor
transistor
used
to
build
most
modern
chips
The
power
supply
voltage
is
also
sometimes
called
VCC
standing
for
the
voltage
on
the
collector
of
a
bipolar
transistor
used
to
build
chips
in
an
older
technology
Ground
is
sometimes
called
VSS
because
it
is
the
voltage
on
the
source
of
a
metal
oxidesemiconductor
transistor
See
Section
for
more
information
on
transistors
Chapter
qxd
input
is
changed
slowly
enough
that
the
output
can
keep
up
They
are
called
transfer
characteristics
because
they
describe
the
relationship
between
input
and
output
voltages
An
ideal
inverter
would
have
an
abrupt
switching
threshold
at
VDD
as
shown
in
Figure
a
For
V
A
VDD
V
Y
VDD
For
V
A
VDD
V
Y
In
such
a
case
VIH
VIL
VDD
VOH
VDD
and
VOL
A
real
inverter
changes
more
gradually
between
the
extremes
as
shown
in
Figure
b
When
the
input
voltage
V
A
is
the
output
voltage
V
Y
VDD
When
V
A
VDD
V
Y
However
the
transition
between
these
endpoints
is
smooth
and
may
not
be
centered
at
exactly
VDD
This
raises
the
question
of
how
to
define
the
logic
levels
A
reasonable
place
to
choose
the
logic
levels
is
where
the
slope
of
the
transfer
characteristic
dV
Y
dV
A
is
These
two
points
are
called
the
unity
gain
points
Choosing
logic
levels
at
the
unity
gain
points
usually
maximizes
the
noise
margins
If
VIL
were
reduced
VOH
would
only
increase
by
a
small
amount
But
if
VIL
were
increased
VOH
would
drop
precipitously
The
Static
Discipline
To
avoid
inputs
falling
into
the
forbidden
zone
digital
logic
gates
are
designed
to
conform
to
the
static
discipline
The
static
discipline
requires
that
given
logically
valid
inputs
every
circuit
element
will
produce
logically
valid
outputs
By
conforming
to
the
static
discipline
digital
designers
sacrifice
the
freedom
of
using
arbitrary
analog
circuit
elements
in
return
for
the
CHAPTER
ONE
From
Zero
to
One
I
I
Noise
VO
VI
Figure
Inverter
circuit
DC
indicates
behavior
when
an
input
voltage
is
held
constant
or
changes
slowly
enough
for
the
rest
of
the
system
to
keep
up
The
term
s
historical
root
comes
from
direct
current
a
method
of
transmitting
power
across
a
line
with
a
constant
voltage
In
contrast
the
transient
response
of
a
circuit
is
the
behavior
when
an
input
voltage
changes
rapidly
Section
explores
transient
response
further
Forbidden
Zone
NML
NMH
Output
Characteristics
Input
Characteristics
VOH
VDD
VOL
GND
VIH
VIL
Logic
High
Input
Range
Logic
Low
Input
Range
Logic
High
Output
Range
Logic
Low
Output
Range
Driver
Receiver
Figure
Logic
levels
and
noise
margins
Chapter
simplicity
and
robustness
of
digital
circuits
They
raise
the
level
of
abstraction
from
analog
to
digital
increasing
design
productivity
by
hiding
needless
detail
The
choice
of
VDD
and
logic
levels
is
arbitrary
but
all
gates
that
communicate
must
have
compatible
logic
levels
Therefore
gates
are
grouped
into
logic
families
such
that
all
gates
in
a
logic
family
obey
the
static
discipline
when
used
with
other
gates
in
the
family
Logic
gates
in
the
same
logic
family
snap
together
like
Legos
in
that
they
use
consistent
power
supply
voltages
and
logic
levels
Four
major
logic
families
that
predominated
from
the
s
through
the
s
are
Transistor
Transistor
Logic
TTL
Complementary
MetalOxide
Semiconductor
Logic
CMOS
pronounced
sea
moss
Low
Voltage
TTL
Logic
LVTTL
and
Low
Voltage
CMOS
Logic
LVCMOS
Their
logic
levels
are
compared
in
Table
Since
then
logic
families
have
balkanized
with
a
proliferation
of
even
lower
power
supply
voltages
Appendix
A
revisits
popular
logic
families
in
more
detail
Beneath
the
Digital
Abstraction
VDD
V
A
V
Y
VOH
VDD
VOL
VIL
VIH
A
Y
VDD
V
A
V
Y
VOH
VDD
VOL
VIL
VIH
Unity
Gain
Points
Slope
a
b
VDD
Figure
DC
transfer
characteristics
and
logic
levels
Table
Logic
levels
of
V
and
V
logic
families
Logic
Family
VDD
VIL
VIH
VOL
VOH
TTL
CMOS
LVTTL
LVCMOS
Example
LOGIC
FAMILY
COMPATIBILITY
Which
of
the
logic
families
in
Table
can
communicate
with
each
other
reliably
Solution
Table
lists
which
logic
families
have
compatible
logic
levels
Note
that
a
V
logic
family
such
as
TTL
or
CMOS
may
produce
an
output
voltage
as
HIGH
as
V
If
this
V
signal
drives
the
input
of
a
V
logic
family
such
as
LVTTL
or
LVCMOS
it
can
damage
the
receiver
unless
the
receiver
is
specially
designed
to
be
volt
compatible
CMOS
TRANSISTORS
This
section
and
other
sections
marked
with
a
are
optional
and
are
not
necessary
to
understand
the
main
flow
of
the
book
Babbage
s
Analytical
Engine
was
built
from
gears
and
early
electrical
computers
used
relays
or
vacuum
tubes
Modern
computers
use
transistors
because
they
are
cheap
small
and
reliable
Transistors
are
electrically
controlled
switches
that
turn
ON
or
OFF
when
a
voltage
or
current
is
applied
to
a
control
terminal
The
two
main
types
of
transistors
are
bipolar
transistors
and
metal
oxide
semiconductor
field
effect
transistors
MOSFETs
or
MOS
transistors
pronounced
moss
fets
or
M
O
S
respectively
In
Jack
Kilby
at
Texas
Instruments
built
the
first
integrated
circuit
containing
two
transistors
In
Robert
Noyce
at
Fairchild
Semiconductor
patented
a
method
of
interconnecting
multiple
transistors
on
a
single
silicon
chip
At
the
time
transistors
cost
about
each
Thanks
to
more
than
three
decades
of
unprecedented
manufacturing
advances
engineers
can
now
pack
roughly
one
billion
MOSFETs
onto
a
cm
chip
of
silicon
and
these
transistors
cost
less
than
microcents
apiece
The
capacity
and
cost
continue
to
improve
by
an
order
of
magnitude
every
years
or
so
MOSFETs
are
now
the
building
blocks
of
CHAPTER
ONE
From
Zero
to
One
Table
Compatibility
of
logic
families
Receiver
TTL
CMOS
LVTTL
LVCMOS
Driver
TTL
OK
NO
VOH
VIH
MAYBEa
MAYBEa
CMOS
OK
OK
MAYBEa
MAYBEa
LVTTL
OK
NO
VOH
VIH
OK
OK
LVCMOS
OK
NO
VOH
VIH
OK
OK
a
As
long
as
a
V
HIGH
level
does
not
damage
the
receiver
input
Robert
Noyce
Born
in
Burlington
Iowa
Received
a
B
A
in
physics
from
Grinnell
College
and
a
Ph
D
in
physics
from
MIT
Nicknamed
Mayor
of
Silicon
Valley
for
his
profound
influence
on
the
industry
Cofounded
Fairchild
Semiconductor
in
and
Intel
in
Coinvented
the
integrated
circuit
Many
engineers
from
his
teams
went
on
to
found
other
seminal
semiconductor
companies
Intel
Corporation
Reproduced
by
permission
almost
all
digital
systems
In
this
section
we
will
peer
beneath
the
digital
abstraction
to
see
how
logic
gates
are
built
from
MOSFETs
Semiconductors
MOS
transistors
are
built
from
silicon
the
predominant
atom
in
rock
and
sand
Silicon
Si
is
a
group
IV
atom
so
it
has
four
electrons
in
its
valence
shell
and
forms
bonds
with
four
adjacent
atoms
resulting
in
a
crystalline
lattice
Figure
a
shows
the
lattice
in
two
dimensions
for
ease
of
drawing
but
remember
that
the
lattice
actually
forms
a
cubic
crystal
In
the
figure
a
line
represents
a
covalent
bond
By
itself
silicon
is
a
poor
conductor
because
all
the
electrons
are
tied
up
in
covalent
bonds
However
it
becomes
a
better
conductor
when
small
amounts
of
impurities
called
dopant
atoms
are
carefully
added
If
a
group
V
dopant
such
as
arsenic
As
is
added
the
dopant
atoms
have
an
extra
electron
that
is
not
involved
in
the
bonds
The
electron
can
easily
move
about
the
lattice
leaving
an
ionized
dopant
atom
As
behind
as
shown
in
Figure
b
The
electron
carries
a
negative
charge
so
we
call
arsenic
an
n
type
dopant
On
the
other
hand
if
a
group
III
dopant
such
as
boron
B
is
added
the
dopant
atoms
are
missing
an
electron
as
shown
in
Figure
c
This
missing
electron
is
called
a
hole
An
electron
from
a
neighboring
silicon
atom
may
move
over
to
fill
the
missing
bond
forming
an
ionized
dopant
atom
B
and
leaving
a
hole
at
the
neighboring
silicon
atom
In
a
similar
fashion
the
hole
can
migrate
around
the
lattice
The
hole
is
a
lack
of
negative
charge
so
it
acts
like
a
positively
charged
particle
Hence
we
call
boron
a
p
type
dopant
Because
the
conductivity
of
silicon
changes
over
many
orders
of
magnitude
depending
on
the
concentration
of
dopants
silicon
is
called
a
semiconductor
Diodes
The
junction
between
p
type
and
n
type
silicon
is
called
a
diode
The
p
type
region
is
called
the
anode
and
the
n
type
region
is
called
the
cathode
as
illustrated
in
Figure
When
the
voltage
on
the
anode
rises
above
the
voltage
on
the
cathode
the
diode
is
forward
biased
and
CMOS
Transistors
Si
Si
Si
Si
Si
Si
Si
Si
Si
a
Si
As
Si
Si
Si
Si
Si
Si
Si
b
Free
electron
Si
B
Si
Si
Si
Si
Si
Si
Si
c
Free
hole
Figure
Silicon
lattice
and
dopant
atoms
C
current
flows
through
the
diode
from
the
anode
to
the
cathode
But
when
the
anode
voltage
is
lower
than
the
voltage
on
the
cathode
the
diode
is
reverse
biased
and
no
current
flows
The
diode
symbol
intuitively
shows
that
current
only
flows
in
one
direction
Capacitors
A
capacitor
consists
of
two
conductors
separated
by
an
insulator
When
a
voltage
V
is
applied
to
one
of
the
conductors
the
conductor
accumulates
electric
charge
Q
and
the
other
conductor
accumulates
the
opposite
charge
Q
The
capacitance
C
of
the
capacitor
is
the
ratio
of
charge
to
voltage
C
Q
V
The
capacitance
is
proportional
to
the
size
of
the
conductors
and
inversely
proportional
the
distance
between
them
The
symbol
for
a
capacitor
is
shown
in
Figure
Capacitance
is
important
because
charging
or
discharging
a
conductor
takes
time
and
energy
More
capacitance
means
that
a
circuit
will
be
slower
and
require
more
energy
to
operate
Speed
and
energy
will
be
discussed
throughout
this
book
nMOS
and
pMOS
Transistors
A
MOSFET
is
a
sandwich
of
several
layers
of
conducting
and
insulating
materials
MOSFETs
are
built
on
thin
flat
wafers
of
silicon
of
about
to
cm
in
diameter
The
manufacturing
process
begins
with
a
bare
wafer
The
process
involves
a
sequence
of
steps
in
which
dopants
are
implanted
into
the
silicon
thin
films
of
silicon
dioxide
and
silicon
are
grown
and
metal
is
deposited
Between
each
step
the
wafer
is
patterned
so
that
the
materials
appear
only
where
they
are
desired
Because
transistors
are
a
fraction
of
a
micron
in
length
and
the
entire
wafer
is
processed
at
once
it
is
inexpensive
to
manufacture
billions
of
transistors
at
a
time
Once
processing
is
complete
the
wafer
is
cut
into
rectangles
called
chips
or
dice
that
contain
thousands
millions
or
even
billions
of
transistors
The
chip
is
tested
then
placed
in
a
plastic
or
ceramic
package
with
metal
pins
to
connect
it
to
a
circuit
board
The
MOSFET
sandwich
consists
of
a
conducting
layer
called
the
gate
on
top
of
an
insulating
layer
of
silicon
dioxide
SiO
on
top
of
the
silicon
wafer
called
the
substrate
Historically
the
gate
was
constructed
from
metal
hence
the
name
metal
oxide
semiconductor
Modern
manufacturing
processes
use
polycrystalline
silicon
for
the
gate
because
it
does
not
melt
during
subsequent
high
temperature
processing
steps
Silicon
dioxide
is
better
known
as
glass
and
is
often
simply
called
oxide
in
the
semiconductor
industry
The
metal
oxide
semiconductor
sandwich
forms
a
capacitor
in
which
a
thin
layer
of
insulating
oxide
called
a
dielectric
separates
the
metal
and
semiconductor
plates
CHAPTER
ONE
From
Zero
to
One
p
type
n
type
anode
cathode
Figure
The
p
n
junction
diode
structure
and
symbol
C
Figure
Capacitor
symbol
Technicians
in
an
Intel
clean
room
wear
Gore
Tex
bunny
suits
to
prevent
particulates
from
their
hair
skin
and
clothing
from
contaminating
the
microscopic
transistors
on
silicon
wafers
Intel
Corporation
Reproduced
by
permission
A
pin
dual
inline
package
DIP
contains
a
small
chip
scarcely
visible
in
the
center
that
is
connected
to
metal
pins
on
a
side
by
gold
wires
thinner
than
a
strand
of
hair
photograph
by
Kevin
Mapp
Harvey
Mudd
College
m
micron
m
Chapt
There
are
two
flavors
of
MOSFETs
nMOS
and
pMOS
pronounced
n
moss
and
p
moss
Figure
shows
cross
sections
of
each
type
made
by
sawing
through
a
wafer
and
looking
at
it
from
the
side
The
n
type
transistors
called
nMOS
have
regions
of
n
type
dopants
adjacent
to
the
gate
called
the
source
and
the
drain
and
are
built
on
a
p
type
semiconductor
substrate
The
pMOS
transistors
are
just
the
opposite
consisting
of
p
type
source
and
drain
regions
in
an
n
type
substrate
A
MOSFET
behaves
as
a
voltage
controlled
switch
in
which
the
gate
voltage
creates
an
electric
field
that
turns
ON
or
OFF
a
connection
between
the
source
and
drain
The
term
field
effect
transistor
comes
from
this
principle
of
operation
Let
us
start
by
exploring
the
operation
of
an
nMOS
transistor
The
substrate
of
an
nMOS
transistor
is
normally
tied
to
GND
the
lowest
voltage
in
the
system
First
consider
the
situation
when
the
gate
is
also
at
V
as
shown
in
Figure
a
The
diodes
between
the
source
or
drain
and
the
substrate
are
reverse
biased
because
the
source
or
drain
voltage
is
nonnegative
Hence
there
is
no
path
for
current
to
flow
between
the
source
and
drain
so
the
transistor
is
OFF
Now
consider
when
the
gate
is
raised
to
VDD
as
shown
in
Figure
b
When
a
positive
voltage
is
applied
to
the
top
plate
of
a
capacitor
it
establishes
an
electric
field
that
attracts
positive
charge
on
the
top
plate
and
negative
charge
to
the
bottom
plate
If
the
voltage
is
sufficiently
large
so
much
negative
charge
is
attracted
to
the
underside
of
the
gate
that
the
region
inverts
from
p
type
to
effectively
become
n
type
This
inverted
region
is
called
the
channel
Now
the
transistor
has
a
continuous
path
from
the
n
type
source
through
the
n
type
channel
to
the
n
type
drain
so
electrons
can
flow
from
source
to
drain
The
transistor
is
ON
The
gate
voltage
required
to
turn
on
a
transistor
is
called
the
threshold
voltage
Vt
and
is
typically
to
V
CMOS
Transistors
n
p
source
gate
drain
substrate
SiO
n
source
gate
drain
Polysilicon
n
p
p
gate
source
drain
gate
source
drain
substrate
a
nMOS
b
pMOS
Figure
nMOS
and
pMOS
transistors
The
source
and
drain
terminals
are
physically
symmetric
However
we
say
that
charge
flows
from
the
source
to
the
drain
In
an
nMOS
transistor
the
charge
is
carried
by
electrons
which
flow
from
negative
voltage
to
positive
voltage
In
a
pMOS
transistor
the
charge
is
carried
by
holes
which
flow
from
positive
voltage
to
negative
voltage
If
we
draw
schematics
with
the
most
positive
voltage
at
the
top
and
the
most
negative
at
the
bottom
the
source
of
negative
charges
in
an
nMOS
transistor
is
the
bottom
terminal
and
the
source
of
positive
charges
in
a
pMOS
transistor
is
the
top
terminal
A
technician
holds
a
inch
wafer
containing
hundreds
of
microprocessor
chips
Intel
Corporation
Reproduced
by
permission
pMOS
transistors
work
in
just
the
opposite
fashion
as
might
be
guessed
from
the
bubble
on
their
symbol
The
substrate
is
tied
to
VDD
When
the
gate
is
also
at
VDD
the
pMOS
transistor
is
OFF
When
the
gate
is
at
GND
the
channel
inverts
to
p
type
and
the
pMOS
transistor
is
ON
Unfortunately
MOSFETs
are
not
perfect
switches
In
particular
nMOS
transistors
pass
s
well
but
pass
s
poorly
Specifically
when
the
gate
of
an
nMOS
transistor
is
at
VDD
the
drain
will
only
swing
between
and
VDD
Vt
Similarly
pMOS
transistors
pass
s
well
but
s
poorly
However
we
will
see
that
it
is
possible
to
build
logic
gates
that
use
transistors
only
in
their
good
mode
nMOS
transistors
need
a
p
type
substrate
and
pMOS
transistors
need
an
n
type
substrate
To
build
both
flavors
of
transistors
on
the
same
chip
manufacturing
processes
typically
start
with
a
p
type
wafer
then
implant
n
type
regions
called
wells
where
the
pMOS
transistors
should
go
These
processes
that
provide
both
flavors
of
transistors
are
called
Complementary
MOS
or
CMOS
CMOS
processes
are
used
to
build
the
vast
majority
of
all
transistors
fabricated
today
In
summary
CMOS
processes
give
us
two
types
of
electrically
controlled
switches
as
shown
in
Figure
The
voltage
at
the
gate
g
regulates
the
flow
of
current
between
the
source
s
and
drain
d
nMOS
transistors
are
OFF
when
the
gate
is
and
ON
when
the
gate
is
CHAPTER
ONE
From
Zero
to
One
n
p
gate
source
drain
substrate
n
a
GND
GND
n
p
source
gate
drain
substrate
n
b
VDD
GND
channel
Figure
nMOS
transistor
operation
Figure
Switch
models
of
MOSFETs
g
s
d
g
d
s
nMOS
pMOS
g
s
d
d
s
OFF
ON
g
s
d
d
s
ON
OFF
Gordon
Moore
Born
in
San
Francisco
Received
a
B
S
in
chemistry
from
UC
Berkeley
and
a
Ph
D
in
chemistry
and
physics
from
Caltech
Cofounded
Intel
in
with
Robert
Noyce
Observed
in
that
the
number
of
transistors
on
a
computer
chip
doubles
every
year
This
trend
has
become
known
as
Moore
s
Law
Since
transistor
counts
have
doubled
every
two
years
A
corollary
of
Moore
s
Law
is
that
microprocessor
performance
doubles
every
to
months
Semiconductor
sales
have
also
increased
exponentially
Unfortunately
power
consumption
has
increased
exponentially
as
well
Intel
Corporation
Reproduced
by
permission
C
pMOS
transistors
are
just
the
opposite
ON
when
the
gate
is
and
OFF
when
the
gate
is
CMOS
NOT
Gate
Figure
shows
a
schematic
of
a
NOT
gate
built
with
CMOS
transistors
The
triangle
indicates
GND
and
the
flat
bar
indicates
VDD
these
labels
will
be
omitted
from
future
schematics
The
nMOS
transistor
N
is
connected
between
GND
and
the
Y
output
The
pMOS
transistor
P
is
connected
between
VDD
and
the
Y
output
Both
transistor
gates
are
controlled
by
the
input
A
If
A
N
is
OFF
and
P
is
ON
Hence
Y
is
connected
to
VDD
but
not
to
GND
and
is
pulled
up
to
a
logic
P
passes
a
good
If
A
N
is
ON
and
P
is
OFF
and
Y
is
pulled
down
to
a
logic
N
passes
a
good
Checking
against
the
truth
table
in
Figure
we
see
that
the
circuit
is
indeed
a
NOT
gate
Other
CMOS
Logic
Gates
Figure
shows
a
schematic
of
a
two
input
NAND
gate
In
schematic
diagrams
wires
are
always
joined
at
three
way
junctions
They
are
joined
at
four
way
junctions
only
if
a
dot
is
shown
The
nMOS
transistors
N
and
N
are
connected
in
series
both
nMOS
transistors
must
be
ON
to
pull
the
output
down
to
GND
The
pMOS
transistors
P
and
P
are
in
parallel
only
one
pMOS
transistor
must
be
ON
to
pull
the
output
up
to
VDD
Table
lists
the
operation
of
the
pull
down
and
pull
up
networks
and
the
state
of
the
output
demonstrating
that
the
gate
does
function
as
a
NAND
For
example
when
A
and
B
N
is
ON
but
N
is
OFF
blocking
the
path
from
Y
to
GND
P
is
OFF
but
P
is
ON
creating
a
path
from
VDD
to
Y
Therefore
Y
is
pulled
up
to
Figure
shows
the
general
form
used
to
construct
any
inverting
logic
gate
such
as
NOT
NAND
or
NOR
nMOS
transistors
are
good
at
passing
s
so
a
pull
down
network
of
nMOS
transistors
is
placed
between
the
output
and
GND
to
pull
the
output
down
to
pMOS
transistors
are
CMOS
Transistors
Figure
NOT
gate
schematic
Figure
Two
input
NAND
gate
schematic
VDD
A
Y
GND
N
P
A
B
Y
N
N
P
P
Table
NAND
gate
operation
A
B
Pull
Down
Network
Pull
Up
Network
Y
OFF
ON
OFF
ON
OFF
ON
ON
OFF
pMOS
pull
up
network
output
inputs
nMOS
pull
down
network
Figure
General
form
of
an
inverting
logic
gate
Chap
good
at
passing
s
so
a
pull
up
network
of
pMOS
transistors
is
placed
between
the
output
and
VDD
to
pull
the
output
up
to
The
networks
may
consist
of
transistors
in
series
or
in
parallel
When
transistors
are
in
parallel
the
network
is
ON
if
either
transistor
is
ON
When
transistors
are
in
series
the
network
is
ON
only
if
both
transistors
are
ON
The
slash
across
the
input
wire
indicates
that
the
gate
may
receive
multiple
inputs
If
both
the
pull
up
and
pull
down
networks
were
ON
simultaneously
a
short
circuit
would
exist
between
VDD
and
GND
The
output
of
the
gate
might
be
in
the
forbidden
zone
and
the
transistors
would
consume
large
amounts
of
power
possibly
enough
to
burn
out
On
the
other
hand
if
both
the
pull
up
and
pull
down
networks
were
OFF
simultaneously
the
output
would
be
connected
to
neither
VDD
nor
GND
We
say
that
the
output
floats
Its
value
is
again
undefined
Floating
outputs
are
usually
undesirable
but
in
Section
we
will
see
how
they
can
occasionally
be
used
to
the
designer
s
advantage
In
a
properly
functioning
logic
gate
one
of
the
networks
should
be
ON
and
the
other
OFF
at
any
given
time
so
that
the
output
is
pulled
HIGH
or
LOW
but
not
shorted
or
floating
We
can
guarantee
this
by
using
the
rule
of
conduction
complements
When
nMOS
transistors
are
in
series
the
pMOS
transistors
must
be
in
parallel
When
nMOS
transistors
are
in
parallel
the
pMOS
transistors
must
be
in
series
Example
THREE
INPUT
NAND
SCHEMATIC
Draw
a
schematic
for
a
three
input
NAND
gate
using
CMOS
transistors
Solution
The
NAND
gate
should
produce
a
output
only
when
all
three
inputs
are
Hence
the
pull
down
network
should
have
three
nMOS
transistors
in
series
By
the
conduction
complements
rule
the
pMOS
transistors
must
be
in
parallel
Such
a
gate
is
shown
in
Figure
you
can
verify
the
function
by
checking
that
it
has
the
correct
truth
table
Example
TWO
INPUT
NOR
SCHEMATIC
Draw
a
schematic
for
a
two
input
NOR
gate
using
CMOS
transistors
Solution
The
NOR
gate
should
produce
a
output
if
either
input
is
Hence
the
pull
down
network
should
have
two
nMOS
transistors
in
parallel
By
the
conduction
complements
rule
the
pMOS
transistors
must
be
in
series
Such
a
gate
is
shown
in
Figure
Example
TWO
INPUT
AND
SCHEMATIC
Draw
a
schematic
for
a
two
input
AND
gate
CHAPTER
ONE
From
Zero
to
One
A
B
Y
C
A
B
Y
Figure
Three
input
NAND
gate
schematic
Figure
Two
input
NOR
gate
schematic
Experienced
designers
claim
that
electronic
devices
operate
because
they
contain
magic
smoke
They
confirm
this
theory
with
the
observation
that
if
the
magic
smoke
is
ever
let
out
of
the
device
it
ceases
to
work
Solution
It
is
impossible
to
build
an
AND
gate
with
a
single
CMOS
gate
However
building
NAND
and
NOT
gates
is
easy
Thus
the
best
way
to
build
an
AND
gate
using
CMOS
transistors
is
to
use
a
NAND
followed
by
a
NOT
as
shown
in
Figure
Transmission
Gates
At
times
designers
find
it
convenient
to
use
an
ideal
switch
that
can
pass
both
and
well
Recall
that
nMOS
transistors
are
good
at
passing
and
pMOS
transistors
are
good
at
passing
so
the
parallel
combination
of
the
two
passes
both
values
well
Figure
shows
such
a
circuit
called
a
transmission
gate
or
pass
gate
The
two
sides
of
the
switch
are
called
A
and
B
because
a
switch
is
bidirectional
and
has
no
preferred
input
or
output
side
The
control
signals
are
called
enables
EN
and
EN
When
EN
and
EN
both
transistors
are
OFF
Hence
the
transmission
gate
is
OFF
or
disabled
so
A
and
B
are
not
connected
When
EN
and
EN
the
transmission
gate
is
ON
or
enabled
and
any
logic
value
can
flow
between
A
and
B
Pseudo
nMOS
Logic
An
N
input
CMOS
NOR
gate
uses
N
nMOS
transistors
in
parallel
and
N
pMOS
transistors
in
series
Transistors
in
series
are
slower
than
transistors
in
parallel
just
as
resistors
in
series
have
more
resistance
than
resistors
in
parallel
Moreover
pMOS
transistors
are
slower
than
nMOS
transistors
because
holes
cannot
move
around
the
silicon
lattice
as
fast
as
electrons
Therefore
the
parallel
nMOS
transistors
are
fast
and
the
series
pMOS
transistors
are
slow
especially
when
many
are
in
series
Pseudo
nMOS
logic
replaces
the
slow
stack
of
pMOS
transistors
with
a
single
weak
pMOS
transistor
that
is
always
ON
as
shown
in
Figure
This
pMOS
transistor
is
often
called
a
weak
pull
up
The
physical
dimensions
of
the
pMOS
transistor
are
selected
so
that
the
pMOS
transistor
will
pull
the
output
Y
HIGH
weakly
that
is
only
if
none
of
the
nMOS
transistors
are
ON
But
if
any
nMOS
transistor
is
ON
it
overpowers
the
weak
pull
up
and
pulls
Y
down
close
enough
to
GND
to
produce
a
logic
The
advantage
of
pseudo
nMOS
logic
is
that
it
can
be
used
to
build
fast
NOR
gates
with
many
inputs
For
example
Figure
shows
a
pseudo
nMOS
four
input
NOR
Pseudo
nMOS
gates
are
useful
for
certain
memory
and
logic
arrays
discussed
in
Chapter
The
disadvantage
is
that
a
short
circuit
exists
between
VDD
and
GND
when
the
output
is
LOW
the
weak
pMOS
and
nMOS
transistors
are
both
ON
The
short
circuit
draws
continuous
power
so
pseudo
nMOS
logic
must
be
used
sparingly
CMOS
Transistors
Figure
Pseudo
nMOS
fourinput
NOR
gate
Figure
Two
input
AND
gate
schematic
Figure
Transmission
gate
Figure
Generic
pseudonMOS
gate
A
B
Y
A
B
EN
EN
Y
inputs
nMOS
pull
down
network
weak
A
B
Y
weak
C
D
Chap
Pseudo
nMOS
gates
got
their
name
from
the
s
when
manufacturing
processes
only
had
nMOS
transistors
A
weak
nMOS
transistor
was
used
to
pull
the
output
HIGH
because
pMOS
transistors
were
not
available
POWER
CONSUMPTION
Power
consumption
is
the
amount
of
energy
used
per
unit
time
Power
consumption
is
of
great
importance
in
digital
systems
The
battery
life
of
portable
systems
such
as
cell
phones
and
laptop
computers
is
limited
by
power
consumption
Power
is
also
significant
for
systems
that
are
plugged
in
because
electricity
costs
money
and
because
the
system
will
overheat
if
it
draws
too
much
power
Digital
systems
draw
both
dynamic
and
static
power
Dynamic
power
is
the
power
used
to
charge
capacitance
as
signals
change
between
and
Static
power
is
the
power
used
even
when
signals
do
not
change
and
the
system
is
idle
Logic
gates
and
the
wires
that
connect
them
have
capacitance
The
energy
drawn
from
the
power
supply
to
charge
a
capacitance
C
to
voltage
VDD
is
CVDD
If
the
voltage
on
the
capacitor
switches
at
frequency
f
i
e
f
times
per
second
it
charges
the
capacitor
f
times
and
discharges
it
f
times
per
second
Discharging
does
not
draw
energy
from
the
power
supply
so
the
dynamic
power
consumption
is
Pdynamic
CV
DDf
Electrical
systems
draw
some
current
even
when
they
are
idle
When
transistors
are
OFF
they
leak
a
small
amount
of
current
Some
circuits
such
as
the
pseudo
nMOS
gate
discussed
in
Section
have
a
path
from
VDD
to
GND
through
which
current
flows
continuously
The
total
static
current
IDD
is
also
called
the
leakage
current
or
the
quiescent
supply
current
flowing
between
VDD
and
GND
The
static
power
consumption
is
proportional
to
this
static
current
Pstatic
IDDVDD
Example
POWER
CONSUMPTION
A
particular
cell
phone
has
a
watt
hour
W
hr
battery
and
operates
at
V
Suppose
that
when
it
is
in
use
the
cell
phone
operates
at
MHz
and
the
average
amount
of
capacitance
in
the
chip
switching
at
any
given
time
is
nF
Farads
When
in
use
it
also
broadcasts
W
of
power
out
of
its
antenna
When
the
phone
is
not
in
use
the
dynamic
power
drops
to
almost
zero
because
the
signal
processing
is
turned
off
But
the
phone
also
draws
mA
of
quiescent
current
whether
it
is
in
use
or
not
Determine
the
battery
life
of
the
phone
a
if
it
is
not
being
used
and
b
if
it
is
being
used
continuously
CHAPTER
ONE
From
Zero
to
One
Cha
Solution
The
static
power
is
Pstatic
A
V
mW
If
the
phone
is
not
being
used
this
is
the
only
power
consumption
so
the
battery
life
is
Whr
W
hours
about
days
If
the
phone
is
being
used
the
dynamic
power
is
Pdynamic
F
V
Hz
W
Together
with
the
static
and
broadcast
power
the
total
active
power
is
W
W
W
W
so
the
battery
life
is
W
hr
W
hours
This
example
somewhat
oversimplifies
the
actual
operation
of
a
cell
phone
but
it
illustrates
the
key
ideas
of
power
consumption
SUMMARY
AND
A
LOOK
AHEAD
There
are
kinds
of
people
in
this
world
those
who
can
count
in
binary
and
those
who
can
t
This
chapter
has
introduced
principles
for
understanding
and
designing
complex
systems
Although
the
real
world
is
analog
digital
designers
discipline
themselves
to
use
a
discrete
subset
of
possible
signals
In
particular
binary
variables
have
just
two
states
and
also
called
FALSE
and
TRUE
or
LOW
and
HIGH
Logic
gates
compute
a
binary
output
from
one
or
more
binary
inputs
Some
of
the
common
logic
gates
are
NOT
TRUE
when
input
is
FALSE
AND
TRUE
when
all
inputs
are
TRUE
OR
TRUE
when
any
inputs
are
TRUE
XOR
TRUE
when
an
odd
number
of
inputs
are
TRUE
Logic
gates
are
commonly
built
from
CMOS
transistors
which
behave
as
electrically
controlled
switches
nMOS
transistors
turn
ON
when
the
gate
is
pMOS
transistors
turn
ON
when
the
gate
is
In
Chapters
through
we
continue
the
study
of
digital
logic
Chapter
addresses
combinational
logic
in
which
the
outputs
depend
only
on
the
current
inputs
The
logic
gates
introduced
already
are
examples
of
combinational
logic
You
will
learn
to
design
circuits
involving
multiple
gates
to
implement
a
relationship
between
inputs
and
outputs
specified
by
a
truth
table
or
Boolean
equation
Chapter
addresses
sequential
logic
in
which
the
outputs
depend
on
both
current
and
past
inputs
Registers
are
common
sequential
elements
that
remember
their
previous
input
Finite
state
machines
built
from
registers
and
combinational
logic
are
a
powerful
way
to
build
complicated
systems
in
a
systematic
fashion
We
also
study
timing
of
digital
systems
to
analyze
how
fast
the
systems
can
operate
Chapter
describes
hardware
description
languages
HDLs
HDLs
are
related
to
conventional
programming
languages
but
are
used
to
simulate
and
build
hardware
rather
than
software
Most
digital
systems
today
are
designed
with
HDLs
Verilog
Summary
and
a
Look
Ahead
Chapter
q
and
VHDL
are
the
two
prevalent
languages
and
they
are
covered
sideby
side
in
this
book
Chapter
studies
other
combinational
and
sequential
building
blocks
such
as
adders
multipliers
and
memories
Chapter
shifts
to
computer
architecture
It
describes
the
MIPS
processor
an
industry
standard
microprocessor
used
in
consumer
electronics
some
Silicon
Graphics
workstations
and
many
communications
systems
such
as
televisions
networking
hardware
and
wireless
links
The
MIPS
architecture
is
defined
by
its
registers
and
assembly
language
instruction
set
You
will
learn
to
write
programs
in
assembly
language
for
the
MIPS
processor
so
that
you
can
communicate
with
the
processor
in
its
native
language
Chapters
and
bridge
the
gap
between
digital
logic
and
computer
architecture
Chapter
investigates
microarchitecture
the
arrangement
of
digital
building
blocks
such
as
adders
and
registers
needed
to
construct
a
processor
In
that
chapter
you
learn
to
build
your
own
MIPS
processor
Indeed
you
learn
three
microarchitectures
illustrating
different
trade
offs
of
performance
and
cost
Processor
performance
has
increased
exponentially
requiring
ever
more
sophisticated
memory
systems
to
feed
the
insatiable
demand
for
data
Chapter
delves
into
memory
system
architecture
and
also
describes
how
computers
communicate
with
peripheral
devices
such
as
keyboards
and
printers
CHAPTER
ONE
From
Zero
to
One
Exercises
Exercises
Exercise
Explain
in
one
paragraph
at
least
three
levels
of
abstraction
that
are
used
by
a
biologists
studying
the
operation
of
cells
b
chemists
studying
the
composition
of
matter
Exercise
Explain
in
one
paragraph
how
the
techniques
of
hierarchy
modularity
and
regularity
may
be
used
by
a
automobile
designers
b
businesses
to
manage
their
operations
Exercise
Ben
Bitdiddle
is
building
a
house
Explain
how
he
can
use
the
principles
of
hierarchy
modularity
and
regularity
to
save
time
and
money
during
construction
Exercise
An
analog
voltage
is
in
the
range
of
V
If
it
can
be
measured
with
an
accuracy
of
mV
at
most
how
many
bits
of
information
does
it
convey
Exercise
A
classroom
has
an
old
clock
on
the
wall
whose
minute
hand
broke
off
a
If
you
can
read
the
hour
hand
to
the
nearest
minutes
how
many
bits
of
information
does
the
clock
convey
about
the
time
b
If
you
know
whether
it
is
before
or
after
noon
how
many
additional
bits
of
information
do
you
know
about
the
time
Exercise
The
Babylonians
developed
the
sexagesimal
base
number
system
about
years
ago
How
many
bits
of
information
is
conveyed
with
one
sexagesimal
digit
How
do
you
write
the
number
in
sexagesimal
Exercise
How
many
different
numbers
can
be
represented
with
bits
Exercise
What
is
the
largest
unsigned
bit
binary
number
Exercise
What
is
the
largest
bit
binary
number
that
can
be
represented
with
a
unsigned
numbers
b
two
s
complement
numbers
c
sign
magnitude
numbers
CHAPTER
ONE
From
Zero
to
One
Exercise
What
is
the
smallest
most
negative
bit
binary
number
that
can
be
represented
with
a
unsigned
numbers
b
two
s
complement
numbers
c
sign
magnitude
numbers
Exercise
Convert
the
following
unsigned
binary
numbers
to
decimal
a
b
c
d
Exercise
Repeat
Exercise
but
convert
to
hexadecimal
Exercise
Convert
the
following
hexadecimal
numbers
to
decimal
a
A
b
B
c
FFFF
d
D
Exercise
Repeat
Exercise
but
convert
to
unsigned
binary
Exercise
Convert
the
following
two
s
complement
binary
numbers
to
decimal
a
b
c
d
Exercise
Repeat
Exercise
assuming
the
binary
numbers
are
in
sign
magnitude
form
rather
than
two
s
complement
representation
Exercise
Convert
the
following
decimal
numbers
to
unsigned
binary
numbers
a
b
c
d
Exercise
Repeat
Exercise
but
convert
to
hexadecimal
Exercise
Convert
the
following
decimal
numbers
to
bit
two
s
complement
numbers
or
indicate
that
the
decimal
number
would
overflow
the
range
a
b
c
d
e
Exercise
Repeat
Exercise
but
convert
to
bit
sign
magnitude
numbers
Exercise
Convert
the
following
bit
two
s
complement
numbers
to
bit
two
s
complement
numbers
a
b
Exercise
Repeat
Exercise
if
the
numbers
are
unsigned
rather
than
two
s
complement
Exercise
Base
is
referred
to
as
octal
Convert
each
of
the
numbers
from
Exercise
to
octal
Exercise
Convert
each
of
the
following
octal
numbers
to
binary
hexadecimal
and
decimal
a
b
c
d
Exercise
How
many
bit
two
s
complement
numbers
are
greater
than
How
many
are
less
than
How
would
your
answers
differ
for
sign
magnitude
numbers
Exercises
Ch
Exercise
How
many
bytes
are
in
a
bit
word
How
many
nibbles
are
in
the
word
Exercise
How
many
bytes
are
in
a
bit
word
Exercise
A
particular
DSL
modem
operates
at
kbits
sec
How
many
bytes
can
it
receive
in
minute
Exercise
Hard
disk
manufacturers
use
the
term
megabyte
to
mean
bytes
and
gigabyte
to
mean
bytes
How
many
real
GBs
of
music
can
you
store
on
a
GB
hard
disk
Exercise
Estimate
the
value
of
without
using
a
calculator
Exercise
A
memory
on
the
Pentium
II
microprocessor
is
organized
as
a
rectangular
array
of
bits
with
rows
and
columns
Estimate
how
many
bits
it
has
without
using
a
calculator
Exercise
Draw
a
number
line
analogous
to
Figure
for
bit
unsigned
two
s
complement
and
sign
magnitude
numbers
Exercise
Perform
the
following
additions
of
unsigned
binary
numbers
Indicate
whether
or
not
the
sum
overflows
a
bit
result
a
b
Exercise
Perform
the
following
additions
of
unsigned
binary
numbers
Indicate
whether
or
not
the
sum
overflows
an
bit
result
a
b
Exercise
Repeat
Exercise
assuming
that
the
binary
numbers
are
in
two
s
complement
form
Exercise
Convert
the
following
decimal
numbers
to
bit
two
s
complement
binary
numbers
and
add
them
Indicate
whether
or
not
the
sum
overflows
a
bit
result
a
b
c
CHAPTER
ONE
From
Zero
to
One
C
d
e
f
Exercise
Perform
the
following
additions
of
unsigned
hexadecimal
numbers
Indicate
whether
or
not
the
sum
overflows
an
bit
two
hex
digit
result
a
b
c
AB
E
d
F
AD
Exercise
Convert
the
following
decimal
numbers
to
bit
two
s
complement
binary
numbers
and
subtract
them
Indicate
whether
or
not
the
difference
overflows
a
bit
result
a
b
c
d
Exercise
In
a
biased
N
bit
binary
number
system
with
bias
B
positive
and
negative
numbers
are
represented
as
their
value
plus
the
bias
B
For
example
for
bit
numbers
with
a
bias
of
the
number
is
represented
as
as
and
so
forth
Biased
number
systems
are
sometimes
used
in
floating
point
mathematics
which
will
be
discussed
in
Chapter
Consider
a
biased
bit
binary
number
system
with
a
bias
of
a
What
decimal
value
does
the
binary
number
represent
b
What
binary
number
represents
the
value
c
What
is
the
representation
and
value
of
the
most
negative
number
d
What
is
the
representation
and
value
of
the
most
positive
number
Exercise
Draw
a
number
line
analogous
to
Figure
for
bit
biased
numbers
with
a
bias
of
see
Exercise
for
a
definition
of
biased
numbers
Exercise
In
a
binary
coded
decimal
BCD
system
bits
are
used
to
represent
a
decimal
digit
from
to
For
example
is
written
as
BCD
Exercises
Chapter
a
Write
in
BCD
b
Convert
BCD
to
decimal
c
Convert
BCD
to
binary
d
Explain
why
BCD
might
be
a
useful
way
to
represent
numbers
Exercise
A
flying
saucer
crashes
in
a
Nebraska
cornfield
The
FBI
investigates
the
wreckage
and
finds
an
engineering
manual
containing
an
equation
in
the
Martian
number
system
If
this
equation
is
correct
how
many
fingers
would
you
expect
Martians
have
Exercise
Ben
Bitdiddle
and
Alyssa
P
Hacker
are
having
an
argument
Ben
says
All
integers
greater
than
zero
and
exactly
divisible
by
six
have
exactly
two
s
in
their
binary
representation
Alyssa
disagrees
She
says
No
but
all
such
numbers
have
an
even
number
of
s
in
their
representation
Do
you
agree
with
Ben
or
Alyssa
or
both
or
neither
Explain
Exercise
Ben
Bitdiddle
and
Alyssa
P
Hacker
are
having
another
argument
Ben
says
I
can
get
the
two
s
complement
of
a
number
by
subtracting
then
inverting
all
the
bits
of
the
result
Alyssa
says
No
I
can
do
it
by
examining
each
bit
of
the
number
starting
with
the
least
significant
bit
When
the
first
is
found
invert
each
subsequent
bit
Do
you
agree
with
Ben
or
Alyssa
or
both
or
neither
Explain
Exercise
Write
a
program
in
your
favorite
language
e
g
C
Java
Perl
to
convert
numbers
from
binary
to
decimal
The
user
should
type
in
an
unsigned
binary
number
The
program
should
print
the
decimal
equivalent
Exercise
Repeat
Exercise
but
convert
from
decimal
to
hexadecimal
Exercise
Repeat
Exercise
but
convert
from
an
arbitrary
base
b
to
another
base
b
as
specified
by
the
user
Support
bases
up
to
using
the
letters
of
the
alphabet
for
digits
greater
than
The
user
should
enter
b
b
and
then
the
number
to
convert
in
base
b
The
program
should
print
the
equivalent
number
in
base
b
Exercise
Draw
the
symbol
Boolean
equation
and
truth
table
for
a
a
three
input
OR
gate
b
a
three
input
exclusive
OR
XOR
gate
c
a
four
input
XNOR
gate
CHAPTER
ONE
From
Zero
to
One
C
Exercise
A
majority
gate
produces
a
TRUE
output
if
and
only
if
more
than
half
of
its
inputs
are
TRUE
Complete
a
truth
table
for
the
three
input
majority
gate
shown
in
Figure
Exercise
A
three
input
AND
OR
AO
gate
shown
in
Figure
produces
a
TRUE
output
if
both
A
and
B
are
TRUE
or
if
C
is
TRUE
Complete
a
truth
table
for
the
gate
Exercise
A
three
input
OR
AND
INVERT
OAI
gate
shown
in
Figure
produces
a
FALSE
input
if
C
is
TRUE
and
A
or
B
is
TRUE
Otherwise
it
produces
a
TRUE
output
Complete
a
truth
table
for
the
gate
Exercise
There
are
different
truth
tables
for
Boolean
functions
of
two
variables
List
each
truth
table
Give
each
one
a
short
descriptive
name
such
as
OR
NAND
and
so
on
Exercise
How
many
different
truth
tables
exist
for
Boolean
functions
of
N
variables
Exercise
Is
it
possible
to
assign
logic
levels
so
that
a
device
with
the
transfer
characteristics
shown
in
Figure
would
serve
as
an
inverter
If
so
what
are
the
input
and
output
low
and
high
levels
VIL
VOL
VIH
and
VOH
and
noise
margins
NML
and
NMH
If
not
explain
why
not
Exercises
A
B
Y
C
MAJ
A
B
Y
C
A
B
Y
C
Figure
Three
input
majority
gate
Figure
Three
input
AND
OR
gate
Figure
Three
input
OR
AND
INVERT
gate
Exercise
Repeat
Exercise
for
the
transfer
characteristics
shown
in
Figure
Exercise
Is
it
possible
to
assign
logic
levels
so
that
a
device
with
the
transfer
characteristics
shown
in
Figure
would
serve
as
a
buffer
If
so
what
are
the
input
and
output
low
and
high
levels
VIL
VOL
VIH
and
VOH
and
noise
margins
NML
and
NMH
If
not
explain
why
not
CHAPTER
ONE
From
Zero
to
One
Figure
DC
transfer
characteristics
Vin
Vout
Figure
DC
transfer
characteristics
Vin
Vout
Exercise
Ben
Bitdiddle
has
invented
a
circuit
with
the
transfer
characteristics
shown
in
Figure
that
he
would
like
to
use
as
a
buffer
Will
it
work
Why
or
why
not
He
would
like
to
advertise
that
it
is
compatible
with
LVCMOS
and
LVTTL
logic
Can
Ben
s
buffer
correctly
receive
inputs
from
those
logic
families
Can
its
output
properly
drive
those
logic
families
Explain
Exercise
While
walking
down
a
dark
alley
Ben
Bitdiddle
encounters
a
twoinput
gate
with
the
transfer
function
shown
in
Figure
The
inputs
are
A
and
B
and
the
output
is
Y
a
What
kind
of
logic
gate
did
he
find
b
What
are
the
approximate
high
and
low
logic
levels
Exercises
Vin
Vout
Vin
Vout
Figure
DC
transfer
characteristics
Figure
Ben
s
buffer
DC
transfer
characteristics
Exercise
Repeat
Exercise
for
Figure
Exercise
Sketch
a
transistor
level
circuit
for
the
following
CMOS
gates
Use
a
minimum
number
of
transistors
a
A
four
input
NAND
gate
b
A
three
input
OR
AND
INVERT
gate
see
Exercise
c
A
three
input
AND
OR
gate
see
Exercise
Exercise
A
minority
gate
produces
a
TRUE
output
if
and
only
if
fewer
than
half
of
its
inputs
are
TRUE
Otherwise
it
produces
a
FALSE
output
Sketch
a
transistor
level
circuit
for
a
CMOS
minority
gate
Use
a
minimum
number
of
transistors
CHAPTER
ONE
From
Zero
to
One
Figure
Two
input
DC
transfer
characteristics
Figure
Two
input
DC
transfer
characteristics
A
B
Y
A
B
Y
Exercise
Write
a
truth
table
for
the
function
performed
by
the
gate
in
Figure
The
truth
table
should
have
two
inputs
A
and
B
What
is
the
name
of
this
function
Exercise
Write
a
truth
table
for
the
function
performed
by
the
gate
in
Figure
The
truth
table
should
have
three
inputs
A
B
and
C
Exercise
Implement
the
following
three
input
gates
using
only
pseudonMOS
logic
gates
Your
gates
receive
three
inputs
A
B
and
C
Use
a
minimum
number
of
transistors
a
three
input
NOR
gate
b
three
input
NAND
gate
a
three
input
AND
gate
Exercise
Resistor
Transistor
Logic
RTL
uses
nMOS
transistors
to
pull
the
gate
output
LOW
and
a
weak
resistor
to
pull
the
output
HIGH
when
none
of
the
paths
to
ground
are
active
A
NOT
gate
built
using
RTL
is
shown
in
Figure
Sketch
a
three
input
RTL
NOR
gate
Use
a
minimum
number
of
transistors
Exercises
A
B
A
B
A
A
B
B
Y
A
B
C
C
A
B
Y
A
Y
weak
Figure
Mystery
schematic
Figure
Mystery
schematic
Figure
RTL
NOT
gate
Interview
Questions
These
questions
have
been
asked
at
interviews
for
digital
design
jobs
Question
Sketch
a
transistor
level
circuit
for
a
CMOS
four
input
NOR
gate
Question
The
king
receives
gold
coins
in
taxes
but
has
reason
to
believe
that
one
is
counterfeit
He
summons
you
to
identify
the
fake
coin
You
have
a
balance
that
can
hold
coins
on
each
side
How
many
times
do
you
need
to
use
the
balance
to
find
the
lighter
fake
coin
Question
The
professor
the
teaching
assistant
the
digital
design
student
and
the
freshman
track
star
need
to
cross
a
rickety
bridge
on
a
dark
night
The
bridge
is
so
shakey
that
only
two
people
can
cross
at
a
time
They
have
only
one
flashlight
among
them
and
the
span
is
too
long
to
throw
the
flashlight
so
somebody
must
carry
it
back
to
the
other
people
The
freshman
track
star
can
cross
the
bridge
in
minute
The
digital
design
student
can
cross
the
bridge
in
minutes
The
teaching
assistant
can
cross
the
bridge
in
minutes
The
professor
always
gets
distracted
and
takes
minutes
to
cross
the
bridge
What
is
the
fastest
time
to
get
everyone
across
the
bridge
CHAPTER
ONE
From
Zero
to
One
Introduction
Boolean
Equations
Boolean
Algebra
From
Logic
to
Gates
Multilevel
Combinational
Logic
X
s
and
Z
s
Oh
My
Karnaugh
Maps
Combinational
Building
Blocks
Timing
Summary
Exercises
Interview
Questions
Combinational
Logic
Design
INTRODUCTION
In
digital
electronics
a
circuit
is
a
network
that
processes
discretevalued
variables
A
circuit
can
be
viewed
as
a
black
box
shown
in
Figure
with
one
or
more
discrete
valued
input
terminals
one
or
more
discrete
valued
output
terminals
a
functional
specification
describing
the
relationship
between
inputs
and
outputs
a
timing
specification
describing
the
delay
between
inputs
changing
and
outputs
responding
Peering
inside
the
black
box
circuits
are
composed
of
nodes
and
elements
An
element
is
itself
a
circuit
with
inputs
outputs
and
a
specification
A
node
is
a
wire
whose
voltage
conveys
a
discrete
valued
variable
Nodes
are
classified
as
input
output
or
internal
Inputs
receive
values
from
the
external
world
Outputs
deliver
values
to
the
external
world
Wires
that
are
not
inputs
or
outputs
are
called
internal
nodes
Figure
inputs
outputs
functional
spec
timing
spec
Figure
Circuit
as
a
black
box
with
inputs
outputs
and
specifications
Figure
Elements
and
nodes
A
E
E
B
E
C
n
Y
Z
Chap
illustrates
a
circuit
with
three
elements
E
E
and
E
and
six
nodes
Nodes
A
B
and
C
are
inputs
Y
and
Z
are
outputs
n
is
an
internal
node
between
E
and
E
Digital
circuits
are
classified
as
combinational
or
sequential
A
combinational
circuit
s
outputs
depend
only
on
the
current
values
of
the
inputs
in
other
words
it
combines
the
current
input
values
to
compute
the
output
For
example
a
logic
gate
is
a
combinational
circuit
A
sequential
circuit
s
outputs
depend
on
both
current
and
previous
values
of
the
inputs
in
other
words
it
depends
on
the
input
sequence
A
combinational
circuit
is
memoryless
but
a
sequential
circuit
has
memory
This
chapter
focuses
on
combinational
circuits
and
Chapter
examines
sequential
circuits
The
functional
specification
of
a
combinational
circuit
expresses
the
output
values
in
terms
of
the
current
input
values
The
timing
specification
of
a
combinational
circuit
consists
of
lower
and
upper
bounds
on
the
delay
from
input
to
output
We
will
initially
concentrate
on
the
functional
specification
then
return
to
the
timing
specification
later
in
this
chapter
Figure
shows
a
combinational
circuit
with
two
inputs
and
one
output
On
the
left
of
the
figure
are
the
inputs
A
and
B
and
on
the
right
is
the
output
Y
The
symbol
CL
inside
the
box
indicates
that
it
is
implemented
using
only
combinational
logic
In
this
example
the
function
F
is
specified
to
be
OR
Y
F
A
B
A
B
In
words
we
say
the
output
Y
is
a
function
of
the
two
inputs
A
and
B
namely
Y
A
OR
B
Figure
shows
two
possible
implementations
for
the
combinational
logic
circuit
in
Figure
As
we
will
see
repeatedly
throughout
the
book
there
are
often
many
implementations
for
a
single
function
You
choose
which
to
use
given
the
building
blocks
at
your
disposal
and
your
design
constraints
These
constraints
often
include
area
speed
power
and
design
time
Figure
shows
a
combinational
circuit
with
multiple
outputs
This
particular
combinational
circuit
is
called
a
full
adder
and
we
will
revisit
it
in
Section
The
two
equations
specify
the
function
of
the
outputs
S
and
Cout
in
terms
of
the
inputs
A
B
and
Cin
To
simplify
drawings
we
often
use
a
single
line
with
a
slash
through
it
and
a
number
next
to
it
to
indicate
a
bus
a
bundle
of
multiple
signals
The
number
specifies
how
many
signals
are
in
the
bus
For
example
Figure
a
represents
a
block
of
combinational
logic
with
three
inputs
and
two
outputs
If
the
number
of
bits
is
unimportant
or
obvious
from
the
context
the
slash
may
be
shown
without
a
number
Figure
b
indicates
two
blocks
of
combinational
logic
with
an
arbitrary
number
of
outputs
from
one
block
serving
as
inputs
to
the
second
block
The
rules
of
combinational
composition
tell
us
how
we
can
build
a
large
combinational
circuit
from
smaller
combinational
circuit
CHAPTER
TWO
Combinational
Logic
Design
Figure
Combinational
logic
circuit
Figure
Two
OR
implementations
A
B
Y
Y
F
A
B
A
B
CL
A
B
Y
a
Y
b
A
B
Figure
Multiple
output
combinational
circuit
Figure
Slash
notation
for
multiple
signals
A
S
S
A
B
Cin
Cout
AB
ACin
BCin
B
Cin
CL
Cout
CL
a
CL
CL
b
Chap
elements
A
circuit
is
combinational
if
it
consists
of
interconnected
circuit
elements
such
that
Every
circuit
element
is
itself
combinational
Every
node
of
the
circuit
is
either
designated
as
an
input
to
the
circuit
or
connects
to
exactly
one
output
terminal
of
a
circuit
element
The
circuit
contains
no
cyclic
paths
every
path
through
the
circuit
visits
each
circuit
node
at
most
once
Example
COMBINATIONAL
CIRCUITS
Which
of
the
circuits
in
Figure
are
combinational
circuits
according
to
the
rules
of
combinational
composition
Solution
Circuit
a
is
combinational
It
is
constructed
from
two
combinational
circuit
elements
inverters
I
and
I
It
has
three
nodes
n
n
and
n
n
is
an
input
to
the
circuit
and
to
I
n
is
an
internal
node
which
is
the
output
of
I
and
the
input
to
I
n
is
the
output
of
the
circuit
and
of
I
b
is
not
combinational
because
there
is
a
cyclic
path
the
output
of
the
XOR
feeds
back
to
one
of
its
inputs
Hence
a
cyclic
path
starting
at
n
passes
through
the
XOR
to
n
which
returns
to
n
c
is
combinational
d
is
not
combinational
because
node
n
connects
to
the
output
terminals
of
both
I
and
I
e
is
combinational
illustrating
two
combinational
circuits
connected
to
form
a
larger
combinational
circuit
f
does
not
obey
the
rules
of
combinational
composition
because
it
has
a
cyclic
path
through
the
two
elements
Depending
on
the
functions
of
the
elements
it
may
or
may
not
be
a
combinational
circuit
Large
circuits
such
as
microprocessors
can
be
very
complicated
so
we
use
the
principles
from
Chapter
to
manage
the
complexity
Viewing
a
circuit
as
a
black
box
with
a
well
defined
interface
and
function
is
an
Introduction
The
rules
of
combinational
composition
are
sufficient
but
not
strictly
necessary
Certain
circuits
that
disobey
these
rules
are
still
combinational
so
long
as
the
outputs
depend
only
on
the
current
values
of
the
inputs
However
determining
whether
oddball
circuits
are
combinational
is
more
difficult
so
we
will
usually
restrict
ourselves
to
combinational
composition
as
a
way
to
build
combinational
circuits
Figure
Example
circuits
a
n
n
n
I
I
c
CL
CL
e
n
n
b
n
I
I
d
CL
CL
f
Cha
application
of
abstraction
and
modularity
Building
the
circuit
out
of
smaller
circuit
elements
is
an
application
of
hierarchy
The
rules
of
combinational
composition
are
an
application
of
discipline
The
functional
specification
of
a
combinational
circuit
is
usually
expressed
as
a
truth
table
or
a
Boolean
equation
In
the
next
sections
we
describe
how
to
derive
a
Boolean
equation
from
any
truth
table
and
how
to
use
Boolean
algebra
and
Karnaugh
maps
to
simplify
equations
We
show
how
to
implement
these
equations
using
logic
gates
and
how
to
analyze
the
speed
of
these
circuits
BOOLEAN
EQUATIONS
Boolean
equations
deal
with
variables
that
are
either
TRUE
or
FALSE
so
they
are
perfect
for
describing
digital
logic
This
section
defines
some
terminology
commonly
used
in
Boolean
equations
then
shows
how
to
write
a
Boolean
equation
for
any
logic
function
given
its
truth
table
Terminology
The
complement
of
a
variable
A
is
its
inverse
The
variable
or
its
complement
is
called
a
literal
For
example
A
B
and
are
literals
We
call
A
the
true
form
of
the
variable
and
the
complementary
form
true
form
does
not
mean
that
A
is
TRUE
but
merely
that
A
does
not
have
a
line
over
it
The
AND
of
one
or
more
literals
is
called
a
product
or
an
implicant
and
B
are
all
implicants
for
a
function
of
three
variables
A
minterm
is
a
product
involving
all
of
the
inputs
to
the
function
is
a
minterm
for
a
function
of
the
three
variables
A
B
and
C
but
is
not
because
it
does
not
involve
C
Similarly
the
OR
of
one
or
more
literals
is
called
a
sum
A
maxterm
is
a
sum
involving
all
of
the
inputs
to
the
function
is
a
maxterm
for
a
function
of
the
three
variables
A
B
and
C
The
order
of
operations
is
important
when
interpreting
Boolean
equations
Does
Y
A
BC
mean
Y
A
OR
B
AND
C
or
Y
A
OR
B
AND
C
In
Boolean
equations
NOT
has
the
highest
precedence
followed
by
AND
then
OR
Just
as
in
ordinary
equations
products
are
performed
before
sums
Therefore
the
equation
is
read
as
Y
A
OR
B
AND
C
Equation
gives
another
example
of
order
of
operations
Sum
of
Products
Form
A
truth
table
of
N
inputs
contains
N
rows
one
for
each
possible
value
of
the
inputs
Each
row
in
a
truth
table
is
associated
with
a
minterm
AB
BCD
A
B
BC
D
A
B
C
AB
ABC
AB
ABC
A
A
B
A
CHAPTER
TWO
Combinational
Logic
Design
Chapter
that
is
TRUE
for
that
row
Figure
shows
a
truth
table
of
two
inputs
A
and
B
Each
row
shows
its
corresponding
minterm
For
example
the
minterm
for
the
first
row
is
because
is
TRUE
when
A
B
We
can
write
a
Boolean
equation
for
any
truth
table
by
summing
each
of
the
minterms
for
which
the
output
Y
is
TRUE
For
example
in
Figure
there
is
only
one
row
or
minterm
for
which
the
output
Y
is
TRUE
shown
circled
in
blue
Thus
Figure
shows
a
truth
table
with
more
than
one
row
in
which
the
output
is
TRUE
Taking
the
sum
of
each
of
the
circled
minterms
gives
This
is
called
the
sum
of
products
canonical
form
of
a
function
because
it
is
the
sum
OR
of
products
ANDs
forming
minterms
Although
there
are
many
ways
to
write
the
same
function
such
as
we
will
sort
the
minterms
in
the
same
order
that
they
appear
in
the
truth
table
so
that
we
always
write
the
same
Boolean
expression
for
the
same
truth
table
Example
SUM
OF
PRODUCTS
FORM
Ben
Bitdiddle
is
having
a
picnic
He
won
t
enjoy
it
if
it
rains
or
if
there
are
ants
Design
a
circuit
that
will
output
TRUE
only
if
Ben
enjoys
the
picnic
Solution
First
define
the
inputs
and
outputs
The
inputs
are
A
and
R
which
indicate
if
there
are
ants
and
if
it
rains
A
is
TRUE
when
there
are
ants
and
FALSE
when
there
are
no
ants
Likewise
R
is
TRUE
when
it
rains
and
FALSE
when
the
sun
smiles
on
Ben
The
output
is
E
Ben
s
enjoyment
of
the
picnic
E
is
TRUE
if
Ben
enjoys
the
picnic
and
FALSE
if
he
suffers
Figure
shows
the
truth
table
for
Ben
s
picnic
experience
Using
sum
of
products
form
we
write
the
equation
as
We
can
build
the
equation
using
two
inverters
and
a
two
input
AND
gate
shown
in
Figure
a
You
may
recognize
this
truth
table
as
the
NOR
function
from
Section
E
A
NOR
Figure
b
shows
the
NOR
implementation
In
Section
we
show
that
the
two
equations
and
are
equivalent
The
sum
of
products
form
provides
a
Boolean
equation
for
any
truth
table
with
any
number
of
variables
Figure
shows
a
random
threeinput
truth
table
The
sum
of
products
form
of
the
logic
function
is
Unfortunately
sum
of
products
form
does
not
necessarily
generate
the
simplest
equation
In
Section
we
show
how
to
write
the
same
function
using
fewer
terms
Y
ABC
ABC
ABC
AR
A
R
R
A
R
E
AR
Y
BA
BA
Y
AB
AB
Y
AB
AB
AB
Boolean
Equations
Figure
Truth
table
and
minterms
Figure
Truth
table
with
multiple
TRUE
minterms
ABY
minterm
A
B
A
B
A
B
A
B
ABY
minterm
A
B
A
B
A
B
A
B
Canonical
form
is
just
a
fancy
word
for
standard
form
You
can
use
the
term
to
impress
your
friends
and
scare
your
enemies
Chapter
qxd
Product
of
Sums
Form
An
alternative
way
of
expressing
Boolean
functions
is
the
product
ofsums
canonical
form
Each
row
of
a
truth
table
corresponds
to
a
maxterm
that
is
FALSE
for
that
row
For
example
the
maxterm
for
the
first
row
of
a
two
input
truth
table
is
A
B
because
A
B
is
FALSE
when
A
B
We
can
write
a
Boolean
equation
for
any
circuit
directly
from
the
truth
table
as
the
AND
of
each
of
the
maxterms
for
which
the
output
is
FALSE
Example
PRODUCT
OF
SUMS
FORM
Write
an
equation
in
product
of
sums
form
for
the
truth
table
in
Figure
Solution
The
truth
table
has
two
rows
in
which
the
output
is
FALSE
Hence
the
function
can
be
written
in
product
of
sums
form
as
The
first
maxterm
A
B
guarantees
that
Y
for
A
B
because
any
value
AND
is
Likewise
the
second
maxterm
guarantees
that
Y
for
A
B
Figure
is
the
same
truth
table
as
Figure
showing
that
the
same
function
can
be
written
in
more
than
one
way
Similarly
a
Boolean
equation
for
Ben
s
picnic
from
Figure
can
be
written
in
product
of
sums
form
by
circling
the
three
rows
of
s
to
obtain
This
is
uglier
than
the
sumof
products
equation
but
the
two
equations
are
logically
equivalent
Sum
of
products
produces
the
shortest
equations
when
the
output
is
TRUE
on
only
a
few
rows
of
a
truth
table
product
of
sums
is
simpler
when
the
output
is
FALSE
on
only
a
few
rows
of
a
truth
table
BOOLEAN
ALGEBRA
In
the
previous
section
we
learned
how
to
write
a
Boolean
expression
given
a
truth
table
However
that
expression
does
not
necessarily
lead
to
the
simplest
set
of
logic
gates
Just
as
you
use
algebra
to
simplify
mathematical
equations
you
can
use
Boolean
algebra
to
simplify
Boolean
equations
The
rules
of
Boolean
algebra
are
much
like
those
of
ordinary
algebra
but
are
in
some
cases
simpler
because
variables
have
only
two
possible
values
or
Boolean
algebra
is
based
on
a
set
of
axioms
that
we
assume
are
correct
Axioms
are
unprovable
in
the
sense
that
a
definition
cannot
be
proved
From
these
axioms
we
prove
all
the
theorems
of
Boolean
algebra
These
theorems
have
great
practical
significance
because
they
teach
us
how
to
simplify
logic
to
produce
smaller
and
less
costly
circuits
E
AR
E
A
R
A
R
A
R
A
B
Y
A
B
A
B
CHAPTER
TWO
Combinational
Logic
Design
Figure
Truth
table
with
multiple
FALSE
maxterms
A
B
ABY
maxterm
A
B
A
B
A
B
Figure
Random
three
input
truth
table
BCY
A
Figure
Ben
s
circuit
A
R
E
a
A
R
E
b
Figure
Ben
s
truth
table
ARE
Chapter
qxd
Axioms
and
theorems
of
Boolean
algebra
obey
the
principle
of
duality
If
the
symbols
and
and
the
operators
AND
and
OR
are
interchanged
the
statement
will
still
be
correct
We
use
the
prime
symbol
to
denote
the
dual
of
a
statement
Axioms
Table
states
the
axioms
of
Boolean
algebra
These
five
axioms
and
their
duals
define
Boolean
variables
and
the
meanings
of
NOT
AND
and
OR
Axiom
A
states
that
a
Boolean
variable
B
is
if
it
is
not
The
axiom
s
dual
A
states
that
the
variable
is
if
it
is
not
Together
A
and
A
tell
us
that
we
are
working
in
a
Boolean
or
binary
field
of
s
and
s
Axioms
A
and
A
define
the
NOT
operation
Axioms
A
to
A
define
AND
their
duals
A
to
A
define
OR
Theorems
of
One
Variable
Theorems
T
to
T
in
Table
describe
how
to
simplify
equations
involving
one
variable
The
identity
theorem
T
states
that
for
any
Boolean
variable
B
B
AND
B
Its
dual
states
that
B
OR
B
In
hardware
as
shown
in
Figure
T
means
that
if
one
input
of
a
two
input
AND
gate
is
always
we
can
remove
the
AND
gate
and
replace
it
with
a
wire
connected
to
the
variable
input
B
Likewise
T
means
that
if
one
input
of
a
two
input
OR
gate
is
always
we
can
replace
the
OR
gate
with
a
wire
connected
to
B
In
general
gates
cost
money
power
and
delay
so
replacing
a
gate
with
a
wire
is
beneficial
The
null
element
theorem
T
says
that
B
AND
is
always
equal
to
Therefore
is
called
the
null
element
for
the
AND
operation
because
it
nullifies
the
effect
of
any
other
input
The
dual
states
that
B
OR
is
always
equal
to
Hence
is
the
null
element
for
the
OR
operation
In
hardware
as
shown
in
Figure
if
one
input
of
an
AND
gate
is
we
can
replace
the
AND
gate
with
a
wire
that
is
tied
Boolean
Algebra
Axiom
Dual
Name
A
B
if
B
A
B
if
B
Binary
field
A
A
NOT
A
A
AND
OR
A
A
AND
OR
A
A
AND
OR
Table
Axioms
of
Boolean
algebra
The
null
element
theorem
leads
to
some
outlandish
statements
that
are
actually
true
It
is
particularly
dangerous
when
left
in
the
hands
of
advertisers
YOU
WILL
GET
A
MILLION
DOLLARS
or
we
ll
send
you
a
toothbrush
in
the
mail
You
ll
most
likely
be
receiving
a
toothbrush
in
the
mail
Figure
Identity
theorem
in
hardware
a
T
b
T
a
B
B
B
B
b
Chapter
qxd
LOW
to
Likewise
if
one
input
of
an
OR
gate
is
we
can
replace
the
OR
gate
with
a
wire
that
is
tied
HIGH
to
Idempotency
T
says
that
a
variable
AND
itself
is
equal
to
just
itself
Likewise
a
variable
OR
itself
is
equal
to
itself
The
theorem
gets
its
name
from
the
Latin
roots
idem
same
and
potent
power
The
operations
return
the
same
thing
you
put
into
them
Figure
shows
that
idempotency
again
permits
replacing
a
gate
with
a
wire
Involution
T
is
a
fancy
way
of
saying
that
complementing
a
variable
twice
results
in
the
original
variable
In
digital
electronics
two
wrongs
make
a
right
Two
inverters
in
series
logically
cancel
each
other
out
and
are
logically
equivalent
to
a
wire
as
shown
in
Figure
The
dual
of
T
is
itself
The
complement
theorem
T
Figure
states
that
a
variable
AND
its
complement
is
because
one
of
them
has
to
be
And
by
duality
a
variable
OR
its
complement
is
because
one
of
them
has
to
be
Theorems
of
Several
Variables
Theorems
T
to
T
in
Table
describe
how
to
simplify
equations
involving
more
than
one
Boolean
variable
Commutativity
and
associativity
T
and
T
work
the
same
as
in
traditional
algebra
By
commutativity
the
order
of
inputs
for
an
AND
or
OR
function
does
not
affect
the
value
of
the
output
By
associativity
the
specific
groupings
of
inputs
do
not
affect
the
value
of
the
output
The
distributivity
theorem
T
is
the
same
as
in
traditional
algebra
but
its
dual
T
is
not
By
T
AND
distributes
over
OR
and
by
T
OR
distributes
over
AND
In
traditional
algebra
multiplication
distributes
over
addition
but
addition
does
not
distribute
over
multiplication
so
that
B
C
B
D
B
C
D
The
covering
combining
and
consensus
theorems
T
to
T
permit
us
to
eliminate
redundant
variables
With
some
thought
you
should
be
able
to
convince
yourself
that
these
theorems
are
correct
CHAPTER
TWO
Combinational
Logic
Design
Theorem
Dual
Name
T
B
B
T
B
B
Identity
T
B
T
B
Null
Element
T
B
B
B
T
B
B
B
Idempotency
T
B
B
Involution
T
B
B
T
B
B
Complements
Table
Boolean
theorems
of
one
variable
Figure
Idempotency
theorem
in
hardware
a
T
b
T
B
a
B
B
B
B
B
b
Figure
Involution
theorem
in
hardware
T
Figure
Complement
theorem
in
hardware
a
T
b
T
B
B
B
a
B
B
B
b
Figure
Null
element
theorem
in
hardware
a
T
b
T
a
B
B
b
Chapter
qxd
De
Morgan
s
Theorem
T
is
a
particularly
powerful
tool
in
digital
design
The
theorem
explains
that
the
complement
of
the
product
of
all
the
terms
is
equal
to
the
sum
of
the
complement
of
each
term
Likewise
the
complement
of
the
sum
of
all
the
terms
is
equal
to
the
product
of
the
complement
of
each
term
According
to
De
Morgan
s
theorem
a
NAND
gate
is
equivalent
to
an
OR
gate
with
inverted
inputs
Similarly
a
NOR
gate
is
equivalent
to
an
AND
gate
with
inverted
inputs
Figure
shows
these
De
Morgan
equivalent
gates
for
NAND
and
NOR
gates
The
two
symbols
shown
for
each
function
are
called
duals
They
are
logically
equivalent
and
can
be
used
interchangeably
Boolean
Algebra
Theorem
Dual
Name
T
B
C
C
B
T
B
C
C
B
Commutativity
T
B
C
D
B
C
D
T
B
C
D
B
C
D
Associativity
T
B
C
B
D
B
C
D
T
B
C
B
D
B
C
D
Distributivity
T
B
B
C
B
T
B
B
C
B
Covering
T
T
Combining
T
T
Consensus
T
T
De
Morgan
s
B
B
B
B
B
B
Theorem
B
B
B
B
B
B
B
C
B
D
B
C
B
D
B
C
B
D
C
D
B
C
B
D
C
D
B
C
B
C
B
B
C
B
C
B
Table
Boolean
theorems
of
several
variables
Augustus
De
Morgan
died
A
British
mathematician
born
in
India
Blind
in
one
eye
His
father
died
when
he
was
Attended
Trinity
College
Cambridge
at
age
and
was
appointed
Professor
of
Mathematics
at
the
newly
founded
London
University
at
age
Wrote
widely
on
many
mathematical
subjects
including
logic
algebra
and
paradoxes
De
Morgan
s
crater
on
the
moon
is
named
for
him
He
proposed
a
riddle
for
the
year
of
his
birth
I
was
x
years
of
age
in
the
year
x
Figure
De
Morgan
equivalent
gates
ABY
NAND
A
B
Y
A
B
Y
NOR
A
B
Y
A
B
Y
ABY
Y
AB
A
B
Y
A
B
A
B
Chapter
qxd
PM
Page
The
inversion
circle
is
called
a
bubble
Intuitively
you
can
imagine
that
pushing
a
bubble
through
the
gate
causes
it
to
come
out
at
the
other
side
and
flips
the
body
of
the
gate
from
AND
to
OR
or
vice
versa
For
example
the
NAND
gate
in
Figure
consists
of
an
AND
body
with
a
bubble
on
the
output
Pushing
the
bubble
to
the
left
results
in
an
OR
body
with
bubbles
on
the
inputs
The
underlying
rules
for
bubble
pushing
are
Pushing
bubbles
backward
from
the
output
or
forward
from
the
inputs
changes
the
body
of
the
gate
from
AND
to
OR
or
vice
versa
Pushing
a
bubble
from
the
output
back
to
the
inputs
puts
bubbles
on
all
gate
inputs
Pushing
bubbles
on
all
gate
inputs
forward
toward
the
output
puts
a
bubble
on
the
output
Section
uses
bubble
pushing
to
help
analyze
circuits
Example
DERIVE
THE
PRODUCT
OF
SUMS
FORM
Figure
shows
the
truth
table
for
a
Boolean
function
Y
and
its
complement
Using
De
Morgan
s
Theorem
derive
the
product
of
sums
canonical
form
of
Y
from
the
sum
of
products
form
of
Solution
Figure
shows
the
minterms
circled
contained
in
The
sum
ofproducts
canonical
form
of
is
Taking
the
complement
of
both
sides
and
applying
De
Morgan
s
Theorem
twice
we
get
The
Truth
Behind
It
All
The
curious
reader
might
wonder
how
to
prove
that
a
theorem
is
true
In
Boolean
algebra
proofs
of
theorems
with
a
finite
number
of
variables
are
easy
just
show
that
the
theorem
holds
for
all
possible
values
of
these
variables
This
method
is
called
perfect
induction
and
can
be
done
with
a
truth
table
Example
PROVING
THE
CONSENSUS
THEOREM
Prove
the
consensus
theorem
T
from
Table
Solution
Check
both
sides
of
the
equation
for
all
eight
combinations
of
B
C
and
D
The
truth
table
in
Figure
illustrates
these
combinations
Because
BC
BD
CD
BC
BD
for
all
cases
the
theorem
is
proved
Y
Y
AB
AB
AB
AB
A
B
A
B
Y
AB
AB
Y
Y
Y
Y
CHAPTER
TWO
Combinational
Logic
Design
FIGURE
Truth
table
showing
Y
and
Y
Figure
Truth
table
showing
minterms
for
Y
ABY
Y
ABY
Y
minterm
A
B
A
B
A
B
A
B
Chapter
qxd
Simplifying
Equations
The
theorems
of
Boolean
algebra
help
us
simplify
Boolean
equations
For
example
consider
the
sum
of
products
expression
from
the
truth
table
of
Figure
By
Theorem
T
the
equation
simplifies
to
This
may
have
been
obvious
looking
at
the
truth
table
In
general
multiple
steps
may
be
necessary
to
simplify
more
complex
equations
The
basic
principle
of
simplifying
sum
of
products
equations
is
to
combine
terms
using
the
relationship
where
P
may
be
any
implicant
How
far
can
an
equation
be
simplified
We
define
an
equation
in
sum
of
products
form
to
be
minimized
if
it
uses
the
fewest
possible
implicants
If
there
are
several
equations
with
the
same
number
of
implicants
the
minimal
one
is
the
one
with
the
fewest
literals
An
implicant
is
called
a
prime
implicant
if
it
cannot
be
combined
with
any
other
implicants
to
form
a
new
implicant
with
fewer
literals
The
implicants
in
a
minimal
equation
must
all
be
prime
implicants
Otherwise
they
could
be
combined
to
reduce
the
number
of
literals
Example
EQUATION
MINIMIZATION
Minimize
Equation
Solution
We
start
with
the
original
equation
and
apply
Boolean
theorems
step
by
step
as
shown
in
Table
Have
we
simplified
the
equation
completely
at
this
point
Let
s
take
a
closer
look
From
the
original
equation
the
minterms
and
differ
only
in
the
variable
A
So
we
combined
the
minterms
to
form
However
if
we
look
at
the
original
equation
we
note
that
the
last
two
minterms
and
also
differ
by
a
single
literal
C
and
Thus
using
the
same
method
we
could
have
combined
these
two
minterms
to
form
the
minterm
We
say
that
implicants
and
share
the
minterm
So
are
we
stuck
with
simplifying
only
one
of
the
minterm
pairs
or
can
we
simplify
both
Using
the
idempotency
theorem
we
can
duplicate
terms
as
many
times
as
we
want
B
B
B
B
B
Using
this
principle
we
simplify
the
equation
completely
to
its
two
prime
implicants
as
shown
in
Table
BC
AB
BC
AB
ABC
AB
C
ABC
ABC
BC
ABC
ABC
ABC
ABC
AB
C
PA
PA
P
Y
B
Y
AB
AB
Boolean
Algebra
Figure
Truth
table
proving
T
BCD
BC
BD
CD
BC
BD
Chapter
q
Although
it
is
a
bit
counterintuitive
expanding
an
implicant
for
example
turning
AB
into
is
sometimes
useful
in
minimizing
equations
By
doing
this
you
can
repeat
one
of
the
expanded
minterms
to
be
combined
shared
with
another
minterm
You
may
have
noticed
that
completely
simplifying
a
Boolean
equation
with
the
theorems
of
Boolean
algebra
can
take
some
trial
and
error
Section
describes
a
methodical
technique
called
Karnaugh
maps
that
makes
the
process
easier
Why
bother
simplifying
a
Boolean
equation
if
it
remains
logically
equivalent
Simplifying
reduces
the
number
of
gates
used
to
physically
implement
the
function
thus
making
it
smaller
cheaper
and
possibly
faster
The
next
section
describes
how
to
implement
Boolean
equations
with
logic
gates
FROM
LOGIC
TO
GATES
A
schematic
is
a
diagram
of
a
digital
circuit
showing
the
elements
and
the
wires
that
connect
them
together
For
example
the
schematic
in
Figure
shows
a
possible
hardware
implementation
of
our
favorite
logic
function
Equation
Y
ABC
ABC
ABC
ABC
ABC
CHAPTER
TWO
Combinational
Logic
Design
Step
Equation
Justification
T
Idempotency
T
Distributivity
T
Complements
BC
AB
T
Identity
BC
AB
BC
A
A
AB
C
C
ABC
ABC
ABC
ABC
ABC
ABC
ABC
Table
Improved
equation
minimization
Step
Equation
Justification
T
Distributivity
T
Complements
BC
ABC
T
Identity
BC
ABC
BC
A
A
ABC
ABC
ABC
ABC
Table
Equation
minimization
Chapter
qxd
By
drawing
schematics
in
a
consistent
fashion
we
make
them
easier
to
read
and
debug
We
will
generally
obey
the
following
guidelines
Inputs
are
on
the
left
or
top
side
of
a
schematic
Outputs
are
on
the
right
or
bottom
side
of
a
schematic
Whenever
possible
gates
should
flow
from
left
to
right
Straight
wires
are
better
to
use
than
wires
with
multiple
corners
jagged
wires
waste
mental
effort
following
the
wire
rather
than
thinking
of
what
the
circuit
does
Wires
always
connect
at
a
T
junction
A
dot
where
wires
cross
indicates
a
connection
between
the
wires
Wires
crossing
without
a
dot
make
no
connection
The
last
three
guidelines
are
illustrated
in
Figure
Any
Boolean
equation
in
sum
of
products
form
can
be
drawn
as
a
schematic
in
a
systematic
way
similar
to
Figure
First
draw
columns
for
the
inputs
Place
inverters
in
adjacent
columns
to
provide
the
complementary
inputs
if
necessary
Draw
rows
of
AND
gates
for
each
of
the
minterms
Then
for
each
output
draw
an
OR
gate
connected
to
the
minterms
related
to
that
output
This
style
is
called
a
programmable
logic
array
PLA
because
the
inverters
AND
gates
and
OR
gates
are
arrayed
in
a
systematic
fashion
PLAs
will
be
discussed
further
in
Section
Figure
shows
an
implementation
of
the
simplified
equation
we
found
using
Boolean
algebra
in
Example
Notice
that
the
simplified
circuit
has
significantly
less
hardware
than
that
of
Figure
It
may
also
be
faster
because
it
uses
gates
with
fewer
inputs
We
can
reduce
the
number
of
gates
even
further
albeit
by
a
single
inverter
by
taking
advantage
of
inverting
gates
Observe
that
is
an
BC
From
Logic
to
Gates
Figure
Schematic
of
Y
A
B
C
AB
C
AB
C
A
C
B
Y
minterm
ABC
minterm
ABC
minterm
ABC
ABC
Figure
Wire
connections
wires
connect
at
a
T
junction
wires
connect
at
a
dot
wires
crossing
without
a
dot
do
not
connect
A
B
C
Y
Figure
Schematic
of
Y
B
C
AB
Chapter
q
Black
light
twinkies
and
beer
AND
with
inverted
inputs
Figure
shows
a
schematic
using
this
optimization
to
eliminate
the
inverter
on
C
Recall
that
by
De
Morgan
s
theorem
the
AND
with
inverted
inputs
is
equivalent
to
a
NOR
gate
Depending
on
the
implementation
technology
it
may
be
cheaper
to
use
the
fewest
gates
or
to
use
certain
types
of
gates
in
preference
to
others
For
example
NANDs
and
NORs
are
preferred
over
ANDs
and
ORs
in
CMOS
implementations
Many
circuits
have
multiple
outputs
each
of
which
computes
a
separate
Boolean
function
of
the
inputs
We
can
write
a
separate
truth
table
for
each
output
but
it
is
often
convenient
to
write
all
of
the
outputs
on
a
single
truth
table
and
sketch
one
schematic
with
all
of
the
outputs
Example
MULTIPLE
OUTPUT
CIRCUITS
The
dean
the
department
chair
the
teaching
assistant
and
the
dorm
social
chair
each
use
the
auditorium
from
time
to
time
Unfortunately
they
occasionally
conflict
leading
to
disasters
such
as
the
one
that
occurred
when
the
dean
s
fundraising
meeting
with
crusty
trustees
happened
at
the
same
time
as
the
dorm
s
BTB
party
Alyssa
P
Hacker
has
been
called
in
to
design
a
room
reservation
system
The
system
has
four
inputs
A
A
and
four
outputs
Y
Y
These
signals
can
also
be
written
as
A
and
Y
Each
user
asserts
her
input
when
she
requests
the
auditorium
for
the
next
day
The
system
asserts
at
most
one
output
granting
the
auditorium
to
the
highest
priority
user
The
dean
who
is
paying
for
the
system
demands
highest
priority
The
department
chair
teaching
assistant
and
dorm
social
chair
have
decreasing
priority
Write
a
truth
table
and
Boolean
equations
for
the
system
Sketch
a
circuit
that
performs
this
function
Solution
This
function
is
called
a
four
input
priority
circuit
Its
symbol
and
truth
table
are
shown
in
Figure
We
could
write
each
output
in
sum
of
products
form
and
reduce
the
equations
using
Boolean
algebra
However
the
simplified
equations
are
clear
by
inspection
from
the
functional
description
and
the
truth
table
Y
is
TRUE
whenever
A
is
asserted
so
Y
A
Y
is
TRUE
if
A
is
asserted
and
A
is
not
asserted
so
Y
is
TRUE
if
A
is
asserted
and
neither
of
the
higher
priority
inputs
is
asserted
And
Y
is
TRUE
whenever
A
and
no
other
input
is
asserted
The
schematic
is
shown
in
Figure
An
experienced
designer
can
often
implement
a
logic
circuit
by
inspection
Given
a
clear
specification
simply
turn
the
words
into
equations
and
the
equations
into
gates
Y
A
A
A
A
Y
A
A
A
Y
A
A
CHAPTER
TWO
Combinational
Logic
Design
Y
A
B
C
Figure
Schematic
using
fewer
gates
Chap
Notice
that
if
A
is
asserted
in
the
priority
circuit
the
outputs
don
t
care
what
the
other
inputs
are
We
use
the
symbol
X
to
describe
inputs
that
the
output
doesn
t
care
about
Figure
shows
that
the
four
input
priority
circuit
truth
table
becomes
much
smaller
with
don
t
cares
From
this
truth
table
we
can
easily
read
the
Boolean
equations
in
sum
ofproducts
form
by
ignoring
inputs
with
X
s
Don
t
cares
can
also
appear
in
truth
table
outputs
as
we
will
see
in
Section
MULTILEVEL
COMBINATIONAL
LOGIC
Logic
in
sum
of
products
form
is
called
two
level
logic
because
it
consists
of
literals
connected
to
a
level
of
AND
gates
connected
to
a
level
of
Multilevel
Combinational
Logic
A
A
Priority
Circuit
A
A
Y
Y
Y
Y
A
A
A
A
Y
Y
Y
Y
Figure
Priority
circuit
Figure
Priority
circuit
schematic
A
A
A
A
Y
Y
Y
Y
Figure
Priority
circuit
truth
table
with
don
t
cares
X
s
A
A
X
X
X
A
A
X
X
X
Y
Y
Y
Y
X
is
an
overloaded
symbol
that
means
don
t
care
in
truth
tables
and
contention
in
logic
simulation
see
Section
Think
about
the
context
so
you
don
t
mix
up
the
meanings
Some
authors
use
D
or
instead
for
don
t
care
to
avoid
this
ambiguity
OR
gates
Designers
often
build
circuits
with
more
than
two
levels
of
logic
gates
These
multilevel
combinational
circuits
may
use
less
hardware
than
their
two
level
counterparts
Bubble
pushing
is
especially
helpful
in
analyzing
and
designing
multilevel
circuits
Hardware
Reduction
Some
logic
functions
require
an
enormous
amount
of
hardware
when
built
using
two
level
logic
A
notable
example
is
the
XOR
function
of
multiple
variables
For
example
consider
building
a
three
input
XOR
using
the
two
level
techniques
we
have
studied
so
far
Recall
that
an
N
input
XOR
produces
a
TRUE
output
when
an
odd
number
of
inputs
are
TRUE
Figure
shows
the
truth
table
for
a
three
input
XOR
with
the
rows
circled
that
produce
TRUE
outputs
From
the
truth
table
we
read
off
a
Boolean
equation
in
sum
of
products
form
in
Equation
Unfortunately
there
is
no
way
to
simplify
this
equation
into
fewer
implicants
On
the
other
hand
A
B
C
A
B
C
prove
this
to
yourself
by
perfect
induction
if
you
are
in
doubt
Therefore
the
three
input
XOR
can
be
built
out
of
a
cascade
of
two
input
XORs
as
shown
in
Figure
Similarly
an
eight
input
XOR
would
require
eight
input
AND
gates
and
one
input
OR
gate
for
a
two
level
sum
of
products
implementation
A
much
better
option
is
to
use
a
tree
of
two
input
XOR
gates
as
shown
in
Figure
Y
ABC
ABC
ABC
ABC
CHAPTER
TWO
Combinational
Logic
Design
Figure
Three
input
XOR
a
functional
specification
and
b
two
level
logic
implementation
B
C
A
Y
XOR
Y
A
B
C
A
B
Y
C
A
B
C
Y
a
b
Chapter
Selecting
the
best
multilevel
implementation
of
a
specific
logic
function
is
not
a
simple
process
Moreover
best
has
many
meanings
fewest
gates
fastest
shortest
design
time
least
cost
least
power
consumption
In
Chapter
you
will
see
that
the
best
circuit
in
one
technology
is
not
necessarily
the
best
in
another
For
example
we
have
been
using
ANDs
and
ORs
but
in
CMOS
NANDs
and
NORs
are
more
efficient
With
some
experience
you
will
find
that
you
can
create
a
good
multilevel
design
by
inspection
for
most
circuits
You
will
develop
some
of
this
experience
as
you
study
circuit
examples
through
the
rest
of
this
book
As
you
are
learning
explore
various
design
options
and
think
about
the
trade
offs
Computer
aided
design
CAD
tools
are
also
available
to
search
a
vast
space
of
possible
multilevel
designs
and
seek
the
one
that
best
fits
your
constraints
given
the
available
building
blocks
Bubble
Pushing
You
may
recall
from
Section
that
CMOS
circuits
prefer
NANDs
and
NORs
over
ANDs
and
ORs
But
reading
the
equation
by
inspection
from
a
multilevel
circuit
with
NANDs
and
NORs
can
get
pretty
hairy
Figure
shows
a
multilevel
circuit
whose
function
is
not
immediately
clear
by
inspection
Bubble
pushing
is
a
helpful
way
to
redraw
these
circuits
so
that
the
bubbles
cancel
out
and
the
function
can
be
more
easily
determined
Building
on
the
principles
from
Section
the
guidelines
for
bubble
pushing
are
as
follows
Begin
at
the
output
of
the
circuit
and
work
toward
the
inputs
Push
any
bubbles
on
the
final
output
back
toward
the
inputs
so
that
you
can
read
an
equation
in
terms
of
the
output
for
example
Y
instead
of
the
complement
of
the
output
Working
backward
draw
each
gate
in
a
form
so
that
bubbles
cancel
If
the
current
gate
has
an
input
bubble
draw
the
preceding
gate
with
an
output
bubble
If
the
current
gate
does
not
have
an
input
bubble
draw
the
preceding
gate
without
an
output
bubble
Figure
shows
how
to
redraw
Figure
according
to
the
bubble
pushing
guidelines
Starting
at
the
output
Y
the
NAND
gate
has
a
bubble
on
the
output
that
we
wish
to
eliminate
We
push
the
output
bubble
back
to
form
an
OR
with
inverted
inputs
shown
in
Y
Multilevel
Combinational
Logic
Figure
Three
input
XOR
using
two
two
input
XORs
A
B
Y
C
Figure
Eight
input
XOR
using
seven
two
input
XORs
Figure
Multilevel
circuit
using
NANDs
and
NORs
A
B
C
D
Y
Cha
Figure
a
Working
to
the
left
the
rightmost
gate
has
an
input
bubble
that
cancels
with
the
output
bubble
of
the
middle
NAND
gate
so
no
change
is
necessary
as
shown
in
Figure
b
The
middle
gate
has
no
input
bubble
so
we
transform
the
leftmost
gate
to
have
no
output
bubble
as
shown
in
Figure
c
Now
all
of
the
bubbles
in
the
circuit
cancel
except
at
the
inputs
so
the
function
can
be
read
by
inspection
in
terms
of
ANDs
and
ORs
of
true
or
complementary
inputs
For
emphasis
of
this
last
point
Figure
shows
a
circuit
logically
equivalent
to
the
one
in
Figure
The
functions
of
internal
nodes
are
labeled
in
blue
Because
bubbles
in
series
cancel
we
can
ignore
the
bubble
on
the
output
of
the
middle
gate
and
the
input
of
the
rightmost
gate
to
produce
the
logically
equivalent
circuit
of
Figure
Y
ABC
D
CHAPTER
TWO
Combinational
Logic
Design
A
B
C
Y
D
a
no
output
bubble
bubble
on
A
input
and
output
B
C
D
Y
b
A
B
C
D
Y
c
Y
ABC
D
no
bubble
on
input
and
output
Figure
Bubble
pushed
circuit
Figure
Logically
equivalent
bubble
pushed
circuit
A
B
C
D
Y
AB
ABC
Y
ABC
D
Ch
Example
BUBBLE
PUSHING
FOR
CMOS
LOGIC
Most
designers
think
in
terms
of
AND
and
OR
gates
but
suppose
you
would
like
to
implement
the
circuit
in
Figure
in
CMOS
logic
which
favors
NAND
and
NOR
gates
Use
bubble
pushing
to
convert
the
circuit
to
NANDs
NORs
and
inverters
Solution
A
brute
force
solution
is
to
just
replace
each
AND
gate
with
a
NAND
and
an
inverter
and
each
OR
gate
with
a
NOR
and
an
inverter
as
shown
in
Figure
This
requires
eight
gates
Notice
that
the
inverter
is
drawn
with
the
bubble
on
the
front
rather
than
back
to
emphasize
how
the
bubble
can
cancel
with
the
preceding
inverting
gate
For
a
better
solution
observe
that
bubbles
can
be
added
to
the
output
of
a
gate
and
the
input
of
the
next
gate
without
changing
the
function
as
shown
in
Figure
a
The
final
AND
is
converted
to
a
NAND
and
an
inverter
as
shown
in
Figure
b
This
solution
requires
only
five
gates
X
S
AND
Z
S
OH
MY
Boolean
algebra
is
limited
to
s
and
s
However
real
circuits
can
also
have
illegal
and
floating
values
represented
symbolically
by
X
and
Z
Illegal
Value
X
The
symbol
X
indicates
that
the
circuit
node
has
an
unknown
or
illegal
value
This
commonly
happens
if
it
is
being
driven
to
both
and
at
the
same
time
Figure
shows
a
case
where
node
Y
is
driven
both
HIGH
and
LOW
This
situation
called
contention
is
considered
to
be
an
error
X
s
and
Z
s
Oh
My
Figure
Circuit
using
ANDs
and
ORs
Figure
Poor
circuit
using
NANDs
and
NORs
a
b
Figure
Better
circuit
using
NANDs
and
NORs
Figure
Circuit
with
contention
A
Y
X
B
and
must
be
avoided
The
actual
voltage
on
a
node
with
contention
may
be
somewhere
between
and
VDD
depending
on
the
relative
strengths
of
the
gates
driving
HIGH
and
LOW
It
is
often
but
not
always
in
the
forbidden
zone
Contention
also
can
cause
large
amounts
of
power
to
flow
between
the
fighting
gates
resulting
in
the
circuit
getting
hot
and
possibly
damaged
X
values
are
also
sometimes
used
by
circuit
simulators
to
indicate
an
uninitialized
value
For
example
if
you
forget
to
specify
the
value
of
an
input
the
simulator
may
assume
it
is
an
X
to
warn
you
of
the
problem
As
mentioned
in
Section
digital
designers
also
use
the
symbol
X
to
indicate
don
t
care
values
in
truth
tables
Be
sure
not
to
mix
up
the
two
meanings
When
X
appears
in
a
truth
table
it
indicates
that
the
value
of
the
variable
in
the
truth
table
is
unimportant
When
X
appears
in
a
circuit
it
means
that
the
circuit
node
has
an
unknown
or
illegal
value
Floating
Value
Z
The
symbol
Z
indicates
that
a
node
is
being
driven
neither
HIGH
nor
LOW
The
node
is
said
to
be
floating
high
impedance
or
high
Z
A
typical
misconception
is
that
a
floating
or
undriven
node
is
the
same
as
a
logic
In
reality
a
floating
node
might
be
might
be
or
might
be
at
some
voltage
in
between
depending
on
the
history
of
the
system
A
floating
node
does
not
always
mean
there
is
an
error
in
the
circuit
so
long
as
some
other
circuit
element
does
drive
the
node
to
a
valid
logic
level
when
the
value
of
the
node
is
relevant
to
circuit
operation
One
common
way
to
produce
a
floating
node
is
to
forget
to
connect
a
voltage
to
a
circuit
input
or
to
assume
that
an
unconnected
input
is
the
same
as
an
input
with
the
value
of
This
mistake
may
cause
the
circuit
to
behave
erratically
as
the
floating
input
randomly
changes
from
to
Indeed
touching
the
circuit
may
be
enough
to
trigger
the
change
by
means
of
static
electricity
from
the
body
We
have
seen
circuits
that
operate
correctly
only
as
long
as
the
student
keeps
a
finger
pressed
on
a
chip
The
tristate
buffer
shown
in
Figure
has
three
possible
output
states
HIGH
LOW
and
floating
Z
The
tristate
buffer
has
an
input
A
an
output
Y
and
an
enable
E
When
the
enable
is
TRUE
the
tristate
buffer
acts
as
a
simple
buffer
transferring
the
input
value
to
the
output
When
the
enable
is
FALSE
the
output
is
allowed
to
float
Z
The
tristate
buffer
in
Figure
has
an
active
high
enable
That
is
when
the
enable
is
HIGH
the
buffer
is
enabled
Figure
shows
a
tristate
buffer
with
an
active
low
enable
When
the
enable
is
LOW
CHAPTER
TWO
Combinational
Logic
Design
Figure
Tristate
buffer
Figure
Tristate
buffer
with
active
low
enable
EAY
Z
Z
A
E
Y
EAY
Z
Z
A
E
Y
the
buffer
is
enabled
We
show
that
the
signal
is
active
low
by
putting
a
bubble
on
its
input
wire
We
often
indicate
an
active
low
input
by
drawing
a
bar
over
its
name
or
appending
the
word
bar
after
its
name
Ebar
Tristate
buffers
are
commonly
used
on
busses
that
connect
multiple
chips
For
example
a
microprocessor
a
video
controller
and
an
Ethernet
controller
might
all
need
to
communicate
with
the
memory
system
in
a
personal
computer
Each
chip
can
connect
to
a
shared
memory
bus
using
tristate
buffers
as
shown
in
Figure
Only
one
chip
at
a
time
is
allowed
to
assert
its
enable
signal
to
drive
a
value
onto
the
bus
The
other
chips
must
produce
floating
outputs
so
that
they
do
not
cause
contention
with
the
chip
talking
to
the
memory
Any
chip
can
read
the
information
from
the
shared
bus
at
any
time
Such
tristate
busses
were
once
common
However
in
modern
computers
higher
speeds
are
possible
with
point
to
point
links
in
which
chips
are
connected
to
each
other
directly
rather
than
over
a
shared
bus
KARNAUGH
MAPS
After
working
through
several
minimizations
of
Boolean
equations
using
Boolean
algebra
you
will
realize
that
if
you
re
not
careful
you
sometimes
end
up
with
a
completely
different
equation
instead
of
a
simplified
equation
Karnaugh
maps
K
maps
are
a
graphical
method
for
simplifying
Boolean
equations
They
were
invented
in
by
Maurice
Karnaugh
a
telecommunications
engineer
at
Bell
Labs
K
maps
work
well
for
problems
with
up
to
four
variables
More
important
they
give
insight
into
manipulating
Boolean
equations
E
Karnaugh
Maps
Figure
Tristate
bus
connecting
multiple
chips
en
to
bus
from
bus
en
to
bus
from
bus
en
to
bus
from
bus
en
to
bus
from
bus
Processor
Video
Ethernet
shared
bus
Memory
Maurice
Karnaugh
Graduated
with
a
bachelor
s
degree
in
physics
from
the
City
College
of
New
York
in
and
earned
a
Ph
D
in
physics
from
Yale
in
Worked
at
Bell
Labs
and
IBM
from
to
and
as
a
computer
science
professor
at
the
Polytechnic
University
of
New
York
from
to
Recall
that
logic
minimization
involves
combining
terms
Two
terms
containing
an
implicant
P
and
the
true
and
complementary
forms
of
some
variable
A
are
combined
to
eliminate
A
Karnaugh
maps
make
these
combinable
terms
easy
to
see
by
putting
them
next
to
each
other
in
a
grid
Figure
shows
the
truth
table
and
K
map
for
a
three
input
function
The
top
row
of
the
K
map
gives
the
four
possible
values
for
the
A
and
B
inputs
The
left
column
gives
the
two
possible
values
for
the
C
input
Each
square
in
the
K
map
corresponds
to
a
row
in
the
truth
table
and
contains
the
value
of
the
output
Y
for
that
row
For
example
the
top
left
square
corresponds
to
the
first
row
in
the
truth
table
and
indicates
that
the
output
value
Y
when
ABC
Just
like
each
row
in
a
truth
table
each
square
in
a
K
map
represents
a
single
minterm
For
the
purpose
of
explanation
Figure
c
shows
the
minterm
corresponding
to
each
square
in
the
K
map
Each
square
or
minterm
differs
from
an
adjacent
square
by
a
change
in
a
single
variable
This
means
that
adjacent
squares
share
all
the
same
literals
except
one
which
appears
in
true
form
in
one
square
and
in
complementary
form
in
the
other
For
example
the
squares
representing
the
minterms
and
are
adjacent
and
differ
only
in
the
variable
C
You
may
have
noticed
that
the
A
and
B
combinations
in
the
top
row
are
in
a
peculiar
order
This
order
is
called
a
Gray
code
It
differs
from
ordinary
binary
order
in
that
adjacent
entries
differ
only
in
a
single
variable
For
example
only
changes
A
from
to
while
would
change
A
from
to
and
B
from
to
Hence
writing
the
combinations
in
binary
order
would
not
have
produced
our
desired
property
of
adjacent
squares
differing
only
in
one
variable
The
K
map
also
wraps
around
The
squares
on
the
far
right
are
effectively
adjacent
to
the
squares
on
the
far
left
in
that
they
differ
only
in
one
variable
A
In
other
words
you
could
take
the
map
and
roll
it
into
a
cylinder
then
join
the
ends
of
the
cylinder
to
form
a
torus
i
e
a
donut
and
still
guarantee
that
adjacent
squares
would
differ
only
in
one
variable
Circular
Thinking
In
the
K
map
in
Figure
only
two
minterms
are
present
in
the
equation
and
as
indicated
by
the
s
in
the
left
column
Reading
the
minterms
from
the
K
map
is
exactly
equivalent
to
reading
equations
in
sum
of
products
form
directly
from
the
truth
table
ABC
ABC
ABC
ABC
PA
PA
P
CHAPTER
TWO
Combinational
Logic
Design
Gray
codes
were
patented
U
S
Patent
by
Frank
Gray
a
Bell
Labs
researcher
in
They
are
especially
useful
in
mechanical
encoders
because
a
slight
misalignment
causes
an
error
in
only
one
bit
Gray
codes
generalize
to
any
number
of
bits
For
example
a
bit
Gray
code
sequence
is
Lewis
Carroll
posed
a
related
puzzle
in
Vanity
Fair
in
The
rules
of
the
Puzzle
are
simple
enough
Two
words
are
proposed
of
the
same
length
and
the
puzzle
consists
of
linking
these
together
by
interposing
other
words
each
of
which
shall
differ
from
the
next
word
in
one
letter
only
That
is
to
say
one
letter
may
be
changed
in
one
of
the
given
words
then
one
letter
in
the
word
so
obtained
and
so
on
till
we
arrive
at
the
other
given
word
For
example
SHIP
to
DOCK
SHIP
SLIP
SLOP
SLOT
SOOT
LOOT
LOOK
LOCK
DOCK
Can
you
find
a
shorter
sequence
Chap
As
before
we
can
use
Boolean
algebra
to
minimize
equations
in
sum
ofproducts
form
K
maps
help
us
do
this
simplification
graphically
by
circling
s
in
adjacent
squares
as
shown
in
Figure
For
each
circle
we
write
the
corresponding
implicant
Remember
from
Section
that
an
implicant
is
the
product
of
one
or
more
literals
Variables
whose
true
and
complementary
forms
are
both
in
the
circle
are
excluded
from
the
implicant
In
this
case
the
variable
C
has
both
its
true
form
and
its
complementary
form
in
the
circle
so
we
do
not
include
it
in
the
implicant
In
other
words
Y
is
TRUE
when
A
B
independent
of
C
So
the
implicant
is
This
K
map
gives
the
same
answer
we
reached
using
Boolean
algebra
Logic
Minimization
with
K
Maps
K
maps
provide
an
easy
visual
way
to
minimize
logic
Simply
circle
all
the
rectangular
blocks
of
s
in
the
map
using
the
fewest
possible
number
of
circles
Each
circle
should
be
as
large
as
possible
Then
read
off
the
implicants
that
were
circled
More
formally
recall
that
a
Boolean
equation
is
minimized
when
it
is
written
as
a
sum
of
the
fewest
number
of
prime
implicants
Each
circle
on
the
K
map
represents
an
implicant
The
largest
possible
circles
are
prime
implicants
AB
Y
ABC
ABC
AB
C
C
AB
Karnaugh
Maps
Figure
Three
input
function
a
truth
table
b
K
map
c
K
map
showing
minterms
B
C
A
Y
a
C
Y
AB
b
C
Y
AB
ABC
ABC
ABC
ABC
ABC
ABC
ABC
ABC
c
C
Y
AB
Figure
K
map
minimization
Chapter
For
example
in
the
K
map
of
Figure
and
are
implicants
but
not
prime
implicants
Only
is
a
prime
implicant
in
that
K
map
Rules
for
finding
a
minimized
equation
from
a
K
map
are
as
follows
Use
the
fewest
circles
necessary
to
cover
all
the
s
All
the
squares
in
each
circle
must
contain
s
Each
circle
must
span
a
rectangular
block
that
is
a
power
of
i
e
or
squares
in
each
direction
Each
circle
should
be
as
large
as
possible
A
circle
may
wrap
around
the
edges
of
the
K
map
A
in
a
K
map
may
be
circled
multiple
times
if
doing
so
allows
fewer
circles
to
be
used
Example
MINIMIZATION
OF
A
THREE
VARIABLE
FUNCTION
USING
A
K
MAP
Suppose
we
have
the
function
Y
F
A
B
C
with
the
K
map
shown
in
Figure
Minimize
the
equation
using
the
K
map
Solution
Circle
the
s
in
the
K
map
using
as
few
circles
as
possible
as
shown
in
Figure
Each
circle
in
the
K
map
represents
a
prime
implicant
and
the
dimension
of
each
circle
is
a
power
of
two
and
We
form
the
prime
implicant
for
each
circle
by
writing
those
variables
that
appear
in
the
circle
only
in
true
or
only
in
complementary
form
For
example
in
the
circle
the
true
and
complementary
forms
of
B
are
included
in
the
circle
so
we
do
not
include
B
in
the
prime
implicant
However
only
the
true
form
of
A
A
and
complementary
form
of
C
are
in
this
circle
so
we
include
these
variables
in
the
prime
implicant
Similarly
the
circle
covers
all
squares
where
B
so
the
prime
implicant
is
Notice
how
the
top
right
square
minterm
is
covered
twice
to
make
the
prime
implicant
circles
as
large
as
possible
As
we
saw
with
Boolean
algebra
techniques
this
is
equivalent
to
sharing
a
minterm
to
reduce
the
size
of
the
B
AC
C
AB
ABC
ABC
CHAPTER
TWO
Combinational
Logic
Design
Figure
K
map
for
Example
Y
AB
C
Chapter
implicant
Also
notice
how
the
circle
covering
four
squares
wraps
around
the
sides
of
the
K
map
Example
SEVEN
SEGMENT
DISPLAY
DECODER
A
seven
segment
display
decoder
takes
a
bit
data
input
D
and
produces
seven
outputs
to
control
light
emitting
diodes
to
display
a
digit
from
to
The
seven
outputs
are
often
called
segments
a
through
g
or
Sa
Sg
as
defined
in
Figure
The
digits
are
shown
in
Figure
Write
a
truth
table
for
the
outputs
and
use
K
maps
to
find
Boolean
equations
for
outputs
Sa
and
Sb
Assume
that
illegal
input
values
produce
a
blank
readout
Solution
The
truth
table
is
given
in
Table
For
example
an
input
of
should
turn
on
all
segments
except
Sg
Each
of
the
seven
outputs
is
an
independent
function
of
four
variables
The
K
maps
for
outputs
Sa
and
Sb
are
shown
in
Figure
Remember
that
adjacent
squares
may
differ
in
only
a
single
variable
so
we
label
the
rows
and
columns
in
Gray
code
order
Be
careful
to
also
remember
this
ordering
when
entering
the
output
values
into
the
squares
Next
circle
the
prime
implicants
Use
the
fewest
number
of
circles
necessary
to
cover
all
the
s
A
circle
can
wrap
around
the
edges
vertical
and
horizontal
and
a
may
be
circled
more
than
once
Figure
shows
the
prime
implicants
and
the
simplified
Boolean
equations
Note
that
the
minimal
set
of
prime
implicants
is
not
unique
For
example
the
entry
in
the
Sa
K
map
was
circled
along
with
the
entry
to
produce
the
minterm
The
circle
could
have
included
the
entry
instead
producing
a
minterm
as
shown
with
dashed
lines
in
Figure
Figure
illustrates
see
page
a
common
error
in
which
a
nonprime
implicant
was
chosen
to
cover
the
in
the
upper
left
corner
This
minterm
gives
a
sum
of
products
equation
that
is
not
minimal
The
minterm
could
have
been
combined
with
either
of
the
adjacent
ones
to
form
a
larger
circle
as
was
done
in
the
previous
two
figures
D
D
D
D
D
D
D
D
D
D
Karnaugh
Maps
Figure
Solution
for
Example
Figure
Seven
segment
display
decoder
icon
Y
AB
C
AC
Y
AC
B
B
segment
display
decoder
a
b
c
d
g
e
f
D
S
CHAPTER
TWO
Combinational
Logic
Design
Figure
Karnaugh
maps
for
Sa
and
Sb
D
D
D
D
Sa
Sb
D
Sa
Sb
Sc
Sd
Se
Sf
Sg
others
Table
Seven
segment
display
decoder
truth
table
Figure
Seven
segment
display
digits
Don
t
Cares
Recall
that
don
t
care
entries
for
truth
table
inputs
were
introduced
in
Section
to
reduce
the
number
of
rows
in
the
table
when
some
variables
do
not
affect
the
output
They
are
indicated
by
the
symbol
X
which
means
that
the
entry
can
be
either
or
Don
t
cares
also
appear
in
truth
table
outputs
where
the
output
value
is
unimportant
or
the
corresponding
input
combination
can
never
happen
Such
outputs
can
be
treated
as
either
s
or
s
at
the
designer
s
discretion
Karnaugh
Maps
Figure
K
map
solution
for
Example
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
Sa
D
D
D
D
D
Sb
D
D
D
D
D
D
D
D
D
D
D
D
D
D
Sb
D
D
D
D
D
D
Figure
Alternative
K
map
for
Sa
showing
different
set
of
prime
implicants
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
Sa
In
a
K
map
X
s
allow
for
even
more
logic
minimization
They
can
be
circled
if
they
help
cover
the
s
with
fewer
or
larger
circles
but
they
do
not
have
to
be
circled
if
they
are
not
helpful
Example
SEVEN
SEGMENT
DISPLAY
DECODER
WITH
DON
T
CARES
Repeat
Example
if
we
don
t
care
about
the
output
values
for
illegal
input
values
of
to
Solution
The
K
map
is
shown
in
Figure
with
X
entries
representing
don
t
care
Because
don
t
cares
can
be
or
we
circle
a
don
t
care
if
it
allows
us
to
cover
the
s
with
fewer
or
bigger
circles
Circled
don
t
cares
are
treated
as
s
whereas
uncircled
don
t
cares
are
s
Observe
how
a
square
wrapping
around
all
four
corners
is
circled
for
segment
Sa
Use
of
don
t
cares
simplifies
the
logic
substantially
The
Big
Picture
Boolean
algebra
and
Karnaugh
maps
are
two
methods
for
logic
simplification
Ultimately
the
goal
is
to
find
a
low
cost
method
of
implementing
a
particular
logic
function
In
modern
engineering
practice
computer
programs
called
logic
synthesizers
produce
simplified
circuits
from
a
description
of
the
logic
function
as
we
will
see
in
Chapter
For
large
problems
logic
synthesizers
are
much
more
efficient
than
humans
For
small
problems
a
human
with
a
bit
of
experience
can
find
a
good
solution
by
inspection
Neither
of
the
authors
has
ever
used
a
Karnaugh
map
in
real
life
to
CHAPTER
TWO
Combinational
Logic
Design
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
Figure
Alternative
K
map
for
Sa
showing
incorrect
nonprime
implicant
solve
a
practical
problem
But
the
insight
gained
from
the
principles
underlying
Karnaugh
maps
is
valuable
And
Karnaugh
maps
often
appear
at
job
interviews
COMBINATIONAL
BUILDING
BLOCKS
Combinational
logic
is
often
grouped
into
larger
building
blocks
to
build
more
complex
systems
This
is
an
application
of
the
principle
of
abstraction
hiding
the
unnecessary
gate
level
details
to
emphasize
the
function
of
the
building
block
We
have
already
studied
three
such
building
blocks
full
adders
from
Section
priority
circuits
from
Section
and
seven
segment
display
decoders
from
Section
This
section
introduces
two
more
commonly
used
building
blocks
multiplexers
and
decoders
Chapter
covers
other
combinational
building
blocks
Multiplexers
Multiplexers
are
among
the
most
commonly
used
combinational
circuits
They
choose
an
output
from
among
several
possible
inputs
based
on
the
value
of
a
select
signal
A
multiplexer
is
sometimes
affectionately
called
a
mux
Multiplexer
Figure
shows
the
schematic
and
truth
table
for
a
multiplexer
with
two
data
inputs
D
and
D
a
select
input
S
and
one
output
Y
The
multiplexer
chooses
between
the
two
data
inputs
based
on
the
select
if
S
Y
D
and
if
S
Y
D
S
is
also
called
a
control
signal
because
it
controls
what
the
multiplexer
does
Combinational
Building
Blocks
Figure
K
map
solution
with
don
t
cares
X
X
X
X
X
X
X
X
X
X
X
X
D
D
Sa
D
D
Sb
Sa
D
D
D
D
D
D
Sb
D
D
D
D
D
D
D
Figure
multiplexer
symbol
and
truth
table
Y
S
D
Y
D
S
D
D
Chap
A
multiplexer
can
be
built
from
sum
of
products
logic
as
shown
in
Figure
The
Boolean
equation
for
the
multiplexer
may
be
derived
with
a
Karnaugh
map
or
read
off
by
inspection
Y
is
if
S
AND
D
is
OR
if
S
AND
D
is
Alternatively
multiplexers
can
be
built
from
tristate
buffers
as
shown
in
Figure
The
tristate
enables
are
arranged
such
that
at
all
times
exactly
one
tristate
buffer
is
active
When
S
tristate
T
is
enabled
allowing
D
to
flow
to
Y
When
S
tristate
T
is
enabled
allowing
D
to
flow
to
Y
Wider
Multiplexers
A
multiplexer
has
four
data
inputs
and
one
output
as
shown
in
Figure
Two
select
signals
are
needed
to
choose
among
the
four
data
inputs
The
multiplexer
can
be
built
using
sum
of
products
logic
tristates
or
multiple
multiplexers
as
shown
in
Figure
The
product
terms
enabling
the
tristates
can
be
formed
using
AND
gates
and
inverters
They
can
also
be
formed
using
a
decoder
which
we
will
introduce
in
Section
Wider
multiplexers
such
as
and
multiplexers
can
be
built
by
expanding
the
methods
shown
in
Figure
In
general
an
N
multiplexer
needs
log
N
select
lines
Again
the
best
implementation
choice
depends
on
the
target
technology
Multiplexer
Logic
Multiplexers
can
be
used
as
lookup
tables
to
perform
logic
functions
Figure
shows
a
multiplexer
used
to
implement
a
two
input
CHAPTER
TWO
Combinational
Logic
Design
D
Y
D
S
S
Y
D
Y
D
S
D
S
Figure
multiplexer
implementation
using
two
level
logic
Figure
Multiplexer
using
tristate
buffers
Figure
multiplexer
Y
D
S
T
T
Y
D
S
D
S
D
S
D
D
Y
D
D
Shorting
together
the
outputs
of
multiple
gates
technically
violates
the
rules
for
combinational
circuits
given
in
Section
But
because
exactly
one
of
the
outputs
is
driven
at
any
time
this
exception
is
allowed
Chap
AND
gate
The
inputs
A
and
B
serve
as
select
lines
The
multiplexer
data
inputs
are
connected
to
or
according
to
the
corresponding
row
of
the
truth
table
In
general
a
N
input
multiplexer
can
be
programmed
to
perform
any
N
input
logic
function
by
applying
s
and
s
to
the
appropriate
data
inputs
Indeed
by
changing
the
data
inputs
the
multiplexer
can
be
reprogrammed
to
perform
a
different
function
With
a
little
cleverness
we
can
cut
the
multiplexer
size
in
half
using
only
a
N
input
multiplexer
to
perform
any
N
input
logic
function
The
strategy
is
to
provide
one
of
the
literals
as
well
as
s
and
s
to
the
multiplexer
data
inputs
To
illustrate
this
principle
Figure
shows
two
input
AND
and
XOR
functions
implemented
with
multiplexers
We
start
with
an
ordinary
truth
table
and
then
combine
pairs
of
rows
to
eliminate
the
rightmost
input
variable
by
expressing
the
output
in
terms
of
this
variable
For
example
in
the
case
of
AND
when
A
Y
regardless
of
B
When
A
Y
if
B
and
Y
if
B
so
Y
B
We
then
use
the
multiplexer
as
a
lookup
table
according
to
the
new
smaller
truth
table
Example
LOGIC
WITH
MULTIPLEXERS
Alyssa
P
Hacker
needs
to
implement
the
function
to
finish
her
senior
project
but
when
she
looks
in
her
lab
kit
the
only
part
she
has
left
is
an
multiplexer
How
does
she
implement
the
function
Solution
Figure
shows
Alyssa
s
implementation
using
a
single
multiplexer
The
multiplexer
acts
as
a
lookup
table
where
each
row
in
the
truth
table
corresponds
to
a
multiplexer
input
Y
AB
BC
ABC
Combinational
Building
Blocks
Figure
multiplexer
implementations
a
twolevel
logic
b
tristates
c
hierarchical
a
Y
D
D
D
D
b
c
S
Y
S
D
D
D
D
Y
S
S
S
S
S
S
S
S
D
D
D
D
S
S
ABY
Y
AB
Y
A
B
Figure
multiplexer
implementation
of
two
input
AND
function
Chapter
Example
LOGIC
WITH
MULTIPLEXERS
REPRISED
Alyssa
turns
on
her
circuit
one
more
time
before
the
final
presentation
and
blows
up
the
multiplexer
She
accidently
powered
it
with
V
instead
of
V
after
not
sleeping
all
night
She
begs
her
friends
for
spare
parts
and
they
give
her
a
multiplexer
and
an
inverter
Can
she
build
her
circuit
with
only
these
parts
Solution
Alyssa
reduces
her
truth
table
to
four
rows
by
letting
the
output
depend
on
C
She
could
also
have
chosen
to
rearrange
the
columns
of
the
truth
table
to
let
the
output
depend
on
A
or
B
Figure
shows
the
new
design
Decoders
A
decoder
has
N
inputs
and
N
outputs
It
asserts
exactly
one
of
its
outputs
depending
on
the
input
combination
Figure
shows
a
decoder
When
A
Y
is
When
A
Y
is
And
so
forth
The
outputs
are
called
one
hot
because
exactly
one
is
hot
HIGH
at
a
given
time
CHAPTER
TWO
Combinational
Logic
Design
Figure
Alyssa
s
circuit
a
truth
table
b
multiplexer
implementation
AB
Y
C
a
Y
AB
BC
ABC
A
B
C
b
Y
Figure
Multiplexer
logic
using
variable
inputs
ABY
A
Y
A
B
Y
B
Y
A
B
B
B
b
ABY
Y
AB
A
Y
A
B
Y
B
a
Ch
Example
DECODER
IMPLEMENTATION
Implement
a
decoder
with
AND
OR
and
NOT
gates
Solution
Figure
shows
an
implementation
for
the
decoder
using
four
AND
gates
Each
gate
depends
on
either
the
true
or
the
complementary
form
of
each
input
In
general
an
N
N
decoder
can
be
constructed
from
N
N
input
AND
gates
that
accept
the
various
combinations
of
true
or
complementary
inputs
Each
output
in
a
decoder
represents
a
single
minterm
For
example
Y
represents
the
minterm
This
fact
will
be
handy
when
using
decoders
with
other
digital
building
blocks
A
A
Decoder
Logic
Decoders
can
be
combined
with
OR
gates
to
build
logic
functions
Figure
shows
the
two
input
XNOR
function
using
a
decoder
and
a
single
OR
gate
Because
each
output
of
a
decoder
represents
a
single
minterm
the
function
is
built
as
the
OR
of
all
the
minterms
in
the
function
In
Figure
Y
AB
AB
AB
Combinational
Building
Blocks
AB
Y
C
a
A
Y
B
C
C
Y
A
B
C
b
c
Figure
Alyssa
s
new
circuit
Figure
decoder
Decoder
A
A
Y
Y
Y
Y
A
A
Y
Y
Y
Y
Figure
decoder
implementation
A
A
Y
Y
Y
Y
Figure
Logic
function
using
decoder
Decoder
A
B
Y
A
B
Y
AB
AB
AB
AB
Minterm
Chap
When
using
decoders
to
build
logic
it
is
easiest
to
express
functions
as
a
truth
table
or
in
canonical
sum
of
products
form
An
N
input
function
with
M
s
in
the
truth
table
can
be
built
with
an
N
N
decoder
and
an
M
input
OR
gate
attached
to
all
of
the
minterms
containing
s
in
the
truth
table
This
concept
will
be
applied
to
the
building
of
Read
Only
Memories
ROMs
in
Section
TIMING
In
previous
sections
we
have
been
concerned
primarily
with
whether
the
circuit
works
ideally
using
the
fewest
gates
However
as
any
seasoned
circuit
designer
will
attest
one
of
the
most
challenging
issues
in
circuit
design
is
timing
making
a
circuit
run
fast
An
output
takes
time
to
change
in
response
to
an
input
change
Figure
shows
the
delay
between
an
input
change
and
the
subsequent
output
change
for
a
buffer
The
figure
is
called
a
timing
diagram
it
portrays
the
transient
response
of
the
buffer
circuit
when
an
input
changes
The
transition
from
LOW
to
HIGH
is
called
the
rising
edge
Similarly
the
transition
from
HIGH
to
LOW
not
shown
in
the
figure
is
called
the
falling
edge
The
blue
arrow
indicates
that
the
rising
edge
of
Y
is
caused
by
the
rising
edge
of
A
We
measure
delay
from
the
point
of
the
input
signal
A
to
the
point
of
the
output
signal
Y
The
point
is
the
point
at
which
the
signal
is
half
way
between
its
LOW
and
HIGH
values
as
it
transitions
Propagation
and
Contamination
Delay
Combinational
logic
is
characterized
by
its
propagation
delay
and
contamination
delay
The
propagation
delay
tpd
is
the
maximum
time
from
when
an
input
changes
until
the
output
or
outputs
reach
their
final
value
The
contamination
delay
tcd
is
the
minimum
time
from
when
an
input
changes
until
any
output
starts
to
change
its
value
CHAPTER
TWO
Combinational
Logic
Design
Figure
Circuit
delay
A
Y
Time
delay
A
Y
When
designers
speak
of
calculating
the
delay
of
a
circuit
they
generally
are
referring
to
the
worst
case
value
the
propagation
delay
unless
it
is
clear
otherwise
from
the
context
Figure
illustrates
a
buffer
s
propagation
delay
and
contamination
delay
in
blue
and
gray
respectively
The
figure
shows
that
A
is
initially
either
HIGH
or
LOW
and
changes
to
the
other
state
at
a
particular
time
we
are
interested
only
in
the
fact
that
it
changes
not
what
value
it
has
In
response
Y
changes
some
time
later
The
arcs
indicate
that
Y
may
start
to
change
tcd
after
A
transitions
and
that
Y
definitely
settles
to
its
new
value
within
tpd
The
underlying
causes
of
delay
in
circuits
include
the
time
required
to
charge
the
capacitance
in
a
circuit
and
the
speed
of
light
tpd
and
tcd
may
be
different
for
many
reasons
including
different
rising
and
falling
delays
multiple
inputs
and
outputs
some
of
which
are
faster
than
others
circuits
slowing
down
when
hot
and
speeding
up
when
cold
Calculating
tpd
and
tcd
requires
delving
into
the
lower
levels
of
abstraction
beyond
the
scope
of
this
book
However
manufacturers
normally
supply
data
sheets
specifying
these
delays
for
each
gate
Along
with
the
factors
already
listed
propagation
and
contamination
delays
are
also
determined
by
the
path
a
signal
takes
from
input
to
output
Figure
shows
a
four
input
logic
circuit
The
critical
path
shown
in
blue
is
the
path
from
input
A
or
B
to
output
Y
It
is
the
longest
and
therefore
the
slowest
path
because
the
input
travels
Timing
Figure
Propagation
and
contamination
delay
A
Y
A
Y
Time
tpd
tcd
A
B
C
D
Y
Critical
Path
Short
Path
n
n
Figure
Short
path
and
critical
path
Circuit
delays
are
ordinarily
on
the
order
of
picoseconds
ps
seconds
to
nanoseconds
ns
seconds
Trillions
of
picoseconds
have
elapsed
in
the
time
you
spent
reading
this
sidebar
Chapt
through
three
gates
to
the
output
This
path
is
critical
because
it
limits
the
speed
at
which
the
circuit
operates
The
short
path
through
the
circuit
shown
in
gray
is
from
input
D
to
output
Y
This
is
the
shortest
and
therefore
the
fastest
path
through
the
circuit
because
the
input
travels
through
only
a
single
gate
to
the
output
The
propagation
delay
of
a
combinational
circuit
is
the
sum
of
the
propagation
delays
through
each
element
on
the
critical
path
The
contamination
delay
is
the
sum
of
the
contamination
delays
through
each
element
on
the
short
path
These
delays
are
illustrated
in
Figure
and
are
described
by
the
following
equations
Example
FINDING
DELAYS
Ben
Bitdiddle
needs
to
find
the
propagation
delay
and
contamination
delay
of
the
circuit
shown
in
Figure
According
to
his
data
book
each
gate
has
a
propagation
delay
of
picoseconds
ps
and
a
contamination
delay
of
ps
Solution
Ben
begins
by
finding
the
critical
path
and
the
shortest
path
through
the
circuit
The
critical
path
highlighted
in
blue
in
Figure
is
from
input
A
tcd
tcd
AND
tpd
tpd
AND
tpd
OR
CHAPTER
TWO
Combinational
Logic
Design
A
Y
D
Y
delay
Time
A
Y
delay
A
B
C
D
Y
Short
Path
Critical
Path
Time
n
n
n
n
n
n
B
C
D
Figure
Critical
and
short
path
waveforms
Although
we
are
ignoring
wire
delay
in
this
analysis
digital
circuits
are
now
so
fast
that
the
delay
of
long
wires
can
be
as
important
as
the
delay
of
the
gates
The
speed
of
light
delay
in
wires
is
covered
in
Appendix
A
Cha
or
B
through
three
gates
to
the
output
Y
Hence
tpd
is
three
times
the
propagation
delay
of
a
single
gate
or
ps
The
shortest
path
shown
in
gray
in
Figure
is
from
input
C
D
or
E
through
two
gates
to
the
output
Y
There
are
only
two
gates
in
the
shortest
path
so
tcd
is
ps
Example
MULTIPLEXER
TIMING
CONTROL
CRITICAL
VS
DATA
CRITICAL
Compare
the
worst
case
timing
of
the
three
four
input
multiplexer
designs
shown
in
Figure
in
Section
Table
lists
the
propagation
delays
for
the
components
What
is
the
critical
path
for
each
design
Given
your
timing
analysis
why
might
you
choose
one
design
over
the
other
Solution
One
of
the
critical
paths
for
each
of
the
three
design
options
is
highlighted
in
blue
in
Figures
and
tpd
sy
indicates
the
propagation
delay
from
input
S
to
output
Y
tpd
dy
indicates
the
propagation
delay
from
input
D
to
output
Y
tpd
is
the
worst
of
the
two
max
tpd
sy
tpd
dy
For
both
the
two
level
logic
and
tristate
implementations
in
Figure
the
critical
path
is
from
one
of
the
control
signals
S
to
the
output
Y
tpd
tpd
sy
These
circuits
are
control
critical
because
the
critical
path
is
from
the
control
signals
to
the
output
Any
additional
delay
in
the
control
signals
will
add
directly
to
the
worst
case
delay
The
delay
from
D
to
Y
in
Figure
b
is
only
ps
compared
with
the
delay
from
S
to
Y
of
ps
Timing
A
B
C
D
E
Y
Figure
Ben
s
circuit
Figure
Ben
s
critical
path
A
B
C
D
E
Y
Figure
Ben
s
shortest
path
A
B
C
D
E
Y
C
Figure
shows
the
hierarchical
implementation
of
the
multiplexer
using
two
stages
of
multiplexers
The
critical
path
is
from
any
of
the
D
inputs
to
the
output
This
circuit
is
data
critical
because
the
critical
path
is
from
the
data
input
to
the
output
tpd
tpd
dy
If
data
inputs
arrive
well
before
the
control
inputs
we
would
prefer
the
design
with
the
shortest
control
to
output
delay
the
hierarchical
design
in
Figure
Similarly
if
the
control
inputs
arrive
well
before
the
data
inputs
we
would
prefer
the
design
with
the
shortest
data
to
output
delay
the
tristate
design
in
Figure
b
The
best
choice
depends
not
only
on
the
critical
path
through
the
circuit
and
the
input
arrival
times
but
also
on
the
power
cost
and
availability
of
parts
Glitches
So
far
we
have
discussed
the
case
where
a
single
input
transition
causes
a
single
output
transition
However
it
is
possible
that
a
single
input
transition
can
cause
multiple
output
transitions
These
are
called
glitches
or
hazards
Although
glitches
usually
don
t
cause
problems
it
is
important
to
realize
that
they
exist
and
recognize
them
when
looking
at
timing
diagrams
Figure
shows
a
circuit
with
a
glitch
and
the
Karnaugh
map
of
the
circuit
The
Boolean
equation
is
correctly
minimized
but
let
s
look
at
what
happens
when
A
C
and
B
transitions
from
to
Figure
see
page
illustrates
this
scenario
The
short
path
shown
in
gray
goes
through
two
gates
the
AND
and
OR
gates
The
critical
path
shown
in
blue
goes
through
an
inverter
and
two
gates
the
AND
and
OR
gates
CHAPTER
TWO
Combinational
Logic
Design
Gate
tpd
ps
NOT
input
AND
input
AND
input
OR
tristate
A
to
Y
tristate
enable
to
Y
Table
Timing
specifications
for
multiplexer
circuit
elements
Hazards
have
another
meaning
related
to
microarchitecture
in
Chapter
so
we
will
stick
with
the
term
glitches
for
multiple
output
transitions
to
avoid
confusion
Cha
Timing
Figure
multiplexer
propagation
delays
a
two
level
logic
b
tristate
tpd
sy
tpd
INV
tpd
AND
tpd
OR
ps
ps
ps
ps
S
D
D
D
D
Out
S
a
tpd
dy
tpd
AND
tpd
OR
ps
D
D
Out
S
S
tpd
sy
tpd
INV
tpd
AND
tpd
TRI
SY
ps
ps
ps
ps
b
tpd
dy
tpd
TRI
AY
ps
D
D
S
D
D
D
D
S
Y
tpd
s
y
tpd
TRLSY
tpd
TRI
AY
ns
mux
mux
mux
tpd
dy
tpd
TRI
AY
ns
Figure
multiplexer
propagation
delays
hierarchical
using
multiplexers
As
B
transitions
from
to
n
on
the
short
path
falls
before
n
on
the
critical
path
can
rise
Until
n
rises
the
two
inputs
to
the
OR
gate
are
and
the
output
Y
drops
to
When
n
eventually
rises
Y
returns
to
As
shown
in
the
timing
diagram
of
Figure
Y
starts
at
and
ends
at
but
momentarily
glitches
to
A
B
C
Y
Y
AB
C
Y
AB
BC
Figure
Circuit
with
a
glitch
CHAPTER
TWO
Combinational
Logic
Design
Figure
Input
change
crosses
implicant
boundary
Y
AB
C
Y
AB
BC
Figure
Timing
of
a
glitch
A
C
B
Y
Short
Path
Critical
Path
B
Y
Time
glitch
n
n
n
n
As
long
as
we
wait
for
the
propagation
delay
to
elapse
before
we
depend
on
the
output
glitches
are
not
a
problem
because
the
output
eventually
settles
to
the
right
answer
If
we
choose
to
we
can
avoid
this
glitch
by
adding
another
gate
to
the
implementation
This
is
easiest
to
understand
in
terms
of
the
K
map
Figure
shows
how
an
input
transition
on
B
from
ABC
to
ABC
moves
from
one
prime
implicant
circle
to
another
The
transition
across
the
boundary
of
two
prime
implicants
in
the
K
map
indicates
a
possible
glitch
As
we
saw
from
the
timing
diagram
in
Figure
if
the
circuitry
implementing
one
of
the
prime
implicants
turns
off
before
the
circuitry
of
the
other
prime
implicant
can
turn
on
there
is
a
glitch
To
fix
this
we
add
another
circle
that
covers
that
prime
implicant
boundary
as
shown
in
Figure
You
might
recognize
this
as
the
consensus
theorem
where
the
added
term
is
the
consensus
or
redundant
term
AC
Ch
Summary
Y
AB
C
AC
Y
AB
BC
AC
B
Y
A
C
Figure
K
map
without
glitch
Figure
Circuit
without
glitch
Figure
shows
the
glitch
proof
circuit
The
added
AND
gate
is
highlighted
in
blue
Now
a
transition
on
B
when
A
and
C
does
not
cause
a
glitch
on
the
output
because
the
blue
AND
gate
outputs
throughout
the
transition
In
general
a
glitch
can
occur
when
a
change
in
a
single
variable
crosses
the
boundary
between
two
prime
implicants
in
a
K
map
We
can
eliminate
the
glitch
by
adding
redundant
implicants
to
the
K
map
to
cover
these
boundaries
This
of
course
comes
at
the
cost
of
extra
hardware
However
simultaneous
transitions
on
multiple
variables
can
also
cause
glitches
These
glitches
cannot
be
fixed
by
adding
hardware
Because
the
vast
majority
of
interesting
systems
have
simultaneous
or
near
simultaneous
transitions
on
multiple
variables
glitches
are
a
fact
of
life
in
most
circuits
Although
we
have
shown
how
to
eliminate
one
kind
of
glitch
the
point
of
discussing
glitches
is
not
to
eliminate
them
but
to
be
aware
that
they
exist
This
is
especially
important
when
looking
at
timing
diagrams
on
a
simulator
or
oscilloscope
SUMMARY
A
digital
circuit
is
a
module
with
discrete
valued
inputs
and
outputs
and
a
specification
describing
the
function
and
timing
of
the
module
This
chapter
has
focused
on
combinational
circuits
circuits
whose
outputs
depend
only
on
the
current
values
of
the
inputs
Ch
The
function
of
a
combinational
circuit
can
be
given
by
a
truth
table
or
a
Boolean
equation
The
Boolean
equation
for
any
truth
table
can
be
obtained
systematically
using
sum
of
products
or
product
ofsums
form
In
sum
of
products
form
the
function
is
written
as
the
sum
OR
of
one
or
more
implicants
Implicants
are
the
product
AND
of
literals
Literals
are
the
true
or
complementary
forms
of
the
input
variables
Boolean
equations
can
be
simplified
using
the
rules
of
Boolean
algebra
In
particular
they
can
be
simplified
into
minimal
sum
of
products
form
by
combining
implicants
that
differ
only
in
the
true
and
complementary
forms
of
one
of
the
literals
Karnaugh
maps
are
a
visual
tool
for
minimizing
functions
of
up
to
four
variables
With
practice
designers
can
usually
simplify
functions
of
a
few
variables
by
inspection
Computer
aided
design
tools
are
used
for
more
complicated
functions
such
methods
and
tools
are
discussed
in
Chapter
Logic
gates
are
connected
to
create
combinational
circuits
that
perform
the
desired
function
Any
function
in
sum
of
products
form
can
be
built
using
two
level
logic
with
the
literals
as
inputs
NOT
gates
form
the
complementary
literals
AND
gates
form
the
products
and
OR
gates
form
the
sum
Depending
on
the
function
and
the
building
blocks
available
multilevel
logic
implementations
with
various
types
of
gates
may
be
more
efficient
For
example
CMOS
circuits
favor
NAND
and
NOR
gates
because
these
gates
can
be
built
directly
from
CMOS
transistors
without
requiring
extra
NOT
gates
When
using
NAND
and
NOR
gates
bubble
pushing
is
helpful
to
keep
track
of
the
inversions
Logic
gates
are
combined
to
produce
larger
circuits
such
as
multiplexers
decoders
and
priority
circuits
A
multiplexer
chooses
one
of
the
data
inputs
based
on
the
select
input
A
decoder
sets
one
of
the
outputs
HIGH
according
to
the
input
A
priority
circuit
produces
an
output
indicating
the
highest
priority
input
These
circuits
are
all
examples
of
combinational
building
blocks
Chapter
will
introduce
more
building
blocks
including
other
arithmetic
circuits
These
building
blocks
will
be
used
extensively
to
build
a
microprocessor
in
Chapter
The
timing
specification
of
a
combinational
circuit
consists
of
the
propagation
and
contamination
delays
through
the
circuit
These
indicate
the
longest
and
shortest
times
between
an
input
change
and
the
consequent
output
change
Calculating
the
propagation
delay
of
a
circuit
involves
identifying
the
critical
path
through
the
circuit
then
adding
up
the
propagation
delays
of
each
element
along
that
path
There
are
many
different
ways
to
implement
complicated
combinational
circuits
these
ways
offer
trade
offs
between
speed
and
cost
The
next
chapter
will
move
to
sequential
circuits
whose
outputs
depend
on
previous
as
well
as
current
values
of
the
inputs
In
other
words
sequential
circuits
have
memory
of
the
past
PA
PA
P
CHAPTER
TWO
Combinational
Logic
Design
Ch
Exercises
Exercises
Exercise
Write
a
Boolean
equation
in
sum
of
products
canonical
form
for
each
of
the
truth
tables
in
Figure
Exercise
Write
a
Boolean
equation
in
product
of
sums
canonical
form
for
the
truth
tables
in
Figure
Exercise
Minimize
each
of
the
Boolean
equations
from
Exercise
Exercise
Sketch
a
reasonably
simple
combinational
circuit
implementing
each
of
the
functions
from
Exercise
Reasonably
simple
means
that
you
are
not
wasteful
of
gates
but
you
don
t
waste
vast
amounts
of
time
checking
every
possible
implementation
of
the
circuit
either
Exercise
Repeat
Exercise
using
only
NOT
gates
and
AND
and
OR
gates
Exercise
Repeat
Exercise
using
only
NOT
gates
and
NAND
and
NOR
gates
Exercise
Simplify
the
following
Boolean
equations
using
Boolean
theorems
Check
for
correctness
using
a
truth
table
or
K
map
a
b
c
Y
ABCD
ABC
ABCD
ABD
ABCD
BCD
A
Y
AB
ABC
A
C
Y
AC
ABC
BCY
A
BCY
A
ABY
CDY
B
A
CDY
B
A
a
b
c
d
e
Figure
Truth
tables
Chapter
qx
CHAPTER
TWO
Combinational
Logic
Design
Exercise
Sketch
a
reasonably
simple
combinational
circuit
implementing
each
of
the
functions
from
Exercise
Exercise
Simplify
each
of
the
following
Boolean
equations
Sketch
a
reasonably
simple
combinational
circuit
implementing
the
simplified
equation
a
b
c
Y
ABC
ABD
ABE
ACD
ACE
Exercise
Give
an
example
of
a
truth
table
requiring
between
billion
and
billion
rows
that
can
be
constructed
using
fewer
than
but
at
least
two
input
gates
Exercise
Give
an
example
of
a
circuit
with
a
cyclic
path
that
is
nevertheless
combinational
Exercise
Alyssa
P
Hacker
says
that
any
Boolean
function
can
be
written
in
minimal
sum
of
products
form
as
the
sum
of
all
of
the
prime
implicants
of
the
function
Ben
Bitdiddle
says
that
there
are
some
functions
whose
minimal
equation
does
not
involve
all
of
the
prime
implicants
Explain
why
Alyssa
is
right
or
provide
a
counterexample
demonstrating
Ben
s
point
Exercise
Prove
that
the
following
theorems
are
true
using
perfect
induction
You
need
not
prove
their
duals
a
The
idempotency
theorem
T
b
The
distributivity
theorem
T
c
The
combining
theorem
T
Exercise
Prove
De
Morgan
s
Theorem
T
for
three
variables
B
B
B
using
perfect
induction
Exercise
Write
Boolean
equations
for
the
circuit
in
Figure
You
need
not
minimize
the
equations
BCE
BDE
CDE
A
D
E
BCD
Y
A
AB
AB
A
B
Y
BC
ABC
BC
Chapter
qxd
Exercises
Exercise
Minimize
the
Boolean
equations
from
Exercise
and
sketch
an
improved
circuit
with
the
same
function
Exercise
Using
De
Morgan
equivalent
gates
and
bubble
pushing
methods
redraw
the
circuit
in
Figure
so
that
you
can
find
the
Boolean
equation
by
inspection
Write
the
Boolean
equation
Exercise
Repeat
Exercise
for
the
circuit
in
Figure
A
B
C
D
Y
Z
A
B
C
D
E
Y
Figure
Circuit
schematic
Figure
Circuit
schematic
CHAPTER
TWO
Combinational
Logic
Design
Exercise
Find
a
minimal
Boolean
equation
for
the
function
in
Figure
Remember
to
take
advantage
of
the
don
t
care
entries
Exercise
Sketch
a
circuit
for
the
function
from
Exercise
Exercise
Does
your
circuit
from
Exercise
have
any
potential
glitches
when
one
of
the
inputs
changes
If
not
explain
why
not
If
so
show
how
to
modify
the
circuit
to
eliminate
the
glitches
Exercise
Ben
Bitdiddle
will
enjoy
his
picnic
on
sunny
days
that
have
no
ants
He
will
also
enjoy
his
picnic
any
day
he
sees
a
hummingbird
as
well
as
on
days
where
there
are
ants
and
ladybugs
Write
a
Boolean
equation
for
his
enjoyment
E
in
terms
of
sun
S
ants
A
hummingbirds
H
and
ladybugs
L
CDY
X
X
X
B
X
X
A
X
X
Figure
Truth
table
A
B
C
D
E
F
G
Y
Figure
Circuit
schematic
Exercises
Exercise
Complete
the
design
of
the
seven
segment
decoder
segments
Sc
through
Sg
see
Example
a
Derive
Boolean
equations
for
the
outputs
Sc
through
Sg
assuming
that
inputs
greater
than
must
produce
blank
outputs
b
Derive
Boolean
equations
for
the
outputs
Sc
through
Sg
assuming
that
inputs
greater
than
are
don
t
cares
c
Sketch
a
reasonably
simple
gate
level
implementation
of
part
b
Multiple
outputs
can
share
gates
where
appropriate
Exercise
A
circuit
has
four
inputs
and
two
outputs
The
inputs
A
represent
a
number
from
to
Output
P
should
be
TRUE
if
the
number
is
prime
and
are
not
prime
but
and
so
on
are
prime
Output
D
should
be
TRUE
if
the
number
is
divisible
by
Give
simplified
Boolean
equations
for
each
output
and
sketch
a
circuit
Exercise
A
priority
encoder
has
N
inputs
It
produces
an
N
bit
binary
output
indicating
the
most
significant
bit
of
the
input
that
is
TRUE
or
if
none
of
the
inputs
are
TRUE
It
also
produces
an
output
NONE
that
is
TRUE
if
none
of
the
input
bits
are
TRUE
Design
an
eight
input
priority
encoder
with
inputs
A
and
outputs
Y
and
NONE
For
example
if
the
input
is
the
output
Y
should
be
and
NONE
should
be
Give
a
simplified
Boolean
equation
for
each
output
and
sketch
a
schematic
Exercise
Design
a
modified
priority
encoder
see
Exercise
that
receives
an
bit
input
A
and
produces
two
bit
outputs
Y
and
Z
Y
indicates
the
most
significant
bit
of
the
input
that
is
TRUE
Z
indicates
the
second
most
significant
bit
of
the
input
that
is
TRUE
Y
should
be
if
none
of
the
inputs
are
TRUE
Z
should
be
if
no
more
than
one
of
the
inputs
is
TRUE
Give
a
simplified
Boolean
equation
for
each
output
and
sketch
a
schematic
Exercise
An
M
bit
thermometer
code
for
the
number
k
consists
of
k
s
in
the
least
significant
bit
positions
and
M
k
s
in
all
the
more
significant
bit
positions
A
binary
to
thermometer
code
converter
has
N
inputs
and
N
outputs
It
produces
a
N
bit
thermometer
code
for
the
number
specified
by
the
input
For
example
if
the
input
is
the
output
should
be
Design
a
binary
to
thermometer
code
converter
Give
a
simplified
Boolean
equation
for
each
output
and
sketch
a
schematic
Exercise
Write
a
minimized
Boolean
equation
for
the
function
performed
by
the
circuit
in
Figure
CHAPTER
TWO
Combinational
Logic
Design
Exercise
Implement
the
function
from
Figure
b
using
a
an
multiplexer
b
a
multiplexer
and
one
inverter
c
a
multiplexer
and
two
other
logic
gates
Exercise
Implement
the
function
from
Exercise
a
using
a
an
multiplexer
b
a
multiplexer
and
no
other
gates
c
a
multiplexer
one
OR
gate
and
an
inverter
Exercise
Determine
the
propagation
delay
and
contamination
delay
of
the
circuit
in
Figure
Use
the
gate
delays
given
in
Table
Exercise
Write
a
minimized
Boolean
equation
for
the
function
performed
by
the
circuit
in
Figure
Figure
Multiplexer
circuit
C
D
A
Y
Figure
Multiplexer
circuit
C
D
Y
A
B
Exercises
Exercise
Sketch
a
schematic
for
a
fast
decoder
Suppose
gate
delays
are
given
in
Table
and
only
the
gates
in
that
table
are
available
Design
your
decoder
to
have
the
shortest
possible
critical
path
and
indicate
what
that
path
is
What
are
its
propagation
delay
and
contamination
delay
Exercise
Redesign
the
circuit
from
Exercise
to
be
as
fast
as
possible
Use
only
the
gates
from
Table
Sketch
the
new
circuit
and
indicate
the
critical
path
What
are
its
propagation
delay
and
contamination
delay
Exercise
Redesign
the
priority
encoder
from
Exercise
to
be
as
fast
as
possible
You
may
use
any
of
the
gates
from
Table
Sketch
the
new
circuit
and
indicate
the
critical
path
What
are
its
propagation
delay
and
contamination
delay
Exercise
Design
an
multiplexer
with
the
shortest
possible
delay
from
the
data
inputs
to
the
output
You
may
use
any
of
the
gates
from
Table
on
page
Sketch
a
schematic
Using
the
gate
delays
from
the
table
determine
this
delay
Gate
tpd
ps
tcd
ps
NOT
input
NAND
input
NAND
input
NOR
input
NOR
input
AND
input
AND
input
OR
input
OR
input
XOR
Table
Gate
delays
for
Exercises
CHAPTER
TWO
Combinational
Logic
Design
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Sketch
a
schematic
for
the
two
input
XOR
function
using
only
NAND
gates
How
few
can
you
use
Question
Design
a
circuit
that
will
tell
whether
a
given
month
has
days
in
it
The
month
is
specified
by
a
bit
input
A
For
example
if
the
inputs
are
the
month
is
January
and
if
the
inputs
are
the
month
is
December
The
circuit
output
Y
should
be
HIGH
only
when
the
month
specified
by
the
inputs
has
days
in
it
Write
the
simplified
equation
and
draw
the
circuit
diagram
using
a
minimum
number
of
gates
Hint
Remember
to
take
advantage
of
don
t
cares
Question
What
is
a
tristate
buffer
How
and
why
is
it
used
Question
A
gate
or
set
of
gates
is
universal
if
it
can
be
used
to
construct
any
Boolean
function
For
example
the
set
AND
OR
NOT
is
universal
a
Is
an
AND
gate
by
itself
universal
Why
or
why
not
b
Is
the
set
OR
NOT
universal
Why
or
why
not
c
Is
a
NAND
gate
by
itself
universal
Why
or
why
not
Question
Explain
why
a
circuit
s
contamination
delay
might
be
less
than
instead
of
equal
to
its
propagation
delay
Introduction
Latches
and
Flip
Flops
Synchronous
Logic
Design
Finite
State
Machines
Timing
of
Sequential
Logic
Parallelism
Summary
Exercises
Interview
Questions
Sequential
Logic
Design
INTRODUCTION
In
the
last
chapter
we
showed
how
to
analyze
and
design
combinational
logic
The
output
of
combinational
logic
depends
only
on
current
input
values
Given
a
specification
in
the
form
of
a
truth
table
or
Boolean
equation
we
can
create
an
optimized
circuit
to
meet
the
specification
In
this
chapter
we
will
analyze
and
design
sequential
logic
The
outputs
of
sequential
logic
depend
on
both
current
and
prior
input
values
Hence
sequential
logic
has
memory
Sequential
logic
might
explicitly
remember
certain
previous
inputs
or
it
might
distill
the
prior
inputs
into
a
smaller
amount
of
information
called
the
state
of
the
system
The
state
of
a
digital
sequential
circuit
is
a
set
of
bits
called
state
variables
that
contain
all
the
information
about
the
past
necessary
to
explain
the
future
behavior
of
the
circuit
The
chapter
begins
by
studying
latches
and
flip
flops
which
are
simple
sequential
circuits
that
store
one
bit
of
state
In
general
sequential
circuits
are
complicated
to
analyze
To
simplify
design
we
discipline
ourselves
to
build
only
synchronous
sequential
circuits
consisting
of
combinational
logic
and
banks
of
flip
flops
containing
the
state
of
the
circuit
The
chapter
describes
finite
state
machines
which
are
an
easy
way
to
design
sequential
circuits
Finally
we
analyze
the
speed
of
sequential
circuits
and
discuss
parallelism
as
a
way
to
increase
clock
speed
LATCHES
AND
FLIP
FLOPS
The
fundamental
building
block
of
memory
is
a
bistable
element
an
element
with
two
stable
states
Figure
a
shows
a
simple
bistable
element
consisting
of
a
pair
of
inverters
connected
in
a
loop
Figure
b
shows
the
same
circuit
redrawn
to
emphasize
the
symmetry
The
inverters
are
cross
coupled
meaning
that
the
input
of
I
is
the
output
of
I
and
vice
versa
The
circuit
has
no
inputs
but
it
does
have
two
outputs
Q
and
Analyzing
this
circuit
is
different
from
analyzing
a
combinational
circuit
because
it
is
cyclic
Q
depends
on
and
depends
on
Q
Consider
the
two
cases
Q
is
or
Q
is
Working
through
the
consequences
of
each
case
we
have
Case
I
Q
As
shown
in
Figure
a
I
receives
a
FALSE
input
Q
so
it
produces
a
TRUE
output
on
I
receives
a
TRUE
input
so
it
produces
a
FALSE
output
on
Q
This
is
consistent
with
the
original
assumption
that
Q
so
the
case
is
said
to
be
stable
Case
II
Q
As
shown
in
Figure
b
I
receives
a
TRUE
input
and
produces
a
FALSE
output
on
I
receives
a
FALSE
input
and
produces
a
TRUE
output
on
Q
This
is
again
stable
Because
the
cross
coupled
inverters
have
two
stable
states
Q
and
Q
the
circuit
is
said
to
be
bistable
A
subtle
point
is
that
the
circuit
has
a
third
possible
state
with
both
outputs
approximately
halfway
between
and
This
is
called
a
metastable
state
and
will
be
discussed
in
Section
An
element
with
N
stable
states
conveys
log
N
bits
of
information
so
a
bistable
element
stores
one
bit
The
state
of
the
cross
coupled
inverters
is
contained
in
one
binary
state
variable
Q
The
value
of
Q
tells
us
everything
about
the
past
that
is
necessary
to
explain
the
future
behavior
of
the
circuit
Specifically
if
Q
it
will
remain
forever
and
if
Q
it
will
remain
forever
The
circuit
does
have
another
node
but
does
not
contain
any
additional
information
because
if
Q
is
known
is
also
known
On
the
other
hand
is
also
an
acceptable
choice
for
the
state
variable
Q
Q
Q
Q
Q
Q
Q
Q
Q
Q
CHAPTER
THREE
Sequential
Logic
Design
b
Q
Q
I
I
Q
Q
a
I
I
Figure
Cross
coupled
inverter
pair
b
Q
Q
I
I
a
I
Q
I
Q
Figure
Bistable
operation
of
cross
coupled
inverters
Just
as
Y
is
commonly
used
for
the
output
of
combinational
logic
Q
is
commonly
used
for
the
output
of
sequential
logic
Chapter
When
power
is
first
applied
to
a
sequential
circuit
the
initial
state
is
unknown
and
usually
unpredictable
It
may
differ
each
time
the
circuit
is
turned
on
Although
the
cross
coupled
inverters
can
store
a
bit
of
information
they
are
not
practical
because
the
user
has
no
inputs
to
control
the
state
However
other
bistable
elements
such
as
latches
and
flip
flops
provide
inputs
to
control
the
value
of
the
state
variable
The
remainder
of
this
section
considers
these
circuits
SR
Latch
One
of
the
simplest
sequential
circuits
is
the
SR
latch
which
is
composed
of
two
cross
coupled
NOR
gates
as
shown
in
Figure
The
latch
has
two
inputs
S
and
R
and
two
outputs
Q
and
The
SR
latch
is
similar
to
the
cross
coupled
inverters
but
its
state
can
be
controlled
through
the
S
and
R
inputs
which
set
and
reset
the
output
Q
A
good
way
to
understand
an
unfamiliar
circuit
is
to
work
out
its
truth
table
so
that
is
where
we
begin
Recall
that
a
NOR
gate
produces
a
FALSE
output
when
either
input
is
TRUE
Consider
the
four
possible
combinations
of
R
and
S
Case
I
R
S
N
sees
at
least
one
TRUE
input
R
so
it
produces
a
FALSE
output
on
Q
N
sees
both
Q
and
S
FALSE
so
it
produces
a
TRUE
output
on
Case
II
R
S
N
receives
inputs
of
and
Because
we
don
t
yet
know
we
can
t
determine
the
output
Q
N
receives
at
least
one
TRUE
input
S
so
it
produces
a
FALSE
output
on
Now
we
can
revisit
N
knowing
that
both
inputs
are
FALSE
so
the
output
Q
is
TRUE
Case
III
R
S
N
and
N
both
see
at
least
one
TRUE
input
R
or
S
so
each
produces
a
FALSE
output
Hence
Q
and
are
both
FALSE
Case
IV
R
S
N
receives
inputs
of
and
Because
we
don
t
yet
know
we
can
t
determine
the
output
N
receives
inputs
of
and
Q
Because
we
don
t
yet
know
Q
we
can
t
determine
the
output
Now
we
are
stuck
This
is
reminiscent
of
the
cross
coupled
inverters
But
we
know
that
Q
must
either
be
or
So
we
can
solve
the
problem
by
checking
what
happens
in
each
of
these
subcases
Q
Q
Q
Q
Q
Q
Q
Q
Latches
and
Flip
Flops
R
S
N
Q
N
Q
Figure
SR
latch
schematic
Chapter
q
Case
IVa
Q
Because
S
and
Q
are
FALSE
N
produces
a
TRUE
output
on
as
shown
in
Figure
a
Now
N
receives
one
TRUE
input
so
its
output
Q
is
FALSE
just
as
we
had
assumed
Case
IVb
Q
Because
Q
is
TRUE
N
produces
a
FALSE
output
on
as
shown
in
Figure
b
Now
N
receives
two
FALSE
inputs
R
and
so
its
output
Q
is
TRUE
just
as
we
had
assumed
Putting
this
all
together
suppose
Q
has
some
known
prior
value
which
we
will
call
Qprev
before
we
enter
Case
IV
Qprev
is
either
or
and
represents
the
state
of
the
system
When
R
and
S
are
Q
will
remember
this
old
value
Qprev
and
will
be
its
complement
This
circuit
has
memory
The
truth
table
in
Figure
summarizes
these
four
cases
The
inputs
S
and
R
stand
for
Set
and
Reset
To
set
a
bit
means
to
make
it
TRUE
To
reset
a
bit
means
to
make
it
FALSE
The
outputs
Q
and
are
normally
complementary
When
R
is
asserted
Q
is
reset
to
and
does
the
opposite
When
S
is
asserted
Q
is
set
to
and
does
the
opposite
When
neither
input
is
asserted
Q
remembers
its
old
value
Qprev
Asserting
both
S
and
R
simultaneously
doesn
t
make
much
sense
because
it
means
the
latch
should
be
set
and
reset
at
the
same
time
which
is
impossible
The
poor
confused
circuit
responds
by
making
both
outputs
The
SR
latch
is
represented
by
the
symbol
in
Figure
Using
the
symbol
is
an
application
of
abstraction
and
modularity
There
are
various
ways
to
build
an
SR
latch
such
as
using
different
logic
gates
or
transistors
Nevertheless
any
circuit
element
with
the
relationship
specified
by
the
truth
table
in
Figure
and
the
symbol
in
Figure
is
called
an
SR
latch
Like
the
cross
coupled
inverters
the
SR
latch
is
a
bistable
element
with
one
bit
of
state
stored
in
Q
However
the
state
can
be
controlled
through
the
S
and
R
inputs
When
R
is
asserted
the
state
is
reset
to
When
S
is
asserted
the
state
is
set
to
When
neither
is
asserted
the
state
retains
its
old
value
Notice
that
the
entire
history
of
inputs
can
be
Q
Q
Q
Q
Qprev
Q
Q
Q
Q
CHAPTER
THREE
Sequential
Logic
Design
R
S
N
Q
N
b
Q
R
S
N
Q
N
a
Q
Figure
Bistable
states
of
SR
latch
SRQ
Qprev
Case
IV
I
II
III
Q
Qprev
Figure
SR
latch
truth
table
S
R
Q
Q
Figure
SR
latch
symbol
Cha
accounted
for
by
the
single
state
variable
Q
No
matter
what
pattern
of
setting
and
resetting
occurred
in
the
past
all
that
is
needed
to
predict
the
future
behavior
of
the
SR
latch
is
whether
it
was
most
recently
set
or
reset
D
Latch
The
SR
latch
is
awkward
because
it
behaves
strangely
when
both
S
and
R
are
simultaneously
asserted
Moreover
the
S
and
R
inputs
conflate
the
issues
of
what
and
when
Asserting
one
of
the
inputs
determines
not
only
what
the
state
should
be
but
also
when
it
should
change
Designing
circuits
becomes
easier
when
these
questions
of
what
and
when
are
separated
The
D
latch
in
Figure
a
solves
these
problems
It
has
two
inputs
The
data
input
D
controls
what
the
next
state
should
be
The
clock
input
CLK
controls
when
the
state
should
change
Again
we
analyze
the
latch
by
writing
the
truth
table
given
in
Figure
b
For
convenience
we
first
consider
the
internal
nodes
S
and
R
If
CLK
both
S
and
R
are
FALSE
regardless
of
the
value
of
D
If
CLK
one
AND
gate
will
produce
TRUE
and
the
other
FALSE
depending
on
the
value
of
D
Given
S
and
R
Q
and
are
determined
using
Figure
Observe
that
when
CLK
Q
remembers
its
old
value
Qprev
When
CLK
Q
D
In
all
cases
is
the
complement
of
Q
as
would
seem
logical
The
D
latch
avoids
the
strange
case
of
simultaneously
asserted
R
and
S
inputs
Putting
it
all
together
we
see
that
the
clock
controls
when
data
flows
through
the
latch
When
CLK
the
latch
is
transparent
The
data
at
D
flows
through
to
Q
as
if
the
latch
were
just
a
buffer
When
CLK
the
latch
is
opaque
It
blocks
the
new
data
from
flowing
through
to
Q
and
Q
retains
the
old
value
Hence
the
D
latch
is
sometimes
called
a
transparent
latch
or
a
level
sensitive
latch
The
D
latch
symbol
is
given
in
Figure
c
The
D
latch
updates
its
state
continuously
while
CLK
We
shall
see
later
in
this
chapter
that
it
is
useful
to
update
the
state
only
at
a
specific
instant
in
time
The
D
flip
flop
described
in
the
next
section
does
just
that
Q
Q
D
Latches
and
Flip
Flops
S
R
Q
Q
D
CLK
D
R
S
a
Q
Q
CLK
D
Q
c
Q
S
R
Q
Qprev
CLK
D
X
D
X
b
Qprev
Q
Figure
D
latch
a
schematic
b
truth
table
c
symbol
Some
people
call
a
latch
open
or
closed
rather
than
transparent
or
opaque
However
we
think
those
terms
are
ambiguous
does
open
mean
transparent
like
an
open
door
or
opaque
like
an
open
circuit
Chapter
D
Flip
Flop
A
D
flip
flop
can
be
built
from
two
back
to
back
D
latches
controlled
by
complementary
clocks
as
shown
in
Figure
a
The
first
latch
L
is
called
the
master
The
second
latch
L
is
called
the
slave
The
node
between
them
is
named
N
A
symbol
for
the
D
flip
flop
is
given
in
Figure
b
When
the
output
is
not
needed
the
symbol
is
often
condensed
as
in
Figure
c
When
CLK
the
master
latch
is
transparent
and
the
slave
is
opaque
Therefore
whatever
value
was
at
D
propagates
through
to
N
When
CLK
the
master
goes
opaque
and
the
slave
becomes
transparent
The
value
at
N
propagates
through
to
Q
but
N
is
cut
off
from
D
Hence
whatever
value
was
at
D
immediately
before
the
clock
rises
from
to
gets
copied
to
Q
immediately
after
the
clock
rises
At
all
other
times
Q
retains
its
old
value
because
there
is
always
an
opaque
latch
blocking
the
path
between
D
and
Q
In
other
words
a
D
flip
flop
copies
D
to
Q
on
the
rising
edge
of
the
clock
and
remembers
its
state
at
all
other
times
Reread
this
definition
until
you
have
it
memorized
one
of
the
most
common
problems
for
beginning
digital
designers
is
to
forget
what
a
flip
flop
does
The
rising
edge
of
the
clock
is
often
just
called
the
clock
edge
for
brevity
The
D
input
specifies
what
the
new
state
will
be
The
clock
edge
indicates
when
the
state
should
be
updated
A
D
flip
flop
is
also
known
as
a
master
slave
flip
flop
an
edge
triggered
flip
flop
or
a
positive
edge
triggered
flip
flop
The
triangle
in
the
symbols
denotes
an
edge
triggered
clock
input
The
output
is
often
omitted
when
it
is
not
needed
Example
FLIP
FLOP
TRANSISTOR
COUNT
How
many
transistors
are
needed
to
build
the
D
flip
flop
described
in
this
section
Solution
A
NAND
or
NOR
gate
uses
four
transistors
A
NOT
gate
uses
two
transistors
An
AND
gate
is
built
from
a
NAND
and
a
NOT
so
it
uses
six
transistors
The
SR
latch
uses
two
NOR
gates
or
eight
transistors
The
D
latch
uses
an
SR
latch
two
AND
gates
and
a
NOT
gate
or
transistors
The
D
flip
flop
uses
two
D
latches
and
a
NOT
gate
or
transistors
Section
describes
a
more
efficient
CMOS
implementation
using
transmission
gates
Register
An
N
bit
register
is
a
bank
of
N
flip
flops
that
share
a
common
CLK
input
so
that
all
bits
of
the
register
are
updated
at
the
same
time
Q
Q
CHAPTER
THREE
Sequential
Logic
Design
a
CLK
D
Q
CLK
D
D
Q
Q
N
CLK
L
L
master
slave
b
D
Q
c
Q
Q
Q
Q
Figure
D
flip
flop
a
schematic
b
symbol
c
condensed
symbol
The
precise
distinction
between
flip
flops
and
latches
is
somewhat
muddled
and
has
evolved
over
time
In
common
industry
usage
a
flip
flop
is
edge
triggered
In
other
words
it
is
a
bistable
element
with
a
clock
input
The
state
of
the
flip
flop
changes
only
in
response
to
a
clock
edge
such
as
when
the
clock
rises
from
to
Bistable
elements
without
an
edge
triggered
clock
are
commonly
called
latches
The
term
flip
flop
or
latch
by
itself
usually
refers
to
a
D
flip
flop
or
D
latch
respectively
because
these
are
the
types
most
commonly
used
in
practice
Ch
Registers
are
the
key
building
block
of
most
sequential
circuits
Figure
shows
the
schematic
and
symbol
for
a
four
bit
register
with
inputs
D
and
outputs
Q
D
and
Q
are
both
bit
busses
Enabled
Flip
Flop
An
enabled
flip
flop
adds
another
input
called
EN
or
ENABLE
to
determine
whether
data
is
loaded
on
the
clock
edge
When
EN
is
TRUE
the
enabled
flip
flop
behaves
like
an
ordinary
D
flip
flop
When
EN
is
FALSE
the
enabled
flip
flop
ignores
the
clock
and
retains
its
state
Enabled
flip
flops
are
useful
when
we
wish
to
load
a
new
value
into
a
flip
flop
only
some
of
the
time
rather
than
on
every
clock
edge
Figure
shows
two
ways
to
construct
an
enabled
flip
flop
from
a
D
flip
flop
and
an
extra
gate
In
Figure
a
an
input
multiplexer
chooses
whether
to
pass
the
value
at
D
if
EN
is
TRUE
or
to
recycle
the
old
state
from
Q
if
EN
is
FALSE
In
Figure
b
the
clock
is
gated
Latches
and
Flip
Flops
CLK
D
Q
D
Q
D
Q
D
Q
D
D
D
D
Q
Q
Q
Q
a
D
Q
CLK
b
Figure
A
bit
register
a
schematic
and
b
symbol
b
D
Q
CLK
EN
D
Q
D
Q
EN
a
c
D
Q
EN
CLK
D
Q
Figure
Enabled
flip
flop
a
b
schematics
c
symbol
If
EN
is
TRUE
the
CLK
input
to
the
flip
flop
toggles
normally
If
EN
is
FALSE
the
CLK
input
is
also
FALSE
and
the
flip
flop
retains
its
old
value
Notice
that
EN
must
not
change
while
CLK
lest
the
flip
flop
see
a
clock
glitch
switch
at
an
incorrect
time
Generally
performing
logic
on
the
clock
is
a
bad
idea
Clock
gating
delays
the
clock
and
can
cause
timing
errors
as
we
will
see
in
Section
so
do
it
only
if
you
are
sure
you
know
what
you
are
doing
The
symbol
for
an
enabled
flipflop
is
given
in
Figure
c
Resettable
Flip
Flop
A
resettable
flip
flop
adds
another
input
called
RESET
When
RESET
is
FALSE
the
resettable
flip
flop
behaves
like
an
ordinary
D
flip
flop
When
RESET
is
TRUE
the
resettable
flip
flop
ignores
D
and
resets
the
output
to
Resettable
flip
flops
are
useful
when
we
want
to
force
a
known
state
i
e
into
all
the
flip
flops
in
a
system
when
we
first
turn
it
on
Such
flip
flops
may
be
synchronously
or
asynchronously
resettable
Synchronously
resettable
flip
flops
reset
themselves
only
on
the
rising
edge
of
CLK
Asynchronously
resettable
flip
flops
reset
themselves
as
soon
as
RESET
becomes
TRUE
independent
of
CLK
Figure
a
shows
how
to
construct
a
synchronously
resettable
flip
flop
from
an
ordinary
D
flip
flop
and
an
AND
gate
When
is
FALSE
the
AND
gate
forces
a
into
the
input
of
the
flip
flop
When
is
TRUE
the
AND
gate
passes
D
to
the
flip
flop
In
this
example
is
an
active
low
signal
meaning
that
the
reset
signal
performs
its
function
when
it
is
not
By
adding
an
inverter
the
circuit
could
have
accepted
an
active
high
RESET
signal
instead
Figures
b
and
c
show
symbols
for
the
resettable
flip
flop
with
active
high
RESET
Asynchronously
resettable
flip
flops
require
modifying
the
internal
structure
of
the
flip
flop
and
are
left
to
you
to
design
in
Exercise
however
they
are
frequently
available
to
the
designer
as
a
standard
component
As
you
might
imagine
settable
flip
flops
are
also
occasionally
used
They
load
a
into
the
flip
flop
when
SET
is
asserted
and
they
too
come
in
synchronous
and
asynchronous
flavors
Resettable
and
settable
flip
flops
may
also
have
an
enable
input
and
may
be
grouped
into
N
bit
registers
Transistor
Level
Latch
and
Flip
Flop
Designs
Example
showed
that
latches
and
flip
flops
require
a
large
number
of
transistors
when
built
from
logic
gates
But
the
fundamental
role
of
a
RESET
RESET
RESET
CHAPTER
THREE
Sequential
Logic
Design
a
D
Q
CLK
D
Q
RESET
D
Q
RESET
b
c
r
Figure
Synchronously
resettable
flip
flop
a
schematic
b
c
symbols
C
latch
is
to
be
transparent
or
opaque
much
like
a
switch
Recall
from
Section
that
a
transmission
gate
is
an
efficient
way
to
build
a
CMOS
switch
so
we
might
expect
that
we
could
take
advantage
of
transmission
gates
to
reduce
the
transistor
count
A
compact
D
latch
can
be
constructed
from
a
single
transmission
gate
as
shown
in
Figure
a
When
CLK
and
the
transmission
gate
is
ON
so
D
flows
to
Q
and
the
latch
is
transparent
When
CLK
and
the
transmission
gate
is
OFF
so
Q
is
isolated
from
D
and
the
latch
is
opaque
This
latch
suffers
from
two
major
limitations
Floating
output
node
When
the
latch
is
opaque
Q
is
not
held
at
its
value
by
any
gates
Thus
Q
is
called
a
floating
or
dynamic
node
After
some
time
noise
and
charge
leakage
may
disturb
the
value
of
Q
No
buffers
The
lack
of
buffers
has
caused
malfunctions
on
several
commercial
chips
A
spike
of
noise
that
pulls
D
to
a
negative
voltage
can
turn
on
the
nMOS
transistor
making
the
latch
transparent
even
when
CLK
Likewise
a
spike
on
D
above
VDD
can
turn
on
the
pMOS
transistor
even
when
CLK
And
the
transmission
gate
is
symmetric
so
it
could
be
driven
backward
with
noise
on
Q
affecting
the
input
D
The
general
rule
is
that
neither
the
input
of
a
transmission
gate
nor
the
state
node
of
a
sequential
circuit
should
ever
be
exposed
to
the
outside
world
where
noise
is
likely
Figure
b
shows
a
more
robust
transistor
D
latch
used
on
modern
commercial
chips
It
is
still
built
around
a
clocked
transmission
gate
but
it
adds
inverters
I
and
I
to
buffer
the
input
and
output
The
state
of
the
latch
is
held
on
node
N
Inverter
I
and
the
tristate
buffer
T
provide
feedback
to
turn
N
into
a
static
node
If
a
small
amount
of
noise
occurs
on
N
while
CLK
T
will
drive
N
back
to
a
valid
logic
value
Figure
shows
a
D
flip
flop
constructed
from
two
static
latches
controlled
by
and
CLK
Some
redundant
internal
inverters
have
been
removed
so
the
flip
flop
requires
only
transistors
CLK
CLK
CLK
Latches
and
Flip
Flops
a
CLK
D
Q
CLK
CLK
D
Q
N
b
CLK
CLK
CLK
I
I
I
T
Figure
D
latch
schematic
This
circuit
assumes
CLK
and
are
both
available
If
not
two
more
transistors
are
needed
for
a
CLK
inverter
CLK
CLK
D
N
CLK
CLK
CLK
I
I
T
I
CLK
CLK
T
CLK
CLK
I
Q
N
Figure
D
flip
flop
schematic
Chapter
Putting
It
All
Together
Latches
and
flip
flops
are
the
fundamental
building
blocks
of
sequential
circuits
Remember
that
a
D
latch
is
level
sensitive
whereas
a
D
flip
flop
is
edge
triggered
The
D
latch
is
transparent
when
CLK
allowing
the
input
D
to
flow
through
to
the
output
Q
The
D
flip
flop
copies
D
to
Q
on
the
rising
edge
of
CLK
At
all
other
times
latches
and
flip
flops
retain
their
old
state
A
register
is
a
bank
of
several
D
flip
flops
that
share
a
common
CLK
signal
Example
FLIP
FLOP
AND
LATCH
COMPARISON
Ben
Bitdiddle
applies
the
D
and
CLK
inputs
shown
in
Figure
to
a
D
latch
and
a
D
flip
flop
Help
him
determine
the
output
Q
of
each
device
Solution
Figure
shows
the
output
waveforms
assuming
a
small
delay
for
Q
to
respond
to
input
changes
The
arrows
indicate
the
cause
of
an
output
change
The
initial
value
of
Q
is
unknown
and
could
be
or
as
indicated
by
the
pair
of
horizontal
lines
First
consider
the
latch
On
the
first
rising
edge
of
CLK
D
so
Q
definitely
becomes
Each
time
D
changes
while
CLK
Q
also
follows
When
D
changes
while
CLK
it
is
ignored
Now
consider
the
flip
flop
On
each
rising
edge
of
CLK
D
is
copied
to
Q
At
all
other
times
Q
retains
its
state
CHAPTER
THREE
Sequential
Logic
Design
CLK
D
Q
latch
Q
flop
Figure
Example
waveforms
CLK
D
Q
latch
Q
flop
Figure
Solution
waveforms
Chap
SYNCHRONOUS
LOGIC
DESIGN
In
general
sequential
circuits
include
all
circuits
that
are
not
combinational
that
is
those
whose
output
cannot
be
determined
simply
by
looking
at
the
current
inputs
Some
sequential
circuits
are
just
plain
kooky
This
section
begins
by
examining
some
of
those
curious
circuits
It
then
introduces
the
notion
of
synchronous
sequential
circuits
and
the
dynamic
discipline
By
disciplining
ourselves
to
synchronous
sequential
circuits
we
can
develop
easy
systematic
ways
to
analyze
and
design
sequential
systems
Some
Problematic
Circuits
Example
ASTABLE
CIRCUITS
Alyssa
P
Hacker
encounters
three
misbegotten
inverters
who
have
tied
themselves
in
a
loop
as
shown
in
Figure
The
output
of
the
third
inverter
is
fed
back
to
the
first
inverter
Each
inverter
has
a
propagation
delay
of
ns
Determine
what
the
circuit
does
Solution
Suppose
node
X
is
initially
Then
Y
Z
and
hence
X
which
is
inconsistent
with
our
original
assumption
The
circuit
has
no
stable
states
and
is
said
to
be
unstable
or
astable
Figure
shows
the
behavior
of
the
circuit
If
X
rises
at
time
Y
will
fall
at
ns
Z
will
rise
at
ns
and
X
will
fall
again
at
ns
In
turn
Y
will
rise
at
ns
Z
will
fall
at
ns
and
X
will
rise
again
at
ns
and
then
the
pattern
will
repeat
Each
node
oscillates
between
and
with
a
period
repetition
time
of
ns
This
circuit
is
called
a
ring
oscillator
The
period
of
the
ring
oscillator
depends
on
the
propagation
delay
of
each
inverter
This
delay
depends
on
how
the
inverter
was
manufactured
the
power
supply
voltage
and
even
the
temperature
Therefore
the
ring
oscillator
period
is
difficult
to
accurately
predict
In
short
the
ring
oscillator
is
a
sequential
circuit
with
zero
inputs
and
one
output
that
changes
periodically
Synchronous
Logic
Design
X
Y
Z
Figure
Three
inverter
loop
X
Y
Z
time
ns
Figure
Ring
oscillator
waveforms
Example
RACE
CONDITIONS
Ben
Bitdiddle
designed
a
new
D
latch
that
he
claims
is
better
than
the
one
in
Figure
because
it
uses
fewer
gates
He
has
written
the
truth
table
to
find
Cha
the
output
Q
given
the
two
inputs
D
and
CLK
and
the
old
state
of
the
latch
Qprev
Based
on
this
truth
table
he
has
derived
Boolean
equations
He
obtains
Qprev
by
feeding
back
the
output
Q
His
design
is
shown
in
Figure
Does
his
latch
work
correctly
independent
of
the
delays
of
each
gate
Solution
Figure
shows
that
the
circuit
has
a
race
condition
that
causes
it
to
fail
when
certain
gates
are
slower
than
others
Suppose
CLK
D
The
latch
is
transparent
and
passes
D
through
to
make
Q
Now
CLK
falls
The
latch
should
remember
its
old
value
keeping
Q
However
suppose
the
delay
through
the
inverter
from
CLK
to
is
rather
long
compared
to
the
delays
of
the
AND
and
OR
gates
Then
nodes
N
and
Q
may
both
fall
before
rises
In
such
a
case
N
will
never
rise
and
Q
becomes
stuck
at
This
is
an
example
of
asynchronous
circuit
design
in
which
outputs
are
directly
fed
back
to
inputs
Asynchronous
circuits
are
infamous
for
having
race
conditions
where
the
behavior
of
the
circuit
depends
on
which
of
two
paths
through
logic
gates
is
fastest
One
circuit
may
work
while
a
seemingly
identical
one
built
from
gates
with
slightly
different
delays
may
not
work
Or
the
circuit
may
work
only
at
certain
temperatures
or
voltages
at
which
the
delays
are
just
right
These
mistakes
are
extremely
difficult
to
track
down
Synchronous
Sequential
Circuits
The
previous
two
examples
contain
loops
called
cyclic
paths
in
which
outputs
are
fed
directly
back
to
inputs
They
are
sequential
rather
than
combinational
circuits
Combinational
logic
has
no
cyclic
paths
and
no
races
If
inputs
are
applied
to
combinational
logic
the
outputs
will
always
settle
to
the
correct
value
within
a
propagation
delay
However
sequential
circuits
with
cyclic
paths
can
have
undesirable
races
or
unstable
behavior
Analyzing
such
circuits
for
problems
is
time
consuming
and
many
bright
people
have
made
mistakes
CLK
CLK
CHAPTER
THREE
Sequential
Logic
Design
CLK
N
N
Q
CLK
Figure
Latch
waveforms
illustrating
race
condition
Q
CLK
D
Qprev
CLK
D
CLK
Qprev
Q
N
CLK
D
N
CLK
Qprev
Q
CLK
D
CLK
Qprev
Figure
An
improved
D
latch
Chap
To
avoid
these
problems
designers
break
the
cyclic
paths
by
inserting
registers
somewhere
in
the
path
This
transforms
the
circuit
into
a
collection
of
combinational
logic
and
registers
The
registers
contain
the
state
of
the
system
which
changes
only
at
the
clock
edge
so
we
say
the
state
is
synchronized
to
the
clock
If
the
clock
is
sufficiently
slow
so
that
the
inputs
to
all
registers
settle
before
the
next
clock
edge
all
races
are
eliminated
Adopting
this
discipline
of
always
using
registers
in
the
feedback
path
leads
us
to
the
formal
definition
of
a
synchronous
sequential
circuit
Recall
that
a
circuit
is
defined
by
its
input
and
output
terminals
and
its
functional
and
timing
specifications
A
sequential
circuit
has
a
finite
set
of
discrete
states
S
S
Sk
A
synchronous
sequential
circuit
has
a
clock
input
whose
rising
edges
indicate
a
sequence
of
times
at
which
state
transitions
occur
We
often
use
the
terms
current
state
and
next
state
to
distinguish
the
state
of
the
system
at
the
present
from
the
state
to
which
it
will
enter
on
the
next
clock
edge
The
functional
specification
details
the
next
state
and
the
value
of
each
output
for
each
possible
combination
of
current
state
and
input
values
The
timing
specification
consists
of
an
upper
bound
tpcq
and
a
lower
bound
tccq
on
the
time
from
the
rising
edge
of
the
clock
until
the
output
changes
as
well
as
setup
and
hold
times
tsetup
and
thold
that
indicate
when
the
inputs
must
be
stable
relative
to
the
rising
edge
of
the
clock
The
rules
of
synchronous
sequential
circuit
composition
teach
us
that
a
circuit
is
a
synchronous
sequential
circuit
if
it
consists
of
interconnected
circuit
elements
such
that
Every
circuit
element
is
either
a
register
or
a
combinational
circuit
At
least
one
circuit
element
is
a
register
All
registers
receive
the
same
clock
signal
Every
cyclic
path
contains
at
least
one
register
Sequential
circuits
that
are
not
synchronous
are
called
asynchronous
A
flip
flop
is
the
simplest
synchronous
sequential
circuit
It
has
one
input
D
one
clock
CLK
one
output
Q
and
two
states
The
functional
specification
for
a
flip
flop
is
that
the
next
state
is
D
and
that
the
output
Q
is
the
current
state
as
shown
in
Figure
We
often
call
the
current
state
variable
S
and
the
next
state
variable
S
In
this
case
the
prime
after
S
indicates
next
state
not
inversion
The
timing
of
sequential
circuits
will
be
analyzed
in
Section
Two
other
common
types
of
synchronous
sequential
circuits
are
called
finite
state
machines
and
pipelines
These
will
be
covered
later
in
this
chapter
Synchronous
Logic
Design
tpcq
stands
for
the
time
of
propagation
from
clock
to
Q
where
Q
indicates
the
output
of
a
synchronous
sequential
circuit
tccq
stands
for
the
time
of
contamination
from
clock
to
Q
These
are
analogous
to
tpd
and
tcd
in
combinational
logic
D
Q
Next
State
Current
State
S
S
CLK
Figure
Flip
flop
current
state
and
next
state
This
definition
of
a
synchronous
sequential
circuit
is
sufficient
but
more
restrictive
than
necessary
For
example
in
high
performance
microprocessors
some
registers
may
receive
delayed
or
gated
clocks
to
squeeze
out
the
last
bit
of
performance
or
power
Similarly
some
microprocessors
use
latches
instead
of
registers
However
the
definition
is
adequate
for
all
of
the
synchronous
sequential
circuits
covered
in
this
book
and
for
most
commercial
digital
systems
Chapt
Example
SYNCHRONOUS
SEQUENTIAL
CIRCUITS
Which
of
the
circuits
in
Figure
are
synchronous
sequential
circuits
Solution
Circuit
a
is
combinational
not
sequential
because
it
has
no
registers
b
is
a
simple
sequential
circuit
with
no
feedback
c
is
neither
a
combinational
circuit
nor
a
synchronous
sequential
circuit
because
it
has
a
latch
that
is
neither
a
register
nor
a
combinational
circuit
d
and
e
are
synchronous
sequential
logic
they
are
two
forms
of
finite
state
machines
which
are
discussed
in
Section
f
is
neither
combinational
nor
synchronous
sequential
because
it
has
a
cyclic
path
from
the
output
of
the
combinational
logic
back
to
the
input
of
the
same
logic
but
no
register
in
the
path
g
is
synchronous
sequential
logic
in
the
form
of
a
pipeline
which
we
will
study
in
Section
h
is
not
strictly
speaking
a
synchronous
sequential
circuit
because
the
second
register
receives
a
different
clock
signal
than
the
first
delayed
by
two
inverter
delays
Synchronous
and
Asynchronous
Circuits
Asynchronous
design
in
theory
is
more
general
than
synchronous
design
because
the
timing
of
the
system
is
not
limited
by
clocked
registers
Just
as
analog
circuits
are
more
general
than
digital
circuits
because
analog
circuits
can
use
any
voltage
asynchronous
circuits
are
more
general
than
synchronous
circuits
because
they
can
use
any
kind
of
feedback
However
synchronous
circuits
have
proved
to
be
easier
to
design
and
use
than
asynchronous
circuits
just
as
digital
are
easier
than
analog
circuits
Despite
decades
of
research
on
asynchronous
circuits
virtually
all
digital
systems
are
essentially
synchronous
CHAPTER
THREE
Sequential
Logic
Design
CL
CLK
CL
a
CL
b
CLK
CL
CLK
c
d
CL
CLK
CL
e
CL
f
CLK
g
CL
CLK
CL
CLK
CLK
CLK
h
CL
Figure
Example
circuits
Of
course
asynchronous
circuits
are
occasionally
necessary
when
communicating
between
systems
with
different
clocks
or
when
receiving
inputs
at
arbitrary
times
just
as
analog
circuits
are
necessary
when
communicating
with
the
real
world
of
continuous
voltages
Furthermore
research
in
asynchronous
circuits
continues
to
generate
interesting
insights
some
of
which
can
improve
synchronous
circuits
too
FINITE
STATE
MACHINES
Synchronous
sequential
circuits
can
be
drawn
in
the
forms
shown
in
Figure
These
forms
are
called
finite
state
machines
FSMs
They
get
their
name
because
a
circuit
with
k
registers
can
be
in
one
of
a
finite
number
k
of
unique
states
An
FSM
has
M
inputs
N
outputs
and
k
bits
of
state
It
also
receives
a
clock
and
optionally
a
reset
signal
An
FSM
consists
of
two
blocks
of
combinational
logic
next
state
logic
and
output
logic
and
a
register
that
stores
the
state
On
each
clock
edge
the
FSM
advances
to
the
next
state
which
was
computed
based
on
the
current
state
and
inputs
There
are
two
general
classes
of
finite
state
machines
characterized
by
their
functional
specifications
In
Moore
machines
the
outputs
depend
only
on
the
current
state
of
the
machine
In
Mealy
machines
the
outputs
depend
on
both
the
current
state
and
the
current
inputs
Finite
state
machines
provide
a
systematic
way
to
design
synchronous
sequential
circuits
given
a
functional
specification
This
method
will
be
explained
in
the
remainder
of
this
section
starting
with
an
example
FSM
Design
Example
To
illustrate
the
design
of
FSMs
consider
the
problem
of
inventing
a
controller
for
a
traffic
light
at
a
busy
intersection
on
campus
Engineering
students
are
moseying
between
their
dorms
and
the
labs
on
Academic
Ave
They
are
busy
reading
about
FSMs
in
their
favorite
Finite
State
Machines
CLK
M
next
k
N
state
logic
output
logic
a
inputs
outputs
state
next
state
k
b
CLK
M
next
k
N
state
logic
output
logic
inputs
outputs
state
next
state
k
Figure
Finite
state
machines
a
Moore
machine
b
Mealy
machine
Moore
and
Mealy
machines
are
named
after
their
promoters
researchers
who
developed
automata
theory
the
mathematical
underpinnings
of
state
machines
at
Bell
Labs
Edward
F
Moore
not
to
be
confused
with
Intel
founder
Gordon
Moore
published
his
seminal
article
Gedankenexperiments
on
Sequential
Machines
in
He
subsequently
became
a
professor
of
mathematics
and
computer
science
at
the
University
of
Wisconsin
George
H
Mealy
published
A
Method
of
Synthesizing
Sequential
Circuits
in
He
subsequently
wrote
the
first
Bell
Labs
operating
system
for
the
IBM
computer
He
later
joined
Harvard
University
textbook
and
aren
t
looking
where
they
are
going
Football
players
are
hustling
between
the
athletic
fields
and
the
dining
hall
on
Bravado
Boulevard
They
are
tossing
the
ball
back
and
forth
and
aren
t
looking
where
they
are
going
either
Several
serious
injuries
have
already
occurred
at
the
intersection
of
these
two
roads
and
the
Dean
of
Students
asks
Ben
Bitdiddle
to
install
a
traffic
light
before
there
are
fatalities
Ben
decides
to
solve
the
problem
with
an
FSM
He
installs
two
traffic
sensors
TA
and
TB
on
Academic
Ave
and
Bravado
Blvd
respectively
Each
sensor
indicates
TRUE
if
students
are
present
and
FALSE
if
the
street
is
empty
He
also
installs
two
traffic
lights
LA
and
LB
to
control
traffic
Each
light
receives
digital
inputs
specifying
whether
it
should
be
green
yellow
or
red
Hence
his
FSM
has
two
inputs
TA
and
TB
and
two
outputs
LA
and
LB
The
intersection
with
lights
and
sensors
is
shown
in
Figure
Ben
provides
a
clock
with
a
second
period
On
each
clock
tick
rising
edge
the
lights
may
change
based
on
the
traffic
sensors
He
also
provides
a
reset
button
so
that
Physical
Plant
technicians
can
put
the
controller
in
a
known
initial
state
when
they
turn
it
on
Figure
shows
a
black
box
view
of
the
state
machine
Ben
s
next
step
is
to
sketch
the
state
transition
diagram
shown
in
Figure
to
indicate
all
the
possible
states
of
the
system
and
the
transitions
between
these
states
When
the
system
is
reset
the
lights
are
green
on
Academic
Ave
and
red
on
Bravado
Blvd
Every
seconds
the
controller
examines
the
traffic
pattern
and
decides
what
to
do
next
As
long
CHAPTER
THREE
Sequential
Logic
Design
TA
TB
LA
LB
CLK
Reset
Traffic
Light
Controller
Figure
Black
box
view
of
finite
state
machine
TA
LA
TA
LB
TB
TB
LA
LB
Academic
Ave
Bravado
Blvd
Dorms
Fields
Athletic
Dining
Hall
Labs
Figure
Campus
map
as
traffic
is
present
on
Academic
Ave
the
lights
do
not
change
When
there
is
no
longer
traffic
on
Academic
Ave
the
light
on
Academic
Ave
becomes
yellow
for
seconds
before
it
turns
red
and
Bravado
Blvd
s
light
turns
green
Similarly
the
Bravado
Blvd
light
remains
green
as
long
as
traffic
is
present
on
the
boulevard
then
turns
yellow
and
eventually
red
In
a
state
transition
diagram
circles
represent
states
and
arcs
represent
transitions
between
states
The
transitions
take
place
on
the
rising
edge
of
the
clock
we
do
not
bother
to
show
the
clock
on
the
diagram
because
it
is
always
present
in
a
synchronous
sequential
circuit
Moreover
the
clock
simply
controls
when
the
transitions
should
occur
whereas
the
diagram
indicates
which
transitions
occur
The
arc
labeled
Reset
pointing
from
outer
space
into
state
S
indicates
that
the
system
should
enter
that
state
upon
reset
regardless
of
what
previous
state
it
was
in
If
a
state
has
multiple
arcs
leaving
it
the
arcs
are
labeled
to
show
what
input
triggers
each
transition
For
example
when
in
state
S
the
system
will
remain
in
that
state
if
TA
is
TRUE
and
move
to
S
if
TA
is
FALSE
If
a
state
has
a
single
arc
leaving
it
that
transition
always
occurs
regardless
of
the
inputs
For
example
when
in
state
S
the
system
will
always
move
to
S
The
value
that
the
outputs
have
while
in
a
particular
state
are
indicated
in
the
state
For
example
while
in
state
S
LA
is
red
and
LB
is
green
Ben
rewrites
the
state
transition
diagram
as
a
state
transition
table
Table
which
indicates
for
each
state
and
input
what
the
next
state
S
should
be
Note
that
the
table
uses
don
t
care
symbols
X
whenever
the
next
state
does
not
depend
on
a
particular
input
Also
note
that
Reset
is
omitted
from
the
table
Instead
we
use
resettable
flip
flops
that
always
go
to
state
S
on
reset
independent
of
the
inputs
The
state
transition
diagram
is
abstract
in
that
it
uses
states
labeled
S
S
S
S
and
outputs
labeled
red
yellow
green
To
build
a
real
circuit
the
states
and
outputs
must
be
assigned
binary
encodings
Ben
chooses
the
simple
encodings
given
in
Tables
and
Each
state
and
each
output
is
encoded
with
two
bits
S
LA
and
LB
Finite
State
Machines
S
LA
green
LB
red
LA
red
LB
yellow
LA
yellow
LB
red
LA
red
LB
green
S
S
S
TA
TA
TB
TB
Reset
Figure
State
transition
diagram
Ben
updates
the
state
transition
table
to
use
these
binary
encodings
as
shown
in
Table
The
revised
state
transition
table
is
a
truth
table
specifying
the
next
state
logic
It
defines
next
state
S
as
a
function
of
the
current
state
S
and
the
inputs
The
revised
output
table
is
a
truth
table
specifying
the
output
logic
It
defines
the
outputs
LA
and
LB
as
functions
of
the
current
state
S
From
this
table
it
is
straightforward
to
read
off
the
Boolean
equations
for
the
next
state
in
sum
of
products
form
The
equations
can
be
simplified
using
Karnaugh
maps
but
often
doing
it
by
inspection
is
easier
For
example
the
TB
and
terms
in
the
S
equation
are
clearly
redundant
Thus
S
reduces
to
an
XOR
operation
Equation
gives
the
next
state
equations
TB
S
S
S
TA
S
S
TB
S
S
S
S
S
TB
S
S
TB
CHAPTER
THREE
Sequential
Logic
Design
Table
Output
encoding
Output
Encoding
L
green
yellow
red
Table
State
transition
table
with
binary
encodings
Current
State
Inputs
Next
State
S
S
TA
TB
S
S
X
X
XX
X
X
XX
Table
State
transition
table
Current
Inputs
Next
State
State
S
TA
TB
S
S
X
S
S
X
S
S
X
X
S
S
X
S
S
X
S
S
X
X
S
Table
State
encoding
State
Encoding
S
S
S
S
S
Ch
Similarly
Ben
writes
an
output
table
Table
indicating
for
each
state
what
the
output
should
be
in
that
state
Again
it
is
straightforward
to
read
off
and
simplify
the
Boolean
equations
for
the
outputs
For
example
observe
that
LA
is
TRUE
only
on
the
rows
where
S
is
TRUE
Finally
Ben
sketches
his
Moore
FSM
in
the
form
of
Figure
a
First
he
draws
the
bit
state
register
as
shown
in
Figure
a
On
each
clock
edge
the
state
register
copies
the
next
state
S
to
become
the
state
S
The
state
register
receives
a
synchronous
or
asynchronous
reset
to
initialize
the
FSM
at
startup
Then
he
draws
the
next
state
logic
based
on
Equation
which
computes
the
next
state
based
on
the
current
state
and
inputs
as
shown
in
Figure
b
Finally
he
draws
the
output
logic
based
on
Equation
which
computes
the
outputs
based
on
the
current
state
as
shown
in
Figure
c
Figure
shows
a
timing
diagram
illustrating
the
traffic
light
controller
going
through
a
sequence
of
states
The
diagram
shows
CLK
Reset
the
inputs
TA
and
TB
next
state
S
state
S
and
outputs
LA
and
LB
Arrows
indicate
causality
for
example
changing
the
state
causes
the
outputs
to
change
and
changing
the
inputs
causes
the
next
state
to
change
Dashed
lines
indicate
the
rising
edge
of
CLK
when
the
state
changes
The
clock
has
a
second
period
so
the
traffic
lights
change
at
most
once
every
seconds
When
the
finite
state
machine
is
first
turned
on
its
state
is
unknown
as
indicated
by
the
question
marks
Therefore
the
system
should
be
reset
to
put
it
into
a
known
state
In
this
timing
diagram
LB
S
S
LB
S
LA
S
S
LA
S
S
S
S
TA
S
S
TB
S
S
S
Finite
State
Machines
Table
Output
table
Current
State
Outputs
S
S
LA
LA
LB
LB
Chapte
CHAPTER
THREE
Sequential
Logic
Design
a
S
S
S
S
CLK
state
register
Reset
r
S
S
S
S
CLK
next
state
logic
state
register
Reset
TA
TB
inputs
b
S
S
r
S
S
S
S
CLK
next
state
logic
output
logic
state
register
Reset
LA
LB
LB
LA
TA
TB
inputs
outputs
c
S
S
r
Figure
State
machine
circuit
for
traffic
light
controller
CLK
Reset
TA
TB
S
S
LA
LB
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
S
S
S
S
t
sec
S
S
S
S
S
S
Green
Red
S
Yellow
Red
Green
Green
Yellow
Red
Figure
Timing
diagram
for
traffic
light
controller
This
schematic
uses
some
AND
gates
with
bubbles
on
the
inputs
They
might
be
constructed
with
AND
gates
and
input
inverters
with
NOR
gates
and
inverters
for
the
non
bubbled
inputs
or
with
some
other
combination
of
gates
The
best
choice
depends
on
the
particular
implementation
technology
S
immediately
resets
to
S
indicating
that
asynchronously
resettable
flipflops
are
being
used
In
state
S
light
LA
is
green
and
light
LB
is
red
In
this
example
traffic
arrives
immediately
on
Academic
Ave
Therefore
the
controller
remains
in
state
S
keeping
LA
green
even
though
traffic
arrives
on
Bravado
Blvd
and
starts
waiting
After
seconds
the
traffic
on
Academic
Ave
has
all
passed
through
and
TA
falls
At
the
following
clock
edge
the
controller
moves
to
state
S
turning
LA
yellow
In
another
seconds
the
controller
proceeds
to
state
S
in
which
LA
turns
red
and
LB
turns
green
The
controller
waits
in
state
S
until
all
the
traffic
on
Bravado
Blvd
has
passed
through
It
then
proceeds
to
state
S
turning
LB
yellow
seconds
later
the
controller
enters
state
S
turning
LB
red
and
LA
green
The
process
repeats
State
Encodings
In
the
previous
example
the
state
and
output
encodings
were
selected
arbitrarily
A
different
choice
would
have
resulted
in
a
different
circuit
A
natural
question
is
how
to
determine
the
encoding
that
produces
the
circuit
with
the
fewest
logic
gates
or
the
shortest
propagation
delay
Unfortunately
there
is
no
simple
way
to
find
the
best
encoding
except
to
try
all
possibilities
which
is
infeasible
when
the
number
of
states
is
large
However
it
is
often
possible
to
choose
a
good
encoding
by
inspection
so
that
related
states
or
outputs
share
bits
Computer
aided
design
CAD
tools
are
also
good
at
searching
the
set
of
possible
encodings
and
selecting
a
reasonable
one
One
important
decision
in
state
encoding
is
the
choice
between
binary
encoding
and
one
hot
encoding
With
binary
encoding
as
was
used
in
the
traffic
light
controller
example
each
state
is
represented
as
a
binary
number
Because
K
binary
numbers
can
be
represented
by
log
K
bits
a
system
with
K
states
only
needs
log
K
bits
of
state
In
one
hot
encoding
a
separate
bit
of
state
is
used
for
each
state
It
is
called
one
hot
because
only
one
bit
is
hot
or
TRUE
at
any
time
For
example
a
one
hot
encoded
FSM
with
three
states
would
have
state
encodings
of
and
Each
bit
of
state
is
stored
in
a
flip
flop
so
onehot
encoding
requires
more
flip
flops
than
binary
encoding
However
with
one
hot
encoding
the
next
state
and
output
logic
is
often
simpler
so
fewer
gates
are
required
The
best
encoding
choice
depends
on
the
specific
FSM
Example
FSM
STATE
ENCODING
A
divide
by
N
counter
has
one
output
and
no
inputs
The
output
Y
is
HIGH
for
one
clock
cycle
out
of
every
N
In
other
words
the
output
divides
the
frequency
of
the
clock
by
N
The
waveform
and
state
transition
diagram
for
a
divide
by
counter
is
shown
in
Figure
Sketch
circuit
designs
for
such
a
counter
using
binary
and
one
hot
state
encodings
Finite
State
Machines
Despite
Ben
s
best
efforts
students
don
t
pay
attention
to
traffic
lights
and
collisions
continue
to
occur
The
Dean
of
Students
next
asks
him
to
design
a
catapult
to
throw
engineering
students
directly
from
their
dorm
roofs
through
the
open
windows
of
the
lab
bypassing
the
troublesome
intersection
all
together
But
that
is
the
subject
of
another
textbook
Solution
Tables
and
show
the
abstract
state
transition
and
output
tables
before
encoding
Table
compares
binary
and
one
hot
encodings
for
the
three
states
The
binary
encoding
uses
two
bits
of
state
Using
this
encoding
the
state
transition
table
is
shown
in
Table
Note
that
there
are
no
inputs
the
next
state
depends
only
on
the
current
state
The
output
table
is
left
as
an
exercise
to
the
reader
The
next
state
and
output
equations
are
The
one
hot
encoding
uses
three
bits
of
state
The
state
transition
table
for
this
encoding
is
shown
in
Table
and
the
output
table
is
again
left
as
an
exercise
to
the
reader
The
next
state
and
output
equations
are
as
follows
Figure
shows
schematics
for
each
of
these
designs
Note
that
the
hardware
for
the
binary
encoded
design
could
be
optimized
to
share
the
same
gate
for
Y
and
S
Also
observe
that
the
one
hot
encoding
requires
both
settable
s
and
resettable
r
flip
flops
to
initialize
the
machine
to
S
on
reset
The
best
implementation
choice
depends
on
the
relative
cost
of
gates
and
flip
flops
but
the
one
hot
design
is
usually
preferable
for
this
specific
example
A
related
encoding
is
the
one
cold
encoding
in
which
K
states
are
represented
with
K
bits
exactly
one
of
which
is
FALSE
Y
S
S
S
S
S
S
S
Y
S
S
S
S
S
S
S
S
CHAPTER
THREE
Sequential
Logic
Design
CLK
Y
a
S
Y
S
Y
S
Y
Reset
b
Figure
Divide
by
counter
a
waveform
and
b
state
transition
diagram
Table
Divide
by
counter
state
transition
table
Current
Next
State
State
S
S
S
S
S
S
Table
Divide
by
counter
output
table
Current
State
Output
S
S
S
Chapter
Finite
State
Machines
Table
Binary
and
one
hot
encodings
for
divide
by
counter
State
Binary
Encoding
One
Hot
Encoding
S
S
S
S
S
S
S
S
Table
State
transition
table
with
binary
encoding
Current
State
Next
State
S
S
S
S
Table
State
transition
table
with
one
hot
encoding
Current
State
Next
State
S
S
S
S
S
S
S
S
S
S
CLK
Reset
Y
next
state
logic
state
register
output
logic
output
a
Reset
CLK
rrs
S
S
S
Y
b
S
S
r
Figure
Divide
by
circuits
for
a
binary
and
b
one
hot
encodings
Moore
and
Mealy
Machines
So
far
we
have
shown
examples
of
Moore
machines
in
which
the
output
depends
only
on
the
state
of
the
system
Hence
in
state
transition
diagrams
for
Moore
machines
the
outputs
are
labeled
in
the
circles
Recall
that
Mealy
machines
are
much
like
Moore
machines
but
the
outputs
can
depend
on
inputs
as
well
as
the
current
state
Hence
in
state
transition
diagrams
for
Mealy
machines
the
outputs
are
labeled
on
the
arcs
instead
of
in
the
circles
The
block
of
combinational
logic
that
computes
the
outputs
uses
the
current
state
and
inputs
as
was
shown
in
Figure
b
Example
MOORE
VERSUS
MEALY
MACHINES
Alyssa
P
Hacker
owns
a
pet
robotic
snail
with
an
FSM
brain
The
snail
crawls
from
left
to
right
along
a
paper
tape
containing
a
sequence
of
s
and
s
On
each
clock
cycle
the
snail
crawls
to
the
next
bit
The
snail
smiles
when
the
last
four
bits
that
it
has
crawled
over
are
from
left
to
right
Design
the
FSM
to
compute
when
the
snail
should
smile
The
input
A
is
the
bit
underneath
the
snail
s
antennae
The
output
Y
is
TRUE
when
the
snail
smiles
Compare
Moore
and
Mealy
state
machine
designs
Sketch
a
timing
diagram
for
each
machine
showing
the
input
states
and
output
as
your
snail
crawls
along
the
sequence
Solution
The
Moore
machine
requires
five
states
as
shown
in
Figure
a
Convince
yourself
that
the
state
transition
diagram
is
correct
In
particular
why
is
there
an
arc
from
S
to
S
when
the
input
is
In
comparison
the
Mealy
machine
requires
only
four
states
as
shown
in
Figure
b
Each
arc
is
labeled
as
A
Y
A
is
the
value
of
the
input
that
causes
that
transition
and
Y
is
the
corresponding
output
Tables
and
show
the
state
transition
and
output
tables
for
the
Moore
machine
The
Moore
machine
requires
at
least
three
bits
of
state
Consider
using
a
binary
state
encoding
S
S
S
S
and
S
Tables
and
rewrite
the
state
transition
and
output
tables
with
these
encodings
These
four
tables
follow
on
page
From
these
tables
we
find
the
next
state
and
output
equations
by
inspection
Note
that
these
equations
are
simplified
using
the
fact
that
states
and
do
not
exist
Thus
the
corresponding
next
state
and
output
for
the
nonexistent
states
are
don
t
cares
not
shown
in
the
tables
We
use
the
don
t
cares
to
minimize
our
equations
S
S
S
S
A
S
S
A
S
S
S
A
S
S
S
A
S
S
S
A
CHAPTER
THREE
Sequential
Logic
Design
An
easy
way
to
remember
the
difference
between
the
two
types
of
finite
state
machines
is
that
a
Moore
machine
typically
has
more
states
than
a
Mealy
machine
for
a
given
problem
Chapter
Table
shows
the
combined
state
transition
and
output
table
for
the
Mealy
machine
The
Mealy
machine
requires
at
least
two
bits
of
state
Consider
using
a
binary
state
encoding
S
S
S
and
S
Table
rewrites
the
state
transition
and
output
table
with
these
encodings
From
these
tables
we
find
the
next
state
and
output
equations
by
inspection
The
Moore
and
Mealy
machine
schematics
are
shown
in
Figure
a
and
b
respectively
The
timing
diagrams
for
the
Moore
and
Mealy
machines
are
shown
in
Figure
see
page
The
two
machines
follow
a
different
sequence
of
states
Moreover
the
Mealy
machine
s
output
rises
a
cycle
sooner
because
it
responds
to
the
input
rather
than
waiting
for
the
state
change
If
the
Mealy
output
were
delayed
through
a
flip
flop
it
would
match
the
Moore
output
When
choosing
your
FSM
design
style
consider
when
you
want
your
outputs
to
respond
Y
S
S
A
S
S
S
A
S
S
A
S
S
A
S
S
S
S
S
A
Y
S
Finite
State
Machines
Reset
a
S
S
S
S
S
Reset
b
S
S
S
S
Figure
FSM
state
transition
diagrams
a
Moore
machine
b
Mealy
machine
Chapter
CHAPTER
THREE
Sequential
Logic
Design
Table
Moore
output
table
with
state
encodings
Current
State
Output
S
S
S
Y
Table
Moore
state
transition
table
with
state
encodings
Current
State
Input
Next
State
S
S
S
A
S
S
S
Table
Moore
state
transition
table
Current
State
Input
Next
State
S
AS
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
Table
Moore
output
table
Current
Output
State
S
Y
S
S
S
S
S
Factoring
State
Machines
Designing
complex
FSMs
is
often
easier
if
they
can
be
broken
down
into
multiple
interacting
simpler
state
machines
such
that
the
output
of
some
machines
is
the
input
of
others
This
application
of
hierarchy
and
modularity
is
called
factoring
of
state
machines
Finite
State
Machines
Table
Mealy
state
transition
and
output
table
Current
State
Input
Next
State
Output
S
AS
Y
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
Table
Mealy
state
transition
and
output
table
with
state
encodings
Current
State
Input
Next
State
Output
S
S
A
S
S
Y
CHAPTER
THREE
Sequential
Logic
Design
Example
UNFACTORED
AND
FACTORED
STATE
MACHINES
Modify
the
traffic
light
controller
from
Section
to
have
a
parade
mode
which
keeps
the
Bravado
Boulevard
light
green
while
spectators
and
the
band
march
to
football
games
in
scattered
groups
The
controller
receives
two
more
inputs
P
and
R
Asserting
P
for
at
least
one
cycle
enters
parade
mode
Asserting
R
for
at
least
one
cycle
leaves
parade
mode
When
in
parade
mode
the
controller
proceeds
through
its
usual
sequence
until
LB
turns
green
then
remains
in
that
state
with
LB
green
until
parade
mode
ends
First
sketch
a
state
transition
diagram
for
a
single
FSM
as
shown
in
Figure
a
Then
sketch
the
state
transition
diagrams
for
two
interacting
FSMs
as
S
S
S
S
S
S
Y
CLK
Reset
A
a
S
S
S
b
S
S
CLK
Reset
S
S
A
Y
S
S
Figure
FSM
schematics
for
a
Moore
and
b
Mealy
machines
Finite
State
Machines
Mealy
Machine
Moore
Machine
CLK
Reset
A
S
Y
S
Y
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
Figure
Timing
diagrams
for
Moore
and
Mealy
machines
Controller
TA
FSM
a
TB
LA
LB
b
Mode
FSM
Lights
FSM
P
M
Controller
FSM
LA
LB
TA
TB
R
P
R
Figure
a
single
and
b
factored
designs
for
modified
traffic
light
controller
FSM
shown
in
Figure
b
The
Mode
FSM
asserts
the
output
M
when
it
is
in
parade
mode
The
Lights
FSM
controls
the
lights
based
on
M
and
the
traffic
sensors
TA
and
TB
Solution
Figure
a
shows
the
single
FSM
design
States
S
to
S
handle
normal
mode
States
S
to
S
handle
parade
mode
The
two
halves
of
the
diagram
are
almost
identical
but
in
parade
mode
the
FSM
remains
in
S
with
a
green
light
on
Bravado
Blvd
The
P
and
R
inputs
control
movement
between
these
two
halves
The
FSM
is
messy
and
tedious
to
design
Figure
b
shows
the
factored
FSM
design
The
mode
FSM
has
two
states
to
track
whether
the
lights
are
in
normal
or
parade
mode
The
Lights
FSM
is
modified
to
remain
in
S
while
M
is
TRUE
FSM
Review
Finite
state
machines
are
a
powerful
way
to
systematically
design
sequential
circuits
from
a
written
specification
Use
the
following
procedure
to
design
an
FSM
Identify
the
inputs
and
outputs
Sketch
a
state
transition
diagram
CHAPTER
THREE
Sequential
Logic
Design
S
LA
green
LB
red
LA
green
LB
red
LA
yellow
LB
red
LA
yellow
LB
red
LA
red
LB
green
LA
red
LB
green
LA
red
LB
yellow
LA
red
LB
yellow
S
S
S
TA
TA
TB
TB
Reset
S
S
S
S
TA
TA
P
P
P
P
P
P
R
R
R
R
R
P
R
P
PTA
PTA
P
RTA
RTA
R
RTB
RTB
a
LA
green
LB
red
LA
yellow
LB
red
LA
red
LB
green
LA
red
LB
yellow
S
S
S
S
TA
TA
M
TB
MTB
Reset
Lights
FSM
b
S
M
S
M
P
Reset
P
Mode
FSM
R
R
Figure
State
transition
diagrams
a
unfactored
b
factored
Ch
For
a
Moore
machine
Write
a
state
transition
table
Write
an
output
table
For
a
Mealy
machine
Write
a
combined
state
transition
and
output
table
Select
state
encodings
your
selection
affects
the
hardware
design
Write
Boolean
equations
for
the
next
state
and
output
logic
Sketch
the
circuit
schematic
We
will
repeatedly
use
FSMs
to
design
complex
digital
systems
throughout
this
book
TIMING
OF
SEQUENTIAL
LOGIC
Recall
that
a
flip
flop
copies
the
input
D
to
the
output
Q
on
the
rising
edge
of
the
clock
This
process
is
called
sampling
D
on
the
clock
edge
If
D
is
stable
at
either
or
when
the
clock
rises
this
behavior
is
clearly
defined
But
what
happens
if
D
is
changing
at
the
same
time
the
clock
rises
This
problem
is
similar
to
that
faced
by
a
camera
when
snapping
a
picture
Imagine
photographing
a
frog
jumping
from
a
lily
pad
into
the
lake
If
you
take
the
picture
before
the
jump
you
will
see
a
frog
on
a
lily
pad
If
you
take
the
picture
after
the
jump
you
will
see
ripples
in
the
water
But
if
you
take
it
just
as
the
frog
jumps
you
may
see
a
blurred
image
of
the
frog
stretching
from
the
lily
pad
into
the
water
A
camera
is
characterized
by
its
aperture
time
during
which
the
object
must
remain
still
for
a
sharp
image
to
be
captured
Similarly
a
sequential
element
has
an
aperture
time
around
the
clock
edge
during
which
the
input
must
be
stable
for
the
flip
flop
to
produce
a
well
defined
output
The
aperture
of
a
sequential
element
is
defined
by
a
setup
time
and
a
hold
time
before
and
after
the
clock
edge
respectively
Just
as
the
static
discipline
limited
us
to
using
logic
levels
outside
the
forbidden
zone
the
dynamic
discipline
limits
us
to
using
signals
that
change
outside
the
aperture
time
By
taking
advantage
of
the
dynamic
discipline
we
can
think
of
time
in
discrete
units
called
clock
cycles
just
as
we
think
of
signal
levels
as
discrete
s
and
s
A
signal
may
glitch
and
oscillate
wildly
for
some
bounded
amount
of
time
Under
the
dynamic
discipline
we
are
concerned
only
about
its
final
value
at
the
end
of
the
clock
cycle
after
it
has
settled
to
a
stable
value
Hence
we
can
simply
write
A
n
the
value
of
signal
A
at
the
end
of
the
nth
clock
cycle
where
n
is
an
integer
rather
than
A
t
the
value
of
A
at
some
instant
t
where
t
is
any
real
number
The
clock
period
has
to
be
long
enough
for
all
signals
to
settle
This
sets
a
limit
on
the
speed
of
the
system
In
real
systems
the
clock
does
not
Timing
of
Sequential
Logic
Chap
reach
all
flip
flops
at
precisely
the
same
time
This
variation
in
time
called
clock
skew
further
increases
the
necessary
clock
period
Sometimes
it
is
impossible
to
satisfy
the
dynamic
discipline
especially
when
interfacing
with
the
real
world
For
example
consider
a
circuit
with
an
input
coming
from
a
button
A
monkey
might
press
the
button
just
as
the
clock
rises
This
can
result
in
a
phenomenon
called
metastability
where
the
flip
flop
captures
a
value
partway
between
and
that
can
take
an
unlimited
amount
of
time
to
resolve
into
a
good
logic
value
The
solution
to
such
asynchronous
inputs
is
to
use
a
synchronizer
which
has
a
very
small
but
nonzero
probability
of
producing
an
illegal
logic
value
We
expand
on
all
of
these
ideas
in
the
rest
of
this
section
The
Dynamic
Discipline
So
far
we
have
focused
on
the
functional
specification
of
sequential
circuits
Recall
that
a
synchronous
sequential
circuit
such
as
a
flip
flop
or
FSM
also
has
a
timing
specification
as
illustrated
in
Figure
When
the
clock
rises
the
output
or
outputs
may
start
to
change
after
the
clock
to
Q
contamination
delay
tccq
and
must
definitely
settle
to
the
final
value
within
the
clock
to
Q
propagation
delay
tpcq
These
represent
the
fastest
and
slowest
delays
through
the
circuit
respectively
For
the
circuit
to
sample
its
input
correctly
the
input
or
inputs
must
have
stabilized
at
least
some
setup
time
tsetup
before
the
rising
edge
of
the
clock
and
must
remain
stable
for
at
least
some
hold
time
thold
after
the
rising
edge
of
the
clock
The
sum
of
the
setup
and
hold
times
is
called
the
aperture
time
of
the
circuit
because
it
is
the
total
time
for
which
the
input
must
remain
stable
The
dynamic
discipline
states
that
the
inputs
of
a
synchronous
sequential
circuit
must
be
stable
during
the
setup
and
hold
aperture
time
around
the
clock
edge
By
imposing
this
requirement
we
guarantee
that
the
flip
flops
sample
signals
while
they
are
not
changing
Because
we
are
concerned
only
about
the
final
values
of
the
inputs
at
the
time
they
are
sampled
we
can
treat
signals
as
discrete
in
time
as
well
as
in
logic
levels
CHAPTER
THREE
Sequential
Logic
Design
CLK
tccq
tpcq
tsetup
output
s
input
s
thold
Figure
Timing
specification
for
synchronous
sequential
circuit
System
Timing
The
clock
period
or
cycle
time
Tc
is
the
time
between
rising
edges
of
a
repetitive
clock
signal
Its
reciprocal
fc
Tc
is
the
clock
frequency
All
else
being
the
same
increasing
the
clock
frequency
increases
the
work
that
a
digital
system
can
accomplish
per
unit
time
Frequency
is
measured
in
units
of
Hertz
Hz
or
cycles
per
second
megahertz
MHz
Hz
and
gigahertz
GHz
Hz
Figure
a
illustrates
a
generic
path
in
a
synchronous
sequential
circuit
whose
clock
period
we
wish
to
calculate
On
the
rising
edge
of
the
clock
register
R
produces
output
or
outputs
Q
These
signals
enter
a
block
of
combinational
logic
producing
D
the
input
or
inputs
to
register
R
The
timing
diagram
in
Figure
b
shows
that
each
output
signal
may
start
to
change
a
contamination
delay
after
its
input
change
and
settles
to
the
final
value
within
a
propagation
delay
after
its
input
settles
The
gray
arrows
represent
the
contamination
delay
through
R
and
the
combinational
logic
and
the
blue
arrows
represent
the
propagation
delay
through
R
and
the
combinational
logic
We
analyze
the
timing
constraints
with
respect
to
the
setup
and
hold
time
of
the
second
register
R
Setup
Time
Constraint
Figure
is
the
timing
diagram
showing
only
the
maximum
delay
through
the
path
indicated
by
the
blue
arrows
To
satisfy
the
setup
time
of
R
D
must
settle
no
later
than
the
setup
time
before
the
next
clock
edge
Timing
of
Sequential
Logic
CL
CLK
CLK
R
R
Q
D
a
CLK
Q
D
b
Tc
Figure
Path
between
registers
and
timing
diagram
CLK
Q
D
Tc
tpcq
tpd
tsetup
CL
CLK
CLK
Q
D
R
R
Figure
Maximum
delay
for
setup
time
constraint
In
the
three
decades
from
when
one
of
the
authors
families
bought
an
Apple
II
computer
to
the
present
time
of
writing
microprocessor
clock
frequencies
have
increased
from
MHz
to
several
GHz
a
factor
of
more
than
This
speedup
partially
explains
the
revolutionary
changes
computers
have
made
in
society
Cha
Hence
we
find
an
equation
for
the
minimum
clock
period
In
commercial
designs
the
clock
period
is
often
dictated
by
the
Director
of
Engineering
or
by
the
marketing
department
to
ensure
a
competitive
product
Moreover
the
flip
flop
clock
to
Q
propagation
delay
and
setup
time
tpcq
and
tsetup
are
specified
by
the
manufacturer
Hence
we
rearrange
Equation
to
solve
for
the
maximum
propagation
delay
through
the
combinational
logic
which
is
usually
the
only
variable
under
the
control
of
the
individual
designer
The
term
in
parentheses
tpcq
tsetup
is
called
the
sequencing
overhead
Ideally
the
entire
cycle
time
Tc
would
be
available
for
useful
computation
in
the
combinational
logic
tpd
However
the
sequencing
overhead
of
the
flip
flop
cuts
into
this
time
Equation
is
called
the
setup
time
constraint
or
max
delay
constraint
because
it
depends
on
the
setup
time
and
limits
the
maximum
delay
through
combinational
logic
If
the
propagation
delay
through
the
combinational
logic
is
too
great
D
may
not
have
settled
to
its
final
value
by
the
time
R
needs
it
to
be
stable
and
samples
it
Hence
R
may
sample
an
incorrect
result
or
even
an
illegal
logic
level
a
level
in
the
forbidden
region
In
such
a
case
the
circuit
will
malfunction
The
problem
can
be
solved
by
increasing
the
clock
period
or
by
redesigning
the
combinational
logic
to
have
a
shorter
propagation
delay
Hold
Time
Constraint
The
register
R
in
Figure
a
also
has
a
hold
time
constraint
Its
input
D
must
not
change
until
some
time
thold
after
the
rising
edge
of
the
clock
According
to
Figure
D
might
change
as
soon
as
tccq
tcd
after
the
rising
edge
of
the
clock
tpd
Tc
tpcq
tsetup
Tc
tpcq
tpd
tsetup
CHAPTER
THREE
Sequential
Logic
Design
CLK
Q
D
tccq
tcd
thold
CL
CLK
CLK
Q
D
R
R
Figure
Minimum
delay
for
hold
time
constraint
C
Hence
we
find
Again
tccq
and
thold
are
characteristics
of
the
flip
flop
that
are
usually
outside
the
designer
s
control
Rearranging
we
can
solve
for
the
minimum
contamination
delay
through
the
combinational
logic
Equation
is
also
called
the
min
delay
constraint
because
it
limits
the
minimum
delay
through
combinational
logic
We
have
assumed
that
any
logic
elements
can
be
connected
to
each
other
without
introducing
timing
problems
In
particular
we
would
expect
that
two
flip
flops
may
be
directly
cascaded
as
in
Figure
without
causing
hold
time
problems
In
such
a
case
tcd
because
there
is
no
combinational
logic
between
flip
flops
Substituting
into
Equation
yields
the
requirement
that
In
other
words
a
reliable
flip
flop
must
have
a
hold
time
shorter
than
its
contamination
delay
Often
flip
flops
are
designed
with
thold
so
that
Equation
is
always
satisfied
Unless
noted
otherwise
we
will
usually
make
that
assumption
and
ignore
the
hold
time
constraint
in
this
book
Nevertheless
hold
time
constraints
are
critically
important
If
they
are
violated
the
only
solution
is
to
increase
the
contamination
delay
through
the
logic
which
requires
redesigning
the
circuit
Unlike
setup
time
constraints
they
cannot
be
fixed
by
adjusting
the
clock
period
Redesigning
an
integrated
circuit
and
manufacturing
the
corrected
design
takes
months
and
millions
of
dollars
in
today
s
advanced
technologies
so
hold
time
violations
must
be
taken
extremely
seriously
Putting
It
All
Together
Sequential
circuits
have
setup
and
hold
time
constraints
that
dictate
the
maximum
and
minimum
delays
of
the
combinational
logic
between
flipflops
Modern
flip
flops
are
usually
designed
so
that
the
minimum
delay
through
the
combinational
logic
is
that
is
flip
flops
can
be
placed
back
to
back
The
maximum
delay
constraint
limits
the
number
of
consecutive
gates
on
the
critical
path
of
a
high
speed
circuit
because
a
high
clock
frequency
means
a
short
clock
period
Example
TIMING
ANALYSIS
Ben
Bitdiddle
designed
the
circuit
in
Figure
According
to
the
data
sheets
for
the
components
he
is
using
flip
flops
have
a
clock
to
Q
contamination
delay
thold
tccq
tcd
thold
tccq
tccq
tcd
thold
Timing
of
Sequential
Logic
CLK
Figure
Back
to
back
flip
flops
Cha
of
ps
and
a
propagation
delay
of
ps
They
have
a
setup
time
of
ps
and
a
hold
time
of
ps
Each
logic
gate
has
a
propagation
delay
of
ps
and
a
contamination
delay
of
ps
Help
Ben
determine
the
maximum
clock
frequency
and
whether
any
hold
time
violations
could
occur
This
process
is
called
timing
analysis
Solution
Figure
a
shows
waveforms
illustrating
when
the
signals
might
change
The
inputs
A
to
D
are
registered
so
they
change
shortly
after
CLK
rises
only
The
critical
path
occurs
when
B
C
D
and
A
rises
from
to
triggering
n
to
rise
X
to
rise
and
Y
to
fall
as
shown
in
Figure
b
This
path
involves
three
gate
delays
For
the
critical
path
we
assume
that
each
gate
requires
its
full
propagation
delay
Y
must
setup
before
the
next
rising
edge
of
the
CLK
Hence
the
minimum
cycle
time
is
The
maximum
clock
frequency
is
fc
Tc
GHz
A
short
path
occurs
when
A
and
C
rises
causing
X
to
rise
as
shown
in
Figure
c
For
the
short
path
we
assume
that
each
gate
switches
after
only
a
contamination
delay
This
path
involves
only
one
gate
delay
so
it
may
occur
after
tccq
tcd
ps
But
recall
that
the
flip
flop
has
a
hold
time
of
ps
meaning
that
X
must
remain
stable
for
ps
after
the
rising
edge
of
CLK
for
the
flip
flop
to
reliably
sample
its
value
In
this
case
X
at
the
first
rising
edge
of
CLK
so
we
want
the
flip
flop
to
capture
X
Because
X
did
not
hold
stable
long
enough
the
actual
value
of
X
is
unpredictable
The
circuit
has
a
hold
time
violation
and
may
behave
erratically
at
any
clock
frequency
Tc
tpcq
tpd
tsetup
ps
CHAPTER
THREE
Sequential
Logic
Design
CLK
CLK
A
B
C
D
n
X
Y
X
Y
Figure
Sample
circuit
for
timing
analysis
Chapter
q
Example
FIXING
HOLD
TIME
VIOLATIONS
Alyssa
P
Hacker
proposes
to
fix
Ben
s
circuit
by
adding
buffers
to
slow
down
the
short
paths
as
shown
in
Figure
The
buffers
have
the
same
delays
as
other
gates
Determine
the
maximum
clock
frequency
and
whether
any
hold
time
problems
could
occur
Solution
Figure
shows
waveforms
illustrating
when
the
signals
might
change
The
critical
path
from
A
to
Y
is
unaffected
because
it
does
not
pass
through
any
buffers
Therefore
the
maximum
clock
frequency
is
still
GHz
However
the
short
paths
are
slowed
by
the
contamination
delay
of
the
buffer
Now
X
will
not
change
until
tccq
tcd
ps
This
is
after
the
ps
hold
time
has
elapsed
so
the
circuit
now
operates
correctly
This
example
had
an
unusually
long
hold
time
to
illustrate
the
point
of
hold
time
problems
Most
flip
flops
are
designed
with
thold
tccq
to
avoid
such
problems
However
several
high
performance
microprocessors
including
the
Pentium
use
an
element
called
a
pulsed
latch
in
place
of
a
flip
flop
The
pulsed
latch
behaves
like
a
flip
flop
but
has
a
short
clock
to
Q
delay
and
a
long
hold
time
In
general
adding
buffers
can
usually
but
not
always
solve
hold
time
problems
without
slowing
the
critical
path
Timing
of
Sequential
Logic
A
D
CLK
n
X
Y
t
ps
a
A
n
X
Y
tpcq
tpd
tpd
tpd
tsetup
C
X
tccq
tcd
thold
b
c
Figure
Timing
diagram
a
general
case
b
critical
path
c
short
path
Ch
Clock
Skew
In
the
previous
analysis
we
assumed
that
the
clock
reaches
all
registers
at
exactly
the
same
time
In
reality
there
is
some
variation
in
this
time
This
variation
in
clock
edges
is
called
clock
skew
For
example
the
wires
from
the
clock
source
to
different
registers
may
be
of
different
lengths
resulting
in
slightly
different
delays
as
shown
in
Figure
Noise
also
results
in
different
delays
Clock
gating
described
in
Section
further
delays
the
clock
If
some
clocks
are
gated
and
others
are
not
there
will
be
substantial
skew
between
the
gated
and
ungated
clocks
In
CHAPTER
THREE
Sequential
Logic
Design
CLK
CLK
A
B
C
D
n
X
Y
X
Y
Buffers
added
to
fix
hold
time
violation
n
n
Figure
Corrected
circuit
to
fix
hold
time
problem
A
D
CLK
n
n
X
Y
t
ps
Figure
Timing
diagram
with
buffers
to
fix
hold
time
problem
t
skew
CLK
CLK
CL
CLK
CLK
R
R
Q
D
CLK
delay
CLK
Figure
Clock
skew
caused
by
wire
delay
Figure
CLK
is
early
with
respect
to
CLK
because
the
clock
wire
between
the
two
registers
follows
a
scenic
route
If
the
clock
had
been
routed
differently
CLK
might
have
been
early
instead
When
doing
timing
analysis
we
consider
the
worst
case
scenario
so
that
we
can
guarantee
that
the
circuit
will
work
under
all
circumstances
Figure
adds
skew
to
the
timing
diagram
from
Figure
The
heavy
clock
line
indicates
the
latest
time
at
which
the
clock
signal
might
reach
any
register
the
hashed
lines
show
that
the
clock
might
arrive
up
to
tskew
earlier
First
consider
the
setup
time
constraint
shown
in
Figure
In
the
worst
case
R
receives
the
latest
skewed
clock
and
R
receives
the
earliest
skewed
clock
leaving
as
little
time
as
possible
for
data
to
propagate
between
the
registers
The
data
propagates
through
the
register
and
combinational
logic
and
must
setup
before
R
samples
it
Hence
we
conclude
that
Next
consider
the
hold
time
constraint
shown
in
Figure
In
the
worst
case
R
receives
an
early
skewed
clock
CLK
and
R
receives
a
late
skewed
clock
CLK
The
data
zips
through
the
register
tpd
Tc
tpcq
tsetup
tskew
Tc
tpcq
tpd
tsetup
tskew
Timing
of
Sequential
Logic
CL
CLK
CLK
R
R
Q
D
a
CLK
Q
D
b
Tc
tskew
Figure
Timing
diagram
with
clock
skew
CLK
Q
D
Tc
tpcq
tpd
tsetuptskew
CL
CLK
CLK
R
R
Q
D
CLK
Figure
Setup
time
constraint
with
clock
skew
C
and
combinational
logic
but
must
not
arrive
until
a
hold
time
after
the
late
clock
Thus
we
find
that
In
summary
clock
skew
effectively
increases
both
the
setup
time
and
the
hold
time
It
adds
to
the
sequencing
overhead
reducing
the
time
available
for
useful
work
in
the
combinational
logic
It
also
increases
the
required
minimum
delay
through
the
combinational
logic
Even
if
thold
a
pair
of
back
to
back
flip
flops
will
violate
Equation
if
tskew
tccq
To
prevent
serious
hold
time
failures
designers
must
not
permit
too
much
clock
skew
Sometimes
flip
flops
are
intentionally
designed
to
be
particularly
slow
i
e
large
tccq
to
prevent
hold
time
problems
even
when
the
clock
skew
is
substantial
Example
TIMING
ANALYSIS
WITH
CLOCK
SKEW
Revisit
Example
and
assume
that
the
system
has
ps
of
clock
skew
Solution
The
critical
path
remains
the
same
but
the
setup
time
is
effectively
increased
by
the
skew
Hence
the
minimum
cycle
time
is
The
maximum
clock
frequency
is
fc
Tc
GHz
The
short
path
also
remains
the
same
at
ps
The
hold
time
is
effectively
increased
by
the
skew
to
ps
which
is
much
greater
than
ps
Hence
the
circuit
will
violate
the
hold
time
and
malfunction
at
any
frequency
The
circuit
violated
the
hold
time
constraint
even
without
skew
Skew
in
the
system
just
makes
the
violation
worse
ps
Tc
tpcq
tpd
tsetup
tskew
tcd
thold
tskew
tccq
tccq
tcd
thold
tskew
CHAPTER
THREE
Sequential
Logic
Design
tcd
thold
Q
D
tskew
CL
CLK
CLK
R
R
Q
D
CLK
CLK
tccq
Figure
Hold
time
constraint
with
clock
skew
Chapter
Example
FIXING
HOLD
TIME
VIOLATIONS
Revisit
Example
and
assume
that
the
system
has
ps
of
clock
skew
Solution
The
critical
path
is
unaffected
so
the
maximum
clock
frequency
remains
GHz
The
short
path
increases
to
ps
This
is
still
less
than
thold
tskew
ps
so
the
circuit
still
violates
its
hold
time
constraint
To
fix
the
problem
even
more
buffers
could
be
inserted
Buffers
would
need
to
be
added
on
the
critical
path
as
well
reducing
the
clock
frequency
Alternatively
a
better
flip
flop
with
a
shorter
hold
time
might
be
used
Metastability
As
noted
earlier
it
is
not
always
possible
to
guarantee
that
the
input
to
a
sequential
circuit
is
stable
during
the
aperture
time
especially
when
the
input
arrives
from
the
external
world
Consider
a
button
connected
to
the
input
of
a
flip
flop
as
shown
in
Figure
When
the
button
is
not
pressed
D
When
the
button
is
pressed
D
A
monkey
presses
the
button
at
some
random
time
relative
to
the
rising
edge
of
CLK
We
want
to
know
the
output
Q
after
the
rising
edge
of
CLK
In
Case
I
when
the
button
is
pressed
much
before
CLK
Q
In
Case
II
when
the
button
is
not
pressed
until
long
after
CLK
Q
But
in
Case
III
when
the
button
is
pressed
sometime
between
tsetup
before
CLK
and
thold
after
CLK
the
input
violates
the
dynamic
discipline
and
the
output
is
undefined
Metastable
State
In
reality
when
a
flip
flop
samples
an
input
that
is
changing
during
its
aperture
the
output
Q
may
momentarily
take
on
a
voltage
between
and
VDD
that
is
in
the
forbidden
zone
This
is
called
a
metastable
state
Eventually
the
flip
flop
will
resolve
the
output
to
a
stable
state
of
either
or
However
the
resolution
time
required
to
reach
the
stable
state
is
unbounded
The
metastable
state
of
a
flip
flop
is
analogous
to
a
ball
on
the
summit
of
a
hill
between
two
valleys
as
shown
in
Figure
The
two
valleys
are
stable
states
because
a
ball
in
the
valley
will
remain
there
as
long
as
it
is
not
disturbed
The
top
of
the
hill
is
called
metastable
because
the
ball
would
remain
there
if
it
were
perfectly
balanced
But
because
nothing
is
perfect
the
ball
will
eventually
roll
to
one
side
or
the
other
The
time
required
for
this
change
to
occur
depends
on
how
nearly
well
balanced
the
ball
originally
was
Every
bistable
device
has
a
metastable
state
between
the
two
stable
states
Timing
of
Sequential
Logic
D
Q
CLK
button
CLK
tsetup
thold
aperture
D
Q
D
Q
D
Q
Case
I
Case
II
Case
III
Figure
Input
changing
before
after
or
during
aperture
Chapt
Resolution
Time
If
a
flip
flop
input
changes
at
a
random
time
during
the
clock
cycle
the
resolution
time
tres
required
to
resolve
to
a
stable
state
is
also
a
random
variable
If
the
input
changes
outside
the
aperture
then
tres
tpcq
But
if
the
input
happens
to
change
within
the
aperture
tres
can
be
substantially
longer
Theoretical
and
experimental
analyses
see
Section
have
shown
that
the
probability
that
the
resolution
time
tres
exceeds
some
arbitrary
time
t
decreases
exponentially
with
t
where
Tc
is
the
clock
period
and
T
and
are
characteristic
of
the
flipflop
The
equation
is
valid
only
for
t
substantially
longer
than
tpcq
Intuitively
T
Tc
describes
the
probability
that
the
input
changes
at
a
bad
time
i
e
during
the
aperture
time
this
probability
decreases
with
the
cycle
time
Tc
is
a
time
constant
indicating
how
fast
the
flip
flop
moves
away
from
the
metastable
state
it
is
related
to
the
delay
through
the
cross
coupled
gates
in
the
flip
flop
In
summary
if
the
input
to
a
bistable
device
such
as
a
flip
flop
changes
during
the
aperture
time
the
output
may
take
on
a
metastable
value
for
some
time
before
resolving
to
a
stable
or
The
amount
of
time
required
to
resolve
is
unbounded
because
for
any
finite
time
t
the
probability
that
the
flip
flop
is
still
metastable
is
nonzero
However
this
probability
drops
off
exponentially
as
t
increases
Therefore
if
we
wait
long
enough
much
longer
than
tpcq
we
can
expect
with
exceedingly
high
probability
that
the
flip
flop
will
reach
a
valid
logic
level
Synchronizers
Asynchronous
inputs
to
digital
systems
from
the
real
world
are
inevitable
Human
input
is
asynchronous
for
example
If
handled
carelessly
these
asynchronous
inputs
can
lead
to
metastable
voltages
within
the
system
causing
erratic
system
failures
that
are
extremely
difficult
to
track
down
and
correct
The
goal
of
a
digital
system
designer
should
be
to
ensure
that
given
asynchronous
inputs
the
probability
of
encountering
a
metastable
voltage
is
sufficiently
small
Sufficiently
depends
on
the
context
For
a
digital
cell
phone
perhaps
one
failure
in
years
is
acceptable
because
the
user
can
always
turn
the
phone
off
and
back
on
if
it
locks
up
For
a
medical
device
one
failure
in
the
expected
life
of
the
universe
years
is
a
better
target
To
guarantee
good
logic
levels
all
asynchronous
inputs
should
be
passed
through
synchronizers
P
tres
t
T
Tc
et
CHAPTER
THREE
Sequential
Logic
Design
metastable
stable
stable
Figure
Stable
and
metastable
states
Cha
A
synchronizer
shown
in
Figure
is
a
device
that
receives
an
asynchronous
input
D
and
a
clock
CLK
It
produces
an
output
Q
within
a
bounded
amount
of
time
the
output
has
a
valid
logic
level
with
extremely
high
probability
If
D
is
stable
during
the
aperture
Q
should
take
on
the
same
value
as
D
If
D
changes
during
the
aperture
Q
may
take
on
either
a
HIGH
or
LOW
value
but
must
not
be
metastable
Figure
shows
a
simple
way
to
build
a
synchronizer
out
of
two
flip
flops
F
samples
D
on
the
rising
edge
of
CLK
If
D
is
changing
at
that
time
the
output
D
may
be
momentarily
metastable
If
the
clock
period
is
long
enough
D
will
with
high
probability
resolve
to
a
valid
logic
level
before
the
end
of
the
period
F
then
samples
D
which
is
now
stable
producing
a
good
output
Q
We
say
that
a
synchronizer
fails
if
Q
the
output
of
the
synchronizer
becomes
metastable
This
may
happen
if
D
has
not
resolved
to
a
valid
level
by
the
time
it
must
setup
at
F
that
is
if
tres
Tctsetup
According
to
Equation
the
probability
of
failure
for
a
single
input
change
at
a
random
time
is
The
probability
of
failure
P
failure
is
the
probability
that
the
output
Q
will
be
metastable
upon
a
single
change
in
D
If
D
changes
once
per
second
the
probability
of
failure
per
second
is
just
P
failure
However
if
D
changes
N
times
per
second
the
probability
of
failure
per
second
is
N
times
as
great
P
failure
sec
N
T
Tc
e
Tctsetup
P
failure
T
Tc
e
Tctsetup
Timing
of
Sequential
Logic
D
Q
CLKSYNC
Figure
Synchronizer
symbol
D
Q
D
Q
D
Tc
tsetup
tpcq
CLK
CLK
CLK
tres
metastable
F
F
Figure
Simple
synchronizer
Chapter
System
reliability
is
usually
measured
in
mean
time
between
failures
MTBF
As
the
name
might
suggest
MTBF
is
the
average
amount
of
time
between
failures
of
the
system
It
is
the
reciprocal
of
the
probability
that
the
system
will
fail
in
any
given
second
Equation
shows
that
the
MTBF
improves
exponentially
as
the
synchronizer
waits
for
a
longer
time
Tc
For
most
systems
a
synchronizer
that
waits
for
one
clock
cycle
provides
a
safe
MTBF
In
exceptionally
high
speed
systems
waiting
for
more
cycles
may
be
necessary
Example
SYNCHRONIZER
FOR
FSM
INPUT
The
traffic
light
controller
FSM
from
Section
receives
asynchronous
inputs
from
the
traffic
sensors
Suppose
that
a
synchronizer
is
used
to
guarantee
stable
inputs
to
the
controller
Traffic
arrives
on
average
times
per
second
The
flipflops
in
the
synchronizer
have
the
following
characteristics
ps
T
ps
and
tsetup
ps
How
long
must
the
synchronizer
clock
period
be
for
the
MTBF
to
exceed
year
Solution
year
seconds
Solve
Equation
This
equation
has
no
closed
form
solution
However
it
is
easy
enough
to
solve
by
guess
and
check
In
a
spreadsheet
try
a
few
values
of
Tc
and
calculate
the
MTBF
until
discovering
the
value
of
Tc
that
gives
an
MTBF
of
year
Tc
ns
Derivation
of
Resolution
Time
Equation
can
be
derived
using
a
basic
knowledge
of
circuit
theory
differential
equations
and
probability
This
section
can
be
skipped
if
you
are
not
interested
in
the
derivation
or
if
you
are
unfamiliar
with
the
mathematics
A
flip
flop
output
will
be
metastable
after
some
time
t
if
the
flipflop
samples
a
changing
input
causing
a
metastable
condition
and
the
output
does
not
resolve
to
a
valid
level
within
that
time
after
the
clock
edge
Symbolically
this
can
be
expressed
as
P
t
res
t
P
samples
changing
input
P
unresolved
Tc
e
Tc
MTBF
P
failure
sec
Tc
e
Tc
tsetup
NT
CHAPTER
THREE
Sequential
Logic
Design
Chapter
qxd
We
consider
each
probability
term
individually
The
asynchronous
input
signal
switches
between
and
in
some
time
tswitch
as
shown
in
Figure
The
probability
that
the
input
changes
during
the
aperture
around
the
clock
edge
is
If
the
flip
flop
does
enter
metastability
that
is
with
probability
P
samples
changing
input
the
time
to
resolve
from
metastability
depends
on
the
inner
workings
of
the
circuit
This
resolution
time
determines
P
unresolved
the
probability
that
the
flip
flop
has
not
yet
resolved
to
a
valid
logic
level
after
a
time
t
The
remainder
of
this
section
analyzes
a
simple
model
of
a
bistable
device
to
estimate
this
probability
A
bistable
device
uses
storage
with
positive
feedback
Figure
a
shows
this
feedback
implemented
with
a
pair
of
inverters
this
circuit
s
behavior
is
representative
of
most
bistable
elements
A
pair
of
inverters
behaves
like
a
buffer
Let
us
model
it
as
having
the
symmetric
DC
transfer
characteristics
shown
in
Figure
b
with
a
slope
of
G
The
buffer
can
deliver
only
a
finite
amount
of
output
current
we
can
model
this
as
an
output
resistance
R
All
real
circuits
also
have
some
capacitance
C
that
must
be
charged
up
Charging
the
capacitor
through
the
resistor
causes
an
RC
delay
preventing
the
buffer
from
switching
instantaneously
Hence
the
complete
circuit
model
is
shown
in
Figure
c
where
vout
t
is
the
voltage
of
interest
conveying
the
state
of
the
bistable
device
The
metastable
point
for
this
circuit
is
vout
t
vin
t
VDD
if
the
circuit
began
at
exactly
that
point
it
would
remain
there
indefinitely
in
the
P
samples
changing
input
tswitch
tsetup
thold
Tc
Timing
of
Sequential
Logic
CLK
D
Tc
tswitch
tsetup
thold
Figure
Input
timing
a
v
t
vout
t
R
C
c
vin
t
i
t
G
b
vin
vout
slope
G
VDD
VDD
VDD
VDD
V
Figure
Circuit
model
of
bistable
device
Cha
absence
of
noise
Because
voltages
are
continuous
variables
the
chance
that
the
circuit
will
begin
at
exactly
the
metastable
point
is
vanishingly
small
However
the
circuit
might
begin
at
time
near
metastability
at
vout
VDD
V
for
some
small
offset
V
In
such
a
case
the
positive
feedback
will
eventually
drive
vout
t
to
VDD
if
V
and
to
if
V
The
time
required
to
reach
VDD
or
is
the
resolution
time
of
the
bistable
device
The
DC
transfer
characteristic
is
nonlinear
but
it
appears
linear
near
the
metastable
point
which
is
the
region
of
interest
to
us
Specifically
if
vin
t
VDD
V
G
then
vout
t
VDD
V
for
small
V
The
current
through
the
resistor
is
i
t
vout
t
vin
t
R
The
capacitor
charges
at
a
rate
dvin
t
dt
i
t
C
Putting
these
facts
together
we
find
the
governing
equation
for
the
output
voltage
This
is
a
linear
first
order
differential
equation
Solving
it
with
the
initial
condition
vout
VDD
V
gives
Figure
plots
trajectories
for
vout
t
given
various
starting
points
vout
t
moves
exponentially
away
from
the
metastable
point
VDD
until
it
saturates
at
VDD
or
The
output
voltage
eventually
resolves
to
or
The
amount
of
time
this
takes
depends
on
the
initial
voltage
offset
V
from
the
metastable
point
VDD
Solving
Equation
for
the
resolution
time
tres
such
that
vout
tres
VDD
or
gives
t
res
RC
G
ln
VDD
V
V
e
G
tres
RC
VDD
vout
t
VDD
Ve
G
t
RC
dvout
t
dt
G
RC
vout
t
VDD
CHAPTER
THREE
Sequential
Logic
Design
VDD
VDD
t
vout
t
Figure
Resolution
trajectories
Chapter
qxd
In
summary
the
resolution
time
increases
if
the
bistable
device
has
high
resistance
or
capacitance
that
causes
the
output
to
change
slowly
It
decreases
if
the
bistable
device
has
high
gain
G
The
resolution
time
also
increases
logarithmically
as
the
circuit
starts
closer
to
the
metastable
point
V
Define
as
Solving
Equation
for
V
finds
the
initial
offset
Vres
that
gives
a
particular
resolution
time
tres
Suppose
that
the
bistable
device
samples
the
input
while
it
is
changing
It
measures
a
voltage
vin
which
we
will
assume
is
uniformly
distributed
between
and
VDD
The
probability
that
the
output
has
not
resolved
to
a
legal
value
after
time
tres
depends
on
the
probability
that
the
initial
offset
is
sufficiently
small
Specifically
the
initial
offset
on
vout
must
be
less
than
Vres
so
the
initial
offset
on
vin
must
be
less
than
Vres
G
Then
the
probability
that
the
bistable
device
samples
the
input
at
a
time
to
obtain
a
sufficiently
small
initial
offset
is
Putting
this
all
together
the
probability
that
the
resolution
time
exceeds
some
time
t
is
given
by
the
following
equation
Observe
that
Equation
is
in
the
form
of
Equation
where
T
tswitch
tsetup
thold
G
and
RC
G
In
summary
we
have
derived
Equation
and
shown
how
T
and
depend
on
physical
properties
of
the
bistable
device
PARALLELISM
The
speed
of
a
system
is
measured
in
latency
and
throughput
of
tokens
moving
through
a
system
We
define
a
token
to
be
a
group
of
inputs
that
are
processed
to
produce
a
group
of
outputs
The
term
conjures
up
the
notion
of
placing
subway
tokens
on
a
circuit
diagram
and
moving
them
around
to
visualize
data
moving
through
the
circuit
The
latency
of
a
system
is
the
time
required
for
one
token
to
pass
through
the
system
from
start
to
end
The
throughput
is
the
number
of
tokens
that
can
be
produced
per
unit
time
P
tres
t
tswitch
tsetup
thold
GTc
e
t
P
unresolved
P
vin
VDD
Vres
G
Vres
GVDD
Vres
VDD
etres
RC
G
Parallelism
Chapter
Example
COOKIE
THROUGHPUT
AND
LATENCY
Ben
Bitdiddle
is
throwing
a
milk
and
cookies
party
to
celebrate
the
installation
of
his
traffic
light
controller
It
takes
him
minutes
to
roll
cookies
and
place
them
on
his
tray
It
then
takes
minutes
for
the
cookies
to
bake
in
the
oven
Once
the
cookies
are
baked
he
starts
another
tray
What
is
Ben
s
throughput
and
latency
for
a
tray
of
cookies
Solution
In
this
example
a
tray
of
cookies
is
a
token
The
latency
is
hour
per
tray
The
throughput
is
trays
hour
As
you
might
imagine
the
throughput
can
be
improved
by
processing
several
tokens
at
the
same
time
This
is
called
parallelism
and
it
comes
in
two
forms
spatial
and
temporal
With
spatial
parallelism
multiple
copies
of
the
hardware
are
provided
so
that
multiple
tasks
can
be
done
at
the
same
time
With
temporal
parallelism
a
task
is
broken
into
stages
like
an
assembly
line
Multiple
tasks
can
be
spread
across
the
stages
Although
each
task
must
pass
through
all
stages
a
different
task
will
be
in
each
stage
at
any
given
time
so
multiple
tasks
can
overlap
Temporal
parallelism
is
commonly
called
pipelining
Spatial
parallelism
is
sometimes
just
called
parallelism
but
we
will
avoid
that
naming
convention
because
it
is
ambiguous
Example
COOKIE
PARALLELISM
Ben
Bitdiddle
has
hundreds
of
friends
coming
to
his
party
and
needs
to
bake
cookies
faster
He
is
considering
using
spatial
and
or
temporal
parallelism
Spatial
Parallelism
Ben
asks
Alyssa
P
Hacker
to
help
out
She
has
her
own
cookie
tray
and
oven
Temporal
Parallelism
Ben
gets
a
second
cookie
tray
Once
he
puts
one
cookie
tray
in
the
oven
he
starts
rolling
cookies
on
the
other
tray
rather
than
waiting
for
the
first
tray
to
bake
What
is
the
throughput
and
latency
using
spatial
parallelism
Using
temporal
parallelism
Using
both
Solution
The
latency
is
the
time
required
to
complete
one
task
from
start
to
finish
In
all
cases
the
latency
is
hour
If
Ben
starts
with
no
cookies
the
latency
is
the
time
needed
for
him
to
produce
the
first
cookie
tray
The
throughput
is
the
number
of
cookie
trays
per
hour
With
spatial
parallelism
Ben
and
Alyssa
each
complete
one
tray
every
minutes
Hence
the
throughput
doubles
to
trays
hour
With
temporal
parallelism
Ben
puts
a
new
tray
in
the
oven
every
minutes
for
a
throughput
of
trays
hour
These
are
illustrated
in
Figure
If
Ben
and
Alyssa
use
both
techniques
they
can
bake
trays
hour
CHAPTER
THREE
Sequential
Logic
Design
Consider
a
task
with
latency
L
In
a
system
with
no
parallelism
the
throughput
is
L
In
a
spatially
parallel
system
with
N
copies
of
the
hardware
the
throughput
is
N
L
In
a
temporally
parallel
system
the
task
is
ideally
broken
into
N
steps
or
stages
of
equal
length
In
such
a
case
the
throughput
is
also
N
L
and
only
one
copy
of
the
hardware
is
required
However
as
the
cookie
example
showed
finding
N
steps
of
equal
length
is
often
impractical
If
the
longest
step
has
a
latency
L
the
pipelined
throughput
is
L
Pipelining
temporal
parallelism
is
particularly
attractive
because
it
speeds
up
a
circuit
without
duplicating
the
hardware
Instead
registers
are
placed
between
blocks
of
combinational
logic
to
divide
the
logic
into
shorter
stages
that
can
run
with
a
faster
clock
The
registers
prevent
a
token
in
one
pipeline
stage
from
catching
up
with
and
corrupting
the
token
in
the
next
stage
Figure
shows
an
example
of
a
circuit
with
no
pipelining
It
contains
four
blocks
of
logic
between
the
registers
The
critical
path
passes
through
blocks
and
Assume
that
the
register
has
a
clock
to
Q
propagation
delay
of
ns
and
a
setup
time
of
ns
Then
the
cycle
time
is
Tc
ns
The
circuit
has
a
latency
of
ns
and
a
throughput
of
ns
MHz
Figure
shows
the
same
circuit
partitioned
into
a
two
stage
pipeline
by
adding
a
register
between
blocks
and
The
first
stage
has
a
minimum
clock
period
of
ns
The
second
Parallelism
Spatial
Parallelism
Temporal
Parallelism
Ben
Ben
Roll
Bake
Ben
Ben
Ben
Ben
Ben
Ben
Alyssa
Alyssa
Ben
Ben
Alyssa
Alyssa
Time
Tray
Tray
Tray
Tray
Latency
time
to
first
tray
Legend
Tray
Tray
Tray
Figure
Spatial
and
temporal
parallelism
in
the
cookie
kitchen
Chap
stage
has
a
minimum
clock
period
of
ns
The
clock
must
be
slow
enough
for
all
stages
to
work
Hence
Tc
ns
The
latency
is
two
clock
cycles
or
ns
The
throughput
is
ns
MHz
This
example
shows
that
in
a
real
circuit
pipelining
with
two
stages
almost
doubles
the
throughput
and
slightly
increases
the
latency
In
comparison
ideal
pipelining
would
exactly
double
the
throughput
at
no
penalty
in
latency
The
discrepancy
comes
about
because
the
circuit
cannot
be
divided
into
two
exactly
equal
halves
and
because
the
registers
introduce
more
sequencing
overhead
Figure
shows
the
same
circuit
partitioned
into
a
three
stage
pipeline
Note
that
two
more
registers
are
needed
to
store
the
results
of
blocks
and
at
the
end
of
the
first
pipeline
stage
The
cycle
time
is
now
limited
by
the
third
stage
to
ns
The
latency
is
three
cycles
or
ns
The
throughput
is
ns
MHz
Again
adding
a
pipeline
stage
improves
throughput
at
the
expense
of
some
latency
Although
these
techniques
are
powerful
they
do
not
apply
to
all
situations
The
bane
of
parallelism
is
dependencies
If
a
current
task
is
CHAPTER
THREE
Sequential
Logic
Design
CL
CL
CL
CL
CLK
CLK
tpd
ns
tpd
ns
tpd
ns
tpd
ns
Stage
ns
Stage
ns
CLK
Figure
Circuit
with
twostage
pipeline
CL
CL
CL
CL
CLK
CLK
tpd
ns
tpd
ns
tpd
ns
tpd
ns
Stage
ns
Stage
ns
CLK
CLK
Stage
ns
Figure
Circuit
with
threestage
pipeline
CL
CL
CL
CL
CLK
CLK
tpd
ns
tpd
ns
tpd
ns
tpd
ns
Tc
ns
Figure
Circuit
with
no
pipelining
Chap
dependent
on
the
result
of
a
prior
task
rather
than
just
prior
steps
in
the
current
task
the
task
cannot
start
until
the
prior
task
has
completed
For
example
if
Ben
wants
to
check
that
the
first
tray
of
cookies
tastes
good
before
he
starts
preparing
the
second
he
has
a
dependency
that
prevents
pipelining
or
parallel
operation
Parallelism
is
one
of
the
most
important
techniques
for
designing
high
performance
microprocessors
Chapter
discusses
pipelining
further
and
shows
examples
of
handling
dependencies
SUMMARY
This
chapter
has
described
the
analysis
and
design
of
sequential
logic
In
contrast
to
combinational
logic
whose
outputs
depend
only
on
the
current
inputs
sequential
logic
outputs
depend
on
both
current
and
prior
inputs
In
other
words
sequential
logic
remembers
information
about
prior
inputs
This
memory
is
called
the
state
of
the
logic
Sequential
circuits
can
be
difficult
to
analyze
and
are
easy
to
design
incorrectly
so
we
limit
ourselves
to
a
small
set
of
carefully
designed
building
blocks
The
most
important
element
for
our
purposes
is
the
flip
flop
which
receives
a
clock
and
an
input
D
and
produces
an
output
Q
The
flip
flop
copies
D
to
Q
on
the
rising
edge
of
the
clock
and
otherwise
remembers
the
old
state
of
Q
A
group
of
flip
flops
sharing
a
common
clock
is
called
a
register
Flip
flops
may
also
receive
reset
or
enable
control
signals
Although
many
forms
of
sequential
logic
exist
we
discipline
ourselves
to
use
synchronous
sequential
circuits
because
they
are
easy
to
design
Synchronous
sequential
circuits
consist
of
blocks
of
combinational
logic
separated
by
clocked
registers
The
state
of
the
circuit
is
stored
in
the
registers
and
updated
only
on
clock
edges
Finite
state
machines
are
a
powerful
technique
for
designing
sequential
circuits
To
design
an
FSM
first
identify
the
inputs
and
outputs
of
the
machine
and
sketch
a
state
transition
diagram
indicating
the
states
and
the
transitions
between
them
Select
an
encoding
for
the
states
and
rewrite
the
diagram
as
a
state
transition
table
and
output
table
indicating
the
next
state
and
output
given
the
current
state
and
input
From
these
tables
design
the
combinational
logic
to
compute
the
next
state
and
output
and
sketch
the
circuit
Synchronous
sequential
circuits
have
a
timing
specification
including
the
clock
to
Q
propagation
and
contamination
delays
tpcq
and
tccq
and
the
setup
and
hold
times
tsetup
and
thold
For
correct
operation
their
inputs
must
be
stable
during
an
aperture
time
that
starts
a
setup
time
before
the
rising
edge
of
the
clock
and
ends
a
hold
time
after
the
rising
edge
of
the
clock
The
minimum
cycle
time
Tc
of
the
system
is
equal
to
the
propagation
delay
tpd
through
the
combinational
logic
plus
Summary
Anyone
who
could
invent
logic
whose
outputs
depend
on
future
inputs
would
be
fabulously
wealthy
CHAPTER
THREE
Sequential
Logic
Design
tpcq
tsetup
of
the
register
For
correct
operation
the
contamination
delay
through
the
register
and
combinational
logic
must
be
greater
than
thold
Despite
the
common
misconception
to
the
contrary
hold
time
does
not
affect
the
cycle
time
Overall
system
performance
is
measured
in
latency
and
throughput
The
latency
is
the
time
required
for
a
token
to
pass
from
start
to
end
The
throughput
is
the
number
of
tokens
that
the
system
can
process
per
unit
time
Parallelism
improves
the
system
throughput
Exercises
Exercises
Exercise
Given
the
input
waveforms
shown
in
Figure
sketch
the
output
Q
of
an
SR
latch
S
R
Figure
Input
waveform
of
SR
latch
Exercise
Given
the
input
waveforms
shown
in
Figure
sketch
the
output
Q
of
a
D
latch
Exercise
Given
the
input
waveforms
shown
in
Figure
sketch
the
output
Q
of
a
D
flip
flop
Exercise
Is
the
circuit
in
Figure
combinational
logic
or
sequential
logic
Explain
in
a
simple
fashion
what
the
relationship
is
between
the
inputs
and
outputs
What
would
you
call
this
circuit
Exercise
Is
the
circuit
in
Figure
combinational
logic
or
sequential
logic
Explain
in
a
simple
fashion
what
the
relationship
is
between
the
inputs
and
outputs
What
would
you
call
this
circuit
CLK
D
Figure
Input
waveform
of
D
latch
or
flip
flop
S
R
Q
Q
Figure
Mystery
circuit
CHAPTER
THREE
Sequential
Logic
Design
Exercise
The
toggle
T
flip
flop
has
one
input
CLK
and
one
output
Q
On
each
rising
edge
of
CLK
Q
toggles
to
the
complement
of
its
previous
value
Draw
a
schematic
for
a
T
flip
flop
using
a
D
flip
flop
and
an
inverter
Exercise
A
JK
flip
flop
receives
a
clock
and
two
inputs
J
and
K
On
the
rising
edge
of
the
clock
it
updates
the
output
Q
If
J
and
K
are
both
Q
retains
its
old
value
If
only
J
is
Q
becomes
If
only
K
is
Q
becomes
If
both
J
and
K
are
Q
becomes
the
opposite
of
its
present
state
a
Construct
a
JK
flip
flop
using
a
D
flip
flop
and
some
combinational
logic
b
Construct
a
D
flip
flop
using
a
JK
flip
flop
and
some
combinational
logic
c
Construct
a
T
flip
flop
see
Exercise
using
a
JK
flip
flop
Exercise
The
circuit
in
Figure
is
called
a
Muller
C
element
Explain
in
a
simple
fashion
what
the
relationship
is
between
the
inputs
and
output
Q
Q
S
R
S
R
D
R
CLK
Figure
Mystery
circuit
A
B
A
B
C
weak
Figure
Muller
C
element
Exercise
Design
an
asynchronously
resettable
D
latch
using
logic
gates
Exercise
Design
an
asynchronously
resettable
D
flip
flop
using
logic
gates
Exercise
Design
a
synchronously
settable
D
flip
flop
using
logic
gates
Exercises
Exercise
Design
an
asynchronously
settable
D
flip
flop
using
logic
gates
Exercise
Suppose
a
ring
oscillator
is
built
from
N
inverters
connected
in
a
loop
Each
inverter
has
a
minimum
delay
of
tcd
and
a
maximum
delay
of
tpd
If
N
is
odd
determine
the
range
of
frequencies
at
which
the
oscillator
might
operate
Exercise
Why
must
N
be
odd
in
Exercise
Exercise
Which
of
the
circuits
in
Figure
are
synchronous
sequential
circuits
Explain
Exercise
You
are
designing
an
elevator
controller
for
a
building
with
floors
The
controller
has
two
inputs
UP
and
DOWN
It
produces
an
output
indicating
the
floor
that
the
elevator
is
on
There
is
no
floor
What
is
the
minimum
number
of
bits
of
state
in
the
controller
Exercise
You
are
designing
an
FSM
to
keep
track
of
the
mood
of
four
students
working
in
the
digital
design
lab
Each
student
s
mood
is
either
HAPPY
the
circuit
works
SAD
the
circuit
blew
up
BUSY
working
on
the
circuit
CLUELESS
confused
about
the
circuit
or
ASLEEP
face
down
on
the
circuit
board
How
many
states
does
the
FSM
have
What
is
the
minimum
number
of
bits
necessary
to
represent
these
states
Exercise
How
would
you
factor
the
FSM
from
Exercise
into
multiple
simpler
machines
How
many
states
does
each
simpler
machine
have
What
is
the
minimum
total
number
of
bits
necessary
in
this
factored
design
a
CL
CL
CLK
CL
CL
CL
CL
CLK
CL
b
c
d
CL
CL
CLK
Figure
Circuits
Exercise
Describe
in
words
what
the
state
machine
in
Figure
does
Using
binary
state
encodings
complete
a
state
transition
table
and
output
table
for
the
FSM
Write
Boolean
equations
for
the
next
state
and
output
and
sketch
a
schematic
of
the
FSM
Exercise
Describe
in
words
what
the
state
machine
in
Figure
does
Using
binary
state
encodings
complete
a
state
transition
table
and
output
table
for
the
FSM
Write
Boolean
equations
for
the
next
state
and
output
and
sketch
a
schematic
of
the
FSM
Exercise
Accidents
are
still
occurring
at
the
intersection
of
Academic
Avenue
and
Bravado
Boulevard
The
football
team
is
rushing
into
the
intersection
the
moment
light
B
turns
green
They
are
colliding
with
sleep
deprived
CS
majors
who
stagger
into
the
intersection
just
before
light
A
turns
red
Extend
the
traffic
light
controller
from
Section
so
that
both
lights
are
red
for
seconds
before
either
light
turns
green
again
Sketch
your
improved
Moore
machine
state
transition
diagram
state
encodings
state
transition
table
output
table
next
state
and
output
equations
and
your
FSM
schematic
Exercise
Alyssa
P
Hacker
s
snail
from
Section
has
a
daughter
with
a
Mealy
machine
FSM
brain
The
daughter
snail
smiles
whenever
she
slides
over
the
pattern
or
the
pattern
Sketch
the
state
transition
diagram
for
this
happy
snail
using
as
few
states
as
possible
Choose
state
encodings
and
write
a
CHAPTER
THREE
Sequential
Logic
Design
S
Q
S
Q
S
Q
Reset
A
B
A
B
Figure
State
transition
diagram
S
S
S
Reset
A
B
A
B
AB
A
B
Figure
State
transition
diagram
Exercises
combined
state
transition
and
output
table
using
your
encodings
Write
the
next
state
and
output
equations
and
sketch
your
FSM
schematic
Exercise
You
have
been
enlisted
to
design
a
soda
machine
dispenser
for
your
department
lounge
Sodas
are
partially
subsidized
by
the
student
chapter
of
the
IEEE
so
they
cost
only
cents
The
machine
accepts
nickels
dimes
and
quarters
When
enough
coins
have
been
inserted
it
dispenses
the
soda
and
returns
any
necessary
change
Design
an
FSM
controller
for
the
soda
machine
The
FSM
inputs
are
Nickel
Dime
and
Quarter
indicating
which
coin
was
inserted
Assume
that
exactly
one
coin
is
inserted
on
each
cycle
The
outputs
are
Dispense
ReturnNickel
ReturnDime
and
ReturnTwoDimes
When
the
FSM
reaches
cents
it
asserts
Dispense
and
the
necessary
Return
outputs
required
to
deliver
the
appropriate
change
Then
it
should
be
ready
to
start
accepting
coins
for
another
soda
Exercise
Gray
codes
have
a
useful
property
in
that
consecutive
numbers
differ
in
only
a
single
bit
position
Table
lists
a
bit
Gray
code
representing
the
numbers
to
Design
a
bit
modulo
Gray
code
counter
FSM
with
no
inputs
and
three
outputs
A
modulo
N
counter
counts
from
to
N
then
repeats
For
example
a
watch
uses
a
modulo
counter
for
the
minutes
and
seconds
that
counts
from
to
When
reset
the
output
should
be
On
each
clock
edge
the
output
should
advance
to
the
next
Gray
code
After
reaching
it
should
repeat
with
Table
bit
Gray
code
Number
Gray
code
Exercise
Extend
your
modulo
Gray
code
counter
from
Exercise
to
be
an
UP
DOWN
counter
by
adding
an
UP
input
If
UP
the
counter
advances
to
the
next
number
If
UP
the
counter
retreats
to
the
previous
number
Cha
Exercise
Your
company
Detect
o
rama
would
like
to
design
an
FSM
that
takes
two
inputs
A
and
B
and
generates
one
output
Z
The
output
in
cycle
n
Zn
is
either
the
Boolean
AND
or
OR
of
the
corresponding
input
An
and
the
previous
input
An
depending
on
the
other
input
Bn
a
Sketch
the
waveform
for
Z
given
the
inputs
shown
in
Figure
b
Is
this
FSM
a
Moore
or
a
Mealy
machine
c
Design
the
FSM
Show
your
state
transition
diagram
encoded
state
transition
table
next
state
and
output
equations
and
schematic
ZnAn
An
if
Bn
Zn
An
An
if
Bn
CHAPTER
THREE
Sequential
Logic
Design
CLK
A
B
Figure
FSM
input
waveforms
Exercise
Design
an
FSM
with
one
input
A
and
two
outputs
X
and
Y
X
should
be
if
A
has
been
for
at
least
three
cycles
altogether
not
necessarily
consecutively
Y
should
be
if
A
has
been
for
at
least
two
consecutive
cycles
Show
your
state
transition
diagram
encoded
state
transition
table
next
state
and
output
equations
and
schematic
Exercise
Analyze
the
FSM
shown
in
Figure
Write
the
state
transition
and
output
tables
and
sketch
the
state
transition
diagram
Describe
in
words
what
the
FSM
does
CLK
CLK
X
Q
Figure
FSM
schematic
Chapter
Exercises
Exercise
Repeat
Exercise
for
the
FSM
shown
in
Figure
Recall
that
the
r
and
s
register
inputs
indicate
set
and
reset
respectively
Exercise
Ben
Bitdiddle
has
designed
the
circuit
in
Figure
to
compute
a
registered
four
input
XOR
function
Each
two
input
XOR
gate
has
a
propagation
delay
of
ps
and
a
contamination
delay
of
ps
Each
flip
flop
has
a
setup
time
of
ps
a
hold
time
of
ps
a
clock
to
Q
maximum
delay
of
ps
and
a
clock
to
Q
minimum
delay
of
ps
a
If
there
is
no
clock
skew
what
is
the
maximum
operating
frequency
of
the
circuit
b
How
much
clock
skew
can
the
circuit
tolerate
if
it
must
operate
at
GHz
c
How
much
clock
skew
can
the
circuit
tolerate
before
it
might
experience
a
hold
time
violation
d
Alyssa
P
Hacker
points
out
that
she
can
redesign
the
combinational
logic
between
the
registers
to
be
faster
and
tolerate
more
clock
skew
Her
improved
circuit
also
uses
three
two
input
XORs
but
they
are
arranged
differently
What
is
her
circuit
What
is
its
maximum
frequency
if
there
is
no
clock
skew
How
much
clock
skew
can
the
circuit
tolerate
before
it
might
experience
a
hold
time
violation
CLK
A
CLK
CLK
Q
reset
r
r
s
Figure
FSM
schematic
CLK
CLK
Figure
Registered
four
input
XOR
circuit
Exercise
You
are
designing
an
adder
for
the
blindingly
fast
bit
RePentium
Processor
The
adder
is
built
from
two
full
adders
such
that
the
carry
out
of
the
first
adder
is
the
carry
in
to
the
second
adder
as
shown
in
Figure
Your
adder
has
input
and
output
registers
and
must
complete
the
addition
in
one
clock
cycle
Each
full
adder
has
the
following
propagation
delays
ps
from
Cin
to
Cout
or
to
Sum
S
ps
from
A
or
B
to
Cout
and
ps
from
A
or
B
to
S
The
adder
has
a
contamination
delay
of
ps
from
Cin
to
either
output
and
ps
from
A
or
B
to
either
output
Each
flip
flop
has
a
setup
time
of
ps
a
hold
time
of
ps
a
clock
to
Q
propagation
delay
of
ps
and
a
clock
to
Q
contamination
delay
of
ps
a
If
there
is
no
clock
skew
what
is
the
maximum
operating
frequency
of
the
circuit
b
How
much
clock
skew
can
the
circuit
tolerate
if
it
must
operate
at
GHz
c
How
much
clock
skew
can
the
circuit
tolerate
before
it
might
experience
a
hold
time
violation
Exercise
A
field
programmable
gate
array
FPGA
uses
configurable
logic
blocks
CLBs
rather
than
logic
gates
to
implement
combinational
logic
The
Xilinx
Spartan
FPGA
has
propagation
and
contamination
delays
of
and
ns
respectively
for
each
CLB
It
also
contains
flip
flops
with
propagation
and
contamination
delays
of
and
ns
and
setup
and
hold
times
of
and
ns
respectively
a
If
you
are
building
a
system
that
needs
to
run
at
MHz
how
many
consecutive
CLBs
can
you
use
between
two
flip
flops
Assume
there
is
no
clock
skew
and
no
delay
through
wires
between
CLBs
b
Suppose
that
all
paths
between
flip
flops
pass
through
at
least
one
CLB
How
much
clock
skew
can
the
FPGA
have
without
violating
the
hold
time
Exercise
A
synchronizer
is
built
from
a
pair
of
flip
flops
with
tsetup
ps
T
ps
and
ps
It
samples
an
asynchronous
input
that
changes
times
per
second
What
is
the
minimum
clock
period
of
the
synchronizer
to
achieve
a
mean
time
between
failures
MTBF
of
years
CHAPTER
THREE
Sequential
Logic
Design
A
B
Cin
Cout
S
A
B
Cin
Cout
S
CLK
C
A
B
A
B
S
S
CLK
Figure
bit
adder
schematic
Cha
Exercises
Exercise
You
would
like
to
build
a
synchronizer
that
can
receive
asynchronous
inputs
with
an
MTBF
of
years
Your
system
is
running
at
GHz
and
you
use
sampling
flip
flops
with
ps
T
ps
and
tsetup
ps
The
synchronizer
receives
a
new
asynchronous
input
on
average
times
per
second
i
e
once
every
seconds
What
is
the
required
probability
of
failure
to
satisfy
this
MTBF
How
many
clock
cycles
would
you
have
to
wait
before
reading
the
sampled
input
signal
to
give
that
probability
of
error
Exercise
You
are
walking
down
the
hallway
when
you
run
into
your
lab
partner
walking
in
the
other
direction
The
two
of
you
first
step
one
way
and
are
still
in
each
other
s
way
Then
you
both
step
the
other
way
and
are
still
in
each
other
s
way
Then
you
both
wait
a
bit
hoping
the
other
person
will
step
aside
You
can
model
this
situation
as
a
metastable
point
and
apply
the
same
theory
that
has
been
applied
to
synchronizers
and
flip
flops
Suppose
you
create
a
mathematical
model
for
yourself
and
your
lab
partner
You
start
the
unfortunate
encounter
in
the
metastable
state
The
probability
that
you
remain
in
this
state
after
t
seconds
is
indicates
your
response
rate
today
your
brain
has
been
blurred
by
lack
of
sleep
and
has
seconds
a
How
long
will
it
be
until
you
have
certainty
that
you
will
have
resolved
from
metastability
i
e
figured
out
how
to
pass
one
another
b
You
are
not
only
sleepy
but
also
ravenously
hungry
In
fact
you
will
starve
to
death
if
you
don
t
get
going
to
the
cafeteria
within
minutes
What
is
the
probability
that
your
lab
partner
will
have
to
drag
you
to
the
morgue
Exercise
You
have
built
a
synchronizer
using
flip
flops
with
T
ps
and
ps
Your
boss
tells
you
that
you
need
to
increase
the
MTBF
by
a
factor
of
By
how
much
do
you
need
to
increase
the
clock
period
Exercise
Ben
Bitdiddle
invents
a
new
and
improved
synchronizer
in
Figure
that
he
claims
eliminates
metastability
in
a
single
cycle
He
explains
that
the
circuit
in
box
M
is
an
analog
metastability
detector
that
produces
a
HIGH
output
if
the
input
voltage
is
in
the
forbidden
zone
between
VIL
and
VIH
The
metastability
detector
checks
to
determine
e
t
CLK
D
r
CLK
Q
M
D
Figure
New
and
improved
synchronizer
Chapter
CHAPTER
THREE
Sequential
Logic
Design
whether
the
first
flip
flop
has
produced
a
metastable
output
on
D
If
so
it
asynchronously
resets
the
flip
flop
to
produce
a
good
at
D
The
second
flip
flop
then
samples
D
always
producing
a
valid
logic
level
on
Q
Alyssa
P
Hacker
tells
Ben
that
there
must
be
a
bug
in
the
circuit
because
eliminating
metastability
is
just
as
impossible
as
building
a
perpetual
motion
machine
Who
is
right
Explain
showing
Ben
s
error
or
showing
why
Alyssa
is
wrong
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Draw
a
state
machine
that
can
detect
when
it
has
received
the
serial
input
sequence
Question
Design
a
serial
one
bit
at
a
time
two
s
complementer
FSM
with
two
inputs
Start
and
A
and
one
output
Q
A
binary
number
of
arbitrary
length
is
provided
to
input
A
starting
with
the
least
significant
bit
The
corresponding
bit
of
the
output
appears
at
Q
on
the
same
cycle
Start
is
asserted
for
one
cycle
to
initialize
the
FSM
before
the
least
significant
bit
is
provided
Question
What
is
the
difference
between
a
latch
and
a
flip
flop
Under
what
circumstances
is
each
one
preferable
Question
Design
a
bit
counter
finite
state
machine
Question
Design
an
edge
detector
circuit
The
output
should
go
HIGH
for
one
cycle
after
the
input
makes
a
transition
Question
Describe
the
concept
of
pipelining
and
why
it
is
used
Question
Describe
what
it
means
for
a
flip
flop
to
have
a
negative
hold
time
Question
Given
signal
A
shown
in
Figure
design
a
circuit
that
produces
signal
B
Interview
Questions
A
B
Figure
Signal
waveforms
Question
Consider
a
block
of
logic
between
two
registers
Explain
the
timing
constraints
If
you
add
a
buffer
on
the
clock
input
of
the
receiver
the
second
flip
flop
does
the
setup
time
constraint
get
better
or
worse
Introduction
Combinational
Logic
Structural
Modeling
Sequential
Logic
More
Combinational
Logic
Finite
State
Machines
Parameterized
Modules
Testbenches
Summary
Exercises
Interview
Questions
Hardware
Description
Languages
INTRODUCTION
Thus
far
we
have
focused
on
designing
combinational
and
sequential
digital
circuits
at
the
schematic
level
The
process
of
finding
an
efficient
set
of
logic
gates
to
perform
a
given
function
is
labor
intensive
and
error
prone
requiring
manual
simplification
of
truth
tables
or
Boolean
equations
and
manual
translation
of
finite
state
machines
FSMs
into
gates
In
the
s
designers
discovered
that
they
were
far
more
productive
if
they
worked
at
a
higher
level
of
abstraction
specifying
just
the
logical
function
and
allowing
a
computer
aided
design
CAD
tool
to
produce
the
optimized
gates
The
specifications
are
generally
given
in
a
hardware
description
language
HDL
The
two
leading
hardware
description
languages
are
Verilog
and
VHDL
Verilog
and
VHDL
are
built
on
similar
principles
but
have
different
syntax
Discussion
of
these
languages
in
this
chapter
is
divided
into
two
columns
for
literal
side
by
side
comparison
with
Verilog
on
the
left
and
VHDL
on
the
right
When
you
read
the
chapter
for
the
first
time
focus
on
one
language
or
the
other
Once
you
know
one
you
ll
quickly
master
the
other
if
you
need
it
Subsequent
chapters
show
hardware
in
both
schematic
and
HDL
form
If
you
choose
to
skip
this
chapter
and
not
learn
one
of
the
HDLs
you
will
still
be
able
to
master
the
principles
of
computer
organization
from
the
schematics
However
the
vast
majority
of
commercial
systems
are
now
built
using
HDLs
rather
than
schematics
If
you
expect
to
do
digital
design
at
any
point
in
your
professional
life
we
urge
you
to
learn
one
of
the
HDLs
Modules
A
block
of
hardware
with
inputs
and
outputs
is
called
a
module
An
AND
gate
a
multiplexer
and
a
priority
circuit
are
all
examples
of
hardware
modules
The
two
general
styles
for
describing
module
functionality
are
behavioral
and
structural
Behavioral
models
describe
what
a
module
does
Structural
models
describe
how
a
module
is
built
from
simpler
pieces
it
is
an
application
of
hierarchy
The
Verilog
and
VHDL
code
in
HDL
Example
illustrate
behavioral
descriptions
of
a
module
that
computes
the
Boolean
function
from
Example
In
both
languages
the
module
is
named
sillyfunction
and
has
three
inputs
a
b
and
c
and
one
output
y
y
abcabc
abc
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
sillyfunction
input
a
b
c
output
y
assign
y
a
b
c
a
b
c
a
b
c
endmodule
A
Verilog
module
begins
with
the
module
name
and
a
listing
of
the
inputs
and
outputs
The
assign
statement
describes
combinational
logic
indicates
NOT
indicates
AND
and
indicates
OR
Verilog
signals
such
as
the
inputs
and
outputs
are
Boolean
variables
or
They
may
also
have
floating
and
undefined
values
as
discussed
in
Section
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
sillyfunction
is
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
sillyfunction
is
begin
y
not
a
and
not
b
and
not
c
or
a
and
not
b
and
not
c
or
a
and
not
b
and
c
end
VHDL
code
has
three
parts
the
library
use
clause
the
entity
declaration
and
the
architecture
body
The
library
use
clause
is
required
and
will
be
discussed
in
Section
The
entity
declaration
lists
the
module
name
and
its
inputs
and
outputs
The
architecture
body
defines
what
the
module
does
VHDL
signals
such
as
inputs
and
outputs
must
have
a
type
declaration
Digital
signals
should
be
declared
to
be
STD
LOGIC
type
STD
LOGIC
signals
can
have
a
value
of
or
as
well
as
floating
and
undefined
values
that
will
be
described
in
Section
The
STD
LOGIC
type
is
defined
in
the
IEEE
STD
LOGIC
library
which
is
why
the
library
must
be
used
VHDL
lacks
a
good
default
order
of
operations
so
Boolean
equations
should
be
parenthesized
HDL
Example
COMBINATIONAL
LOGIC
A
module
as
you
might
expect
is
a
good
application
of
modularity
It
has
a
well
defined
interface
consisting
of
its
inputs
and
outputs
and
it
performs
a
specific
function
The
particular
way
in
which
it
is
coded
is
unimportant
to
others
that
might
use
the
module
as
long
as
it
performs
its
function
Language
Origins
Universities
are
almost
evenly
split
on
which
of
these
languages
is
taught
in
a
first
course
and
industry
is
similarly
split
on
which
language
is
preferred
Compared
to
Verilog
VHDL
is
more
verbose
and
cumbersome
Chapt
as
you
might
expect
of
a
language
developed
by
committee
U
S
military
contractors
the
European
Space
Agency
and
telecommunications
companies
use
VHDL
extensively
Both
languages
are
fully
capable
of
describing
any
hardware
system
and
both
have
their
quirks
The
best
language
to
use
is
the
one
that
is
already
being
used
at
your
site
or
the
one
that
your
customers
demand
Most
CAD
tools
today
allow
the
two
languages
to
be
mixed
so
that
different
modules
can
be
described
in
different
languages
Simulation
and
Synthesis
The
two
major
purposes
of
HDLs
are
logic
simulation
and
synthesis
During
simulation
inputs
are
applied
to
a
module
and
the
outputs
are
checked
to
verify
that
the
module
operates
correctly
During
synthesis
the
textual
description
of
a
module
is
transformed
into
logic
gates
Simulation
Humans
routinely
make
mistakes
Such
errors
in
hardware
designs
are
called
bugs
Eliminating
the
bugs
from
a
digital
system
is
obviously
important
especially
when
customers
are
paying
money
and
lives
depend
on
the
correct
operation
Testing
a
system
in
the
laboratory
is
time
consuming
Discovering
the
cause
of
errors
in
the
lab
can
be
extremely
difficult
because
only
signals
routed
to
the
chip
pins
can
be
observed
There
is
no
way
to
directly
observe
what
is
happening
inside
a
chip
Correcting
errors
after
the
system
is
built
can
be
devastatingly
expensive
For
example
correcting
a
mistake
in
a
cutting
edge
integrated
circuit
costs
more
than
a
million
dollars
and
takes
several
months
Intel
s
infamous
FDIV
floating
point
division
bug
in
the
Pentium
processor
forced
the
company
to
recall
chips
after
they
had
shipped
at
a
total
cost
of
million
Logic
simulation
is
essential
to
test
a
system
before
it
is
built
Introduction
The
term
bug
predates
the
invention
of
the
computer
Thomas
Edison
called
the
little
faults
and
difficulties
with
his
inventions
bugs
in
The
first
real
computer
bug
was
a
moth
which
got
caught
between
the
relays
of
the
Harvard
Mark
II
electromechanical
computer
in
It
was
found
by
Grace
Hopper
who
logged
the
incident
along
with
the
moth
itself
and
the
comment
first
actual
case
of
bug
being
found
Source
Notebook
entry
courtesy
Naval
Historical
Center
US
Navy
photo
No
NII
KN
The
Institute
of
Electrical
and
Electronics
Engineers
IEEE
is
a
professional
society
responsible
for
many
computing
standards
including
WiFi
Ethernet
and
floating
point
numbers
see
Chapter
VHDL
VHDL
is
an
acronym
for
the
VHSIC
Hardware
Description
Language
VHSIC
is
in
turn
an
acronym
for
the
Very
High
Speed
Integrated
Circuits
program
of
the
US
Department
of
Defense
VHDL
was
originally
developed
in
by
the
Department
of
Defense
to
describe
the
structure
and
function
of
hardware
Its
roots
draw
from
the
Ada
programming
language
The
IEEE
standardized
it
in
IEEE
STD
and
has
updated
the
standard
several
times
since
The
language
was
first
envisioned
for
documentation
but
was
quickly
adopted
for
simulation
and
synthesis
Verilog
Verilog
was
developed
by
Gateway
Design
Automation
as
a
proprietary
language
for
logic
simulation
in
Gateway
was
acquired
by
Cadence
in
and
Verilog
was
made
an
open
standard
in
under
the
control
of
Open
Verilog
International
The
language
became
an
IEEE
standard
in
IEEE
STD
and
was
updated
in
Figure
shows
waveforms
from
a
simulation
of
the
previous
sillyfunction
module
demonstrating
that
the
module
works
correctly
y
is
TRUE
when
a
b
and
c
are
or
as
specified
by
the
Boolean
equation
Synthesis
Logic
synthesis
transforms
HDL
code
into
a
netlist
describing
the
hardware
e
g
the
logic
gates
and
the
wires
connecting
them
The
logic
synthesizer
might
perform
optimizations
to
reduce
the
amount
of
hardware
required
The
netlist
may
be
a
text
file
or
it
may
be
drawn
as
a
schematic
to
help
visualize
the
circuit
Figure
shows
the
results
of
synthesizing
the
sillyfunction
module
Notice
how
the
three
threeinput
AND
gates
are
simplified
into
two
two
input
AND
gates
as
we
discovered
in
Example
using
Boolean
algebra
Circuit
descriptions
in
HDL
resemble
code
in
a
programming
language
However
you
must
remember
that
the
code
is
intended
to
represent
hardware
Verilog
and
VHDL
are
rich
languages
with
many
commands
Not
all
of
these
commands
can
be
synthesized
into
hardware
For
example
a
command
to
print
results
on
the
screen
during
simulation
does
not
translate
into
hardware
Because
our
primary
interest
is
CHAPTER
FOUR
Hardware
Description
Languages
ns
a
Now
ns
b
c
y
ns
ns
Figure
Simulation
waveforms
un
y
un
y
y
c
y
b
a
Figure
Synthesized
circuit
The
simulation
was
performed
with
the
Xilinx
ISE
Simulator
which
is
part
of
the
Xilinx
ISE
software
The
simulator
was
selected
because
it
is
used
commercially
yet
is
freely
available
to
universities
Synthesis
was
performed
with
Synplify
Pro
from
Synplicity
The
tool
was
selected
because
it
is
the
leading
commercial
tool
for
synthesizing
HDL
to
field
programmable
gate
arrays
see
Section
and
because
it
is
available
inexpensively
for
universities
to
build
hardware
we
will
emphasize
a
synthesizable
subset
of
the
languages
Specifically
we
will
divide
HDL
code
into
synthesizable
modules
and
a
testbench
The
synthesizable
modules
describe
the
hardware
The
testbench
contains
code
to
apply
inputs
to
a
module
check
whether
the
output
results
are
correct
and
print
discrepancies
between
expected
and
actual
outputs
Testbench
code
is
intended
only
for
simulation
and
cannot
be
synthesized
One
of
the
most
common
mistakes
for
beginners
is
to
think
of
HDL
as
a
computer
program
rather
than
as
a
shorthand
for
describing
digital
hardware
If
you
don
t
know
approximately
what
hardware
your
HDL
should
synthesize
into
you
probably
won
t
like
what
you
get
You
might
create
far
more
hardware
than
is
necessary
or
you
might
write
code
that
simulates
correctly
but
cannot
be
implemented
in
hardware
Instead
think
of
your
system
in
terms
of
blocks
of
combinational
logic
registers
and
finite
state
machines
Sketch
these
blocks
on
paper
and
show
how
they
are
connected
before
you
start
writing
code
In
our
experience
the
best
way
to
learn
an
HDL
is
by
example
HDLs
have
specific
ways
of
describing
various
classes
of
logic
these
ways
are
called
idioms
This
chapter
will
teach
you
how
to
write
the
proper
HDL
idioms
for
each
type
of
block
and
then
how
to
put
the
blocks
together
to
produce
a
working
system
When
you
need
to
describe
a
particular
kind
of
hardware
look
for
a
similar
example
and
adapt
it
to
your
purpose
We
do
not
attempt
to
rigorously
define
all
the
syntax
of
the
HDLs
because
that
is
deathly
boring
and
because
it
tends
to
encourage
thinking
of
HDLs
as
programming
languages
not
shorthand
for
hardware
The
IEEE
Verilog
and
VHDL
specifications
and
numerous
dry
but
exhaustive
textbooks
contain
all
of
the
details
should
you
find
yourself
needing
more
information
on
a
particular
topic
See
Further
Readings
section
at
back
of
the
book
COMBINATIONAL
LOGIC
Recall
that
we
are
disciplining
ourselves
to
design
synchronous
sequential
circuits
which
consist
of
combinational
logic
and
registers
The
outputs
of
combinational
logic
depend
only
on
the
current
inputs
This
section
describes
how
to
write
behavioral
models
of
combinational
logic
with
HDLs
Bitwise
Operators
Bitwise
operators
act
on
single
bit
signals
or
on
multi
bit
busses
For
example
the
inv
module
in
HDL
Example
describes
four
inverters
connected
to
bit
busses
Combinational
Logic
The
endianness
of
a
bus
is
purely
arbitrary
See
the
sidebar
in
Section
for
the
origin
of
the
term
Indeed
endianness
is
also
irrelevant
to
this
example
because
a
bank
of
inverters
doesn
t
care
what
the
order
of
the
bits
are
Endianness
matters
only
for
operators
such
as
addition
where
the
sum
of
one
column
carries
over
into
the
next
Either
ordering
is
acceptable
as
long
as
it
is
used
consistently
We
will
consistently
use
the
little
endian
order
N
in
Verilog
and
N
downto
in
VHDL
for
an
N
bit
bus
After
each
code
example
in
this
chapter
is
a
schematic
produced
from
the
Verilog
code
by
the
Synplify
Pro
synthesis
tool
Figure
shows
that
the
inv
module
synthesizes
to
a
bank
of
four
inverters
indicated
by
the
inverter
symbol
labeled
y
The
bank
of
inverters
connects
to
bit
input
and
output
busses
Similar
hardware
is
produced
from
the
synthesized
VHDL
code
The
gates
module
in
HDL
Example
demonstrates
bitwise
operations
acting
on
bit
busses
for
other
basic
logic
functions
CHAPTER
FOUR
Hardware
Description
Languages
y
a
y
Figure
inv
synthesized
circuit
Verilog
module
inv
input
a
output
y
assign
y
a
endmodule
a
represents
a
bit
bus
The
bits
from
most
significant
to
least
significant
are
a
a
a
and
a
This
is
called
little
endian
order
because
the
least
significant
bit
has
the
smallest
bit
number
We
could
have
named
the
bus
a
in
which
case
a
would
have
been
the
most
significant
Or
we
could
have
used
a
in
which
case
the
bits
from
most
significant
to
least
significant
would
be
a
a
a
and
a
This
is
called
big
endian
order
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
inv
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
inv
is
begin
y
not
a
end
VHDL
uses
STD
LOGIC
VECTOR
to
indicate
busses
of
STD
LOGIC
STD
LOGIC
VECTOR
downto
represents
a
bit
bus
The
bits
from
most
significant
to
least
significant
are
and
This
is
called
little
endian
order
because
the
least
significant
bit
has
the
smallest
bit
number
We
could
have
declared
the
bus
to
be
STD
LOGIC
VECTOR
downto
in
which
case
bit
would
have
been
the
most
significant
Or
we
could
have
written
STD
LOGIC
VECTOR
to
in
which
case
the
bits
from
most
significant
to
least
significant
would
be
and
This
is
called
big
endian
order
HDL
Example
INVERTERS
Ch
Combinational
Logic
y
y
y
y
y
y
y
y
y
y
b
a
Figure
gates
synthesized
circuit
Verilog
module
gates
input
a
b
output
y
y
y
y
y
Five
different
two
input
logic
gates
acting
on
bit
busses
assign
y
a
b
AND
assign
y
a
b
OR
assign
y
a
b
XOR
assign
y
a
b
NAND
assign
y
a
b
NOR
endmodule
and
are
examples
of
Verilog
operators
whereas
a
b
and
y
are
operands
A
combination
of
operators
and
operands
such
as
a
b
or
a
b
is
called
an
expression
A
complete
command
such
as
assign
y
a
b
is
called
a
statement
assign
out
in
op
in
is
called
a
continuous
assignment
statement
Continuous
assignment
statements
end
with
a
semicolon
Anytime
the
inputs
on
the
right
side
of
the
in
a
continuous
assignment
statement
change
the
output
on
the
left
side
is
recomputed
Thus
continuous
assignment
statements
describe
combinational
logic
HDL
Example
LOGIC
GATES
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
gates
is
port
a
b
in
STD
LOGIC
VECTOR
downto
y
y
y
y
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
gates
is
begin
Five
different
two
input
logic
gates
acting
on
bit
busses
y
a
and
b
y
a
or
b
y
a
xor
b
y
a
nand
b
y
a
nor
b
end
not
xor
and
or
are
examples
of
VHDL
operators
whereas
a
b
and
y
are
operands
A
combination
of
operators
and
operands
such
as
a
and
b
or
a
nor
b
is
called
an
expression
A
complete
command
such
as
y
a
nand
b
is
called
a
statement
out
in
op
in
is
called
a
concurrent
signal
assignment
statement
VHDL
assignment
statements
end
with
a
semicolon
Anytime
the
inputs
on
the
right
side
of
the
in
a
concurrent
signal
assignment
statement
change
the
output
on
the
left
side
is
recomputed
Thus
concurrent
signal
assignment
statements
describe
combinational
logic
Chapter
qxd
Reduction
Operators
Reduction
operators
imply
a
multiple
input
gate
acting
on
a
single
bus
HDL
Example
describes
an
eight
input
AND
gate
with
inputs
a
a
a
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
Verilog
comments
are
just
like
those
in
C
or
Java
Comments
beginning
with
continue
possibly
across
multiple
lines
to
the
next
Comments
beginning
with
continue
to
the
end
of
the
line
Verilog
is
case
sensitive
y
and
Y
are
different
signals
in
Verilog
VHDL
VHDL
comments
begin
with
and
continue
to
the
end
of
the
line
Comments
spanning
multiple
lines
must
use
at
the
beginning
of
each
line
VHDL
is
not
case
sensitive
y
and
Y
are
the
same
signal
in
VHDL
However
other
tools
that
may
read
your
file
might
be
case
sensitive
leading
to
nasty
bugs
if
you
blithely
mix
upper
and
lower
case
Comments
and
White
Space
The
gates
example
showed
how
to
format
comments
Verilog
and
VHDL
are
not
picky
about
the
use
of
white
space
i
e
spaces
tabs
and
line
breaks
Nevertheless
proper
indenting
and
use
of
blank
lines
is
helpful
to
make
nontrivial
designs
readable
Be
consistent
in
your
use
of
capitalization
and
underscores
in
signal
and
module
names
Module
and
signal
names
must
not
begin
with
a
digit
Verilog
module
and
input
a
output
y
assign
y
a
a
is
much
easier
to
write
than
assign
y
a
a
a
a
a
a
a
a
endmodule
As
one
would
expect
and
reduction
operators
are
available
for
OR
XOR
NAND
and
NOR
as
well
Recall
that
a
multi
input
XOR
performs
parity
returning
TRUE
if
an
odd
number
of
inputs
are
TRUE
VHDL
VHDL
does
not
have
reduction
operators
Instead
it
provides
the
generate
command
see
Section
Alternatively
the
operation
can
be
written
explicitly
as
shown
below
library
IEEE
use
IEEE
STD
LOGIC
all
entity
and
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
end
architecture
synth
of
and
is
begin
y
a
and
a
and
a
and
a
and
a
and
a
and
a
and
a
end
HDL
Example
EIGHT
INPUT
AND
Chap
Conditional
Assignment
Conditional
assignments
select
the
output
from
among
alternatives
based
on
an
input
called
the
condition
HDL
Example
illustrates
a
multiplexer
using
conditional
assignment
Combinational
Logic
y
y
a
Figure
and
synthesized
circuit
Verilog
The
conditional
operator
chooses
based
on
a
first
expression
between
a
second
and
third
expression
The
first
expression
is
called
the
condition
If
the
condition
is
the
operator
chooses
the
second
expression
If
the
condition
is
the
operator
chooses
the
third
expression
is
especially
useful
for
describing
a
multiplexer
because
based
on
the
first
input
it
selects
between
two
others
The
following
code
demonstrates
the
idiom
for
a
multiplexer
with
bit
inputs
and
outputs
using
the
conditional
operator
module
mux
input
d
d
input
s
output
y
assign
y
s
d
d
endmodule
If
s
is
then
y
d
If
s
is
then
y
d
is
also
called
a
ternary
operator
because
it
takes
three
inputs
It
is
used
for
the
same
purpose
in
the
C
and
Java
programming
languages
VHDL
Conditional
signal
assignments
perform
different
operations
depending
on
some
condition
They
are
especially
useful
for
describing
a
multiplexer
For
example
a
multiplexer
can
use
conditional
signal
assignment
to
select
one
of
two
bit
inputs
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
mux
is
begin
y
d
when
s
else
d
end
The
conditional
signal
assignment
sets
y
to
d
if
s
is
Otherwise
it
sets
y
to
d
HDL
Example
MULTIPLEXER
y
y
s
d
d
Figure
mux
synthesized
circuit
Chapt
HDL
Example
shows
a
multiplexer
based
on
the
same
principle
as
the
multiplexer
in
HDL
Example
Figure
shows
the
schematic
for
the
multiplexer
produced
by
Synplify
Pro
The
software
uses
a
different
multiplexer
symbol
than
this
text
has
shown
so
far
The
multiplexer
has
multiple
data
d
and
one
hot
enable
e
inputs
When
one
of
the
enables
is
asserted
the
associated
data
is
passed
to
the
output
For
example
when
s
s
the
bottom
AND
gate
un
s
produces
a
enabling
the
bottom
input
of
the
multiplexer
and
causing
it
to
select
d
Internal
Variables
Often
it
is
convenient
to
break
a
complex
function
into
intermediate
steps
For
example
a
full
adder
which
will
be
described
in
Section
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
A
multiplexer
can
select
one
of
four
inputs
using
nested
conditional
operators
module
mux
input
d
d
d
d
input
s
output
y
assign
y
s
s
d
d
s
d
d
endmodule
If
s
is
then
the
multiplexer
chooses
the
first
expression
s
d
d
This
expression
in
turn
chooses
either
d
or
d
based
on
s
y
d
if
s
is
and
d
if
s
is
If
s
is
then
the
multiplexer
similarly
chooses
the
second
expression
which
gives
either
d
or
d
based
on
s
VHDL
A
multiplexer
can
select
one
of
four
inputs
using
multiple
else
clauses
in
the
conditional
signal
assignment
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synthl
of
mux
is
begin
y
d
when
s
else
d
when
s
else
d
when
s
else
d
end
VHDL
also
supports
selected
signal
assignment
statements
to
provide
a
shorthand
when
selecting
from
one
of
several
possibilities
This
is
analogous
to
using
a
case
statement
in
place
of
multiple
if
else
statements
in
some
programming
languages
The
multiplexer
can
be
rewritten
with
selected
signal
assignment
as
follows
architecture
synth
of
mux
is
begin
with
a
select
y
d
when
d
when
d
when
d
when
others
end
HDL
Example
MULTIPLEXER
Chapter
is
a
circuit
with
three
inputs
and
two
outputs
defined
by
the
following
equations
If
we
define
intermediate
signals
P
and
G
we
can
rewrite
the
full
adder
as
follows
P
and
G
are
called
internal
variables
because
they
are
neither
inputs
nor
outputs
but
are
used
only
internal
to
the
module
They
are
similar
to
local
variables
in
programming
languages
HDL
Example
shows
how
they
are
used
in
HDLs
HDL
assignment
statements
assign
in
Verilog
and
in
VHDL
take
place
concurrently
This
is
different
from
conventional
programming
languages
such
as
C
or
Java
in
which
statements
are
evaluated
in
the
order
in
which
they
are
written
In
a
conventional
language
it
is
CoutG
PCin
S
P
Cin
GAB
PA
B
Cout
AB
ACin
BCin
S
A
B
Cin
Combinational
Logic
un
s
un
s
un
s
un
s
y
e
d
e
d
e
d
e
d
y
s
d
d
d
d
Figure
mux
synthesized
circuit
Check
this
by
filling
out
the
truth
table
to
convince
yourself
it
is
correct
Chapter
important
that
S
P
Cin
comes
after
P
A
B
because
statements
are
executed
sequentially
In
an
HDL
the
order
does
not
matter
Like
hardware
HDL
assignment
statements
are
evaluated
any
time
the
inputs
signals
on
the
right
hand
side
change
their
value
regardless
of
the
order
in
which
the
assignment
statements
appear
in
a
module
Precedence
Notice
that
we
parenthesized
the
cout
computation
in
HDL
Example
to
define
the
order
of
operations
as
Cout
G
P
Cin
rather
than
Cout
G
P
Cin
If
we
had
not
used
parentheses
the
default
operation
order
is
defined
by
the
language
HDL
Example
specifies
operator
precedence
from
highest
to
lowest
for
each
language
The
tables
include
arithmetic
shift
and
comparison
operators
that
will
be
defined
in
Chapter
CHAPTER
FOUR
Hardware
Description
Languages
p
g
s
un
cout
cout
cout
s
cin
b
a
Figure
fulladder
synthesized
circuit
Verilog
In
Verilog
wires
are
used
to
represent
internal
variables
whose
values
are
defined
by
assign
statements
such
as
assign
p
a
b
Wires
technically
have
to
be
declared
only
for
multibit
busses
but
it
is
good
practice
to
include
them
for
all
internal
variables
their
declaration
could
have
been
omitted
in
this
example
module
fulladder
input
a
b
cin
output
s
cout
wire
p
g
assign
p
a
b
assign
g
a
b
assign
s
p
cin
assign
cout
g
p
cin
endmodule
VHDL
In
VHDL
signals
are
used
to
represent
internal
variables
whose
values
are
defined
by
concurrent
signal
assignment
statements
such
as
p
a
xor
b
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fulladder
is
port
a
b
cin
in
STD
LOGIC
s
cout
out
STD
LOGIC
end
architecture
synth
of
fulladder
is
signal
p
g
STD
LOGIC
begin
p
a
xor
b
g
a
and
b
s
p
xor
cin
cout
g
or
p
and
cin
end
HDL
Example
FULL
ADDER
Chapter
qxd
Combinational
Logic
Verilog
VHDL
HDL
Example
OPERATOR
PRECEDENCE
Table
VHDL
operator
precedence
Op
Meaning
not
NOT
mod
rem
MUL
DIV
MOD
REM
PLUS
MINUS
CONCATENATE
rol
ror
Rotate
srl
sll
Shift
logical
sra
sla
Shift
arithmetic
Comparison
and
or
nand
Logical
Operations
nor
xor
H
i
g
h
e
s
t
L
o
w
e
s
t
The
operator
precedence
for
Verilog
is
much
like
you
would
expect
in
other
programming
languages
In
particular
AND
has
precedence
over
OR
We
could
take
advantage
of
this
precedence
to
eliminate
the
parentheses
assign
cout
g
p
cin
Multiplication
has
precedence
over
addition
in
VHDL
as
you
would
expect
However
unlike
Verilog
all
of
the
logical
operations
and
or
etc
have
equal
precedence
unlike
what
one
might
expect
in
Boolean
algebra
Thus
parentheses
are
necessary
otherwise
cout
g
or
p
and
cin
would
be
interpreted
from
left
to
right
as
cout
g
or
p
and
cin
Numbers
Numbers
can
be
specified
in
a
variety
of
bases
Underscores
in
numbers
are
ignored
and
can
be
helpful
in
breaking
long
numbers
into
more
readable
chunks
HDL
Example
explains
how
numbers
are
written
in
each
language
Z
s
and
X
s
HDLs
use
z
to
indicate
a
floating
value
z
is
particularly
useful
for
describing
a
tristate
buffer
whose
output
floats
when
the
enable
is
Recall
from
Section
that
a
bus
can
be
driven
by
several
tristate
buffers
exactly
one
of
which
should
be
enabled
HDL
Example
shows
the
idiom
for
a
tristate
buffer
If
the
buffer
is
enabled
the
output
is
the
same
as
the
input
If
the
buffer
is
disabled
the
output
is
assigned
a
floating
value
z
Table
Verilog
operator
precedence
Op
Meaning
NOT
MUL
DIV
MOD
PLUS
MINUS
Logical
Left
Right
Shift
Arithmetic
Left
Right
Shift
Relative
Comparison
Equality
Comparison
AND
NAND
XOR
XNOR
OR
NOR
Conditional
H
i
g
h
e
s
t
L
o
w
e
s
t
Chapter
qxd
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
Verilog
numbers
can
specify
their
base
and
size
the
number
of
bits
used
to
represent
them
The
format
for
declaring
constants
is
N
Bvalue
where
N
is
the
size
in
bits
B
is
the
base
and
value
gives
the
value
For
example
h
indicates
a
bit
number
with
a
value
of
Verilog
supports
b
for
binary
base
o
for
octal
base
d
for
decimal
base
and
h
for
hexadecimal
base
If
the
base
is
omitted
the
base
defaults
to
decimal
If
the
size
is
not
given
the
number
is
assumed
to
have
as
many
bits
as
the
expression
in
which
it
is
being
used
Zeros
are
automatically
padded
on
the
front
of
the
number
to
bring
it
up
to
full
size
For
example
if
w
is
a
bit
bus
assign
w
b
gives
w
the
value
It
is
better
practice
to
explicitly
give
the
size
VHDL
In
VHDL
STD
LOGIC
numbers
are
written
in
binary
and
enclosed
in
single
quotes
and
indicate
logic
and
STD
LOGIC
VECTOR
numbers
are
written
in
binary
or
hexadecimal
and
enclosed
in
double
quotation
marks
The
base
is
binary
by
default
and
can
be
explicitly
defined
with
the
prefix
X
for
hexadecimal
or
B
for
binary
HDL
Example
NUMBERS
Table
Verilog
numbers
Numbers
Bits
Base
Val
Stored
b
b
b
b
d
o
hAB
Table
VHDL
numbers
Numbers
Bits
Base
Val
Stored
B
X
AB
Verilog
module
tristate
input
a
input
en
output
y
assign
y
en
a
bz
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
tristate
is
port
a
in
STD
LOGIC
VECTOR
downto
en
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
tristate
is
begin
y
ZZZZ
when
en
else
a
end
HDL
Example
TRISTATE
BUFFER
Chapte
Combinational
Logic
y
y
en
a
Figure
tristate
synthesized
circuit
Similarly
HDLs
use
x
to
indicate
an
invalid
logic
level
If
a
bus
is
simultaneously
driven
to
and
by
two
enabled
tristate
buffers
or
other
gates
the
result
is
x
indicating
contention
If
all
the
tristate
buffers
driving
a
bus
are
simultaneously
OFF
the
bus
will
float
indicated
by
z
At
the
start
of
simulation
state
nodes
such
as
flip
flop
outputs
are
initialized
to
an
unknown
state
x
in
Verilog
and
u
in
VHDL
This
is
helpful
to
track
errors
caused
by
forgetting
to
reset
a
flip
flop
before
its
output
is
used
If
a
gate
receives
a
floating
input
it
may
produce
an
x
output
when
it
can
t
determine
the
correct
output
value
Similarly
if
it
receives
an
illegal
or
uninitialized
input
it
may
produce
an
x
output
HDL
Example
shows
how
Verilog
and
VHDL
combine
these
different
signal
values
in
logic
gates
Verilog
Verilog
signal
values
are
z
and
x
Verilog
constants
starting
with
z
or
x
are
padded
with
leading
z
s
or
x
s
instead
of
s
to
reach
their
full
length
when
necessary
Table
shows
a
truth
table
for
an
AND
gate
using
all
four
possible
signal
values
Note
that
the
gate
can
sometimes
determine
the
output
despite
some
inputs
being
unknown
For
example
z
returns
because
the
output
of
an
AND
gate
is
always
if
either
input
is
Otherwise
floating
or
invalid
inputs
cause
invalid
outputs
displayed
as
x
in
Verilog
VHDL
VHDL
STD
LOGIC
signals
are
z
x
and
u
Table
shows
a
truth
table
for
an
AND
gate
using
all
five
possible
signal
values
Notice
that
the
gate
can
sometimes
determine
the
output
despite
some
inputs
being
unknown
For
example
and
z
returns
because
the
output
of
an
AND
gate
is
always
if
either
input
is
Otherwise
floating
or
invalid
inputs
cause
invalid
outputs
displayed
as
x
in
VHDL
Uninitialized
inputs
cause
uninitialized
outputs
displayed
as
u
in
VHDL
HDL
Example
TRUTH
TABLES
WITH
UNDEFINED
AND
FLOATING
INPUTS
Table
Verilog
AND
gate
truth
table
with
z
and
x
A
zx
B
xx
z
x
xx
x
x
xx
Table
VHDL
AND
gate
truth
table
with
z
x
and
u
AND
A
zxu
xxu
B
z
xxxu
x
xxxu
u
uuuu
Seeing
x
or
u
values
in
simulation
is
almost
always
an
indication
of
a
bug
or
bad
coding
practice
In
the
synthesized
circuit
this
corresponds
to
a
floating
gate
input
uninitialized
state
or
contention
The
x
or
u
may
be
interpreted
randomly
by
the
circuit
as
or
leading
to
unpredictable
behavior
Bit
Swizzling
Often
it
is
necessary
to
operate
on
a
subset
of
a
bus
or
to
concatenate
join
together
signals
to
form
busses
These
operations
are
collectively
known
as
bit
swizzling
In
HDL
Example
y
is
given
the
bit
value
c
c
d
d
d
c
using
bit
swizzling
operations
Delays
HDL
statements
may
be
associated
with
delays
specified
in
arbitrary
units
They
are
helpful
during
simulation
to
predict
how
fast
a
circuit
will
work
if
you
specify
meaningful
delays
and
also
for
debugging
purposes
to
understand
cause
and
effect
deducing
the
source
of
a
bad
output
is
tricky
if
all
signals
change
simultaneously
in
the
simulation
results
These
delays
are
ignored
during
synthesis
the
delay
of
a
gate
produced
by
the
synthesizer
depends
on
its
tpd
and
tcd
specifications
not
on
numbers
in
HDL
code
HDL
Example
adds
delays
to
the
original
function
from
HDL
Example
It
assumes
that
inverters
have
a
delay
of
ns
three
input
AND
gates
have
a
delay
of
ns
and
three
input
OR
gates
have
a
delay
of
ns
Figure
shows
the
simulation
waveforms
with
y
lagging
ns
after
the
inputs
Note
that
y
is
initially
unknown
at
the
beginning
of
the
simulation
y
abcabc
abc
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
assign
y
c
d
c
b
The
operator
is
used
to
concatenate
busses
d
indicates
three
copies
of
d
Don
t
confuse
the
bit
binary
constant
b
with
a
bus
named
b
Note
that
it
was
critical
to
specify
the
length
of
bits
in
the
constant
otherwise
it
would
have
had
an
unknown
number
of
leading
zeros
that
might
appear
in
the
middle
of
y
If
y
were
wider
than
bits
zeros
would
be
placed
in
the
most
significant
bits
VHDL
y
c
downto
d
d
d
c
The
operator
is
used
to
concatenate
busses
y
must
be
a
bit
STD
LOGIC
VECTOR
Do
not
confuse
with
the
and
operator
in
VHDL
HDL
Example
BIT
SWIZZLING
Chapt
Combinational
Logic
VHDL
Libraries
and
Types
This
section
may
be
skipped
by
Verilog
users
Unlike
Verilog
VHDL
enforces
a
strict
data
typing
system
that
can
protect
the
user
from
some
errors
but
that
is
also
clumsy
at
times
Despite
its
fundamental
importance
the
STD
LOGIC
type
is
not
built
into
VHDL
Instead
it
is
part
of
the
IEEE
STD
LOGIC
library
Thus
every
file
must
contain
the
library
statements
shown
in
the
previous
examples
Verilog
timescale
ns
ps
module
example
input
a
b
c
output
y
wire
ab
bb
cb
n
n
n
assign
ab
bb
cb
a
b
c
assign
n
ab
bb
cb
assign
n
a
bb
cb
assign
n
a
bb
c
assign
y
n
n
n
endmodule
Verilog
files
can
include
a
timescale
directive
that
indicates
the
value
of
each
time
unit
The
statement
is
of
the
form
timescale
unit
precision
In
this
file
each
unit
is
ns
and
the
simulation
has
ps
precision
If
no
timescale
directive
is
given
in
the
file
a
default
unit
and
precision
usually
ns
for
both
is
used
In
Verilog
a
symbol
is
used
to
indicate
the
number
of
units
of
delay
It
can
be
placed
in
assign
statements
as
well
as
non
blocking
and
blocking
assignments
which
will
be
discussed
in
Section
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
example
is
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
example
is
signal
ab
bb
cb
n
n
n
STD
LOGIC
begin
ab
not
a
after
ns
bb
not
b
after
ns
cb
not
c
after
ns
n
ab
and
bb
and
cb
after
ns
n
a
and
bb
and
cb
after
ns
n
a
and
bb
and
c
after
ns
y
n
or
n
or
n
after
ns
end
In
VHDL
the
after
clause
is
used
to
indicate
delay
The
units
in
this
case
are
specified
as
nanoseconds
HDL
Example
LOGIC
GATES
WITH
DELAYS
Figure
Example
simulation
waveforms
with
delays
from
the
ModelSim
simulator
Chapter
qxd
Moreover
IEEE
STD
LOGIC
lacks
basic
operations
such
as
addition
comparison
shifts
and
conversion
to
integers
for
the
STD
LOGIC
VECTOR
data
Most
CAD
vendors
have
adopted
yet
more
libraries
containing
these
functions
IEEE
STD
LOGIC
UNSIGNED
and
IEEE
STD
LOGIC
SIGNED
See
Section
for
a
discussion
of
unsigned
and
signed
numbers
and
examples
of
these
operations
VHDL
also
has
a
BOOLEAN
type
with
two
values
true
and
false
BOOLEAN
values
are
returned
by
comparisons
such
as
the
equality
comparison
s
and
are
used
in
conditional
statements
such
as
when
Despite
the
temptation
to
believe
a
BOOLEAN
true
value
should
be
equivalent
to
a
STD
LOGIC
and
BOOLEAN
false
should
mean
STD
LOGIC
these
types
are
not
interchangeable
Thus
the
following
code
is
illegal
y
d
when
s
else
d
q
state
S
Instead
we
must
write
y
d
when
s
else
d
q
when
state
S
else
Although
we
do
not
declare
any
signals
to
be
BOOLEAN
they
are
automatically
implied
by
comparisons
and
used
by
conditional
statements
Similarly
VHDL
has
an
INTEGER
type
that
represents
both
positive
and
negative
integers
Signals
of
type
INTEGER
span
at
least
the
values
to
Integer
values
are
used
as
indices
of
busses
For
example
in
the
statement
y
a
and
a
and
a
and
a
and
are
integers
serving
as
an
index
to
choose
bits
of
the
a
signal
We
cannot
directly
index
a
bus
with
a
STD
LOGIC
or
STD
LOGIC
VECTOR
signal
Instead
we
must
convert
the
signal
to
an
INTEGER
This
is
demonstrated
in
HDL
Example
for
an
multiplexer
that
selects
one
bit
from
a
vector
using
a
bit
index
The
CONV
INTEGER
function
is
defined
in
the
IEEE
STD
LOGIC
UNSIGNED
library
and
performs
the
conversion
from
STD
LOGIC
VECTOR
to
INTEGER
for
positive
unsigned
values
VHDL
is
also
strict
about
out
ports
being
exclusively
for
output
For
example
the
following
code
for
two
and
three
input
AND
gates
is
illegal
VHDL
because
v
is
an
output
and
is
also
used
to
compute
w
library
IEEE
use
IEEE
STD
LOGIC
all
entity
and
is
port
a
b
c
in
STD
LOGIC
v
w
out
STD
LOGIC
end
CHAPTER
FOUR
Hardware
Description
Languages
Chapter
architecture
synth
of
and
is
begin
v
a
and
b
w
v
and
c
end
VHDL
defines
a
special
port
type
buffer
to
solve
this
problem
A
signal
connected
to
a
buffer
port
behaves
as
an
output
but
may
also
be
used
within
the
module
The
corrected
entity
definition
follows
Verilog
does
not
have
this
limitation
and
does
not
require
buffer
ports
entity
and
is
port
a
b
c
in
STD
LOGIC
v
buffer
STD
LOGIC
w
out
STD
LOGIC
end
VHDL
supports
enumeration
types
as
an
abstract
way
of
representing
information
without
assigning
specific
binary
encodings
For
example
the
divide
by
FSM
described
in
Section
uses
three
states
We
can
give
the
states
names
using
the
enumeration
type
rather
than
referring
to
them
by
binary
values
This
is
powerful
because
it
allows
VHDL
to
search
for
the
best
state
encoding
during
synthesis
rather
than
depending
on
an
arbitrary
encoding
specified
by
the
user
type
statetype
is
S
S
S
signal
state
nextstate
statetype
STRUCTURAL
MODELING
The
previous
section
discussed
behavioral
modeling
describing
a
module
in
terms
of
the
relationships
between
inputs
and
outputs
This
section
Structural
Modeling
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
mux
is
port
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
end
architecture
synth
of
mux
is
begin
y
d
CONV
INTEGER
s
end
HDL
Example
MULTIPLEXER
WITH
TYPE
CONVERSION
Figure
follows
on
next
page
Cha
CHAPTER
FOUR
Hardware
Description
Languages
un
s
un
s
un
s
un
s
un
s
un
s
un
s
un
s
y
e
d
e
d
e
d
e
d
e
d
e
d
e
d
e
d
y
s
d
Figure
mux
synthesized
circuit
examines
structural
modeling
describing
a
module
in
terms
of
how
it
is
composed
of
simpler
modules
For
example
HDL
Example
shows
how
to
assemble
a
multiplexer
from
three
multiplexers
Each
copy
of
the
multiplexer
Structural
Modeling
v
w
w
v
c
b
a
Figure
and
synthesized
circuit
Verilog
module
mux
input
d
d
d
d
input
s
output
y
wire
low
high
mux
lowmux
d
d
s
low
mux
highmux
d
d
s
high
mux
finalmux
low
high
s
y
endmodule
The
three
mux
instances
are
called
lowmux
highmux
and
finalmux
The
mux
module
must
be
defined
elsewhere
in
the
Verilog
code
HDL
Example
STRUCTURAL
MODEL
OF
MULTIPLEXER
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
mux
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
signal
low
high
STD
LOGIC
VECTOR
downto
begin
lowmux
mux
port
map
d
d
s
low
highmux
mux
port
map
d
d
s
high
finalmux
mux
port
map
low
high
s
y
end
The
architecture
must
first
declare
the
mux
ports
using
the
component
declaration
statement
This
allows
VHDL
tools
to
check
that
the
component
you
wish
to
use
has
the
same
ports
as
the
entity
that
was
declared
somewhere
else
in
another
entity
statement
preventing
errors
caused
by
changing
the
entity
but
not
the
instance
However
component
declaration
makes
VHDL
code
rather
cumbersome
Note
that
this
architecture
of
mux
was
named
struct
whereas
architectures
of
modules
with
behavioral
descriptions
from
Section
were
named
synth
VHDL
allows
multiple
architectures
implementations
for
the
same
entity
the
architectures
are
distinguished
by
name
The
names
themselves
have
no
significance
to
the
CAD
tools
but
struct
and
synth
are
common
Synthesizable
VHDL
code
generally
contains
only
one
architecture
for
each
entity
so
we
will
not
discuss
the
VHDL
syntax
to
configure
which
architecture
is
used
when
multiple
architectures
are
defined
CHAPTER
FOUR
Hardware
Description
Languages
mux
lowmux
mux
highmux
mux
finalmux
y
s
d
d
d
d
s
d
d
y
s
d
d
y
s
d
d
y
Figure
mux
synthesized
circuit
Verilog
module
mux
input
d
d
input
s
output
y
tristate
t
d
s
y
tristate
t
d
s
y
endmodule
In
Verilog
expressions
such
as
s
are
permitted
in
the
port
list
for
an
instance
Arbitrarily
complicated
expressions
are
legal
but
discouraged
because
they
make
the
code
difficult
to
read
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
tristate
port
a
in
STD
LOGIC
VECTOR
downto
en
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
signal
sbar
STD
LOGIC
begin
sbar
not
s
t
tristate
port
map
d
sbar
y
t
tristate
port
map
d
s
y
end
In
VHDL
expressions
such
as
not
s
are
not
permitted
in
the
port
map
for
an
instance
Thus
sbar
must
be
defined
as
a
separate
signal
HDL
Example
STRUCTURAL
MODEL
OF
MULTIPLEXER
is
called
an
instance
Multiple
instances
of
the
same
module
are
distinguished
by
distinct
names
in
this
case
lowmux
highmux
and
finalmux
This
is
an
example
of
regularity
in
which
the
multiplexer
is
reused
many
times
HDL
Example
uses
structural
modeling
to
construct
a
multiplexer
from
a
pair
of
tristate
buffers
C
Structural
Modeling
HDL
Example
shows
how
modules
can
access
part
of
a
bus
An
bit
wide
multiplexer
is
built
using
two
of
the
bit
multiplexers
already
defined
operating
on
the
low
and
high
nibbles
of
the
byte
In
general
complex
systems
are
designed
hierarchically
The
overall
system
is
described
structurally
by
instantiating
its
major
components
Each
of
these
components
is
described
structurally
from
its
building
blocks
and
so
forth
recursively
until
the
pieces
are
simple
enough
to
describe
behaviorally
It
is
good
style
to
avoid
or
at
least
to
minimize
mixing
structural
and
behavioral
descriptions
within
a
single
module
tristate
t
tristate
t
y
s
d
d
en
a
y
en
a
y
Figure
mux
synthesized
circuit
Verilog
module
mux
input
d
d
input
s
output
y
mux
lsbmux
d
d
s
y
mux
msbmux
d
d
s
y
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
mux
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
begin
lsbmux
mux
port
map
d
downto
d
downto
s
y
downto
msbhmux
mux
port
map
d
downto
d
downto
s
y
downto
end
HDL
Example
ACCESSING
PARTS
OF
BUSSES
SEQUENTIAL
LOGIC
HDL
synthesizers
recognize
certain
idioms
and
turn
them
into
specific
sequential
circuits
Other
coding
styles
may
simulate
correctly
but
synthesize
into
circuits
with
blatant
or
subtle
errors
This
section
presents
the
proper
idioms
to
describe
registers
and
latches
Registers
The
vast
majority
of
modern
commercial
systems
are
built
with
registers
using
positive
edge
triggered
D
flip
flops
HDL
Example
shows
the
idiom
for
such
flip
flops
In
Verilog
always
statements
and
VHDL
process
statements
signals
keep
their
old
value
until
an
event
in
the
sensitivity
list
takes
place
that
explicitly
causes
them
to
change
Hence
such
code
with
appropriate
sensitivity
lists
can
be
used
to
describe
sequential
circuits
with
memory
For
example
the
flip
flop
includes
only
clk
in
the
sensitive
list
It
remembers
its
old
value
of
q
until
the
next
rising
edge
of
the
clk
even
if
d
changes
in
the
interim
In
contrast
Verilog
continuous
assignment
statements
assign
and
VHDL
concurrent
assignment
statements
are
reevaluated
anytime
any
of
the
inputs
on
the
right
hand
side
changes
Therefore
such
code
necessarily
describes
combinational
logic
CHAPTER
FOUR
Hardware
Description
Languages
mux
lsbmux
mux
msbmux
y
s
d
d
s
d
d
y
s
d
d
y
Figure
mux
synthesized
circuit
C
Sequential
Logic
d
q
clk
D
Q
Figure
flop
synthesized
circuit
Verilog
module
flop
input
clk
input
d
output
reg
q
always
posedge
clk
q
d
endmodule
A
Verilog
always
statement
is
written
in
the
form
always
sensitivity
list
statement
The
statement
is
executed
only
when
the
event
specified
in
the
sensitivity
list
occurs
In
this
example
the
statement
is
q
d
pronounced
q
gets
d
Hence
the
flip
flop
copies
d
to
q
on
the
positive
edge
of
the
clock
and
otherwise
remembers
the
old
state
of
q
is
called
a
nonblocking
assignment
Think
of
it
as
a
regular
sign
for
now
we
ll
return
to
the
more
subtle
points
in
Section
Note
that
is
used
instead
of
assign
inside
an
always
statement
All
signals
on
the
left
hand
side
of
or
in
an
always
statement
must
be
declared
as
reg
In
this
example
q
is
both
an
output
and
a
reg
so
it
is
declared
as
output
reg
q
Declaring
a
signal
as
reg
does
not
mean
the
signal
is
actually
the
output
of
a
register
All
it
means
is
that
the
signal
appears
on
the
left
hand
side
of
an
assignment
in
an
always
statement
We
will
see
later
examples
of
always
statements
describing
combinational
logic
in
which
the
output
is
declared
reg
but
does
not
come
from
a
flip
flop
HDL
Example
REGISTER
Resettable
Registers
When
simulation
begins
or
power
is
first
applied
to
a
circuit
the
output
of
a
flop
or
register
is
unknown
This
is
indicated
with
x
in
Verilog
and
u
in
VHDL
Generally
it
is
good
practice
to
use
resettable
registers
so
that
on
powerup
you
can
put
your
system
in
a
known
state
The
reset
may
be
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
flop
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
flop
is
begin
process
clk
begin
if
clk
event
and
clk
then
q
d
end
if
end
process
end
A
VHDL
process
is
written
in
the
form
process
sensitivity
list
begin
statement
end
process
The
statement
is
executed
when
any
of
the
variables
in
the
sensitivity
list
change
In
this
example
the
if
statement
is
executed
when
clk
changes
indicated
by
clk
event
If
the
change
is
a
rising
edge
clk
after
the
event
then
q
d
pronounced
q
gets
d
Hence
the
flip
flop
copies
d
to
q
on
the
positive
edge
of
the
clock
and
otherwise
remembers
the
old
state
of
q
An
alternative
VHDL
idiom
for
a
flip
flop
is
process
clk
begin
if
RISING
EDGE
clk
then
q
d
end
if
end
process
RISING
EDGE
clk
is
synonymous
with
clk
event
and
clk
Chapter
qx
either
asynchronous
or
synchronous
Recall
that
asynchronous
reset
occurs
immediately
whereas
synchronous
reset
clears
the
output
only
on
the
next
rising
edge
of
the
clock
HDL
Example
demonstrates
the
idioms
for
flip
flops
with
asynchronous
and
synchronous
resets
Note
that
distinguishing
synchronous
and
asynchronous
reset
in
a
schematic
can
be
difficult
The
schematic
produced
by
Synplify
Pro
places
asynchronous
reset
at
the
bottom
of
a
flip
flop
and
synchronous
reset
on
the
left
side
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
flopr
input
clk
input
reset
input
d
output
reg
q
asynchronous
reset
always
posedge
clk
posedge
reset
if
reset
q
b
else
q
d
endmodule
module
flopr
input
clk
input
reset
input
d
output
reg
q
synchronous
reset
always
posedge
clk
if
reset
q
b
else
q
d
endmodule
Multiple
signals
in
an
always
statement
sensitivity
list
are
separated
with
a
comma
or
the
word
or
Notice
that
posedge
reset
is
in
the
sensitivity
list
on
the
asynchronously
resettable
flop
but
not
on
the
synchronously
resettable
flop
Thus
the
asynchronously
resettable
flop
immediately
responds
to
a
rising
edge
on
reset
but
the
synchronously
resettable
flop
responds
to
reset
only
on
the
rising
edge
of
the
clock
Because
the
modules
have
the
same
name
flopr
you
may
include
only
one
or
the
other
in
your
design
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
flopr
is
port
clk
reset
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
asynchronous
of
flopr
is
begin
process
clk
reset
begin
if
reset
then
q
elsif
clk
event
and
clk
then
q
d
end
if
end
process
end
architecture
synchronous
of
flopr
is
begin
process
clk
begin
if
clk
event
and
clk
then
if
reset
then
q
else
q
d
end
if
end
if
end
process
end
Multiple
signals
in
a
process
sensitivity
list
are
separated
with
a
comma
Notice
that
reset
is
in
the
sensitivity
list
on
the
asynchronously
resettable
flop
but
not
on
the
synchronously
resettable
flop
Thus
the
asynchronously
resettable
flop
immediately
responds
to
a
rising
edge
on
reset
but
the
synchronously
resettable
flop
responds
to
reset
only
on
the
rising
edge
of
the
clock
Recall
that
the
state
of
a
flop
is
initialized
to
u
at
startup
during
VHDL
simulation
As
mentioned
earlier
the
name
of
the
architecture
asynchronous
or
synchronous
in
this
example
is
ignored
by
the
VHDL
tools
but
may
be
helpful
to
the
human
reading
the
code
Because
both
architectures
describe
the
entity
flopr
you
may
include
only
one
or
the
other
in
your
design
HDL
Example
RESETTABLE
REGISTER
Chapter
q
Sequential
Logic
Enabled
Registers
Enabled
registers
respond
to
the
clock
only
when
the
enable
is
asserted
HDL
Example
shows
an
asynchronously
resettable
enabled
register
that
retains
its
old
value
if
both
reset
and
en
are
FALSE
R
d
q
reset
clk
Q
D
a
d
q
reset
clk
Q
D
R
b
Figure
flopr
synthesized
circuit
a
asynchronous
reset
b
synchronous
reset
Verilog
module
flopenr
input
clk
input
reset
input
en
input
d
output
reg
q
asynchronous
reset
always
posedge
clk
posedge
reset
if
reset
q
b
else
if
en
q
d
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
flopenr
is
port
clk
reset
en
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
asynchronous
of
flopenr
is
asynchronous
reset
begin
process
clk
reset
begin
if
reset
then
q
elsif
clk
event
and
clk
then
if
en
then
q
d
end
if
end
if
end
process
end
HDL
Example
RESETTABLE
ENABLED
REGISTER
Chapter
Multiple
Registers
A
single
always
process
statement
can
be
used
to
describe
multiple
pieces
of
hardware
For
example
consider
the
synchronizer
from
Section
made
of
two
back
to
back
flip
flops
as
shown
in
Figure
HDL
Example
describes
the
synchronizer
On
the
rising
edge
of
clk
d
is
copied
to
n
At
the
same
time
n
is
copied
to
q
CHAPTER
FOUR
Hardware
Description
Languages
R
d
q
en
reset
clk
D
Q
E
Figure
flopenr
synthesized
circuit
CLK
CLK
D
N
Q
Figure
Synchronizer
circuit
Verilog
module
sync
input
clk
input
d
output
reg
q
reg
n
always
posedge
clk
begin
n
d
q
n
end
endmodule
n
must
be
declared
as
a
reg
because
it
is
an
internal
signal
used
on
the
left
hand
side
of
in
an
always
statement
Also
notice
that
the
begin
end
construct
is
necessary
because
multiple
statements
appear
in
the
always
statement
This
is
analogous
to
in
C
or
Java
The
begin
end
was
not
needed
in
the
flopr
example
because
if
else
counts
as
a
single
statement
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
sync
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
q
out
STD
LOGIC
end
architecture
good
of
sync
is
signal
n
STD
LOGIC
begin
process
clk
begin
if
clk
event
and
clk
then
n
d
q
n
end
if
end
process
end
n
must
be
declared
as
a
signal
because
it
is
an
internal
signal
used
in
the
module
HDL
Example
SYNCHRONIZER
n
q
d
q
clk
QD
QD
Figure
sync
synthesized
circuit
Chapte
More
Combinational
Logic
Latches
Recall
from
Section
that
a
D
latch
is
transparent
when
the
clock
is
HIGH
allowing
data
to
flow
from
input
to
output
The
latch
becomes
opaque
when
the
clock
is
LOW
retaining
its
old
state
HDL
Example
shows
the
idiom
for
a
D
latch
Not
all
synthesis
tools
support
latches
well
Unless
you
know
that
your
tool
does
support
latches
and
you
have
a
good
reason
to
use
them
avoid
them
and
use
edge
triggered
flip
flops
instead
Furthermore
take
care
that
your
HDL
does
not
imply
any
unintended
latches
something
that
is
easy
to
do
if
you
aren
t
attentive
Many
synthesis
tools
warn
you
when
a
latch
is
created
if
you
didn
t
expect
one
track
down
the
bug
in
your
HDL
MORE
COMBINATIONAL
LOGIC
In
Section
we
used
assignment
statements
to
describe
combinational
logic
behaviorally
Verilog
always
statements
and
VHDL
process
statements
are
used
to
describe
sequential
circuits
because
they
remember
the
old
state
when
no
new
state
is
prescribed
However
always
process
lat
q
q
d
clk
D
Q
C
Figure
latch
synthesized
circuit
Verilog
module
latch
input
clk
input
d
output
reg
q
always
clk
d
if
clk
q
d
endmodule
The
sensitivity
list
contains
both
clk
and
d
so
the
always
statement
evaluates
any
time
clk
or
d
changes
If
clk
is
HIGH
d
flows
through
to
q
q
must
be
declared
to
be
a
reg
because
it
appears
on
the
left
hand
side
of
in
an
always
statement
This
does
not
always
mean
that
q
is
the
output
of
a
register
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
latch
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
latch
is
begin
process
clk
d
begin
if
clk
then
q
d
end
if
end
process
end
The
sensitivity
list
contains
both
clk
and
d
so
the
process
evaluates
anytime
clk
or
d
changes
If
clk
is
HIGH
d
flows
through
to
q
HDL
Example
D
LATCH
Chap
statements
can
also
be
used
to
describe
combinational
logic
behaviorally
if
the
sensitivity
list
is
written
to
respond
to
changes
in
all
of
the
inputs
and
the
body
prescribes
the
output
value
for
every
possible
input
combination
HDL
Example
uses
always
process
statements
to
describe
a
bank
of
four
inverters
see
Figure
for
the
synthesized
circuit
HDLs
support
blocking
and
nonblocking
assignments
in
an
always
process
statement
A
group
of
blocking
assignments
are
evaluated
in
the
order
in
which
they
appear
in
the
code
just
as
one
would
expect
in
a
standard
programming
language
A
group
of
nonblocking
assignments
are
evaluated
concurrently
all
of
the
statements
are
evaluated
before
any
of
the
signals
on
the
left
hand
sides
are
updated
HDL
Example
defines
a
full
adder
using
intermediate
signals
p
and
g
to
compute
s
and
cout
It
produces
the
same
circuit
from
Figure
but
uses
always
process
statements
in
place
of
assignment
statements
These
two
examples
are
poor
applications
of
always
process
statements
for
modeling
combinational
logic
because
they
require
more
lines
than
the
equivalent
approach
with
assignment
statements
from
Section
Moreover
they
pose
the
risk
of
inadvertently
implying
sequential
logic
if
the
inputs
are
left
out
of
the
sensitivity
list
However
case
and
if
statements
are
convenient
for
modeling
more
complicated
combinational
logic
case
and
if
statements
must
appear
within
always
process
statements
and
are
examined
in
the
next
sections
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
inv
input
a
output
reg
y
always
y
a
endmodule
always
reevaluates
the
statements
inside
the
always
statement
any
time
any
of
the
signals
on
the
right
hand
side
of
or
inside
the
always
statement
change
Thus
is
a
safe
way
to
model
combinational
logic
In
this
particular
example
a
would
also
have
sufficed
The
in
the
always
statement
is
called
a
blocking
assignment
in
contrast
to
the
nonblocking
assignment
In
Verilog
it
is
good
practice
to
use
blocking
assignments
for
combinational
logic
and
nonblocking
assignments
for
sequential
logic
This
will
be
discussed
further
in
Section
Note
that
y
must
be
declared
as
reg
because
it
appears
on
the
left
hand
side
of
a
or
sign
in
an
always
statement
Nevertheless
y
is
the
output
of
combinational
logic
not
a
register
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
inv
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
proc
of
inv
is
begin
process
a
begin
y
not
a
end
process
end
The
begin
and
end
process
statements
are
required
in
VHDL
even
though
the
process
contains
only
one
assignment
HDL
Example
INVERTER
USING
always
process
Chapter
More
Combinational
Logic
Verilog
In
a
Verilog
always
statement
indicates
a
blocking
assignment
and
indicates
a
nonblocking
assignment
also
called
a
concurrent
assignment
Do
not
confuse
either
type
with
continuous
assignment
using
the
assign
statement
assign
statements
must
be
used
outside
always
statements
and
are
also
evaluated
concurrently
VHDL
In
a
VHDL
process
statement
indicates
a
blocking
assignment
and
indicates
a
nonblocking
assignment
also
called
a
concurrent
assignment
This
is
the
first
section
where
is
introduced
Nonblocking
assignments
are
made
to
outputs
and
to
signals
Blocking
assignments
are
made
to
variables
which
are
declared
in
process
statements
see
HDL
Example
can
also
appear
outside
process
statements
where
it
is
also
evaluated
concurrently
Verilog
module
fulladder
input
a
b
cin
output
reg
s
cout
reg
p
g
always
begin
p
a
b
blocking
g
a
b
blocking
s
p
cin
blocking
cout
g
p
cin
blocking
end
endmodule
In
this
case
an
a
b
cin
would
have
been
equivalent
to
However
is
better
because
it
avoids
common
mistakes
of
missing
signals
in
the
stimulus
list
For
reasons
that
will
be
discussed
in
Section
it
is
best
to
use
blocking
assignments
for
combinational
logic
This
example
uses
blocking
assignments
first
computing
p
then
g
then
s
and
finally
cout
Because
p
and
g
appear
on
the
left
hand
side
of
an
assignment
in
an
always
statement
they
must
be
declared
to
be
reg
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fulladder
is
port
a
b
cin
in
STD
LOGIC
s
cout
out
STD
LOGIC
end
architecture
synth
of
fulladder
is
begin
process
a
b
cin
variable
p
g
STD
LOGIC
begin
p
a
xor
b
blocking
g
a
and
b
blocking
s
p
xor
cin
cout
g
or
p
and
cin
end
process
end
The
process
sensitivity
list
must
include
a
b
and
cin
because
combinational
logic
should
respond
to
changes
of
any
input
For
reasons
that
will
be
discussed
in
Section
it
is
best
to
use
blocking
assignments
for
intermediate
variables
in
combinational
logic
This
example
uses
blocking
assignments
for
p
and
g
so
that
they
get
their
new
values
before
being
used
to
compute
s
and
cout
that
depend
on
them
Because
p
and
g
appear
on
the
left
hand
side
of
a
blocking
assignment
in
a
process
statement
they
must
be
declared
to
be
variable
rather
than
signal
The
variable
declaration
appears
before
the
begin
in
the
process
where
the
variable
is
used
HDL
Example
FULL
ADDER
USING
always
process
Chapter
qxd
Case
Statements
A
better
application
of
using
the
always
process
statement
for
combinational
logic
is
a
seven
segment
display
decoder
that
takes
advantage
of
the
case
statement
that
must
appear
inside
an
always
process
statement
As
you
might
have
noticed
in
Example
the
design
process
for
large
blocks
of
combinational
logic
is
tedious
and
prone
to
error
HDLs
offer
a
great
improvement
allowing
you
to
specify
the
function
at
a
higher
level
of
abstraction
and
then
automatically
synthesize
the
function
into
gates
HDL
Example
uses
case
statements
to
describe
a
seven
segment
display
decoder
based
on
its
truth
table
See
Example
for
a
description
of
the
seven
segment
display
decoder
The
case
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
sevenseg
input
data
output
reg
segments
always
case
data
abc
defg
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
default
segments
b
endcase
endmodule
The
case
statement
checks
the
value
of
data
When
data
is
the
statement
performs
the
action
after
the
colon
setting
segments
to
The
case
statement
similarly
checks
other
data
values
up
to
note
the
use
of
the
default
base
base
The
default
clause
is
a
convenient
way
to
define
the
output
for
all
cases
not
explicitly
listed
guaranteeing
combinational
logic
In
Verilog
case
statements
must
appear
inside
always
statements
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
seven
seg
decoder
is
port
data
in
STD
LOGIC
VECTOR
downto
segments
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
seven
seg
decoder
is
begin
process
data
begin
case
data
is
abcdefg
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
others
segments
end
case
end
process
end
The
case
statement
checks
the
value
of
data
When
data
is
the
statement
performs
the
action
after
the
setting
segments
to
The
case
statement
similarly
checks
other
data
values
up
to
note
the
use
of
X
for
hexadecimal
numbers
The
others
clause
is
a
convenient
way
to
define
the
output
for
all
cases
not
explicitly
listed
guaranteeing
combinational
logic
Unlike
Verilog
VHDL
supports
selected
signal
assignment
statements
see
HDL
Example
which
are
much
like
case
statements
but
can
appear
outside
processes
Thus
there
is
less
reason
to
use
processes
to
describe
combinational
logic
HDL
Example
SEVEN
SEGMENT
DISPLAY
DECODER
Chapter
qxd
PM
Pag
More
Combinational
Logic
statement
performs
different
actions
depending
on
the
value
of
its
input
A
case
statement
implies
combinational
logic
if
all
possible
input
combinations
are
defined
otherwise
it
implies
sequential
logic
because
the
output
will
keep
its
old
value
in
the
undefined
cases
Synplify
Pro
synthesizes
the
seven
segment
display
decoder
into
a
read
only
memory
ROM
containing
the
outputs
for
each
of
the
possible
inputs
ROMs
are
discussed
further
in
Section
If
the
default
or
others
clause
were
left
out
of
the
case
statement
the
decoder
would
have
remembered
its
previous
output
anytime
data
were
in
the
range
of
This
is
strange
behavior
for
hardware
Ordinary
decoders
are
also
commonly
written
with
case
statements
HDL
Example
describes
a
decoder
If
Statements
always
process
statements
may
also
contain
if
statements
The
if
statement
may
be
followed
by
an
else
statement
If
all
possible
input
rom
segments
data
segments
DOUT
A
Figure
sevenseg
synthesized
circuit
Verilog
module
decoder
input
a
output
reg
y
always
case
a
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
endcase
endmodule
No
default
statement
is
needed
because
all
cases
are
covered
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
decoder
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
decoder
is
begin
process
a
begin
case
a
is
when
y
when
y
when
y
when
y
when
y
when
y
when
y
when
y
end
case
end
process
end
No
others
clause
is
needed
because
all
cases
are
covered
HDL
Example
DECODER
Chapter
qxd
CHAPTER
FOUR
Hardware
Description
Languages
y
y
y
y
y
y
y
y
y
a
Figure
decoder
synthesized
circuit
More
Combinational
Logic
combinations
are
handled
the
statement
implies
combinational
logic
otherwise
it
produces
sequential
logic
like
the
latch
in
Section
HDL
Example
uses
if
statements
to
describe
a
priority
circuit
defined
in
Section
Recall
that
an
N
input
priority
circuit
sets
the
output
TRUE
that
corresponds
to
the
most
significant
input
that
is
TRUE
Verilog
casez
This
section
may
be
skipped
by
VHDL
users
Verilog
also
provides
the
casez
statement
to
describe
truth
tables
with
don
t
cares
indicated
with
in
the
casez
statement
HDL
Example
shows
how
to
describe
a
priority
circuit
with
casez
Synplify
Pro
synthesizes
a
slightly
different
circuit
for
this
module
shown
in
Figure
than
it
did
for
the
priority
circuit
in
Figure
However
the
circuits
are
logically
equivalent
Blocking
and
Nonblocking
Assignments
The
guidelines
on
page
explain
when
and
how
to
use
each
type
of
assignment
If
these
guidelines
are
not
followed
it
is
possible
to
write
Verilog
module
priority
input
a
output
reg
y
always
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
y
b
endmodule
In
Verilog
if
statements
must
appear
inside
of
always
statements
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
priority
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
priority
is
begin
process
a
begin
if
a
then
y
elsif
a
then
y
elsif
a
then
y
elsif
a
then
y
else
y
end
if
end
process
end
Unlike
Verilog
VHDL
supports
conditional
signal
assignment
statements
see
HDL
Example
which
are
much
like
if
statements
but
can
appear
outside
processes
Thus
there
is
less
reason
to
use
processes
to
describe
combinational
logic
Figure
follows
on
next
page
HDL
Example
PRIORITY
CIRCUIT
Chapter
qxd
code
that
appears
to
work
in
simulation
but
synthesizes
to
incorrect
hardware
The
optional
remainder
of
this
section
explains
the
principles
behind
the
guidelines
Combinational
Logic
The
full
adder
from
HDL
Example
is
correctly
modeled
using
blocking
assignments
This
section
explores
how
it
operates
and
how
it
would
differ
if
nonblocking
assignments
had
been
used
Imagine
that
a
b
and
cin
are
all
initially
p
g
s
and
cout
are
thus
as
well
At
some
time
a
changes
to
triggering
the
always
process
statement
The
four
blocking
assignments
evaluate
in
CHAPTER
FOUR
Hardware
Description
Languages
module
priority
casez
input
a
output
reg
y
always
casez
a
b
y
b
b
y
b
b
y
b
b
y
b
default
y
b
endcase
endmodule
HDL
Example
PRIORITY
CIRCUIT
USING
casez
y
un
y
un
y
y
y
y
a
Figure
priority
synthesized
circuit
See
figure
Chapt
More
Combinational
Logic
y
y
y
a
y
Figure
priority
casez
synthesized
circuit
Verilog
Use
always
posedge
clk
and
nonblocking
assignments
to
model
synchronous
sequential
logic
always
posedge
clk
begin
nl
d
nonblocking
q
nl
nonblocking
end
Use
continuous
assignments
to
model
simple
combinational
logic
assign
y
s
d
d
Use
always
and
blocking
assignments
to
model
more
complicated
combinational
logic
where
the
always
statement
is
helpful
always
begin
p
a
b
blocking
g
a
b
blocking
s
p
cin
cout
g
p
cin
end
Do
not
make
assignments
to
the
same
signal
in
more
than
one
always
statement
or
continuous
assignment
statement
VHDL
Use
process
clk
and
nonblocking
assignments
to
model
synchronous
sequential
logic
process
clk
begin
if
clk
event
and
clk
then
nl
d
nonblocking
q
nl
nonblocking
end
if
end
process
Use
concurrent
assignments
outside
process
statements
to
model
simple
combinational
logic
y
d
when
s
else
d
Use
process
in
in
to
model
more
complicated
combinational
logic
where
the
process
is
helpful
Use
blocking
assignments
for
internal
variables
process
a
b
cin
variable
p
g
STD
LOGIC
begin
p
a
xor
b
blocking
g
a
and
b
blocking
s
p
xor
cin
cout
g
or
p
and
cin
end
process
Do
not
make
assignments
to
the
same
variable
in
more
than
one
process
or
concurrent
assignment
statement
BLOCKING
AND
NONBLOCKING
ASSIGNMENT
GUIDELINES
Chapter
qxd
the
order
shown
here
In
the
VHDL
code
s
and
cout
are
assigned
concurrently
Note
that
p
and
g
get
their
new
values
before
s
and
cout
are
computed
because
of
the
blocking
assignments
This
is
important
because
we
want
to
compute
s
and
cout
using
the
new
values
of
p
and
g
p
g
s
cout
In
contrast
HDL
Example
illustrates
the
use
of
nonblocking
assignments
Now
consider
the
same
case
of
a
rising
from
to
while
b
and
cin
are
The
four
nonblocking
assignments
evaluate
concurrently
p
g
s
cout
Observe
that
s
is
computed
concurrently
with
p
and
hence
uses
the
old
value
of
p
not
the
new
value
Therefore
s
remains
rather
than
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
nonblocking
assignments
not
recommended
module
fulladder
input
a
b
cin
output
reg
s
cout
reg
p
g
always
begin
p
a
b
nonblocking
g
a
b
nonblocking
s
p
cin
cout
g
p
cin
end
endmodule
Because
p
and
g
appear
on
the
left
hand
side
of
an
assignment
in
an
always
statement
they
must
be
declared
to
be
reg
VHDL
nonblocking
assignments
not
recommended
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fulladder
is
port
a
b
cin
in
STD
LOGIC
s
cout
out
STD
LOGIC
end
architecture
nonblocking
of
fulladder
is
signal
p
g
STD
LOGIC
begin
process
a
b
cin
p
g
begin
p
a
xor
b
nonblocking
g
a
and
b
nonblocking
s
p
xor
cin
cout
g
or
p
and
cin
end
process
end
Because
p
and
g
appear
on
the
left
hand
side
of
a
nonblocking
assignment
in
a
process
statement
they
must
be
declared
to
be
signal
rather
than
variable
The
signal
declaration
appears
before
the
begin
in
the
architecture
not
the
process
HDL
Example
FULL
ADDER
USING
NONBLOCKING
ASSIGNMENTS
Chapter
qxd
More
Combinational
Logic
becoming
However
p
does
change
from
to
This
change
triggers
the
always
process
statement
to
evaluate
a
second
time
as
follows
p
g
s
cout
This
time
p
is
already
so
s
correctly
changes
to
The
nonblocking
assignments
eventually
reach
the
right
answer
but
the
always
process
statement
had
to
evaluate
twice
This
makes
simulation
slower
though
it
synthesizes
to
the
same
hardware
Another
drawback
of
nonblocking
assignments
in
modeling
combinational
logic
is
that
the
HDL
will
produce
the
wrong
result
if
you
forget
to
include
the
intermediate
variables
in
the
sensitivity
list
Worse
yet
some
synthesis
tools
will
synthesize
the
correct
hardware
even
when
a
faulty
sensitivity
list
causes
incorrect
simulation
This
leads
to
a
mismatch
between
the
simulation
results
and
what
the
hardware
actually
does
Sequential
Logic
The
synchronizer
from
HDL
Example
is
correctly
modeled
using
nonblocking
assignments
On
the
rising
edge
of
the
clock
d
is
copied
to
n
at
the
same
time
that
n
is
copied
to
q
so
the
code
properly
describes
two
registers
For
example
suppose
initially
that
d
n
and
q
On
the
rising
edge
of
the
clock
the
following
two
assignments
occur
concurrently
so
that
after
the
clock
edge
n
and
q
nl
d
q
n
HDL
Example
tries
to
describe
the
same
module
using
blocking
assignments
On
the
rising
edge
of
clk
d
is
copied
to
n
Then
this
new
value
of
n
is
copied
to
q
resulting
in
d
improperly
appearing
at
both
n
and
q
The
assignments
occur
one
after
the
other
so
that
after
the
clock
edge
q
n
n
d
q
n
Verilog
If
the
sensitivity
list
of
the
always
statement
in
HDL
Example
were
written
as
always
a
b
cin
rather
than
always
then
the
statement
would
not
reevaluate
when
p
or
g
changes
In
the
previous
example
s
would
be
incorrectly
left
at
not
VHDL
If
the
sensitivity
list
of
the
process
statement
in
HDL
Example
were
written
as
process
a
b
cin
rather
than
process
a
b
cin
p
g
then
the
statement
would
not
reevaluate
when
p
or
g
changes
In
the
previous
example
s
would
be
incorrectly
left
at
not
Chapter
qxd
Because
n
is
invisible
to
the
outside
world
and
does
not
influence
the
behavior
of
q
the
synthesizer
optimizes
it
away
entirely
as
shown
in
Figure
The
moral
of
this
illustration
is
to
exclusively
use
nonblocking
assignment
in
always
statements
when
modeling
sequential
logic
With
sufficient
cleverness
such
as
reversing
the
orders
of
the
assignments
you
could
make
blocking
assignments
work
correctly
but
blocking
assignments
offer
no
advantages
and
only
introduce
the
risk
of
unintended
behavior
Certain
sequential
circuits
will
not
work
with
blocking
assignments
no
matter
what
the
order
FINITE
STATE
MACHINES
Recall
that
a
finite
state
machine
FSM
consists
of
a
state
register
and
two
blocks
of
combinational
logic
to
compute
the
next
state
and
the
output
given
the
current
state
and
the
input
as
was
shown
in
Figure
HDL
descriptions
of
state
machines
are
correspondingly
divided
into
three
parts
to
model
the
state
register
the
next
state
logic
and
the
output
logic
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
Bad
implementation
using
blocking
assignments
module
syncbad
input
clk
input
d
output
reg
q
reg
n
always
posedge
clk
begin
n
d
blocking
q
n
blocking
end
endmodule
VHDL
Bad
implementation
using
blocking
assignment
library
IEEE
use
IEEE
STD
LOGIC
all
entity
syncbad
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
q
out
STD
LOGIC
end
architecture
bad
of
syncbad
is
begin
process
clk
variable
n
STD
LOGIC
begin
if
clk
event
and
clk
then
n
d
blocking
q
n
end
if
end
process
end
HDL
Example
BAD
SYNCHRONIZER
WITH
BLOCKING
ASSIGNMENTS
q
d
q
clk
D
Q
Figure
syncbad
synthesized
circuit
Chapt
Finite
State
Machines
HDL
Example
describes
the
divide
by
FSM
from
Section
It
provides
an
asynchronous
reset
to
initialize
the
FSM
The
state
register
uses
the
ordinary
idiom
for
flip
flops
The
next
state
and
output
logic
blocks
are
combinational
The
Synplify
Pro
Synthesis
tool
just
produces
a
block
diagram
and
state
transition
diagram
for
state
machines
it
does
not
show
the
logic
gates
or
the
inputs
and
outputs
on
the
arcs
and
states
Therefore
be
careful
that
you
have
specified
the
FSM
correctly
in
your
HDL
code
The
state
transition
diagram
in
Figure
for
the
divide
by
FSM
is
analogous
to
the
diagram
in
Figure
b
The
double
circle
indicates
that
Verilog
module
divideby
FSM
input
clk
input
reset
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
state
register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
next
state
logic
always
case
state
S
nextstate
S
S
nextstate
S
S
nextstate
S
default
nextstate
S
endcase
output
logic
assign
y
state
S
endmodule
The
parameter
statement
is
used
to
define
constants
within
a
module
Naming
the
states
with
parameters
is
not
required
but
it
makes
changing
state
encodings
much
easier
and
makes
the
code
more
readable
Notice
how
a
case
statement
is
used
to
define
the
state
transition
table
Because
the
next
state
logic
should
be
combinational
a
default
is
necessary
even
though
the
state
b
should
never
arise
The
output
y
is
when
the
state
is
S
The
equality
comparison
a
b
evaluates
to
if
a
equals
b
and
otherwise
The
inequality
comparison
a
b
does
the
inverse
evaluating
to
if
a
does
not
equal
b
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
divideby
FSM
is
port
clk
reset
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
divideby
FSM
is
type
statetype
is
S
S
S
signal
state
nextstate
statetype
begin
state
register
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
next
state
logic
nextstate
S
when
state
S
else
S
when
state
S
else
S
output
logic
y
when
state
S
else
end
This
example
defines
a
new
enumeration
data
type
statetype
with
three
possibilities
S
S
and
S
state
and
nextstate
are
statetype
signals
By
using
an
enumeration
instead
of
choosing
the
state
encoding
VHDL
frees
the
synthesizer
to
explore
various
state
encodings
to
choose
the
best
one
The
output
y
is
when
the
state
is
S
The
inequalitycomparison
uses
To
produce
an
output
of
when
the
state
is
anything
but
S
change
the
comparison
to
state
S
HDL
Example
DIVIDE
BY
FINITE
STATE
MACHINE
Notice
that
the
synthesis
tool
uses
a
bit
encoding
Q
instead
of
the
bit
encoding
suggested
in
the
Verilog
code
Chapter
qxd
S
is
the
reset
state
Gate
level
implementations
of
the
divide
by
FSM
were
shown
in
Section
If
for
some
reason
we
had
wanted
the
output
to
be
HIGH
in
states
S
and
S
the
output
logic
would
be
modified
as
follows
The
next
two
examples
describe
the
snail
pattern
recognizer
FSM
from
Section
The
code
shows
how
to
use
case
and
if
statements
to
handle
next
state
and
output
logic
that
depend
on
the
inputs
as
well
as
the
current
state
We
show
both
Moore
and
Mealy
modules
In
the
Moore
machine
HDL
Example
the
output
depends
only
on
the
current
state
whereas
in
the
Mealy
machine
HDL
Example
the
output
logic
depends
on
both
the
current
state
and
inputs
CHAPTER
FOUR
Hardware
Description
Languages
S
S
S
statemachine
state
y
reset
clk
C
Q
R
Figure
divideby
fsm
synthesized
circuit
Verilog
Output
Logic
assign
y
state
S
state
S
VHDL
output
logic
y
when
state
S
or
state
S
else
Chapter
Verilog
module
patternMoore
input
clk
input
reset
input
a
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
state
register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
next
state
logic
always
case
state
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
default
nextstate
S
endcase
output
logic
assign
y
state
S
endmodule
Note
how
nonblocking
assignments
are
used
in
the
state
register
to
describe
sequential
logic
whereas
blocking
assignments
are
used
in
the
next
state
logic
to
describe
combinational
logic
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
patternMoore
is
port
clk
reset
in
STD
LOGIC
a
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
patternMoore
is
type
statetype
is
S
S
S
S
S
signal
state
nextstate
statetype
begin
state
register
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
next
state
logic
process
state
a
begin
case
state
is
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
others
nextstate
S
end
case
end
process
output
logic
y
when
state
S
else
end
HDL
Example
PATTERN
RECOGNIZER
MOORE
FSM
S
S
S
S
S
statemachine
state
y
a
reset
clk
I
C
Q
R
Figure
patternMoore
synthesized
circuit
Chapter
qxd
PM
Page
CHAPTER
FOUR
Hardware
Description
Languages
S
S
S
S
statemachine
state
y
y
a
reset
clk
I
C
Q
R
Figure
patternMealy
synthesized
circuit
Verilog
module
patternMealy
input
clk
input
reset
input
a
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
state
register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
next
state
logic
always
case
state
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
default
nextstate
S
endcase
output
logic
assign
y
a
state
S
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
patternMealy
is
port
clk
reset
in
STD
LOGIC
a
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
patternMealy
is
type
statetype
is
S
S
S
S
signal
state
nextstate
statetype
begin
state
register
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
next
state
logic
process
state
a
begin
case
state
is
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
others
nextstate
S
end
case
end
process
output
logic
y
when
a
and
state
S
else
end
HDL
Example
PATTERN
RECOGNIZER
MEALY
FSM
Chapter
qxd
PM
Page
Parameterized
Modules
Verilog
module
mux
parameter
width
input
width
d
d
input
s
output
width
y
assign
y
s
d
d
endmodule
Verilog
allows
a
parameter
statement
before
the
inputs
and
outputs
to
define
parameters
The
parameter
statement
includes
a
default
value
of
the
parameter
width
The
number
of
bits
in
the
inputs
and
outputs
can
depend
on
this
parameter
module
mux
input
d
d
d
d
input
s
output
y
wire
low
hi
mux
lowmux
d
d
s
low
mux
himux
d
d
s
hi
mux
outmux
low
hi
s
y
endmodule
The
bit
multiplexer
instantiates
three
multiplexers
using
their
default
widths
In
contrast
a
bit
multiplexer
mux
would
need
to
override
the
default
width
using
before
the
instance
name
as
shown
below
module
mux
input
d
d
d
d
input
s
output
y
wire
low
hi
mux
lowmux
d
d
s
low
mux
himux
d
d
s
hi
mux
outmux
low
hi
s
y
endmodule
Do
not
confuse
the
use
of
the
sign
indicating
delays
with
the
use
of
in
defining
and
overriding
parameters
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
architecture
synth
of
mux
is
begin
y
d
when
s
else
d
end
The
generic
statement
includes
a
default
value
of
width
The
value
is
an
integer
entity
mux
is
port
d
d
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
mux
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
component
signal
low
hi
STD
LOGIC
VECTOR
downto
begin
lowmux
mux
port
map
d
d
s
low
himux
mux
port
map
d
d
s
hi
outmux
mux
port
map
low
hi
s
y
end
The
bit
multiplexer
mux
instantiates
three
multiplexers
using
their
default
widths
In
contrast
a
bit
multiplexer
mux
would
need
to
override
the
default
width
using
generic
map
as
shown
below
lowmux
mux
generic
map
port
map
d
d
s
low
himux
mux
generic
map
port
map
d
d
s
hi
outmux
mux
generic
map
port
map
low
hi
s
y
HDL
Example
PARAMETERIZED
N
BIT
MULTIPLEXERS
PARAMETERIZED
MODULES
So
far
all
of
our
modules
have
had
fixed
width
inputs
and
outputs
For
example
we
had
to
define
separate
modules
for
and
bit
wide
multiplexers
HDLs
permit
variable
bit
widths
using
parameterized
modules
Chapt
HDL
Example
declares
a
parameterized
multiplexer
with
a
default
width
of
then
uses
it
to
create
and
bit
multiplexers
HDL
Example
shows
a
decoder
which
is
an
even
better
application
of
parameterized
modules
A
large
N
N
decoder
is
cumbersome
to
CHAPTER
FOUR
Hardware
Description
Languages
mux
lowmux
mux
himux
mux
outmux
y
s
d
d
d
d
s
d
d
y
s
d
d
y
s
d
d
y
Figure
mux
synthesized
circuit
Verilog
module
decoder
parameter
N
input
N
a
output
reg
N
y
always
begin
y
y
a
end
endmodule
N
indicates
N
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
use
IEEE
STD
LOGIC
ARITH
all
entity
decoder
is
generic
N
integer
port
a
in
STD
LOGIC
VECTOR
N
downto
y
out
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
decoder
is
begin
process
a
variable
tmp
STD
LOGIC
VECTOR
N
downto
begin
tmp
CONV
STD
LOGIC
VECTOR
N
tmp
CONV
INTEGER
a
y
tmp
end
process
end
N
indicates
N
CONV
STD
LOGIC
VECTOR
N
produces
a
STD
LOGIC
VECTOR
of
length
N
containing
all
s
It
requires
the
STD
LOGIC
ARITH
library
The
function
is
useful
in
other
parameterized
functions
such
as
resettable
flip
flops
that
need
to
be
able
to
produce
constants
with
a
parameterized
number
of
bits
The
bit
index
in
VHDL
must
be
an
integer
so
the
CONV
INTEGER
function
is
used
to
convert
a
from
a
STD
LOGIC
VECTOR
to
an
integer
HDL
Example
PARAMETERIZED
N
N
DECODER
Chapter
Parameterized
Modules
specify
with
case
statements
but
easy
using
parameterized
code
that
simply
sets
the
appropriate
output
bit
to
Specifically
the
decoder
uses
blocking
assignments
to
set
all
the
bits
to
then
changes
the
appropriate
bit
to
HDLs
also
provide
generate
statements
to
produce
a
variable
amount
of
hardware
depending
on
the
value
of
a
parameter
generate
supports
for
loops
and
if
statements
to
determine
how
many
of
what
types
of
hardware
to
produce
HDL
Example
demonstrates
how
to
use
generate
statements
to
produce
an
N
input
AND
function
from
a
cascade
of
two
input
AND
gates
Use
generate
statements
with
caution
it
is
easy
to
produce
a
large
amount
of
hardware
unintentionally
Verilog
module
andN
parameter
width
input
width
a
output
y
genvar
i
wire
width
x
generate
for
i
i
width
ii
begin
forloop
if
i
assign
x
a
a
else
assign
x
i
a
i
x
i
end
endgenerate
assign
y
x
width
endmodule
The
for
statement
loops
through
i
width
to
produce
many
consecutive
AND
gates
The
begin
in
a
generate
for
loop
must
be
followed
by
a
and
an
arbitrary
label
forloop
in
this
case
Of
course
writing
assign
y
a
would
be
much
easier
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
andN
is
generic
width
integer
port
a
in
STD
LOGIC
VECTOR
width
downto
y
out
STD
LOGIC
end
architecture
synth
of
andN
is
signal
i
integer
signal
x
STD
LOGIC
VECTOR
width
downto
begin
AllBits
for
i
in
to
width
generate
LowBit
if
i
generate
A
x
a
and
a
end
generate
OtherBits
if
i
generate
Ai
x
i
a
i
and
x
i
end
generate
end
generate
y
x
width
end
HDL
Example
PARAMETERIZED
N
INPUT
AND
GATE
x
x
x
x
x
x
x
y
a
Figure
andN
synthesized
circuit
Chapter
qxd
TESTBENCHES
A
testbench
is
an
HDL
module
that
is
used
to
test
another
module
called
the
device
under
test
DUT
The
testbench
contains
statements
to
apply
inputs
to
the
DUT
and
ideally
to
check
that
the
correct
outputs
are
produced
The
input
and
desired
output
patterns
are
called
test
vectors
Consider
testing
the
sillyfunction
module
from
Section
that
computes
This
is
a
simple
module
so
we
can
perform
exhaustive
testing
by
applying
all
eight
possible
test
vectors
HDL
Example
demonstrates
a
simple
testbench
It
instantiates
the
DUT
then
applies
the
inputs
Blocking
assignments
and
delays
are
used
to
apply
the
inputs
in
the
appropriate
order
The
user
must
view
the
results
of
the
simulation
and
verify
by
inspection
that
the
correct
outputs
are
produced
Testbenches
are
simulated
the
same
as
other
HDL
modules
However
they
are
not
synthesizeable
y
a
b
c
ab
c
abc
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
testbench
reg
a
b
c
wire
y
instantiate
device
under
test
sillyfunction
dut
a
b
c
y
apply
inputs
one
at
a
time
initial
begin
a
b
c
c
b
c
c
a
b
c
c
b
c
c
end
endmodule
The
initial
statement
executes
the
statements
in
its
body
at
the
start
of
simulation
In
this
case
it
first
applies
the
input
pattern
and
waits
for
time
units
It
then
applies
and
waits
more
units
and
so
forth
until
all
eight
possible
inputs
have
been
applied
initial
statements
should
be
used
only
in
testbenches
for
simulation
not
in
modules
intended
to
be
synthesized
into
actual
hardware
Hardware
has
no
way
of
magically
executing
a
sequence
of
special
steps
when
it
is
first
turned
on
Like
signals
in
always
statements
signals
in
initial
statements
must
be
declared
to
be
reg
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
testbench
is
no
inputs
or
outputs
end
architecture
sim
of
testbench
is
component
sillyfunction
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
component
signal
a
b
c
y
STD
LOGIC
begin
instantiate
device
under
test
dut
sillyfunction
port
map
a
b
c
y
apply
inputs
one
at
a
time
process
begin
a
b
c
wait
for
ns
c
wait
for
ns
b
c
wait
for
ns
c
wait
for
ns
a
b
c
wait
for
ns
c
wait
for
ns
b
c
wait
for
ns
c
wait
for
ns
wait
wait
forever
end
process
end
The
process
statement
first
applies
the
input
pattern
and
waits
for
ns
It
then
applies
and
waits
more
ns
and
so
forth
until
all
eight
possible
inputs
have
been
applied
At
the
end
the
process
waits
indefinitely
otherwise
the
process
would
begin
again
repeatedly
applying
the
pattern
of
test
vectors
HDL
Example
TESTBENCH
Some
tools
also
call
the
module
to
be
tested
the
unit
under
test
UUT
Chapter
qxd
PM
Testbenches
Checking
for
correct
outputs
is
tedious
and
error
prone
Moreover
determining
the
correct
outputs
is
much
easier
when
the
design
is
fresh
in
your
mind
if
you
make
minor
changes
and
need
to
retest
weeks
later
determining
the
correct
outputs
becomes
a
hassle
A
much
better
approach
is
to
write
a
self
checking
testbench
shown
in
HDL
Example
Writing
code
for
each
test
vector
also
becomes
tedious
especially
for
modules
that
require
a
large
number
of
vectors
An
even
better
approach
is
to
place
the
test
vectors
in
a
separate
file
The
testbench
simply
reads
the
test
vectors
from
the
file
applies
the
input
test
vector
to
the
DUT
waits
checks
that
the
output
values
from
the
DUT
match
the
output
vector
and
repeats
until
reaching
the
end
of
the
test
vectors
file
Verilog
module
testbench
reg
a
b
c
wire
y
instantiate
device
under
test
sillyfunction
dut
a
b
c
y
apply
inputs
one
at
a
time
checking
results
initial
begin
a
b
c
if
y
display
failed
c
if
y
display
failed
b
c
if
y
display
failed
c
if
y
display
failed
a
b
c
if
y
display
failed
c
if
y
display
failed
b
c
if
y
display
failed
c
if
y
display
failed
end
endmodule
This
module
checks
y
against
expectations
after
each
input
test
vector
is
applied
In
Verilog
comparison
using
or
is
effective
between
signals
that
do
not
take
on
the
values
of
x
and
z
Testbenches
use
the
and
operators
for
comparisons
of
equality
and
inequality
respectively
because
these
operators
work
correctly
with
operands
that
could
be
x
or
z
It
uses
the
display
system
task
to
print
a
message
on
the
simulator
console
if
an
error
occurs
display
is
meaningful
only
in
simulation
not
synthesis
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
testbench
is
no
inputs
or
outputs
end
architecture
sim
of
testbench
is
component
sillyfunction
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
component
signal
a
b
c
y
STD
LOGIC
begin
instantiate
device
under
test
dut
sillyfunction
port
map
a
b
c
y
apply
inputs
one
at
a
time
checking
results
process
begin
a
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
a
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
wait
wait
forever
end
process
end
The
assert
statement
checks
a
condition
and
prints
the
message
given
in
the
report
clause
if
the
condition
is
not
satisfied
assert
is
meaningful
only
in
simulation
not
in
synthesis
HDL
Example
SELF
CHECKING
TESTBENCH
Chapter
qxd
PM
Page
HDL
Example
demonstrates
such
a
testbench
The
testbench
generates
a
clock
using
an
always
process
statement
with
no
stimulus
list
so
that
it
is
continuously
reevaluated
At
the
beginning
of
the
simulation
it
reads
the
test
vectors
from
a
text
file
and
pulses
reset
for
two
cycles
example
tv
is
a
text
file
containing
the
inputs
and
expected
output
written
in
binary
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
testbench
reg
clk
reset
reg
a
b
c
yexpected
wire
y
reg
vectornum
errors
reg
testvectors
instantiate
device
under
test
sillyfunction
dut
a
b
c
y
generate
clock
always
begin
clk
clk
end
at
start
of
test
load
vectors
and
pulse
reset
initial
begin
readmemb
example
tv
testvectors
vectornum
errors
reset
reset
end
apply
test
vectors
on
rising
edge
of
clk
always
posedge
clk
begin
a
b
c
yexpected
testvectors
vectornum
end
check
results
on
falling
edge
of
clk
always
negedge
clk
if
reset
begin
skip
during
reset
if
y
yexpected
begin
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
STD
TEXTIO
all
entity
testbench
is
no
inputs
or
outputs
end
architecture
sim
of
testbench
is
component
sillyfunction
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
component
signal
a
b
c
y
STD
LOGIC
signal
clk
reset
STD
LOGIC
signal
yexpected
STD
LOGIC
constant
MEMSIZE
integer
type
tvarray
is
array
MEMSIZE
downto
of
STD
LOGIC
VECTOR
downto
signal
testvectors
tvarray
shared
variable
vectornum
errors
integer
begin
instantiate
device
under
test
dut
sillyfunction
port
map
a
b
c
y
generate
clock
process
begin
clk
wait
for
ns
clk
wait
for
ns
end
process
at
start
of
test
load
vectors
and
pulse
reset
process
is
file
tv
TEXT
variable
i
j
integer
variable
L
line
variable
ch
character
HDL
Example
TESTBENCH
WITH
TEST
VECTOR
FILE
Chapter
q
Testbenches
display
Error
inputs
b
a
b
c
display
outputs
b
b
expected
y
yexpected
errors
errors
end
vectornum
vectornum
if
testvectors
vectornum
bx
begin
display
d
tests
completed
with
d
errors
vectornum
errors
finish
end
end
endmodule
readmemb
reads
a
file
of
binary
numbers
into
the
testvectors
array
readmemh
is
similar
but
reads
a
file
of
hexadecimal
numbers
The
next
block
of
code
waits
one
time
unit
after
the
rising
edge
of
the
clock
to
avoid
any
confusion
if
clock
and
data
change
simultaneously
then
sets
the
three
inputs
and
the
expected
output
based
on
the
four
bits
in
the
current
test
vector
The
next
block
of
code
checks
the
output
of
the
DUT
at
the
negative
edge
of
the
clock
after
the
inputs
have
had
time
to
propagate
through
the
DUT
to
produce
the
output
y
The
testbench
compares
the
generated
output
y
with
the
expected
output
yexpected
and
prints
an
error
if
they
don
t
match
b
and
d
indicate
to
print
the
values
in
binary
and
decimal
respectively
For
example
display
b
b
y
yexpected
prints
the
two
values
y
and
yexpected
in
binary
h
prints
a
value
in
hexadecimal
This
process
repeats
until
there
are
no
more
valid
test
vectors
in
the
testvectors
array
finish
terminates
the
simulation
Note
that
even
though
the
Verilog
module
supports
up
to
test
vectors
it
will
terminate
the
simulation
after
executing
the
eight
vectors
in
the
file
begin
read
file
of
test
vectors
i
FILE
OPEN
tv
example
tv
READ
MODE
while
not
endfile
tv
loop
readline
tv
L
for
j
in
to
loop
read
L
ch
if
ch
then
read
L
ch
end
if
if
ch
then
testvectors
i
j
else
testvectors
i
j
end
if
end
loop
i
i
end
loop
vectornum
errors
reset
wait
for
ns
reset
wait
end
process
apply
test
vectors
on
rising
edge
of
clk
process
clk
begin
if
clk
event
and
clk
then
a
testvectors
vectornum
after
ns
b
testvectors
vectornum
after
ns
c
testvectors
vectornum
after
ns
yexpected
testvectors
vectornum
after
ns
end
if
end
process
check
results
on
falling
edge
of
clk
process
clk
begin
if
clk
event
and
clk
and
reset
then
assert
y
yexpected
report
Error
y
STD
LOGIC
image
y
if
y
yexpected
then
errors
errors
end
if
vectornum
vectornum
if
is
x
testvectors
vectornum
then
if
errors
then
report
Just
kidding
integer
image
vectornum
tests
completed
successfully
severity
failure
else
report
integer
image
vectornum
tests
completed
errors
integer
image
errors
severity
failure
end
if
end
if
end
if
end
process
end
The
VHDL
code
is
rather
ungainly
and
uses
file
reading
commands
beyond
the
scope
of
this
chapter
but
it
gives
the
sense
of
what
a
self
checking
testbench
looks
like
Chapter
qxd
PM
Page
CHAPTER
FOUR
Hardware
Description
Languages
New
inputs
are
applied
on
the
rising
edge
of
the
clock
and
the
output
is
checked
on
the
falling
edge
of
the
clock
This
clock
and
reset
would
also
be
provided
to
the
DUT
if
sequential
logic
were
being
tested
Errors
are
reported
as
they
occur
At
the
end
of
the
simulation
the
testbench
prints
the
total
number
of
test
vectors
applied
and
the
number
of
errors
detected
The
testbench
in
HDL
Example
is
overkill
for
such
a
simple
circuit
However
it
can
easily
be
modified
to
test
more
complex
circuits
by
changing
the
example
tv
file
instantiating
the
new
DUT
and
changing
a
few
lines
of
code
to
set
the
inputs
and
check
the
outputs
SUMMARY
Hardware
description
languages
HDLs
are
extremely
important
tools
for
modern
digital
designers
Once
you
have
learned
Verilog
or
VHDL
you
will
be
able
to
specify
digital
systems
much
faster
than
if
you
had
to
draw
the
complete
schematics
The
debug
cycle
is
also
often
much
faster
because
modifications
require
code
changes
instead
of
tedious
schematic
rewiring
However
the
debug
cycle
can
be
much
longer
using
HDLs
if
you
don
t
have
a
good
idea
of
the
hardware
your
code
implies
HDLs
are
used
for
both
simulation
and
synthesis
Logic
simulation
is
a
powerful
way
to
test
a
system
on
a
computer
before
it
is
turned
into
hardware
Simulators
let
you
check
the
values
of
signals
inside
your
system
that
might
be
impossible
to
measure
on
a
physical
piece
of
hardware
Logic
synthesis
converts
the
HDL
code
into
digital
logic
circuits
The
most
important
thing
to
remember
when
you
are
writing
HDL
code
is
that
you
are
describing
real
hardware
not
writing
a
computer
program
The
most
common
beginner
s
mistake
is
to
write
HDL
code
without
thinking
about
the
hardware
you
intend
to
produce
If
you
don
t
know
what
hardware
you
are
implying
you
are
almost
certain
not
to
get
what
you
want
Instead
begin
by
sketching
a
block
diagram
of
your
system
identifying
which
portions
are
combinational
logic
which
portions
are
sequential
circuits
or
finite
state
machines
and
so
forth
Then
write
HDL
code
for
each
portion
using
the
correct
idioms
to
imply
the
kind
of
hardware
you
need
Exercises
Exercises
The
following
exercises
may
be
done
using
your
favorite
HDL
If
you
have
a
simulator
available
test
your
design
Print
the
waveforms
and
explain
how
they
prove
that
it
works
If
you
have
a
synthesizer
available
synthesize
your
code
Print
the
generated
circuit
diagram
and
explain
why
it
matches
your
expectations
Exercise
Sketch
a
schematic
of
the
circuit
described
by
the
following
HDL
code
Simplify
the
schematic
so
that
it
shows
a
minimum
number
of
gates
Exercise
Sketch
a
schematic
of
the
circuit
described
by
the
following
HDL
code
Simplify
the
schematic
so
that
it
shows
a
minimum
number
of
gates
Exercise
Write
an
HDL
module
that
computes
a
four
input
XOR
function
The
input
is
a
and
the
output
is
y
Verilog
module
exercise
input
a
b
c
output
y
z
assign
y
a
b
c
a
b
c
a
b
c
assign
z
a
b
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
exercise
is
port
a
b
c
in
STD
LOGIC
y
z
out
STD
LOGIC
end
architecture
synth
of
exercisel
is
begin
y
a
and
b
and
c
or
a
and
b
and
not
c
or
a
and
not
b
and
c
z
a
and
b
or
not
a
and
not
b
end
Verilog
module
exercise
input
a
output
reg
y
always
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
y
a
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
exercise
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
exercise
is
begin
process
a
begin
if
a
then
y
elsif
a
then
y
elsif
a
then
y
elsif
a
then
y
else
y
a
downto
end
if
end
process
end
Chapter
qxd
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Write
a
self
checking
testbench
for
Exercise
Create
a
test
vector
file
containing
all
test
cases
Simulate
the
circuit
and
show
that
it
works
Introduce
an
error
in
the
test
vector
file
and
show
that
the
testbench
reports
a
mismatch
Exercise
Write
an
HDL
module
called
minority
It
receives
three
inputs
a
b
and
c
It
produces
one
output
y
that
is
TRUE
if
at
least
two
of
the
inputs
are
FALSE
Exercise
Write
an
HDL
module
for
a
hexadecimal
seven
segment
display
decoder
The
decoder
should
handle
the
digits
A
B
C
D
E
and
F
as
well
as
Exercise
Write
a
self
checking
testbench
for
Exercise
Create
a
test
vector
file
containing
all
test
cases
Simulate
the
circuit
and
show
that
it
works
Introduce
an
error
in
the
test
vector
file
and
show
that
the
testbench
reports
a
mismatch
Exercise
Write
an
multiplexer
module
called
mux
with
inputs
s
d
d
d
d
d
d
d
d
and
output
y
Exercise
Write
a
structural
module
to
compute
the
logic
function
using
multiplexer
logic
Use
the
multiplexer
from
Exercise
Exercise
Repeat
Exercise
using
a
multiplexer
and
as
many
NOT
gates
as
you
need
Exercise
Section
pointed
out
that
a
synchronizer
could
be
correctly
described
with
blocking
assignments
if
the
assignments
were
given
in
the
proper
order
Think
of
a
simple
sequential
circuit
that
cannot
be
correctly
described
with
blocking
assignments
regardless
of
order
Exercise
Write
an
HDL
module
for
an
eight
input
priority
circuit
Exercise
Write
an
HDL
module
for
a
decoder
Exercise
Write
an
HDL
module
for
a
decoder
using
three
instances
of
the
decoders
from
Exercise
and
a
bunch
of
three
input
AND
gates
Exercise
Write
HDL
modules
that
implement
the
Boolean
equations
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
circuit
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
logic
function
from
Exercise
Pay
careful
attention
to
how
you
handle
don
t
cares
y
abb
cabc
Cha
Exercises
Exercise
Write
an
HDL
module
that
implements
the
functions
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
priority
encoder
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
binary
to
thermometer
code
converter
from
Exercise
Exercise
Write
an
HDL
module
implementing
the
days
in
month
function
from
Question
Exercise
Sketch
the
state
transition
diagram
for
the
FSM
described
by
the
following
HDL
code
Verilog
module
fsm
input
clk
reset
input
a
b
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
always
case
state
S
if
a
b
nextstate
S
else
nextstate
S
S
if
a
b
nextstate
S
else
nextstate
S
S
if
a
b
nextstate
S
else
nextstate
S
S
if
a
b
nextstate
S
else
nextstate
S
endcase
assign
y
state
S
state
S
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fsm
is
port
clk
reset
in
STD
LOGIC
a
b
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
fsm
is
type
statetype
is
S
S
S
S
signal
state
nextstate
statetype
begin
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
process
state
a
b
begin
case
state
is
when
S
if
a
xor
b
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
and
b
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
or
b
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
or
b
then
nextstate
S
else
nextstate
S
end
if
end
case
end
process
y
when
state
S
or
state
S
else
end
Chapter
qxd
PM
Page
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Sketch
the
state
transition
diagram
for
the
FSM
described
by
the
following
HDL
code
An
FSM
of
this
nature
is
used
in
a
branch
predictor
on
some
microprocessors
Verilog
module
fsm
input
clk
reset
input
taken
back
output
predicttaken
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
always
case
state
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
default
nextstate
S
endcase
assign
predicttaken
state
S
state
S
state
S
back
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fsm
is
port
clk
reset
in
STD
LOGIC
taken
back
in
STD
LOGIC
predicttaken
out
STD
LOGIC
end
architecture
synth
of
fsm
is
type
statetype
is
S
S
S
S
S
signal
state
nextstate
statetype
begin
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
process
state
taken
begin
case
state
is
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
others
nextstate
S
end
case
end
process
output
logic
predicttaken
when
state
S
or
state
S
or
state
S
and
back
else
end
Chapter
qxd
PM
Page
Exercises
Exercise
Write
an
HDL
module
for
an
SR
latch
Exercise
Write
an
HDL
module
for
a
JK
flip
flop
The
flip
flop
has
inputs
clk
J
and
K
and
output
Q
On
the
rising
edge
of
the
clock
Q
keeps
its
old
value
if
J
K
It
sets
Q
to
if
J
resets
Q
to
if
K
and
inverts
Q
if
J
K
Exercise
Write
an
HDL
module
for
the
latch
from
Figure
Use
one
assignment
statement
for
each
gate
Specify
delays
of
unit
or
ns
to
each
gate
Simulate
the
latch
and
show
that
it
operates
correctly
Then
increase
the
inverter
delay
How
long
does
the
delay
have
to
be
before
a
race
condition
causes
the
latch
to
malfunction
Exercise
Write
an
HDL
module
for
the
traffic
light
controller
from
Section
Exercise
Write
three
HDL
modules
for
the
factored
parade
mode
traffic
light
controller
from
Example
The
modules
should
be
called
controller
mode
and
lights
and
they
should
have
the
inputs
and
outputs
shown
in
Figure
b
Exercise
Write
an
HDL
module
describing
the
circuit
in
Figure
Exercise
Write
an
HDL
module
for
the
FSM
with
the
state
transition
diagram
given
in
Figure
from
Exercise
Exercise
Write
an
HDL
module
for
the
FSM
with
the
state
transition
diagram
given
in
Figure
from
Exercise
Exercise
Write
an
HDL
module
for
the
improved
traffic
light
controller
from
Exercise
Exercise
Write
an
HDL
module
for
the
daughter
snail
from
Exercise
Exercise
Write
an
HDL
module
for
the
soda
machine
dispenser
from
Exercise
Exercise
Write
an
HDL
module
for
the
Gray
code
counter
from
Exercise
Exercise
Write
an
HDL
module
for
the
UP
DOWN
Gray
code
counter
from
Exercise
Exercise
Write
an
HDL
module
for
the
FSM
from
Exercise
Chapte
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Write
an
HDL
module
for
the
FSM
from
Exercise
Exercise
Write
an
HDL
module
for
the
serial
two
s
complementer
from
Question
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
You
may
use
the
full
adder
from
Section
Verilog
Exercises
The
following
exercises
are
specific
to
Verilog
Exercise
What
does
it
mean
for
a
signal
to
be
declared
reg
in
Verilog
Exercise
Rewrite
the
syncbad
module
from
HDL
Example
Use
nonblocking
assignments
but
change
the
code
to
produce
a
correct
synchronizer
with
two
flip
flops
Exercise
Consider
the
following
two
Verilog
modules
Do
they
have
the
same
function
Sketch
the
hardware
each
one
implies
module
code
input
clk
a
b
c
output
reg
y
reg
x
always
posedge
clk
begin
x
a
b
y
x
c
end
endmodule
module
code
input
a
b
c
clk
output
reg
y
reg
x
always
posedge
clk
begin
y
x
c
x
a
b
end
endmodule
Exercise
Repeat
Exercise
if
the
is
replaced
by
in
every
assignment
Chapte
Exercises
Exercise
The
following
Verilog
modules
show
errors
that
the
authors
have
seen
students
make
in
the
laboratory
Explain
the
error
in
each
module
and
show
how
to
fix
it
a
module
latch
input
clk
input
d
output
reg
q
always
clk
if
clk
q
d
endmodule
b
module
gates
input
a
b
output
reg
y
y
y
y
y
always
a
begin
y
a
b
y
a
b
y
a
b
y
a
b
y
a
b
end
endmodule
c
module
mux
input
d
d
input
s
output
reg
y
always
posedge
s
if
s
y
d
else
y
d
endmodule
d
module
twoflops
input
clk
input
d
d
output
reg
q
q
always
posedge
clk
q
d
q
d
endmodule
Chapter
CHAPTER
FOUR
Hardware
Description
Languages
e
module
FSM
input
clk
input
a
output
reg
out
out
reg
state
next
state
logic
and
register
sequential
always
posedge
clk
if
state
begin
if
a
state
end
else
begin
if
a
state
end
always
output
logic
combinational
if
state
out
else
out
endmodule
f
module
priority
input
a
output
reg
y
always
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
if
a
y
b
endmodule
g
module
divideby
FSM
input
clk
input
reset
output
out
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
State
Register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
Next
State
Logic
always
state
case
state
S
nextstate
S
S
nextstate
S
nextstate
S
endcase
Output
Logic
assign
out
state
S
endmodule
Chapter
qxd
Exercises
h
module
mux
tri
input
d
d
input
s
output
y
tristate
t
d
s
y
tristate
t
d
s
y
endmodule
i
module
floprsen
input
clk
input
reset
input
set
input
d
output
reg
q
always
posedge
clk
posedge
reset
if
reset
q
else
q
d
always
set
if
set
q
endmodule
j
module
and
input
a
b
c
output
reg
y
reg
tmp
always
a
b
c
begin
tmp
a
b
y
tmp
c
end
endmodule
VHDL
Exercises
The
following
exercises
are
specific
to
VHDL
Exercise
In
VHDL
why
is
it
necessary
to
write
q
when
state
S
else
rather
than
simply
q
state
S
Chapter
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Each
of
the
following
VHDL
modules
contains
an
error
For
brevity
only
the
architecture
is
shown
assume
that
the
library
use
clause
and
entity
declaration
are
correct
Explain
the
error
and
show
how
to
fix
it
a
architecture
synth
of
latch
is
begin
process
clk
begin
if
clk
then
q
d
end
if
end
process
end
b
architecture
proc
of
gates
is
begin
process
a
begin
y
a
and
b
y
a
or
b
y
a
xor
b
y
a
nand
b
y
a
nor
b
end
process
end
c
architecture
synth
of
flop
is
begin
process
clk
if
clk
event
and
clk
then
q
d
end
d
architecture
synth
of
priority
is
begin
process
a
begin
if
a
then
y
elsif
a
then
y
elsif
a
then
y
elsif
a
then
y
end
if
end
process
end
e
architecture
synth
of
divideby
FSM
is
type
statetype
is
S
S
S
signal
state
nextstate
statetype
begin
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
Chapter
qxd
Exercises
process
state
begin
case
state
is
when
S
nextstate
S
when
S
nextstate
S
when
S
nextstate
S
end
case
end
process
q
when
state
S
else
end
f
architecture
struct
of
mux
is
component
tristate
port
a
in
STD
LOGIC
VECTOR
downto
en
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
begin
t
tristate
port
map
d
s
y
t
tristate
port
map
d
s
y
end
g
architecture
asynchronous
of
flopr
is
begin
process
clk
reset
begin
if
reset
then
q
elsif
clk
event
and
clk
then
q
d
end
if
end
process
process
set
begin
if
set
then
q
end
if
end
process
end
h
architecture
synth
of
mux
is
begin
y
d
when
s
else
d
when
s
else
d
end
Chapter
qxd
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Write
a
line
of
HDL
code
that
gates
a
bit
bus
called
data
with
another
signal
called
sel
to
produce
a
bit
result
If
sel
is
TRUE
result
data
Otherwise
result
should
be
all
s
Question
Explain
the
difference
between
blocking
and
nonblocking
assignments
in
Verilog
Give
examples
Question
What
does
the
following
Verilog
statement
do
result
data
hC
CHAPTER
FOUR
Hardware
Description
Languages
Ch
Introduction
Arithmetic
Circuits
Number
Systems
Sequential
Building
Blocks
Memory
Arrays
Logic
Arrays
Summary
Exercises
Interview
Questions
Digital
Building
Blocks
INTRODUCTION
Up
to
this
point
we
have
examined
the
design
of
combinational
and
sequential
circuits
using
Boolean
equations
schematics
and
HDLs
This
chapter
introduces
more
elaborate
combinational
and
sequential
building
blocks
used
in
digital
systems
These
blocks
include
arithmetic
circuits
counters
shift
registers
memory
arrays
and
logic
arrays
These
building
blocks
are
not
only
useful
in
their
own
right
but
they
also
demonstrate
the
principles
of
hierarchy
modularity
and
regularity
The
building
blocks
are
hierarchically
assembled
from
simpler
components
such
as
logic
gates
multiplexers
and
decoders
Each
building
block
has
a
well
defined
interface
and
can
be
treated
as
a
black
box
when
the
underlying
implementation
is
unimportant
The
regular
structure
of
each
building
block
is
easily
extended
to
different
sizes
In
Chapter
we
use
many
of
these
building
blocks
to
build
a
microprocessor
ARITHMETIC
CIRCUITS
Arithmetic
circuits
are
the
central
building
blocks
of
computers
Computers
and
digital
logic
perform
many
arithmetic
functions
addition
subtraction
comparisons
shifts
multiplication
and
division
This
section
describes
hardware
implementations
for
all
of
these
operations
Addition
Addition
is
one
of
the
most
common
operations
in
digital
systems
We
first
consider
how
to
add
two
bit
binary
numbers
We
then
extend
to
N
bit
binary
numbers
Adders
also
illustrate
trade
offs
between
speed
and
complexity
Half
Adder
We
begin
by
building
a
bit
half
adder
As
shown
in
Figure
the
half
adder
has
two
inputs
A
and
B
and
two
outputs
S
and
Cout
S
is
the
A
B
out
SC
S
A
B
Cout
AB
Half
Adder
A
B
S
Cout
FIGURE
bit
half
adder
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Carry
bit
A
B
out
SC
S
A
B
Cin
Cout
AB
ACin
BCin
Full
Adder
Cin
A
B
S
Cout
Cin
FIGURE
bit
full
adder
A
B
S
Cout
Cin
N
N
N
Figure
Carry
propagate
adder
sum
of
A
and
B
If
A
and
B
are
both
S
is
which
cannot
be
represented
with
a
single
binary
digit
Instead
it
is
indicated
with
a
carry
out
Cout
in
the
next
column
The
half
adder
can
be
built
from
an
XOR
gate
and
an
AND
gate
In
a
multi
bit
adder
Cout
is
added
or
carried
in
to
the
next
most
significant
bit
For
example
in
Figure
the
carry
bit
shown
in
blue
is
the
output
Cout
of
the
first
column
of
bit
addition
and
the
input
Cin
to
the
second
column
of
addition
However
the
half
adder
lacks
a
Cin
input
to
accept
Cout
of
the
previous
column
The
full
adder
described
in
the
next
section
solves
this
problem
Full
Adder
A
full
adder
introduced
in
Section
accepts
the
carry
in
Cin
as
shown
in
Figure
The
figure
also
shows
the
output
equations
for
S
and
Cout
Carry
Propagate
Adder
An
N
bit
adder
sums
two
N
bit
inputs
A
and
B
and
a
carry
in
Cin
to
produce
an
N
bit
result
S
and
a
carry
out
Cout
It
is
commonly
called
a
carry
propagate
adder
CPA
because
the
carry
out
of
one
bit
propagates
into
the
next
bit
The
symbol
for
a
CPA
is
shown
in
Figure
it
is
drawn
just
like
a
full
adder
except
that
A
B
and
S
are
busses
rather
than
single
bits
Three
common
CPA
implementations
are
called
ripplecarry
adders
carry
lookahead
adders
and
prefix
adders
Ripple
Carry
Adder
The
simplest
way
to
build
an
N
bit
carry
propagate
adder
is
to
chain
together
N
full
adders
The
Cout
of
one
stage
acts
as
the
Cin
of
the
next
stage
as
shown
in
Figure
for
bit
addition
This
is
called
a
ripplecarry
adder
It
is
a
good
application
of
modularity
and
regularity
the
full
adder
module
is
reused
many
times
to
form
a
larger
system
The
ripplecarry
adder
has
the
disadvantage
of
being
slow
when
N
is
large
S
depends
on
C
which
depends
on
C
which
depends
on
C
and
so
forth
all
the
way
back
to
Cin
as
shown
in
blue
in
Figure
We
say
that
the
carry
ripples
through
the
carry
chain
The
delay
of
the
adder
tripple
grows
directly
with
the
number
of
bits
as
given
in
Equation
where
tFA
is
the
delay
of
a
full
adder
t
ripple
NtFA
S
A
B
S
A
B
S
A
B
S
C
C
C
C
Cout
A
B
Cin
Figure
bit
ripple
carry
adder
Schematics
typically
show
signals
flowing
from
left
to
right
Arithmetic
circuits
break
this
rule
because
the
carries
flow
from
right
to
left
from
the
least
significant
column
to
the
most
significant
column
C
Carry
Lookahead
Adder
The
fundamental
reason
that
large
ripple
carry
adders
are
slow
is
that
the
carry
signals
must
propagate
through
every
bit
in
the
adder
A
carrylookahead
adder
is
another
type
of
carry
propagate
adder
that
solves
this
problem
by
dividing
the
adder
into
blocks
and
providing
circuitry
to
quickly
determine
the
carry
out
of
a
block
as
soon
as
the
carry
in
is
known
Thus
it
is
said
to
look
ahead
across
the
blocks
rather
than
waiting
to
ripple
through
all
the
full
adders
inside
a
block
For
example
a
bit
adder
may
be
divided
into
eight
bit
blocks
Carry
lookahead
adders
use
generate
G
and
propagate
P
signals
that
describe
how
a
column
or
block
determines
the
carry
out
The
ith
column
of
an
adder
is
said
to
generate
a
carry
if
it
produces
a
carry
out
independent
of
the
carry
in
The
ith
column
of
an
adder
is
guaranteed
to
generate
a
carry
Ci
if
Ai
and
Bi
are
both
Hence
Gi
the
generate
signal
for
column
i
is
calculated
as
Gi
AiBi
The
column
is
said
to
propagate
a
carry
if
it
produces
a
carry
out
whenever
there
is
a
carry
in
The
ith
column
will
propagate
a
carry
in
Ci
if
either
Ai
or
Bi
is
Thus
Pi
Ai
Bi
Using
these
definitions
we
can
rewrite
the
carry
logic
for
a
particular
column
of
the
adder
The
ith
column
of
an
adder
will
generate
a
carryout
Ci
if
it
either
generates
a
carry
Gi
or
propagates
a
carry
in
Pi
Ci
In
equation
form
The
generate
and
propagate
definitions
extend
to
multiple
bit
blocks
A
block
is
said
to
generate
a
carry
if
it
produces
a
carry
out
independent
of
the
carry
in
to
the
block
The
block
is
said
to
propagate
a
carry
if
it
produces
a
carry
out
whenever
there
is
a
carry
in
to
the
block
We
define
Gi
j
and
Pi
j
as
generate
and
propagate
signals
for
blocks
spanning
columns
i
through
j
A
block
generates
a
carry
if
the
most
significant
column
generates
a
carry
or
if
the
most
significant
column
propagates
a
carry
and
the
previous
column
generated
a
carry
and
so
forth
For
example
the
generate
logic
for
a
block
spanning
columns
through
is
A
block
propagates
a
carry
if
all
the
columns
in
the
block
propagate
the
carry
For
example
the
propagate
logic
for
a
block
spanning
columns
through
is
Using
the
block
generate
and
propagate
signals
we
can
quickly
compute
the
carry
out
of
the
block
Ci
using
the
carry
in
to
the
block
Cj
C
i
Gi
j
Pi
j
Cj
P
P
P
P
P
G
G
P
G
P
G
P
G
Ci
Ai
Bi
Ai
Bi
Ci
Gi
Pi
Ci
Arithmetic
Circuits
Throughout
the
ages
people
have
used
many
devices
to
perform
arithmetic
Toddlers
count
on
their
fingers
and
some
adults
stealthily
do
too
The
Chinese
and
Babylonians
invented
the
abacus
as
early
as
BC
Slide
rules
invented
in
were
in
use
until
the
s
when
scientific
hand
calculators
became
prevalent
Computers
and
digital
calculators
are
ubiquitous
today
What
will
be
next
Chapter
Figure
a
shows
a
bit
carry
lookahead
adder
composed
of
eight
bit
blocks
Each
block
contains
a
bit
ripple
carry
adder
and
some
lookahead
logic
to
compute
the
carry
out
of
the
block
given
the
carry
in
as
shown
in
Figure
b
The
AND
and
OR
gates
needed
to
compute
the
single
bit
generate
and
propagate
signals
Gi
and
Pi
from
Ai
and
Bi
are
left
out
for
brevity
Again
the
carry
lookahead
adder
demonstrates
modularity
and
regularity
All
of
the
CLA
blocks
compute
the
single
bit
and
block
generate
and
propagate
signals
simultaneously
The
critical
path
starts
with
computing
G
and
G
in
the
first
CLA
block
Cin
then
advances
directly
to
Cout
through
the
AND
OR
gate
in
each
block
until
the
last
For
a
large
adder
this
is
much
faster
than
waiting
for
the
carries
to
ripple
through
each
consecutive
bit
of
the
adder
Finally
the
critical
path
through
the
last
block
contains
a
short
ripple
carry
adder
Thus
an
N
bit
adder
divided
into
k
bit
blocks
has
a
delay
tCLA
tpg
tpg
block
N
k
tAND
OR
ktFA
CHAPTER
FIVE
Digital
Building
Blocks
B
P
G
P
G
P
G
P
G
P
P
P
P
G
Cin
Cout
A
S
C
B
A
S
C
B
A
S
C
B
A
S
Cin
b
a
B
A
S
Cin
B
A
S
C
C
B
A
S
C
B
A
S
bit
CLA
Block
bit
CLA
Block
bit
CLA
Block
bit
CLA
Block
C
Cout
Figure
a
bit
carrylookahead
adder
CLA
b
bit
CLA
block
Chapte
where
tpg
is
the
delay
of
the
individual
generate
propagate
gates
a
single
AND
or
OR
gate
to
generate
P
and
G
tpg
block
is
the
delay
to
find
the
generate
propagate
signals
Pi
j
and
Gi
j
for
a
k
bit
block
and
tAND
OR
is
the
delay
from
Cin
to
Cout
through
the
AND
OR
logic
of
the
k
bit
CLA
block
For
N
the
carry
lookahead
adder
is
generally
much
faster
than
the
ripple
carry
adder
However
the
adder
delay
still
increases
linearly
with
N
Example
RIPPLE
CARRY
ADDER
AND
CARRY
LOOKAHEAD
ADDER
DELAY
Compare
the
delays
of
a
bit
ripple
carry
adder
and
a
bit
carry
lookahead
adder
with
bit
blocks
Assume
that
each
two
input
gate
delay
is
ps
and
that
a
full
adder
delay
is
ps
Solution
According
to
Equation
the
propagation
delay
of
the
bit
ripplecarry
adder
is
ps
ns
The
CLA
has
tpg
ps
tpg
block
ps
ps
and
tAND
OR
ps
ps
According
to
Equation
the
propagation
delay
of
the
bit
carry
lookahead
adder
with
bit
blocks
is
thus
ps
ps
ps
ps
ns
almost
three
times
faster
than
the
ripple
carry
adder
Prefix
Adder
Prefix
adders
extend
the
generate
and
propagate
logic
of
the
carrylookahead
adder
to
perform
addition
even
faster
They
first
compute
G
and
P
for
pairs
of
columns
then
for
blocks
of
then
for
blocks
of
then
and
so
forth
until
the
generate
signal
for
every
column
is
known
The
sums
are
computed
from
these
generate
signals
In
other
words
the
strategy
of
a
prefix
adder
is
to
compute
the
carry
in
Ci
for
each
column
i
as
quickly
as
possible
then
to
compute
the
sum
using
Si
Ai
Bi
Ci
Define
column
i
to
hold
Cin
so
G
Cin
and
P
Then
Ci
Gi
because
there
will
be
a
carry
out
of
column
i
if
the
block
spanning
columns
i
through
generates
a
carry
The
generated
carry
is
either
generated
in
column
i
or
generated
in
a
previous
column
and
propagated
Thus
we
rewrite
Equation
as
Si
Ai
Bi
Gi
Hence
the
main
challenge
is
to
rapidly
compute
all
the
block
generate
signals
G
G
G
G
GN
These
signals
along
with
P
P
P
P
PN
are
called
prefixes
Arithmetic
Circuits
Early
computers
used
ripple
carry
adders
because
components
were
expensive
and
ripple
carry
adders
used
the
least
hardware
Virtually
all
modern
PCs
use
prefix
adders
on
critical
paths
because
transistors
are
now
cheap
and
speed
is
of
great
importance
Chapter
qxd
AM
Page
Figure
shows
an
N
bit
prefix
adder
The
adder
begins
with
a
precomputation
to
form
Pi
and
Gi
for
each
column
from
Ai
and
Bi
using
AND
and
OR
gates
It
then
uses
log
N
levels
of
black
cells
to
form
the
prefixes
of
Gi
j
and
Pi
j
A
black
cell
takes
inputs
from
the
upper
part
of
a
block
spanning
bits
i
k
and
from
the
lower
part
spanning
bits
k
j
It
combines
these
parts
to
form
generate
and
propagate
signals
for
the
entire
block
spanning
bits
i
j
using
the
equations
In
other
words
a
block
spanning
bits
i
j
will
generate
a
carry
if
the
upper
part
generates
a
carry
or
if
the
upper
part
propagates
a
carry
generated
in
the
lower
part
The
block
will
propagate
a
carry
if
both
the
Pi
j
Pi
k
Pk
j
Gi
j
Gi
k
Pi
k
Gk
j
CHAPTER
FIVE
Digital
Building
Blocks
Ai
Bi
Pi
i
Gi
i
Pi
kPk
j
Gi
k
Gk
j
Pi
j
Gi
j
Gi
Ai
Bi
Si
i
i
j
Legend
i
Figure
bit
prefix
adder
Chapter
upper
and
lower
parts
propagate
the
carry
Finally
the
prefix
adder
computes
the
sums
using
Equation
In
summary
the
prefix
adder
achieves
a
delay
that
grows
logarithmically
rather
than
linearly
with
the
number
of
columns
in
the
adder
This
speedup
is
significant
especially
for
adders
with
or
more
bits
but
it
comes
at
the
expense
of
more
hardware
than
a
simple
carrylookahead
adder
The
network
of
black
cells
is
called
a
prefix
tree
The
general
principle
of
using
prefix
trees
to
perform
computations
in
time
that
grows
logarithmically
with
the
number
of
inputs
is
a
powerful
technique
With
some
cleverness
it
can
be
applied
to
many
other
types
of
circuits
see
for
example
Exercise
The
critical
path
for
an
N
bit
prefix
adder
involves
the
precomputation
of
Pi
and
Gi
followed
by
log
N
stages
of
black
prefix
cells
to
obtain
all
the
prefixes
Gi
then
proceeds
through
the
final
XOR
gate
at
the
bottom
to
compute
Si
Mathematically
the
delay
of
an
N
bit
prefix
adder
is
where
tpg
prefix
is
the
delay
of
a
black
prefix
cell
Example
PREFIX
ADDER
DELAY
Compute
the
delay
of
a
bit
prefix
adder
Assume
that
each
two
input
gate
delay
is
ps
Solution
The
propagation
delay
of
each
black
prefix
cell
tpg
prefix
is
ps
i
e
two
gate
delays
Thus
using
Equation
the
propagation
delay
of
the
bit
prefix
adder
is
ps
log
ps
ps
ns
which
is
about
three
times
faster
than
the
carry
lookahead
adder
and
eight
times
faster
than
the
ripple
carry
adder
from
Example
In
practice
the
benefits
are
not
quite
this
great
but
prefix
adders
are
still
substantially
faster
than
the
alternatives
Putting
It
All
Together
This
section
introduced
the
half
adder
full
adder
and
three
types
of
carry
propagate
adders
ripple
carry
carry
lookahead
and
prefix
adders
Faster
adders
require
more
hardware
and
therefore
are
more
expensive
and
power
hungry
These
trade
offs
must
be
considered
when
choosing
an
appropriate
adder
for
a
design
Hardware
description
languages
provide
the
operation
to
specify
a
CPA
Modern
synthesis
tools
select
among
many
possible
implementations
choosing
the
cheapest
smallest
design
that
meets
the
speed
requirements
This
greatly
simplifies
the
designer
s
job
HDL
Example
describes
a
CPA
with
carries
in
and
out
tPA
tpg
log
N
tpgprefix
tXOR
Arithmetic
Circuits
Chapt
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
adder
parameter
N
input
N
a
b
input
cin
output
N
s
output
cout
assign
cout
s
a
b
cin
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
adder
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
cin
in
STD
LOGIC
s
out
STD
LOGIC
VECTOR
N
downto
cout
out
STD
LOGIC
end
architecture
synth
of
adder
is
signal
result
STD
LOGIC
VECTOR
N
downto
begin
result
a
b
cin
s
result
N
downto
cout
result
N
end
HDL
Example
ADDER
Subtraction
Recall
from
Section
that
adders
can
add
positive
and
negative
numbers
using
two
s
complement
number
representation
Subtraction
is
almost
as
easy
flip
the
sign
of
the
second
number
then
add
Flipping
the
sign
of
a
two
s
complement
number
is
done
by
inverting
the
bits
and
adding
To
compute
Y
A
B
first
create
the
two
s
complement
of
B
Invert
the
bits
of
B
to
obtain
and
add
to
get
Add
this
quantity
to
A
to
get
This
sum
can
be
performed
with
a
single
CPA
by
adding
with
Cin
Figure
shows
the
symbol
for
a
subtractor
and
the
underlying
hardware
for
performing
Y
A
B
HDL
Example
describes
a
subtractor
Comparators
A
comparator
determines
whether
two
binary
numbers
are
equal
or
if
one
is
greater
or
less
than
the
other
A
comparator
receives
two
N
bit
binary
numbers
A
and
B
There
are
two
common
types
of
comparators
A
B
Y
A
B
A
B
B
B
B
A
B
Y
a
N
N
N
Y
A
B
b
N
N
N
N
Figure
Subtractor
a
symbol
b
implementation
cout
s
cin
b
a
Figure
Synthesized
adder
Chapter
qxd
Arithmetic
Circuits
Verilog
module
subtractor
parameter
N
input
N
a
b
output
N
y
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
subtractor
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
y
out
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
subtractor
is
begin
y
a
b
end
HDL
Example
SUBTRACTOR
An
equality
comparator
produces
a
single
output
indicating
whether
A
is
equal
to
B
A
B
A
magnitude
comparator
produces
one
or
more
outputs
indicating
the
relative
values
of
A
and
B
The
equality
comparator
is
the
simpler
piece
of
hardware
Figure
shows
the
symbol
and
implementation
of
a
bit
equality
comparator
It
first
checks
to
determine
whether
the
corresponding
bits
in
each
column
of
A
and
B
are
equal
using
XNOR
gates
The
numbers
are
equal
if
all
of
the
columns
are
equal
Magnitude
comparison
is
usually
done
by
computing
A
B
and
looking
at
the
sign
most
significant
bit
of
the
result
as
shown
in
Figure
If
the
result
is
negative
i
e
the
sign
bit
is
then
A
is
less
than
B
Otherwise
A
is
greater
than
or
equal
to
B
HDL
Example
shows
how
to
use
various
comparison
operations
A
B
A
B
A
B
A
B
Equal
b
Figure
bit
equality
comparator
a
symbol
b
implementation
y
b
a
Figure
Synthesized
subtractor
a
A
B
Equal
A
B
BA
N
N
N
N
Figure
N
bit
magnitude
comparator
Chapter
qx
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
comparators
parameter
N
input
N
a
b
output
eq
neq
output
lt
lte
output
gt
gte
assign
eq
a
b
assign
neq
a
b
assign
lt
a
b
assign
lte
a
b
assign
gt
a
b
assign
gte
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
entity
comparators
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
eq
neq
lt
lte
gt
gte
out
STD
LOGIC
end
architecture
synth
of
comparators
is
begin
eq
when
a
b
else
neq
when
a
b
else
lt
when
a
b
else
lte
when
a
b
else
gt
when
a
b
else
gte
when
a
b
else
end
HDL
Example
COMPARATORS
ALU
An
Arithmetic
Logical
Unit
ALU
combines
a
variety
of
mathematical
and
logical
operations
into
a
single
unit
For
example
a
typical
ALU
might
perform
addition
subtraction
magnitude
comparison
AND
and
OR
operations
The
ALU
forms
the
heart
of
most
computer
systems
Figure
shows
the
symbol
for
an
N
bit
ALU
with
N
bit
inputs
and
outputs
The
ALU
receives
a
control
signal
F
that
specifies
which
gte
gt
lte
lt
neq
eq
b
a
Figure
Synthesized
comparators
ALU
N
N
N
A
B
Y
F
Figure
ALU
symbol
Chapter
qxd
function
to
perform
Control
signals
will
generally
be
shown
in
blue
to
distinguish
them
from
the
data
Table
lists
typical
functions
that
the
ALU
can
perform
The
SLT
function
is
used
for
magnitude
comparison
and
will
be
discussed
later
in
this
section
Figure
shows
an
implementation
of
the
ALU
The
ALU
contains
an
N
bit
adder
and
N
two
input
AND
and
OR
gates
It
also
contains
an
inverter
and
a
multiplexer
to
optionally
invert
input
B
when
the
F
control
signal
is
asserted
A
multiplexer
chooses
the
desired
function
based
on
the
F
control
signals
Arithmetic
Circuits
A
B
Cout
Y
F
F
N
S
N
N
N
N
N
N
N
N
N
extend
Zero
BB
Figure
N
bit
ALU
Table
ALU
operations
F
Function
A
AND
B
A
OR
B
A
B
not
used
A
AND
A
OR
A
B
SLT
B
B
C
More
specifically
the
arithmetic
and
logical
blocks
in
the
ALU
operate
on
A
and
BB
BB
is
either
B
or
depending
on
F
If
F
the
output
multiplexer
chooses
A
AND
BB
If
F
the
ALU
computes
A
OR
BB
If
F
the
ALU
performs
addition
or
subtraction
Note
that
F
is
also
the
carry
in
to
the
adder
Also
remember
that
in
two
s
complement
arithmetic
If
F
the
ALU
computes
A
B
If
F
the
ALU
computes
When
F
the
ALU
performs
the
set
if
less
than
SLT
operation
When
A
B
Y
Otherwise
Y
In
other
words
Y
is
set
to
if
A
is
less
than
B
SLT
is
performed
by
computing
S
A
B
If
S
is
negative
i
e
the
sign
bit
is
set
A
B
The
zero
extend
unit
produces
an
N
bit
output
by
concatenating
its
bit
input
with
s
in
the
most
significant
bits
The
sign
bit
the
N
th
bit
of
S
is
the
input
to
the
zero
extend
unit
Example
SET
LESS
THAN
Configure
a
bit
ALU
for
the
SLT
operation
Suppose
A
and
B
Show
the
control
signals
and
output
Y
Solution
Because
A
B
we
expect
Y
to
be
For
SLT
F
With
F
this
configures
the
adder
unit
as
a
subtractor
with
an
output
S
of
With
F
the
final
multiplexer
sets
Y
S
Some
ALUs
produce
extra
outputs
called
flags
that
indicate
information
about
the
ALU
output
For
example
an
overflow
flag
indicates
that
the
result
of
the
adder
overflowed
A
zero
flag
indicates
that
the
ALU
output
is
The
HDL
for
an
N
bit
ALU
is
left
to
Exercise
There
are
many
variations
on
this
basic
ALU
that
support
other
functions
such
as
XOR
or
equality
comparison
Shifters
and
Rotators
Shifters
and
rotators
move
bits
and
multiply
or
divide
by
powers
of
As
the
name
implies
a
shifter
shifts
a
binary
number
left
or
right
by
a
specified
number
of
positions
There
are
several
kinds
of
commonly
used
shifters
Logical
shifter
shifts
the
number
to
the
left
LSL
or
right
LSR
and
fills
empty
spots
with
s
Ex
LSR
LSL
Arithmetic
shifter
is
the
same
as
a
logical
shifter
but
on
right
shifts
fills
the
most
significant
bits
with
a
copy
of
the
old
most
significant
bit
msb
This
is
useful
for
multiplying
and
dividing
signed
numbers
A
B
A
B
B
B
B
CHAPTER
FIVE
Digital
Building
Blocks
Chapter
qxd
A
see
Sections
and
Arithmetic
shift
left
ASL
is
the
same
as
logical
shift
left
LSL
Ex
ASR
ASL
Rotator
rotates
number
in
circle
such
that
empty
spots
are
filled
with
bits
shifted
off
the
other
end
Ex
ROR
ROL
An
N
bit
shifter
can
be
built
from
N
N
multiplexers
The
input
is
shifted
by
to
N
bits
depending
on
the
value
of
the
log
N
bit
select
lines
Figure
shows
the
symbol
and
hardware
of
bit
shifters
The
operators
and
typically
indicate
shift
left
logical
shift
right
and
arithmetic
shift
right
respectively
Depending
on
the
value
of
the
bit
shift
amount
shamt
the
output
Y
receives
the
input
A
shifted
by
to
bits
For
all
shifters
when
shamt
Y
A
Exercise
covers
rotator
designs
A
left
shift
is
a
special
case
of
multiplication
A
left
shift
by
N
bits
multiplies
the
number
by
N
For
example
is
equivalent
to
Arithmetic
Circuits
Figure
bit
shifters
a
shift
left
b
logical
shift
right
c
arithmetic
shift
right
shamt
A
A
A
A
Y
Y
Y
Y
a
S
S
S
S
b
A
A
A
A
Y
Y
Y
Y
shamt
A
Y
shamt
S
S
S
S
c
A
A
A
A
Y
Y
Y
Y
shamt
S
S
S
S
A
Y
shamt
A
Y
shamt
Chapter
An
arithmetic
right
shift
is
a
special
case
of
division
An
arithmetic
right
shift
by
N
bits
divides
the
number
by
N
For
example
is
equivalent
to
Multiplication
Multiplication
of
unsigned
binary
numbers
is
similar
to
decimal
multiplication
but
involves
only
s
and
s
Figure
compares
multiplication
in
decimal
and
binary
In
both
cases
partial
products
are
formed
by
multiplying
a
single
digit
of
the
multiplier
with
the
entire
multiplicand
The
shifted
partial
products
are
summed
to
form
the
result
In
general
an
N
N
multiplier
multiplies
two
N
bit
numbers
and
produces
a
N
bit
result
The
partial
products
in
binary
multiplication
are
either
the
multiplicand
or
all
s
Multiplication
of
bit
binary
numbers
is
equivalent
to
the
AND
operation
so
AND
gates
are
used
to
form
the
partial
products
Figure
shows
the
symbol
function
and
implementation
of
a
multiplier
The
multiplier
receives
the
multiplicand
and
multiplier
A
and
B
and
produces
the
product
P
Figure
b
shows
how
partial
products
are
formed
Each
partial
product
is
a
single
multiplier
bit
B
B
B
or
B
AND
the
multiplicand
bits
A
A
A
A
With
N
bit
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Multiplication
a
decimal
b
binary
Figure
multiplier
a
symbol
b
function
c
implementation
a
multiplier
multiplicand
partial
products
result
b
a
x
A
B
P
b
B
B
B
B
A
B
A
B
A
B
A
B
A
A
A
A
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
P
P
P
P
P
P
P
P
P
c
P
P
P
P
P
P
P
A
A
A
A
B
B
B
B
Chap
operands
there
are
N
partial
products
and
N
stages
of
bit
adders
For
example
for
a
multiplier
the
partial
product
of
the
first
row
is
B
AND
A
A
A
A
This
partial
product
is
added
to
the
shifted
second
partial
product
B
AND
A
A
A
A
Subsequent
rows
of
AND
gates
and
adders
form
and
add
the
remaining
partial
products
The
HDL
for
a
multiplier
is
in
HDL
Example
As
with
adders
many
different
multiplier
designs
with
different
speed
cost
trade
offs
exist
Synthesis
tools
may
pick
the
most
appropriate
design
given
the
timing
constraints
Arithmetic
Circuits
Verilog
module
multiplier
parameter
N
input
N
a
b
output
N
y
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
multiplier
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
y
out
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
multiplier
is
begin
y
a
b
end
HDL
Example
MULTIPLIER
Division
Binary
division
can
be
performed
using
the
following
algorithm
for
normalized
unsigned
numbers
in
the
range
N
N
R
A
for
i
N
to
D
R
B
if
D
then
Qi
R
R
R
B
else
Qi
R
D
R
B
if
i
then
R
R
The
partial
remainder
R
is
initialized
to
the
dividend
A
The
divisor
B
is
repeatedly
subtracted
from
this
partial
remainder
to
determine
whether
it
fits
If
the
difference
D
is
negative
i
e
the
sign
bit
of
D
is
then
the
quotient
bit
Qi
is
and
the
difference
is
discarded
Otherwise
Qi
is
Figure
Synthesized
multiplier
y
b
a
Chapter
qxd
and
the
partial
remainder
is
updated
to
be
the
difference
In
any
event
the
partial
remainder
is
then
doubled
left
shifted
by
one
column
and
the
process
repeats
The
result
satisfies
Figure
shows
a
schematic
of
a
bit
array
divider
The
divider
computes
A
B
and
produces
a
quotient
Q
and
a
remainder
R
The
legend
shows
the
symbol
and
schematic
for
each
block
in
the
array
divider
The
signal
P
indicates
whether
R
B
is
negative
It
is
obtained
from
the
Cout
output
of
the
leftmost
block
in
the
row
which
is
the
sign
of
the
difference
The
delay
of
an
N
bit
array
divider
increases
proportionally
to
N
because
the
carry
must
ripple
through
all
N
stages
in
a
row
before
the
sign
is
determined
and
the
multiplexer
selects
R
or
D
This
repeats
for
all
N
rows
Division
is
a
slow
and
expensive
operation
in
hardware
and
therefore
should
be
used
as
infrequently
as
possible
Further
Reading
Computer
arithmetic
could
be
the
subject
of
an
entire
text
Digital
Arithmetic
by
Ercegovac
and
Lang
is
an
excellent
overview
of
the
entire
field
CMOS
VLSI
Design
by
Weste
and
Harris
covers
highperformance
circuit
designs
for
arithmetic
operations
A
B
Q
R
B
N
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Array
divider
R
B
D
R
P
Cout
Cin
R
B
P
R
Cout
Cin
A
A
A
A
Q
Q
Q
Q
B
B
B
B
R
R
R
R
Legend
Chapte
NUMBER
SYSTEMS
Computers
operate
on
both
integers
and
fractions
So
far
we
have
only
considered
representing
signed
or
unsigned
integers
as
introduced
in
Section
This
section
introduces
fixed
and
floating
point
number
systems
that
can
also
represent
rational
numbers
Fixed
point
numbers
are
analogous
to
decimals
some
of
the
bits
represent
the
integer
part
and
the
rest
represent
the
fraction
Floating
point
numbers
are
analogous
to
scientific
notation
with
a
mantissa
and
an
exponent
Fixed
Point
Number
Systems
Fixed
point
notation
has
an
implied
binary
point
between
the
integer
and
fraction
bits
analogous
to
the
decimal
point
between
the
integer
and
fraction
digits
of
an
ordinary
decimal
number
For
example
Figure
a
shows
a
fixed
point
number
with
four
integer
bits
and
four
fraction
bits
Figure
b
shows
the
implied
binary
point
in
blue
and
Figure
c
shows
the
equivalent
decimal
value
Signed
fixed
point
numbers
can
use
either
two
s
complement
or
sign
magnitude
notations
Figure
shows
the
fixed
point
representation
of
using
both
notations
with
four
integer
and
four
fraction
bits
The
implicit
binary
point
is
shown
in
blue
for
clarity
In
sign
magnitude
form
the
most
significant
bit
is
used
to
indicate
the
sign
The
two
s
complement
representation
is
formed
by
inverting
the
bits
of
the
absolute
value
and
adding
a
to
the
least
significant
rightmost
bit
In
this
case
the
least
significant
bit
position
is
in
the
column
Like
all
binary
number
representations
fixed
point
numbers
are
just
a
collection
of
bits
There
is
no
way
of
knowing
the
existence
of
the
binary
point
except
through
agreement
of
those
people
interpreting
the
number
Example
ARITHMETIC
WITH
FIXED
POINT
NUMBERS
Compute
using
fixed
point
numbers
Solution
First
convert
the
magnitude
of
the
second
number
to
fixedpoint
binary
notation
so
there
is
a
in
the
column
leaving
Because
there
is
a
in
the
column
Because
there
is
a
in
the
column
leaving
Thus
there
must
be
a
in
the
column
Putting
this
all
together
Use
two
s
complement
representation
for
signed
numbers
so
that
addition
works
correctly
Figure
shows
the
conversion
of
to
fixed
point
two
s
complement
notation
Figure
shows
the
fixed
point
binary
addition
and
the
decimal
equivalent
for
comparison
Note
that
the
leading
in
the
binary
fixed
point
addition
of
Figure
a
is
discarded
from
the
bit
result
Number
Systems
Figure
Fixed
point
notation
of
with
four
integer
bits
and
four
fraction
bits
a
b
c
Figure
Fixed
point
representation
of
a
absolute
value
b
sign
and
magnitude
c
two
s
complement
a
b
c
Fixed
point
number
systems
are
commonly
used
for
banking
and
financial
applications
that
require
precision
but
not
a
large
range
Chapter
qxd
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Fixed
point
two
s
complement
conversion
Add
Two
s
Complement
One
s
Complement
Binary
Magnitude
Figure
Addition
a
binary
fixed
point
b
decimal
equivalent
a
b
Figure
Floating
point
numbers
Figure
bit
floatingpoint
version
M
BE
Sign
Exponent
Mantissa
bit
bits
bits
Floating
Point
Number
Systems
Floating
point
numbers
are
analogous
to
scientific
notation
They
circumvent
the
limitation
of
having
a
constant
number
of
integer
and
fractional
bits
allowing
the
representation
of
very
large
and
very
small
numbers
Like
scientific
notation
floating
point
numbers
have
a
sign
mantissa
M
base
B
and
exponent
E
as
shown
in
Figure
For
example
the
number
is
the
decimal
scientific
notation
for
It
has
a
mantissa
of
a
base
of
and
an
exponent
of
The
decimal
point
floats
to
the
position
right
after
the
most
significant
digit
Floating
point
numbers
are
base
with
a
binary
mantissa
bits
are
used
to
represent
sign
bit
exponent
bits
and
mantissa
bits
Example
BIT
FLOATING
POINT
NUMBERS
Show
the
floating
point
representation
of
the
decimal
number
Solution
First
convert
the
decimal
number
into
binary
Figure
shows
the
bit
encoding
which
will
be
modified
later
for
efficiency
The
sign
bit
is
positive
the
exponent
bits
give
the
value
and
the
remaining
bits
are
the
mantissa
In
binary
floating
point
the
first
bit
of
the
mantissa
to
the
left
of
the
binary
point
is
always
and
therefore
need
not
be
stored
It
is
called
the
implicit
leading
one
Figure
shows
the
modified
floatingpoint
representation
of
The
implicit
leading
one
is
not
included
in
the
bit
mantissa
for
efficiency
Only
the
fraction
bits
are
stored
This
frees
up
an
extra
bit
for
useful
data
Chap
We
make
one
final
modification
to
the
exponent
field
The
exponent
needs
to
represent
both
positive
and
negative
exponents
To
do
so
floating
point
uses
a
biased
exponent
which
is
the
original
exponent
plus
a
constant
bias
bit
floating
point
uses
a
bias
of
For
example
for
the
exponent
the
biased
exponent
is
For
the
exponent
the
biased
exponent
is
Figure
shows
represented
in
floatingpoint
notation
with
an
implicit
leading
one
and
a
biased
exponent
of
This
notation
conforms
to
the
IEEE
floating
point
standard
Special
Cases
and
NaN
The
IEEE
floating
point
standard
has
special
cases
to
represent
numbers
such
as
zero
infinity
and
illegal
results
For
example
representing
the
number
zero
is
problematic
in
floating
point
notation
because
of
the
implicit
leading
one
Special
codes
with
exponents
of
all
s
or
all
s
are
reserved
for
these
special
cases
Table
shows
the
floating
point
representations
of
and
NaN
As
with
sign
magnitude
numbers
floating
point
has
both
positive
and
negative
NaN
is
used
for
numbers
that
don
t
exist
such
as
or
log
Single
and
Double
Precision
Formats
So
far
we
have
examined
bit
floating
point
numbers
This
format
is
also
called
single
precision
single
or
float
The
IEEE
standard
also
Number
Systems
Figure
Floating
point
version
Sign
Exponent
Fraction
bit
bits
bits
Figure
IEEE
floatingpoint
notation
Sign
Biased
Exponent
Fraction
bit
bits
bits
Table
IEEE
floating
point
notations
for
and
NaN
Number
Sign
Exponent
Fraction
X
NaN
X
non
zero
As
may
be
apparent
there
are
many
reasonable
ways
to
represent
floating
point
numbers
For
many
years
computer
manufacturers
used
incompatible
floating
point
formats
Results
from
one
computer
could
not
directly
be
interpreted
by
another
computer
The
Institute
of
Electrical
and
Electronics
Engineers
solved
this
problem
by
defining
the
IEEE
floatingpoint
standard
in
defining
floating
point
numbers
This
floating
point
format
is
now
almost
universally
used
and
is
the
one
discussed
in
this
section
Chapter
defines
bit
double
precision
also
called
double
numbers
that
provide
greater
precision
and
greater
range
Table
shows
the
number
of
bits
used
for
the
fields
in
each
format
Excluding
the
special
cases
mentioned
earlier
normal
single
precision
numbers
span
a
range
of
to
They
have
a
precision
of
about
seven
significant
decimal
digits
because
Similarly
normal
double
precision
numbers
span
a
range
of
to
and
have
a
precision
of
about
significant
decimal
digits
Rounding
Arithmetic
results
that
fall
outside
of
the
available
precision
must
round
to
a
neighboring
number
The
rounding
modes
are
round
down
round
up
round
toward
zero
and
round
to
nearest
The
default
rounding
mode
is
round
to
nearest
In
the
round
to
nearest
mode
if
two
numbers
are
equally
near
the
one
with
a
in
the
least
significant
position
of
the
fraction
is
chosen
Recall
that
a
number
overflows
when
its
magnitude
is
too
large
to
be
represented
Likewise
a
number
underflows
when
it
is
too
tiny
to
be
represented
In
round
to
nearest
mode
overflows
are
rounded
up
to
and
underflows
are
rounded
down
to
Floating
Point
Addition
Addition
with
floating
point
numbers
is
not
as
simple
as
addition
with
two
s
complement
numbers
The
steps
for
adding
floating
point
numbers
with
the
same
sign
are
as
follows
Extract
exponent
and
fraction
bits
Prepend
leading
to
form
the
mantissa
Compare
exponents
Shift
smaller
mantissa
if
necessary
Add
mantissas
Normalize
mantissa
and
adjust
exponent
if
necessary
Round
result
Assemble
exponent
and
fraction
back
into
floating
point
number
CHAPTER
FIVE
Digital
Building
Blocks
Floating
point
cannot
represent
some
numbers
exactly
like
However
when
you
type
into
your
calculator
you
see
exactly
not
To
handle
this
some
applications
such
as
calculators
and
financial
software
use
binary
coded
decimal
BCD
numbers
or
formats
with
a
base
exponent
BCD
numbers
encode
each
decimal
digit
using
four
bits
with
a
range
of
to
For
example
the
BCD
fixedpoint
notation
of
with
four
integer
bits
and
four
fraction
bits
would
be
Of
course
nothing
is
free
The
cost
is
increased
complexity
in
arithmetic
hardware
and
wasted
encodings
A
F
encodings
are
not
used
and
thus
decreased
performance
So
for
compute
intensive
applications
floating
point
is
much
faster
Table
Single
and
double
precision
floating
point
formats
Format
Total
Bits
Sign
Bits
Exponent
Bits
Fraction
Bits
single
double
Chap
Figure
shows
the
floating
point
addition
of
and
The
result
is
After
the
fraction
and
exponent
bits
are
extracted
and
the
implicit
leading
is
prepended
in
steps
and
the
exponents
are
compared
by
subtracting
the
smaller
exponent
from
the
larger
exponent
The
result
is
the
number
of
bits
by
which
the
smaller
number
is
shifted
to
the
right
to
align
the
implied
binary
point
i
e
to
make
the
exponents
equal
in
step
The
aligned
numbers
are
added
Because
the
sum
has
a
mantissa
that
is
greater
than
or
equal
to
the
result
is
normalized
by
shifting
it
to
the
right
one
bit
and
incrementing
the
exponent
In
this
example
the
result
is
exact
so
no
rounding
is
necessary
The
result
is
stored
in
floating
point
notation
by
removing
the
implicit
leading
one
of
the
mantissa
and
prepending
the
sign
bit
Number
Systems
Figure
Floating
point
addition
Step
Exponent
Step
Step
shift
amount
Step
Step
Step
Step
Floating
point
numbers
Step
No
rounding
necessary
Fraction
Floating
point
arithmetic
is
usually
done
in
hardware
to
make
it
fast
This
hardware
called
the
floating
point
unit
FPU
is
typically
distinct
from
the
central
processing
unit
CPU
The
infamous
floating
point
division
FDIV
bug
in
the
Pentium
FPU
cost
Intel
million
to
recall
and
replace
defective
chips
The
bug
occurred
simply
because
a
lookup
table
was
not
loaded
correctly
C
SEQUENTIAL
BUILDING
BLOCKS
This
section
examines
sequential
building
blocks
including
counters
and
shift
registers
Counters
An
N
bit
binary
counter
shown
in
Figure
is
a
sequential
arithmetic
circuit
with
clock
and
reset
inputs
and
an
N
bit
output
Q
Reset
initializes
the
output
to
The
counter
then
advances
through
all
N
possible
outputs
in
binary
order
incrementing
on
the
rising
edge
of
the
clock
Figure
shows
an
N
bit
counter
composed
of
an
adder
and
a
resettable
register
On
each
cycle
the
counter
adds
to
the
value
stored
in
the
register
HDL
Example
describes
a
binary
counter
with
asynchronous
reset
Other
types
of
counters
such
as
Up
Down
counters
are
explored
in
Exercises
through
CHAPTER
FIVE
Digital
Building
Blocks
Figure
N
bit
counter
Figure
Counter
symbol
Q
CLK
Reset
N
N
CLK
Reset
B
S
A
N
Q
N
r
Verilog
module
counter
parameter
N
input
clk
input
reset
output
reg
N
q
always
posedge
clk
or
posedge
reset
if
reset
q
else
q
q
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
use
IEEE
STD
LOGIC
ARITH
ALL
entity
counter
is
generic
N
integer
port
clk
reset
in
STD
LOGIC
q
buffer
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
counter
is
begin
process
clk
reset
begin
if
reset
then
q
CONV
STD
LOGIC
VECTOR
N
elsif
clk
event
and
clk
then
q
q
end
if
end
process
end
HDL
Example
COUNTER
Figure
Synthesized
counter
R
q
reset
clk
Q
D
Chapter
Shift
Registers
A
shift
register
has
a
clock
a
serial
input
Sin
a
serial
output
Sout
and
N
parallel
outputs
QN
as
shown
in
Figure
On
each
rising
edge
of
the
clock
a
new
bit
is
shifted
in
from
Sin
and
all
the
subsequent
contents
are
shifted
forward
The
last
bit
in
the
shift
register
is
available
at
Sout
Shift
registers
can
be
viewed
as
serial
to
parallel
converters
The
input
is
provided
serially
one
bit
at
a
time
at
Sin
After
N
cycles
the
past
N
inputs
are
available
in
parallel
at
Q
A
shift
register
can
be
constructed
from
N
flip
flops
connected
in
series
as
shown
in
Figure
Some
shift
registers
also
have
a
reset
signal
to
initialize
all
of
the
flip
flops
A
related
circuit
is
a
parallel
to
serial
converter
that
loads
N
bits
in
parallel
then
shifts
them
out
one
at
a
time
A
shift
register
can
be
modified
to
perform
both
serial
to
parallel
and
parallel
to
serial
operations
by
adding
a
parallel
input
DN
and
a
control
signal
Load
as
shown
in
Figure
When
Load
is
asserted
the
flip
flops
are
loaded
in
parallel
from
the
D
inputs
Otherwise
the
shift
register
shifts
normally
HDL
Example
describes
such
a
shift
register
Scan
Chains
Shift
registers
are
often
used
to
test
sequential
circuits
using
a
technique
called
scan
chains
Testing
combinational
circuits
is
relatively
straightforward
Known
inputs
called
test
vectors
are
applied
and
the
outputs
are
checked
against
the
expected
result
Testing
sequential
circuits
is
more
difficult
because
the
circuits
have
state
Starting
from
a
known
initial
condition
a
large
number
of
cycles
of
test
vectors
may
be
needed
to
put
the
circuit
into
a
desired
state
For
example
testing
that
the
most
significant
bit
of
a
bit
counter
advances
from
to
requires
resetting
the
counter
then
applying
about
two
billion
clock
pulses
Sequential
Building
Blocks
Don
t
confuse
shift
registers
with
the
shifters
from
Section
Shift
registers
are
sequential
logic
blocks
that
shift
in
a
new
bit
on
each
clock
edge
Shifters
are
unclocked
combinational
logic
blocks
that
shift
an
input
by
a
specified
amount
Figure
Shift
register
symbol
N
Q
Sin
Sout
Figure
Shift
register
schematic
Figure
Shift
register
with
parallel
load
CLK
Sin
Sout
Q
Q
Q
QN
CLK
D
D
D
DN
Q
Q
Q
QN
Sin
Sout
Load
To
solve
this
problem
designers
like
to
be
able
to
directly
observe
and
control
all
the
state
of
the
machine
This
is
done
by
adding
a
test
mode
in
which
the
contents
of
all
flip
flops
can
be
read
out
or
loaded
with
desired
values
Most
systems
have
too
many
flip
flops
to
dedicate
individual
pins
to
read
and
write
each
flip
flop
Instead
all
the
flip
flops
in
the
system
are
connected
together
into
a
shift
register
called
a
scan
chain
In
normal
operation
the
flip
flops
load
data
from
their
D
input
and
ignore
the
scan
chain
In
test
mode
the
flip
flops
serially
shift
their
contents
out
and
shift
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
shiftreg
parameter
N
input
clk
input
reset
load
input
sin
input
N
d
output
reg
N
q
output
sout
always
posedge
clk
or
posedge
reset
if
reset
q
else
if
load
q
d
else
q
q
N
sin
assign
sout
q
N
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
ARITH
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
shiftreg
is
generic
N
integer
port
clk
reset
load
in
STD
LOGIC
sin
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
N
downto
q
buffer
STD
LOGIC
VECTOR
N
downto
sout
out
STD
LOGIC
end
architecture
synth
of
shiftreg
is
begin
process
clk
reset
begin
if
reset
then
q
CONV
STD
LOGIC
VECTOR
N
elsif
clk
event
and
clk
then
if
load
then
q
d
else
q
q
N
downto
sin
end
if
end
if
end
process
sout
q
N
end
HDL
Example
SHIFT
REGISTER
WITH
PARALLEL
LOAD
Figure
Synthesized
shiftreg
R
sout
q
d
sin
load
reset
clk
D
Q
Chapter
qxd
Memory
Arrays
Figure
Scannable
flip
flop
a
schematic
b
symbol
and
c
N
bit
scannable
register
Test
D
Sin
Q
Sout
a
D
Q
Sin
Sout
Test
b
D
Q
Sin
Sout
Test
D
Q
Sin
Sout
Test
D
Q
Sin
Sout
Test
D
Q
Sin
Sout
Test
c
Test
CLK
CLK
CLK
D
Q
D
Q
D
Q
DN
QN
Sin
Sout
in
new
contents
using
Sin
and
Sout
The
load
multiplexer
is
usually
integrated
into
the
flip
flop
to
produce
a
scannable
flip
flop
Figure
shows
the
schematic
and
symbol
for
a
scannable
flip
flop
and
illustrates
how
the
flops
are
cascaded
to
build
an
N
bit
scannable
register
For
example
the
bit
counter
could
be
tested
by
shifting
in
the
pattern
in
test
mode
counting
for
one
cycle
in
normal
mode
then
shifting
out
the
result
which
should
be
This
requires
only
cycles
MEMORY
ARRAYS
The
previous
sections
introduced
arithmetic
and
sequential
circuits
for
manipulating
data
Digital
systems
also
require
memories
to
store
the
data
used
and
generated
by
such
circuits
Registers
built
from
flipflops
are
a
kind
of
memory
that
stores
small
amounts
of
data
This
section
describes
memory
arrays
that
can
efficiently
store
large
amounts
of
data
The
section
begins
with
an
overview
describing
characteristics
shared
by
all
memory
arrays
It
then
introduces
three
types
of
memory
arrays
dynamic
random
access
memory
DRAM
static
random
access
memory
SRAM
and
read
only
memory
ROM
Each
memory
differs
in
the
way
it
stores
data
The
section
briefly
discusses
area
and
delay
trade
offs
and
shows
how
memory
arrays
are
used
not
only
to
store
data
but
also
to
perform
logic
functions
The
section
finishes
with
the
HDL
for
a
memory
array
Overview
Figure
shows
a
generic
symbol
for
a
memory
array
The
memory
is
organized
as
a
two
dimensional
array
of
memory
cells
The
memory
reads
or
writes
the
contents
of
one
of
the
rows
of
the
array
This
row
is
Address
Data
Array
N
M
Figure
Generic
memory
array
symbol
C
specified
by
an
Address
The
value
read
or
written
is
called
Data
An
array
with
N
bit
addresses
and
M
bit
data
has
N
rows
and
M
columns
Each
row
of
data
is
called
a
word
Thus
the
array
contains
N
M
bit
words
Figure
shows
a
memory
array
with
two
address
bits
and
three
data
bits
The
two
address
bits
specify
one
of
the
four
rows
data
words
in
the
array
Each
data
word
is
three
bits
wide
Figure
b
shows
some
possible
contents
of
the
memory
array
The
depth
of
an
array
is
the
number
of
rows
and
the
width
is
the
number
of
columns
also
called
the
word
size
The
size
of
an
array
is
given
as
depth
width
Figure
is
a
word
bit
array
or
simply
array
The
symbol
for
a
word
bit
array
is
shown
in
Figure
The
total
size
of
this
array
is
kilobits
Kb
Bit
Cells
Memory
arrays
are
built
as
an
array
of
bit
cells
each
of
which
stores
bit
of
data
Figure
shows
that
each
bit
cell
is
connected
to
a
wordline
and
a
bitline
For
each
combination
of
address
bits
the
memory
asserts
a
single
wordline
that
activates
the
bit
cells
in
that
row
When
the
wordline
is
HIGH
the
stored
bit
transfers
to
or
from
the
bitline
Otherwise
the
bitline
is
disconnected
from
the
bit
cell
The
circuitry
to
store
the
bit
varies
with
memory
type
To
read
a
bit
cell
the
bitline
is
initially
left
floating
Z
Then
the
wordline
is
turned
ON
allowing
the
stored
value
to
drive
the
bitline
to
or
To
write
a
bit
cell
the
bitline
is
strongly
driven
to
the
desired
value
Then
the
wordline
is
turned
ON
connecting
the
bitline
to
the
stored
bit
The
strongly
driven
bitline
overpowers
the
contents
of
the
bit
cell
writing
the
desired
value
into
the
stored
bit
Organization
Figure
shows
the
internal
organization
of
a
memory
array
Of
course
practical
memories
are
much
larger
but
the
behavior
of
larger
arrays
can
be
extrapolated
from
the
smaller
array
In
this
example
the
array
stores
the
data
from
Figure
b
During
a
memory
read
a
wordline
is
asserted
and
the
corresponding
row
of
bit
cells
drives
the
bitlines
HIGH
or
LOW
During
a
memory
write
the
bitlines
are
driven
HIGH
or
LOW
first
and
then
a
wordline
is
asserted
allowing
the
bitline
values
to
be
stored
in
that
row
of
bit
cells
For
example
to
read
Address
the
bitlines
are
left
floating
the
decoder
asserts
wordline
and
the
data
stored
in
that
row
of
bit
cells
reads
out
onto
the
Data
bitlines
To
write
the
value
to
Address
the
bitlines
are
driven
to
the
value
then
wordline
is
asserted
and
the
new
value
is
stored
in
the
bit
cells
CHAPTER
FIVE
Digital
Building
Blocks
a
Address
Data
Array
b
Address
depth
width
Data
Figure
memory
array
a
symbol
b
function
Address
Data
word
bit
Array
Figure
Kb
array
depth
words
width
bits
stored
bit
wordline
bitline
Figure
Bit
cell
Cha
Memory
Ports
All
memories
have
one
or
more
ports
Each
port
gives
read
and
or
write
access
to
one
memory
address
The
previous
examples
were
all
singleported
memories
Multiported
memories
can
access
several
addresses
simultaneously
Figure
shows
a
three
ported
memory
with
two
read
ports
and
one
write
port
Port
reads
the
data
from
address
A
onto
the
read
data
output
RD
Port
reads
the
data
from
address
A
onto
RD
Port
writes
the
data
from
the
write
data
input
WD
into
address
A
on
the
rising
edge
of
the
clock
if
the
write
enable
WE
is
asserted
Memory
Types
Memory
arrays
are
specified
by
their
size
depth
width
and
the
number
and
type
of
ports
All
memory
arrays
store
data
as
an
array
of
bit
cells
but
they
differ
in
how
they
store
bits
Memories
are
classified
based
on
how
they
store
bits
in
the
bit
cell
The
broadest
classification
is
random
access
memory
RAM
versus
read
only
memory
ROM
RAM
is
volatile
meaning
that
it
loses
its
data
when
the
power
is
turned
off
ROM
is
nonvolatile
meaning
that
it
retains
its
data
indefinitely
even
without
a
power
source
RAM
and
ROM
received
their
names
for
historical
reasons
that
are
no
longer
very
meaningful
RAM
is
called
random
access
memory
because
any
data
word
is
accessed
with
the
same
delay
as
any
other
A
sequential
access
memory
such
as
a
tape
recorder
accesses
nearby
data
more
quickly
than
faraway
data
e
g
at
the
other
end
of
the
tape
Memory
Arrays
wordline
Decoder
Address
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
wordline
wordline
wordline
bitline
bitline
bitline
Data
Data
Data
Figure
memory
array
A
A
WD
WE
A
CLK
Array
RD
RD
M
M
N
N
N
M
Figure
Three
ported
memory
ROM
is
called
read
only
memory
because
historically
it
could
only
be
read
but
not
written
These
names
are
confusing
because
ROMs
are
randomly
accessed
too
Worse
yet
most
modern
ROMs
can
be
written
as
well
as
read
The
important
distinction
to
remember
is
that
RAMs
are
volatile
and
ROMs
are
nonvolatile
The
two
major
types
of
RAMs
are
dynamic
RAM
DRAM
and
static
RAM
SRAM
Dynamic
RAM
stores
data
as
a
charge
on
a
capacitor
whereas
static
RAM
stores
data
using
a
pair
of
cross
coupled
inverters
There
are
many
flavors
of
ROMs
that
vary
by
how
they
are
written
and
erased
These
various
types
of
memories
are
discussed
in
the
subsequent
sections
Dynamic
Random
Access
Memory
Dynamic
RAM
DRAM
pronounced
dee
ram
stores
a
bit
as
the
presence
or
absence
of
charge
on
a
capacitor
Figure
shows
a
DRAM
bit
cell
The
bit
value
is
stored
on
a
capacitor
The
nMOS
transistor
behaves
as
a
switch
that
either
connects
or
disconnects
the
capacitor
from
the
bitline
When
the
wordline
is
asserted
the
nMOS
transistor
turns
ON
and
the
stored
bit
value
transfers
to
or
from
the
bitline
As
shown
in
Figure
a
when
the
capacitor
is
charged
to
VDD
the
stored
bit
is
when
it
is
discharged
to
GND
Figure
b
the
stored
bit
is
The
capacitor
node
is
dynamic
because
it
is
not
actively
driven
HIGH
or
LOW
by
a
transistor
tied
to
VDD
or
GND
Upon
a
read
data
values
are
transferred
from
the
capacitor
to
the
bitline
Upon
a
write
data
values
are
transferred
from
the
bitline
to
the
capacitor
Reading
destroys
the
bit
value
stored
on
the
capacitor
so
the
data
word
must
be
restored
rewritten
after
each
read
Even
when
DRAM
is
not
read
the
contents
must
be
refreshed
read
and
rewritten
every
few
milliseconds
because
the
charge
on
the
capacitor
gradually
leaks
away
Static
Random
Access
Memory
SRAM
Static
RAM
SRAM
pronounced
es
ram
is
static
because
stored
bits
do
not
need
to
be
refreshed
Figure
shows
an
SRAM
bit
cell
The
data
bit
is
stored
on
cross
coupled
inverters
like
those
described
in
Section
Each
cell
has
two
outputs
bitline
and
When
the
wordline
is
asserted
both
nMOS
transistors
turn
on
and
data
values
are
transferred
to
or
from
the
bitlines
Unlike
DRAM
if
noise
degrades
the
value
of
the
stored
bit
the
cross
coupled
inverters
restore
the
value
bitline
CHAPTER
FIVE
Digital
Building
Blocks
Robert
Dennard
Invented
DRAM
in
at
IBM
Although
many
were
skeptical
that
the
idea
would
work
by
the
mid
s
DRAM
was
in
virtually
all
computers
He
claims
to
have
done
little
creative
work
until
arriving
at
IBM
they
handed
him
a
patent
notebook
and
said
put
all
your
ideas
in
there
Since
he
has
received
patents
in
semiconductors
and
microelectronics
Photo
courtesy
of
IBM
wordline
bitline
stored
bit
Figure
DRAM
bit
cell
Area
and
Delay
Flip
flops
SRAMs
and
DRAMs
are
all
volatile
memories
but
each
has
different
area
and
delay
characteristics
Table
shows
a
comparison
of
these
three
types
of
volatile
memory
The
data
bit
stored
in
a
flip
flop
is
available
immediately
at
its
output
But
flip
flops
take
at
least
transistors
to
build
Generally
the
more
transistors
a
device
has
the
more
area
power
and
cost
it
requires
DRAM
latency
is
longer
than
that
of
SRAM
because
its
bitline
is
not
actively
driven
by
a
transistor
DRAM
must
wait
for
charge
to
move
relatively
slowly
from
the
capacitor
to
the
bitline
DRAM
also
has
lower
throughput
than
SRAM
because
it
must
refresh
data
periodically
and
after
a
read
Memory
latency
and
throughput
also
depend
on
memory
size
larger
memories
tend
to
be
slower
than
smaller
ones
if
all
else
is
the
same
The
best
memory
type
for
a
particular
design
depends
on
the
speed
cost
and
power
constraints
Register
Files
Digital
systems
often
use
a
number
of
registers
to
store
temporary
variables
This
group
of
registers
called
a
register
file
is
usually
built
as
a
small
multiported
SRAM
array
because
it
is
more
compact
than
an
array
of
flip
flops
Figure
shows
a
register
bit
three
ported
register
file
built
from
a
three
ported
memory
similar
to
that
of
Figure
The
Memory
Arrays
Figure
DRAM
stored
values
stored
bit
wordline
bitline
bitline
Figure
SRAM
bit
cell
Table
Memory
comparison
Memory
Transistors
per
Latency
Type
Bit
Cell
flip
flop
fast
SRAM
medium
DRAM
slow
CLK
A
A
WD
RD
RD
WE
A
Register
File
Figure
register
file
with
two
read
ports
and
one
write
port
wordline
bitline
a
stored
bit
wordline
bitline
b
stored
bit
register
file
has
two
read
ports
A
RD
and
A
RD
and
one
write
port
A
WD
The
bit
addresses
A
A
and
A
can
each
access
all
registers
So
two
registers
can
be
read
and
one
register
written
simultaneously
Read
Only
Memory
Read
only
memory
ROM
stores
a
bit
as
the
presence
or
absence
of
a
transistor
Figure
shows
a
simple
ROM
bit
cell
To
read
the
cell
the
bitline
is
weakly
pulled
HIGH
Then
the
wordline
is
turned
ON
If
the
transistor
is
present
it
pulls
the
bitline
LOW
If
it
is
absent
the
bitline
remains
HIGH
Note
that
the
ROM
bit
cell
is
a
combinational
circuit
and
has
no
state
to
forget
if
power
is
turned
off
The
contents
of
a
ROM
can
be
indicated
using
dot
notation
Figure
shows
the
dot
notation
for
a
word
bit
ROM
containing
the
data
from
Figure
A
dot
at
the
intersection
of
a
row
wordline
and
a
column
bitline
indicates
that
the
data
bit
is
For
example
the
top
wordline
has
a
single
dot
on
Data
so
the
data
word
stored
at
Address
is
Conceptually
ROMs
can
be
built
using
two
level
logic
with
a
group
of
AND
gates
followed
by
a
group
of
OR
gates
The
AND
gates
produce
all
possible
minterms
and
hence
form
a
decoder
Figure
shows
the
ROM
of
Figure
built
using
a
decoder
and
OR
gates
Each
dotted
row
in
Figure
is
an
input
to
an
OR
gate
in
Figure
For
data
bits
with
a
single
dot
in
this
case
Data
no
OR
gate
is
needed
This
representation
of
a
ROM
is
interesting
because
it
shows
how
the
ROM
can
perform
any
two
level
logic
function
In
practice
ROMs
are
built
from
transistors
instead
of
logic
gates
to
reduce
their
size
and
cost
Section
explores
the
transistor
level
implementation
further
CHAPTER
FIVE
Digital
Building
Blocks
wordline
bitline
wordline
bitline
bit
cell
containing
bit
cell
containing
Figure
ROM
bit
cells
containing
and
Decoder
Address
Data
Data
Data
Figure
ROM
dot
notation
C
The
contents
of
the
ROM
bit
cell
in
Figure
are
specified
during
manufacturing
by
the
presence
or
absence
of
a
transistor
in
each
bit
cell
A
programmable
ROM
PROM
pronounced
like
the
dance
places
a
transistor
in
every
bit
cell
but
provides
a
way
to
connect
or
disconnect
the
transistor
to
ground
Figure
shows
the
bit
cell
for
a
fuse
programmable
ROM
The
user
programs
the
ROM
by
applying
a
high
voltage
to
selectively
blow
fuses
If
the
fuse
is
present
the
transistor
is
connected
to
GND
and
the
cell
holds
a
If
the
fuse
is
destroyed
the
transistor
is
disconnected
from
ground
and
the
cell
holds
a
This
is
also
called
a
onetime
programmable
ROM
because
the
fuse
cannot
be
repaired
once
it
is
blown
Reprogrammable
ROMs
provide
a
reversible
mechanism
for
connecting
or
disconnecting
the
transistor
to
GND
Erasable
PROMs
EPROMs
pronounced
e
proms
replace
the
nMOS
transistor
and
fuse
with
a
floating
gate
transistor
The
floating
gate
is
not
physically
attached
to
any
other
wires
When
suitable
high
voltages
are
applied
electrons
tunnel
through
an
insulator
onto
the
floating
gate
turning
on
the
transistor
and
connecting
the
bitline
to
the
wordline
decoder
output
When
the
EPROM
is
exposed
to
intense
ultraviolet
UV
light
for
about
half
an
hour
the
electrons
are
knocked
off
the
floating
gate
turning
the
transistor
off
These
actions
are
called
programming
and
erasing
respectively
Electrically
erasable
PROMs
EEPROMs
pronounced
e
e
proms
or
double
e
proms
and
Flash
memory
use
similar
principles
but
include
circuitry
on
the
chip
for
erasing
as
well
as
programming
so
no
UV
light
is
necessary
EEPROM
bit
cells
are
individually
erasable
Flash
memory
erases
larger
blocks
of
bits
and
is
cheaper
because
fewer
erasing
circuits
are
needed
In
Flash
Memory
Arrays
Decoder
Data
Data
Data
Address
Figure
ROM
implementation
using
gates
wordline
bitline
bit
cell
containing
intact
fuse
wordline
bitline
bit
cell
containing
blown
fuse
Figure
Fuse
programmable
ROM
bit
cell
Fujio
Masuoka
Received
a
Ph
D
in
electrical
engineering
from
Tohoku
University
Japan
Developed
memories
and
highspeed
circuits
at
Toshiba
from
to
Invented
Flash
memory
as
an
unauthorized
project
pursued
during
nights
and
weekends
in
the
late
s
Flash
received
its
name
because
the
process
of
erasing
the
memory
reminds
one
of
the
flash
of
a
camera
Toshiba
was
slow
to
commercialize
the
idea
Intel
was
first
to
market
in
Flash
has
grown
into
a
billion
per
year
market
Dr
Masuoka
later
joined
the
faculty
at
Tohoku
University
memory
costs
less
than
per
GB
and
the
price
continues
to
drop
by
to
per
year
Flash
has
become
an
extremely
popular
way
to
store
large
amounts
of
data
in
portable
battery
powered
systems
such
as
cameras
and
music
players
In
summary
modern
ROMs
are
not
really
read
only
they
can
be
programmed
written
as
well
The
difference
between
RAM
and
ROM
is
that
ROMs
take
a
longer
time
to
write
but
are
nonvolatile
Logic
Using
Memory
Arrays
Although
they
are
used
primarily
for
data
storage
memory
arrays
can
also
perform
combinational
logic
functions
For
example
the
Data
output
of
the
ROM
in
Figure
is
the
XOR
of
the
two
Address
inputs
Likewise
Data
is
the
NAND
of
the
two
inputs
A
N
word
M
bit
memory
can
perform
any
combinational
function
of
N
inputs
and
M
outputs
For
example
the
ROM
in
Figure
performs
three
functions
of
two
inputs
Memory
arrays
used
to
perform
logic
are
called
lookup
tables
LUTs
Figure
shows
a
word
bit
memory
array
used
as
a
lookup
table
to
perform
the
function
Y
AB
Using
memory
to
perform
logic
the
user
can
look
up
the
output
value
for
a
given
input
combination
address
Each
address
corresponds
to
a
row
in
the
truth
table
and
each
data
bit
corresponds
to
an
output
value
Memory
HDL
HDL
Example
describes
a
N
word
M
bit
RAM
The
RAM
has
a
synchronous
enabled
write
In
other
words
writes
occur
on
the
rising
CHAPTER
FIVE
Digital
Building
Blocks
stored
bit
stored
bit
Decoder
A
stored
bit
bitline
stored
bit
Y
B
word
x
bit
Array
ABY
Truth
Table
A
A
Figure
word
bit
memory
array
used
as
a
lookup
table
Programmable
ROMs
can
be
configured
with
a
device
programmer
like
the
one
shown
below
The
device
programmer
is
attached
to
a
computer
which
specifies
the
type
of
ROM
and
the
data
values
to
program
The
device
programmer
blows
fuses
or
injects
charge
onto
a
floating
gate
on
the
ROM
Thus
the
programming
process
is
sometimes
called
burning
a
ROM
Flash
memory
drives
with
Universal
Serial
Bus
USB
connectors
have
replaced
floppy
disks
and
CDs
for
sharing
files
because
Flash
costs
have
dropped
so
dramatically
C
edge
of
the
clock
if
the
write
enable
we
is
asserted
Reads
occur
immediately
When
power
is
first
applied
the
contents
of
the
RAM
are
unpredictable
HDL
Example
describes
a
word
bit
ROM
The
contents
of
the
ROM
are
specified
in
the
HDL
case
statement
A
ROM
as
small
as
this
one
may
be
synthesized
into
logic
gates
rather
than
an
array
Note
that
the
seven
segment
decoder
from
HDL
Example
synthesizes
into
a
ROM
in
Figure
Memory
Arrays
Verilog
module
ram
parameter
N
M
input
clk
input
we
input
N
adr
input
M
din
output
M
dout
reg
M
mem
N
always
posedge
clk
if
we
mem
adr
din
assign
dout
mem
adr
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
ARITH
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
ram
array
is
generic
N
integer
M
integer
port
clk
we
in
STD
LOGIC
adr
in
STD
LOGIC
VECTOR
N
downto
din
in
STD
LOGIC
VECTOR
M
downto
dout
out
STD
LOGIC
VECTOR
M
downto
end
architecture
synth
of
ram
array
is
type
mem
array
is
array
N
downto
of
STD
LOGIC
VECTOR
M
downto
signal
mem
mem
array
begin
process
clk
begin
if
clk
event
and
clk
then
if
we
then
mem
CONV
INTEGER
adr
din
end
if
end
if
end
process
dout
mem
CONV
INTEGER
adr
end
HDL
Example
RAM
ram
mem
dout
din
adr
we
clk
RADDR
DATA
DOUT
WADDR
WE
CLK
Figure
Synthesized
ram
Chapter
qxd
LOGIC
ARRAYS
Like
memory
gates
can
be
organized
into
regular
arrays
If
the
connections
are
made
programmable
these
logic
arrays
can
be
configured
to
perform
any
function
without
the
user
having
to
connect
wires
in
specific
ways
The
regular
structure
simplifies
design
Logic
arrays
are
mass
produced
in
large
quantities
so
they
are
inexpensive
Software
tools
allow
users
to
map
logic
designs
onto
these
arrays
Most
logic
arrays
are
also
reconfigurable
allowing
designs
to
be
modified
without
replacing
the
hardware
Reconfigurability
is
valuable
during
development
and
is
also
useful
in
the
field
because
a
system
can
be
upgraded
by
simply
downloading
the
new
configuration
This
section
introduces
two
types
of
logic
arrays
programmable
logic
arrays
PLAs
and
field
programmable
gate
arrays
FPGAs
PLAs
the
older
technology
perform
only
combinational
logic
functions
FPGAs
can
perform
both
combinational
and
sequential
logic
Programmable
Logic
Array
Programmable
logic
arrays
PLAs
implement
two
level
combinational
logic
in
sum
of
products
SOP
form
PLAs
are
built
from
an
AND
array
followed
by
an
OR
array
as
shown
in
Figure
The
inputs
in
true
and
complementary
form
drive
an
AND
array
which
produces
implicants
which
in
turn
are
ORed
together
to
form
the
outputs
An
M
N
P
bit
PLA
has
M
inputs
N
implicants
and
P
outputs
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
rom
input
adr
output
reg
dout
always
adr
case
adr
b
dout
b
b
dout
b
b
dout
b
b
dout
b
endcase
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
rom
is
port
adr
in
STD
LOGIC
VECTOR
downto
dout
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
rom
is
begin
process
adr
begin
case
adr
is
when
dout
when
dout
when
dout
when
dout
end
case
end
process
end
HDL
Example
ROM
Chapter
q
Figure
shows
the
dot
notation
for
a
bit
PLA
performing
the
functions
and
Each
row
in
the
AND
array
forms
an
implicant
Dots
in
each
row
of
the
AND
array
indicate
which
literals
comprise
the
implicant
The
AND
array
in
Figure
forms
three
implicants
and
Dots
in
the
OR
array
indicate
which
implicants
are
part
of
the
output
function
Figure
shows
how
PLAs
can
be
built
using
two
level
logic
An
alternative
implementation
is
given
in
Section
ROMs
can
be
viewed
as
a
special
case
of
PLAs
A
M
word
N
bit
ROM
is
simply
an
M
M
N
bit
PLA
The
decoder
behaves
as
an
AND
plane
that
produces
all
M
minterms
The
ROM
array
behaves
as
an
OR
plane
that
produces
the
outputs
If
the
function
does
not
depend
on
all
M
minterms
a
PLA
is
likely
to
be
smaller
than
a
ROM
For
example
an
word
bit
ROM
is
required
to
perform
the
same
functions
performed
by
the
bit
PLA
shown
in
Figures
and
ABC
ABC
AB
X
ABC
ABC
Y
AB
Logic
Arrays
AND
Array
OR
Array
Inputs
Outputs
Implicants
N
M
P
Figure
M
N
P
bit
PLA
X
Y
ABC
AB
ABC
ABC
AND
Array
OR
Array
Figure
bit
PLA
dot
notation
Ch
Programmable
logic
devices
PLDs
are
souped
up
PLAs
that
add
registers
and
various
other
features
to
the
basic
AND
OR
planes
However
PLDs
and
PLAs
have
largely
been
displaced
by
FPGAs
which
are
more
flexible
and
efficient
for
building
large
systems
Field
Programmable
Gate
Array
A
field
programmable
gate
array
FPGA
is
an
array
of
reconfigurable
gates
Using
software
programming
tools
a
user
can
implement
designs
on
the
FPGA
using
either
an
HDL
or
a
schematic
FPGAs
are
more
powerful
and
more
flexible
than
PLAs
for
several
reasons
They
can
implement
both
combinational
and
sequential
logic
They
can
also
implement
multilevel
logic
functions
whereas
PLAs
can
only
implement
two
level
logic
Modern
FPGAs
integrate
other
useful
functions
such
as
built
in
multipliers
and
large
RAM
arrays
FPGAs
are
built
as
an
array
of
configurable
logic
blocks
CLBs
Figure
shows
the
block
diagram
of
the
Spartan
FPGA
introduced
by
Xilinx
in
Each
CLB
can
be
configured
to
perform
combinational
or
sequential
functions
The
CLBs
are
surrounded
by
input
output
blocks
IOBs
for
interfacing
with
external
devices
The
IOBs
connect
CLB
inputs
and
outputs
to
pins
on
the
chip
package
CLBs
can
connect
to
other
CLBs
and
IOBs
through
programmable
routing
channels
The
remaining
blocks
shown
in
the
figure
aid
in
programming
the
device
Figure
shows
a
single
CLB
for
the
Spartan
FPGA
Other
brands
of
FPGAs
are
organized
somewhat
differently
but
the
same
general
principles
apply
The
CLB
contains
lookup
tables
LUTs
configurable
multiplexers
and
registers
The
FPGA
is
configured
by
specifying
the
contents
of
the
lookup
tables
and
the
select
signals
for
the
multiplexers
CHAPTER
FIVE
Digital
Building
Blocks
FPGAs
are
the
brains
of
many
consumer
products
including
automobiles
medical
equipment
and
media
devices
like
MP
players
The
Mercedes
Benz
S
Class
series
for
example
has
over
a
dozen
Xilinx
FPGAs
or
PLDs
for
uses
ranging
from
entertainment
to
navigation
to
cruise
control
systems
FPGAs
allow
for
quick
time
to
market
and
make
debugging
or
adding
features
late
in
the
design
process
easier
X
Y
ABC
AND
ARRAY
OR
ARRAY
ABC
AB
ABC
Figure
bit
PLA
using
two
level
logic
Each
Spartan
CLB
has
three
LUTs
the
four
input
F
and
G
LUTs
and
the
three
input
H
LUT
By
loading
the
appropriate
values
into
the
lookup
tables
the
F
and
G
LUTs
can
each
be
configured
to
perform
any
function
of
up
to
four
variables
and
the
H
LUT
can
perform
any
function
of
up
to
three
variables
Configuring
the
FPGA
also
involves
choosing
the
select
signals
that
determine
how
the
multiplexers
route
data
through
the
CLB
For
example
depending
on
the
multiplexer
configuration
the
H
LUT
may
receive
one
of
its
inputs
from
either
DIN
or
the
F
LUT
Similarly
it
receives
another
input
from
either
SR
or
the
G
LUT
The
third
input
always
comes
from
H
Logic
Arrays
Figure
Spartan
block
diagram
The
FPGA
produces
two
combinational
outputs
X
and
Y
Depending
on
the
multiplexer
configuration
X
comes
from
either
the
F
or
H
LUT
Y
comes
from
either
the
G
or
H
LUT
These
outputs
can
be
connected
to
other
CLBs
via
the
routing
channels
The
CLB
also
contains
two
flip
flops
Depending
on
the
configuration
the
flip
flop
inputs
may
come
from
DIN
or
from
the
F
G
or
H
LUT
The
flip
flop
outputs
XQ
and
YQ
also
can
be
connected
to
other
CLBs
via
the
routing
channels
In
summary
the
CLB
can
perform
up
to
two
combinational
and
or
two
registered
functions
All
of
the
functions
can
involve
at
least
four
variables
and
some
can
involve
up
to
nine
The
designer
configures
an
FPGA
by
first
creating
a
schematic
or
HDL
description
of
the
design
The
design
is
then
synthesized
onto
the
FPGA
The
synthesis
tool
determines
how
the
LUTs
multiplexers
and
routing
channels
should
be
configured
to
perform
the
specified
functions
This
configuration
information
is
then
downloaded
to
the
FPGA
Because
Xilinx
FPGAs
store
their
configuration
information
in
SRAM
they
can
be
easily
reprogrammed
They
may
download
the
SRAM
contents
from
a
computer
in
the
laboratory
or
from
an
EEPROM
chip
when
the
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Spartan
CLB
system
is
turned
on
Some
manufacturers
include
EEPROM
directly
on
the
FPGA
or
use
one
time
programmable
fuses
to
configure
the
FPGA
Example
FUNCTIONS
BUILT
USING
CLBS
Explain
how
to
configure
a
CLB
to
perform
the
following
functions
a
and
b
Y
JKLMPQR
c
a
divide
by
counter
with
binary
state
encoding
see
Figure
a
Solution
a
Configure
the
F
LUT
to
compute
X
and
the
G
LUT
to
compute
Y
as
shown
in
Figure
Inputs
F
F
and
F
are
A
B
and
C
respectively
these
connections
are
set
by
the
routing
channels
Inputs
G
and
G
are
A
and
B
F
G
and
G
are
don
t
cares
and
may
be
connected
to
Configure
the
final
multiplexers
to
select
X
from
the
F
LUT
and
Y
from
the
G
LUT
In
general
a
CLB
can
compute
any
two
functions
of
up
to
four
variables
each
in
this
fashion
b
Configure
the
F
LUT
to
compute
F
JKLM
and
the
G
LUT
to
compute
G
PQR
Then
configure
the
H
LUT
to
compute
H
FG
Configure
the
final
multiplexer
to
select
Y
from
the
H
LUT
This
configuration
is
shown
in
Figure
In
general
a
CLB
can
compute
certain
functions
of
up
to
nine
variables
in
this
way
c
The
FSM
has
two
bits
of
state
S
and
one
output
Y
The
next
state
depends
on
the
two
bits
of
current
state
Use
the
F
LUT
and
G
LUT
to
compute
the
next
state
from
the
current
state
as
shown
in
Figure
Use
the
two
flipflops
to
hold
this
state
The
flip
flops
have
a
dedicated
reset
input
from
the
SR
signal
in
the
CLB
The
registered
outputs
are
fed
back
to
the
inputs
using
the
routing
channels
as
indicated
by
the
dashed
blue
lines
In
general
another
CLB
might
be
necessary
to
compute
the
output
Y
However
in
this
case
Y
so
Y
can
come
from
the
same
F
LUT
used
to
compute
Hence
the
entire
FSM
fits
in
a
single
CLB
In
general
an
FSM
requires
at
least
one
CLB
for
every
two
bits
of
state
and
it
may
require
more
CLBs
for
the
output
or
next
state
logic
if
they
are
too
complex
to
fit
in
a
single
LUT
S
S
X
ABC
ABC
Y
AB
Logic
Arrays
F
F
F
F
F
F
F
F
F
X
X
X
X
X
X
X
X
F
A
B
C
X
G
G
G
G
X
X
X
X
X
X
X
X
G
A
B
Y
G
G
G
G
G
A
B
A
B
C
Y
X
Figure
CLB
configuration
for
two
functions
of
up
to
four
inputs
each
Chapter
Example
CLB
DELAY
Alyssa
P
Hacker
is
building
a
finite
state
machine
that
must
run
at
MHz
She
uses
a
Spartan
FPGA
with
the
following
specifications
tCLB
ns
per
CLB
tsetup
ns
and
tpcq
ns
for
all
flip
flops
What
is
the
maximum
number
of
CLBs
her
design
can
use
You
can
ignore
interconnect
delay
Solution
Alyssa
uses
Equation
to
solve
for
the
maximum
propagation
delay
of
the
logic
tpd
Tc
tpcq
tsetup
Thus
tpd
ns
ns
ns
so
tpd
ns
The
delay
of
each
CLB
tCLB
is
ns
and
the
maximum
number
of
CLBs
N
is
NtCLB
ns
Thus
N
CHAPTER
FIVE
Digital
Building
Blocks
F
F
F
F
F
F
F
F
F
X
X
X
X
X
X
X
X
F
S
S
G
G
G
G
X
X
X
X
X
X
X
X
G
G
G
G
G
G
S
YQ
XQ
S
S
S
S
S
clk
clk
Reset
Reset
Y
S
S
Figure
CLB
configuration
for
FSM
with
two
bits
of
state
F
F
F
F
F
F
F
F
F
F
K
L
M
G
G
G
G
G
P
Q
G
G
G
G
G
P
Q
R
K
L
M
J
J
R
Y
X
X
X
X
X
X
X
X
H
FGH
X
X
X
X
G
H
F
H
Y
Figure
CLB
configuration
for
one
function
of
more
than
four
inputs
Chapte
Array
Implementations
To
minimize
their
size
and
cost
ROMs
and
PLAs
commonly
use
pseudonMOS
or
dynamic
circuits
see
Section
instead
of
conventional
logic
gates
Figure
a
shows
the
dot
notation
for
a
bit
ROM
that
performs
the
following
functions
X
A
B
and
Z
These
are
the
same
functions
as
those
of
Figure
with
the
address
inputs
renamed
A
and
B
and
the
data
outputs
renamed
X
Y
and
Z
The
pseudo
nMOS
implementation
is
given
in
Figure
b
Each
decoder
output
is
connected
to
the
gates
of
the
nMOS
transistors
in
its
row
Remember
that
in
pseudo
nMOS
circuits
the
weak
pMOS
transistor
pulls
the
output
HIGH
only
if
there
is
no
path
to
GND
through
the
pulldown
nMOS
network
Pull
down
transistors
are
placed
at
every
junction
without
a
dot
The
dots
from
the
dot
notation
diagram
of
Figure
a
are
left
faintly
visible
in
Figure
b
for
easy
comparison
The
weak
pull
up
transistors
pull
the
output
HIGH
for
each
wordline
without
a
pull
down
transistor
For
example
when
AB
the
wordline
is
HIGH
and
transistors
on
X
and
Z
turn
on
and
pull
those
outputs
LOW
The
Y
output
has
no
transistor
connecting
to
the
wordline
so
Y
is
pulled
HIGH
by
the
weak
pull
up
PLAs
can
also
be
built
using
pseudo
nMOS
circuits
as
shown
in
Figure
for
the
PLA
from
Figure
Pull
down
nMOS
transistors
are
placed
on
the
complement
of
dotted
literals
in
the
AND
array
and
on
dotted
rows
in
the
OR
array
The
columns
in
the
OR
array
are
sent
through
an
inverter
before
they
are
fed
to
the
output
bits
Again
the
blue
dots
from
the
dot
notation
diagram
of
Figure
are
left
faintly
visible
in
Figure
for
easy
comparison
Y
A
B
AB
Logic
Arrays
Decoder
A
A
X
a
A
B
Y
Z
Decoder
A
A
A
B
weak
b
XYZ
Figure
ROM
implementation
a
dot
notation
b
pseudo
nMOS
circuit
Many
ROMs
and
PLAs
use
dynamic
circuits
in
place
of
pseudo
nMOS
circuits
Dynamic
gates
turn
the
pMOS
transistor
ON
for
only
part
of
the
time
saving
power
when
the
pMOS
is
OFF
and
the
result
is
not
needed
Aside
from
this
dynamic
and
pseudo
nMOS
memory
arrays
are
similar
in
design
and
behavior
Chapt
SUMMARY
This
chapter
introduced
digital
building
blocks
used
in
many
digital
systems
These
blocks
include
arithmetic
circuits
such
as
adders
subtractors
comparators
shifters
multipliers
and
dividers
sequential
circuits
such
as
counters
and
shift
registers
and
arrays
for
memory
and
logic
The
chapter
also
explored
fixed
point
and
floating
point
representations
of
fractional
numbers
In
Chapter
we
use
these
building
blocks
to
build
a
microprocessor
Adders
form
the
basis
of
most
arithmetic
circuits
A
half
adder
adds
two
bit
inputs
A
and
B
and
produces
a
sum
and
a
carry
out
A
full
adder
extends
the
half
adder
to
also
accept
a
carry
in
N
full
adders
can
be
cascaded
to
form
a
carry
propagate
adder
CPA
that
adds
two
N
bit
numbers
This
type
of
CPA
is
called
a
ripple
carry
adder
because
the
carry
ripples
through
each
of
the
full
adders
Faster
CPAs
can
be
constructed
using
lookahead
or
prefix
techniques
A
subtractor
negates
the
second
input
and
adds
it
to
the
first
A
magnitude
comparator
subtracts
one
number
from
another
and
determines
the
relative
value
based
on
the
sign
of
the
result
A
multiplier
forms
partial
products
using
AND
gates
then
sums
these
bits
using
full
adders
A
divider
repeatedly
subtracts
the
divisor
from
the
partial
remainder
and
checks
the
sign
of
the
difference
to
determine
the
quotient
bits
A
counter
uses
an
adder
and
a
register
to
increment
a
running
count
Fractional
numbers
are
represented
using
fixed
point
or
floating
point
forms
Fixed
point
numbers
are
analogous
to
decimals
and
floating
point
numbers
are
analogous
to
scientific
notation
Fixed
point
numbers
use
ordinary
arithmetic
circuits
whereas
floating
point
numbers
require
more
elaborate
hardware
to
extract
and
process
the
sign
exponent
and
mantissa
CHAPTER
FIVE
Digital
Building
Blocks
X
Y
ABC
AB
ABC
ABC
AND
Array
OR
Array
weak
weak
Figure
bit
PLA
using
pseudo
nMOS
circuits
Large
memories
are
organized
into
arrays
of
words
The
memories
have
one
or
more
ports
to
read
and
or
write
the
words
Volatile
memories
such
as
SRAM
and
DRAM
lose
their
state
when
the
power
is
turned
off
SRAM
is
faster
than
DRAM
but
requires
more
transistors
A
register
file
is
a
small
multiported
SRAM
array
Nonvolatile
memories
called
ROMs
retain
their
state
indefinitely
Despite
their
names
most
modern
ROMs
can
be
written
Arrays
are
also
a
regular
way
to
build
logic
Memory
arrays
can
be
used
as
lookup
tables
to
perform
combinational
functions
PLAs
are
composed
of
dedicated
connections
between
configurable
AND
and
OR
arrays
they
only
implement
combinational
logic
FPGAs
are
composed
of
many
small
lookup
tables
and
registers
they
implement
combinational
and
sequential
logic
The
lookup
table
contents
and
their
interconnections
can
be
configured
to
perform
any
logic
function
Modern
FPGAs
are
easy
to
reprogram
and
are
large
and
cheap
enough
to
build
highly
sophisticated
digital
systems
so
they
are
widely
used
in
low
and
medium
volume
commercial
products
as
well
as
in
education
Summary
EXERCISES
Exercise
What
is
the
delay
for
the
following
types
of
bit
adders
Assume
that
each
two
input
gate
delay
is
ps
and
that
a
full
adder
delay
is
ps
a
a
ripple
carry
adder
b
a
carry
lookahead
adder
with
bit
blocks
c
a
prefix
adder
Exercise
Design
two
adders
a
bit
ripple
carry
adder
and
a
bit
carrylookahead
adder
with
bit
blocks
Use
only
two
input
gates
Each
two
input
gate
is
m
has
a
ps
delay
and
has
pF
of
total
gate
capacitance
You
may
assume
that
the
static
power
is
negligible
a
Compare
the
area
delay
and
power
of
the
adders
operating
at
MHz
b
Discuss
the
trade
offs
between
power
area
and
delay
Exercise
Explain
why
a
designer
might
choose
to
use
a
ripple
carry
adder
instead
of
a
carry
lookahead
adder
Exercise
Design
the
bit
prefix
adder
of
Figure
in
an
HDL
Simulate
and
test
your
module
to
prove
that
it
functions
correctly
Exercise
The
prefix
network
shown
in
Figure
uses
black
cells
to
compute
all
of
the
prefixes
Some
of
the
block
propagate
signals
are
not
actually
necessary
Design
a
gray
cell
that
receives
G
and
P
signals
for
bits
i
k
and
k
j
but
produces
only
Gi
j
not
Pi
j
Redraw
the
prefix
network
replacing
black
cells
with
gray
cells
wherever
possible
Exercise
The
prefix
network
shown
in
Figure
is
not
the
only
way
to
calculate
all
of
the
prefixes
in
logarithmic
time
The
Kogge
Stone
network
is
another
common
prefix
network
that
performs
the
same
function
using
a
different
connection
of
black
cells
Research
Kogge
Stone
adders
and
draw
a
schematic
similar
to
Figure
showing
the
connection
of
black
cells
in
a
Kogge
Stone
adder
Exercise
Recall
that
an
N
input
priority
encoder
has
log
N
outputs
that
encodes
which
of
the
N
inputs
gets
priority
see
Exercise
a
Design
an
N
input
priority
encoder
that
has
delay
that
increases
logarithmically
with
N
Sketch
your
design
and
give
the
delay
of
the
circuit
in
terms
of
the
delay
of
its
circuit
elements
b
Code
your
design
in
HDL
Simulate
and
test
your
module
to
prove
that
it
functions
correctly
CHAPTER
FIVE
Digital
Building
Blocks
C
Exercise
Design
the
following
comparators
for
bit
numbers
Sketch
the
schematics
a
not
equal
b
greater
than
c
less
than
or
equal
to
Exercise
Design
the
bit
ALU
shown
in
Figure
using
your
favorite
HDL
You
can
make
the
top
level
module
either
behavioral
or
structural
Exercise
Add
an
Overflow
output
to
the
bit
ALU
from
Exercise
The
output
is
TRUE
when
the
result
of
the
adder
overflows
Otherwise
it
is
FALSE
a
Write
a
Boolean
equation
for
the
Overflow
output
b
Sketch
the
Overflow
circuit
c
Design
the
modified
ALU
in
an
HDL
Exercise
Add
a
Zero
output
to
the
bit
ALU
from
Exercise
The
output
is
TRUE
when
Y
Exercise
Write
a
testbench
to
test
the
bit
ALU
from
Exercise
or
Then
use
it
to
test
the
ALU
Include
any
test
vector
files
necessary
Be
sure
to
test
enough
corner
cases
to
convince
a
reasonable
skeptic
that
the
ALU
functions
correctly
Exercise
Design
a
shifter
that
always
shifts
a
bit
input
left
by
bits
The
input
and
output
are
both
bits
Explain
the
design
in
words
and
sketch
a
schematic
Implement
your
design
in
your
favourite
HDL
Exercise
Design
bit
left
and
right
rotators
Sketch
a
schematic
of
your
design
Implement
your
design
in
your
favourite
HDL
Exercise
Design
an
bit
left
shifter
using
only
multiplexers
The
shifter
accepts
an
bit
input
A
and
a
bit
shift
amount
shamt
It
produces
an
bit
output
Y
Sketch
the
schematic
Exercise
Explain
how
to
build
any
N
bit
shifter
or
rotator
using
only
Nlog
N
multiplexers
Exercise
The
funnel
shifter
in
Figure
can
perform
any
N
bit
shift
or
rotate
operation
It
shifts
a
N
bit
input
right
by
k
bits
The
output
Y
is
the
N
least
significant
bits
of
the
result
The
most
significant
N
bits
of
the
input
are
Exercises
Ch
called
B
and
the
least
significant
N
bits
are
called
C
By
choosing
appropriate
values
of
B
C
and
k
the
funnel
shifter
can
perform
any
type
of
shift
or
rotate
Explain
what
these
values
should
be
in
terms
of
A
shamt
and
N
for
a
logical
right
shift
of
A
by
shamt
b
arithmetic
right
shift
of
A
by
shamt
c
left
shift
of
A
by
shamt
d
right
rotate
of
A
by
shamt
e
left
rotate
of
A
by
shamt
CHAPTER
FIVE
Digital
Building
Blocks
B
C
k
N
k
N
N
Y
N
Figure
Funnel
shifter
Exercise
Find
the
critical
path
for
the
multiplier
from
Figure
in
terms
of
an
AND
gate
delay
tAND
and
a
full
adder
delay
tFA
What
is
the
delay
of
an
N
N
multiplier
built
in
the
same
way
Exercise
Design
a
multiplier
that
handles
two
s
complement
numbers
Exercise
A
sign
extension
unit
extends
a
two
s
complement
number
from
M
to
N
N
M
bits
by
copying
the
most
significant
bit
of
the
input
into
the
upper
bits
of
the
output
see
Section
It
receives
an
M
bit
input
A
and
produces
an
N
bit
output
Y
Sketch
a
circuit
for
a
sign
extension
unit
with
a
bit
input
and
an
bit
output
Write
the
HDL
for
your
design
Exercise
A
zero
extension
unit
extends
an
unsigned
number
from
M
to
N
bits
N
M
by
putting
zeros
in
the
upper
bits
of
the
output
Sketch
a
circuit
for
a
zero
extension
unit
with
a
bit
input
and
an
bit
output
Write
the
HDL
for
your
design
Exercise
Compute
in
binary
using
the
standard
division
algorithm
from
elementary
school
Show
your
work
Exercise
What
is
the
range
of
numbers
that
can
be
represented
by
the
following
number
systems
a
bit
unsigned
fixed
point
numbers
with
integer
bits
and
fraction
bits
b
bit
sign
and
magnitude
fixed
point
numbers
with
integer
bits
and
fraction
bits
c
bit
two
s
complement
fixed
point
numbers
with
integer
bits
and
fraction
bits
Exercise
Express
the
following
base
numbers
in
bit
fixed
point
sign
magnitude
format
with
eight
integer
bits
and
eight
fraction
bits
Express
your
answer
in
hexadecimal
a
b
c
Exercise
Express
the
base
numbers
in
Exercise
in
bit
fixedpoint
two
s
complement
format
with
eight
integer
bits
and
eight
fraction
bits
Express
your
answer
in
hexadecimal
Exercise
Express
the
base
numbers
in
Exercise
in
IEEE
single
precision
floating
point
format
Express
your
answer
in
hexadecimal
Exercise
Convert
the
following
two
s
complement
binary
fixed
point
numbers
to
base
a
b
c
Exercise
When
adding
two
floating
point
numbers
the
number
with
the
smaller
exponent
is
shifted
Why
is
this
Explain
in
words
and
give
an
example
to
justify
your
explanation
Exercise
Add
the
following
IEEE
single
precision
floating
point
numbers
a
C
D
b
C
D
DC
c
FBE
FF
DFDE
Why
is
the
result
counterintuitive
Explain
Exercises
Ch
Exercise
Expand
the
steps
in
section
for
performing
floating
point
addition
to
work
for
negative
as
well
as
positive
floating
point
numbers
Exercise
Consider
IEEE
single
precision
floating
point
numbers
a
How
many
numbers
can
be
represented
by
IEEE
single
precision
floating
point
format
You
need
not
count
or
NaN
b
How
many
additional
numbers
could
be
represented
if
and
NaN
were
not
represented
c
Explain
why
and
NaN
are
given
special
representations
Exercise
Consider
the
following
decimal
numbers
and
a
Write
the
two
numbers
using
single
precision
floating
point
notation
Give
your
answers
in
hexadecimal
b
Perform
a
magnitude
comparison
of
the
two
bit
numbers
from
part
a
In
other
words
interpret
the
two
bit
numbers
as
two
s
complement
numbers
and
compare
them
Does
the
integer
comparison
give
the
correct
result
c
You
decide
to
come
up
with
a
new
single
precision
floating
point
notation
Everything
is
the
same
as
the
IEEE
single
precision
floating
point
standard
except
that
you
represent
the
exponent
using
two
s
complement
instead
of
a
bias
Write
the
two
numbers
using
your
new
standard
Give
your
answers
in
hexadecimal
e
Does
integer
comparison
work
with
your
new
floating
point
notation
from
part
d
f
Why
is
it
convenient
for
integer
comparison
to
work
with
floating
point
numbers
Exercise
Design
a
single
precision
floating
point
adder
using
your
favorite
HDL
Before
coding
the
design
in
an
HDL
sketch
a
schematic
of
your
design
Simulate
and
test
your
adder
to
prove
to
a
skeptic
that
it
functions
correctly
You
may
consider
positive
numbers
only
and
use
round
toward
zero
truncate
You
may
also
ignore
the
special
cases
given
in
Table
Exercise
In
this
problem
you
will
explore
the
design
of
a
bit
floatingpoint
multiplier
The
multiplier
has
two
bit
floating
point
inputs
and
produces
a
bit
floating
point
output
You
may
consider
positive
numbers
only
CHAPTER
FIVE
Digital
Building
Blocks
and
use
round
toward
zero
truncate
You
may
also
ignore
the
special
cases
given
in
Table
a
Write
the
steps
necessary
to
perform
bit
floating
point
multiplication
b
Sketch
the
schematic
of
a
bit
floating
point
multiplier
c
Design
a
bit
floating
point
multiplier
in
an
HDL
Simulate
and
test
your
multiplier
to
prove
to
a
skeptic
that
it
functions
correctly
Exercise
In
this
problem
you
will
explore
the
design
of
a
bit
prefix
adder
a
Sketch
a
schematic
of
your
design
b
Design
the
bit
prefix
adder
in
an
HDL
Simulate
and
test
your
adder
to
prove
that
it
functions
correctly
c
What
is
the
delay
of
your
bit
prefix
adder
from
part
a
Assume
that
each
two
input
gate
delay
is
ps
d
Design
a
pipelined
version
of
the
bit
prefix
adder
Sketch
the
schematic
of
your
design
How
fast
can
your
pipelined
prefix
adder
run
Make
the
design
run
as
fast
as
possible
e
Design
the
pipelined
bit
prefix
adder
in
an
HDL
Exercise
An
incrementer
adds
to
an
N
bit
number
Build
an
bit
incrementer
using
half
adders
Exercise
Build
a
bit
synchronous
Up
Down
counter
The
inputs
are
Reset
and
Up
When
Reset
is
the
outputs
are
all
Otherwise
when
Up
the
circuit
counts
up
and
when
Up
the
circuit
counts
down
Exercise
Design
a
bit
counter
that
adds
at
each
clock
edge
The
counter
has
reset
and
clock
inputs
Upon
reset
the
counter
output
is
all
Exercise
Modify
the
counter
from
Exercise
such
that
the
counter
will
either
increment
by
or
load
a
new
bit
value
D
on
each
clock
edge
depending
on
a
control
signal
PCSrc
When
PCSrc
the
counter
loads
the
new
value
D
Exercise
An
N
bit
Johnson
counter
consists
of
an
N
bit
shift
register
with
a
reset
signal
The
output
of
the
shift
register
Sout
is
inverted
and
fed
back
to
the
input
Sin
When
the
counter
is
reset
all
of
the
bits
are
cleared
to
a
Show
the
sequence
of
outputs
Q
produced
by
a
bit
Johnson
counter
starting
immediately
after
the
counter
is
reset
Exercises
Cha
b
How
many
cycles
elapse
until
an
N
bit
Johnson
counter
repeats
its
sequence
Explain
c
Design
a
decimal
counter
using
a
bit
Johnson
counter
ten
AND
gates
and
inverters
The
decimal
counter
has
a
clock
a
reset
and
ten
one
hot
outputs
Y
When
the
counter
is
reset
Y
is
asserted
On
each
subsequent
cycle
the
next
output
should
be
asserted
After
ten
cycles
the
counter
should
repeat
Sketch
a
schematic
of
the
decimal
counter
d
What
advantages
might
a
Johnson
counter
have
over
a
conventional
counter
Exercise
Write
the
HDL
for
a
bit
scannable
flip
flop
like
the
one
shown
in
Figure
Simulate
and
test
your
HDL
module
to
prove
that
it
functions
correctly
Exercise
The
English
language
has
a
good
deal
of
redundancy
that
allows
us
to
reconstruct
garbled
transmissions
Binary
data
can
also
be
transmitted
in
redundant
form
to
allow
error
correction
For
example
the
number
could
be
coded
as
and
the
number
could
be
coded
as
The
value
could
then
be
sent
over
a
noisy
channel
that
might
flip
up
to
two
of
the
bits
The
receiver
could
reconstruct
the
original
data
because
a
will
have
at
least
three
of
the
five
received
bits
as
s
similarly
a
will
have
at
least
three
s
a
Propose
an
encoding
to
send
or
encoded
using
five
bits
of
information
such
that
all
errors
that
corrupt
one
bit
of
the
encoded
data
can
be
corrected
Hint
the
encodings
and
for
and
respectively
will
not
work
b
Design
a
circuit
that
receives
your
five
bit
encoded
data
and
decodes
it
to
or
even
if
one
bit
of
the
transmitted
data
has
been
changed
c
Suppose
you
wanted
to
change
to
an
alternative
bit
encoding
How
might
you
implement
your
design
to
make
it
easy
to
change
the
encoding
without
having
to
use
different
hardware
Exercise
Flash
EEPROM
simply
called
Flash
memory
is
a
fairly
recent
invention
that
has
revolutionized
consumer
electronics
Research
and
explain
how
Flash
memory
works
Use
a
diagram
illustrating
the
floating
gate
Describe
how
a
bit
in
the
memory
is
programmed
Properly
cite
your
sources
Exercise
The
extraterrestrial
life
project
team
has
just
discovered
aliens
living
on
the
bottom
of
Mono
Lake
They
need
to
construct
a
circuit
to
classify
the
aliens
by
potential
planet
of
origin
based
on
measured
features
CHAPTER
FIVE
Digital
Building
Blocks
available
from
the
NASA
probe
greenness
brownness
sliminess
and
ugliness
Careful
consultation
with
xenobiologists
leads
to
the
following
conclusions
If
the
alien
is
green
and
slimy
or
ugly
brown
and
slimy
it
might
be
from
Mars
If
the
critter
is
ugly
brown
and
slimy
or
green
and
neither
ugly
nor
slimy
it
might
be
from
Venus
If
the
beastie
is
brown
and
neither
ugly
nor
slimy
or
is
green
and
slimy
it
might
be
from
Jupiter
Note
that
this
is
an
inexact
science
for
example
a
life
form
which
is
mottled
green
and
brown
and
is
slimy
but
not
ugly
might
be
from
either
Mars
or
Jupiter
a
Program
a
PLA
to
identify
the
alien
You
may
use
dot
notation
b
Program
a
ROM
to
identify
the
alien
You
may
use
dot
notation
c
Implement
your
design
in
an
HDL
Exercise
Implement
the
following
functions
using
a
single
ROM
Use
dot
notation
to
indicate
the
ROM
contents
a
b
c
Z
A
B
C
D
Exercise
Implement
the
functions
from
Exercise
using
an
PLA
You
may
use
dot
notation
Exercise
Specify
the
size
of
a
ROM
that
you
could
use
to
program
each
of
the
following
combinational
circuits
Is
using
a
ROM
to
implement
these
functions
a
good
design
choice
Explain
why
or
why
not
a
a
bit
adder
subtractor
with
Cin
and
Cout
b
an
multiplier
c
a
bit
priority
encoder
see
Exercise
Exercise
Consider
the
ROM
circuits
in
Figure
For
each
row
can
the
circuit
in
column
I
be
replaced
by
an
equivalent
circuit
in
column
II
by
proper
programming
of
the
latter
s
ROM
Y
AB
BD
X
AB
BCD
AB
Exercises
Chapte
CHAPTER
FIVE
Digital
Building
Blocks
K
I
A
RD
ROM
CLK
N
N
K
In
A
RD
ROM
N
N
A
RD
ROM
N
A
RD
ROM
K
CLK
K
A
RD
Out
ROM
K
K
CLK
K
In
A
RD
ROM
K
K
Out
CLK
K
In
A
RD
ROM
K
K
N
N
A
RD
ROM
N
Out
CLK
K
In
A
RD
ROM
K
K
N
N
Out
CLK
N
In
A
RD
ROM
N
N
N
A
RD
ROM
N
Out
CLK
N
In
A
RD
ROM
N
N
N
Out
II
a
b
c
d
Figure
ROM
circuits
Exercise
Give
an
example
of
a
nine
input
function
that
can
be
performed
using
only
one
Spartan
FPGA
CLB
Give
an
example
of
an
eight
input
function
that
cannot
be
performed
using
only
one
CLB
Exercise
How
many
Spartan
FPGA
CLBs
are
required
to
perform
each
of
the
following
functions
Show
how
to
configure
one
or
more
CLBs
to
perform
the
function
You
should
be
able
to
do
this
by
inspection
without
performing
logic
synthesis
a
The
combinational
function
from
Exercise
c
b
The
combinational
function
from
Exercise
c
c
The
two
output
function
from
Exercise
d
The
function
from
Exercise
e
A
four
input
priority
encoder
see
Exercise
f
An
eight
input
priority
encoder
see
Exercise
g
A
decoder
h
A
bit
carry
propagate
adder
with
no
carry
in
or
out
i
The
FSM
from
Exercise
j
The
Gray
code
counter
from
Exercise
Exercise
Consider
the
Spartan
CLB
shown
in
Figure
It
has
the
following
specifications
tpd
tcd
ns
per
CLB
tsetup
ns
thold
ns
and
tpcq
ns
for
all
flip
flops
a
What
is
the
minimum
number
of
Spartan
CLBs
required
to
implement
the
FSM
of
Figure
b
Without
clock
skew
what
is
the
fastest
clock
frequency
at
which
this
FSM
will
run
reliably
c
With
ns
of
clock
skew
what
is
the
fastest
frequency
at
which
the
FSM
will
run
reliably
Exercise
You
would
like
to
use
an
FPGA
to
implement
an
M
M
sorter
with
a
color
sensor
and
motors
to
put
red
candy
in
one
jar
and
green
candy
in
another
The
design
is
to
be
implemented
as
an
FSM
using
a
Spartan
XC
S
FPGA
a
chip
from
the
Spartan
series
family
It
is
considerably
faster
than
the
original
Spartan
FPGA
According
to
the
data
sheet
the
FPGA
has
timing
characteristics
shown
in
Table
Assume
that
the
design
is
small
enough
that
wire
delay
is
negligible
Exercises
Table
Spartan
XC
S
timing
Name
Value
ns
tpcq
tsetup
thold
tpd
per
CLB
tskew
You
would
like
your
FSM
to
run
at
MHz
What
is
the
maximum
number
of
CLBs
on
the
critical
path
What
is
the
fastest
speed
at
which
the
FSM
will
run
Chapt
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
What
is
the
largest
possible
result
of
multiplying
two
unsigned
N
bit
numbers
Question
Binary
coded
decimal
BCD
representation
uses
four
bits
to
encode
each
decimal
digit
For
example
is
represented
as
BCD
Explain
in
words
why
processors
might
use
BCD
representation
Question
Design
hardware
to
add
two
bit
unsigned
BCD
numbers
see
Question
Sketch
a
schematic
for
your
design
and
write
an
HDL
module
for
the
BCD
adder
The
inputs
are
A
B
and
Cin
and
the
outputs
are
S
and
Cout
Cin
and
Cout
are
bit
carries
and
A
B
and
S
are
bit
BCD
numbers
CHAPTER
FIVE
Digital
Building
Blocks
Introduction
Assembly
Language
Machine
Language
Programming
Addressing
Modes
Lights
Camera
Action
Compiling
Assembling
and
Loading
Odds
and
Ends
Real
World
Perspective
IA
Architecture
Summary
Exercises
Interview
Questions
Architecture
INTRODUCTION
The
previous
chapters
introduced
digital
design
principles
and
building
blocks
In
this
chapter
we
jump
up
a
few
levels
of
abstraction
to
define
the
architecture
of
a
computer
see
Figure
The
architecture
is
the
programmer
s
view
of
a
computer
It
is
defined
by
the
instruction
set
language
and
operand
locations
registers
and
memory
Many
different
architectures
exist
such
as
IA
MIPS
SPARC
and
PowerPC
The
first
step
in
understanding
any
computer
architecture
is
to
learn
its
language
The
words
in
a
computer
s
language
are
called
instructions
The
computer
s
vocabulary
is
called
the
instruction
set
All
programs
running
on
a
computer
use
the
same
instruction
set
Even
complex
software
applications
such
as
word
processing
and
spreadsheet
applications
are
eventually
compiled
into
a
series
of
simple
instructions
such
as
add
subtract
and
jump
Computer
instructions
indicate
both
the
operation
to
perform
and
the
operands
to
use
The
operands
may
come
from
memory
from
registers
or
from
the
instruction
itself
Computer
hardware
understands
only
s
and
s
so
instructions
are
encoded
as
binary
numbers
in
a
format
called
machine
language
Just
as
we
use
letters
to
encode
human
language
computers
use
binary
numbers
to
encode
machine
language
Microprocessors
are
digital
systems
that
read
and
execute
machine
language
instructions
However
humans
consider
reading
machine
language
to
be
tedious
so
we
prefer
to
represent
the
instructions
in
a
symbolic
format
called
assembly
language
The
instruction
sets
of
different
architectures
are
more
like
different
dialects
than
different
languages
Almost
all
architectures
define
basic
instructions
such
as
add
subtract
and
jump
that
operate
on
memory
or
registers
Once
you
have
learned
one
instruction
set
understanding
others
is
fairly
straightforward
A
computer
architecture
does
not
define
the
underlying
hardware
implementation
Often
many
different
hardware
implementations
of
a
single
architecture
exist
For
example
Intel
and
Advanced
Micro
Devices
AMD
sell
various
microprocessors
belonging
to
the
same
IA
architecture
They
all
can
run
the
same
programs
but
they
use
different
underlying
hardware
and
therefore
offer
trade
offs
in
performance
price
and
power
Some
microprocessors
are
optimized
for
high
performance
servers
whereas
others
are
optimized
for
long
battery
life
in
laptop
computers
The
specific
arrangement
of
registers
memories
ALUs
and
other
building
blocks
to
form
a
microprocessor
is
called
the
microarchitecture
and
will
be
the
subject
of
Chapter
Often
many
different
microarchitectures
exist
for
a
single
architecture
In
this
text
we
introduce
the
MIPS
architecture
that
was
first
developed
by
John
Hennessy
and
his
colleagues
at
Stanford
in
the
s
MIPS
processors
are
used
by
among
others
Silicon
Graphics
Nintendo
and
Cisco
We
start
by
introducing
the
basic
instructions
operand
locations
and
machine
language
formats
We
then
introduce
more
instructions
used
in
common
programming
constructs
such
as
branches
loops
array
manipulations
and
procedure
calls
Throughout
the
chapter
we
motivate
the
design
of
the
MIPS
architecture
using
four
principles
articulated
by
Patterson
and
Hennessy
simplicity
favors
regularity
make
the
common
case
fast
smaller
is
faster
and
good
design
demands
good
compromises
ASSEMBLY
LANGUAGE
Assembly
language
is
the
human
readable
representation
of
the
computer
s
native
language
Each
assembly
language
instruction
specifies
both
the
operation
to
perform
and
the
operands
on
which
to
operate
We
introduce
simple
arithmetic
instructions
and
show
how
these
operations
are
written
in
assembly
language
We
then
define
the
MIPS
instruction
operands
registers
memory
and
constants
We
assume
that
you
already
have
some
familiarity
with
a
high
level
programming
language
such
as
C
C
or
Java
These
languages
are
practically
identical
for
most
of
the
examples
in
this
chapter
but
where
they
differ
we
will
use
C
Instructions
The
most
common
operation
computers
perform
is
addition
Code
Example
shows
code
for
adding
variables
b
and
c
and
writing
the
result
to
a
The
program
is
shown
on
the
left
in
a
high
level
language
using
the
syntax
of
C
C
and
Java
and
then
rewritten
on
the
right
in
MIPS
assembly
language
Note
that
statements
in
a
C
program
end
with
a
semicolon
CHAPTER
SIX
Architecture
What
is
the
best
architecture
to
study
when
first
learning
the
subject
Commercially
successful
architectures
such
as
IA
are
satisfying
to
study
because
you
can
use
them
to
write
programs
on
real
computers
Unfortunately
many
of
these
architectures
are
full
of
warts
and
idiosyncrasies
accumulated
over
years
of
haphazard
development
by
different
engineering
teams
making
the
architectures
difficult
to
understand
and
implement
Many
textbooks
teach
imaginary
architectures
that
are
simplified
to
illustrate
the
key
concepts
We
follow
the
lead
of
David
Patterson
and
John
Hennessy
in
their
text
Computer
Organization
and
Design
by
focusing
on
the
MIPS
architecture
Hundreds
of
millions
of
MIPS
microprocessors
have
shipped
so
the
architecture
is
commercially
very
important
Yet
it
is
a
clean
architecture
with
little
odd
behavior
At
the
end
of
this
chapter
we
briefly
visit
the
IA
architecture
to
compare
and
contrast
it
with
MIPS
Chap
Assembly
Language
High
Level
Code
a
b
c
MIPS
Assembly
Code
add
a
b
c
Code
Example
ADDITION
High
Level
Code
a
b
c
MIPS
Assembly
Code
sub
a
b
c
Code
Example
SUBTRACTION
High
Level
Code
a
b
c
d
single
line
comment
multiple
line
comment
MIPS
Assembly
Code
sub
t
c
d
t
c
d
add
a
b
t
a
b
t
Code
Example
MORE
COMPLEX
CODE
The
first
part
of
the
assembly
instruction
add
is
called
the
mnemonic
and
indicates
what
operation
to
perform
The
operation
is
performed
on
b
and
c
the
source
operands
and
the
result
is
written
to
a
the
destination
operand
Code
Example
shows
that
subtraction
is
similar
to
addition
The
instruction
format
is
the
same
as
the
add
instruction
except
for
the
operation
specification
sub
This
consistent
instruction
format
is
an
example
of
the
first
design
principle
Design
Principle
Simplicity
favors
regularity
Instructions
with
a
consistent
number
of
operands
in
this
case
two
sources
and
one
destination
are
easier
to
encode
and
handle
in
hardware
More
complex
high
level
code
translates
into
multiple
MIPS
instructions
as
shown
in
Code
Example
In
the
high
level
language
examples
single
line
comments
begin
with
and
continue
until
the
end
of
the
line
Multiline
comments
begin
with
and
end
with
In
assembly
language
only
single
line
comments
are
used
They
begin
with
and
continue
until
the
end
of
the
line
The
assembly
language
program
in
Code
Example
requires
a
temporary
variable
t
to
store
the
intermediate
result
Using
multiple
assembly
mnemonic
pronounced
ni
mon
ik
comes
from
the
Greek
word
E
to
remember
The
assembly
language
mnemonic
is
easier
to
remember
than
a
machine
language
pattern
of
s
and
s
representing
the
same
operation
Chapter
language
instructions
to
perform
more
complex
operations
is
an
example
of
the
second
design
principle
of
computer
architecture
Design
Principle
Make
the
common
case
fast
The
MIPS
instruction
set
makes
the
common
case
fast
by
including
only
simple
commonly
used
instructions
The
number
of
instructions
is
kept
small
so
that
the
hardware
required
to
decode
the
instruction
and
its
operands
can
be
simple
small
and
fast
More
elaborate
operations
that
are
less
common
are
performed
using
sequences
of
multiple
simple
instructions
Thus
MIPS
is
a
reduced
instruction
set
computer
RISC
architecture
Architectures
with
many
complex
instructions
such
as
Intel
s
IA
architecture
are
complex
instruction
set
computers
CISC
For
example
IA
defines
a
string
move
instruction
that
copies
a
string
a
series
of
characters
from
one
part
of
memory
to
another
Such
an
operation
requires
many
possibly
even
hundreds
of
simple
instructions
in
a
RISC
machine
However
the
cost
of
implementing
complex
instructions
in
a
CISC
architecture
is
added
hardware
and
overhead
that
slows
down
the
simple
instructions
A
RISC
architecture
minimizes
the
hardware
complexity
and
the
necessary
instruction
encoding
by
keeping
the
set
of
distinct
instructions
small
For
example
an
instruction
set
with
simple
instructions
would
need
log
bits
to
encode
the
operation
An
instruction
set
with
complex
instructions
would
need
log
bits
of
encoding
per
instruction
In
a
CISC
machine
even
though
the
complex
instructions
may
be
used
only
rarely
they
add
overhead
to
all
instructions
even
the
simple
ones
Operands
Registers
Memory
and
Constants
An
instruction
operates
on
operands
In
Code
Example
the
variables
a
b
and
c
are
all
operands
But
computers
operate
on
s
and
s
not
variable
names
The
instructions
need
a
physical
location
from
which
to
retrieve
the
binary
data
Operands
can
be
stored
in
registers
or
memory
or
they
may
be
constants
stored
in
the
instruction
itself
Computers
use
various
locations
to
hold
operands
to
optimize
for
speed
and
data
capacity
Operands
stored
as
constants
or
in
registers
are
accessed
quickly
but
they
hold
only
a
small
amount
of
data
Additional
data
must
be
accessed
from
memory
which
is
large
but
slow
MIPS
is
called
a
bit
architecture
because
it
operates
on
bit
data
The
MIPS
architecture
has
been
extended
to
bits
in
commercial
products
but
we
will
consider
only
the
bit
form
in
this
book
Registers
Instructions
need
to
access
operands
quickly
so
that
they
can
run
fast
But
operands
stored
in
memory
take
a
long
time
to
retrieve
Therefore
CHAPTER
SIX
Architecture
Ch
most
architectures
specify
a
small
number
of
registers
that
hold
commonly
used
operands
The
MIPS
architecture
uses
registers
called
the
register
set
or
register
file
The
fewer
the
registers
the
faster
they
can
be
accessed
This
leads
to
the
third
design
principle
Design
Principle
Smaller
is
faster
Looking
up
information
from
a
small
number
of
relevant
books
on
your
desk
is
a
lot
faster
than
searching
for
the
information
in
the
stacks
at
a
library
Likewise
reading
data
from
a
small
set
of
registers
for
example
is
faster
than
reading
it
from
registers
or
a
large
memory
A
small
register
file
is
typically
built
from
a
small
SRAM
array
see
Section
The
SRAM
array
uses
a
small
decoder
and
bitlines
connected
to
relatively
few
memory
cells
so
it
has
a
shorter
critical
path
than
a
large
memory
does
Code
Example
shows
the
add
instruction
with
register
operands
MIPS
register
names
are
preceded
by
the
sign
The
variables
a
b
and
c
are
arbitrarily
placed
in
s
s
and
s
The
name
s
is
pronounced
register
s
or
dollar
s
The
instruction
adds
the
bit
values
contained
in
s
b
and
s
c
and
writes
the
bit
result
to
s
a
MIPS
generally
stores
variables
in
of
the
registers
s
s
and
t
t
Register
names
beginning
with
s
are
called
saved
registers
Following
MIPS
convention
these
registers
store
variables
such
as
a
b
and
c
Saved
registers
have
special
connotations
when
they
are
used
with
procedure
calls
see
Section
Register
names
beginning
with
t
are
called
temporary
registers
They
are
used
for
storing
temporary
variables
Code
Example
shows
MIPS
assembly
code
using
a
temporary
register
t
to
store
the
intermediate
calculation
of
c
d
Assembly
Language
High
Level
Code
a
b
c
MIPS
Assembly
Code
s
a
s
b
s
c
add
s
s
s
a
b
c
Code
Example
REGISTER
OPERANDS
High
Level
Code
a
b
c
d
MIPS
Assembly
Code
s
a
s
b
s
c
s
d
sub
t
s
s
t
c
d
add
s
s
t
a
b
t
Code
Example
TEMPORARY
REGISTERS
Chapter
qx
Example
TRANSLATING
HIGH
LEVEL
CODE
TO
ASSEMBLY
LANGUAGE
Translate
the
following
high
level
code
into
assembly
language
Assume
variables
a
c
are
held
in
registers
s
s
and
f
j
are
in
s
s
a
b
c
f
g
h
i
j
Solution
The
program
uses
four
assembly
language
instructions
MIPS
assembly
code
s
a
s
b
s
c
s
f
s
g
s
h
s
i
s
j
sub
s
s
s
a
b
c
add
t
s
s
t
g
h
add
t
s
s
t
i
j
sub
s
t
t
f
g
h
i
j
The
Register
Set
The
MIPS
architecture
defines
registers
Each
register
has
a
name
and
a
number
ranging
from
to
Table
lists
the
name
number
and
use
for
each
register
always
contains
the
value
because
this
constant
is
so
frequently
used
in
computer
programs
We
have
also
discussed
the
s
and
t
registers
The
remaining
registers
will
be
described
throughout
this
chapter
CHAPTER
SIX
Architecture
Table
MIPS
register
set
Name
Number
Use
the
constant
value
at
assembler
temporary
v
v
procedure
return
values
a
a
procedure
arguments
t
t
temporary
variables
s
s
saved
variables
t
t
temporary
variables
k
k
operating
system
OS
temporaries
gp
global
pointer
sp
stack
pointer
fp
frame
pointer
ra
procedure
return
address
Chapter
qxd
Memory
If
registers
were
the
only
storage
space
for
operands
we
would
be
confined
to
simple
programs
with
no
more
than
variables
However
data
can
also
be
stored
in
memory
When
compared
to
the
register
file
memory
has
many
data
locations
but
accessing
it
takes
a
longer
amount
of
time
Whereas
the
register
file
is
small
and
fast
memory
is
large
and
slow
For
this
reason
commonly
used
variables
are
kept
in
registers
By
using
a
combination
of
memory
and
registers
a
program
can
access
a
large
amount
of
data
fairly
quickly
As
described
in
Section
memories
are
organized
as
an
array
of
data
words
The
MIPS
architecture
uses
bit
memory
addresses
and
bit
data
words
MIPS
uses
a
byte
addressable
memory
That
is
each
byte
in
memory
has
a
unique
address
However
for
explanation
purposes
only
we
first
introduce
a
word
addressable
memory
and
afterward
describe
the
MIPS
byte
addressable
memory
Figure
shows
a
memory
array
that
is
word
addressable
That
is
each
bit
data
word
has
a
unique
bit
address
Both
the
bit
word
address
and
the
bit
data
value
are
written
in
hexadecimal
in
Figure
For
example
data
xF
F
AC
is
stored
at
memory
address
Hexadecimal
constants
are
written
with
the
prefix
x
By
convention
memory
is
drawn
with
low
memory
addresses
toward
the
bottom
and
high
memory
addresses
toward
the
top
MIPS
uses
the
load
word
instruction
lw
to
read
a
data
word
from
memory
into
a
register
Code
Example
loads
memory
word
into
s
The
lw
instruction
specifies
the
effective
address
in
memory
as
the
sum
of
a
base
address
and
an
offset
The
base
address
written
in
parentheses
in
the
instruction
is
a
register
The
offset
is
a
constant
written
before
the
parentheses
In
Code
Example
the
base
address
Assembly
Language
Data
F
E
E
F
F
AC
ABC
DEF
Word
Address
Word
Word
Word
Word
Figure
Word
addressable
memory
Assembly
Code
This
assembly
code
unlike
MIPS
assumes
word
addressable
memory
lw
s
read
memory
word
into
s
Code
Example
READING
WORD
ADDRESSABLE
MEMORY
is
which
holds
the
value
and
the
offset
is
so
the
lw
instruction
reads
from
memory
address
After
the
load
word
instruction
lw
is
executed
s
holds
the
value
xF
F
AC
which
is
the
data
value
stored
at
memory
address
in
Figure
Similarly
MIPS
uses
the
store
word
instruction
sw
to
write
a
data
word
from
a
register
into
memory
Code
Example
writes
the
contents
of
register
s
into
memory
word
These
examples
have
used
as
the
base
address
for
simplicity
but
remember
that
any
register
can
be
used
to
supply
the
base
address
The
previous
two
code
examples
have
shown
a
computer
architecture
with
a
word
addressable
memory
The
MIPS
memory
model
however
is
byte
addressable
not
word
addressable
Each
data
byte
has
a
unique
address
A
bit
word
consists
of
four
bit
bytes
So
each
word
address
is
a
multiple
of
as
shown
in
Figure
Again
both
the
bit
word
address
and
the
data
value
are
given
in
hexadecimal
Code
Example
shows
how
to
read
and
write
words
in
the
MIPS
byte
addressable
memory
The
word
address
is
four
times
the
word
number
The
MIPS
assembly
code
reads
words
and
and
writes
words
and
The
offset
can
be
written
in
decimal
or
hexadecimal
The
MIPS
architecture
also
provides
the
lb
and
sb
instructions
that
load
and
store
single
bytes
in
memory
rather
than
words
They
are
similar
to
lw
and
sw
and
will
be
discussed
further
in
Section
Byte
addressable
memories
are
organized
in
a
big
endian
or
littleendian
fashion
as
shown
in
Figure
In
both
formats
the
most
significant
byte
MSB
is
on
the
left
and
the
least
significant
byte
LSB
is
on
the
right
In
big
endian
machines
bytes
are
numbered
starting
with
CHAPTER
SIX
Architecture
Assembly
Code
This
assembly
code
unlike
MIPS
assumes
word
addressable
memory
sw
s
write
s
to
memory
word
Code
Example
WRITING
WORD
ADDRESSABLE
MEMORY
Word
Address
Data
C
width
bytes
F
EE
F
F
AC
ABCD
EF
Word
Word
Word
Word
Figure
Byte
addressable
memory
MSB
LSB
AB
C
DEF
Byte
Address
B
A
C
F
EDC
Byte
Address
Word
Address
Big
Endian
Little
Endian
MSB
LSB
Figure
Big
and
littleendian
memory
addressing
C
at
the
big
most
significant
end
In
little
endian
machines
bytes
are
numbered
starting
with
at
the
little
least
significant
end
Word
addresses
are
the
same
in
both
formats
and
refer
to
the
same
four
bytes
Only
the
addresses
of
bytes
within
a
word
differ
Example
BIG
AND
LITTLE
ENDIAN
MEMORY
Suppose
that
s
initially
contains
x
After
the
following
program
is
run
on
a
big
endian
system
what
value
does
s
contain
In
a
little
endian
system
lb
s
loads
the
data
at
byte
address
into
the
least
significant
byte
of
s
lb
is
discussed
in
detail
in
Section
sw
s
lb
s
Solution
Figure
shows
how
big
and
little
endian
machines
store
the
value
x
in
memory
word
After
the
load
byte
instruction
lb
s
s
would
contain
x
on
a
big
endian
system
and
x
on
a
little
endian
system
Assembly
Language
MIPS
Assembly
Code
w
s
read
data
word
xABCDEF
into
s
w
s
read
data
word
x
EE
into
s
w
s
xC
read
data
word
x
F
into
s
sw
s
write
s
to
data
word
sw
s
x
write
s
to
data
word
sw
s
write
s
to
data
word
Code
Example
ACCESSING
BYTE
ADDRESSABLE
MEMORY
The
terms
big
endian
and
littleendian
come
from
Jonathan
Swift
s
Gulliver
s
Travels
first
published
in
under
the
pseudonym
of
Isaac
Bickerstaff
In
his
stories
the
Lilliputian
king
required
his
citizens
the
LittleEndians
to
break
their
eggs
on
the
little
end
The
Big
Endians
were
rebels
who
broke
their
eggs
on
the
big
end
The
terms
were
first
applied
to
computer
architectures
by
Danny
Cohen
in
his
paper
On
Holy
Wars
and
a
Plea
for
Peace
published
on
April
Fools
Day
USC
ISI
IEN
Photo
courtesy
The
Brotherton
Collection
IEEDS
University
Library
SPIM
the
MIPS
simulator
that
comes
with
this
text
uses
the
endianness
of
the
machine
it
is
run
on
For
example
when
using
SPIM
on
an
Intel
IA
machine
the
memory
is
little
endian
With
an
older
Macintosh
or
Sun
SPARC
machine
memory
is
big
endian
Word
Address
Big
Endian
Little
Endian
Byte
Address
Data
Value
Byte
Address
Data
Value
MSB
LSB
MSB
LSB
Figure
Big
endian
and
little
endian
data
storage
IBM
s
PowerPC
formerly
found
in
Macintosh
computers
uses
big
endian
addressing
Intel
s
IA
architecture
found
in
PCs
uses
little
endian
addressing
Some
MIPS
processors
are
little
endian
and
some
are
big
endian
The
choice
of
endianness
is
completely
arbitrary
but
leads
to
hassles
when
sharing
data
between
big
endian
and
littleendian
computers
In
examples
in
this
text
we
will
use
little
endian
format
whenever
byte
ordering
matters
Ch
In
the
MIPS
architecture
word
addresses
for
lw
and
sw
must
be
word
aligned
That
is
the
address
must
be
divisible
by
Thus
the
instruction
lw
s
is
an
illegal
instruction
Some
architectures
such
as
IA
allow
non
word
aligned
data
reads
and
writes
but
MIPS
requires
strict
alignment
for
simplicity
Of
course
byte
addresses
for
load
byte
and
store
byte
lb
and
sb
need
not
be
word
aligned
Constants
Immediates
Load
word
and
store
word
lw
and
sw
also
illustrate
the
use
of
constants
in
MIPS
instructions
These
constants
are
called
immediates
because
their
values
are
immediately
available
from
the
instruction
and
do
not
require
a
register
or
memory
access
Add
immediate
addi
is
another
common
MIPS
instruction
that
uses
an
immediate
operand
addi
adds
the
immediate
specified
in
the
instruction
to
a
value
in
a
register
as
shown
in
Code
Example
The
immediate
specified
in
an
instruction
is
a
bit
two
s
complement
number
in
the
range
Subtraction
is
equivalent
to
adding
a
negative
number
so
in
the
interest
of
simplicity
there
is
no
subi
instruction
in
the
MIPS
architecture
Recall
that
the
add
and
sub
instructions
use
three
register
operands
But
the
lw
sw
and
addi
instructions
use
two
register
operands
and
a
constant
Because
the
instruction
formats
differ
lw
and
sw
instructions
violate
design
principle
simplicity
favors
regularity
However
this
issue
allows
us
to
introduce
the
last
design
principle
CHAPTER
SIX
Architecture
High
Level
Code
a
a
b
a
MIPS
Assembly
Code
s
a
s
b
addi
s
s
a
a
addi
s
s
b
a
Code
Example
IMMEDIATE
OPERANDS
Design
Principle
Good
design
demands
good
compromises
A
single
instruction
format
would
be
simple
but
not
flexible
The
MIPS
instruction
set
makes
the
compromise
of
supporting
three
instruction
formats
One
format
used
for
instructions
such
as
add
and
sub
has
three
register
operands
Another
used
for
instructions
such
as
lw
and
addi
has
two
register
operands
and
a
bit
immediate
A
third
to
be
discussed
later
has
a
bit
immediate
and
no
registers
The
next
section
discusses
the
three
MIPS
instruction
formats
and
shows
how
they
are
encoded
into
binary
Chapter
MACHINE
LANGUAGE
Assembly
language
is
convenient
for
humans
to
read
However
digital
circuits
understand
only
s
and
s
Therefore
a
program
written
in
assembly
language
is
translated
from
mnemonics
to
a
representation
using
only
s
and
s
called
machine
language
MIPS
uses
bit
instructions
Again
simplicity
favors
regularity
and
the
most
regular
choice
is
to
encode
all
instructions
as
words
that
can
be
stored
in
memory
Even
though
some
instructions
may
not
require
all
bits
of
encoding
variable
length
instructions
would
add
too
much
complexity
Simplicity
would
also
encourage
a
single
instruction
format
but
as
already
mentioned
that
is
too
restrictive
MIPS
makes
the
compromise
of
defining
three
instruction
formats
R
type
I
type
and
J
type
This
small
number
of
formats
allows
for
some
regularity
among
all
the
types
and
thus
simpler
hardware
while
also
accommodating
different
instruction
needs
such
as
the
need
to
encode
large
constants
in
the
instruction
R
type
instructions
operate
on
three
registers
I
type
instructions
operate
on
two
registers
and
a
bit
immediate
J
type
jump
instructions
operate
on
one
bit
immediate
We
introduce
all
three
formats
in
this
section
but
leave
the
discussion
of
J
type
instructions
for
Section
R
type
Instructions
The
name
R
type
is
short
for
register
type
R
type
instructions
use
three
registers
as
operands
two
as
sources
and
one
as
a
destination
Figure
shows
the
R
type
machine
instruction
format
The
bit
instruction
has
six
fields
op
rs
rt
rd
shamt
and
funct
Each
field
is
five
or
six
bits
as
indicated
The
operation
the
instruction
performs
is
encoded
in
the
two
fields
highlighted
in
blue
op
also
called
opcode
or
operation
code
and
funct
also
called
the
function
All
R
type
instructions
have
an
opcode
of
The
specific
R
type
operation
is
determined
by
the
funct
field
For
example
the
opcode
and
funct
fields
for
the
add
instruction
are
and
respectively
Similarly
the
sub
instruction
has
an
opcode
and
funct
field
of
and
The
operands
are
encoded
in
the
three
fields
rs
rt
and
rd
The
first
two
registers
rs
and
rt
are
the
source
registers
rd
is
the
destination
Machine
Language
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
R
type
Figure
R
type
machine
instruction
format
register
The
fields
contain
the
register
numbers
that
were
given
in
Table
For
example
s
is
register
The
fifth
field
shamt
is
used
only
in
shift
operations
In
those
instructions
the
binary
value
stored
in
the
bit
shamt
field
indicates
the
amount
to
shift
For
all
other
R
type
instructions
shamt
is
Figure
shows
the
machine
code
for
the
R
type
instructions
add
and
sub
Notice
that
the
destination
is
the
first
register
in
an
assembly
language
instruction
but
it
is
the
third
register
field
rd
in
the
machine
language
instruction
For
example
the
assembly
instruction
add
s
s
s
has
rs
s
rt
s
and
rd
s
Tables
B
and
B
in
Appendix
B
define
the
opcode
values
for
all
MIPS
instructions
and
the
funct
field
values
for
R
type
instructions
Example
TRANSLATING
ASSEMBLY
LANGUAGE
TO
MACHINE
LANGUAGE
Translate
the
following
assembly
language
statement
into
machine
language
add
t
s
s
Solution
According
to
Table
t
s
and
s
are
registers
and
According
to
Tables
B
and
B
add
has
an
opcode
of
and
a
funct
code
of
Thus
the
fields
and
machine
code
are
given
in
Figure
The
easiest
way
to
write
the
machine
language
in
hexadecimal
is
to
first
write
it
in
binary
then
look
at
consecutive
groups
of
four
bits
which
correspond
to
hexadecimal
digits
indicated
in
blue
Hence
the
machine
language
instruction
is
x
CHAPTER
SIX
Architecture
add
s
s
s
sub
t
t
t
Assembly
Code
Machine
Code
Field
Values
x
x
D
op
rs
rt
rd
shamt
funct
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
Figure
Machine
code
for
R
type
instructions
add
t
s
s
Assembly
Code
Machine
Code
Field
Values
x
op
rs
rt
rd
shamt
funct
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
Figure
Machine
code
for
the
R
type
instruction
of
Example
rs
is
short
for
register
source
rt
comes
after
rs
alphabetically
and
usually
indicates
the
second
register
source
Cha
I
Type
Instructions
The
name
I
type
is
short
for
immediate
type
I
type
instructions
use
two
register
operands
and
one
immediate
operand
Figure
shows
the
I
type
machine
instruction
format
The
bit
instruction
has
four
fields
op
rs
rt
and
imm
The
first
three
fields
op
rs
and
rt
are
like
those
of
R
type
instructions
The
imm
field
holds
the
bit
immediate
The
operation
is
determined
solely
by
the
opcode
highlighted
in
blue
The
operands
are
specified
in
the
three
fields
rs
rt
and
imm
rs
and
imm
are
always
used
as
source
operands
rt
is
used
as
a
destination
for
some
instructions
such
as
addi
and
lw
but
as
another
source
for
others
such
as
sw
Figure
shows
several
examples
of
encoding
I
type
instructions
Recall
that
negative
immediate
values
are
represented
using
bit
two
s
complement
notation
rt
is
listed
first
in
the
assembly
language
instruction
when
it
is
used
as
a
destination
but
it
is
the
second
register
field
in
the
machine
language
instruction
Machine
Language
op
rs
rt
imm
bits
bits
bits
bits
I
type
Figure
I
type
instruction
format
x
x
FFF
x
C
A
xAD
Assembly
Code
Machine
Code
Field
Values
op
rs
rt
imm
op
rs
rt
imm
addi
s
s
addi
t
s
lw
t
sw
s
t
bits
bits
bits
bits
bits
bits
bits
bits
Figure
Machine
code
for
I
type
instructions
Example
TRANSLATING
I
TYPE
ASSEMBLY
INSTRUCTIONS
INTO
MACHINE
CODE
Translate
the
following
I
type
instruction
into
machine
code
lw
s
s
Solution
According
to
Table
s
and
s
are
registers
and
respectively
Table
B
indicates
that
lw
has
an
opcode
of
rs
specifies
the
base
address
s
and
rt
specifies
the
destination
register
s
The
immediate
imm
encodes
the
bit
offset
Thus
the
fields
and
machine
code
are
given
in
Figure
I
type
instructions
have
a
bit
immediate
field
but
the
immediates
are
used
in
bit
operations
For
example
lw
adds
a
bit
offset
to
a
bit
base
register
What
should
go
in
the
upper
half
of
the
bits
For
positive
immediates
the
upper
half
should
be
all
s
but
for
negative
immediates
the
upper
half
should
be
all
s
Recall
from
Section
that
this
is
called
sign
extension
An
N
bit
two
s
complement
number
is
sign
extended
to
an
M
bit
number
M
N
by
copying
the
sign
bit
most
significant
bit
of
the
N
bit
number
into
all
of
the
upper
bits
of
the
M
bit
number
Sign
extending
a
two
s
complement
number
does
not
change
its
value
Most
MIPS
instructions
sign
extend
the
immediate
For
example
addi
lw
and
sw
do
sign
extension
to
support
both
positive
and
negative
immediates
An
exception
to
this
rule
is
that
logical
operations
andi
ori
xori
place
s
in
the
upper
half
this
is
called
zero
extension
rather
than
sign
extension
Logical
operations
are
discussed
further
in
Section
J
type
Instructions
The
name
J
type
is
short
for
jump
type
This
format
is
used
only
with
jump
instructions
see
Section
This
instruction
format
uses
a
single
bit
address
operand
addr
as
shown
in
Figure
Like
other
formats
J
type
instructions
begin
with
a
bit
opcode
The
remaining
bits
are
used
to
specify
an
address
addr
Further
discussion
and
machine
code
examples
of
J
type
instructions
are
given
in
Sections
and
Interpreting
Machine
Language
Code
To
interpret
machine
language
one
must
decipher
the
fields
of
each
bit
instruction
word
Different
instructions
use
different
formats
but
all
formats
start
with
a
bit
opcode
field
Thus
the
best
place
to
begin
is
to
look
at
the
opcode
If
it
is
the
instruction
is
R
type
otherwise
it
is
I
type
or
J
type
CHAPTER
SIX
Architecture
op
rs
rt
imm
op
rs
rt
imm
lw
s
s
Assembly
Code
Machine
Code
Field
Values
x
E
FFE
bits
bits
bits
bits
E
FF
E
Figure
Machine
code
for
the
I
type
instruction
op
addr
bits
bits
J
type
Figure
J
type
instruction
format
Example
TRANSLATING
MACHINE
LANGUAGE
TO
ASSEMBLY
LANGUAGE
Translate
the
following
machine
language
code
into
assembly
language
x
FFF
x
F
Solution
First
we
represent
each
instruction
in
binary
and
look
at
the
six
most
significant
bits
to
find
the
opcode
for
each
instruction
as
shown
in
Figure
The
opcode
determines
how
to
interpret
the
rest
of
the
bits
The
opcodes
are
and
indicating
an
addi
and
R
type
instruction
respectively
The
funct
field
of
the
R
type
instruction
is
indicating
that
it
is
a
sub
instruction
Figure
shows
the
assembly
code
equivalent
of
the
two
machine
instructions
Machine
Language
addi
s
s
Machine
Code
Assembly
Code
Field
Values
x
FFF
op
rs
rt
imm
op
rs
rt
imm
FF
F
x
F
sub
t
s
s
op
F
op
rs
rt
rd
shamt
funct
rs
rt
rd
shamt
funct
Figure
Machine
code
to
assembly
code
translation
The
Power
of
the
Stored
Program
A
program
written
in
machine
language
is
a
series
of
bit
numbers
representing
the
instructions
Like
other
binary
numbers
these
instructions
can
be
stored
in
memory
This
is
called
the
stored
program
concept
and
it
is
a
key
reason
why
computers
are
so
powerful
Running
a
different
program
does
not
require
large
amounts
of
time
and
effort
to
reconfigure
or
rewire
hardware
it
only
requires
writing
the
new
program
to
memory
Instead
of
dedicated
hardware
the
stored
program
offers
general
purpose
computing
In
this
way
a
computer
can
execute
applications
ranging
from
a
calculator
to
a
word
processor
to
a
video
player
simply
by
changing
the
stored
program
Instructions
in
a
stored
program
are
retrieved
or
fetched
from
memory
and
executed
by
the
processor
Even
large
complex
programs
are
simplified
to
a
series
of
memory
reads
and
instruction
executions
Figure
shows
how
machine
instructions
are
stored
in
memory
In
MIPS
programs
the
instructions
are
normally
stored
starting
at
address
x
Remember
that
MIPS
memory
is
byte
addressable
so
bit
byte
instruction
addresses
advance
by
bytes
not
To
run
or
execute
the
stored
program
the
processor
fetches
the
instructions
from
memory
sequentially
The
fetched
instructions
are
then
decoded
and
executed
by
the
digital
hardware
The
address
of
the
current
instruction
is
kept
in
a
bit
register
called
the
program
counter
PC
The
PC
is
separate
from
the
registers
shown
previously
in
Table
To
execute
the
code
in
Figure
the
operating
system
sets
the
PC
to
address
x
The
processor
reads
the
instruction
at
that
memory
address
and
executes
the
instruction
x
C
A
The
processor
then
increments
the
PC
by
to
x
fetches
and
executes
that
instruction
and
repeats
The
architectural
state
of
a
microprocessor
holds
the
state
of
a
program
For
MIPS
the
architectural
state
consists
of
the
register
file
and
PC
If
the
operating
system
saves
the
architectural
state
at
some
point
in
the
program
it
can
interrupt
the
program
do
something
else
then
restore
the
state
such
that
the
program
continues
properly
unaware
that
it
was
ever
interrupted
The
architectural
state
is
also
of
great
importance
when
we
build
a
microprocessor
in
Chapter
PROGRAMMING
Software
languages
such
as
C
or
Java
are
called
high
level
programming
languages
because
they
are
written
at
a
more
abstract
level
than
assembly
language
Many
high
level
languages
use
common
software
constructs
such
as
arithmetic
and
logical
operations
if
else
statements
for
and
while
loops
array
indexing
and
procedure
calls
In
this
section
we
explore
how
to
translate
these
high
level
constructs
into
MIPS
assembly
code
Arithmetic
Logical
Instructions
The
MIPS
architecture
defines
a
variety
of
arithmetic
and
logical
instructions
We
introduce
these
instructions
briefly
here
because
they
are
necessary
to
implement
higher
level
constructs
CHAPTER
SIX
Architecture
Figure
Stored
program
addi
t
s
Assembly
Code
Machine
Code
lw
t
add
s
s
s
sub
t
t
t
x
C
A
x
x
FFF
x
D
Address
Instructions
C
D
FFF
C
A
Stored
Program
Main
Memory
PC
Ada
Lovelace
Wrote
the
first
computer
program
It
calculated
the
Bernoulli
numbers
using
Charles
Babbage
s
Analytical
Engine
She
was
the
only
legitimate
child
of
the
poet
Lord
Byron
Logical
Instructions
MIPS
logical
operations
include
and
or
xor
and
nor
These
R
type
instructions
operate
bit
by
bit
on
two
source
registers
and
write
the
result
to
the
destination
register
Figure
shows
examples
of
these
operations
on
the
two
source
values
xFFFF
and
x
A
F
B
The
figure
shows
the
values
stored
in
the
destination
register
rd
after
the
instruction
executes
The
and
instruction
is
useful
for
masking
bits
i
e
forcing
unwanted
bits
to
For
example
in
Figure
xFFFF
AND
x
A
F
B
x
A
The
and
instruction
masks
off
the
bottom
two
bytes
and
places
the
unmasked
top
two
bytes
of
s
x
A
in
s
Any
subset
of
register
bits
can
be
masked
The
or
instruction
is
useful
for
combining
bits
from
two
registers
For
example
x
A
OR
x
FC
x
A
FC
a
combination
of
the
two
values
MIPS
does
not
provide
a
NOT
instruction
but
A
NOR
NOT
A
so
the
NOR
instruction
can
substitute
Logical
operations
can
also
operate
on
immediates
These
I
type
instructions
are
andi
ori
and
xori
nori
is
not
provided
because
the
same
functionality
can
be
easily
implemented
using
the
other
instructions
as
will
be
explored
in
Exercise
Figure
shows
examples
of
the
andi
ori
and
xori
instructions
The
figure
gives
the
values
of
Programming
s
s
s
s
s
s
Source
Registers
Assembly
Code
Result
and
s
s
s
or
s
s
s
xor
s
s
s
nor
s
s
s
Figure
Logical
operations
s
Assembly
Code
imm
s
s
s
andi
s
s
xFA
Source
Values
Result
ori
s
s
xFA
xori
s
s
xFA
zero
extended
Figure
Logical
operations
with
immediates
Cha
the
source
register
and
immediate
and
the
value
of
the
destination
register
rt
after
the
instruction
executes
Because
these
instructions
operate
on
a
bit
value
from
a
register
and
a
bit
immediate
they
first
zeroextend
the
immediate
to
bits
Shift
Instructions
Shift
instructions
shift
the
value
in
a
register
left
or
right
by
up
to
bits
Shift
operations
multiply
or
divide
by
powers
of
two
MIPS
shift
operations
are
sll
shift
left
logical
srl
shift
right
logical
and
sra
shift
right
arithmetic
As
discussed
in
Section
left
shifts
always
fill
the
least
significant
bits
with
s
However
right
shifts
can
be
either
logical
s
shift
into
the
most
significant
bits
or
arithmetic
the
sign
bit
shifts
into
the
most
significant
bits
Figure
shows
the
machine
code
for
the
R
type
instructions
sll
srl
and
sra
rt
i
e
s
holds
the
bit
value
to
be
shifted
and
shamt
gives
the
amount
by
which
to
shift
The
shifted
result
is
placed
in
rd
Figure
shows
the
register
values
for
the
shift
instructions
sll
srl
and
sra
Shifting
a
value
left
by
N
is
equivalent
to
multiplying
it
by
N
Likewise
arithmetically
shifting
a
value
right
by
N
is
equivalent
to
dividing
it
by
N
as
discussed
in
Section
MIPS
also
has
variable
shift
instructions
sllv
shift
left
logical
variable
srlv
shift
right
logical
variable
and
srav
shift
right
arithmetic
variable
Figure
shows
the
machine
code
for
these
instructions
CHAPTER
SIX
Architecture
sll
t
s
srl
s
s
sra
s
s
op
rs
rt
rd
shamt
funct
op
rs
rt
rd
shamt
funct
Assembly
Code
Machine
Code
Field
Values
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
x
x
x
Figure
Shift
instruction
machine
code
s
Assembly
Code
shamt
t
s
s
sll
t
s
Source
Values
Result
srl
s
s
sra
s
s
Figure
Shift
operations
rt
i
e
s
holds
the
value
to
be
shifted
and
the
five
least
significant
bits
of
rs
i
e
s
give
the
amount
to
shift
The
shifted
result
is
placed
in
rd
as
before
The
shamt
field
is
ignored
and
should
be
all
s
Figure
shows
register
values
for
each
type
of
variable
shift
instruction
Generating
Constants
The
addi
instruction
is
helpful
for
assigning
bit
constants
as
shown
in
Code
Example
Programming
sllv
s
s
s
srlv
s
s
s
srav
s
s
s
op
rs
rt
rd
shamt
funct
Machine
Code
Assembly
Code
Field
Values
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
x
x
A
x
A
Figure
Variable
shift
instruction
machine
code
s
s
s
s
Assembly
Code
sllv
s
s
s
srlv
s
s
s
srav
s
s
s
Source
Values
Result
s
Figure
Variable
shift
operations
High
Level
Code
int
a
x
f
c
MIPS
Assembly
code
s
a
addi
s
x
f
c
a
x
f
c
Code
Example
BIT
CONSTANT
High
Level
Code
int
a
x
d
e
f
c
MIPS
Assembly
Code
s
a
lui
s
x
d
e
a
x
d
e
ori
s
s
x
f
c
a
x
d
e
f
c
Code
Example
BIT
CONSTANT
Chapter
To
assign
bit
constants
use
a
load
upper
immediate
instruction
lui
followed
by
an
or
immediate
ori
instruction
as
shown
in
Code
Example
lui
loads
a
bit
immediate
into
the
upper
half
of
a
register
and
sets
the
lower
half
to
As
mentioned
earlier
ori
merges
a
bit
immediate
into
the
lower
half
Multiplication
and
Division
Instructions
Multiplication
and
division
are
somewhat
different
from
other
arithmetic
operations
Multiplying
two
bit
numbers
produces
a
bit
product
Dividing
two
bit
numbers
produces
a
bit
quotient
and
a
bit
remainder
The
MIPS
architecture
has
two
special
purpose
registers
hi
and
lo
which
are
used
to
hold
the
results
of
multiplication
and
division
mult
s
s
multiplies
the
values
in
s
and
s
The
most
significant
bits
are
placed
in
hi
and
the
least
significant
bits
are
placed
in
lo
Similarly
div
s
s
computes
s
s
The
quotient
is
placed
in
lo
and
the
remainder
is
placed
in
hi
Branching
An
advantage
of
a
computer
over
a
calculator
is
its
ability
to
make
decisions
A
computer
performs
different
tasks
depending
on
the
input
For
example
if
else
statements
case
statements
while
loops
and
for
loops
all
conditionally
execute
code
depending
on
some
test
To
sequentially
execute
instructions
the
program
counter
increments
by
after
each
instruction
Branch
instructions
modify
the
program
counter
to
skip
over
sections
of
code
or
to
go
back
to
repeat
previous
code
Conditional
branch
instructions
perform
a
test
and
branch
only
if
the
test
is
TRUE
Unconditional
branch
instructions
called
jumps
always
branch
Conditional
Branches
The
MIPS
instruction
set
has
two
conditional
branch
instructions
branch
if
equal
beq
and
branch
if
not
equal
bne
beq
branches
when
the
values
in
two
registers
are
equal
and
bne
branches
when
they
are
not
equal
Code
Example
illustrates
the
use
of
beq
Note
that
branches
are
written
as
beq
rs
rt
imm
where
rs
is
the
first
source
register
This
order
is
reversed
from
most
I
type
instructions
When
the
program
in
Code
Example
reaches
the
branch
if
equal
instruction
beq
the
value
in
s
is
equal
to
the
value
in
s
so
the
branch
is
taken
That
is
the
next
instruction
executed
is
the
add
instruction
just
after
the
label
called
target
The
two
instructions
directly
after
the
branch
and
before
the
label
are
not
executed
Assembly
code
uses
labels
to
indicate
instruction
locations
in
the
program
When
the
assembly
code
is
translated
into
machine
code
these
CHAPTER
SIX
Architecture
The
int
data
type
in
C
refers
to
a
word
of
data
representing
a
two
s
complement
integer
MIPS
uses
bit
words
so
an
int
represents
a
number
in
the
range
hi
and
lo
are
not
among
the
usual
MIPS
registers
so
special
instructions
are
needed
to
access
them
mfhi
s
move
from
hi
copies
the
value
in
hi
to
s
mflo
s
move
from
lo
copies
the
value
in
lo
to
s
hi
and
lo
are
technically
part
of
the
architectural
state
however
we
generally
ignore
these
registers
in
this
book
Programming
MIPS
Assembly
Code
addi
s
s
addi
s
s
sll
s
s
s
beq
s
s
target
s
s
so
branch
is
taken
addi
s
s
not
executed
sub
s
s
s
not
executed
target
add
s
s
s
s
Code
Example
CONDITIONAL
BRANCHING
USING
beq
MIPS
Assembly
Code
addi
s
s
addi
s
s
s
s
s
s
bne
s
s
target
s
s
so
branch
is
not
taken
addi
s
s
s
sub
s
s
s
s
target
add
s
s
s
s
Code
Example
CONDITIONAL
BRANCHING
USING
bne
labels
are
translated
into
instruction
addresses
see
Section
MIPS
assembly
labels
are
followed
by
a
and
cannot
use
reserved
words
such
as
instruction
mnemonics
Most
programmers
indent
their
instructions
but
not
the
labels
to
help
make
labels
stand
out
Code
Example
shows
an
example
using
the
branch
if
not
equal
instruction
bne
In
this
case
the
branch
is
not
taken
because
s
is
equal
to
s
and
the
code
continues
to
execute
directly
after
the
bne
instruction
All
instructions
in
this
code
snippet
are
executed
Jump
A
program
can
unconditionally
branch
or
jump
using
the
three
types
of
jump
instructions
jump
j
jump
and
link
jal
and
jump
register
jr
Jump
j
jumps
directly
to
the
instruction
at
the
specified
label
Jump
and
link
jal
is
similar
to
j
but
is
used
by
procedures
to
save
a
return
address
as
will
be
discussed
in
Section
Jump
register
jr
jumps
to
the
address
held
in
a
register
Code
Example
shows
the
use
of
the
jump
instruction
j
After
the
j
target
instruction
the
program
in
Code
Example
unconditionally
continues
executing
the
add
instruction
at
the
label
target
All
of
the
instructions
between
the
jump
and
the
label
are
skipped
j
and
jal
are
J
type
instructions
jr
is
an
R
type
instruction
that
uses
only
the
rs
operand
Chapter
qxd
PM
CHAPTER
SIX
Architecture
MIPS
Assembly
Code
addi
s
s
addi
s
s
j
target
jump
to
target
addi
s
s
not
executed
sub
s
s
s
not
executed
target
add
s
s
s
s
Code
Example
UNCONDITIONAL
BRANCHING
USING
j
MIPS
Assembly
Code
x
addi
s
x
s
x
x
jr
s
jump
to
x
x
addi
s
not
executed
x
c
sra
s
s
not
executed
x
lw
s
s
executed
after
jr
instruction
Code
Example
UNCONDITIONAL
BRANCHING
USING
jr
Code
Example
shows
the
use
of
the
jump
register
instruction
jr
Instruction
addresses
are
given
to
the
left
of
each
instruction
jr
s
jumps
to
the
address
held
in
s
x
Conditional
Statements
if
statements
if
else
statements
and
case
statements
are
conditional
statements
commonly
used
by
high
level
languages
They
each
conditionally
execute
a
block
of
code
consisting
of
one
or
more
instructions
This
section
shows
how
to
translate
these
high
level
constructs
into
MIPS
assembly
language
If
Statements
An
if
statement
executes
a
block
of
code
the
if
block
only
when
a
condition
is
met
Code
Example
shows
how
to
translate
an
if
statement
into
MIPS
assembly
code
High
Level
Code
if
i
j
f
g
h
f
f
i
MIPS
Assembly
Code
s
f
s
g
s
h
s
i
s
j
bne
s
s
L
if
i
j
skip
if
block
add
s
s
s
if
block
f
g
h
L
sub
s
s
s
f
f
i
Code
Example
if
STATEMENT
Chapter
qxd
The
assembly
code
for
the
if
statement
tests
the
opposite
condition
of
the
one
in
the
high
level
code
In
Code
Example
the
high
level
code
tests
for
i
j
and
the
assembly
code
tests
for
i
j
The
bne
instruction
branches
skips
the
if
block
when
i
j
Otherwise
i
j
the
branch
is
not
taken
and
the
if
block
is
executed
as
desired
If
Else
Statements
if
else
statements
execute
one
of
two
blocks
of
code
depending
on
a
condition
When
the
condition
in
the
if
statement
is
met
the
if
block
is
executed
Otherwise
the
else
block
is
executed
Code
Example
shows
an
example
if
else
statement
Like
if
statements
if
else
assembly
code
tests
the
opposite
condition
of
the
one
in
the
high
level
code
For
example
in
Code
Example
the
high
level
code
tests
for
i
j
The
assembly
code
tests
for
the
opposite
condition
i
j
If
that
opposite
condition
is
TRUE
bne
skips
the
if
block
and
executes
the
else
block
Otherwise
the
if
block
executes
and
finishes
with
a
jump
instruction
j
to
jump
past
the
else
block
Programming
High
Level
Code
if
i
j
f
g
h
else
f
f
i
MIPS
Assembly
Code
s
f
s
g
s
h
s
i
s
j
bne
s
s
else
if
i
j
branch
to
else
add
s
s
s
if
block
f
g
h
j
L
skip
past
the
else
block
else
sub
s
s
s
else
block
f
f
i
L
Code
Example
if
else
STATEMENT
Switch
Case
Statements
switch
case
statements
execute
one
of
several
blocks
of
code
depending
on
the
conditions
If
no
conditions
are
met
the
default
block
is
executed
A
case
statement
is
equivalent
to
a
series
of
nested
if
else
statements
Code
Example
shows
two
high
level
code
snippets
with
the
same
functionality
they
calculate
the
fee
for
an
ATM
automatic
teller
machine
withdrawal
of
or
as
defined
by
amount
The
MIPS
assembly
implementation
is
the
same
for
both
high
level
code
snippets
Getting
Loopy
Loops
repeatedly
execute
a
block
of
code
depending
on
a
condition
for
loops
and
while
loops
are
common
loop
constructs
used
by
highlevel
languages
This
section
shows
how
to
translate
them
into
MIPS
assembly
language
Chapter
qxd
CHAPTER
SIX
Architecture
High
Level
Code
switch
amount
case
fee
break
case
fee
break
case
fee
break
default
fee
equivalent
function
using
if
else
statements
if
amount
fee
else
if
amount
fee
else
if
amount
fee
else
fee
MIPS
Assembly
Code
s
amount
s
fee
case
addi
t
t
bne
s
t
case
i
if
not
skip
to
case
addi
s
if
so
fee
j
done
and
break
out
of
case
case
addi
t
t
bne
s
t
case
i
if
not
skip
to
case
addi
s
if
so
fee
j
done
and
break
out
of
case
case
addi
t
t
bne
s
t
default
i
if
not
skip
to
default
addi
s
if
so
fee
j
done
and
break
out
of
case
default
add
s
charge
done
Code
Example
switch
case
STATEMENT
While
Loops
while
loops
repeatedly
execute
a
block
of
code
until
a
condition
is
not
met
The
while
loop
in
Code
Example
determines
the
value
of
x
such
that
x
It
executes
seven
times
until
pow
Like
if
else
statements
the
assembly
code
for
while
loops
tests
the
opposite
condition
of
the
one
given
in
the
high
level
code
If
that
opposite
condition
is
TRUE
the
while
loop
is
finished
High
Level
Code
int
pow
int
x
while
pow
pow
pow
x
x
MIPS
Assembly
Code
s
pow
s
x
addi
s
pow
addi
s
x
addi
t
t
for
comparison
while
beq
s
t
done
if
pow
exit
while
sll
s
s
pow
pow
addi
s
s
x
x
j
while
done
Code
Example
while
LOOP
Chapter
qxd
PM
Page
In
Code
Example
the
while
loop
compares
pow
to
and
exits
the
loop
if
it
is
equal
Otherwise
it
doubles
pow
using
a
left
shift
increments
x
and
jumps
back
to
the
start
of
the
while
loop
For
Loops
for
loops
like
while
loops
repeatedly
execute
a
block
of
code
until
a
condition
is
not
met
However
for
loops
add
support
for
a
loop
variable
which
typically
keeps
track
of
the
number
of
loop
executions
A
general
format
of
the
for
loop
is
for
initialization
condition
loop
operation
The
initialization
code
executes
before
the
for
loop
begins
The
condition
is
tested
at
the
beginning
of
each
loop
If
the
condition
is
not
met
the
loop
exits
The
loop
operation
executes
at
the
end
of
each
loop
Code
Example
adds
the
numbers
from
to
The
loop
variable
i
is
initialized
to
and
is
incremented
at
the
end
of
each
loop
iteration
At
the
beginning
of
each
iteration
the
for
loop
executes
only
when
i
is
not
equal
to
Otherwise
the
loop
is
finished
In
this
case
the
for
loop
executes
times
for
loops
can
be
implemented
using
a
while
loop
but
the
for
loop
is
often
convenient
Magnitude
Comparison
So
far
the
examples
have
used
beq
and
bne
to
perform
equality
or
inequality
comparisons
and
branches
MIPS
provides
the
set
less
than
instruction
slt
for
magnitude
comparison
slt
sets
rd
to
when
rs
rt
Otherwise
rd
is
Programming
High
Level
Code
int
sum
for
i
i
i
i
sum
sum
i
equivalent
to
the
following
while
loop
int
sum
int
i
while
i
sum
sum
i
i
i
MIPS
Assembly
Code
s
i
s
sum
add
s
sum
addi
s
i
addi
t
t
for
beq
s
t
done
if
i
branch
to
done
add
s
s
s
sum
sum
i
addi
s
s
increment
i
j
for
done
Code
Example
for
LOOP
Chapter
qxd
Example
LOOPS
USING
slt
The
following
high
level
code
adds
the
powers
of
from
to
Translate
it
into
assembly
language
high
level
code
int
sum
for
i
i
i
i
sum
sum
i
Solution
The
assembly
language
code
uses
the
set
less
than
slt
instruction
to
perform
the
less
than
comparison
in
the
for
loop
MIPS
assembly
code
s
i
s
sum
addi
s
sum
addi
s
i
addi
t
t
loop
slt
t
s
t
if
i
t
else
t
beq
t
done
if
t
i
branch
to
done
add
s
s
s
sum
sum
i
sll
s
s
i
i
j
loop
done
Exercise
explores
how
to
use
slt
for
other
magnitude
comparisons
including
greater
than
greater
than
or
equal
and
less
than
or
equal
Arrays
Arrays
are
useful
for
accessing
large
amounts
of
similar
data
An
array
is
organized
as
sequential
data
addresses
in
memory
Each
array
element
is
identified
by
a
number
called
its
index
The
number
of
elements
in
the
array
is
called
the
size
of
the
array
This
section
shows
how
to
access
array
elements
in
memory
Array
Indexing
Figure
shows
an
array
of
five
integers
stored
in
memory
The
index
ranges
from
to
In
this
case
the
array
is
stored
in
a
processor
s
main
memory
starting
at
base
address
x
The
base
address
gives
the
address
of
the
first
array
element
array
Code
Example
multiplies
the
first
two
elements
in
array
by
and
stores
them
back
in
the
array
The
first
step
in
accessing
an
array
element
is
to
load
the
base
address
of
the
array
into
a
register
Code
Example
loads
the
base
address
CHAPTER
SIX
Architecture
Chapter
qxd
into
s
Recall
that
the
load
upper
immediate
lui
and
or
immediate
ori
instructions
can
be
used
to
load
a
bit
constant
into
a
register
Code
Example
also
illustrates
why
lw
takes
a
base
address
and
an
offset
The
base
address
points
to
the
start
of
the
array
The
offset
can
be
used
to
access
subsequent
elements
of
the
array
For
example
array
is
stored
at
memory
address
x
one
word
or
four
bytes
after
array
so
it
is
accessed
at
an
offset
of
past
the
base
address
You
might
have
noticed
that
the
code
for
manipulating
each
of
the
two
array
elements
in
Code
Example
is
essentially
the
same
except
for
the
index
Duplicating
the
code
is
not
a
problem
when
accessing
two
array
elements
but
it
would
become
terribly
inefficient
for
accessing
all
of
the
elements
in
a
large
array
Code
Example
uses
a
for
loop
to
multiply
by
all
of
the
elements
of
a
element
array
stored
at
a
base
address
of
x
B
F
Figure
shows
the
element
array
in
memory
The
index
into
the
array
is
now
a
variable
i
rather
than
a
constant
so
we
cannot
take
advantage
of
the
immediate
offset
in
lw
Instead
we
compute
the
address
of
the
ith
element
and
store
it
in
t
Remember
that
each
array
element
is
a
word
but
that
memory
is
byte
addressed
so
the
offset
from
Programming
array
array
array
array
x
array
x
x
x
C
x
Main
Memory
Address
Data
Figure
Five
entry
array
with
base
address
of
x
High
Level
Code
int
array
array
array
array
array
MIPS
Assembly
Code
s
base
address
of
array
lui
s
x
s
x
ori
s
s
x
s
x
lw
t
s
t
array
sll
t
t
t
t
t
sw
t
s
array
t
lw
t
s
t
array
sll
t
t
t
t
t
sw
t
s
array
t
Code
Example
ACCESSING
ARRAYS
Chapter
qx
the
base
address
is
i
Shifting
left
by
is
a
convenient
way
to
multiply
by
in
MIPS
assembly
language
This
example
readily
extends
to
an
array
of
any
size
Bytes
and
Characters
Numbers
in
the
range
can
be
stored
in
a
single
byte
rather
than
an
entire
word
Because
there
are
much
fewer
than
characters
on
an
English
language
keyboard
English
characters
are
often
represented
by
bytes
The
C
language
uses
the
type
char
to
represent
a
byte
or
character
Early
computers
lacked
a
standard
mapping
between
bytes
and
English
characters
so
exchanging
text
between
computers
was
difficult
In
the
American
Standards
Association
published
the
American
Standard
Code
for
Information
Interchange
ASCII
which
assigns
each
text
character
a
unique
byte
value
Table
shows
these
character
encodings
for
printable
characters
The
ASCII
values
are
given
in
hexadecimal
Lower
case
and
upper
case
letters
differ
by
x
CHAPTER
SIX
Architecture
High
Level
Code
int
i
int
array
for
i
i
i
i
array
i
array
i
MIPS
Assembly
Code
s
array
base
address
s
i
initialization
code
lui
s
x
B
s
x
B
ori
s
s
xF
s
x
B
F
addi
s
i
addi
t
t
loop
slt
t
s
t
i
beq
t
done
if
not
then
done
sll
t
s
t
i
byte
offset
add
t
t
s
address
of
array
i
lw
t
t
t
array
i
sll
t
t
t
array
i
sw
t
t
array
i
array
i
addi
s
s
i
i
j
loop
repeat
done
Code
Example
ACCESSING
ARRAYS
USING
A
for
LOOP
Figure
Memory
holding
array
starting
at
base
address
x
B
F
Other
program
languages
such
as
Java
use
different
character
encodings
most
notably
Unicode
Unicode
uses
bits
to
represent
each
character
so
it
supports
accents
umlauts
and
Asian
languages
For
more
information
see
www
unicode
org
B
FF
C
array
B
FF
B
F
B
F
array
array
array
Main
Memory
Address
Data
Chapter
qxd
MIPS
provides
load
byte
and
store
byte
instructions
to
manipulate
bytes
or
characters
of
data
load
byte
unsigned
lbu
load
byte
lb
and
store
byte
sb
All
three
are
illustrated
in
Figure
Programming
ASCII
codes
developed
from
earlier
forms
of
character
encoding
Beginning
in
telegraph
machines
used
Morse
code
a
series
of
dots
and
dashes
to
represent
characters
For
example
the
letters
A
B
C
and
D
were
represented
as
and
respectively
The
number
of
dots
and
dashes
varied
with
each
letter
For
efficiency
common
letters
used
shorter
codes
In
Jean
MauriceEmile
Baudot
invented
a
bit
code
called
the
Baudot
code
For
example
A
B
C
and
D
were
represented
as
and
However
the
possible
encodings
of
this
bit
code
were
not
sufficient
for
all
the
English
characters
But
bit
encoding
was
Thus
as
electronic
communication
became
prevalent
bit
ASCII
encoding
emerged
as
the
standard
Table
ASCII
encodings
Char
Char
Char
Char
Char
Char
space
P
p
A
Q
a
q
B
R
b
r
C
S
c
s
D
T
d
t
E
U
e
u
F
V
f
v
G
W
g
w
H
X
h
x
I
Y
i
y
A
A
A
J
A
Z
A
j
A
z
B
B
B
K
B
B
k
B
C
C
C
L
C
C
l
C
D
D
D
M
D
D
m
D
E
E
E
N
E
E
n
E
F
F
F
O
F
F
o
Byte
Address
Data
CF
s
C
lbu
s
Little
Endian
Memory
Registers
s
FFFF
FF
C
lb
s
s
XX
XX
XX
B
sb
s
Figure
Instructions
for
loading
and
storing
bytes
Cha
Load
byte
unsigned
lbu
zero
extends
the
byte
and
load
byte
lb
sign
extends
the
byte
to
fill
the
entire
bit
register
Store
byte
sb
stores
the
least
significant
byte
of
the
bit
register
into
the
specified
byte
address
in
memory
In
Figure
lbu
loads
the
byte
at
memory
address
into
the
least
significant
byte
of
s
and
fills
the
remaining
register
bits
with
lb
loads
the
sign
extended
byte
at
memory
address
into
s
sb
stores
the
least
significant
byte
of
s
into
memory
byte
it
replaces
xF
with
x
B
The
more
significant
bytes
of
s
are
ignored
Example
USING
lb
AND
sb
TO
ACCESS
A
CHARACTER
ARRAY
The
following
high
level
code
converts
a
ten
entry
array
of
characters
from
lower
case
to
upper
case
by
subtracting
from
each
array
entry
Translate
it
into
MIPS
assembly
language
Remember
that
the
address
difference
between
array
elements
is
now
byte
not
bytes
Assume
that
s
already
holds
the
base
address
of
chararray
high
level
code
char
chararray
int
i
for
i
i
i
i
chararray
i
chararray
i
Solution
MIPS
assembly
code
s
base
address
of
chararray
s
i
addi
s
i
addi
t
t
loop
beq
t
s
done
if
i
exit
loop
add
t
s
s
t
address
of
chararray
i
lb
t
t
t
array
i
addi
t
t
convert
to
upper
case
t
t
sb
t
t
store
new
value
in
array
chararray
i
t
addi
s
s
i
i
j
loop
repeat
done
A
series
of
characters
is
called
a
string
Strings
have
a
variable
length
so
programming
languages
must
provide
a
way
to
determine
the
length
or
end
of
the
string
In
C
the
null
character
x
signifies
the
end
of
a
string
For
example
Figure
shows
the
string
Hello
x
C
C
F
stored
in
memory
The
string
is
seven
bytes
long
and
extends
from
address
x
FFF
to
x
FFF
The
first
character
of
the
string
H
x
is
stored
at
the
lowest
byte
address
x
FFF
CHAPTER
SIX
Architecture
Figure
The
string
Hello
stored
in
memory
Word
Address
FFF
FFF
Data
C
C
F
Little
Endian
Memory
Byte
Byte
Chapter
qxd
Procedure
Calls
High
level
languages
often
use
procedures
also
called
functions
to
reuse
frequently
accessed
code
and
to
make
a
program
more
readable
Procedures
have
inputs
called
arguments
and
an
output
called
the
return
value
Procedures
should
calculate
the
return
value
and
cause
no
other
unintended
side
effects
When
one
procedure
calls
another
the
calling
procedure
the
caller
and
the
called
procedure
the
callee
must
agree
on
where
to
put
the
arguments
and
the
return
value
In
MIPS
the
caller
conventionally
places
up
to
four
arguments
in
registers
a
a
before
making
the
procedure
call
and
the
callee
places
the
return
value
in
registers
v
v
before
finishing
By
following
this
convention
both
procedures
know
where
to
find
the
arguments
and
return
value
even
if
the
caller
and
callee
were
written
by
different
people
The
callee
must
not
interfere
with
the
function
of
the
caller
Briefly
this
means
that
the
callee
must
know
where
to
return
to
after
it
completes
and
it
must
not
trample
on
any
registers
or
memory
needed
by
the
caller
The
caller
stores
the
return
address
in
ra
at
the
same
time
it
jumps
to
the
callee
using
the
jump
and
link
instruction
jal
The
callee
must
not
overwrite
any
architectural
state
or
memory
that
the
caller
is
depending
on
Specifically
the
callee
must
leave
the
saved
registers
s
s
ra
and
the
stack
a
portion
of
memory
used
for
temporary
variables
unmodified
This
section
shows
how
to
call
and
return
from
a
procedure
It
shows
how
procedures
access
input
arguments
and
the
return
value
and
how
they
use
the
stack
to
store
temporary
variables
Procedure
Calls
and
Returns
MIPS
uses
the
jump
and
link
instruction
jal
to
call
a
procedure
and
the
jump
register
instruction
jr
to
return
from
a
procedure
Code
Example
shows
the
main
procedure
calling
the
simple
procedure
main
is
the
caller
and
simple
is
the
callee
The
simple
procedure
is
called
with
no
input
arguments
and
generates
no
return
value
it
simply
returns
to
the
caller
In
Code
Example
instruction
addresses
are
given
to
the
left
of
each
MIPS
instruction
in
hexadecimal
Programming
High
Level
Code
MIPS
Assembly
Code
int
main
simple
x
main
jal
simple
call
procedure
x
void
means
the
function
returns
no
value
void
simple
return
x
simple
jr
ra
return
Code
Example
simple
PROCEDURE
CALL
Jump
and
link
jal
and
jump
register
jr
ra
are
the
two
essential
instructions
needed
for
a
procedure
call
jal
performs
two
functions
it
stores
the
address
of
the
next
instruction
the
instruction
after
jal
in
the
return
address
register
ra
and
it
jumps
to
the
target
instruction
In
Code
Example
the
main
procedure
calls
the
simple
procedure
by
executing
the
jump
and
link
jal
instruction
jal
jumps
to
the
simple
label
and
stores
x
in
ra
The
simple
procedure
returns
immediately
by
executing
the
instruction
jr
ra
jumping
to
the
instruction
address
held
in
ra
The
main
procedure
then
continues
executing
at
this
address
x
Input
Arguments
and
Return
Values
The
simple
procedure
in
Code
Example
is
not
very
useful
because
it
receives
no
input
from
the
calling
procedure
main
and
returns
no
output
By
MIPS
convention
procedures
use
a
a
for
input
arguments
and
v
v
for
the
return
value
In
Code
Example
the
procedure
diffofsums
is
called
with
four
arguments
and
returns
one
result
According
to
MIPS
convention
the
calling
procedure
main
places
the
procedure
arguments
from
left
to
right
into
the
input
registers
a
a
The
called
procedure
diffofsums
stores
the
return
value
in
the
return
register
v
A
procedure
that
returns
a
bit
value
such
as
a
double
precision
floating
point
number
uses
both
return
registers
v
and
v
When
a
procedure
with
more
than
four
arguments
is
called
the
additional
input
arguments
are
placed
on
the
stack
which
we
discuss
next
CHAPTER
SIX
Architecture
High
Level
Code
MIPS
Assembly
Code
s
y
int
main
main
int
y
addi
a
argument
addi
a
argument
addi
a
argument
addi
a
argument
y
diffofsums
jal
diffofsums
call
procedure
add
s
v
y
returned
value
s
result
int
diffofsums
int
f
int
g
int
h
int
i
diffofsums
add
t
a
a
t
f
g
int
result
add
t
a
a
t
h
i
sub
s
t
t
result
f
g
h
i
result
f
g
h
i
add
v
s
put
return
value
in
v
return
result
jr
ra
return
to
caller
Code
Example
PROCEDURE
CALL
WITH
ARGUMENTS
AND
RETURN
VALUES
Code
Example
has
some
subtle
errors
Code
Examples
and
on
page
show
improved
versions
of
the
program
Chapter
qxd
Programming
The
Stack
The
stack
is
memory
that
is
used
to
save
local
variables
within
a
procedure
The
stack
expands
uses
more
memory
as
the
processor
needs
more
scratch
space
and
contracts
uses
less
memory
when
the
processor
no
longer
needs
the
variables
stored
there
Before
explaining
how
procedures
use
the
stack
to
store
temporary
variables
we
explain
how
the
stack
works
The
stack
is
a
last
in
first
out
LIFO
queue
Like
a
stack
of
dishes
the
last
item
pushed
onto
the
stack
the
top
dish
is
the
first
one
that
can
be
pulled
popped
off
Each
procedure
may
allocate
stack
space
to
store
local
variables
but
must
deallocate
it
before
returning
The
top
of
the
stack
is
the
most
recently
allocated
space
Whereas
a
stack
of
dishes
grows
up
in
space
the
MIPS
stack
grows
down
in
memory
The
stack
expands
to
lower
memory
addresses
when
a
program
needs
more
scratch
space
Figure
shows
a
picture
of
the
stack
The
stack
pointer
sp
is
a
special
MIPS
register
that
points
to
the
top
of
the
stack
A
pointer
is
a
fancy
name
for
a
memory
address
It
points
to
gives
the
address
of
data
For
example
in
Figure
a
the
stack
pointer
sp
holds
the
address
value
x
FFFFFFC
and
points
to
the
data
value
x
sp
points
to
the
top
of
the
stack
the
lowest
accessible
memory
address
on
the
stack
Thus
in
Figure
a
the
stack
cannot
access
memory
below
memory
word
x
FFFFFFC
The
stack
pointer
sp
starts
at
a
high
memory
address
and
decrements
to
expand
as
needed
Figure
b
shows
the
stack
expanding
to
allow
two
more
data
words
of
temporary
storage
To
do
so
sp
decrements
by
to
become
x
FFFFFF
Two
additional
data
words
xAABBCCDD
and
x
are
temporarily
stored
on
the
stack
One
of
the
important
uses
of
the
stack
is
to
save
and
restore
registers
that
are
used
by
a
procedure
Recall
that
a
procedure
should
calculate
a
return
value
but
have
no
other
unintended
side
effects
In
particular
it
should
not
modify
any
registers
besides
the
one
containing
the
return
value
v
The
diffofsums
procedure
in
Code
Example
violates
this
rule
because
it
modifies
t
t
and
s
If
main
had
been
using
t
t
or
s
before
the
call
to
diffofsums
the
contents
of
these
registers
would
have
been
corrupted
by
the
procedure
call
To
solve
this
problem
a
procedure
saves
registers
on
the
stack
before
it
modifies
them
then
restores
them
from
the
stack
before
it
returns
Specifically
it
performs
the
following
steps
Makes
space
on
the
stack
to
store
the
values
of
one
or
more
registers
Stores
the
values
of
the
registers
on
the
stack
Executes
the
procedure
using
the
registers
Restores
the
original
values
of
the
registers
from
the
stack
Deallocates
space
on
the
stack
Data
FFFFFFC
FFFFFF
FFFFFF
FFFFFF
Address
sp
a
FFFFFFC
FFFFFF
FFFFFF
FFFFFF
Address
b
Data
sp
AABBCCDD
Figure
The
stack
Code
Example
shows
an
improved
version
of
diffofsums
that
saves
and
restores
t
t
and
s
The
new
lines
are
indicated
in
blue
Figure
shows
the
stack
before
during
and
after
a
call
to
the
diffofsums
procedure
from
Code
Example
diffofsums
makes
room
for
three
words
on
the
stack
by
decrementing
the
stack
pointer
sp
by
It
then
stores
the
current
values
of
s
t
and
t
in
the
newly
allocated
space
It
executes
the
rest
of
the
procedure
changing
the
values
in
these
three
registers
At
the
end
of
the
procedure
diffofsums
restores
the
values
of
s
t
and
t
from
the
stack
deallocates
its
stack
space
and
returns
When
the
procedure
returns
v
holds
the
result
but
there
are
no
other
side
effects
s
t
t
and
sp
have
the
same
values
as
they
did
before
the
procedure
call
The
stack
space
that
a
procedure
allocates
for
itself
is
called
its
stack
frame
diffofsums
s
stack
frame
is
three
words
deep
The
principle
of
modularity
tells
us
that
each
procedure
should
access
only
its
own
stack
frame
not
the
frames
belonging
to
other
procedures
Preserved
Registers
Code
Example
assumes
that
temporary
registers
t
and
t
must
be
saved
and
restored
If
the
calling
procedure
does
not
use
those
registers
the
effort
to
save
and
restore
them
is
wasted
To
avoid
this
waste
MIPS
divides
registers
into
preserved
and
nonpreserved
categories
The
preserved
registers
include
s
s
hence
their
name
saved
The
nonpreserved
registers
include
t
t
hence
their
name
temporary
A
procedure
must
save
and
restore
any
of
the
preserved
registers
that
it
wishes
to
use
but
it
can
change
the
nonpreserved
registers
freely
Code
Example
shows
a
further
improved
version
of
diffofsums
that
saves
only
s
on
the
stack
t
and
t
are
nonpreserved
registers
so
they
need
not
be
saved
Remember
that
when
one
procedure
calls
another
the
former
is
the
caller
and
the
latter
is
the
callee
The
callee
must
save
and
restore
any
preserved
registers
that
it
wishes
to
use
The
callee
may
change
any
of
the
nonpreserved
registers
Hence
if
the
caller
is
holding
active
data
in
a
CHAPTER
SIX
Architecture
Data
FC
F
F
F
Address
sp
a
Data
sp
c
FC
F
F
F
Address
Data
FC
F
F
F
Address
sp
b
s
t
stack
frame
t
Figure
The
stack
a
before
b
during
and
c
after
diffofsums
procedure
call
nonpreserved
register
the
caller
needs
to
save
that
nonpreserved
register
before
making
the
procedure
call
and
then
needs
to
restore
it
afterward
For
these
reasons
preserved
registers
are
also
called
callee
save
and
nonpreserved
registers
are
called
caller
save
Table
summarizes
which
registers
are
preserved
s
s
are
generally
used
to
hold
local
variables
within
a
procedure
so
they
must
be
saved
ra
must
also
be
saved
so
that
the
procedure
knows
where
to
return
t
t
are
used
to
hold
temporary
results
before
they
are
assigned
to
local
variables
These
calculations
typically
complete
before
a
procedure
call
is
made
so
they
are
not
preserved
and
it
is
rare
that
the
caller
needs
to
save
them
a
a
are
often
overwritten
in
the
process
of
calling
a
procedure
Hence
they
must
be
saved
by
the
caller
if
the
caller
depends
on
any
of
its
own
arguments
after
a
called
procedure
returns
v
v
certainly
should
not
be
preserved
because
the
callee
returns
its
result
in
these
registers
Programming
MIPS
Assembly
Code
s
result
diffofsums
addi
sp
sp
make
space
on
stack
to
store
three
registers
sw
s
sp
save
s
on
stack
sw
t
sp
save
t
on
stack
sw
t
sp
save
t
on
stack
add
t
a
a
t
f
g
add
t
a
a
t
h
i
sub
s
t
t
result
f
g
h
i
add
v
s
put
return
value
in
v
lw
t
sp
restore
t
from
stack
lw
t
sp
restore
t
from
stack
lw
s
sp
restore
s
from
stack
addi
sp
sp
deallocate
stack
space
jr
ra
return
to
caller
Code
Example
PROCEDURE
SAVING
REGISTERS
ON
THE
STACK
MIPS
Assembly
Code
s
result
diffofsums
addi
sp
sp
make
space
on
stack
to
store
one
register
sw
s
sp
save
s
on
stack
add
t
a
a
t
f
g
add
t
a
a
t
h
i
sub
s
t
t
result
f
g
h
i
add
v
s
put
return
value
in
v
lw
s
sp
restore
s
from
stack
addi
sp
sp
deallocate
stack
space
jr
ra
return
to
caller
Code
Example
PROCEDURE
SAVING
PRESERVED
REGISTERS
ON
THE
STACK
Chapter
qxd
CHAPTER
SIX
Architecture
The
stack
above
the
stack
pointer
is
automatically
preserved
as
long
as
the
callee
does
not
write
to
memory
addresses
above
sp
In
this
way
it
does
not
modify
the
stack
frame
of
any
other
procedures
The
stack
pointer
itself
is
preserved
because
the
callee
deallocates
its
stack
frame
before
returning
by
adding
back
the
same
amount
that
it
subtracted
from
sp
at
the
beginning
of
the
procedure
Recursive
Procedure
Calls
A
procedure
that
does
not
call
others
is
called
a
leaf
procedure
an
example
is
diffofsums
A
procedure
that
does
call
others
is
called
a
nonleaf
procedure
As
mentioned
earlier
nonleaf
procedures
are
somewhat
more
complicated
because
they
may
need
to
save
nonpreserved
registers
on
the
stack
before
they
call
another
procedure
and
then
restore
those
registers
afterward
Specifically
the
caller
saves
any
nonpreserved
registers
t
t
and
a
a
that
are
needed
after
the
call
The
callee
saves
any
of
the
preserved
registers
s
s
and
ra
that
it
intends
to
modify
A
recursive
procedure
is
a
nonleaf
procedure
that
calls
itself
The
factorial
function
can
be
written
as
a
recursive
procedure
call
Recall
that
factorial
n
n
n
n
The
factorial
function
can
be
rewritten
recursively
as
factorial
n
n
factorial
n
The
factorial
of
is
simply
Code
Example
shows
the
factorial
function
written
as
a
recursive
procedure
To
conveniently
refer
to
program
addresses
we
assume
that
the
program
starts
at
address
x
The
factorial
procedure
might
modify
a
and
ra
so
it
saves
them
on
the
stack
It
then
checks
whether
n
If
so
it
puts
the
return
value
of
in
v
restores
the
stack
pointer
and
returns
to
the
caller
It
does
not
have
to
reload
ra
and
a
in
this
case
because
they
were
never
modified
If
n
the
procedure
recursively
calls
factorial
n
It
then
restores
the
value
of
n
a
and
the
return
address
ra
from
the
stack
performs
the
multiplication
and
returns
this
result
The
multiply
instruction
mul
v
a
v
multiplies
a
and
v
and
places
the
result
in
v
It
is
discussed
further
in
Section
Table
Preserved
and
nonpreserved
registers
Preserved
Nonpreserved
Saved
registers
s
s
Temporary
registers
t
t
Return
address
ra
Argument
registers
a
a
Stack
pointer
sp
Return
value
registers
v
v
Stack
above
the
stack
pointer
Stack
below
the
stack
pointer
Ch
Programming
High
Level
Code
int
factorial
int
n
if
n
return
else
return
n
factorial
n
MIPS
Assembly
Code
x
factorial
addi
sp
sp
make
room
on
stack
x
sw
a
sp
store
a
x
sw
ra
sp
store
ra
x
C
addi
t
t
xA
slt
t
a
t
a
xA
beq
t
else
no
goto
else
xA
addi
v
yes
return
xAC
addi
sp
sp
restore
sp
xB
jr
ra
return
xB
else
addi
a
a
n
n
xB
jal
factorial
recursive
call
xBC
lw
ra
sp
restore
ra
xC
lw
a
sp
restore
a
xC
addi
sp
sp
restore
sp
xC
mul
v
a
v
n
factorial
n
xCC
jr
ra
return
Code
Example
factorial
RECURSIVE
PROCEDURE
CALL
Figure
shows
the
stack
when
executing
factorial
We
assume
that
sp
initially
points
to
xFC
as
shown
in
Figure
a
The
procedure
creates
a
two
word
stack
frame
to
hold
a
and
ra
On
the
first
invocation
factorial
saves
a
holding
n
at
xF
and
ra
at
xF
as
shown
in
Figure
b
The
procedure
then
changes
a
to
n
and
recursively
calls
factorial
making
ra
hold
xBC
On
the
second
invocation
it
saves
a
holding
n
at
xF
and
ra
at
xEC
This
time
we
know
that
ra
contains
xBC
The
procedure
then
changes
a
to
n
and
recursively
calls
factorial
On
the
third
invocation
it
saves
a
holding
n
at
xE
and
ra
at
xE
This
time
ra
again
contains
xBC
The
third
invocation
of
Figure
Stack
during
factorial
procedure
call
when
n
a
before
call
b
after
last
recursive
call
c
after
return
sp
a
FC
F
F
F
EC
E
E
E
DC
Address
Data
FC
F
F
F
b
ra
EC
E
E
E
DC
sp
sp
sp
sp
Address
Data
a
x
ra
xBC
a
x
ra
xBC
a
x
FC
F
F
F
c
EC
E
E
E
DC
sp
sp
sp
sp
a
v
x
a
v
x
a
v
x
v
Address
Data
ra
a
x
ra
xBC
a
x
ra
xBC
a
x
Chapter
factorial
returns
the
value
in
v
and
deallocates
the
stack
frame
before
returning
to
the
second
invocation
The
second
invocation
restores
n
to
restores
ra
to
xBC
it
happened
to
already
have
this
value
deallocates
the
stack
frame
and
returns
v
to
the
first
invocation
The
first
invocation
restores
n
to
restores
ra
to
the
return
address
of
the
caller
deallocates
the
stack
frame
and
returns
v
Figure
c
shows
the
stack
as
the
recursively
called
procedures
return
When
factorial
returns
to
the
caller
the
stack
pointer
is
in
its
original
position
xFC
none
of
the
contents
of
the
stack
above
the
pointer
have
changed
and
all
of
the
preserved
registers
hold
their
original
values
v
holds
the
return
value
Additional
Arguments
and
Local
Variables
Procedures
may
have
more
than
four
input
arguments
and
local
variables
The
stack
is
used
to
store
these
temporary
values
By
MIPS
convention
if
a
procedure
has
more
than
four
arguments
the
first
four
are
passed
in
the
argument
registers
as
usual
Additional
arguments
are
passed
on
the
stack
just
above
sp
The
caller
must
expand
its
stack
to
make
room
for
the
additional
arguments
Figure
a
shows
the
caller
s
stack
for
calling
a
procedure
with
more
than
four
arguments
A
procedure
can
also
declare
local
variables
or
arrays
Local
variables
are
declared
within
a
procedure
and
can
be
accessed
only
within
that
procedure
Local
variables
are
stored
in
s
s
if
there
are
too
many
local
variables
they
can
also
be
stored
in
the
procedure
s
stack
frame
In
particular
local
arrays
are
stored
on
the
stack
Figure
b
shows
the
organization
of
a
callee
s
stack
frame
The
frame
holds
the
procedure
s
own
arguments
if
it
calls
other
procedures
the
return
address
and
any
of
the
saved
registers
that
the
procedure
will
CHAPTER
SIX
Architecture
sp
ra
if
needed
additional
arguments
a
a
if
needed
s
s
if
needed
local
variables
or
arrays
spstack
frame
additional
arguments
Figure
Stack
usage
left
before
call
right
after
call
Chap
modify
It
also
holds
local
arrays
and
any
excess
local
variables
If
the
callee
has
more
than
four
arguments
it
finds
them
in
the
caller
s
stack
frame
Accessing
additional
input
arguments
is
the
one
exception
in
which
a
procedure
can
access
stack
data
not
in
its
own
stack
frame
ADDRESSING
MODES
MIPS
uses
five
addressing
modes
register
only
immediate
base
PCrelative
and
pseudo
direct
The
first
three
modes
register
only
immediate
and
base
addressing
define
modes
of
reading
and
writing
operands
The
last
two
PC
relative
and
pseudo
direct
addressing
define
modes
of
writing
the
program
counter
PC
Register
Only
Addressing
Register
only
addressing
uses
registers
for
all
source
and
destination
operands
All
R
type
instructions
use
register
only
addressing
Immediate
Addressing
Immediate
addressing
uses
the
bit
immediate
along
with
registers
as
operands
Some
I
type
instructions
such
as
add
immediate
addi
and
load
upper
immediate
lui
use
immediate
addressing
Base
Addressing
Memory
access
instructions
such
as
load
word
lw
and
store
word
sw
use
base
addressing
The
effective
address
of
the
memory
operand
is
found
by
adding
the
base
address
in
register
rs
to
the
sign
extended
bit
offset
found
in
the
immediate
field
PC
relative
Addressing
Conditional
branch
instructions
use
PC
relative
addressing
to
specify
the
new
value
of
the
PC
if
the
branch
is
taken
The
signed
offset
in
the
immediate
field
is
added
to
the
PC
to
obtain
the
new
PC
hence
the
branch
destination
address
is
said
to
be
relative
to
the
current
PC
Code
Example
shows
part
of
the
factorial
procedure
from
Code
Example
Figure
shows
the
machine
code
for
the
beq
instruction
The
branch
target
address
BTA
is
the
address
of
the
next
instruction
to
execute
if
the
branch
is
taken
The
beq
instruction
in
Figure
has
a
BTA
of
xB
the
instruction
address
of
the
else
label
The
bit
immediate
field
gives
the
number
of
instructions
between
the
BTA
and
the
instruction
after
the
branch
instruction
the
instruction
at
PC
In
this
case
the
value
in
the
immediate
field
of
beq
is
because
the
BTA
xB
is
instructions
past
PC
xA
The
processor
calculates
the
BTA
from
the
instruction
by
signextending
the
bit
immediate
multiplying
it
by
to
convert
words
to
bytes
and
adding
it
to
PC
Addressing
Modes
Cha
Example
CALCULATING
THE
IMMEDIATE
FIELD
FOR
PC
RELATIVE
ADDRESSING
Calculate
the
immediate
field
and
show
the
machine
code
for
the
branch
not
equal
bne
instruction
in
the
following
program
MIPS
assembly
code
x
loop
add
t
a
s
x
lb
t
t
x
add
t
a
s
x
C
sb
t
t
x
addi
s
s
x
bne
t
loop
x
lw
s
sp
Solution
Figure
shows
the
machine
code
for
the
bne
instruction
Its
branch
target
address
x
is
instructions
behind
PC
x
so
the
immediate
field
is
CHAPTER
SIX
Architecture
op
rs
imm
beq
t
else
Machine
Code
Assembly
Code
bits
bits
bits
bits
x
bits
Field
Values
op
rs
rt
imm
bits
bits
bits
rt
Figure
Machine
code
for
beq
MIPS
Assembly
Code
xA
beq
t
else
xA
addi
v
xAC
addi
sp
sp
xB
jr
ra
xB
else
addi
a
a
xB
jal
factorial
Code
Example
CALCULATING
THE
BRANCH
TARGET
ADDRESS
bne
t
loop
Assembly
Code
Machine
Code
bits
bits
bits
bits
x
FFFA
bits
bits
bits
bits
op
rs
rt
imm
op
rs
rt
imm
Field
Values
Figure
bne
machine
code
Pseudo
Direct
Addressing
In
direct
addressing
an
address
is
specified
in
the
instruction
The
jump
instructions
j
and
jal
ideally
would
use
direct
addressing
to
specify
a
C
bit
jump
target
address
JTA
to
indicate
the
instruction
address
to
execute
next
Unfortunately
the
J
type
instruction
encoding
does
not
have
enough
bits
to
specify
a
full
bit
JTA
Six
bits
of
the
instruction
are
used
for
the
opcode
so
only
bits
are
left
to
encode
the
JTA
Fortunately
the
two
least
significant
bits
JTA
should
always
be
because
instructions
are
word
aligned
The
next
bits
JTA
are
taken
from
the
addr
field
of
the
instruction
The
four
most
significant
bits
JTA
are
obtained
from
the
four
most
significant
bits
of
PC
This
addressing
mode
is
called
pseudo
direct
Code
Example
illustrates
a
jal
instruction
using
pseudo
direct
addressing
The
JTA
of
the
jal
instruction
is
x
A
Figure
shows
the
machine
code
for
this
jal
instruction
The
top
four
bits
and
bottom
two
bits
of
the
JTA
are
discarded
The
remaining
bits
are
stored
in
the
bit
address
field
addr
The
processor
calculates
the
JTA
from
the
J
type
instruction
by
appending
two
s
and
prepending
the
four
most
significant
bits
of
PC
to
the
bit
address
field
addr
Because
the
four
most
significant
bits
of
the
JTA
are
taken
from
PC
the
jump
range
is
limited
The
range
limits
of
branch
and
jump
instructions
are
explored
in
Exercises
to
All
J
type
instructions
j
and
jal
use
pseudo
direct
addressing
Note
that
the
jump
register
instruction
jr
is
not
a
J
type
instruction
It
is
an
R
type
instruction
that
jumps
to
the
bit
value
held
in
register
rs
Addressing
Modes
MIPS
Assembly
Code
x
C
jal
sum
x
A
sum
add
v
a
a
Code
Example
CALCULATING
THE
JUMP
TARGET
ADDRESS
op
jal
sum
Assembly
Code
Machine
Code
x
C
op
bits
JTA
bit
addr
x
x
A
addr
x
bits
bits
bits
addr
Field
Values
Figure
jal
machine
code
Cha
CHAPTER
SIX
Architecture
LIGHTS
CAMERA
ACTION
COMPILING
ASSEMBLING
AND
LOADING
Up
until
now
we
have
shown
how
to
translate
short
high
level
code
snippets
into
assembly
and
machine
code
This
section
describes
how
to
compile
and
assemble
a
complete
high
level
program
and
how
to
load
the
program
into
memory
for
execution
We
begin
by
introducing
the
MIPS
memory
map
which
defines
where
code
data
and
stack
memory
are
located
We
then
show
the
steps
of
code
execution
for
a
sample
program
The
Memory
Map
With
bit
addresses
the
MIPS
address
space
spans
bytes
gigabytes
GB
Word
addresses
are
divisible
by
and
range
from
to
xFFFFFFFC
Figure
shows
the
MIPS
memory
map
The
MIPS
architecture
divides
the
address
space
into
four
parts
or
segments
the
text
segment
global
data
segment
dynamic
data
segment
and
reserved
segments
The
following
sections
describes
each
segment
The
Text
Segment
The
text
segment
stores
the
machine
language
program
It
is
large
enough
to
accommodate
almost
MB
of
code
Note
that
the
four
most
significant
bits
of
the
address
in
the
text
space
are
all
so
the
j
instruction
can
directly
jump
to
any
address
in
the
program
The
Global
Data
Segment
The
global
data
segment
stores
global
variables
that
in
contrast
to
local
variables
can
be
seen
by
all
procedures
in
a
program
Global
variables
Address
Segment
sp
x
FFFFFFC
xFFFFFFFC
x
x
FFFFFFC
x
x
FFFC
x
x
FFFFFFC
x
x
FFFFC
x
Reserved
Stack
Heap
Global
Data
Text
Reserved
gp
x
PC
x
Figure
MIPS
memory
map
Dynamic
Data
C
are
defined
at
start
up
before
the
program
begins
executing
These
variables
are
declared
outside
the
main
procedure
in
a
C
program
and
can
be
accessed
by
any
procedure
The
global
data
segment
is
large
enough
to
store
KB
of
global
variables
Global
variables
are
accessed
using
the
global
pointer
gp
which
is
initialized
to
x
Unlike
the
stack
pointer
sp
gp
does
not
change
during
program
execution
Any
global
variable
can
be
accessed
with
a
bit
positive
or
negative
offset
from
gp
The
offset
is
known
at
assembly
time
so
the
variables
can
be
efficiently
accessed
using
base
addressing
mode
with
constant
offsets
The
Dynamic
Data
Segment
The
dynamic
data
segment
holds
the
stack
and
the
heap
The
data
in
this
segment
are
not
known
at
start
up
but
are
dynamically
allocated
and
deallocated
throughout
the
execution
of
the
program
This
is
the
largest
segment
of
memory
used
by
a
program
spanning
almost
GB
of
the
address
space
As
discussed
in
Section
the
stack
is
used
to
save
and
restore
registers
used
by
procedures
and
to
hold
local
variables
such
as
arrays
The
stack
grows
downward
from
the
top
of
the
dynamic
data
segment
x
FFFFFFC
and
is
accessed
in
last
in
first
out
LIFO
order
The
heap
stores
data
that
is
allocated
by
the
program
during
runtime
In
C
memory
allocations
are
made
by
the
malloc
function
in
C
and
Java
new
is
used
to
allocate
memory
Like
a
heap
of
clothes
on
a
dorm
room
floor
heap
data
can
be
used
and
discarded
in
any
order
The
heap
grows
upward
from
the
bottom
of
the
dynamic
data
segment
If
the
stack
and
heap
ever
grow
into
each
other
the
program
s
data
can
become
corrupted
The
memory
allocator
tries
to
ensure
that
this
never
happens
by
returning
an
out
of
memory
error
if
there
is
insufficient
space
to
allocate
more
dynamic
data
The
Reserved
Segments
The
reserved
segments
are
used
by
the
operating
system
and
cannot
directly
be
used
by
the
program
Part
of
the
reserved
memory
is
used
for
interrupts
see
Section
and
for
memory
mapped
I
O
see
Section
Translating
and
Starting
a
Program
Figure
shows
the
steps
required
to
translate
a
program
from
a
highlevel
language
into
machine
language
and
to
start
executing
that
program
First
the
high
level
code
is
compiled
into
assembly
code
The
assembly
code
is
assembled
into
machine
code
in
an
object
file
The
linker
combines
the
machine
code
with
object
code
from
libraries
and
other
files
to
produce
an
entire
executable
program
In
practice
most
compilers
perform
all
three
steps
of
compiling
assembling
and
linking
Finally
the
loader
loads
Lights
Camera
Action
Compiling
Assembling
and
Loading
Grace
Hopper
Graduated
from
Yale
University
with
a
Ph
D
in
mathematics
Developed
the
first
compiler
while
working
for
the
Remington
Rand
Corporation
and
was
instrumental
in
developing
the
COBOL
programming
language
As
a
naval
officer
she
received
many
awards
including
a
World
War
II
Victory
Medal
and
the
National
Defence
Service
Medal
Ch
the
program
into
memory
and
starts
execution
The
remainder
of
this
section
walks
through
these
steps
for
a
simple
program
Step
Compilation
A
compiler
translates
high
level
code
into
assembly
language
Code
Example
shows
a
simple
high
level
program
with
three
global
CHAPTER
SIX
Architecture
Assembly
Code
High
Level
Code
Compiler
Object
File
Assembler
Executable
Linker
Memory
Loader
Object
Files
Library
Files
Figure
Steps
for
translating
and
starting
a
program
High
Level
Code
int
f
g
y
global
variables
int
main
void
f
g
y
sum
f
g
return
y
int
sum
int
a
int
b
return
a
b
MIPS
Assembly
Code
data
f
g
y
text
main
addi
sp
sp
make
stack
frame
sw
ra
sp
store
ra
on
stack
addi
a
a
sw
a
f
f
addi
a
a
sw
a
g
g
jal
sum
call
sum
procedure
sw
v
y
y
sum
f
g
lw
ra
sp
restore
ra
from
stack
addi
sp
sp
restore
stack
pointer
jr
ra
return
to
operating
system
sum
add
v
a
a
v
a
b
jr
ra
return
to
caller
Code
Example
COMPILING
A
HIGH
LEVEL
PROGRAM
Chapter
variables
and
two
procedures
along
with
the
assembly
code
produced
by
a
typical
compiler
The
data
and
text
keywords
are
assembler
directives
that
indicate
where
the
text
and
data
segments
begin
Labels
are
used
for
global
variables
f
g
and
y
Their
storage
location
will
be
determined
by
the
assembler
for
now
they
are
left
as
symbols
in
the
code
Step
Assembling
The
assembler
turns
the
assembly
language
code
into
an
object
file
containing
machine
language
code
The
assembler
makes
two
passes
through
the
assembly
code
On
the
first
pass
the
assembler
assigns
instruction
addresses
and
finds
all
the
symbols
such
as
labels
and
global
variable
names
The
code
after
the
first
assembler
pass
is
shown
here
x
main
addi
sp
sp
x
sw
ra
sp
x
addi
a
x
C
sw
a
f
x
addi
a
x
sw
a
g
x
jal
sum
x
C
sw
v
y
x
lw
ra
sp
x
addi
sp
sp
x
jr
ra
x
C
sum
add
v
a
a
x
jr
ra
The
names
and
addresses
of
the
symbols
are
kept
in
a
symbol
table
as
shown
in
Table
for
this
code
The
symbol
addresses
are
filled
in
after
the
first
pass
when
the
addresses
of
labels
are
known
Global
variables
are
assigned
storage
locations
in
the
global
data
segment
of
memory
starting
at
memory
address
x
On
the
second
pass
through
the
code
the
assembler
produces
the
machine
language
code
Addresses
for
the
global
variables
and
labels
are
taken
from
the
symbol
table
The
machine
language
code
and
symbol
table
are
stored
in
the
object
file
Lights
Camera
Action
Compiling
Assembling
and
Loading
Table
Symbol
table
Symbol
Address
f
x
g
x
y
x
main
x
sum
x
C
Executable
file
header
Text
Size
Data
Size
Text
segment
Data
segment
Address
Address
x
x
x
x
C
x
x
x
x
C
x
x
x
x
C
x
addi
sp
sp
sw
ra
sp
addi
a
sw
a
x
gp
addi
a
sw
a
x
gp
jal
x
C
sw
v
x
gp
lw
ra
sp
addi
sp
sp
jr
ra
add
v
a
a
jr
ra
x
x
x
f
g
y
x
bytes
xC
bytes
x
BDFFFC
xAFBF
x
xAF
x
xAF
x
C
B
xAF
x
FBF
x
BD
x
E
x
x
E
Instruction
Data
CHAPTER
SIX
Architecture
Step
Linking
Most
large
programs
contain
more
than
one
file
If
the
programmer
changes
only
one
of
the
files
it
would
be
wasteful
to
recompile
and
reassemble
the
other
files
In
particular
programs
often
call
procedures
in
library
files
these
library
files
almost
never
change
If
a
file
of
highlevel
code
is
not
changed
the
associated
object
file
need
not
be
updated
The
job
of
the
linker
is
to
combine
all
of
the
object
files
into
one
machine
language
file
called
the
executable
The
linker
relocates
the
data
and
instructions
in
the
object
files
so
that
they
are
not
all
on
top
of
each
other
It
uses
the
information
in
the
symbol
tables
to
adjust
the
addresses
of
global
variables
and
of
labels
that
are
relocated
In
our
example
there
is
only
one
object
file
so
no
relocation
is
necessary
Figure
shows
the
executable
file
It
has
three
sections
the
executable
file
header
the
text
segment
and
the
data
segment
The
executable
file
header
reports
the
text
size
code
size
and
data
size
amount
of
globally
declared
data
Both
are
given
in
units
of
bytes
The
text
segment
gives
the
instructions
and
the
addresses
where
they
are
to
be
stored
The
figure
shows
the
instructions
in
human
readable
format
next
to
the
machine
code
for
ease
of
interpretation
but
the
executable
file
includes
only
machine
instructions
The
data
segment
gives
the
address
of
each
global
variable
The
global
variables
are
addressed
with
respect
to
the
base
address
given
by
the
global
pointer
gp
For
example
the
Figure
Executable
y
g
f
x
E
x
x
E
x
BD
x
FBF
xAF
x
C
B
xAF
x
xAF
x
xAFBF
x
BDFFFC
Address
Memory
x
FFFFFFC
sp
x
FFFFFFC
x
x
Stack
Heap
gp
x
PC
x
x
Reserved
Reserved
first
store
instruction
sw
a
x
gp
stores
the
value
to
the
global
variable
f
which
is
located
at
memory
address
x
Remember
that
the
offset
x
is
a
bit
signed
number
that
is
sign
extended
and
added
to
the
base
address
gp
So
gp
x
x
xFFFF
x
the
memory
address
of
variable
f
Step
Loading
The
operating
system
loads
a
program
by
reading
the
text
segment
of
the
executable
file
from
a
storage
device
usually
the
hard
disk
into
the
text
segment
of
memory
The
operating
system
sets
gp
to
x
the
middle
of
the
global
data
segment
and
sp
to
x
FFFFFFC
the
top
of
the
dynamic
data
segment
then
performs
a
jal
x
to
jump
to
the
beginning
of
the
program
Figure
shows
the
memory
map
at
the
beginning
of
program
execution
Lights
Camera
Action
Compiling
Assembling
and
Loading
Figure
Executable
loaded
in
memory
Chap
ODDS
AND
ENDS
This
section
covers
a
few
optional
topics
that
do
not
fit
naturally
elsewhere
in
the
chapter
These
topics
include
pseudoinstructions
exceptions
signed
and
unsigned
arithmetic
instructions
and
floating
point
instructions
Pseudoinstructions
If
an
instruction
is
not
available
in
the
MIPS
instruction
set
it
is
probably
because
the
same
operation
can
be
performed
using
one
or
more
existing
MIPS
instructions
Remember
that
MIPS
is
a
reduced
instruction
set
computer
RISC
so
the
instruction
size
and
hardware
complexity
are
minimized
by
keeping
the
number
of
instructions
small
However
MIPS
defines
pseudoinstructions
that
are
not
actually
part
of
the
instruction
set
but
are
commonly
used
by
programmers
and
compilers
When
converted
to
machine
code
pseudoinstructions
are
translated
into
one
or
more
MIPS
instructions
Table
gives
examples
of
pseudoinstructions
and
the
MIPS
instructions
used
to
implement
them
For
example
the
load
immediate
pseudoinstruction
li
loads
a
bit
constant
using
a
combination
of
lui
and
ori
instructions
The
multiply
pseudoinstruction
mul
provides
a
three
operand
multiply
multiplying
two
registers
and
putting
the
least
significant
bits
of
the
result
into
a
third
register
The
no
operation
pseudoinstruction
nop
pronounced
no
op
performs
no
operation
The
PC
is
incremented
by
upon
its
execution
No
other
registers
or
memory
values
are
altered
The
machine
code
for
the
nop
instruction
is
x
Some
pseudoinstructions
require
a
temporary
register
for
intermediate
calculations
For
example
the
pseudoinstruction
beq
t
imm
Loop
compares
t
to
a
bit
immediate
imm
This
pseudoinstruction
CHAPTER
SIX
Architecture
Table
Pseudoinstructions
Corresponding
Pseudoinstruction
MIPS
Instructions
li
s
x
AA
lui
s
x
ori
s
xAA
mul
s
s
s
mult
s
s
mflo
s
clear
t
add
t
move
s
s
add
s
s
nop
sll
requires
a
temporary
register
in
which
to
store
the
bit
immediate
Assemblers
use
the
assembler
register
at
for
such
purposes
Table
shows
how
the
assembler
uses
at
in
converting
a
pseudoinstruction
to
real
MIPS
instructions
We
leave
it
as
Exercise
to
implement
other
pseudoinstructions
such
as
rotate
left
rol
and
rotate
right
ror
Exceptions
An
exception
is
like
an
unscheduled
procedure
call
that
jumps
to
a
new
address
Exceptions
may
be
caused
by
hardware
or
software
For
example
the
processor
may
receive
notification
that
the
user
pressed
a
key
on
a
keyboard
The
processor
may
stop
what
it
is
doing
determine
which
key
was
pressed
save
it
for
future
reference
then
resume
the
program
that
was
running
Such
a
hardware
exception
triggered
by
an
input
output
I
O
device
such
as
a
keyboard
is
often
called
an
interrupt
Alternatively
the
program
may
encounter
an
error
condition
such
as
an
undefined
instruction
The
program
then
jumps
to
code
in
the
operating
system
OS
which
may
choose
to
terminate
the
offending
program
Software
exceptions
are
sometimes
called
traps
Other
causes
of
exceptions
include
division
by
zero
attempts
to
read
nonexistent
memory
hardware
malfunctions
debugger
breakpoints
and
arithmetic
overflow
see
Section
The
processor
records
the
cause
of
an
exception
and
the
value
of
the
PC
at
the
time
the
exception
occurs
It
then
jumps
to
the
exception
handler
procedure
The
exception
handler
is
code
usually
in
the
OS
that
examines
the
cause
of
the
exception
and
responds
appropriately
by
reading
the
keyboard
on
a
hardware
interrupt
for
example
It
then
returns
to
the
program
that
was
executing
before
the
exception
took
place
In
MIPS
the
exception
handler
is
always
located
at
x
When
an
exception
occurs
the
processor
always
jumps
to
this
instruction
address
regardless
of
the
cause
The
MIPS
architecture
uses
a
special
purpose
register
called
the
Cause
register
to
record
the
cause
of
the
exception
Different
codes
are
used
to
record
different
exception
causes
as
given
in
Table
The
exception
handler
code
reads
the
Cause
register
to
determine
how
to
handle
the
exception
Some
other
architectures
jump
to
a
different
exception
handler
for
each
different
cause
instead
of
using
a
Cause
register
MIPS
uses
another
special
purpose
register
called
the
Exception
Program
Counter
EPC
to
store
the
value
of
the
PC
at
the
time
an
exception
Odds
and
Ends
Table
Pseudoinstruction
using
at
Corresponding
Pseudoinstruction
MIPS
Instructions
beq
t
imm
Loop
addi
at
imm
beq
t
at
Loop
takes
place
The
processor
returns
to
the
address
in
EPC
after
handling
the
exception
This
is
analogous
to
using
ra
to
store
the
old
value
of
the
PC
during
a
jal
instruction
The
EPC
and
Cause
registers
are
not
part
of
the
MIPS
register
file
The
mfc
move
from
coprocessor
instruction
copies
these
and
other
special
purpose
registers
into
one
of
the
general
purpose
registers
Coprocessor
is
called
the
MIPS
processor
control
it
handles
interrupts
and
processor
diagnostics
For
example
mfc
t
Cause
copies
the
Cause
register
into
t
The
syscall
and
break
instructions
cause
traps
to
perform
system
calls
or
debugger
breakpoints
The
exception
handler
uses
the
EPC
to
look
up
the
instruction
and
determine
the
nature
of
the
system
call
or
breakpoint
by
looking
at
the
fields
of
the
instruction
In
summary
an
exception
causes
the
processor
to
jump
to
the
exception
handler
The
exception
handler
saves
registers
on
the
stack
then
uses
mfc
to
look
at
the
cause
and
respond
accordingly
When
the
handler
is
finished
it
restores
the
registers
from
the
stack
copies
the
return
address
from
EPC
to
k
using
mfc
and
returns
using
jr
k
Signed
and
Unsigned
Instructions
Recall
that
a
binary
number
may
be
signed
or
unsigned
The
MIPS
architecture
uses
two
s
complement
representation
of
signed
numbers
MIPS
has
certain
instructions
that
come
in
signed
and
unsigned
flavors
including
addition
and
subtraction
multiplication
and
division
set
less
than
and
partial
word
loads
Addition
and
Subtraction
Addition
and
subtraction
are
performed
identically
whether
the
number
is
signed
or
unsigned
However
the
interpretation
of
the
results
is
different
As
mentioned
in
Section
if
two
large
signed
numbers
are
added
together
the
result
may
incorrectly
produce
the
opposite
sign
For
example
adding
the
following
two
huge
positive
numbers
gives
a
negative
CHAPTER
SIX
Architecture
Table
Exception
cause
codes
Exception
Cause
hardware
interrupt
x
system
call
x
breakpoint
divide
by
x
undefined
instruction
x
arithmetic
overflow
x
k
and
k
are
included
in
the
MIPS
register
set
They
are
reserved
by
the
OS
for
exception
handling
They
do
not
need
to
be
saved
and
restored
during
exceptions
result
x
FFFFFFF
x
FFFFFFF
xFFFFFFFE
Similarly
adding
two
huge
negative
numbers
gives
a
positive
result
x
x
x
This
is
called
arithmetic
overflow
The
C
language
ignores
arithmetic
overflows
but
other
languages
such
as
Fortran
require
that
the
program
be
notified
As
mentioned
in
Section
the
MIPS
processor
takes
an
exception
on
arithmetic
overflow
The
program
can
decide
what
to
do
about
the
overflow
for
example
it
might
repeat
the
calculation
with
greater
precision
to
avoid
the
overflow
then
return
to
where
it
left
off
MIPS
provides
signed
and
unsigned
versions
of
addition
and
subtraction
The
signed
versions
are
add
addi
and
sub
The
unsigned
versions
are
addu
addiu
and
subu
The
two
versions
are
identical
except
that
signed
versions
trigger
an
exception
on
overflow
whereas
unsigned
versions
do
not
Because
C
ignores
exceptions
C
programs
technically
use
the
unsigned
versions
of
these
instructions
Multiplication
and
Division
Multiplication
and
division
behave
differently
for
signed
and
unsigned
numbers
For
example
as
an
unsigned
number
xFFFFFFFF
represents
a
large
number
but
as
a
signed
number
it
represents
Hence
xFFFFFFFF
xFFFFFFFF
would
equal
xFFFFFFFE
if
the
numbers
were
unsigned
but
x
if
the
numbers
were
signed
Therefore
multiplication
and
division
come
in
both
signed
and
unsigned
flavors
mult
and
div
treat
the
operands
as
signed
numbers
multu
and
divu
treat
the
operands
as
unsigned
numbers
Set
Less
Than
Set
less
than
instructions
can
compare
either
two
registers
slt
or
a
register
and
an
immediate
slti
Set
less
than
also
comes
in
signed
slt
and
slti
and
unsigned
sltu
and
sltiu
versions
In
a
signed
comparison
x
is
less
than
any
other
number
because
it
is
the
most
negative
two
s
complement
number
In
an
unsigned
comparison
x
is
greater
than
x
FFFFFFF
but
less
than
x
because
all
numbers
are
positive
Beware
that
sltiu
sign
extends
the
immediate
before
treating
it
as
an
unsigned
number
For
example
sltiu
s
s
x
compares
s
to
xFFFF
treating
the
immediate
as
a
large
positive
number
Loads
As
described
in
Section
byte
loads
come
in
signed
lb
and
unsigned
lbu
versions
lb
sign
extends
the
byte
and
lbu
zero
extends
the
byte
to
fill
the
entire
bit
register
Similarly
MIPS
provides
signed
and
unsigned
half
word
loads
lh
and
lhu
which
load
two
bytes
into
the
lower
half
and
sign
or
zero
extend
the
upper
half
of
the
word
Odds
and
Ends
Chapt
cop
ft
fs
fd
funct
bits
bits
bits
bits
bits
bits
F
type
op
Floating
Point
Instructions
The
MIPS
architecture
defines
an
optional
floating
point
coprocessor
known
as
coprocessor
In
early
MIPS
implementations
the
floatingpoint
coprocessor
was
a
separate
chip
that
users
could
purchase
if
they
needed
fast
floating
point
math
In
most
recent
MIPS
implementations
the
floating
point
coprocessor
is
built
in
alongside
the
main
processor
MIPS
defines
bit
floating
point
registers
f
f
These
are
separate
from
the
ordinary
registers
used
so
far
MIPS
supports
both
single
and
double
precision
IEEE
floating
point
arithmetic
Doubleprecision
bit
numbers
are
stored
in
pairs
of
bit
registers
so
only
the
even
numbered
registers
f
f
f
f
are
used
to
specify
double
precision
operations
By
convention
certain
registers
are
reserved
for
certain
purposes
as
given
in
Table
Floating
point
instructions
all
have
an
opcode
of
They
require
both
a
funct
field
and
a
cop
coprocessor
field
to
indicate
the
type
of
instruction
Hence
MIPS
defines
the
F
type
instruction
format
for
floating
point
instructions
shown
in
Figure
Floating
point
instructions
come
in
both
single
and
double
precision
flavors
cop
for
single
precision
instructions
or
for
doubleprecision
instructions
Like
R
type
instructions
F
type
instructions
have
two
source
operands
fs
and
ft
and
one
destination
fd
Instruction
precision
is
indicated
by
s
and
d
in
the
mnemonic
Floating
point
arithmetic
instructions
include
addition
add
s
add
d
subtraction
sub
sz
sub
d
multiplication
mul
s
mul
d
and
division
div
s
div
d
as
well
as
negation
neg
s
neg
d
and
absolute
value
abs
s
abs
d
CHAPTER
SIX
Architecture
Table
MIPS
floating
point
register
set
Name
Number
Use
fv
fv
procedure
return
values
ft
ft
temporary
variables
fa
fa
procedure
arguments
ft
ft
temporary
variables
fs
fs
saved
variables
Figure
F
type
machine
instruction
format
C
Floating
point
branches
have
two
parts
First
a
compare
instruction
is
used
to
set
or
clear
the
floating
point
condition
flag
fpcond
Then
a
conditional
branch
checks
the
value
of
the
flag
The
compare
instructions
include
equality
c
seq
s
c
seq
d
less
than
c
lt
s
c
lt
d
and
less
than
or
equal
to
c
le
s
c
le
d
The
conditional
branch
instructions
are
bc
f
and
bc
t
that
branch
if
fpcond
is
FALSE
or
TRUE
respectively
Inequality
greater
than
or
equal
to
and
greater
than
comparisons
are
performed
with
seq
lt
and
le
followed
by
bc
f
Floating
point
registers
are
loaded
and
stored
from
memory
using
lwc
and
swc
These
instructions
move
bits
so
two
are
necessary
to
handle
a
double
precision
number
REAL
WORLD
PERSPECTIVE
IA
ARCHITECTURE
Almost
all
personal
computers
today
use
IA
architecture
microprocessors
IA
is
a
bit
architecture
originally
developed
by
Intel
AMD
also
sells
IA
compatible
microprocessors
The
IA
architecture
has
a
long
and
convoluted
history
dating
back
to
when
Intel
announced
the
bit
microprocessor
IBM
selected
the
and
its
cousin
the
for
IBM
s
first
personal
computers
In
Intel
introduced
the
bit
microprocessor
which
was
backward
compatible
with
the
so
it
could
run
software
developed
for
earlier
PCs
Processor
architectures
compatible
with
the
are
called
IA
or
x
processors
The
Pentium
Core
and
Athlon
processors
are
well
known
IA
processors
Section
describes
the
evolution
of
IA
microprocessors
in
more
detail
Various
groups
at
Intel
and
AMD
over
many
years
have
shoehorned
more
instructions
and
capabilities
into
the
antiquated
architecture
The
result
is
far
less
elegant
than
MIPS
As
Patterson
and
Hennessy
explain
this
checkered
ancestry
has
led
to
an
architecture
that
is
difficult
to
explain
and
impossible
to
love
However
software
compatibility
is
far
more
important
than
technical
elegance
so
IA
has
been
the
de
facto
PC
standard
for
more
than
two
decades
More
than
million
IA
processors
are
sold
every
year
This
huge
market
justifies
more
than
billion
of
research
and
development
annually
to
continue
improving
the
processors
IA
is
an
example
of
a
Complex
Instruction
Set
Computer
CISC
architecture
In
contrast
to
RISC
architectures
such
as
MIPS
each
CISC
instruction
can
do
more
work
Programs
for
CISC
architectures
usually
require
fewer
instructions
The
instruction
encodings
were
selected
to
be
more
compact
so
as
to
save
memory
when
RAM
was
far
more
expensive
than
it
is
today
instructions
are
of
variable
length
and
are
often
less
than
bits
The
trade
off
is
that
complicated
instructions
are
more
difficult
to
decode
and
tend
to
execute
more
slowly
Real
World
Perspective
IA
Architecture
This
section
introduces
the
IA
architecture
The
goal
is
not
to
make
you
into
an
IA
assembly
language
programmer
but
rather
to
illustrate
some
of
the
similarities
and
differences
between
IA
and
MIPS
We
think
it
is
interesting
to
see
how
IA
works
However
none
of
the
material
in
this
section
is
needed
to
understand
the
rest
of
the
book
Major
differences
between
IA
and
MIPS
are
summarized
in
Table
IA
Registers
The
microprocessor
provided
eight
bit
registers
It
could
separately
access
the
upper
and
lower
eight
bits
of
some
of
these
registers
When
the
bit
was
introduced
the
registers
were
extended
to
bits
These
registers
are
called
EAX
ECX
EDX
EBX
ESP
EBP
ESI
and
EDI
For
backward
compatibility
the
bottom
bits
and
some
of
the
bottom
bit
portions
are
also
usable
as
shown
in
Figure
The
eight
registers
are
almost
but
not
quite
general
purpose
Certain
instructions
cannot
use
certain
registers
Other
instructions
always
put
their
results
in
certain
registers
Like
sp
in
MIPS
ESP
is
normally
reserved
for
the
stack
pointer
The
IA
program
counter
is
called
EIP
the
extended
instruction
pointer
Like
the
MIPS
PC
it
advances
from
one
instruction
to
the
next
or
can
be
changed
with
branch
jump
and
subroutine
call
instructions
IA
Operands
MIPS
instructions
always
act
on
registers
or
immediates
Explicit
load
and
store
instructions
are
needed
to
move
data
between
memory
and
the
registers
In
contrast
IA
instructions
may
operate
on
registers
immediates
or
memory
This
partially
compensates
for
the
small
set
of
registers
CHAPTER
SIX
Architecture
Table
Major
differences
between
MIPS
and
IA
Feature
MIPS
IA
of
registers
general
purpose
some
restrictions
on
purpose
of
operands
source
destination
source
source
destination
operand
location
registers
or
immediates
registers
immediates
or
memory
operand
size
bits
or
bits
condition
codes
no
yes
instruction
types
simple
simple
and
complicated
instruction
encoding
fixed
bytes
variable
bytes
Figure
IA
registers
EAX
AH
AX
ECX
CH
CX
EDX
Byte
Byte
Byte
Byte
DH
DX
EBX
BH
BX
ESP
SP
EBP
BP
ESI
SI
EDI
DI
AL
CL
DL
BL
MIPS
instructions
generally
specify
three
operands
two
sources
and
one
destination
IA
instructions
specify
only
two
operands
The
first
is
a
source
The
second
is
both
a
source
and
the
destination
Hence
IA
instructions
always
overwrite
one
of
their
sources
with
the
result
Table
lists
the
combinations
of
operand
locations
in
IA
All
of
the
combinations
are
possible
except
memory
to
memory
Like
MIPS
IA
has
a
bit
memory
space
that
is
byte
addressable
However
IA
also
supports
a
much
wider
variety
of
memory
addressing
modes
The
memory
location
is
specified
with
any
combination
of
a
base
register
displacement
and
a
scaled
index
register
Table
illustrates
these
combinations
The
displacement
can
be
an
or
bit
value
The
scale
multiplying
the
index
register
can
be
or
The
base
displacement
mode
is
equivalent
to
the
MIPS
base
addressing
mode
for
loads
and
stores
The
scaled
index
provides
an
easy
way
to
access
arrays
or
structures
of
or
byte
elements
without
having
to
issue
a
sequence
of
instructions
to
generate
the
address
Real
World
Perspective
IA
Architecture
Table
Operand
locations
Source
Destination
Source
Example
Meaning
register
register
add
EAX
EBX
EAX
EAX
EBX
register
immediate
add
EAX
EAX
EAX
register
memory
add
EAX
EAX
EAX
Mem
memory
register
add
EAX
Mem
Mem
EAX
memory
immediate
add
Mem
Mem
Table
Memory
addressing
modes
Example
Meaning
Comment
add
EAX
EAX
EAX
Mem
displacement
add
EAX
ESP
EAX
EAX
Mem
ESP
base
addressing
add
EAX
EDX
EAX
EAX
Mem
EDX
base
displacement
add
EAX
EDI
EAX
EAX
Mem
EDI
displacement
scaled
index
add
EAX
EDX
EDI
EAX
EAX
Mem
EDX
EDI
base
displacement
scaled
index
Chapter
qxd
While
MIPS
always
acts
on
bit
words
IA
instructions
can
operate
on
or
bit
data
Table
illustrates
these
variations
Status
Flags
IA
like
many
CISC
architectures
uses
status
flags
also
called
condition
codes
to
make
decisions
about
branches
and
to
keep
track
of
carries
and
arithmetic
overflow
IA
uses
a
bit
register
called
EFLAGS
that
stores
the
status
flags
Some
of
the
bits
of
the
EFLAGS
register
are
given
in
Table
Other
bits
are
used
by
the
operating
system
The
architectural
state
of
an
IA
processor
includes
EFLAGS
as
well
as
the
eight
registers
and
the
EIP
IA
Instructions
IA
has
a
larger
set
of
instructions
than
MIPS
Table
describes
some
of
the
general
purpose
instructions
IA
also
has
instructions
for
floatingpoint
arithmetic
and
for
arithmetic
on
multiple
short
data
elements
packed
into
a
longer
word
D
indicates
the
destination
a
register
or
memory
location
and
S
indicates
the
source
a
register
memory
location
or
immediate
Note
that
some
instructions
always
act
on
specific
registers
For
example
bit
multiplication
always
takes
one
of
the
sources
from
EAX
and
always
puts
the
bit
result
in
EDX
and
EAX
LOOP
always
CHAPTER
SIX
Architecture
Table
Instructions
acting
on
or
bit
data
Example
Meaning
Data
Size
add
AH
BL
AH
AH
BL
bit
add
AX
AX
AX
xFFFF
bit
add
EAX
EDX
EAX
EAX
EDX
bit
Table
Selected
EFLAGS
Name
Meaning
CF
Carry
Flag
Carry
out
generated
by
last
arithmetic
operation
Indicates
overflow
in
unsigned
arithmetic
Also
used
for
propagating
the
carry
between
words
in
multiple
precision
arithmetic
ZF
Zero
Flag
Result
of
last
operation
was
zero
SF
Sign
Flag
Result
of
last
operation
was
negative
msb
OF
Overflow
Flag
Overflow
of
two
s
complement
arithmetic
Chap
Real
World
Perspective
IA
Architecture
Table
Selected
IA
instructions
Instruction
Meaning
Function
ADD
SUB
add
subtract
D
D
S
D
D
S
ADDC
add
with
carry
D
D
S
CF
INC
DEC
increment
decrement
D
D
D
D
CMP
compare
Set
flags
based
on
D
S
NEG
negate
D
D
AND
OR
XOR
logical
AND
OR
XOR
D
D
op
S
NOT
logical
NOT
D
D
IMUL
MUL
signed
unsigned
multiply
EDX
EAX
EAX
D
IDIV
DIV
signed
unsigned
divide
EDX
EAX
D
EAX
Quotient
EDX
Remainder
SAR
SHR
arithmetic
logical
shift
right
D
D
S
D
D
S
SAL
SHL
left
shift
D
D
S
ROR
ROL
rotate
right
left
Rotate
D
by
S
RCR
RCL
rotate
right
left
with
carry
Rotate
CF
and
D
by
S
BT
bit
test
CF
D
S
the
Sth
bit
of
D
BTR
BTS
bit
test
and
reset
set
CF
D
S
D
S
TEST
set
flags
based
on
masked
bits
Set
flags
based
on
D
AND
S
MOV
move
D
S
PUSH
push
onto
stack
ESP
ESP
Mem
ESP
S
POP
pop
off
stack
D
MEM
ESP
ESP
ESP
CLC
STC
clear
set
carry
flag
CF
JMP
unconditional
jump
relative
jump
EIP
EIP
S
absolute
jump
EIP
S
Jcc
conditional
jump
if
flag
EIP
EIP
S
LOOP
loop
ECX
ECX
if
ECX
EIP
EIP
imm
CALL
procedure
call
ESP
ESP
MEM
ESP
EIP
EIP
S
RET
procedure
return
EIP
MEM
ESP
ESP
ESP
Chapter
qxd
PM
Page
CHAPTER
SIX
Architecture
It
is
possible
to
construct
byte
instructions
if
all
the
optional
fields
are
used
However
IA
places
a
byte
limit
on
the
length
of
legal
instructions
Table
Selected
branch
conditions
Instruction
Meaning
Function
After
cmp
d
s
JZ
JE
jump
if
ZF
jump
if
D
S
JNZ
JNE
jump
if
ZF
jump
if
D
S
JGE
jump
if
SF
OF
jump
if
D
S
JG
jump
if
SF
OF
and
ZF
jump
if
D
S
JLE
jump
if
SF
OF
or
ZF
jump
if
D
S
JL
jump
if
SF
OF
jump
if
D
S
JC
JB
jump
if
CF
JNC
jump
if
CF
JO
jump
if
OF
JNO
jump
if
OF
JS
jump
if
SF
JNS
jump
if
SF
stores
the
loop
counter
in
ECX
PUSH
POP
CALL
and
RET
use
the
stack
pointer
ESP
Conditional
jumps
check
the
flags
and
branch
if
the
appropriate
condition
is
met
They
come
in
many
flavors
For
example
JZ
jumps
if
the
zero
flag
ZF
is
JNZ
jumps
if
the
zero
flag
is
The
jumps
usually
follow
an
instruction
such
as
the
compare
instruction
CMP
that
sets
the
flags
Table
lists
some
of
the
conditional
jumps
and
how
they
depend
on
the
flags
set
by
a
prior
compare
operation
IA
Instruction
Encoding
The
IA
instruction
encodings
are
truly
messy
a
legacy
of
decades
of
piecemeal
changes
Unlike
MIPS
whose
instructions
are
uniformly
bits
IA
instructions
vary
from
to
bytes
as
shown
in
Figure
The
opcode
may
be
or
bytes
It
is
followed
by
four
optional
fields
ModR
M
SIB
Displacement
and
Immediate
ModR
M
specifies
an
addressing
mode
SIB
specifies
the
scale
index
and
base
Chapter
qx
Real
World
Perspective
IA
Architecture
registers
in
certain
addressing
modes
Displacement
indicates
a
or
byte
displacement
in
certain
addressing
modes
And
Immediate
is
a
or
byte
constant
for
instructions
using
an
immediate
as
the
source
operand
Moreover
an
instruction
can
be
preceded
by
up
to
four
optional
byte
long
prefixes
that
modify
its
behavior
The
ModR
M
byte
uses
the
bit
Mod
and
bit
R
M
field
to
specify
the
addressing
mode
for
one
of
the
operands
The
operand
can
come
from
one
of
the
eight
registers
or
from
one
of
memory
addressing
modes
Due
to
artifacts
in
the
encodings
the
ESP
and
EBP
registers
are
not
available
for
use
as
the
base
or
index
register
in
certain
addressing
modes
The
Reg
field
specifies
the
register
used
as
the
other
operand
For
certain
instructions
that
do
not
require
a
second
operand
the
Reg
field
is
used
to
specify
three
more
bits
of
the
opcode
In
addressing
modes
using
a
scaled
index
register
the
SIB
byte
specifies
the
index
register
and
the
scale
or
If
both
a
base
and
index
are
used
the
SIB
byte
also
specifies
the
base
register
MIPS
fully
specifies
the
instruction
in
the
opcode
and
funct
fields
of
the
instruction
IA
uses
a
variable
number
of
bits
to
specify
different
instructions
It
uses
fewer
bits
to
specify
more
common
instructions
decreasing
the
average
length
of
the
instructions
Some
instructions
even
have
multiple
opcodes
For
example
add
AL
imm
performs
an
bit
add
of
an
immediate
to
AL
It
is
represented
with
the
byte
opcode
x
followed
by
a
byte
immediate
The
A
register
AL
AX
or
EAX
is
called
the
accumulator
On
the
other
hand
add
D
imm
performs
an
bit
add
of
an
immediate
to
an
arbitrary
destination
D
memory
or
a
register
It
is
represented
with
the
byte
opcode
x
followed
by
one
or
more
bytes
specifying
D
followed
by
a
byte
immediate
Many
instructions
have
shortened
encodings
when
the
destination
is
the
accumulator
In
the
original
the
opcode
specified
whether
the
instruction
acted
on
or
bit
operands
When
the
introduced
bit
operands
no
new
opcodes
were
available
to
specify
the
bit
form
Prefixes
ModR
M
SIB
Displacement
Immediate
Up
to
optional
prefixes
of
byte
each
or
byte
opcode
byte
for
certain
addressing
modes
byte
for
certain
addressing
modes
or
bytes
for
addressing
modes
with
displacement
or
bytes
for
addressing
modes
with
immediate
Mod
R
M
Scale
Index
Base
Reg
Opcode
Opcode
bits
bits
bits
bits
bits
bits
Figure
IA
instruction
encodings
Instead
the
same
opcode
was
used
for
both
and
bit
forms
An
additional
bit
in
the
code
segment
descriptor
used
by
the
OS
specifies
which
form
the
processor
should
choose
The
bit
is
set
to
for
backward
compatibility
with
programs
defaulting
the
opcode
to
bit
operands
It
is
set
to
for
programs
to
default
to
bit
operands
Moreover
the
programmer
can
specify
prefixes
to
change
the
form
for
a
particular
instruction
If
the
prefix
x
appears
before
the
opcode
the
alternative
size
operand
is
used
bits
in
bit
mode
or
bits
in
bit
mode
Other
IA
Peculiarities
The
introduced
segmentation
to
divide
memory
into
segments
of
up
to
KB
in
length
When
the
OS
enables
segmentation
addresses
are
computed
relative
to
the
beginning
of
the
segment
The
processor
checks
for
addresses
that
go
beyond
the
end
of
the
segment
and
indicates
an
error
thus
preventing
programs
from
accessing
memory
outside
their
own
segment
Segmentation
proved
to
be
a
hassle
for
programmers
and
is
not
used
in
modern
versions
of
the
Windows
operating
system
IA
contains
string
instructions
that
act
on
entire
strings
of
bytes
or
words
The
operations
include
moving
comparing
or
scanning
for
a
specific
value
In
modern
processors
these
instructions
are
usually
slower
than
performing
the
equivalent
operation
with
a
series
of
simpler
instructions
so
they
are
best
avoided
As
mentioned
earlier
the
x
prefix
is
used
to
choose
between
and
bit
operand
sizes
Other
prefixes
include
ones
used
to
lock
the
bus
to
control
access
to
shared
variables
in
a
multiprocessor
system
to
predict
whether
a
branch
will
be
taken
or
not
and
to
repeat
the
instruction
during
a
string
move
The
bane
of
any
architecture
is
to
run
out
of
memory
capacity
With
bit
addresses
IA
can
access
GB
of
memory
This
was
far
more
than
the
largest
computers
had
in
but
by
the
early
s
it
had
become
limiting
In
AMD
extended
the
address
space
and
register
sizes
to
bits
calling
the
enhanced
architecture
AMD
AMD
has
a
compatibility
mode
that
allows
it
to
run
bit
programs
unmodified
while
the
OS
takes
advantage
of
the
bigger
address
space
In
Intel
gave
in
and
adopted
the
bit
extensions
renaming
them
Extended
Memory
Technology
EM
T
With
bit
addresses
computers
can
access
exabytes
billion
GB
of
memory
For
those
curious
about
more
details
of
the
IA
architecture
the
IA
Intel
Architecture
Software
Developer
s
Manual
is
freely
available
on
Intel
s
Web
site
CHAPTER
SIX
Architecture
Intel
and
Hewlett
Packard
jointly
developed
a
new
bit
architecture
called
IA
in
the
mid
s
It
was
designed
from
a
clean
slate
bypassing
the
convoluted
history
of
IA
taking
advantage
of
years
of
new
research
in
computer
architecture
and
providing
a
bit
address
space
However
IA
has
yet
to
become
a
market
success
Most
computers
needing
the
large
address
space
now
use
the
bit
extensions
of
IA
The
Big
Picture
This
section
has
given
a
taste
of
some
of
the
differences
between
the
MIPS
RISC
architecture
and
the
IA
CISC
architecture
IA
tends
to
have
shorter
programs
because
a
complex
instruction
is
equivalent
to
a
series
of
simple
MIPS
instructions
and
because
the
instructions
are
encoded
to
minimize
memory
use
However
the
IA
architecture
is
a
hodgepodge
of
features
accumulated
over
the
years
some
of
which
are
no
longer
useful
but
must
be
kept
for
compatibility
with
old
programs
It
has
too
few
registers
and
the
instructions
are
difficult
to
decode
Merely
explaining
the
instruction
set
is
difficult
Despite
all
these
failings
IA
is
firmly
entrenched
as
the
dominant
computer
architecture
for
PCs
because
the
value
of
software
compatibility
is
so
great
and
because
the
huge
market
justifies
the
effort
required
to
build
fast
IA
microprocessors
SUMMARY
To
command
a
computer
you
must
speak
its
language
A
computer
architecture
defines
how
to
command
a
processor
Many
different
computer
architectures
are
in
widespread
commercial
use
today
but
once
you
understand
one
learning
others
is
much
easier
The
key
questions
to
ask
when
approaching
a
new
architecture
are
What
is
the
data
word
length
What
are
the
registers
How
is
memory
organized
What
are
the
instructions
MIPS
is
a
bit
architecture
because
it
operates
on
bit
data
The
MIPS
architecture
has
general
purpose
registers
In
principle
almost
any
register
can
be
used
for
any
purpose
However
by
convention
certain
registers
are
reserved
for
certain
purposes
for
ease
of
programming
and
so
that
procedures
written
by
different
programmers
can
communicate
easily
For
example
register
always
holds
the
constant
ra
holds
the
return
address
after
a
jal
instruction
and
a
a
and
v
v
hold
the
arguments
and
return
value
of
a
procedure
MIPS
has
a
byteaddressable
memory
system
with
bit
addresses
The
memory
map
was
described
in
Section
Instructions
are
bits
long
and
must
be
word
aligned
This
chapter
discussed
the
most
commonly
used
MIPS
instructions
The
power
of
defining
a
computer
architecture
is
that
a
program
written
for
any
given
architecture
can
run
on
many
different
implementations
of
that
architecture
For
example
programs
written
for
the
Intel
Summary
Chap
Pentium
processor
in
will
generally
still
run
and
run
much
faster
on
the
Intel
Core
Duo
or
AMD
Athlon
processors
in
In
the
first
part
of
this
book
we
learned
about
the
circuit
and
logic
levels
of
abstraction
In
this
chapter
we
jumped
up
to
the
architecture
level
In
the
next
chapter
we
study
microarchitecture
the
arrangement
of
digital
building
blocks
that
implement
a
processor
architecture
Microarchitecture
is
the
link
between
hardware
and
software
engineering
And
we
believe
it
is
one
of
the
most
exciting
topics
in
all
of
engineering
you
will
learn
to
build
your
own
microprocessor
CHAPTER
SIX
Architecture
Exercises
Exercise
Give
three
examples
from
the
MIPS
architecture
of
each
of
the
architecture
design
principles
simplicity
favors
regularity
make
the
common
case
fast
smaller
is
faster
and
good
design
demands
good
compromises
Explain
how
each
of
your
examples
exhibits
the
design
principle
Exercise
The
MIPS
architecture
has
a
register
set
that
consists
of
bit
registers
Is
it
possible
to
design
a
computer
architecture
without
a
register
set
If
so
briefly
describe
the
architecture
including
the
instruction
set
What
are
advantages
and
disadvantages
of
this
architecture
over
the
MIPS
architecture
Exercise
Consider
memory
storage
of
a
bit
word
stored
at
memory
word
in
a
byte
addressable
memory
a
What
is
the
byte
address
of
memory
word
b
What
are
the
byte
addresses
that
memory
word
spans
c
Draw
the
number
xFF
stored
at
word
in
both
big
endian
and
little
endian
machines
Your
drawing
should
be
similar
to
Figure
Clearly
label
the
byte
address
corresponding
to
each
data
byte
value
Exercise
Explain
how
the
following
program
can
be
used
to
determine
whether
a
computer
is
big
endian
or
little
endian
li
t
xABCD
sw
t
lb
s
Exercise
Write
the
following
strings
using
ASCII
encoding
Write
your
final
answers
in
hexadecimal
a
SOS
b
Cool
c
your
own
name
Exercise
Show
how
the
strings
in
Exercise
are
stored
in
a
byte
addressable
memory
on
a
a
big
endian
machine
and
b
a
little
endian
machine
starting
at
memory
address
x
C
Use
a
memory
diagram
similar
to
Figure
Clearly
indicate
the
memory
address
of
each
byte
on
each
machine
Exercises
Exercise
Convert
the
following
MIPS
assembly
code
into
machine
language
Write
the
instructions
in
hexadecimal
add
t
s
s
lw
t
x
t
addi
s
Exercise
Repeat
Exercise
for
the
following
MIPS
assembly
code
addi
s
sw
t
t
sub
t
s
s
Exercise
Consider
I
type
instructions
a
Which
instructions
from
Exercise
are
I
type
instructions
b
Sign
extend
the
bit
immediate
of
each
instruction
from
part
a
so
that
it
becomes
a
bit
number
Exercise
Convert
the
following
program
from
machine
language
into
MIPS
assembly
language
The
numbers
on
the
left
are
the
instruction
address
in
memory
and
the
numbers
on
the
right
give
the
instruction
at
that
address
Then
reverse
engineer
a
high
level
program
that
would
compile
into
this
assembly
language
routine
and
write
it
Explain
in
words
what
the
program
does
a
is
the
input
and
it
initially
contains
a
positive
number
n
v
is
the
output
x
x
x
x
x
x
a
x
c
x
x
x
x
x
x
x
x
c
x
Exercise
The
nori
instruction
is
not
part
of
the
MIPS
instruction
set
because
the
same
functionality
can
be
implemented
using
existing
instructions
Write
a
short
assembly
code
snippet
that
has
the
following
functionality
t
t
NOR
xF
Use
as
few
instructions
as
possible
CHAPTER
SIX
Architecture
C
Exercise
Implement
the
following
high
level
code
segments
using
the
slt
instruction
Assume
the
integer
variables
g
and
h
are
in
registers
s
and
s
respectively
a
if
g
h
g
g
h
else
g
g
h
b
if
g
h
g
g
else
h
h
c
if
g
h
g
else
h
Exercise
Write
a
procedure
in
a
high
level
language
for
int
find
int
array
int
size
size
specifies
the
number
of
elements
in
the
array
array
specifies
the
base
address
of
the
array
The
procedure
should
return
the
index
number
of
the
first
array
entry
that
holds
the
value
If
no
array
entry
is
it
should
return
the
value
Exercise
The
high
level
procedure
strcpy
copies
the
character
string
x
to
the
character
string
y
high
level
code
void
strcpy
char
x
char
y
int
i
while
x
i
y
i
x
i
i
i
a
Implement
the
strcpy
procedure
in
MIPS
assembly
code
Use
s
for
i
b
Draw
a
picture
of
the
stack
before
during
and
after
the
strcpy
procedure
call
Assume
sp
x
FFFFF
just
before
strcpy
is
called
Exercise
Convert
the
high
level
procedure
from
Exercise
into
MIPS
assembly
code
Exercises
This
simple
string
copy
program
has
a
serious
flaw
it
has
no
way
of
knowing
that
y
has
enough
space
to
receive
x
If
a
malicious
programmer
were
able
to
execute
strcpy
with
a
long
string
x
the
programmer
might
be
able
to
write
bytes
all
over
memory
possibly
even
modifying
code
stored
in
subsequent
memory
locations
With
some
cleverness
the
modified
code
might
take
over
the
machine
This
is
called
a
buffer
overflow
attack
it
is
employed
by
several
nasty
programs
including
the
infamous
Blaster
worm
which
caused
an
estimated
million
in
damages
in
Chapter
qxd
Exercise
Each
number
in
the
Fibonacci
series
is
the
sum
of
the
previous
two
numbers
Table
lists
the
first
few
numbers
in
the
series
fib
n
a
What
is
fib
n
for
n
and
n
b
Write
a
procedure
called
fib
in
a
high
level
language
that
returns
the
Fibonacci
number
for
any
nonnegative
value
of
n
Hint
You
probably
will
want
to
use
a
loop
Clearly
comment
your
code
c
Convert
the
high
level
procedure
of
part
b
into
MIPS
assembly
code
Add
comments
after
every
line
of
code
that
explain
clearly
what
it
does
Use
the
SPIM
simulator
to
test
your
code
on
fib
Exercise
Consider
the
MIPS
assembly
code
below
proc
proc
and
proc
are
non
leaf
procedures
proc
is
a
leaf
procedure
The
code
is
not
shown
for
each
procedure
but
the
comments
indicate
which
registers
are
used
within
each
procedure
x
proc
proc
uses
s
and
s
x
jal
proc
x
proc
proc
uses
s
s
x
C
jal
proc
x
proc
proc
uses
s
s
x
jal
proc
x
proc
proc
uses
no
preserved
registers
x
jr
ra
a
How
many
words
are
the
stack
frames
of
each
procedure
b
Sketch
the
stack
after
proc
is
called
Clearly
indicate
which
registers
are
stored
where
on
the
stack
Give
values
where
possible
CHAPTER
SIX
Architecture
Table
Fibonacci
series
n
fib
n
Ch
Exercise
Ben
Bitdiddle
is
trying
to
compute
the
function
f
a
b
a
b
for
nonnegative
b
He
goes
overboard
in
the
use
of
procedure
calls
and
recursion
and
produces
the
following
high
level
code
for
procedures
f
and
f
high
level
code
for
procedures
f
and
f
int
f
int
a
int
b
int
j
j
a
return
j
a
f
b
int
f
int
x
int
k
k
if
x
return
else
return
k
f
x
Ben
then
translates
the
two
procedures
into
assembly
language
as
follows
He
also
writes
a
procedure
test
that
calls
the
procedure
f
MIPS
assembly
code
f
a
a
a
b
s
j
f
a
x
s
k
x
test
addi
a
a
a
x
addi
a
a
b
x
jal
f
call
f
x
c
loop
j
loop
and
loop
forever
x
f
addi
sp
sp
make
room
on
the
stack
for
s
a
a
and
ra
x
sw
a
sp
save
a
b
x
sw
a
sp
save
a
a
x
c
sw
ra
sp
save
ra
x
sw
s
sp
save
s
x
add
s
a
s
a
j
a
x
add
a
a
place
b
as
argument
for
f
x
c
jal
f
call
f
b
x
lw
a
sp
restore
a
a
after
call
x
lw
a
sp
restore
a
b
after
call
x
add
v
v
s
v
f
b
j
x
c
add
v
v
a
v
f
b
j
a
x
lw
s
sp
restore
s
x
lw
ra
sp
restore
ra
x
addi
sp
sp
restore
sp
stack
pointer
x
c
jr
ra
return
to
point
of
call
x
f
addi
sp
sp
make
room
on
the
stack
for
s
a
and
ra
x
sw
a
sp
save
a
x
x
sw
ra
sp
save
return
address
x
c
sw
s
sp
save
s
x
addi
s
k
Exercises
Chapter
qxd
x
bne
a
else
x
x
addi
v
yes
return
value
should
be
x
c
j
done
and
clean
up
x
else
addi
a
a
no
a
a
x
x
x
jal
f
call
f
x
x
lw
a
sp
restore
a
x
x
c
add
v
v
s
v
f
x
k
x
done
lw
s
sp
restore
s
x
lw
ra
sp
restore
ra
x
addi
sp
sp
restore
sp
x
c
jr
ra
return
to
point
of
call
You
will
probably
find
it
useful
to
make
drawings
of
the
stack
similar
to
the
one
in
Figure
to
help
you
answer
the
following
questions
a
If
the
code
runs
starting
at
test
what
value
is
in
v
when
the
program
gets
to
loop
Does
his
program
correctly
compute
a
b
b
Suppose
Ben
deletes
the
instructions
at
addresses
x
C
and
x
that
save
and
restore
ra
Will
the
program
enter
an
infinite
loop
but
not
crash
crash
cause
the
stack
to
grow
beyond
the
dynamic
data
segment
or
the
PC
to
jump
to
a
location
outside
the
program
produce
an
incorrect
value
in
v
when
the
program
returns
to
loop
if
so
what
value
or
run
correctly
despite
the
deleted
lines
c
Repeat
part
b
when
the
instructions
at
the
following
instruction
addresses
are
deleted
i
x
and
x
instructions
that
save
and
restore
a
ii
x
and
x
instructions
that
save
and
restore
a
iii
x
and
x
instructions
that
save
and
restore
s
iv
x
and
x
instructions
that
save
and
restore
sp
v
x
C
and
x
instructions
that
save
and
restore
s
vi
x
and
x
instructions
that
save
and
restore
ra
vii
x
and
x
instructions
that
save
and
restore
a
Exercise
Convert
the
following
beq
j
and
jal
assembly
instructions
into
machine
code
Instruction
addresses
are
given
to
the
left
of
each
instruction
a
x
beq
t
s
Loop
x
x
x
C
Loop
CHAPTER
SIX
Architecture
Chapte
b
x
beq
t
s
done
x
done
c
x
C
back
x
beq
t
s
back
d
x
jal
proc
x
C
proc
e
x
back
x
C
j
back
Exercise
Consider
the
following
MIPS
assembly
language
snippet
The
numbers
to
the
left
of
each
instruction
indicate
the
instruction
address
x
add
a
a
x
c
jal
f
x
f
jr
ra
x
f
sw
s
s
x
bne
a
else
x
c
j
f
x
else
addi
a
a
x
j
f
a
Translate
the
instruction
sequence
into
machine
code
Write
the
machine
code
instructions
in
hexadecimal
b
List
the
addressing
mode
used
at
each
line
of
code
Exercise
Consider
the
following
C
code
snippet
C
code
void
set
array
int
num
int
i
int
array
for
i
i
i
i
array
i
compare
num
i
int
compare
int
a
int
b
if
sub
a
b
return
else
Exercises
Chapt
return
int
sub
int
a
int
b
return
a
b
a
Implement
the
C
code
snippet
in
MIPS
assembly
language
Use
s
to
hold
the
variable
i
Be
sure
to
handle
the
stack
pointer
appropriately
The
array
is
stored
on
the
stack
of
the
set
array
procedure
see
Section
b
Assume
set
array
is
the
first
procedure
called
Draw
the
status
of
the
stack
before
calling
set
array
and
during
each
procedure
call
Indicate
the
names
of
registers
and
variables
stored
on
the
stack
and
mark
the
location
of
sp
c
How
would
your
code
function
if
you
failed
to
store
ra
on
the
stack
Exercise
Consider
the
following
high
level
procedure
high
level
code
int
f
int
n
int
k
int
b
b
k
if
n
b
else
b
b
n
n
f
n
k
return
b
k
a
Translate
the
high
level
procedure
f
into
MIPS
assembly
language
Pay
particular
attention
to
properly
saving
and
restoring
registers
across
procedure
calls
and
using
the
MIPS
preserved
register
conventions
Clearly
comment
your
code
You
can
use
the
MIPS
mult
mfhi
and
mflo
instructions
The
procedure
starts
at
instruction
address
x
Keep
local
variable
b
in
s
b
Step
through
your
program
from
part
a
by
hand
for
the
case
of
f
Draw
a
picture
of
the
stack
similar
to
the
one
in
Figure
c
Write
the
register
name
and
data
value
stored
at
each
location
in
the
stack
and
keep
track
of
the
stack
pointer
value
sp
You
might
also
find
it
useful
to
keep
track
of
the
values
in
a
a
v
and
s
throughout
execution
Assume
that
when
f
is
called
s
xABCD
and
ra
x
What
is
the
final
value
of
v
Exercise
What
is
the
range
of
instruction
addresses
to
which
conditional
branches
such
as
beq
and
bne
can
branch
in
MIPS
Give
your
answer
in
number
of
instructions
relative
to
the
conditional
branch
instruction
CHAPTER
SIX
Architecture
Chapter
Exercise
The
following
questions
examine
the
limitations
of
the
jump
instruction
j
Give
your
answer
in
number
of
instructions
relative
to
the
jump
instruction
a
In
the
worst
case
how
far
can
the
jump
instruction
j
jump
forward
i
e
to
higher
addresses
The
worst
case
is
when
the
jump
instruction
cannot
jump
far
Explain
using
words
and
examples
as
needed
b
In
the
best
case
how
far
can
the
jump
instruction
j
jump
forward
The
best
case
is
when
the
jump
instruction
can
jump
the
farthest
Explain
c
In
the
worst
case
how
far
can
the
jump
instruction
j
jump
backward
to
lower
addresses
Explain
d
In
the
best
case
how
far
can
the
jump
instruction
j
jump
backward
Explain
Exercise
Explain
why
it
is
advantageous
to
have
a
large
address
field
addr
in
the
machine
format
for
the
jump
instructions
j
and
jal
Exercise
Write
assembly
code
that
jumps
to
the
instruction
Minstructions
from
the
first
instruction
Recall
that
Minstruction
instructions
instructions
Assume
that
your
code
begins
at
address
x
Use
a
minimum
number
of
instructions
Exercise
Write
a
procedure
in
high
level
code
that
takes
a
ten
entry
array
of
bit
integers
stored
in
little
endian
format
and
converts
it
to
big
endian
format
After
writing
the
high
level
code
convert
it
to
MIPS
assembly
code
Comment
all
your
code
and
use
a
minimum
number
of
instructions
Exercise
Consider
two
strings
string
and
string
a
Write
high
level
code
for
a
procedure
called
concat
that
concatenates
joins
together
the
two
strings
void
concat
char
string
char
string
char
stringconcat
The
procedure
does
not
return
a
value
It
concatenates
string
and
string
and
places
the
resulting
string
in
stringconcat
You
may
assume
that
the
character
array
stringconcat
is
large
enough
to
accommodate
the
concatenated
string
b
Convert
the
procedure
from
part
a
into
MIPS
assembly
language
Exercise
Write
a
MIPS
assembly
program
that
adds
two
positive
singleprecision
floating
point
numbers
held
in
s
and
s
Do
not
use
any
of
the
MIPS
floating
point
instructions
You
need
not
worry
about
any
of
the
encodings
that
are
reserved
for
special
purposes
e
g
NANs
INF
or
numbers
that
overflow
or
underflow
Use
the
SPIM
simulator
to
test
your
code
You
will
need
to
manually
set
the
values
of
s
and
s
to
test
your
code
Demonstrate
that
your
code
functions
reliably
Exercises
Ch
Exercise
Show
how
the
following
MIPS
program
would
be
loaded
into
memory
and
executed
MIPS
assembly
code
main
lw
a
x
lw
a
y
jal
diff
jr
ra
diff
sub
v
a
a
jr
ra
a
First
show
the
instruction
address
next
to
each
assembly
instruction
b
Draw
the
symbol
table
showing
the
labels
and
their
addresses
c
Convert
all
instructions
into
machine
code
d
How
big
how
many
bytes
are
the
data
and
text
segments
e
Sketch
a
memory
map
showing
where
data
and
instructions
are
stored
Exercise
Show
the
MIPS
instructions
that
implement
the
following
pseudoinstructions
You
may
use
the
assembler
register
at
but
you
may
not
corrupt
overwrite
any
other
registers
a
beq
t
imm
L
b
ble
t
t
L
c
bgt
t
t
L
d
bge
t
t
L
e
addi
t
imm
f
lw
t
imm
s
g
rol
t
t
rotate
t
left
by
and
put
the
result
in
t
h
ror
s
t
rotate
t
right
by
and
put
the
result
in
s
CHAPTER
SIX
Architecture
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
but
are
usually
open
to
any
assembly
language
Question
Write
MIPS
assembly
code
for
swapping
the
contents
of
two
registers
t
and
t
You
may
not
use
any
other
registers
Question
Suppose
you
are
given
an
array
of
both
positive
and
negative
integers
Write
MIPS
assembly
code
that
finds
the
subset
of
the
array
with
the
largest
sum
Assume
that
the
array
s
base
address
and
the
number
of
array
elements
are
in
a
and
a
respectively
Your
code
should
place
the
resulting
subset
of
the
array
starting
at
base
address
a
Write
code
that
runs
as
fast
as
possible
Question
You
are
given
an
array
that
holds
a
C
string
The
string
forms
a
sentence
Design
an
algorithm
for
reversing
the
words
in
the
sentence
and
storing
the
new
sentence
back
in
the
array
Implement
your
algorithm
using
MIPS
assembly
code
Question
Design
an
algorithm
for
counting
the
number
of
s
in
a
bit
number
Implement
your
algorithm
using
MIPS
assembly
code
Question
Write
MIPS
assembly
code
to
reverse
the
bits
in
a
register
Use
as
few
instructions
as
possible
Assume
the
register
of
interest
is
t
Question
Write
MIPS
assembly
code
to
test
whether
overflow
occurs
when
t
and
t
are
added
Use
a
minimum
number
of
instructions
Question
Design
an
algorithm
for
testing
whether
a
given
string
is
a
palindrome
Recall
that
a
palindrome
is
a
word
that
is
the
same
forward
and
backward
For
example
the
words
wow
and
racecar
are
palindromes
Implement
your
algorithm
using
MIPS
assembly
code
Interview
Questions
Introduction
Performance
Analysis
Single
Cycle
Processor
Multicycle
Processor
Pipelined
Processor
HDL
Representation
Exceptions
Advanced
Microarchitecture
Real
World
Perspective
IA
Microarchitecture
Summary
Exercises
Interview
Questions
Microarchitecture
INTRODUCTION
In
this
chapter
you
will
learn
how
to
piece
together
a
MIPS
microprocessor
Indeed
you
will
puzzle
out
three
different
versions
each
with
different
trade
offs
between
performance
cost
and
complexity
To
the
uninitiated
building
a
microprocessor
may
seem
like
black
magic
But
it
is
actually
relatively
straightforward
and
by
this
point
you
have
learned
everything
you
need
to
know
Specifically
you
have
learned
to
design
combinational
and
sequential
logic
given
functional
and
timing
specifications
You
are
familiar
with
circuits
for
arithmetic
and
memory
And
you
have
learned
about
the
MIPS
architecture
which
specifies
the
programmer
s
view
of
the
MIPS
processor
in
terms
of
registers
instructions
and
memory
This
chapter
covers
microarchitecture
which
is
the
connection
between
logic
and
architecture
Microarchitecture
is
the
specific
arrangement
of
registers
ALUs
finite
state
machines
FSMs
memories
and
other
logic
building
blocks
needed
to
implement
an
architecture
A
particular
architecture
such
as
MIPS
may
have
many
different
microarchitectures
each
with
different
trade
offs
of
performance
cost
and
complexity
They
all
run
the
same
programs
but
their
internal
designs
vary
widely
We
will
design
three
different
microarchitectures
in
this
chapter
to
illustrate
the
trade
offs
This
chapter
draws
heavily
on
David
Patterson
and
John
Hennessy
s
classic
MIPS
designs
in
their
text
Computer
Organization
and
Design
They
have
generously
shared
their
elegant
designs
which
have
the
virtue
of
illustrating
a
real
commercial
architecture
while
being
relatively
simple
and
easy
to
understand
Architectural
State
and
Instruction
Set
Recall
that
a
computer
architecture
is
defined
by
its
instruction
set
and
architectural
state
The
architectural
state
for
the
MIPS
processor
consists
of
the
program
counter
and
the
registers
Any
MIPS
microarchitecture
must
contain
all
of
this
state
Based
on
the
current
architectural
state
the
processor
executes
a
particular
instruction
with
a
particular
set
of
data
to
produce
a
new
architectural
state
Some
microarchitectures
contain
additional
nonarchitectural
state
to
either
simplify
the
logic
or
improve
performance
we
will
point
this
out
as
it
arises
To
keep
the
microarchitectures
easy
to
understand
we
consider
only
a
subset
of
the
MIPS
instruction
set
Specifically
we
handle
the
following
instructions
R
type
arithmetic
logic
instructions
add
sub
and
or
slt
Memory
instructions
lw
sw
Branches
beq
After
building
the
microarchitectures
with
these
instructions
we
extend
them
to
handle
addi
and
j
These
particular
instructions
were
chosen
because
they
are
sufficient
to
write
many
interesting
programs
Once
you
understand
how
to
implement
these
instructions
you
can
expand
the
hardware
to
handle
others
Design
Process
We
will
divide
our
microarchitectures
into
two
interacting
parts
the
datapath
and
the
control
The
datapath
operates
on
words
of
data
It
contains
structures
such
as
memories
registers
ALUs
and
multiplexers
MIPS
is
a
bit
architecture
so
we
will
use
a
bit
datapath
The
control
unit
receives
the
current
instruction
from
the
datapath
and
tells
the
datapath
how
to
execute
that
instruction
Specifically
the
control
unit
produces
multiplexer
select
register
enable
and
memory
write
signals
to
control
the
operation
of
the
datapath
A
good
way
to
design
a
complex
system
is
to
start
with
hardware
containing
the
state
elements
These
elements
include
the
memories
and
the
architectural
state
the
program
counter
and
registers
Then
add
blocks
of
combinational
logic
between
the
state
elements
to
compute
the
new
state
based
on
the
current
state
The
instruction
is
read
from
part
of
memory
load
and
store
instructions
then
read
or
write
data
from
another
part
of
memory
Hence
it
is
often
convenient
to
partition
the
overall
memory
into
two
smaller
memories
one
containing
instructions
and
the
other
containing
data
Figure
shows
a
block
diagram
with
the
four
state
elements
the
program
counter
register
file
and
instruction
and
data
memories
In
Figure
heavy
lines
are
used
to
indicate
bit
data
busses
Medium
lines
are
used
to
indicate
narrower
busses
such
as
the
bit
address
busses
on
the
register
file
Narrow
blue
lines
are
used
to
indicate
CHAPTER
SEVEN
Microarchitecture
David
Patterson
was
the
first
in
his
family
to
graduate
from
college
UCLA
He
has
been
a
professor
of
computer
science
at
UC
Berkeley
since
where
he
coinvented
RISC
the
Reduced
Instruction
Set
Computer
In
he
developed
the
SPARC
architecture
used
by
Sun
Microsystems
He
is
also
the
father
of
RAID
Redundant
Array
of
Inexpensive
Disks
and
NOW
Network
of
Workstations
John
Hennessy
is
president
of
Stanford
University
and
has
been
a
professor
of
electrical
engineering
and
computer
science
there
since
He
coinvented
RISC
He
developed
the
MIPS
architecture
at
Stanford
in
and
cofounded
MIPS
Computer
Systems
As
of
more
than
million
MIPS
microprocessors
have
been
sold
In
their
copious
free
time
these
two
modern
paragons
write
textbooks
for
recreation
and
relaxation
Cha
control
signals
such
as
the
register
file
write
enable
We
will
use
this
convention
throughout
the
chapter
to
avoid
cluttering
diagrams
with
bus
widths
Also
state
elements
usually
have
a
reset
input
to
put
them
into
a
known
state
at
start
up
Again
to
save
clutter
this
reset
is
not
shown
The
program
counter
is
an
ordinary
bit
register
Its
output
PC
points
to
the
current
instruction
Its
input
PC
indicates
the
address
of
the
next
instruction
The
instruction
memory
has
a
single
read
port
It
takes
a
bit
instruction
address
input
A
and
reads
the
bit
data
i
e
instruction
from
that
address
onto
the
read
data
output
RD
The
element
bit
register
file
has
two
read
ports
and
one
write
port
The
read
ports
take
bit
address
inputs
A
and
A
each
specifying
one
of
registers
as
source
operands
They
read
the
bit
register
values
onto
read
data
outputs
RD
and
RD
respectively
The
write
port
takes
a
bit
address
input
A
a
bit
write
data
input
WD
a
write
enable
input
WE
and
a
clock
If
the
write
enable
is
the
register
file
writes
the
data
into
the
specified
register
on
the
rising
edge
of
the
clock
The
data
memory
has
a
single
read
write
port
If
the
write
enable
WE
is
it
writes
data
WD
into
address
A
on
the
rising
edge
of
the
clock
If
the
write
enable
is
it
reads
address
A
onto
RD
The
instruction
memory
register
file
and
data
memory
are
all
read
combinationally
In
other
words
if
the
address
changes
the
new
data
appears
at
RD
after
some
propagation
delay
no
clock
is
involved
They
are
written
only
on
the
rising
edge
of
the
clock
In
this
fashion
the
state
of
the
system
is
changed
only
at
the
clock
edge
The
address
data
and
write
enable
must
setup
sometime
before
the
clock
edge
and
must
remain
stable
until
a
hold
time
after
the
clock
edge
Because
the
state
elements
change
their
state
only
on
the
rising
edge
of
the
clock
they
are
synchronous
sequential
circuits
The
microprocessor
is
Introduction
Resetting
the
PC
At
the
very
least
the
program
counter
must
have
a
reset
signal
to
initialize
its
value
when
the
processor
turns
on
MIPS
processors
initialize
the
PC
to
xBFC
on
reset
and
begin
executing
code
to
start
up
the
operating
system
OS
The
OS
then
loads
an
application
program
at
x
and
begins
executing
it
For
simplicity
in
this
chapter
we
will
reset
the
PC
to
x
and
place
our
programs
there
instead
This
is
an
oversimplification
used
to
treat
the
instruction
memory
as
a
ROM
in
most
real
processors
the
instruction
memory
must
be
writable
so
that
the
OS
can
load
a
new
program
into
memory
The
multicycle
microarchitecture
described
in
Section
is
more
realistic
in
that
it
uses
a
combined
memory
for
instructions
and
data
that
can
be
both
read
and
written
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
A
RD
Data
Memory
WD
PC
PC
WE
CLK
Figure
State
elements
of
MIPS
processor
Ch
built
of
clocked
state
elements
and
combinational
logic
so
it
too
is
a
synchronous
sequential
circuit
Indeed
the
processor
can
be
viewed
as
a
giant
finite
state
machine
or
as
a
collection
of
simpler
interacting
state
machines
MIPS
Microarchitectures
In
this
chapter
we
develop
three
microarchitectures
for
the
MIPS
processor
architecture
single
cycle
multicycle
and
pipelined
They
differ
in
the
way
that
the
state
elements
are
connected
together
and
in
the
amount
of
nonarchitectural
state
The
single
cycle
microarchitecture
executes
an
entire
instruction
in
one
cycle
It
is
easy
to
explain
and
has
a
simple
control
unit
Because
it
completes
the
operation
in
one
cycle
it
does
not
require
any
nonarchitectural
state
However
the
cycle
time
is
limited
by
the
slowest
instruction
The
multicycle
microarchitecture
executes
instructions
in
a
series
of
shorter
cycles
Simpler
instructions
execute
in
fewer
cycles
than
complicated
ones
Moreover
the
multicycle
microarchitecture
reduces
the
hardware
cost
by
reusing
expensive
hardware
blocks
such
as
adders
and
memories
For
example
the
adder
may
be
used
on
several
different
cycles
for
several
purposes
while
carrying
out
a
single
instruction
The
multicycle
microprocessor
accomplishes
this
by
adding
several
nonarchitectural
registers
to
hold
intermediate
results
The
multicycle
processor
executes
only
one
instruction
at
a
time
but
each
instruction
takes
multiple
clock
cycles
The
pipelined
microarchitecture
applies
pipelining
to
the
single
cycle
microarchitecture
It
therefore
can
execute
several
instructions
simultaneously
improving
the
throughput
significantly
Pipelining
must
add
logic
to
handle
dependencies
between
simultaneously
executing
instructions
It
also
requires
nonarchitectural
pipeline
registers
The
added
logic
and
registers
are
worthwhile
all
commercial
high
performance
processors
use
pipelining
today
We
explore
the
details
and
trade
offs
of
these
three
microarchitectures
in
the
subsequent
sections
At
the
end
of
the
chapter
we
briefly
mention
additional
techniques
that
are
used
to
get
even
more
speed
in
modern
high
performance
microprocessors
PERFORMANCE
ANALYSIS
As
we
mentioned
a
particular
processor
architecture
can
have
many
microarchitectures
with
different
cost
and
performance
trade
offs
The
cost
depends
on
the
amount
of
hardware
required
and
the
implementation
technology
Each
year
CMOS
processes
can
pack
more
transistors
on
a
chip
for
the
same
amount
of
money
and
processors
take
advantage
CHAPTER
SEVEN
Microarchitecture
of
these
additional
transistors
to
deliver
more
performance
Precise
cost
calculations
require
detailed
knowledge
of
the
implementation
technology
but
in
general
more
gates
and
more
memory
mean
more
dollars
This
section
lays
the
foundation
for
analyzing
performance
There
are
many
ways
to
measure
the
performance
of
a
computer
system
and
marketing
departments
are
infamous
for
choosing
the
method
that
makes
their
computer
look
fastest
regardless
of
whether
the
measurement
has
any
correlation
to
real
world
performance
For
example
Intel
and
Advanced
Micro
Devices
AMD
both
sell
compatible
microprocessors
conforming
to
the
IA
architecture
Intel
Pentium
III
and
Pentium
microprocessors
were
largely
advertised
according
to
clock
frequency
in
the
late
s
and
early
s
because
Intel
offered
higher
clock
frequencies
than
its
competitors
However
Intel
s
main
competitor
AMD
sold
Athlon
microprocessors
that
executed
programs
faster
than
Intel
s
chips
at
the
same
clock
frequency
What
is
a
consumer
to
do
The
only
gimmick
free
way
to
measure
performance
is
by
measuring
the
execution
time
of
a
program
of
interest
to
you
The
computer
that
executes
your
program
fastest
has
the
highest
performance
The
next
best
choice
is
to
measure
the
total
execution
time
of
a
collection
of
programs
that
are
similar
to
those
you
plan
to
run
this
may
be
necessary
if
you
haven
t
written
your
program
yet
or
if
somebody
else
who
doesn
t
have
your
program
is
making
the
measurements
Such
collections
of
programs
are
called
benchmarks
and
the
execution
times
of
these
programs
are
commonly
published
to
give
some
indication
of
how
a
processor
performs
The
execution
time
of
a
program
measured
in
seconds
is
given
by
Equation
The
number
of
instructions
in
a
program
depends
on
the
processor
architecture
Some
architectures
have
complicated
instructions
that
do
more
work
per
instruction
thus
reducing
the
number
of
instructions
in
a
program
However
these
complicated
instructions
are
often
slower
to
execute
in
hardware
The
number
of
instructions
also
depends
enormously
on
the
cleverness
of
the
programmer
For
the
purposes
of
this
chapter
we
will
assume
that
we
are
executing
known
programs
on
a
MIPS
processor
so
the
number
of
instructions
for
each
program
is
constant
independent
of
the
microarchitecture
The
number
of
cycles
per
instruction
often
called
CPI
is
the
number
of
clock
cycles
required
to
execute
an
average
instruction
It
is
the
reciprocal
of
the
throughput
instructions
per
cycle
or
IPC
Different
microarchitectures
have
different
CPIs
In
this
chapter
we
will
assume
Execution
Time
instructions
cycles
instructionseconds
cycle
Performance
Analysis
Chapte
we
have
an
ideal
memory
system
that
does
not
affect
the
CPI
In
Chapter
we
examine
how
the
processor
sometimes
has
to
wait
for
the
memory
which
increases
the
CPI
The
number
of
seconds
per
cycle
is
the
clock
period
Tc
The
clock
period
is
determined
by
the
critical
path
through
the
logic
on
the
processor
Different
microarchitectures
have
different
clock
periods
Logic
and
circuit
designs
also
significantly
affect
the
clock
period
For
example
a
carry
lookahead
adder
is
faster
than
a
ripple
carry
adder
Manufacturing
advances
have
historically
doubled
transistor
speeds
every
years
so
a
microprocessor
built
today
will
be
much
faster
than
one
from
last
decade
even
if
the
microarchitecture
and
logic
are
unchanged
The
challenge
of
the
microarchitect
is
to
choose
the
design
that
minimizes
the
execution
time
while
satisfying
constraints
on
cost
and
or
power
consumption
Because
microarchitectural
decisions
affect
both
CPI
and
Tc
and
are
influenced
by
logic
and
circuit
designs
determining
the
best
choice
requires
careful
analysis
There
are
many
other
factors
that
affect
overall
computer
performance
For
example
the
hard
disk
the
memory
the
graphics
system
and
the
network
connection
may
be
limiting
factors
that
make
processor
performance
irrelevant
The
fastest
microprocessor
in
the
world
doesn
t
help
surfing
the
Internet
on
a
dial
up
connection
But
these
other
factors
are
beyond
the
scope
of
this
book
SINGLE
CYCLE
PROCESSOR
We
first
design
a
MIPS
microarchitecture
that
executes
instructions
in
a
single
cycle
We
begin
constructing
the
datapath
by
connecting
the
state
elements
from
Figure
with
combinational
logic
that
can
execute
the
various
instructions
Control
signals
determine
which
specific
instruction
is
carried
out
by
the
datapath
at
any
given
time
The
controller
contains
combinational
logic
that
generates
the
appropriate
control
signals
based
on
the
current
instruction
We
conclude
by
analyzing
the
performance
of
the
single
cycle
processor
Single
Cycle
Datapath
This
section
gradually
develops
the
single
cycle
datapath
adding
one
piece
at
a
time
to
the
state
elements
from
Figure
The
new
connections
are
emphasized
in
black
or
blue
for
new
control
signals
while
the
hardware
that
has
already
been
studied
is
shown
in
gray
The
program
counter
PC
register
contains
the
address
of
the
instruction
to
execute
The
first
step
is
to
read
this
instruction
from
instruction
memory
Figure
shows
that
the
PC
is
simply
connected
to
the
address
input
of
the
instruction
memory
The
instruction
memory
reads
out
or
fetches
the
bit
instruction
labeled
Instr
CHAPTER
SEVEN
Microarchitecture
The
processor
s
actions
depend
on
the
specific
instruction
that
was
fetched
First
we
will
work
out
the
datapath
connections
for
the
lw
instruction
Then
we
will
consider
how
to
generalize
the
datapath
to
handle
the
other
instructions
For
a
lw
instruction
the
next
step
is
to
read
the
source
register
containing
the
base
address
This
register
is
specified
in
the
rs
field
of
the
instruction
Instr
These
bits
of
the
instruction
are
connected
to
the
address
input
of
one
of
the
register
file
read
ports
A
as
shown
in
Figure
The
register
file
reads
the
register
value
onto
RD
The
lw
instruction
also
requires
an
offset
The
offset
is
stored
in
the
immediate
field
of
the
instruction
Instr
Because
the
bit
immediate
might
be
either
positive
or
negative
it
must
be
sign
extended
to
bits
as
shown
in
Figure
The
bit
sign
extended
value
is
called
SignImm
Recall
from
Section
that
sign
extension
simply
copies
the
sign
bit
most
significant
bit
of
a
short
input
into
all
of
the
upper
bits
of
the
longer
output
Specifically
SignImm
Instr
and
SignImm
Instr
Single
Cycle
Processor
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
PC
Instr
A
RD
Data
Memory
WD
WE
CLK
Figure
Fetch
instruction
from
memory
Instr
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
PC
A
RD
Data
Memory
WD
WE
CLK
Figure
Read
source
operand
from
register
file
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
A
RD
Data
Memory
WD
WE
CLK
Figure
Sign
extend
the
immediate
The
processor
must
add
the
base
address
to
the
offset
to
find
the
address
to
read
from
memory
Figure
introduces
an
ALU
to
perform
this
addition
The
ALU
receives
two
operands
SrcA
and
SrcB
SrcA
comes
from
the
register
file
and
SrcB
comes
from
the
sign
extended
immediate
The
ALU
can
perform
many
operations
as
was
described
in
Section
The
bit
ALUControl
signal
specifies
the
operation
The
ALU
generates
a
bit
ALUResult
and
a
Zero
flag
that
indicates
whether
ALUResult
For
a
lw
instruction
the
ALUControl
signal
should
be
set
to
to
add
the
base
address
and
offset
ALUResult
is
sent
to
the
data
memory
as
the
address
for
the
load
instruction
as
shown
in
Figure
The
data
is
read
from
the
data
memory
onto
the
ReadData
bus
then
written
back
to
the
destination
register
in
the
register
file
at
the
end
of
the
cycle
as
shown
in
Figure
Port
of
the
register
file
is
the
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
SrcA
Zero
CLK
ALUControl
ALU
Figure
Compute
memory
address
A
A
WD
RD
RD
WE
A
SignImm
CLK
A
RD
Instruction
Memory
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
SrcA
RegWrite
Zero
CLK
ALUControl
ALU
Figure
Write
data
back
to
register
file
write
port
The
destination
register
for
the
lw
instruction
is
specified
in
the
rt
field
Instr
which
is
connected
to
the
port
address
input
A
of
the
register
file
The
ReadData
bus
is
connected
to
the
port
write
data
input
WD
of
the
register
file
A
control
signal
called
RegWrite
is
connected
to
the
port
write
enable
input
WE
and
is
asserted
during
a
lw
instruction
so
that
the
data
value
is
written
into
the
register
file
The
write
takes
place
on
the
rising
edge
of
the
clock
at
the
end
of
the
cycle
While
the
instruction
is
being
executed
the
processor
must
compute
the
address
of
the
next
instruction
PC
Because
instructions
are
bits
bytes
the
next
instruction
is
at
PC
Figure
uses
another
adder
to
increment
the
PC
by
The
new
address
is
written
into
the
program
counter
on
the
next
rising
edge
of
the
clock
This
completes
the
datapath
for
the
lw
instruction
Next
let
us
extend
the
datapath
to
also
handle
the
sw
instruction
Like
the
lw
instruction
the
sw
instruction
reads
a
base
address
from
port
of
the
register
and
sign
extends
an
immediate
The
ALU
adds
the
base
address
to
the
immediate
to
find
the
memory
address
All
of
these
functions
are
already
supported
by
the
datapath
The
sw
instruction
also
reads
a
second
register
from
the
register
file
and
writes
it
to
the
data
memory
Figure
shows
the
new
connections
for
this
function
The
register
is
specified
in
the
rt
field
Instr
These
bits
of
the
instruction
are
connected
to
the
second
register
file
read
port
A
The
register
value
is
read
onto
the
RD
port
It
is
connected
to
the
write
data
port
of
the
data
memory
The
write
enable
port
of
the
data
memory
WE
is
controlled
by
MemWrite
For
a
sw
instruction
MemWrite
to
write
the
data
to
memory
ALUControl
to
add
the
base
address
Single
Cycle
Processor
CLK
SignImm
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
SrcA
PCPlus
Result
RegWrite
Zero
CLK
ALUControl
ALU
Figure
Determine
address
of
next
instruction
for
PC
C
and
offset
and
RegWrite
because
nothing
should
be
written
to
the
register
file
Note
that
data
is
still
read
from
the
address
given
to
the
data
memory
but
that
this
ReadData
is
ignored
because
RegWrite
Next
consider
extending
the
datapath
to
handle
the
R
type
instructions
add
sub
and
or
and
slt
All
of
these
instructions
read
two
registers
from
the
register
file
perform
some
ALU
operation
on
them
and
write
the
result
back
to
a
third
register
file
They
differ
only
in
the
specific
ALU
operation
Hence
they
can
all
be
handled
with
the
same
hardware
using
different
ALUControl
signals
Figure
shows
the
enhanced
datapath
handling
R
type
instructions
The
register
file
reads
two
registers
The
ALU
performs
an
operation
on
these
two
registers
In
Figure
the
ALU
always
received
its
SrcB
operand
from
the
sign
extended
immediate
SignImm
Now
we
add
a
multiplexer
to
choose
SrcB
from
either
the
register
file
RD
port
or
SignImm
The
multiplexer
is
controlled
by
a
new
signal
ALUSrc
ALUSrc
is
for
R
type
instructions
to
choose
SrcB
from
the
register
file
it
is
for
lw
and
sw
to
choose
SignImm
This
principle
of
enhancing
the
datapath
s
capabilities
by
adding
a
multiplexer
to
choose
inputs
from
several
possibilities
is
extremely
useful
Indeed
we
will
apply
it
twice
more
to
complete
the
handling
of
R
type
instructions
In
Figure
the
register
file
always
got
its
write
data
from
the
data
memory
However
R
type
instructions
write
the
ALUResult
to
the
register
file
Therefore
we
add
another
multiplexer
to
choose
between
ReadData
and
ALUResult
We
call
its
output
Result
This
multiplexer
is
controlled
by
another
new
signal
MemtoReg
MemtoReg
is
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
Result
RegWrite
MemWrite
Zero
CLK
ALUControl
ALU
Figure
Write
data
to
memory
for
sw
instruction
for
R
type
instructions
to
choose
Result
from
the
ALUResult
it
is
for
lw
to
choose
ReadData
We
don
t
care
about
the
value
of
MemtoReg
for
sw
because
sw
does
not
write
to
the
register
file
Similarly
in
Figure
the
register
to
write
was
specified
by
the
rt
field
of
the
instruction
Instr
However
for
R
type
instructions
the
register
is
specified
by
the
rd
field
Instr
Thus
we
add
a
third
multiplexer
to
choose
WriteReg
from
the
appropriate
field
of
the
instruction
The
multiplexer
is
controlled
by
RegDst
RegDst
is
for
R
type
instructions
to
choose
WriteReg
from
the
rd
field
Instr
it
is
for
lw
to
choose
the
rt
field
Instr
We
don
t
care
about
the
value
of
RegDst
for
sw
because
sw
does
not
write
to
the
register
file
Finally
let
us
extend
the
datapath
to
handle
beq
beq
compares
two
registers
If
they
are
equal
it
takes
the
branch
by
adding
the
branch
offset
to
the
program
counter
Recall
that
the
offset
is
a
positive
or
negative
number
stored
in
the
imm
field
of
the
instruction
Instr
The
offset
indicates
the
number
of
instructions
to
branch
past
Hence
the
immediate
must
be
sign
extended
and
multiplied
by
to
get
the
new
program
counter
value
PC
PC
SignImm
Figure
shows
the
datapath
modifications
The
next
PC
value
for
a
taken
branch
PCBranch
is
computed
by
shifting
SignImm
left
by
bits
then
adding
it
to
PCPlus
The
left
shift
by
is
an
easy
way
to
multiply
by
because
a
shift
by
a
constant
amount
involves
just
wires
The
two
registers
are
compared
by
computing
SrcA
SrcB
using
the
ALU
If
ALUResult
is
as
indicated
by
the
Zero
flag
from
the
ALU
the
registers
are
equal
We
add
a
multiplexer
to
choose
PC
from
either
PCPlus
or
PCBranch
PCBranch
is
selected
if
the
instruction
is
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
WriteReg
Result
RegWrite
RegDst
ALUSrc
MemWrite
MemtoReg
Zero
CLK
ALUControl
varies
ALU
Figure
Datapath
enhancements
for
R
type
instruction
Cha
a
branch
and
the
Zero
flag
is
asserted
Hence
Branch
is
for
beq
and
for
other
instructions
For
beq
ALUControl
so
the
ALU
performs
a
subtraction
ALUSrc
to
choose
SrcB
from
the
register
file
RegWrite
and
MemWrite
are
because
a
branch
does
not
write
to
the
register
file
or
memory
We
don
t
care
about
the
values
of
RegDst
and
MemtoReg
because
the
register
file
is
not
written
This
completes
the
design
of
the
single
cycle
MIPS
processor
datapath
We
have
illustrated
not
only
the
design
itself
but
also
the
design
process
in
which
the
state
elements
are
identified
and
the
combinational
logic
connecting
the
state
elements
is
systematically
added
In
the
next
section
we
consider
how
to
compute
the
control
signals
that
direct
the
operation
of
our
datapath
Single
Cycle
Control
The
control
unit
computes
the
control
signals
based
on
the
opcode
and
funct
fields
of
the
instruction
Instr
and
Instr
Figure
shows
the
entire
single
cycle
MIPS
processor
with
the
control
unit
attached
to
the
datapath
Most
of
the
control
information
comes
from
the
opcode
but
R
type
instructions
also
use
the
funct
field
to
determine
the
ALU
operation
Thus
we
will
simplify
our
design
by
factoring
the
control
unit
into
two
blocks
of
combinational
logic
as
shown
in
Figure
The
main
decoder
computes
most
of
the
outputs
from
the
opcode
It
also
determines
a
bit
ALUOp
signal
The
ALU
decoder
uses
this
ALUOp
signal
in
conjunction
with
the
funct
field
to
compute
ALUControl
The
meaning
of
the
ALUOp
signal
is
given
in
Table
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
Zero
PCSrc
CLK
ALUControl
x
x
ALU
Figure
Datapath
enhancements
for
beq
instruction
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
Opcode
Control
Unit
Funct
ALUControl
Main
Decoder
ALUOp
ALU
Decoder
RegWrite
Figure
Control
unit
internal
structure
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
ALU
Figure
Complete
single
cycle
MIPS
processor
Table
ALUOp
encoding
ALUOp
Meaning
add
subtract
look
at
funct
field
n
a
Table
is
a
truth
table
for
the
ALU
decoder
Recall
that
the
meanings
of
the
three
ALUControl
signals
were
given
in
Table
Because
ALUOp
is
never
the
truth
table
can
use
don
t
care
s
X
and
X
instead
of
and
to
simplify
the
logic
When
ALUOp
is
or
the
ALU
should
add
or
subtract
respectively
When
ALUOp
is
the
decoder
examines
the
funct
field
to
determine
the
ALUControl
Note
that
for
the
R
type
instructions
we
implement
the
first
two
bits
of
the
funct
field
are
always
so
we
may
ignore
them
to
simplify
the
decoder
The
control
signals
for
each
instruction
were
described
as
we
built
the
datapath
Table
is
a
truth
table
for
the
main
decoder
that
summarizes
the
control
signals
as
a
function
of
the
opcode
All
R
type
instructions
use
the
same
main
decoder
values
they
differ
only
in
the
ALU
decoder
output
Recall
that
for
instructions
that
do
not
write
to
the
register
file
e
g
sw
and
beq
the
RegDst
and
MemtoReg
control
signals
are
don
t
cares
X
the
address
and
data
to
the
register
write
port
do
not
matter
because
RegWrite
is
not
asserted
The
logic
for
the
decoder
can
be
designed
using
your
favorite
techniques
for
combinational
logic
design
Example
SINGLE
CYCLE
PROCESSOR
OPERATION
Determine
the
values
of
the
control
signals
and
the
portions
of
the
datapath
that
are
used
when
executing
an
or
instruction
Solution
Figure
illustrates
the
control
signals
and
flow
of
data
during
execution
of
the
or
instruction
The
PC
points
to
the
memory
location
holding
the
instruction
and
the
instruction
memory
fetches
this
instruction
The
main
flow
of
data
through
the
register
file
and
ALU
is
represented
with
a
dashed
blue
line
The
register
file
reads
the
two
source
operands
specified
by
Instr
and
Instr
SrcB
should
come
from
the
second
port
of
the
register
CHAPTER
SEVEN
Microarchitecture
Table
ALU
decoder
truth
table
ALUOp
Funct
ALUControl
X
add
X
X
subtract
X
add
add
X
sub
subtract
X
and
and
X
or
or
X
slt
set
less
than
Table
Main
decoder
truth
table
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
R
type
lw
sw
X
X
beq
X
X
file
not
SignImm
so
ALUSrc
must
be
or
is
an
R
type
instruction
so
ALUOp
is
indicating
that
ALUControl
should
be
determined
from
the
funct
field
to
be
Result
is
taken
from
the
ALU
so
MemtoReg
is
The
result
is
written
to
the
register
file
so
RegWrite
is
The
instruction
does
not
write
memory
so
MemWrite
The
selection
of
the
destination
register
is
also
shown
with
a
dashed
blue
line
The
destination
register
is
specified
in
the
rd
field
Instr
so
RegDst
The
updating
of
the
PC
is
shown
with
the
dashed
gray
line
The
instruction
is
not
a
branch
so
Branch
and
hence
PCSrc
is
also
The
PC
gets
its
next
value
from
PCPlus
Note
that
data
certainly
does
flow
through
the
nonhighlighted
paths
but
that
the
value
of
that
data
is
unimportant
for
this
instruction
For
example
the
immediate
is
sign
extended
and
data
is
read
from
memory
but
these
values
do
not
influence
the
next
state
of
the
system
More
Instructions
We
have
considered
a
limited
subset
of
the
full
MIPS
instruction
set
Adding
support
for
the
addi
and
j
instructions
illustrates
the
principle
of
how
to
handle
new
instructions
and
also
gives
us
a
sufficiently
rich
instruction
set
to
write
many
interesting
programs
We
will
see
that
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
ALU
PC
Figure
Control
signals
and
data
flow
while
executing
or
instruction
supporting
some
instructions
simply
requires
enhancing
the
main
decoder
whereas
supporting
others
also
requires
more
hardware
in
the
datapath
Example
addi
INSTRUCTION
The
add
immediate
instruction
addi
adds
the
value
in
a
register
to
the
immediate
and
writes
the
result
to
another
register
The
datapath
already
is
capable
of
this
task
Determine
the
necessary
changes
to
the
controller
to
support
addi
Solution
All
we
need
to
do
is
add
a
new
row
to
the
main
decoder
truth
table
showing
the
control
signal
values
for
addi
as
given
in
Table
The
result
should
be
written
to
the
register
file
so
RegWrite
The
destination
register
is
specified
in
the
rt
field
of
the
instruction
so
RegDst
SrcB
comes
from
the
immediate
so
ALUSrc
The
instruction
is
not
a
branch
nor
does
it
write
memory
so
Branch
MemWrite
The
result
comes
from
the
ALU
not
memory
so
MemtoReg
Finally
the
ALU
should
add
so
ALUOp
CHAPTER
SEVEN
Microarchitecture
Table
Main
decoder
truth
table
enhanced
to
support
addi
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
R
type
lw
sw
X
X
beq
X
X
addi
Example
j
INSTRUCTION
The
jump
instruction
j
writes
a
new
value
into
the
PC
The
two
least
significant
bits
of
the
PC
are
always
because
the
PC
is
word
aligned
i
e
always
a
multiple
of
The
next
bits
are
taken
from
the
jump
address
field
in
Instr
The
upper
four
bits
are
taken
from
the
old
value
of
the
PC
The
existing
datapath
lacks
hardware
to
compute
PC
in
this
fashion
Determine
the
necessary
changes
to
both
the
datapath
and
controller
to
handle
j
Solution
First
we
must
add
hardware
to
compute
the
next
PC
value
PC
in
the
case
of
a
j
instruction
and
a
multiplexer
to
select
this
next
PC
as
shown
in
Figure
The
new
multiplexer
uses
the
new
Jump
control
signal
Ch
Now
we
must
add
a
row
to
the
main
decoder
truth
table
for
the
j
instruction
and
a
column
for
the
Jump
signal
as
shown
in
Table
The
Jump
control
signal
is
for
the
j
instruction
and
for
all
others
j
does
not
write
the
register
file
or
memory
so
RegWrite
MemWrite
Hence
we
don
t
care
about
the
computation
done
in
the
datapath
and
RegDst
ALUSrc
Branch
MemtoReg
ALUOp
X
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
PCJump
Jump
ALU
Figure
Single
cycle
MIPS
datapath
enhanced
to
support
the
j
instruction
Table
Main
decoder
truth
table
enhanced
to
support
j
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
Jump
R
type
lw
sw
X
X
beq
X
X
addi
j
X
X
X
X
XX
Performance
Analysis
Each
instruction
in
the
single
cycle
processor
takes
one
clock
cycle
so
the
CPI
is
The
critical
path
for
the
lw
instruction
is
shown
in
Figure
with
a
heavy
dashed
blue
line
It
starts
with
the
PC
loading
a
new
address
on
the
rising
edge
of
the
clock
The
instruction
memory
reads
the
next
instruction
The
register
file
reads
SrcA
While
the
register
file
is
reading
the
immediate
field
is
sign
extended
and
selected
at
the
ALUSrc
multiplexer
to
determine
SrcB
The
ALU
adds
SrcA
and
SrcB
to
find
the
effective
address
The
data
memory
reads
from
this
address
The
MemtoReg
multiplexer
selects
ReadData
Finally
Result
must
setup
at
the
register
file
before
the
next
rising
clock
edge
so
that
it
can
be
properly
written
Hence
the
cycle
time
is
In
most
implementation
technologies
the
ALU
memory
and
register
file
accesses
are
substantially
slower
than
other
operations
Therefore
the
cycle
time
simplifies
to
The
numerical
values
of
these
times
will
depend
on
the
specific
implementation
technology
Tc
tpcq
PC
tmem
tRFread
tmux
tALU
tRFsetup
tALU
tmem
tmux
tRFsetup
Tc
tpcq
PC
tmem
max
tRFread
tsext
tmux
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
WE
CLK
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
A
RD
ALU
Sign
Extend
Figure
Critical
path
for
lw
instruction
Other
instructions
have
shorter
critical
paths
For
example
R
type
instructions
do
not
need
to
access
data
memory
However
we
are
disciplining
ourselves
to
synchronous
sequential
design
so
the
clock
period
is
constant
and
must
be
long
enough
to
accommodate
the
slowest
instruction
Example
SINGLE
CYCLE
PROCESSOR
PERFORMANCE
Ben
Bitdiddle
is
contemplating
building
the
single
cycle
MIPS
processor
in
a
nm
CMOS
manufacturing
process
He
has
determined
that
the
logic
elements
have
the
delays
given
in
Table
Help
him
compare
the
execution
time
for
a
program
with
billion
instructions
Solution
According
to
Equation
the
cycle
time
of
the
single
cycle
processor
is
Tc
ps
We
use
the
subscript
to
distinguish
it
from
subsequent
processor
designs
According
to
Equation
the
total
execution
time
is
T
instructions
cycle
instruction
s
cycle
seconds
Multicycle
Processor
Table
Delays
of
circuit
elements
Element
Parameter
Delay
ps
register
clk
to
Q
tpcq
register
setup
tsetup
multiplexer
tmux
ALU
tALU
memory
read
tmem
register
file
read
tRFread
register
file
setup
tRFsetup
MULTICYCLE
PROCESSOR
The
single
cycle
processor
has
three
primary
weaknesses
First
it
requires
a
clock
cycle
long
enough
to
support
the
slowest
instruction
lw
even
though
most
instructions
are
faster
Second
it
requires
three
adders
one
in
the
ALU
and
two
for
the
PC
logic
adders
are
relatively
expensive
circuits
especially
if
they
must
be
fast
And
third
it
has
separate
instruction
and
data
memories
which
may
not
be
realistic
Most
computers
have
a
single
large
memory
that
holds
both
instructions
and
data
and
that
can
be
read
and
written
Ch
The
multicycle
processor
addresses
these
weaknesses
by
breaking
an
instruction
into
multiple
shorter
steps
In
each
short
step
the
processor
can
read
or
write
the
memory
or
register
file
or
use
the
ALU
Different
instructions
use
different
numbers
of
steps
so
simpler
instructions
can
complete
faster
than
more
complex
ones
The
processor
needs
only
one
adder
this
adder
is
reused
for
different
purposes
on
various
steps
And
the
processor
uses
a
combined
memory
for
instructions
and
data
The
instruction
is
fetched
from
memory
on
the
first
step
and
data
may
be
read
or
written
on
later
steps
We
design
a
multicycle
processor
following
the
same
procedure
we
used
for
the
single
cycle
processor
First
we
construct
a
datapath
by
connecting
the
architectural
state
elements
and
memories
with
combinational
logic
But
this
time
we
also
add
nonarchitectural
state
elements
to
hold
intermediate
results
between
the
steps
Then
we
design
the
controller
The
controller
produces
different
signals
on
different
steps
during
execution
of
a
single
instruction
so
it
is
now
a
finite
state
machine
rather
than
combinational
logic
We
again
examine
how
to
add
new
instructions
to
the
processor
Finally
we
analyze
the
performance
of
the
multicycle
processor
and
compare
it
to
the
single
cycle
processor
Multicycle
Datapath
Again
we
begin
our
design
with
the
memory
and
architectural
state
of
the
MIPS
processor
shown
in
Figure
In
the
single
cycle
design
we
used
separate
instruction
and
data
memories
because
we
needed
to
read
the
instruction
memory
and
read
or
write
the
data
memory
all
in
one
cycle
Now
we
choose
to
use
a
combined
memory
for
both
instructions
and
data
This
is
more
realistic
and
it
is
feasible
because
we
can
read
the
instruction
in
one
cycle
then
read
or
write
the
data
in
a
separate
cycle
The
PC
and
register
file
remain
unchanged
We
gradually
build
the
datapath
by
adding
components
to
handle
each
step
of
each
instruction
The
new
connections
are
emphasized
in
black
or
blue
for
new
control
signals
whereas
the
hardware
that
has
already
been
studied
is
shown
in
gray
The
PC
contains
the
address
of
the
instruction
to
execute
The
first
step
is
to
read
this
instruction
from
instruction
memory
Figure
shows
that
the
PC
is
simply
connected
to
the
address
input
of
the
instruction
memory
The
instruction
is
read
and
stored
in
a
new
nonarchitectural
CHAPTER
SEVEN
Microarchitecture
CLK
A
RD
Instr
Data
Memory
PC
PC
WD
WE
CLK
EN
A
A
WD
RD
RD
WE
A
CLK
Register
File
Figure
State
elements
with
unified
instruction
data
memory
Instruction
Register
so
that
it
is
available
for
future
cycles
The
Instruction
Register
receives
an
enable
signal
called
IRWrite
that
is
asserted
when
it
should
be
updated
with
a
new
instruction
As
we
did
with
the
single
cycle
processor
we
will
work
out
the
datapath
connections
for
the
lw
instruction
Then
we
will
enhance
the
datapath
to
handle
the
other
instructions
For
a
lw
instruction
the
next
step
is
to
read
the
source
register
containing
the
base
address
This
register
is
specified
in
the
rs
field
of
the
instruction
Instr
These
bits
of
the
instruction
are
connected
to
one
of
the
address
inputs
A
of
the
register
file
as
shown
in
Figure
The
register
file
reads
the
register
onto
RD
This
value
is
stored
in
another
nonarchitectural
register
A
The
lw
instruction
also
requires
an
offset
The
offset
is
stored
in
the
immediate
field
of
the
instruction
Instr
and
must
be
signextended
to
bits
as
shown
in
Figure
The
bit
sign
extended
value
is
called
SignImm
To
be
consistent
we
might
store
SignImm
in
another
nonarchitectural
register
However
SignImm
is
a
combinational
function
of
Instr
and
will
not
change
while
the
current
instruction
is
being
processed
so
there
is
no
need
to
dedicate
a
register
to
hold
the
constant
value
The
address
of
the
load
is
the
sum
of
the
base
address
and
offset
We
use
an
ALU
to
compute
this
sum
as
shown
in
Figure
ALUControl
should
be
set
to
to
perform
an
addition
ALUResult
is
stored
in
a
nonarchitectural
register
called
ALUOut
The
next
step
is
to
load
the
data
from
the
calculated
address
in
the
memory
We
add
a
multiplexer
in
front
of
the
memory
to
choose
the
Multicycle
Processor
PC
Instr
CLK
EN
IRWrite
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
WD
WE
CLK
Figure
Fetch
instruction
from
memory
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
PC
Instr
CLK
WD
WE
CLK
CLK
A
EN
IRWrite
Figure
Read
source
operand
from
register
file
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
CLK
WD
WE
CLK
CLK
A
EN
IRWrite
Figure
Sign
extend
the
immediate
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl
WD
WE
CLK
CLK
A
CLK
EN
IRWrite
ALU
Figure
Add
base
address
to
offset
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
IorD
IRWrite
ALU
Figure
Load
data
from
memory
memory
address
Adr
from
either
the
PC
or
ALUOut
as
shown
in
Figure
The
multiplexer
select
signal
is
called
IorD
to
indicate
either
an
instruction
or
data
address
The
data
read
from
the
memory
is
stored
in
another
nonarchitectural
register
called
Data
Notice
that
the
address
multiplexer
permits
us
to
reuse
the
memory
during
the
lw
instruction
On
the
first
step
the
address
is
taken
from
the
PC
to
fetch
the
instruction
On
a
later
step
the
address
is
taken
from
ALUOut
to
load
the
data
Hence
IorD
must
have
different
values
on
different
steps
In
Section
we
develop
the
FSM
controller
that
generates
these
sequences
of
control
signals
Finally
the
data
is
written
back
to
the
register
file
as
shown
in
Figure
The
destination
register
is
specified
by
the
rt
field
of
the
instruction
Instr
While
all
this
is
happening
the
processor
must
update
the
program
counter
by
adding
to
the
old
PC
In
the
single
cycle
processor
a
separate
adder
was
needed
In
the
multicycle
processor
we
can
use
the
existing
ALU
on
one
of
the
steps
when
it
is
not
busy
To
do
so
we
must
insert
source
multiplexers
to
choose
the
PC
and
the
constant
as
ALU
inputs
as
shown
in
Figure
A
two
input
multiplexer
controlled
by
ALUSrcA
chooses
either
the
PC
or
register
A
as
SrcA
A
four
input
multiplexer
controlled
by
ALUSrcB
chooses
either
or
SignImm
as
SrcB
We
use
the
other
two
multiplexer
inputs
later
when
we
extend
the
datapath
to
handle
other
instructions
The
numbering
of
inputs
to
the
multiplexer
is
arbitrary
To
update
the
PC
the
ALU
adds
SrcA
PC
to
SrcB
and
the
result
is
written
into
the
program
counter
register
The
PCWrite
control
signal
enables
the
PC
register
to
be
written
only
on
certain
cycles
Multicycle
Processor
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
RegWrite
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
IorD
IRWrite
ALU
Figure
Write
data
back
to
register
file
This
completes
the
datapath
for
the
lw
instruction
Next
let
us
extend
the
datapath
to
also
handle
the
sw
instruction
Like
the
lw
instruction
the
sw
instruction
reads
a
base
address
from
port
of
the
register
file
and
sign
extends
the
immediate
The
ALU
adds
the
base
address
to
the
immediate
to
find
the
memory
address
All
of
these
functions
are
already
supported
by
existing
hardware
in
the
datapath
The
only
new
feature
of
sw
is
that
we
must
read
a
second
register
from
the
register
file
and
write
it
into
the
memory
as
shown
in
Figure
The
register
is
specified
in
the
rt
field
of
the
instruction
Instr
which
is
connected
to
the
second
port
of
the
register
file
When
the
register
is
read
it
is
stored
in
a
nonarchitectural
register
B
On
the
next
step
it
is
sent
to
the
write
data
port
WD
of
the
data
memory
to
be
written
The
memory
receives
an
additional
MemWrite
control
signal
to
indicate
that
the
write
should
occur
CHAPTER
SEVEN
Microarchitecture
PCWrite
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
RegWrite
ALUSrcA
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
EN
ALUSrcB
IorD
IRWrite
ALU
Figure
Increment
PC
by
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegWrite
ALUSrcA
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
EN
PCWrite
IorD
IRWrite
ALUSrcB
B
ALU
Figure
Enhanced
datapath
for
sw
instruction
Multicycle
Processor
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
PCWrite
IorD
IRWrite
ALUSrcB
ALU
Figure
Enhanced
datapath
for
R
type
instructions
For
R
type
instructions
the
instruction
is
again
fetched
and
the
two
source
registers
are
read
from
the
register
file
Another
input
of
the
SrcB
multiplexer
is
used
to
choose
register
B
as
the
second
source
register
for
the
ALU
as
shown
in
Figure
The
ALU
performs
the
appropriate
operation
and
stores
the
result
in
ALUOut
On
the
next
step
ALUOut
is
written
back
to
the
register
specified
by
the
rd
field
of
the
instruction
Instr
This
requires
two
new
multiplexers
The
MemtoReg
multiplexer
selects
whether
WD
comes
from
ALUOut
for
R
type
instructions
or
from
Data
for
lw
The
RegDst
instruction
selects
whether
the
destination
register
is
specified
in
the
rt
or
rd
field
of
the
instruction
For
the
beq
instruction
the
instruction
is
again
fetched
and
the
two
source
registers
are
read
from
the
register
file
To
determine
whether
the
registers
are
equal
the
ALU
subtracts
the
registers
and
examines
the
Zero
flag
Meanwhile
the
datapath
must
compute
the
next
value
of
the
PC
if
the
branch
is
taken
PC
PC
SignImm
In
the
single
cycle
processor
yet
another
adder
was
needed
to
compute
the
branch
address
In
the
multicycle
processor
the
ALU
can
be
reused
again
to
save
hardware
On
one
step
the
ALU
computes
PC
and
writes
it
back
to
the
program
counter
as
was
done
for
other
instructions
On
another
step
the
ALU
uses
this
updated
PC
value
to
compute
PC
SignImm
SignImm
is
left
shifted
by
to
multiply
it
by
as
shown
in
Figure
The
SrcB
multiplexer
chooses
this
value
and
adds
it
to
the
PC
This
sum
represents
the
destination
of
the
branch
and
is
stored
in
ALUOut
A
new
multiplexer
controlled
by
PCSrc
chooses
what
signal
should
be
sent
to
PC
The
program
counter
should
be
written
either
when
PCWrite
is
asserted
or
when
a
branch
is
taken
A
new
control
signal
Branch
indicates
that
the
beq
instruction
is
being
executed
The
branch
is
taken
if
Zero
is
also
asserted
Hence
the
datapath
computes
a
new
PC
write
Chap
enable
called
PCEn
which
is
TRUE
either
when
PCWrite
is
asserted
or
when
both
Branch
and
Zero
are
asserted
This
completes
the
design
of
the
multicycle
MIPS
processor
datapath
The
design
process
is
much
like
that
of
the
single
cycle
processor
in
that
hardware
is
systematically
connected
between
the
state
elements
to
handle
each
instruction
The
main
difference
is
that
the
instruction
is
executed
in
several
steps
Nonarchitectural
registers
are
inserted
to
hold
the
results
of
each
step
In
this
way
the
ALU
can
be
reused
several
times
saving
the
cost
of
extra
adders
Similarly
the
instructions
and
data
can
be
stored
in
one
shared
memory
In
the
next
section
we
develop
an
FSM
controller
to
deliver
the
appropriate
sequence
of
control
signals
to
the
datapath
on
each
step
of
each
instruction
Multicycle
Control
As
in
the
single
cycle
processor
the
control
unit
computes
the
control
signals
based
on
the
opcode
and
funct
fields
of
the
instruction
Instr
and
Instr
Figure
shows
the
entire
multicycle
MIPS
processor
with
the
control
unit
attached
to
the
datapath
The
datapath
is
shown
in
black
and
the
control
unit
is
shown
in
blue
As
in
the
single
cycle
processor
the
control
unit
is
partitioned
into
a
main
controller
and
an
ALU
decoder
as
shown
in
Figure
The
ALU
decoder
is
unchanged
and
follows
the
truth
table
of
Table
Now
however
the
main
controller
is
an
FSM
that
applies
the
proper
control
signals
on
the
proper
cycles
or
steps
The
sequence
of
control
signals
depends
on
the
instruction
being
executed
In
the
remainder
of
this
section
we
will
develop
the
FSM
state
transition
diagram
for
the
main
controller
CHAPTER
SEVEN
Microarchitecture
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
IorD
IRWrite
ALUSrcB
PCWrite
PCEn
ALU
Figure
Enhanced
datapath
for
beq
instruction
The
main
controller
produces
multiplexer
select
and
register
enable
signals
for
the
datapath
The
select
signals
are
MemtoReg
RegDst
IorD
PCSrc
ALUSrcB
and
ALUSrcA
The
enable
signals
are
IRWrite
MemWrite
PCWrite
Branch
and
RegWrite
To
keep
the
following
state
transition
diagrams
readable
only
the
relevant
control
signals
are
listed
Select
signals
are
listed
only
when
their
value
matters
otherwise
they
are
don
t
cares
Enable
signals
are
listed
only
when
they
are
asserted
otherwise
they
are
Multicycle
Processor
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
tsDgeR
Branch
MemWrite
geRotmeM
ALUSrcA
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
ULA
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
IRWrite
ALUSrcB
IorD
PCWrite
PCEn
Figure
Complete
multicycle
MIPS
processor
ALUSrcA
PCSrc
Branch
ALUSrcB
Opcode
Control
Unit
ALUControl
Funct
Main
Controller
FSM
ALUOp
ALU
Decoder
RegWrite
PCWrite
IorD
MemWrite
IRWrite
RegDst
MemtoReg
Register
Enables
Multiplexer
Selects
Figure
Control
unit
internal
structure
The
first
step
for
any
instruction
is
to
fetch
the
instruction
from
memory
at
the
address
held
in
the
PC
The
FSM
enters
this
state
on
reset
To
read
memory
IorD
so
the
address
is
taken
from
the
PC
IRWrite
is
asserted
to
write
the
instruction
into
the
instruction
register
IR
Meanwhile
the
PC
should
be
incremented
by
to
point
to
the
next
instruction
Because
the
ALU
is
not
being
used
for
anything
else
the
processor
can
use
it
to
compute
PC
at
the
same
time
that
it
fetches
the
instruction
ALUSrcA
so
SrcA
comes
from
the
PC
ALUSrcB
so
SrcB
is
the
constant
ALUOp
so
the
ALU
decoder
produces
ALUControl
to
make
the
ALU
add
To
update
the
PC
with
this
new
value
PCSrc
and
PCWrite
is
asserted
These
control
signals
are
shown
in
Figure
The
data
flow
on
this
step
is
shown
in
Figure
with
the
instruction
fetch
shown
using
the
dashed
blue
line
and
the
PC
increment
shown
using
the
dashed
gray
line
The
next
step
is
to
read
the
register
file
and
decode
the
instruction
The
register
file
always
reads
the
two
sources
specified
by
the
rs
and
rt
fields
of
the
instruction
Meanwhile
the
immediate
is
sign
extended
Decoding
involves
examining
the
opcode
of
the
instruction
to
determine
what
to
do
next
No
control
signals
are
necessary
to
decode
the
instruction
but
the
FSM
must
wait
cycle
for
the
reading
and
decoding
to
complete
as
shown
in
Figure
The
new
state
is
highlighted
in
blue
The
data
flow
is
shown
in
Figure
CHAPTER
SEVEN
Microarchitecture
Reset
S
Fetch
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
Figure
Fetch
ALU
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
RegDst
MemtoReg
SrcB
ALUResult
SrcA
ALUOut
Branch
MemWrite
ALUSrcA
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IRWrite
IorD
PCWrite
PCEn
X
X
Figure
Data
flow
during
the
fetch
step
Now
the
FSM
proceeds
to
one
of
several
possible
states
depending
on
the
opcode
If
the
instruction
is
a
memory
load
or
store
lw
or
sw
the
multicycle
processor
computes
the
address
by
adding
the
base
address
to
the
sign
extended
immediate
This
requires
ALUSrcA
to
select
register
A
and
ALUSrcB
to
select
SignImm
ALUOp
so
the
ALU
adds
The
effective
address
is
stored
in
the
ALUOut
register
for
use
on
the
next
step
This
FSM
step
is
shown
in
Figure
and
the
data
flow
is
shown
in
Figure
If
the
instruction
is
lw
the
multicycle
processor
must
next
read
data
from
memory
and
write
it
to
the
register
file
These
two
steps
are
shown
in
Figure
To
read
from
memory
IorD
to
select
the
memory
address
that
was
just
computed
and
saved
in
ALUOut
This
address
in
memory
is
read
and
saved
in
the
Data
register
during
step
S
On
the
next
step
S
Data
is
written
to
the
register
file
MemtoReg
to
select
Multicycle
Processor
Reset
S
Fetch
S
Decode
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
Figure
Decode
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IRWrite
IorD
PCWrite
PCEn
X
X
X
X
XX
XXX
X
ALU
Figure
Data
flow
during
the
decode
step
Data
and
RegDst
to
pull
the
destination
register
from
the
rt
field
of
the
instruction
RegWrite
is
asserted
to
perform
the
write
completing
the
lw
instruction
Finally
the
FSM
returns
to
the
initial
state
S
to
fetch
the
next
instruction
For
these
and
subsequent
steps
try
to
visualize
the
data
flow
on
your
own
From
state
S
if
the
instruction
is
sw
the
data
read
from
the
second
port
of
the
register
file
is
simply
written
to
memory
IorD
to
select
the
address
computed
in
S
and
saved
in
ALUOut
MemWrite
is
asserted
to
write
the
memory
Again
the
FSM
returns
to
S
to
fetch
the
next
instruction
The
added
step
is
shown
in
Figure
CHAPTER
SEVEN
Microarchitecture
ALUSrcA
Reset
S
Fetch
S
MemAdr
S
Decode
Op
LW
or
Op
SW
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
ALUOp
ALUSrcB
Figure
Memory
address
computation
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IRWrite
IorD
PCWrite
PCEn
X
X
X
X
ALU
Figure
Data
flow
during
memory
address
computation
If
the
opcode
indicates
an
R
type
instruction
the
multicycle
processor
must
calculate
the
result
using
the
ALU
and
write
that
result
to
the
register
file
Figure
shows
these
two
steps
In
S
the
instruction
is
executed
by
selecting
the
A
and
B
registers
ALUSrcA
ALUSrcB
and
performing
the
ALU
operation
indicated
by
the
funct
field
of
the
instruction
ALUOp
for
all
R
type
instructions
The
ALUResult
is
stored
in
ALUOut
In
S
ALUOut
is
written
to
the
register
file
RegDst
because
the
destination
register
is
specified
in
the
rd
field
of
the
instruction
MemtoReg
because
the
write
data
WD
comes
from
ALUOut
RegWrite
is
asserted
to
write
the
register
file
For
a
beq
instruction
the
processor
must
calculate
the
destination
address
and
compare
the
two
source
registers
to
determine
whether
the
branch
should
be
taken
This
requires
two
uses
of
the
ALU
and
hence
might
seem
to
demand
two
new
states
Notice
however
that
the
ALU
was
not
used
during
S
when
the
registers
were
being
read
The
processor
might
as
well
use
the
ALU
at
that
time
to
compute
the
destination
address
by
adding
the
incremented
PC
PC
to
SignImm
as
shown
in
Multicycle
Processor
IorD
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
Op
LW
or
Op
SW
Op
LW
S
Mem
Writeback
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
ALUSrcA
ALUOp
ALUSrcB
RegDst
MemtoReg
RegWrite
Figure
Memory
read
C
Figure
see
page
ALUSrcA
to
select
the
incremented
PC
ALUSrcB
to
select
SignImm
and
ALUOp
to
add
The
destination
address
is
stored
in
ALUOut
If
the
instruction
is
not
beq
the
computed
address
will
not
be
used
in
subsequent
cycles
but
its
computation
was
harmless
In
S
the
processor
compares
the
two
registers
by
subtracting
them
and
checking
to
determine
whether
the
result
is
If
it
is
the
processor
branches
to
the
address
that
was
just
computed
ALUSrcA
to
select
register
A
ALUSrcB
to
select
register
B
ALUOp
to
subtract
PCSrc
to
take
the
destination
address
from
ALUOut
and
Branch
to
update
the
PC
with
this
address
if
the
ALU
result
is
Putting
these
steps
together
Figure
shows
the
complete
main
controller
state
transition
diagram
for
the
multicycle
processor
see
page
Converting
it
to
hardware
is
a
straightforward
but
tedious
task
using
the
techniques
of
Chapter
Better
yet
the
FSM
can
be
coded
in
an
HDL
and
synthesized
using
the
techniques
of
Chapter
CHAPTER
SEVEN
Microarchitecture
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
Op
LW
or
Op
SW
Op
LW
Op
SW
S
Mem
Writeback
IRWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
PCWrite
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
Figure
Memory
write
Now
we
see
why
the
PCSrc
multiplexer
is
necessary
to
choose
PC
from
either
ALUResult
in
S
or
ALUOut
in
S
Ch
More
Instructions
As
we
did
in
Section
for
the
single
cycle
processor
let
us
now
extend
the
multicycle
processor
to
support
the
addi
and
j
instructions
The
next
two
examples
illustrate
the
general
design
process
to
support
new
instructions
Example
addi
INSTRUCTION
Modify
the
multicycle
processor
to
support
addi
Solution
The
datapath
is
already
capable
of
adding
registers
to
immediates
so
all
we
need
to
do
is
add
new
states
to
the
main
controller
FSM
for
addi
as
shown
in
Figure
see
page
The
states
are
similar
to
those
for
R
type
instructions
In
S
register
A
is
added
to
SignImm
ALUSrcA
ALUSrcB
ALUOp
and
the
result
ALUResult
is
stored
in
ALUOut
In
S
ALUOut
is
written
Multicycle
Processor
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
Op
LW
or
Op
SW
Op
LW
S
Mem
Writeback
S
MemWrite
S
Execute
S
ALU
Writeback
Op
R
type
Op
SW
PCWrite
IRWrite
IorD
PCSrc
ALUOp
ALUSrcB
AluSrcA
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
ALUSrcA
ALUOp
ALUSrcB
Figure
Execute
R
type
operation
to
the
register
specified
by
the
rt
field
of
the
instruction
RegDst
MemtoReg
RegWrite
asserted
The
astute
reader
may
notice
that
S
and
S
are
identical
and
could
be
merged
into
a
single
state
Example
j
INSTRUCTION
Modify
the
multicycle
processor
to
support
j
Solution
First
we
must
modify
the
datapath
to
compute
the
next
PC
value
in
the
case
of
a
j
instruction
Then
we
add
a
state
to
the
main
controller
to
handle
the
instruction
Figure
shows
the
enhanced
datapath
see
page
The
jump
destination
address
is
formed
by
left
shifting
the
bit
addr
field
of
the
instruction
by
two
bits
then
prepending
the
four
most
significant
bits
of
the
already
incremented
PC
The
PCSrc
multiplexer
is
extended
to
take
this
address
as
a
third
input
CHAPTER
SEVEN
Microarchitecture
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
PCWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
IorD
RegDst
MemtoReg
RegWrite
IorD
MemWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
PCSrc
Branch
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
RegDst
MemtoReg
RegWrite
S
Mem
Writeback
Figure
Branch
Figure
shows
the
enhanced
main
controller
see
page
The
new
state
S
simply
selects
PC
as
the
PCJump
value
PCSrc
and
writes
the
PC
Note
that
the
PCSrc
select
signal
is
extended
to
two
bits
in
S
and
S
as
well
Performance
Analysis
The
execution
time
of
an
instruction
depends
on
both
the
number
of
cycles
it
uses
and
the
cycle
time
Whereas
the
single
cycle
processor
performed
all
instructions
in
one
cycle
the
multicycle
processor
uses
varying
numbers
of
cycles
for
the
various
instructions
However
the
multicycle
processor
does
less
work
in
a
single
cycle
and
thus
has
a
shorter
cycle
time
The
multicycle
processor
requires
three
cycles
for
beq
and
j
instructions
four
cycles
for
sw
addi
and
R
type
instructions
and
five
cycles
for
lw
instructions
The
CPI
depends
on
the
relative
likelihood
that
each
instruction
is
used
Multicycle
Processor
IorD
ALUSrcA
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
S
Mem
Writeback
PCWrite
IRWrite
PCSrc
ALUOp
ALUSrcB
AluSrcA
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
ALUOp
ALUSrcB
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
Branch
PCSrc
ALUOp
ALUSrcB
Figure
Complete
multicycle
control
FSM
C
Example
MULTICYCLE
PROCESSOR
CPI
The
SPECINT
benchmark
consists
of
approximately
loads
stores
branches
jumps
and
R
type
instructions
Determine
the
average
CPI
for
this
benchmark
Solution
The
average
CPI
is
the
sum
over
each
instruction
of
the
CPI
for
that
instruction
multiplied
by
the
fraction
of
the
time
that
instruction
is
used
For
this
benchmark
Average
CPI
This
is
better
than
the
worst
case
CPI
of
which
would
be
required
if
all
instructions
took
the
same
time
CHAPTER
SEVEN
Microarchitecture
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
S
Mem
Writeback
Op
ADDI
S
ADDI
Execute
S
ADDI
Writeback
IorD
PCWrite
IRWrite
PCSrc
ALUOp
ALUSrcB
AluSrcA
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
Branch
PCSrc
ALUOp
ALUSrcB
Figure
Main
controller
states
for
addi
Data
from
Patterson
and
Hennessy
Computer
Organization
and
Design
rd
Edition
Morgan
Kaufmann
Recall
that
we
designed
the
multicycle
processor
so
that
each
cycle
involved
one
ALU
operation
memory
access
or
register
file
access
Let
us
assume
that
the
register
file
is
faster
than
the
memory
and
that
writing
memory
is
faster
than
reading
memory
Examining
the
datapath
reveals
two
possible
critical
paths
that
would
limit
the
cycle
time
The
numerical
values
of
these
times
will
depend
on
the
specific
implementation
technology
Example
PROCESSOR
PERFORMANCE
COMPARISON
Ben
Bitdiddle
is
wondering
whether
he
would
be
better
off
building
the
multicycle
processor
instead
of
the
single
cycle
processor
For
both
designs
he
plans
on
using
a
nm
CMOS
manufacturing
process
with
the
delays
given
in
Table
Help
him
compare
each
processor
s
execution
time
for
billion
instructions
from
the
SPECINT
benchmark
see
Example
Solution
According
to
Equation
the
cycle
time
of
the
multicycle
processor
is
Tc
ps
Using
the
CPI
of
from
Example
the
total
execution
time
is
T
instructions
cycles
instruction
s
cycle
seconds
According
to
Example
the
singlecycle
processor
had
a
cycle
time
of
Tc
ps
a
CPI
of
and
a
total
execution
time
of
seconds
One
of
the
original
motivations
for
building
a
multicycle
processor
was
to
avoid
making
all
instructions
take
as
long
as
the
slowest
one
Unfortunately
this
example
shows
that
the
multicycle
processor
is
slower
than
the
single
cycle
Tc
tpcq
tmux
max
tALU
tmux
tmem
tsetup
Multicycle
Processor
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ULA
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
IorD
IRWrite
ALUSrcB
PCWrite
PCEn
addr
PCJump
Figure
Multicycle
MIPS
datapath
enhanced
to
support
the
j
instruction
Ch
processor
given
the
assumptions
of
CPI
and
circuit
element
delays
The
fundamental
problem
is
that
even
though
the
slowest
instruction
lw
was
broken
into
five
steps
the
multicycle
processor
cycle
time
was
not
nearly
improved
fivefold
This
is
partly
because
not
all
of
the
steps
are
exactly
the
same
length
and
partly
because
the
ps
sequencing
overhead
of
the
register
clk
to
Q
and
setup
time
must
now
be
paid
on
every
step
not
just
once
for
the
entire
instruction
In
general
engineers
have
learned
that
it
is
difficult
to
exploit
the
fact
that
some
computations
are
faster
than
others
unless
the
differences
are
large
Compared
with
the
single
cycle
processor
the
multicycle
processor
is
likely
to
be
less
expensive
because
it
eliminates
two
adders
and
combines
the
instruction
and
data
memories
into
a
single
unit
It
does
however
require
five
nonarchitectural
registers
and
additional
multiplexers
CHAPTER
SEVEN
Microarchitecture
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
PCWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
IorD
RegDst
MemtoReg
RegWrite
IorD
MemWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
PCSrc
Branch
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
RegDst
MemtoReg
RegWrite
S
Mem
Writeback
ALUSrcA
ALUSrcB
ALUOp
RegDst
MemtoReg
RegWrite
Op
ADDI
S
ADDI
Execute
S
ADDI
Writeback
PCSrc
PCWrite
Op
J
S
Jump
Figure
Main
controller
state
for
j
PIPELINED
PROCESSOR
Pipelining
introduced
in
Section
is
a
powerful
way
to
improve
the
throughput
of
a
digital
system
We
design
a
pipelined
processor
by
subdividing
the
single
cycle
processor
into
five
pipeline
stages
Thus
five
instructions
can
execute
simultaneously
one
in
each
stage
Because
each
stage
has
only
one
fifth
of
the
entire
logic
the
clock
frequency
is
almost
five
times
faster
Hence
the
latency
of
each
instruction
is
ideally
unchanged
but
the
throughput
is
ideally
five
times
better
Microprocessors
execute
millions
or
billions
of
instructions
per
second
so
throughput
is
more
important
than
latency
Pipelining
introduces
some
overhead
so
the
throughput
will
not
be
quite
as
high
as
we
might
ideally
desire
but
pipelining
nevertheless
gives
such
great
advantage
for
so
little
cost
that
all
modern
high
performance
microprocessors
are
pipelined
Reading
and
writing
the
memory
and
register
file
and
using
the
ALU
typically
constitute
the
biggest
delays
in
the
processor
We
choose
five
pipeline
stages
so
that
each
stage
involves
exactly
one
of
these
slow
steps
Specifically
we
call
the
five
stages
Fetch
Decode
Execute
Memory
and
Writeback
They
are
similar
to
the
five
steps
that
the
multicycle
processor
used
to
perform
lw
In
the
Fetch
stage
the
processor
reads
the
instruction
from
instruction
memory
In
the
Decode
stage
the
processor
reads
the
source
operands
from
the
register
file
and
decodes
the
instruction
to
produce
the
control
signals
In
the
Execute
stage
the
processor
performs
a
computation
with
the
ALU
In
the
Memory
stage
the
processor
reads
or
writes
data
memory
Finally
in
the
Writeback
stage
the
processor
writes
the
result
to
the
register
file
when
applicable
Figure
shows
a
timing
diagram
comparing
the
single
cycle
and
pipelined
processors
Time
is
on
the
horizontal
axis
and
instructions
are
on
the
vertical
axis
The
diagram
assumes
the
logic
element
delays
from
Table
but
ignores
the
delays
of
multiplexers
and
registers
In
the
single
cycle
processor
Figure
a
the
first
instruction
is
read
from
memory
at
time
next
the
operands
are
read
from
the
register
file
and
then
the
ALU
executes
the
necessary
computation
Finally
the
data
memory
may
be
accessed
and
the
result
is
written
back
to
the
register
file
by
ps
The
second
instruction
begins
when
the
first
completes
Hence
in
this
diagram
the
single
cycle
processor
has
an
instruction
latency
of
ps
and
a
throughput
of
instruction
per
ps
billion
instructions
per
second
In
the
pipelined
processor
Figure
b
the
length
of
a
pipeline
stage
is
set
at
ps
by
the
slowest
stage
the
memory
access
in
the
Fetch
or
Memory
stage
At
time
the
first
instruction
is
fetched
from
memory
At
ps
the
first
instruction
enters
the
Decode
stage
and
Pipelined
Processor
Time
ps
Instr
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
a
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Instr
b
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Fetch
Instruction
Figure
Timing
diagrams
a
single
cycle
processor
b
pipelined
processor
a
second
instruction
is
fetched
At
ps
the
first
instruction
executes
the
second
instruction
enters
the
Decode
stage
and
a
third
instruction
is
fetched
And
so
forth
until
all
the
instructions
complete
The
instruction
latency
is
ps
The
throughput
is
instruction
per
ps
billion
instructions
per
second
Because
the
stages
are
not
perfectly
balanced
with
equal
amounts
of
logic
the
latency
is
slightly
longer
for
the
pipelined
than
for
the
single
cycle
processor
Similarly
the
throughput
is
not
quite
five
times
as
great
for
a
five
stage
pipeline
as
for
the
single
cycle
processor
Nevertheless
the
throughput
advantage
is
substantial
Figure
shows
an
abstracted
view
of
the
pipeline
in
operation
in
which
each
stage
is
represented
pictorially
Each
pipeline
stage
is
represented
with
its
major
component
instruction
memory
IM
register
file
RF
read
ALU
execution
data
memory
DM
and
register
file
writeback
to
illustrate
the
flow
of
instructions
through
the
pipeline
Reading
across
a
row
shows
the
clock
cycles
in
which
a
particular
instruction
is
in
each
stage
For
example
the
sub
instruction
is
fetched
in
cycle
and
executed
in
cycle
Reading
down
a
column
shows
what
the
various
pipeline
stages
are
doing
on
a
particular
cycle
For
example
in
cycle
the
or
instruction
is
being
fetched
from
instruction
memory
while
s
is
being
read
from
the
register
file
the
ALU
is
computing
t
AND
t
the
data
memory
is
idle
and
the
register
file
is
writing
a
sum
to
s
Stages
are
shaded
to
indicate
when
they
are
used
For
example
the
data
memory
is
used
by
lw
in
cycle
and
by
sw
in
cycle
The
instruction
memory
and
ALU
are
used
in
every
cycle
The
register
file
is
written
by
Pipelined
Processor
Time
cycles
lw
s
RF
RF
s
DM
RF
t
t
RF
s
DM
RF
s
s
RF
s
DM
RF
t
t
RF
s
DM
RF
s
RF
s
DM
RF
t
t
RF
s
DM
add
s
t
t
sub
s
s
s
and
s
t
t
sw
s
s
or
s
t
t
add
IM
IM
IM
IM
IM
IM
lw
sub
and
sw
or
Figure
Abstract
view
of
pipeline
in
operation
C
every
instruction
except
sw
We
assume
that
in
the
pipelined
processor
the
register
file
is
written
in
the
first
part
of
a
cycle
and
read
in
the
second
part
as
suggested
by
the
shading
This
way
data
can
be
written
and
read
back
within
a
single
cycle
A
central
challenge
in
pipelined
systems
is
handling
hazards
that
occur
when
the
results
of
one
instruction
are
needed
by
a
subsequent
instruction
before
the
former
instruction
has
completed
For
example
if
the
add
in
Figure
used
s
rather
than
t
a
hazard
would
occur
because
the
s
register
has
not
been
written
by
the
lw
by
the
time
it
is
read
by
the
add
This
section
explores
forwarding
stalls
and
flushes
as
methods
to
resolve
hazards
Finally
this
section
revisits
performance
analysis
considering
sequencing
overhead
and
the
impact
of
hazards
Pipelined
Datapath
The
pipelined
datapath
is
formed
by
chopping
the
single
cycle
datapath
into
five
stages
separated
by
pipeline
registers
Figure
a
shows
the
single
cycle
datapath
stretched
out
to
leave
room
for
the
pipeline
registers
Figure
b
shows
the
pipelined
datapath
formed
by
inserting
four
pipeline
registers
to
separate
the
datapath
into
five
stages
The
stages
and
their
boundaries
are
indicated
in
blue
Signals
are
given
a
suffix
F
D
E
M
or
W
to
indicate
the
stage
in
which
they
reside
The
register
file
is
peculiar
because
it
is
read
in
the
Decode
stage
and
written
in
the
Writeback
stage
It
is
drawn
in
the
Decode
stage
but
the
write
address
and
data
come
from
the
Writeback
stage
This
feedback
will
lead
to
pipeline
hazards
which
are
discussed
in
Section
One
of
the
subtle
but
critical
issues
in
pipelining
is
that
all
signals
associated
with
a
particular
instruction
must
advance
through
the
pipeline
in
unison
Figure
b
has
an
error
related
to
this
issue
Can
you
find
it
The
error
is
in
the
register
file
write
logic
which
should
operate
in
the
Writeback
stage
The
data
value
comes
from
ResultW
a
Writeback
stage
signal
But
the
address
comes
from
WriteRegE
an
Execute
stage
signal
In
the
pipeline
diagram
of
Figure
during
cycle
the
result
of
the
lw
instruction
would
be
incorrectly
written
to
register
s
rather
than
s
Figure
shows
a
corrected
datapath
The
WriteReg
signal
is
now
pipelined
along
through
the
Memory
and
Writeback
stages
so
it
remains
in
sync
with
the
rest
of
the
instruction
WriteRegW
and
ResultW
are
fed
back
together
to
the
register
file
in
the
Writeback
stage
The
astute
reader
may
notice
that
the
PC
logic
is
also
problematic
because
it
might
be
updated
with
a
Fetch
or
a
Memory
stage
signal
PCPlus
F
or
PCBranchM
This
control
hazard
will
be
fixed
in
Section
CHAPTER
SEVEN
Microarchitecture
C
Pipelined
Control
The
pipelined
processor
takes
the
same
control
signals
as
the
single
cycle
processor
and
therefore
uses
the
same
control
unit
The
control
unit
examines
the
opcode
and
funct
fields
of
the
instruction
in
the
Decode
stage
to
produce
the
control
signals
as
was
described
in
Section
These
control
signals
must
be
pipelined
along
with
the
data
so
that
they
remain
synchronized
with
the
instruction
The
entire
pipelined
processor
with
control
is
shown
in
Figure
RegWrite
must
be
pipelined
into
the
Writeback
stage
before
it
feeds
back
to
the
register
file
just
as
WriteReg
was
pipelined
in
Figure
Pipelined
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ALU
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
Zero
CLK
a
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RtE
RdE
ALUOutM
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchM
ResultW
PCPlus
F
PCPlus
E
ZeroM
CLK
CLK
WriteRegE
CLK
CLK
CLK
ALU
b
Fetch
Decode
Execute
Memory
Writeback
Figure
Single
cycle
and
pipelined
datapaths
Hazards
In
a
pipelined
system
multiple
instructions
are
handled
concurrently
When
one
instruction
is
dependent
on
the
results
of
another
that
has
not
yet
completed
a
hazard
occurs
CHAPTER
SEVEN
Microarchitecture
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RtE
RdE
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchM
WriteRegM
ResultW
PCPlus
F
PCPlus
E
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
ZeroM
PCSrcM
CLK
CLK
CLK
CLK
CLK
WriteRegW
ALUControlE
RegWriteE
RegWriteM
RegWriteW
MemtoRegE
MemtoRegM
MemtoRegW
MemWriteE
MemWriteM
BranchE
BranchM
RegDstE
ALUSrcE
WriteRegE
Figure
Pipelined
processor
with
control
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RtE
RdE
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchM
WriteRegM
ResultW
PCPlus
F
PCPlus
E
ZeroM
CLK
CLK
WriteRegE
WriteRegW
CLK
CLK
CLK
Fetch
Decode
Execute
Memory
Writeback
Figure
Corrected
pipelined
datapath
The
register
file
can
be
read
and
written
in
the
same
cycle
Let
us
assume
that
the
write
takes
place
during
the
first
half
of
the
cycle
and
the
read
takes
place
during
the
second
half
of
the
cycle
so
that
a
register
can
be
written
and
read
back
in
the
same
cycle
without
introducing
a
hazard
Figure
illustrates
hazards
that
occur
when
one
instruction
writes
a
register
s
and
subsequent
instructions
read
this
register
This
is
called
a
read
after
write
RAW
hazard
The
add
instruction
writes
a
result
into
s
in
the
first
half
of
cycle
However
the
and
instruction
reads
s
on
cycle
obtaining
the
wrong
value
The
or
instruction
reads
s
on
cycle
again
obtaining
the
wrong
value
The
sub
instruction
reads
s
in
the
second
half
of
cycle
obtaining
the
correct
value
which
was
written
in
the
first
half
of
cycle
Subsequent
instructions
also
read
the
correct
value
of
s
The
diagram
shows
that
hazards
may
occur
in
this
pipeline
when
an
instruction
writes
a
register
and
either
of
the
two
subsequent
instructions
read
that
register
Without
special
treatment
the
pipeline
will
compute
the
wrong
result
On
closer
inspection
however
observe
that
the
sum
from
the
add
instruction
is
computed
by
the
ALU
in
cycle
and
is
not
strictly
needed
by
the
and
instruction
until
the
ALU
uses
it
in
cycle
In
principle
we
should
be
able
to
forward
the
result
from
one
instruction
to
the
next
to
resolve
the
RAW
hazard
without
slowing
down
the
pipeline
In
other
situations
explored
later
in
this
section
we
may
have
to
stall
the
pipeline
to
give
time
for
a
result
to
be
computed
before
the
subsequent
instruction
uses
the
result
In
any
event
something
must
be
done
to
solve
hazards
so
that
the
program
executes
correctly
despite
the
pipelining
Hazards
are
classified
as
data
hazards
or
control
hazards
A
data
hazard
occurs
when
an
instruction
tries
to
read
a
register
that
has
not
yet
been
written
back
by
a
previous
instruction
A
control
hazard
occurs
when
the
decision
of
what
instruction
to
fetch
next
has
not
been
made
by
the
time
the
fetch
takes
place
In
the
remainder
of
this
section
we
will
Pipelined
Processor
Time
cycles
add
s
s
s
RF
s
s
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
add
or
sub
Figure
Abstract
pipeline
diagram
illustrating
hazards
enhance
the
pipelined
processor
with
a
hazard
unit
that
detects
hazards
and
handles
them
appropriately
so
that
the
processor
executes
the
program
correctly
Solving
Data
Hazards
with
Forwarding
Some
data
hazards
can
be
solved
by
forwarding
also
called
bypassing
a
result
from
the
Memory
or
Writeback
stage
to
a
dependent
instruction
in
the
Execute
stage
This
requires
adding
multiplexers
in
front
of
the
ALU
to
select
the
operand
from
either
the
register
file
or
the
Memory
or
Writeback
stage
Figure
illustrates
this
principle
In
cycle
s
is
forwarded
from
the
Memory
stage
of
the
add
instruction
to
the
Execute
stage
of
the
dependent
and
instruction
In
cycle
s
is
forwarded
from
the
Writeback
stage
of
the
add
instruction
to
the
Execute
stage
of
the
dependent
or
instruction
Forwarding
is
necessary
when
an
instruction
in
the
Execute
stage
has
a
source
register
matching
the
destination
register
of
an
instruction
in
the
Memory
or
Writeback
stage
Figure
modifies
the
pipelined
processor
to
support
forwarding
It
adds
a
hazard
detection
unit
and
two
forwarding
multiplexers
The
hazard
detection
unit
receives
the
two
source
registers
from
the
instruction
in
the
Execute
stage
and
the
destination
registers
from
the
instructions
in
the
Memory
and
Writeback
stages
It
also
receives
the
RegWrite
signals
from
the
Memory
and
Writeback
stages
to
know
whether
the
destination
register
will
actually
be
written
for
example
the
sw
and
beq
instructions
do
not
write
results
to
the
register
file
and
hence
do
not
need
to
have
their
results
forwarded
Note
that
the
RegWrite
signals
are
connected
by
name
In
other
words
rather
than
cluttering
up
the
diagram
with
long
wires
running
from
the
control
signals
at
the
top
to
the
hazard
unit
at
the
bottom
the
connections
are
indicated
by
a
short
stub
of
wire
labeled
with
the
control
signal
name
to
which
it
is
connected
CHAPTER
SEVEN
Microarchitecture
Time
cycles
add
s
s
s
RF
s
s
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
add
or
sub
Figure
Abstract
pipeline
diagram
illustrating
forwarding
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
MhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
ForwardAE
ForwardBE
RegWriteM
RegWriteW
drazaH
E
sulPCP
EhcnarB
MhcnarB
MoreZ
ALU
Figure
Pipelined
processor
with
forwarding
to
solve
hazards
The
hazard
detection
unit
computes
control
signals
for
the
forwarding
multiplexers
to
choose
operands
from
the
register
file
or
from
the
results
in
the
Memory
or
Writeback
stage
It
should
forward
from
a
stage
if
that
stage
will
write
a
destination
register
and
the
destination
register
matches
the
source
register
However
is
hardwired
to
and
should
never
be
forwarded
If
both
the
Memory
and
Writeback
stages
contain
matching
destination
registers
the
Memory
stage
should
have
priority
because
it
contains
the
more
recently
executed
instruction
In
summary
the
function
of
the
forwarding
logic
for
SrcA
is
given
below
The
forwarding
logic
for
SrcB
ForwardBE
is
identical
except
that
it
checks
rt
rather
than
rs
if
rsE
AND
rsE
WriteRegM
AND
RegWriteM
then
ForwardAE
else
if
rsE
AND
rsE
WriteRegW
AND
RegWriteW
then
ForwardAE
else
ForwardAE
Solving
Data
Hazards
with
Stalls
Forwarding
is
sufficient
to
solve
RAW
data
hazards
when
the
result
is
computed
in
the
Execute
stage
of
an
instruction
because
its
result
can
then
be
forwarded
to
the
Execute
stage
of
the
next
instruction
Unfortunately
the
lw
instruction
does
not
finish
reading
data
until
the
end
of
the
Memory
stage
so
its
result
cannot
be
forwarded
to
the
Execute
stage
of
the
next
instruction
We
say
that
the
lw
instruction
has
a
two
cycle
latency
because
a
dependent
instruction
cannot
use
its
result
until
two
cycles
later
Figure
shows
this
problem
The
lw
instruction
receives
data
from
memory
at
the
end
of
cycle
But
the
and
instruction
needs
that
data
as
a
source
operand
at
the
beginning
of
cycle
There
is
no
way
to
solve
this
hazard
with
forwarding
CHAPTER
SEVEN
Microarchitecture
Time
cycles
lw
s
RF
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
lw
or
sub
Trouble
Figure
Abstract
pipeline
diagram
illustrating
trouble
forwarding
from
lw
The
alternative
solution
is
to
stall
the
pipeline
holding
up
operation
until
the
data
is
available
Figure
shows
stalling
the
dependent
instruction
and
in
the
Decode
stage
and
enters
the
Decode
stage
in
cycle
and
stalls
there
through
cycle
The
subsequent
instruction
or
must
remain
in
the
Fetch
stage
during
both
cycles
as
well
because
the
Decode
stage
is
full
In
cycle
the
result
can
be
forwarded
from
the
Writeback
stage
of
lw
to
the
Execute
stage
of
and
In
cycle
source
s
of
the
or
instruction
is
read
directly
from
the
register
file
with
no
need
for
forwarding
Notice
that
the
Execute
stage
is
unused
in
cycle
Likewise
Memory
is
unused
in
Cycle
and
Writeback
is
unused
in
cycle
This
unused
stage
propagating
through
the
pipeline
is
called
a
bubble
and
it
behaves
like
a
nop
instruction
The
bubble
is
introduced
by
zeroing
out
the
Execute
stage
control
signals
during
a
Decode
stall
so
that
the
bubble
performs
no
action
and
changes
no
architectural
state
In
summary
stalling
a
stage
is
performed
by
disabling
the
pipeline
register
so
that
the
contents
do
not
change
When
a
stage
is
stalled
all
previous
stages
must
also
be
stalled
so
that
no
subsequent
instructions
are
lost
The
pipeline
register
directly
after
the
stalled
stage
must
be
cleared
to
prevent
bogus
information
from
propagating
forward
Stalls
degrade
performance
so
they
should
only
be
used
when
necessary
Figure
modifies
the
pipelined
processor
to
add
stalls
for
lw
data
dependencies
The
hazard
unit
examines
the
instruction
in
the
Execute
stage
If
it
is
lw
and
its
destination
register
rtE
matches
either
source
operand
of
the
instruction
in
the
Decode
stage
rsD
or
rtD
that
instruction
must
be
stalled
in
the
Decode
stage
until
the
source
operand
is
ready
Stalls
are
supported
by
adding
enable
inputs
EN
to
the
Fetch
and
Decode
pipeline
registers
and
a
synchronous
reset
clear
CLR
input
to
the
Execute
pipeline
register
When
a
lw
stall
occurs
StallD
and
StallF
Pipelined
Processor
Time
cycles
lw
s
RF
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
lw
or
sub
RF
s
s
IM
or
Stall
Figure
Abstract
pipeline
diagram
illustrating
stall
to
solve
hazards
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
MhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
drazaH
E
sulPCP
EhcnarB
MhcnarB
MoreZ
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallD
EN
EN
ALU
CLR
StallF
RegWriteW
Figure
Pipelined
processor
with
stalls
to
solve
lw
data
hazard
are
asserted
to
force
the
Decode
and
Fetch
stage
pipeline
registers
to
hold
their
old
values
FlushE
is
also
asserted
to
clear
the
contents
of
the
Execute
stage
pipeline
register
introducing
a
bubble
The
MemtoReg
signal
is
asserted
for
the
lw
instruction
Hence
the
logic
to
compute
the
stalls
and
flushes
is
lwstall
rsD
rtE
OR
rtD
rtE
AND
MemtoRegE
StallF
StallD
FlushE
lwstall
Solving
Control
Hazards
The
beq
instruction
presents
a
control
hazard
the
pipelined
processor
does
not
know
what
instruction
to
fetch
next
because
the
branch
decision
has
not
been
made
by
the
time
the
next
instruction
is
fetched
One
mechanism
for
dealing
with
the
control
hazard
is
to
stall
the
pipeline
until
the
branch
decision
is
made
i
e
PCSrc
is
computed
Because
the
decision
is
made
in
the
Memory
stage
the
pipeline
would
have
to
be
stalled
for
three
cycles
at
every
branch
This
would
severely
degrade
the
system
performance
An
alternative
is
to
predict
whether
the
branch
will
be
taken
and
begin
executing
instructions
based
on
the
prediction
Once
the
branch
decision
is
available
the
processor
can
throw
out
the
instructions
if
the
prediction
was
wrong
In
particular
suppose
that
we
predict
that
branches
are
not
taken
and
simply
continue
executing
the
program
in
order
If
the
branch
should
have
been
taken
the
three
instructions
following
the
branch
must
be
flushed
discarded
by
clearing
the
pipeline
registers
for
those
instructions
These
wasted
instruction
cycles
are
called
the
branch
misprediction
penalty
Figure
shows
such
a
scheme
in
which
a
branch
from
address
to
address
is
taken
The
branch
decision
is
not
made
until
cycle
by
which
point
the
and
or
and
sub
instructions
at
addresses
and
C
have
already
been
fetched
These
instructions
must
be
flushed
and
the
slt
instruction
is
fetched
from
address
in
cycle
This
is
somewhat
of
an
improvement
but
flushing
so
many
instructions
when
the
branch
is
taken
still
degrades
performance
We
could
reduce
the
branch
misprediction
penalty
if
the
branch
decision
could
be
made
earlier
Making
the
decision
simply
requires
comparing
the
values
of
two
registers
Using
a
dedicated
equality
comparator
is
much
faster
than
performing
a
subtraction
and
zero
detection
If
the
comparator
is
fast
enough
it
could
be
moved
back
into
the
Decode
stage
so
that
the
operands
are
read
from
the
register
file
and
compared
to
determine
the
next
PC
by
the
end
of
the
Decode
stage
Pipelined
Processor
Strictly
speaking
only
the
register
designations
RsE
RtE
and
RdE
and
the
control
signals
that
might
update
memory
or
architectural
state
RegWrite
MemWrite
and
Branch
need
to
be
cleared
as
long
as
these
signals
are
cleared
the
bubble
can
contain
random
data
that
has
no
effect
Figure
shows
the
pipeline
operation
with
the
early
branch
decision
being
made
in
cycle
In
cycle
the
and
instruction
is
flushed
and
the
slt
instruction
is
fetched
Now
the
branch
misprediction
penalty
is
reduced
to
only
one
instruction
rather
than
three
Figure
modifies
the
pipelined
processor
to
move
the
branch
decision
earlier
and
handle
control
hazards
An
equality
comparator
is
added
to
the
Decode
stage
and
the
PCSrc
AND
gate
is
moved
earlier
so
CHAPTER
SEVEN
Microarchitecture
Time
cycles
beq
t
t
RF
t
t
RF
DM
RF
s
s
RF
DM
RF
s
s
RF
DM
RF
s
s
RF
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
lw
or
sub
C
Flush
these
instructions
slt
t
s
s
RF
s
s
RF
DM
t
IM
slt
slt
Figure
Abstract
pipeline
diagram
illustrating
flushing
when
a
branch
is
taken
Time
cycles
beq
t
t
RF
t
t
RF
DM
RF
s
s
RF
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
lw
C
Flush
this
instruction
slt
t
s
s
RF
s
s
RF
DM
t
IM
slt
slt
Figure
Abstract
pipeline
diagram
illustrating
earlier
branch
decision
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
DhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
DcrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
drazaH
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallF
StallD
RegWriteW
CLR
EN
EN
ALU
CLR
Figure
Pipelined
processor
handling
branch
control
hazard
that
PCSrc
can
be
determined
in
the
Decoder
stage
rather
than
the
Memory
stage
The
PCBranch
adder
must
also
be
moved
into
the
Decode
stage
so
that
the
destination
address
can
be
computed
in
time
The
synchronous
clear
input
CLR
connected
to
PCSrcD
is
added
to
the
Decode
stage
pipeline
register
so
that
the
incorrectly
fetched
instruction
can
be
flushed
when
a
branch
is
taken
Unfortunately
the
early
branch
decision
hardware
introduces
a
new
RAW
data
hazard
Specifically
if
one
of
the
source
operands
for
the
branch
was
computed
by
a
previous
instruction
and
has
not
yet
been
written
into
the
register
file
the
branch
will
read
the
wrong
operand
value
from
the
register
file
As
before
we
can
solve
the
data
hazard
by
forwarding
the
correct
value
if
it
is
available
or
by
stalling
the
pipeline
until
the
data
is
ready
Figure
shows
the
modifications
to
the
pipelined
processor
needed
to
handle
the
Decode
stage
data
dependency
If
a
result
is
in
the
Writeback
stage
it
will
be
written
in
the
first
half
of
the
cycle
and
read
during
the
second
half
so
no
hazard
exists
If
the
result
of
an
ALU
instruction
is
in
the
Memory
stage
it
can
be
forwarded
to
the
equality
comparator
through
two
new
multiplexers
If
the
result
of
an
ALU
instruction
is
in
the
Execute
stage
or
the
result
of
a
lw
instruction
is
in
the
Memory
stage
the
pipeline
must
be
stalled
at
the
Decode
stage
until
the
result
is
ready
The
function
of
the
Decode
stage
forwarding
logic
is
given
below
ForwardAD
rsD
AND
rsD
WriteRegM
AND
RegWriteM
ForwardBD
rtD
AND
rtD
WriteRegM
AND
RegWriteM
The
function
of
the
stall
detection
logic
for
a
branch
is
given
below
The
processor
must
make
a
branch
decision
in
the
Decode
stage
If
either
of
the
sources
of
the
branch
depends
on
an
ALU
instruction
in
the
Execute
stage
or
on
a
lw
instruction
in
the
Memory
stage
the
processor
must
stall
until
the
sources
are
ready
branchstall
BranchD
AND
RegWriteE
AND
WriteRegE
rsD
OR
WriteRegE
rtD
OR
BranchD
AND
MemtoRegM
AND
WriteRegM
rsD
OR
WriteRegM
rtD
Now
the
processor
might
stall
due
to
either
a
load
or
a
branch
hazard
StallF
StallD
FlushE
lwstall
OR
branchstall
Hazard
Summary
In
summary
RAW
data
hazards
occur
when
an
instruction
depends
on
the
result
of
another
instruction
that
has
not
yet
been
written
into
the
register
file
The
data
hazards
can
be
resolved
by
forwarding
if
the
result
is
computed
soon
enough
otherwise
they
require
stalling
the
pipeline
until
the
result
is
available
Control
hazards
occur
when
the
decision
of
what
instruction
to
fetch
has
not
been
made
by
the
time
the
next
instruction
must
be
fetched
Control
hazards
are
solved
by
predicting
which
CHAPTER
SEVEN
Microarchitecture
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
omeM
yr
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
DhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
Control
Unit
DcrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
drazaH
ForwardAD
ForwardAE
ForwardBE
BranchD
ForwardBD
RegWriteE
RegWriteM
MemtoRegE
MemtoRegM
FlushE
CLR
CLR
EN
EN
ALU
StallF
StallD
RegWriteW
Figure
Pipelined
processor
handling
data
dependencies
for
branch
instructions
instruction
should
be
fetched
and
flushing
the
pipeline
if
the
prediction
is
later
determined
to
be
wrong
Moving
the
decision
as
early
as
possible
minimizes
the
number
of
instructions
that
are
flushed
on
a
misprediction
You
may
have
observed
by
now
that
one
of
the
challenges
of
designing
a
pipelined
processor
is
to
understand
all
the
possible
interactions
between
instructions
and
to
discover
all
the
hazards
that
may
exist
Figure
shows
the
complete
pipelined
processor
handling
all
of
the
hazards
More
Instructions
Supporting
new
instructions
in
the
pipelined
processor
is
much
like
supporting
them
in
the
single
cycle
processor
However
new
instructions
may
introduce
hazards
that
must
be
detected
and
solved
In
particular
supporting
addi
and
j
instructions
on
the
pipelined
processor
requires
enhancing
the
controller
exactly
as
was
described
in
Section
and
adding
a
jump
multiplexer
to
the
datapath
after
the
branch
multiplexer
Like
a
branch
the
jump
takes
place
in
the
Decode
stage
so
the
subsequent
instruction
in
the
Fetch
stage
must
be
flushed
Designing
this
flush
logic
is
left
as
Exercise
Performance
Analysis
The
pipelined
processor
ideally
would
have
a
CPI
of
because
a
new
instruction
is
issued
every
cycle
However
a
stall
or
a
flush
wastes
a
cycle
so
the
CPI
is
slightly
higher
and
depends
on
the
specific
program
being
executed
Example
PIPELINED
PROCESSOR
CPI
The
SPECINT
benchmark
considered
in
Example
consists
of
approximately
loads
stores
branches
jumps
and
R
type
instructions
Assume
that
of
the
loads
are
immediately
followed
by
an
instruction
that
uses
the
result
requiring
a
stall
and
that
one
quarter
of
the
branches
are
mispredicted
requiring
a
flush
Assume
that
jumps
always
flush
the
subsequent
instruction
Ignore
other
hazards
Compute
the
average
CPI
of
the
pipelined
processor
Solution
The
average
CPI
is
the
sum
over
each
instruction
of
the
CPI
for
that
instruction
multiplied
by
the
fraction
of
time
that
instruction
is
used
Loads
take
one
clock
cycle
when
there
is
no
dependency
and
two
cycles
when
the
processor
must
stall
for
a
dependency
so
they
have
a
CPI
of
Branches
take
one
clock
cycle
when
they
are
predicted
properly
and
two
when
they
are
not
so
they
have
a
CPI
of
Jumps
always
have
a
CPI
of
All
other
instructions
have
a
CPI
of
Hence
for
this
benchmark
Average
CPI
CHAPTER
SEVEN
Microarchitecture
EqualD
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RsE
RdE
ALUOutM
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchD
WriteRegM
ResultW
PCPlus
F
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
PCSrcD
CLK
CLK
CLK
CLK
CLK
WriteRegW
ALUControlE
ALU
RegWriteE
RegWriteM
RegWriteW
MemtoRegE
MemtoRegM
MemtoRegW
MemWriteE
MemWriteM
RegDstE
ALUSrcE
WriteRegE
SignImmD
RtE
RsD
RdD
RtD
Hazard
Unit
StallF
StallD
ForwardAD
ForwardBD
ForwardAE
ForwardBE
MemtoRegE
RegWriteE
RegWriteM
RegWriteW
BranchD
FlushE
EN
EN
CLR
CLR
Figure
Pipelined
processor
with
full
hazard
handling
We
can
determine
the
cycle
time
by
considering
the
critical
path
in
each
of
the
five
pipeline
stages
shown
in
Figure
Recall
that
the
register
file
is
written
in
the
first
half
of
the
Writeback
cycle
and
read
in
the
second
half
of
the
Decode
cycle
Therefore
the
cycle
time
of
the
Decode
and
Writeback
stages
is
twice
the
time
necessary
to
do
the
half
cycle
of
work
Example
PROCESSOR
PERFORMANCE
COMPARISON
Ben
Bitdiddle
needs
to
compare
the
pipelined
processor
performance
to
that
of
the
single
cycle
and
multicycle
processors
considered
in
Example
Most
of
the
logic
delays
were
given
in
Table
The
other
element
delays
are
ps
for
an
equality
comparator
ps
for
an
AND
gate
ps
for
a
register
file
write
and
ps
for
a
memory
write
Help
Ben
compare
the
execution
time
of
billion
instructions
from
the
SPECINT
benchmark
for
each
processor
Solution
According
to
Equation
the
cycle
time
of
the
pipelined
processor
is
Tc
max
ps
According
to
Equation
the
total
execution
time
is
T
instructions
cycles
instruction
s
cycle
seconds
This
compares
to
seconds
for
the
single
cycle
processor
and
seconds
for
the
multicycle
processor
The
pipelined
processor
is
substantially
faster
than
the
others
However
its
advantage
over
the
single
cycle
processor
is
nowhere
near
the
five
fold
speedup
one
might
hope
to
get
from
a
five
stage
pipeline
The
pipeline
hazards
introduce
a
small
CPI
penalty
More
significantly
the
sequencing
overhead
clk
to
Q
and
setup
times
of
the
registers
applies
to
every
pipeline
stage
not
just
once
to
the
overall
datapath
Sequencing
overhead
limits
the
benefits
one
can
hope
to
achieve
from
pipelining
The
careful
reader
might
observe
that
the
Decode
stage
is
substantially
slower
than
the
others
because
the
register
file
write
read
and
branch
comparison
must
all
happen
in
half
a
cycle
Perhaps
moving
the
branch
comparison
to
the
Decode
stage
was
not
such
a
good
idea
If
branches
were
resolved
in
the
Execute
stage
instead
the
CPI
would
increase
slightly
because
a
mispredict
would
flush
two
instructions
but
the
cycle
time
would
decrease
substantially
giving
an
overall
speedup
The
pipelined
processor
is
similar
in
hardware
requirements
to
the
single
cycle
processor
but
it
adds
a
substantial
number
of
pipeline
registers
along
with
multiplexers
and
control
logic
to
resolve
hazards
Tc
max
tpcq
tmem
tsetup
tRFread
tmux
teq
tAND
tmux
tsetup
tpcq
tmux
tmux
tALU
tsetup
tpcq
tmemwrite
tsetup
tpcq
tmux
tRFwrite
CHAPTER
SEVEN
Microarchitecture
Fetch
Decode
Execute
Memory
Writeback
Chap
HDL
REPRESENTATION
This
section
presents
HDL
code
for
the
single
cycle
MIPS
processor
supporting
all
of
the
instructions
discussed
in
this
chapter
including
addi
and
j
The
code
illustrates
good
coding
practices
for
a
moderately
complex
system
HDL
code
for
the
multicycle
processor
and
pipelined
processor
are
left
to
Exercises
and
In
this
section
the
instruction
and
data
memories
are
separated
from
the
main
processor
and
connected
by
address
and
data
busses
This
is
more
realistic
because
most
real
processors
have
external
memory
It
also
illustrates
how
the
processor
can
communicate
with
the
outside
world
The
processor
is
composed
of
a
datapath
and
a
controller
The
controller
in
turn
is
composed
of
the
main
decoder
and
the
ALU
decoder
Figure
shows
a
block
diagram
of
the
single
cycle
MIPS
processor
interfaced
to
external
memories
The
HDL
code
is
partitioned
into
several
sections
Section
provides
HDL
for
the
single
cycle
processor
datapath
and
controller
Section
presents
the
generic
building
blocks
such
as
registers
and
multiplexers
that
are
used
by
any
microarchitecture
Section
HDL
Representation
Opcode
Controller
Funct
Datapath
A
RD
Instruction
Memory
PC
Instr
A
RD
Data
Memory
WD
WE
CLK
ALUOut
WriteData
ReadData
ALUControl
Branch
MemtoReg
ALUSrc
RegDst
MemWrite
Main
Decoder
ALUOp
ALU
Decoder
RegWrite
CLK
Reset
MIPS
Processor
External
Memory
PCSrc
Zero
Figure
MIPS
single
cycle
processor
interfaced
to
external
memory
introduces
the
testbench
and
external
memories
The
HDL
is
available
in
electronic
form
on
the
this
book
s
Web
site
see
the
preface
Single
Cycle
Processor
The
main
modules
of
the
single
cycle
MIPS
processor
module
are
given
in
the
following
HDL
examples
CHAPTER
SEVEN
Microarchitecture
Verilog
module
mips
input
clk
reset
output
pc
input
instr
output
memwrite
output
aluout
writedata
input
readdata
wire
memtoreg
branch
alusrc
regdst
regwrite
jump
wire
alucontrol
controller
c
instr
instr
zero
memtoreg
memwrite
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
datapath
dp
clk
reset
memtoreg
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
zero
pc
instr
aluout
writedata
readdata
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mips
is
single
cycle
MIPS
processor
port
clk
reset
in
STD
LOGIC
pc
out
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
memwrite
out
STD
LOGIC
aluout
writedata
out
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mips
is
component
controller
port
op
funct
in
STD
LOGIC
VECTOR
downto
zero
in
STD
LOGIC
memtoreg
memwrite
out
STD
LOGIC
pcsrc
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
alucontrol
out
STD
LOGIC
VECTOR
downto
end
component
component
datapath
port
clk
reset
in
STD
LOGIC
memtoreg
pcsrc
in
STD
LOGIC
alusrc
regdst
in
STD
LOGIC
regwrite
jump
in
STD
LOGIC
alucontrol
in
STD
LOGIC
VECTOR
downto
zero
out
STD
LOGIC
pc
buffer
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
aluout
writedata
buffer
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
component
signal
memtoreg
alusrc
regdst
regwrite
jump
pcsrc
STD
LOGIC
signal
zero
STD
LOGIC
signal
alucontrol
STD
LOGIC
VECTOR
downto
begin
cont
controller
port
map
instr
downto
instr
downto
zero
memtoreg
memwrite
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
dp
datapath
port
map
clk
reset
memtoreg
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
zero
pc
instr
aluout
writedata
readdata
end
HDL
Example
SINGLE
CYCLE
MIPS
PROCESSOR
HDL
Representation
Verilog
module
controller
input
op
funct
input
zero
output
memtoreg
memwrite
output
pcsrc
alusrc
output
regdst
regwrite
output
jump
output
alucontrol
wire
aluop
wire
branch
maindec
md
op
memtoreg
memwrite
branch
alusrc
regdst
regwrite
jump
aluop
aludec
ad
funct
aluop
alucontrol
assign
pcsrc
branch
zero
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
controller
is
single
cycle
control
decoder
port
op
funct
in
STD
LOGIC
VECTOR
downto
zero
in
STD
LOGIC
memtoreg
memwrite
out
STD
LOGIC
pcsrc
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
alucontrol
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
controller
is
component
maindec
port
op
in
STD
LOGIC
VECTOR
downto
memtoreg
memwrite
out
STD
LOGIC
branch
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
aluop
out
STD
LOGIC
VECTOR
downto
end
component
component
aludec
port
funct
in
STD
LOGIC
VECTOR
downto
aluop
in
STD
LOGIC
VECTOR
downto
alucontrol
out
STD
LOGIC
VECTOR
downto
end
component
signal
aluop
STD
LOGIC
VECTOR
downto
signal
branch
STD
LOGIC
begin
md
maindec
port
map
op
memtoreg
memwrite
branch
alusrc
regdst
regwrite
jump
aluop
ad
aludec
port
map
funct
aluop
alucontrol
pcsrc
branch
and
zero
end
HDL
Example
CONTROLLER
Verilog
module
maindec
input
op
output
memtoreg
memwrite
output
branch
alusrc
output
regdst
regwrite
output
jump
output
aluop
reg
controls
assign
regwrite
regdst
alusrc
branch
memwrite
memtoreg
jump
aluop
controls
always
case
op
b
controls
b
Rtyp
b
controls
b
LW
b
controls
b
SW
b
controls
b
BEQ
b
controls
b
ADDI
b
controls
b
J
default
controls
bxxxxxxxxx
endcase
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
maindec
is
main
control
decoder
port
op
in
STD
LOGIC
VECTOR
downto
memtoreg
memwrite
out
STD
LOGIC
branch
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
aluop
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
maindec
is
signal
controls
STD
LOGIC
VECTOR
downto
begin
process
op
begin
case
op
is
when
controls
Rtyp
when
controls
LW
when
controls
SW
when
controls
BEQ
when
controls
ADDI
when
controls
J
when
others
controls
illegal
op
end
case
end
process
regwrite
controls
regdst
controls
alusrc
controls
branch
controls
memwrite
controls
memtoreg
controls
jump
controls
aluop
controls
downto
end
HDL
Example
MAIN
DECODER
Verilog
module
aludec
input
funct
input
aluop
output
reg
alucontrol
always
case
aluop
b
alucontrol
b
add
b
alucontrol
b
sub
default
case
funct
RTYPE
b
alucontrol
b
ADD
b
alucontrol
b
SUB
b
alucontrol
b
AND
b
alucontrol
b
OR
b
alucontrol
b
SLT
default
alucontrol
bxxx
endcase
endcase
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
aludec
is
ALU
control
decoder
port
funct
in
STD
LOGIC
VECTOR
downto
aluop
in
STD
LOGIC
VECTOR
downto
alucontrol
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
aludec
is
begin
process
aluop
funct
begin
case
aluop
is
when
alucontrol
add
for
b
sb
addi
when
alucontrol
sub
for
beq
when
others
case
funct
is
R
type
instructions
when
alucontrol
add
when
alucontrol
sub
when
alucontrol
and
when
alucontrol
or
when
alucontrol
slt
when
others
alucontrol
end
case
end
case
end
process
end
HDL
Example
ALU
DECODER
Chapter
qxd
P
Verilog
module
datapath
input
clk
reset
input
memtoreg
pcsrc
input
alusrc
regdst
input
regwrite
jump
input
alucontrol
output
zero
output
pc
input
instr
output
aluout
writedata
input
readdata
wire
writereg
wire
pcnext
pcnextbr
pcplus
pcbranch
wire
signimm
signimmsh
wire
srca
srcb
wire
result
next
PC
logic
flopr
pcreg
clk
reset
pcnext
pc
adder
pcadd
pc
b
pcplus
sl
immsh
signimm
signimmsh
adder
pcadd
pcplus
signimmsh
pcbranch
mux
pcbrmux
pcplus
pcbranch
pcsrc
pcnextbr
mux
pcmux
pcnextbr
pcplus
instr
b
jump
pcnext
register
file
logic
regfile
rf
clk
regwrite
instr
instr
writereg
result
srca
writedata
mux
wrmux
instr
instr
regdst
writereg
mux
resmux
aluout
readdata
memtoreg
result
signext
se
instr
signimm
ALU
logic
mux
srcbmux
writedata
signimm
alusrc
srcb
alu
alu
srca
srcb
alucontrol
aluout
zero
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
ARITH
all
entity
datapath
is
MIPS
datapath
port
clk
reset
in
STD
LOGIC
memtoreg
pcsrc
in
STD
LOGIC
alusrc
regdst
in
STD
LOGIC
regwrite
jump
in
STD
LOGIC
alucontrol
in
STD
LOGIC
VECTOR
downto
zero
out
STD
LOGIC
pc
buffer
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
aluout
writedata
buffer
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
datapath
is
component
alu
port
a
b
in
STD
LOGIC
VECTOR
downto
alucontrol
in
STD
LOGIC
VECTOR
downto
result
buffer
STD
LOGIC
VECTOR
downto
zero
out
STD
LOGIC
end
component
component
regfile
port
clk
in
STD
LOGIC
we
in
STD
LOGIC
ra
ra
wa
in
STD
LOGIC
VECTOR
downto
wd
in
STD
LOGIC
VECTOR
downto
rd
rd
out
STD
LOGIC
VECTOR
downto
end
component
component
adder
port
a
b
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
component
component
sl
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
component
component
signext
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
component
component
flopr
generic
width
integer
port
clk
reset
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
width
downto
q
out
STD
LOGIC
VECTOR
width
downto
end
component
component
mux
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
component
signal
writereg
STD
LOGIC
VECTOR
downto
signal
pcjump
pcnext
pcnextbr
pcplus
pcbranch
STD
LOGIC
VECTOR
downto
signal
signimm
signimmsh
STD
LOGIC
VECTOR
downto
signal
srca
srcb
result
STD
LOGIC
VECTOR
downto
begin
next
PC
logic
pcjump
pcplus
downto
instr
downto
pcreg
flopr
generic
map
port
map
clk
reset
pcnext
pc
pcadd
adder
port
map
pc
X
pcplus
immsh
sl
port
map
signimm
signimmsh
pcadd
adder
port
map
pcplus
signimmsh
pcbranch
pcbrmux
mux
generic
map
port
map
pcplus
pcbranch
pcsrc
pcnextbr
pcmux
mux
generic
map
port
map
pcnextbr
pcjump
jump
pcnext
register
file
logic
rf
regfile
port
map
clk
regwrite
instr
downto
instr
downto
writereg
result
srca
writedata
wrmux
mux
generic
map
port
map
instr
downto
instr
downto
regdst
writereg
resmux
mux
generic
map
port
map
aluout
readdata
memtoreg
result
se
signext
port
map
instr
downto
signimm
ALU
logic
srcbmux
mux
generic
map
port
map
writedata
signimm
alusrc
srcb
mainalu
alu
port
map
srca
srcb
alucontrol
aluout
zero
end
HDL
Example
DATAPATH
Ch
Verilog
module
regfile
input
clk
input
we
input
ra
ra
wa
input
wd
output
rd
rd
reg
rf
three
ported
register
file
read
two
ports
combinationally
write
third
port
on
rising
edge
of
clock
register
hardwired
to
always
posedge
clk
if
we
rf
wa
wd
assign
rd
ra
rf
ra
assign
rd
ra
rf
ra
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
regfile
is
three
port
register
file
port
clk
in
STD
LOGIC
we
in
STD
LOGIC
ra
ra
wa
in
STD
LOGIC
VECTOR
downto
wd
in
STD
LOGIC
VECTOR
downto
rd
rd
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
regfile
is
type
ramtype
is
array
downto
of
STD
LOGIC
VECTOR
downto
signal
mem
ramtype
begin
three
ported
register
file
read
two
ports
combinationally
write
third
port
on
rising
edge
of
clock
process
clk
begin
if
clk
event
and
clk
then
if
we
then
mem
CONV
INTEGER
wa
wd
end
if
end
if
end
process
process
ra
ra
begin
if
conv
integer
ra
then
rd
X
register
holds
else
rd
mem
CONV
INTEGER
ra
end
if
if
conv
integer
ra
then
rd
X
else
rd
mem
CONV
INTEGER
ra
end
if
end
process
end
HDL
Example
REGISTER
FILE
Verilog
module
adder
input
a
b
output
y
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
adder
is
adder
port
a
b
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
adder
is
begin
y
a
b
end
HDL
Example
ADDER
Generic
Building
Blocks
This
section
contains
generic
building
blocks
that
may
be
useful
in
any
MIPS
microarchitecture
including
a
register
file
adder
left
shift
unit
sign
extension
unit
resettable
flip
flop
and
multiplexer
The
HDL
for
the
ALU
is
left
to
Exercise
Verilog
module
sl
input
a
output
y
shift
left
by
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
sl
is
shift
left
by
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
sl
is
begin
y
a
downto
end
HDL
Example
LEFT
SHIFT
MULTIPLY
BY
Verilog
module
signext
input
a
output
y
assign
y
a
a
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
signext
is
sign
extender
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
signext
is
begin
y
X
a
when
a
else
X
ffff
a
end
HDL
Example
SIGN
EXTENSION
Verilog
module
flopr
parameter
WIDTH
input
clk
reset
input
WIDTH
d
output
reg
WIDTH
q
always
posedge
clk
posedge
reset
if
reset
q
else
q
d
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
ARITH
all
entity
flopr
is
flip
flop
with
synchronous
reset
generic
width
integer
port
clk
reset
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
width
downto
q
out
STD
LOGIC
VECTOR
width
downto
end
architecture
asynchronous
of
flopr
is
begin
process
clk
reset
begin
if
reset
then
q
CONV
STD
LOGIC
VECTOR
width
elsif
clk
event
and
clk
then
q
d
end
if
end
process
end
HDL
Example
RESETTABLE
FLIP
FLOP
HDL
Representation
CHAPTER
SEVEN
Microarchitecture
Verilog
module
mux
parameter
WIDTH
input
WIDTH
d
d
input
s
output
WIDTH
y
assign
y
s
d
d
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
two
input
multiplexer
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
architecture
behave
of
mux
is
begin
y
d
when
s
else
d
end
HDL
Example
MULTIPLEXER
mipstest
asm
David
Harris
hmc
edu
November
Test
the
MIPS
processor
add
sub
and
or
slt
addi
lw
sw
beq
j
If
successful
it
should
write
the
value
to
address
Assembly
Description
Address
Machine
main
addi
initialize
addi
initialize
c
addi
initialize
fff
or
or
c
e
and
and
add
a
beq
end
shouldn
t
be
taken
a
a
slt
c
a
beq
around
should
be
taken
addi
shouldn
t
happen
around
slt
e
a
add
c
sub
e
sw
ac
lw
c
j
end
should
be
taken
c
addi
shouldn
t
happen
end
sw
write
adr
ac
Figure
Assembly
and
machine
code
for
MIPS
test
program
c
fff
e
a
a
a
a
e
a
e
ac
c
ac
Figure
Contents
of
memfile
dat
Testbench
The
MIPS
testbench
loads
a
program
into
the
memories
The
program
in
Figure
exercises
all
of
the
instructions
by
performing
a
computation
that
should
produce
the
correct
answer
only
if
all
of
the
instructions
are
functioning
properly
Specifically
the
program
will
write
the
value
to
address
if
it
runs
correctly
and
is
unlikely
to
do
so
if
the
hardware
is
buggy
This
is
an
example
of
ad
hoc
testing
HDL
Representation
The
machine
code
is
stored
in
a
hexadecimal
file
called
memfile
dat
see
Figure
which
is
loaded
by
the
testbench
during
simulation
The
file
consists
of
the
machine
code
for
the
instructions
one
instruction
per
line
The
testbench
top
level
MIPS
module
and
external
memory
HDL
code
are
given
in
the
following
examples
The
memories
in
this
example
hold
words
each
Verilog
module
testbench
reg
clk
reg
reset
wire
writedata
dataadr
wire
memwrite
instantiate
device
to
be
tested
top
dut
clk
reset
writedata
dataadr
memwrite
initialize
test
initial
begin
reset
reset
end
generate
clock
to
sequence
tests
always
begin
clk
clk
end
check
results
always
negedge
clk
begin
if
memwrite
begin
if
dataadr
writedata
begin
display
Simulation
succeeded
stop
end
else
if
dataadr
begin
display
Simulation
failed
stop
end
end
end
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
testbench
is
end
architecture
test
of
testbench
is
component
top
port
clk
reset
in
STD
LOGIC
writedata
dataadr
out
STD
LOGIC
VECTOR
downto
memwrite
out
STD
LOGIC
end
component
signal
writedata
dataadr
STD
LOGIC
VECTOR
downto
signal
clk
reset
memwrite
STD
LOGIC
begin
instantiate
device
to
be
tested
dut
top
port
map
clk
reset
writedata
dataadr
memwrite
Generate
clock
with
ns
period
process
begin
clk
wait
for
ns
clk
wait
for
ns
end
process
Generate
reset
for
first
two
clock
cycles
process
begin
reset
wait
for
ns
reset
wait
end
process
check
that
gets
written
to
address
at
end
of
program
process
clk
begin
if
clk
event
and
clk
and
memwrite
then
if
conv
integer
dataadr
and
conv
integer
writedata
then
report
Simulation
succeeded
elsif
dataadr
then
report
Simulation
failed
end
if
end
if
end
process
end
HDL
Example
MIPS
TESTBENCH
Verilog
module
top
input
clk
reset
output
writedata
dataadr
output
memwrite
wire
pc
instr
readdata
instantiate
processor
and
memories
mips
mips
clk
reset
pc
instr
memwrite
dataadr
writedata
readdata
imem
imem
pc
instr
dmem
dmem
clk
memwrite
dataadr
writedata
readdata
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
top
is
top
level
design
for
testing
port
clk
reset
in
STD
LOGIC
writedata
dataadr
buffer
STD
LOGIC
VECTOR
downto
memwrite
buffer
STD
LOGIC
end
architecture
test
of
top
is
component
mips
port
clk
reset
in
STD
LOGIC
pc
out
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
memwrite
out
STD
LOGIC
aluout
writedata
out
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
component
component
imem
port
a
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
component
component
dmem
port
clk
we
in
STD
LOGIC
a
wd
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
component
signal
pc
instr
readdata
STD
LOGIC
VECTOR
downto
begin
instantiate
processor
and
memories
mips
mips
port
map
clk
reset
pc
instr
memwrite
dataadr
writedata
readdata
imem
imem
port
map
pc
downto
instr
dmem
dmem
port
map
clk
memwrite
dataadr
writedata
readdata
end
HDL
Example
MIPS
TOP
LEVEL
MODULE
module
dmem
input
clk
we
input
a
wd
output
rd
reg
RAM
assign
rd
RAM
a
word
aligned
always
posedge
clk
if
we
RAM
a
wd
endmodule
library
IEEE
use
IEEE
STD
LOGIC
all
use
STD
TEXTIO
all
use
IEEE
STD
LOGIC
UNSIGNED
all
use
IEEE
STD
LOGIC
ARITH
all
entity
dmem
is
data
memory
port
clk
we
in
STD
LOGIC
a
wd
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
dmem
is
begin
process
is
type
ramtype
is
array
downto
of
STD
LOGIC
VECTOR
downto
variable
mem
ramtype
begin
read
or
write
memory
loop
if
clk
event
and
clk
then
if
we
then
mem
CONV
INTEGER
a
downto
wd
end
if
end
if
rd
mem
CONV
INTEGER
a
downto
wait
on
clk
a
end
loop
end
process
end
HDL
Example
MIPS
DATA
MEMORY
EXCEPTIONS
Section
introduced
exceptions
which
cause
unplanned
changes
in
the
flow
of
a
program
In
this
section
we
enhance
the
multicycle
processor
to
support
two
types
of
exceptions
undefined
instructions
Verilog
module
imem
input
a
output
rd
reg
RAM
initial
begin
readmemh
memfile
dat
RAM
end
assign
rd
RAM
a
word
aligned
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
STD
TEXTIO
all
use
IEEE
STD
LOGIC
UNSIGNED
all
use
IEEE
STD
LOGIC
ARITH
all
entity
imem
is
instruction
memory
port
a
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
imem
is
begin
process
is
file
mem
file
TEXT
variable
L
line
variable
ch
character
variable
index
result
integer
type
ramtype
is
array
downto
of
STD
LOGIC
VECTOR
downto
variable
mem
ramtype
begin
initialize
memory
from
file
for
i
in
to
loop
set
all
contents
low
mem
conv
integer
i
CONV
STD
LOGIC
VECTOR
end
loop
index
FILE
OPEN
mem
file
C
mips
memfile
dat
READ
MODE
while
not
endfile
mem
file
loop
readline
mem
file
L
result
for
i
in
to
loop
read
L
ch
if
ch
and
ch
then
result
result
character
pos
ch
character
pos
elsif
a
ch
and
ch
f
then
result
result
character
pos
ch
character
pos
a
else
report
Format
error
on
line
integer
image
index
severity
error
end
if
end
loop
mem
index
CONV
STD
LOGIC
VECTOR
result
index
index
end
loop
read
memory
loop
rd
mem
CONV
INTEGER
a
wait
on
a
end
loop
end
process
end
HDL
Example
MIPS
INSTRUCTION
MEMORY
and
arithmetic
overflow
Supporting
exceptions
in
other
microarchitectures
follows
similar
principles
As
described
in
Section
when
an
exception
takes
place
the
processor
copies
the
PC
to
the
EPC
register
and
stores
a
code
in
the
Cause
register
indicating
the
source
of
the
exception
Exception
causes
include
x
for
undefined
instructions
and
x
for
overflow
see
Table
The
processor
then
jumps
to
the
exception
handler
at
memory
address
x
The
exception
handler
is
code
that
responds
to
the
exception
It
is
part
of
the
operating
system
Also
as
discussed
in
Section
the
exception
registers
are
part
of
Coprocessor
a
portion
of
the
MIPS
processor
that
is
used
for
system
functions
Coprocessor
defines
up
to
special
purpose
registers
including
Cause
and
EPC
The
exception
handler
may
use
the
mfc
move
from
coprocessor
instruction
to
copy
these
special
purpose
registers
into
a
general
purpose
register
in
the
register
file
the
Cause
register
is
Coprocessor
register
and
EPC
is
register
To
handle
exceptions
we
must
add
EPC
and
Cause
registers
to
the
datapath
and
extend
the
PCSrc
multiplexer
to
accept
the
exception
handler
address
as
shown
in
Figure
The
two
new
registers
have
write
enables
EPCWrite
and
CauseWrite
to
store
the
PC
and
exception
cause
when
an
exception
takes
place
The
cause
is
generated
by
a
multiplexer
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
ALU
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IorD
IRWrite
PCWrite
PCEn
jump
PCJump
x
Overflow
CLK
EN
EPCWrite
CLK
EN
CauseWrite
IntCause
x
x
EPC
Cause
Figure
Datapath
supporting
overflow
and
undefined
instruction
exceptions
Exceptions
that
selects
the
appropriate
code
for
the
exception
The
ALU
must
also
generate
an
overflow
signal
as
was
discussed
in
Section
To
support
the
mfc
instruction
we
also
add
a
way
to
select
the
Coprocessor
registers
and
write
them
to
the
register
file
as
shown
in
Figure
The
mfc
instruction
specifies
the
Coprocessor
register
by
Instr
in
this
diagram
only
the
Cause
and
EPC
registers
are
supported
We
add
another
input
to
the
MemtoReg
multiplexer
to
select
the
value
from
Coprocessor
The
modified
controller
is
shown
in
Figure
The
controller
receives
the
overflow
flag
from
the
ALU
It
generates
three
new
control
signals
one
to
write
the
EPC
a
second
to
write
the
Cause
register
and
a
third
to
select
the
Cause
It
also
includes
two
new
states
to
support
the
two
exceptions
and
another
state
to
handle
mfc
If
the
controller
receives
an
undefined
instruction
one
that
it
does
not
know
how
to
handle
it
proceeds
to
S
saves
the
PC
in
EPC
writes
x
to
the
Cause
register
and
jumps
to
the
exception
handler
Similarly
if
the
controller
detects
arithmetic
overflow
on
an
add
or
sub
instruction
it
proceeds
to
S
saves
the
PC
in
EPC
writes
x
Strictly
speaking
the
ALU
should
assert
overflow
only
for
add
and
sub
not
for
other
ALU
instructions
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
ALU
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IorD
IRWrite
PCWrite
PCEn
jump
PCJump
x
CLK
EN
EPCWrite
CLK
EN
CauseWrite
IntCause
x
x
EPC
Cause
Overflow
C
Figure
Datapath
supporting
mfc
CHAPTER
SEVEN
Microarchitecture
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
IorD
RegDst
MemtoReg
RegWrite
IorD
MemWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
PCSrc
Branch
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
RegDst
MemtoReg
RegWrite
S
Mem
Writeback
ALUSrcA
ALUSrcB
ALUOp
RegDst
MemtoReg
RegWrite
Op
ADDI
S
ADDI
Execute
S
ADDI
Writeback
PCSrc
PCWrite
Op
J
S
Jump
Overflow
Overflow
S
Overflow
PCSrc
PCWrite
IntCause
CauseWrite
EPCWrite
Op
others
PCSrc
PCWrite
IntCause
CauseWrite
EPCWrite
S
Undefined
Memtoreg
RegWrite
Op
MFC
S
MFC
PCWrite
RegDst
Figure
Controller
supporting
exceptions
and
mfc
in
the
Cause
register
and
jumps
to
the
exception
handler
Note
that
when
an
exception
occurs
the
instruction
is
discarded
and
the
register
file
is
not
written
When
a
mfc
instruction
is
decoded
the
processor
goes
to
S
and
writes
the
appropriate
Coprocessor
register
to
the
main
register
file
ADVANCED
MICROARCHITECTURE
High
performance
microprocessors
use
a
wide
variety
of
techniques
to
run
programs
faster
Recall
that
the
time
required
to
run
a
program
is
proportional
to
the
period
of
the
clock
and
to
the
number
of
clock
cycles
per
instruction
CPI
Thus
to
increase
performance
we
would
like
to
speed
up
the
clock
and
or
reduce
the
CPI
This
section
surveys
some
existing
speedup
techniques
The
implementation
details
become
quite
complex
so
we
will
focus
on
the
concepts
Hennessy
Patterson
s
Computer
Architecture
text
is
a
definitive
reference
if
you
want
to
fully
understand
the
details
Every
to
years
advances
in
CMOS
manufacturing
reduce
transistor
dimensions
by
in
each
direction
doubling
the
number
of
transistors
that
can
fit
on
a
chip
A
manufacturing
process
is
characterized
by
its
feature
size
which
indicates
the
smallest
transistor
that
can
be
reliably
built
Smaller
transistors
are
faster
and
generally
consume
less
power
Thus
even
if
the
microarchitecture
does
not
change
the
clock
frequency
can
increase
because
all
the
gates
are
faster
Moreover
smaller
transistors
enable
placing
more
transistors
on
a
chip
Microarchitects
use
the
additional
transistors
to
build
more
complicated
processors
or
to
put
more
processors
on
a
chip
Unfortunately
power
consumption
increases
with
the
number
of
transistors
and
the
speed
at
which
they
operate
see
Section
Power
consumption
is
now
an
essential
concern
Microprocessor
designers
have
a
challenging
task
juggling
the
trade
offs
among
speed
power
and
cost
for
chips
with
billions
of
transistors
in
some
of
the
most
complex
systems
that
humans
have
ever
built
Deep
Pipelines
Aside
from
advances
in
manufacturing
the
easiest
way
to
speed
up
the
clock
is
to
chop
the
pipeline
into
more
stages
Each
stage
contains
less
logic
so
it
can
run
faster
This
chapter
has
considered
a
classic
five
stage
pipeline
but
to
stages
are
now
commonly
used
The
maximum
number
of
pipeline
stages
is
limited
by
pipeline
hazards
sequencing
overhead
and
cost
Longer
pipelines
introduce
more
dependencies
Some
of
the
dependencies
can
be
solved
by
forwarding
but
others
require
stalls
which
increase
the
CPI
The
pipeline
registers
between
each
stage
have
sequencing
overhead
from
their
setup
time
and
clk
to
Q
delay
as
well
as
clock
skew
This
sequencing
overhead
makes
adding
more
pipeline
stages
give
diminishing
returns
Finally
adding
more
stages
increases
the
cost
because
of
the
extra
pipeline
registers
and
hardware
required
to
handle
hazards
Advanced
Microarchitecture
Example
DEEP
PIPELINES
Consider
building
a
pipelined
processor
by
chopping
up
the
single
cycle
processor
into
N
stages
N
The
single
cycle
processor
has
a
propagation
delay
of
ps
through
the
combinational
logic
The
sequencing
overhead
of
a
register
is
ps
Assume
that
the
combinational
delay
can
be
arbitrarily
divided
into
any
number
of
stages
and
that
pipeline
hazard
logic
does
not
increase
the
delay
The
five
stage
pipeline
in
Example
has
a
CPI
of
Assume
that
each
additional
stage
increases
the
CPI
by
because
of
branch
mispredictions
and
other
pipeline
hazards
How
many
pipeline
stages
should
be
used
to
make
the
processor
execute
programs
as
fast
as
possible
Solution
If
the
ps
combinational
logic
delay
is
divided
into
N
stages
and
each
stage
also
pays
ps
of
sequencing
overhead
for
its
pipeline
register
the
cycle
time
is
Tc
N
The
CPI
is
N
The
time
per
instruction
or
instruction
time
is
the
product
of
the
cycle
time
and
the
CPI
Figure
plots
the
cycle
time
and
instruction
time
versus
the
number
of
stages
The
instruction
time
has
a
minimum
of
ps
at
N
stages
This
minimum
is
only
slightly
better
than
the
ps
per
instruction
achieved
with
a
six
stage
pipeline
In
the
late
s
and
early
s
microprocessors
were
marketed
largely
based
on
clock
frequency
Tc
This
pushed
microprocessors
to
use
very
deep
pipelines
to
stages
on
the
Pentium
to
maximize
the
clock
frequency
even
if
the
benefits
for
overall
performance
were
questionable
Power
is
proportional
to
clock
frequency
and
also
increases
with
the
number
of
pipeline
registers
so
now
that
power
consumption
is
so
important
pipeline
depths
are
decreasing
CHAPTER
SEVEN
Microarchitecture
Tc
Instruction
Time
Number
of
pipeline
stages
Time
ps
Figure
Cycle
time
and
instruction
time
versus
the
number
of
pipeline
stages
Branch
Prediction
An
ideal
pipelined
processor
would
have
a
CPI
of
The
branch
misprediction
penalty
is
a
major
reason
for
increased
CPI
As
pipelines
get
deeper
branches
are
resolved
later
in
the
pipeline
Thus
the
branch
misprediction
penalty
gets
larger
because
all
the
instructions
issued
after
the
mispredicted
branch
must
be
flushed
To
address
this
problem
most
pipelined
processors
use
a
branch
predictor
to
guess
whether
the
branch
should
be
taken
Recall
that
our
pipeline
from
Section
simply
predicted
that
branches
are
never
taken
Some
branches
occur
when
a
program
reaches
the
end
of
a
loop
e
g
a
for
or
while
statement
and
branches
back
to
repeat
the
loop
Loops
tend
to
be
executed
many
times
so
these
backward
branches
are
usually
taken
The
simplest
form
of
branch
prediction
checks
the
direction
of
the
branch
and
predicts
that
backward
branches
should
be
taken
This
is
called
static
branch
prediction
because
it
does
not
depend
on
the
history
of
the
program
Forward
branches
are
difficult
to
predict
without
knowing
more
about
the
specific
program
Therefore
most
processors
use
dynamic
branch
predictors
which
use
the
history
of
program
execution
to
guess
whether
a
branch
should
be
taken
Dynamic
branch
predictors
maintain
a
table
of
the
last
several
hundred
or
thousand
branch
instructions
that
the
processor
has
executed
The
table
sometimes
called
a
branch
target
buffer
includes
the
destination
of
the
branch
and
a
history
of
whether
the
branch
was
taken
To
see
the
operation
of
dynamic
branch
predictors
consider
the
following
loop
code
from
Code
Example
The
loop
repeats
times
and
the
beq
out
of
the
loop
is
taken
only
on
the
last
time
add
s
sum
add
s
i
addi
t
t
for
beq
s
t
done
if
i
branch
to
done
add
s
s
s
sum
sum
i
addi
s
s
increment
i
j
for
done
A
one
bit
dynamic
branch
predictor
remembers
whether
the
branch
was
taken
the
last
time
and
predicts
that
it
will
do
the
same
thing
the
next
time
While
the
loop
is
repeating
it
remembers
that
the
beq
was
not
taken
last
time
and
predicts
that
it
should
not
be
taken
next
time
This
is
a
correct
prediction
until
the
last
branch
of
the
loop
when
the
branch
does
get
taken
Unfortunately
if
the
loop
is
run
again
the
branch
predictor
remembers
that
the
last
branch
was
taken
Therefore
Advanced
Microarchitecture
it
incorrectly
predicts
that
the
branch
should
be
taken
when
the
loop
is
first
run
again
In
summary
a
bit
branch
predictor
mispredicts
the
first
and
last
branches
of
a
loop
A
bit
dynamic
branch
predictor
solves
this
problem
by
having
four
states
strongly
taken
weakly
taken
weakly
not
taken
and
strongly
not
taken
as
shown
in
Figure
When
the
loop
is
repeating
it
enters
the
strongly
not
taken
state
and
predicts
that
the
branch
should
not
be
taken
next
time
This
is
correct
until
the
last
branch
of
the
loop
which
is
taken
and
moves
the
predictor
to
the
weakly
not
taken
state
When
the
loop
is
first
run
again
the
branch
predictor
correctly
predicts
that
the
branch
should
not
be
taken
and
reenters
the
strongly
not
taken
state
In
summary
a
bit
branch
predictor
mispredicts
only
the
last
branch
of
a
loop
As
one
can
imagine
branch
predictors
may
be
used
to
track
even
more
history
of
the
program
to
increase
the
accuracy
of
predictions
Good
branch
predictors
achieve
better
than
accuracy
on
typical
programs
The
branch
predictor
operates
in
the
Fetch
stage
of
the
pipeline
so
that
it
can
determine
which
instruction
to
execute
on
the
next
cycle
When
it
predicts
that
the
branch
should
be
taken
the
processor
fetches
the
next
instruction
from
the
branch
destination
stored
in
the
branch
target
buffer
By
keeping
track
of
both
branch
and
jump
destinations
in
the
branch
target
buffer
the
processor
can
also
avoid
flushing
the
pipeline
during
jump
instructions
Superscalar
Processor
A
superscalar
processor
contains
multiple
copies
of
the
datapath
hardware
to
execute
multiple
instructions
simultaneously
Figure
shows
a
block
diagram
of
a
two
way
superscalar
processor
that
fetches
and
executes
two
instructions
per
cycle
The
datapath
fetches
two
instructions
at
a
time
from
the
instruction
memory
It
has
a
six
ported
register
file
to
read
four
source
operands
and
write
two
results
back
in
each
cycle
It
also
contains
two
ALUs
and
a
two
ported
data
memory
to
execute
the
two
instructions
at
the
same
time
CHAPTER
SEVEN
Microarchitecture
A
scalar
processor
acts
on
one
piece
of
data
at
a
time
A
vector
processor
acts
on
several
pieces
of
data
with
a
single
instruction
A
superscalar
processor
issues
several
instructions
at
a
time
each
of
which
operates
on
one
piece
of
data
Our
MIPS
pipelined
processor
is
a
scalar
processor
Vector
processors
were
popular
for
supercomputers
in
the
s
and
s
because
they
efficiently
handled
the
long
vectors
of
data
common
in
scientific
computations
Modern
highperformance
microprocessors
are
superscalar
because
issuing
several
independent
instructions
is
more
flexible
than
processing
vectors
However
modern
processors
also
include
hardware
to
handle
short
vectors
of
data
that
are
common
in
multimedia
and
graphics
applications
These
are
called
single
instruction
multiple
data
SIMD
units
strongly
taken
predict
taken
weakly
taken
predict
taken
weakly
not
taken
predict
not
taken
strongly
not
taken
predict
not
taken
taken
taken
taken
taken
taken
taken
taken
taken
Figure
bit
branch
predictor
state
transition
diagram
Figure
shows
a
pipeline
diagram
illustrating
the
two
way
superscalar
processor
executing
two
instructions
on
each
cycle
For
this
program
the
processor
has
a
CPI
of
Designers
commonly
refer
to
the
reciprocal
of
the
CPI
as
the
instructions
per
cycle
or
IPC
This
processor
has
an
IPC
of
on
this
program
Executing
many
instructions
simultaneously
is
difficult
because
of
dependencies
For
example
Figure
shows
a
pipeline
diagram
running
a
program
with
data
dependencies
The
dependencies
in
the
code
are
shown
in
blue
The
add
instruction
is
dependent
on
t
which
is
produced
by
the
lw
instruction
so
it
cannot
be
issued
at
the
same
time
as
lw
Indeed
the
add
instruction
stalls
for
yet
another
cycle
so
that
lw
can
forward
t
to
add
in
cycle
The
other
dependencies
between
Advanced
Microarchitecture
CLK
CLK
CLK
CLK
A
RD
A
A
A
RD
WD
WD
A
A
A
RD
RD
RD
Instruction
Memory
Register
File
Data
Memory
PC
CLK
A
A
WD
ALUs
WD
RD
RD
Figure
Superscalar
datapath
Time
cycles
RF
s
RF
t
DM
IM
lw
add
lw
t
s
add
t
s
s
sub
t
s
s
and
t
s
s
or
t
s
s
sw
s
s
t
s
s
RF
s
s
RF
t
DM
IM
sub
and
t
s
s
RF
s
s
RF
t
DM
IM
or
sw
s
s
Figure
Abstract
view
of
a
superscalar
pipeline
in
operation
CHAPTER
SEVEN
Microarchitecture
Stall
Time
cycles
RF
s
RF
t
DM
IM
lw
lw
t
s
add
t
t
s
sub
t
s
s
and
t
s
t
sw
s
t
s
t
add
RF
s
t
RF
t
DM
RF
t
s
RF
t
DM
IM
and
IM
or
and
sub
s
s
t
RF
t
RF
DM
sw
IM
s
s
s
s
s
t
or
t
s
s
IM
or
RF
sub
and
and
based
on
t
and
between
or
and
sw
based
on
t
are
handled
by
forwarding
results
produced
in
one
cycle
to
be
consumed
in
the
next
This
program
also
given
below
requires
five
cycles
to
issue
six
instructions
for
an
IPC
of
lw
t
s
add
t
t
s
sub
t
s
s
and
t
s
t
or
t
s
s
sw
s
t
Recall
that
parallelism
comes
in
temporal
and
spatial
forms
Pipelining
is
a
case
of
temporal
parallelism
Multiple
execution
units
is
a
case
of
spatial
parallelism
Superscalar
processors
exploit
both
forms
of
parallelism
to
squeeze
out
performance
far
exceeding
that
of
our
singlecycle
and
multicycle
processors
Commercial
processors
may
be
three
four
or
even
six
way
superscalar
They
must
handle
control
hazards
such
as
branches
as
well
as
data
hazards
Unfortunately
real
programs
have
many
dependencies
so
wide
superscalar
processors
rarely
fully
utilize
all
of
the
execution
units
Moreover
the
large
number
of
execution
units
and
complex
forwarding
networks
consume
vast
amounts
of
circuitry
and
power
Figure
Program
with
data
dependencies
Out
of
Order
Processor
To
cope
with
the
problem
of
dependencies
an
out
of
order
processor
looks
ahead
across
many
instructions
to
issue
or
begin
executing
independent
instructions
as
rapidly
as
possible
The
instructions
can
be
issued
in
a
different
order
than
that
written
by
the
programmer
as
long
as
dependencies
are
honored
so
that
the
program
produces
the
intended
result
Consider
running
the
same
program
from
Figure
on
a
two
way
superscalar
out
of
order
processor
The
processor
can
issue
up
to
two
instructions
per
cycle
from
anywhere
in
the
program
as
long
as
dependencies
are
observed
Figure
shows
the
data
dependencies
and
the
operation
of
the
processor
The
classifications
of
dependencies
as
RAW
and
WAR
will
be
discussed
shortly
The
constraints
on
issuing
instructions
are
described
below
Cycle
The
lw
instruction
issues
The
add
sub
and
and
instructions
are
dependent
on
lw
by
way
of
t
so
they
cannot
issue
yet
However
the
or
instruction
is
independent
so
it
also
issues
Advanced
Microarchitecture
Time
cycles
RF
s
RF
t
DM
IM
lw
lw
t
s
add
t
t
s
sub
t
s
s
and
t
s
t
sw
s
t
or
s
s
t
RF
t
RF
DM
sw
s
or
t
s
s
IM
RF
s
t
RF
t
DM
IM
add
sub
s
s
t
Two
cycle
latency
between
lW
and
use
of
t
RAW
WAR
RAW
RF
t
s
RF
DM
and
IM
t
RAW
Figure
Out
of
order
execution
of
a
program
with
dependencies
C
Cycle
Remember
that
there
is
a
two
cycle
latency
between
when
a
lw
instruction
issues
and
when
a
dependent
instruction
can
use
its
result
so
add
cannot
issue
yet
because
of
the
t
dependence
sub
writes
t
so
it
cannot
issue
before
add
lest
add
receive
the
wrong
value
of
t
and
is
dependent
on
sub
Only
the
sw
instruction
issues
Cycle
On
cycle
t
is
available
so
add
issues
sub
issues
simultaneously
because
it
will
not
write
t
until
after
add
consumes
t
Cycle
The
and
instruction
issues
t
is
forwarded
from
sub
to
and
The
out
of
order
processor
issues
the
six
instructions
in
four
cycles
for
an
IPC
of
The
dependence
of
add
on
lw
by
way
of
t
is
a
read
after
write
RAW
hazard
add
must
not
read
t
until
after
lw
has
written
it
This
is
the
type
of
dependency
we
are
accustomed
to
handling
in
the
pipelined
processor
It
inherently
limits
the
speed
at
which
the
program
can
run
even
if
infinitely
many
execution
units
are
available
Similarly
the
dependence
of
sw
on
or
by
way
of
t
and
of
and
on
sub
by
way
of
t
are
RAW
dependencies
The
dependence
between
sub
and
add
by
way
of
t
is
called
a
write
after
read
WAR
hazard
or
an
antidependence
sub
must
not
write
t
before
add
reads
t
so
that
add
receives
the
correct
value
according
to
the
original
order
of
the
program
WAR
hazards
could
not
occur
in
the
simple
MIPS
pipeline
but
they
may
happen
in
an
out
of
order
processor
if
the
dependent
instruction
in
this
case
sub
is
moved
too
early
A
WAR
hazard
is
not
essential
to
the
operation
of
the
program
It
is
merely
an
artifact
of
the
programmer
s
choice
to
use
the
same
register
for
two
unrelated
instructions
If
the
sub
instruction
had
written
t
instead
of
t
the
dependency
would
disappear
and
sub
could
be
issued
before
add
The
MIPS
architecture
only
has
registers
so
sometimes
the
programmer
is
forced
to
reuse
a
register
and
introduce
a
hazard
just
because
all
the
other
registers
are
in
use
A
third
type
of
hazard
not
shown
in
the
program
is
called
write
after
write
WAW
or
an
output
dependence
A
WAW
hazard
occurs
if
an
instruction
attempts
to
write
a
register
after
a
subsequent
instruction
has
already
written
it
The
hazard
would
result
in
the
wrong
value
being
CHAPTER
SEVEN
Microarchitecture
Ch
written
to
the
register
For
example
in
the
following
program
add
and
sub
both
write
t
The
final
value
in
t
should
come
from
sub
according
to
the
order
of
the
program
If
an
out
of
order
processor
attempted
to
execute
sub
first
the
WAW
hazard
would
occur
add
t
s
s
sub
t
s
s
WAW
hazards
are
not
essential
either
again
they
are
artifacts
caused
by
the
programmer
s
using
the
same
register
for
two
unrelated
instructions
If
the
sub
instruction
were
issued
first
the
program
could
eliminate
the
WAW
hazard
by
discarding
the
result
of
the
add
instead
of
writing
it
to
t
This
is
called
squashing
the
add
Out
of
order
processors
use
a
table
to
keep
track
of
instructions
waiting
to
issue
The
table
sometimes
called
a
scoreboard
contains
information
about
the
dependencies
The
size
of
the
table
determines
how
many
instructions
can
be
considered
for
issue
On
each
cycle
the
processor
examines
the
table
and
issues
as
many
instructions
as
it
can
limited
by
the
dependencies
and
by
the
number
of
execution
units
e
g
ALUs
memory
ports
that
are
available
The
instruction
level
parallelism
ILP
is
the
number
of
instructions
that
can
be
executed
simultaneously
for
a
particular
program
and
microarchitecture
Theoretical
studies
have
shown
that
the
ILP
can
be
quite
large
for
out
of
order
microarchitectures
with
perfect
branch
predictors
and
enormous
numbers
of
execution
units
However
practical
processors
seldom
achieve
an
ILP
greater
than
or
even
with
six
way
superscalar
datapaths
with
out
of
order
execution
Register
Renaming
Out
of
order
processors
use
a
technique
called
register
renaming
to
eliminate
WAR
hazards
Register
renaming
adds
some
nonarchitectural
renaming
registers
to
the
processor
For
example
a
MIPS
processor
might
add
renaming
registers
called
r
r
The
programmer
cannot
use
these
registers
directly
because
they
are
not
part
of
the
architecture
However
the
processor
is
free
to
use
them
to
eliminate
hazards
For
example
in
the
previous
section
a
WAR
hazard
occurred
between
the
sub
and
add
instructions
based
on
reusing
t
The
out
oforder
processor
could
rename
t
to
r
for
the
sub
instruction
Then
Advanced
Microarchitecture
You
might
wonder
why
the
add
needs
to
be
issued
at
all
The
reason
is
that
out
of
order
processors
must
guarantee
that
all
of
the
same
exceptions
occur
that
would
have
occurred
if
the
program
had
been
executed
in
its
original
order
The
add
potentially
may
produce
an
overflow
exception
so
it
must
be
issued
to
check
for
the
exception
even
though
the
result
can
be
discarded
sub
could
be
executed
sooner
because
r
has
no
dependency
on
the
add
instruction
The
processor
keeps
a
table
of
which
registers
were
renamed
so
that
it
can
consistently
rename
registers
in
subsequent
dependent
instructions
In
this
example
t
must
also
be
renamed
to
r
in
the
and
instruction
because
it
refers
to
the
result
of
sub
Figure
shows
the
same
program
from
Figure
executing
on
an
out
of
order
processor
with
register
renaming
t
is
renamed
to
r
in
sub
and
and
to
eliminate
the
WAR
hazard
The
constraints
on
issuing
instructions
are
described
below
Cycle
The
lw
instruction
issues
The
add
instruction
is
dependent
on
lw
by
way
of
t
so
it
cannot
issue
yet
However
the
sub
instruction
is
independent
now
that
its
destination
has
been
renamed
to
r
so
sub
also
issues
Cycle
Remember
that
there
is
a
two
cycle
latency
between
when
a
lw
issues
and
when
a
dependent
instruction
can
use
its
result
so
add
cannot
issue
yet
because
of
the
t
dependence
The
and
instruction
is
dependent
on
sub
so
it
can
issue
r
is
forwarded
from
sub
to
and
The
or
instruction
is
independent
so
it
also
issues
CHAPTER
SEVEN
Microarchitecture
Time
cycles
RF
s
RF
t
DM
IM
lw
lw
t
s
add
t
t
s
sub
r
s
s
and
t
s
r
sw
s
t
sub
s
s
r
RF
r
s
RF
DM
and
s
or
t
s
s
IM
RF
s
t
RF
t
DM
IM
add
sw
t
RAW
s
s
or
RAW
RAW
t
t
Figure
Out
of
order
execution
of
a
program
using
register
renaming
Ch
Cycle
On
cycle
t
is
available
so
add
issues
t
is
also
available
so
sw
issues
The
out
of
order
processor
with
register
renaming
issues
the
six
instructions
in
three
cycles
for
an
IPC
of
Single
Instruction
Multiple
Data
The
term
SIMD
pronounced
sim
dee
stands
for
single
instruction
multiple
data
in
which
a
single
instruction
acts
on
multiple
pieces
of
data
in
parallel
A
common
application
of
SIMD
is
to
perform
many
short
arithmetic
operations
at
once
especially
for
graphics
processing
This
is
also
called
packed
arithmetic
For
example
a
bit
microprocessor
might
pack
four
bit
data
elements
into
one
bit
word
Packed
add
and
subtract
instructions
operate
on
all
four
data
elements
within
the
word
in
parallel
Figure
shows
a
packed
bit
addition
summing
four
pairs
of
bit
numbers
to
produce
four
results
The
word
could
also
be
divided
into
two
bit
elements
Performing
packed
arithmetic
requires
modifying
the
ALU
to
eliminate
carries
between
the
smaller
data
elements
For
example
a
carry
out
of
a
b
should
not
affect
the
result
of
a
b
Short
data
elements
often
appear
in
graphics
processing
For
example
a
pixel
in
a
digital
photo
may
use
bits
to
store
each
of
the
red
green
and
blue
color
components
Using
an
entire
bit
word
to
process
one
of
these
components
wastes
the
upper
bits
When
the
components
from
four
adjacent
pixels
are
packed
into
a
bit
word
the
processing
can
be
performed
four
times
faster
SIMD
instructions
are
even
more
helpful
for
bit
architectures
which
can
pack
eight
bit
elements
four
bit
elements
or
two
bit
elements
into
a
single
bit
word
SIMD
instructions
are
also
used
for
floating
point
computations
for
example
four
bit
single
precision
floating
point
values
can
be
packed
into
a
single
bit
word
Advanced
Microarchitecture
padd
s
s
s
a
s
Bit
position
a
a
a
b
b
b
b
s
a
b
a
b
a
b
a
b
s
Figure
Packed
arithmetic
four
simultaneous
bit
additions
Multithreading
Because
the
ILP
of
real
programs
tends
to
be
fairly
low
adding
more
execution
units
to
a
superscalar
or
out
of
order
processor
gives
diminishing
returns
Another
problem
discussed
in
Chapter
is
that
memory
is
much
slower
than
the
processor
Most
loads
and
stores
access
a
smaller
and
faster
memory
called
a
cache
However
when
the
instructions
or
data
are
not
available
in
the
cache
the
processor
may
stall
for
or
more
cycles
while
retrieving
the
information
from
the
main
memory
Multithreading
is
a
technique
that
helps
keep
a
processor
with
many
execution
units
busy
even
if
the
ILP
of
a
program
is
low
or
the
program
is
stalled
waiting
for
memory
To
explain
multithreading
we
need
to
define
a
few
new
terms
A
program
running
on
a
computer
is
called
a
process
Computers
can
run
multiple
processes
simultaneously
for
example
you
can
play
music
on
a
PC
while
surfing
the
web
and
running
a
virus
checker
Each
process
consists
of
one
or
more
threads
that
also
run
simultaneously
For
example
a
word
processor
may
have
one
thread
handling
the
user
typing
a
second
thread
spell
checking
the
document
while
the
user
works
and
a
third
thread
printing
the
document
In
this
way
the
user
does
not
have
to
wait
for
example
for
a
document
to
finish
printing
before
being
able
to
type
again
In
a
conventional
processor
the
threads
only
give
the
illusion
of
running
simultaneously
The
threads
actually
take
turns
being
executed
on
the
processor
under
control
of
the
OS
When
one
thread
s
turn
ends
the
OS
saves
its
architectural
state
loads
the
architectural
state
of
the
next
thread
and
starts
executing
that
next
thread
This
procedure
is
called
context
switching
As
long
as
the
processor
switches
through
all
the
threads
fast
enough
the
user
perceives
all
of
the
threads
as
running
at
the
same
time
A
multithreaded
processor
contains
more
than
one
copy
of
its
architectural
state
so
that
more
than
one
thread
can
be
active
at
a
time
For
example
if
we
extended
a
MIPS
processor
to
have
four
program
counters
and
registers
four
threads
could
be
available
at
one
time
If
one
thread
stalls
while
waiting
for
data
from
main
memory
the
processor
could
context
switch
to
another
thread
without
any
delay
because
the
program
counter
and
registers
are
already
available
Moreover
if
one
thread
lacks
sufficient
parallelism
to
keep
all
the
execution
units
busy
another
thread
could
issue
instructions
to
the
idle
units
Multithreading
does
not
improve
the
performance
of
an
individual
thread
because
it
does
not
increase
the
ILP
However
it
does
improve
the
overall
throughput
of
the
processor
because
multiple
threads
can
use
processor
resources
that
would
have
been
idle
when
executing
a
single
thread
Multithreading
is
also
relatively
inexpensive
to
implement
because
it
replicates
only
the
PC
and
register
file
not
the
execution
units
and
memories
CHAPTER
SEVEN
Microarchitecture
Multiprocessors
A
multiprocessor
system
consists
of
multiple
processors
and
a
method
for
communication
between
the
processors
A
common
form
of
multiprocessing
in
computer
systems
is
symmetric
multiprocessing
SMP
in
which
two
or
more
identical
processors
share
a
single
main
memory
The
multiple
processors
may
be
separate
chips
or
multiple
cores
on
the
same
chip
Modern
processors
have
enormous
numbers
of
transistors
available
Using
them
to
increase
the
pipeline
depth
or
to
add
more
execution
units
to
a
superscalar
processor
gives
little
performance
benefit
and
is
wasteful
of
power
Around
the
year
computer
architects
made
a
major
shift
to
build
multiple
copies
of
the
processor
on
the
same
chip
these
copies
are
called
cores
Multiprocessors
can
be
used
to
run
more
threads
simultaneously
or
to
run
a
particular
thread
faster
Running
more
threads
simultaneously
is
easy
the
threads
are
simply
divided
up
among
the
processors
Unfortunately
typical
PC
users
need
to
run
only
a
small
number
of
threads
at
any
given
time
Running
a
particular
thread
faster
is
much
more
challenging
The
programmer
must
divide
the
thread
into
pieces
to
perform
on
each
processor
This
becomes
tricky
when
the
processors
need
to
communicate
with
each
other
One
of
the
major
challenges
for
computer
designers
and
programmers
is
to
effectively
use
large
numbers
of
processor
cores
Other
forms
of
multiprocessing
include
asymmetric
multiprocessing
and
clusters
Asymmetric
multiprocessors
use
separate
specialized
microprocessors
for
separate
tasks
For
example
a
cell
phone
contains
a
digital
signal
processor
DSP
with
specialized
instructions
to
decipher
the
wireless
data
in
real
time
and
a
separate
conventional
processor
to
interact
with
the
user
manage
the
phone
book
and
play
games
In
clustered
multiprocessing
each
processor
has
its
own
local
memory
system
Clustering
can
also
refer
to
a
group
of
PCs
connected
together
on
the
network
running
software
to
jointly
solve
a
large
problem
REAL
WORLD
PERSPECTIVE
IA
MICROARCHITECTURE
Section
introduced
the
IA
architecture
used
in
almost
all
PCs
This
section
tracks
the
evolution
of
IA
processors
through
progressively
faster
and
more
complicated
microarchitectures
The
same
principles
we
have
applied
to
the
MIPS
microarchitectures
are
used
in
IA
Intel
invented
the
first
single
chip
microprocessor
the
bit
in
as
a
flexible
controller
for
a
line
of
calculators
It
contained
transistors
manufactured
on
a
mm
sliver
of
silicon
in
a
process
with
a
m
feature
size
and
operated
at
KHz
A
photograph
of
the
chip
taken
under
a
microscope
is
shown
in
Figure
Real
World
Perspective
IA
Microarchitecture
Scientists
searching
for
signs
of
extraterrestrial
intelligence
use
the
world
s
largest
clustered
multiprocessors
to
analyze
radio
telescope
data
for
patterns
that
might
be
signs
of
life
in
other
solar
systems
The
cluster
consists
of
personal
computers
owned
by
more
than
million
volunteers
around
the
world
When
a
computer
in
the
cluster
is
idle
it
fetches
a
piece
of
the
data
from
a
centralized
server
analyzes
the
data
and
sends
the
results
back
to
the
server
You
can
volunteer
your
computer
s
idle
time
for
the
cluster
by
visiting
setiathome
berkeley
edu
In
places
columns
of
four
similar
looking
structures
are
visible
as
one
would
expect
in
a
bit
microprocessor
Around
the
periphery
are
bond
wires
which
are
used
to
connect
the
chip
to
its
package
and
the
circuit
board
The
inspired
the
bit
then
the
which
eventually
evolved
into
the
bit
in
and
the
in
In
Intel
introduced
the
which
extended
the
architecture
to
bits
and
defined
the
IA
architecture
Table
summarizes
major
Intel
IA
microprocessors
In
the
years
since
the
transistor
feature
size
has
shrunk
fold
the
number
of
transistors
CHAPTER
SEVEN
Microarchitecture
Figure
microprocessor
chip
Table
Evolution
of
Intel
IA
microprocessors
Processor
Year
Feature
Size
m
Transistors
Frequency
MHz
Microarchitecture
k
multicycle
M
pipelined
Pentium
M
superscalar
Pentium
II
M
out
of
order
Pentium
III
M
M
out
of
order
Pentium
M
out
of
order
Pentium
M
M
out
of
order
Core
Duo
M
dual
core
on
a
chip
has
increased
by
five
orders
of
magnitude
and
the
operating
frequency
has
increased
by
almost
four
orders
of
magnitude
No
other
field
of
engineering
has
made
such
astonishing
progress
in
such
a
short
time
The
is
a
multicycle
processor
The
major
components
are
labeled
on
the
chip
photograph
in
Figure
The
bit
datapath
is
clearly
visible
on
the
left
Each
of
the
columns
processes
one
bit
of
data
Some
of
the
control
signals
are
generated
using
a
microcode
PLA
that
steps
through
the
various
states
of
the
control
FSM
The
memory
management
unit
in
the
upper
right
controls
access
to
the
external
memory
The
shown
in
Figure
dramatically
improved
performance
using
pipelining
The
datapath
is
again
clearly
visible
along
with
the
control
logic
and
microcode
PLA
The
added
an
on
chip
floating
point
unit
previous
Intel
processors
either
sent
floating
point
instructions
to
a
separate
coprocessor
or
emulated
them
in
software
The
was
too
fast
for
external
memory
to
keep
up
so
it
incorporated
an
KB
cache
onto
the
chip
to
hold
the
most
commonly
used
instructions
and
data
Chapter
describes
caches
in
more
detail
and
revisits
the
cache
systems
on
Intel
IA
processors
Real
World
Perspective
IA
Microarchitecture
Microcode
PLA
bit
Datapath
Memory
Management
Unit
Controller
Figure
microprocessor
chip
The
Pentium
processor
shown
in
Figure
is
a
superscalar
processor
capable
of
executing
two
instructions
simultaneously
Intel
switched
to
the
name
Pentium
instead
of
because
AMD
was
becoming
a
serious
competitor
selling
interchangeable
chips
and
part
numbers
cannot
be
trademarked
The
Pentium
uses
separate
instruction
and
data
caches
It
also
uses
a
branch
predictor
to
reduce
the
performance
penalty
for
branches
The
Pentium
Pro
Pentium
II
and
Pentium
III
processors
all
share
a
common
out
of
order
microarchitecture
code
named
P
The
complex
IA
instructions
are
broken
down
into
one
or
more
micro
ops
similar
CHAPTER
SEVEN
Microarchitecture
bit
Data
path
Controller
KB
Cache
Floating
Point
Unit
Microcode
PLA
Figure
microprocessor
chip
Real
World
Perspective
IA
Microarchitecture
to
MIPS
instructions
The
micro
ops
are
then
executed
on
a
fast
out
oforder
execution
core
with
an
stage
pipeline
Figure
shows
the
Pentium
III
The
bit
datapath
is
called
the
Integer
Execution
Unit
IEU
The
floating
point
datapath
is
called
the
Floating
Point
Unit
KB
Data
Cache
Floating
Point
Unit
KB
Instruction
Cache
Multiprocessor
Logic
Instruction
Fetch
Decode
Complex
Instruction
Support
Bus
Interface
Unit
bit
Datapath
Superscalar
Controller
Branch
Prediction
Figure
Pentium
microprocessor
chip
Instruction
Fetch
KB
Cache
KB
Data
Cache
Data
TLB
Branch
Target
Buffer
FPU
IEU
SIMD
Register
Renaming
Microcode
PLA
KB
Level
Cache
Out
of
Order
Issue
Logic
Bus
Logic
Figure
Pentium
III
microprocessor
chip
CHAPTER
SEVEN
Microarchitecture
IEU
KB
Data
Cache
Trace
Cache
KB
Level
Cache
FPU
SIMD
Bus
Logic
Figure
Pentium
microprocessor
chip
FPU
The
processor
also
has
a
SIMD
unit
to
perform
packed
operations
on
short
integer
and
floating
point
data
A
larger
portion
of
the
chip
is
dedicated
to
issuing
instructions
out
of
order
than
to
actually
executing
the
instructions
The
instruction
and
data
caches
have
grown
to
KB
each
The
Pentium
III
also
has
a
larger
but
slower
KB
second
level
cache
on
the
same
chip
By
the
late
s
processors
were
marketed
largely
on
clock
speed
The
Pentium
is
another
out
of
order
processor
with
a
very
deep
pipeline
to
achieve
extremely
high
clock
frequencies
It
started
with
stages
and
later
versions
adopted
stages
to
achieve
frequencies
greater
than
GHz
The
chip
shown
in
Figure
packs
in
to
million
transistors
depending
on
the
cache
size
so
even
the
major
execution
units
are
difficult
to
see
on
the
photograph
Decoding
three
IA
instructions
per
cycle
is
impossible
at
such
high
clock
frequencies
because
the
instruction
encodings
are
so
complex
and
irregular
Instead
the
processor
predecodes
the
instructions
into
simpler
micro
ops
then
stores
the
micro
ops
in
a
memory
called
a
trace
cache
Later
versions
of
the
Pentium
also
perform
multithreading
to
increase
the
throughput
of
multiple
threads
The
Pentium
s
reliance
on
deep
pipelines
and
high
clock
speed
led
to
extremely
high
power
consumption
sometimes
more
than
W
This
is
unacceptable
in
laptops
and
makes
cooling
of
desktops
expensive
Intel
discovered
that
the
older
P
architecture
could
achieve
comparable
performance
at
much
lower
clock
speed
and
power
The
Pentium
M
uses
an
enhanced
version
of
the
P
out
of
order
microarchitecture
with
KB
instruction
and
data
caches
and
a
to
MB
second
level
cache
The
Core
Duo
is
a
multicore
processor
based
on
two
Pentium
M
cores
connected
to
a
shared
MB
second
level
cache
The
individual
functional
units
in
Figure
are
difficult
to
see
but
the
two
cores
and
the
large
cache
are
clearly
visible
SUMMARY
This
chapter
has
described
three
ways
to
build
MIPS
processors
each
with
different
performance
and
cost
trade
offs
We
find
this
topic
almost
magical
how
can
such
a
seemingly
complicated
device
as
a
microprocessor
actually
be
simple
enough
to
fit
in
a
half
page
schematic
Moreover
the
inner
workings
so
mysterious
to
the
uninitiated
are
actually
reasonably
straightforward
The
MIPS
microarchitectures
have
drawn
together
almost
every
topic
covered
in
the
text
so
far
Piecing
together
the
microarchitecture
puzzle
illustrates
the
principles
introduced
in
previous
chapters
including
the
design
of
combinational
and
sequential
circuits
covered
in
Chapters
and
the
application
of
many
of
the
building
blocks
described
in
Chapter
and
the
implementation
of
the
MIPS
architecture
introduced
in
Chapter
The
MIPS
microarchitectures
can
be
described
in
a
few
pages
of
HDL
using
the
techniques
from
Chapter
Building
the
microarchitectures
has
also
heavily
used
our
techniques
for
managing
complexity
The
microarchitectural
abstraction
forms
the
link
between
the
logic
and
architecture
abstractions
forming
Summary
Core
Core
MB
Shared
Level
Cache
Figure
Core
Duo
microprocessor
chip
the
crux
of
this
book
on
digital
design
and
computer
architecture
We
also
use
the
abstractions
of
block
diagrams
and
HDL
to
succinctly
describe
the
arrangement
of
components
The
microarchitectures
exploit
regularity
and
modularity
reusing
a
library
of
common
building
blocks
such
as
ALUs
memories
multiplexers
and
registers
Hierarchy
is
used
in
numerous
ways
The
microarchitectures
are
partitioned
into
the
datapath
and
control
units
Each
of
these
units
is
built
from
logic
blocks
which
can
be
built
from
gates
which
in
turn
can
be
built
from
transistors
using
the
techniques
developed
in
the
first
five
chapters
This
chapter
has
compared
single
cycle
multicycle
and
pipelined
microarchitectures
for
the
MIPS
processor
All
three
microarchitectures
implement
the
same
subset
of
the
MIPS
instruction
set
and
have
the
same
architectural
state
The
single
cycle
processor
is
the
most
straightforward
and
has
a
CPI
of
The
multicycle
processor
uses
a
variable
number
of
shorter
steps
to
execute
instructions
It
thus
can
reuse
the
ALU
rather
than
requiring
several
adders
However
it
does
require
several
nonarchitectural
registers
to
store
results
between
steps
The
multicycle
design
in
principle
could
be
faster
because
not
all
instructions
must
be
equally
long
In
practice
it
is
generally
slower
because
it
is
limited
by
the
slowest
steps
and
by
the
sequencing
overhead
in
each
step
The
pipelined
processor
divides
the
single
cycle
processor
into
five
relatively
fast
pipeline
stages
It
adds
pipeline
registers
between
the
stages
to
separate
the
five
instructions
that
are
simultaneously
executing
It
nominally
has
a
CPI
of
but
hazards
force
stalls
or
flushes
that
increase
the
CPI
slightly
Hazard
resolution
also
costs
some
extra
hardware
and
design
complexity
The
clock
period
ideally
could
be
five
times
shorter
than
that
of
the
single
cycle
processor
In
practice
it
is
not
that
short
because
it
is
limited
by
the
slowest
stage
and
by
the
sequencing
overhead
in
each
stage
Nevertheless
pipelining
provides
substantial
performance
benefits
All
modern
high
performance
microprocessors
use
pipelining
today
Although
the
microarchitectures
in
this
chapter
implement
only
a
subset
of
the
MIPS
architecture
we
have
seen
that
supporting
more
instructions
involves
straightforward
enhancements
of
the
datapath
and
controller
Supporting
exceptions
also
requires
simple
modifications
A
major
limitation
of
this
chapter
is
that
we
have
assumed
an
ideal
memory
system
that
is
fast
and
large
enough
to
store
the
entire
program
and
data
In
reality
large
fast
memories
are
prohibitively
expensive
The
next
chapter
shows
how
to
get
most
of
the
benefits
of
a
large
fast
memory
with
a
small
fast
memory
that
holds
the
most
commonly
used
information
and
one
or
more
larger
but
slower
memories
that
hold
the
rest
of
the
information
CHAPTER
SEVEN
Microarchitecture
Exercises
Exercises
Exercise
Suppose
that
one
of
the
following
control
signals
in
the
single
cycle
MIPS
processor
has
a
stuck
at
fault
meaning
that
the
signal
is
always
regardless
of
its
intended
value
What
instructions
would
malfunction
Why
a
RegWrite
b
ALUOp
c
MemWrite
Exercise
Repeat
Exercise
assuming
that
the
signal
has
a
stuck
at
fault
Exercise
Modify
the
single
cycle
MIPS
processor
to
implement
one
of
the
following
instructions
See
Appendix
B
for
a
definition
of
the
instructions
Mark
up
a
copy
of
Figure
to
indicate
the
changes
to
the
datapath
Name
any
new
control
signals
Mark
up
a
copy
of
Table
to
show
the
changes
to
the
main
decoder
Describe
any
other
changes
that
are
required
a
sll
b
lui
c
slti
d
blez
e
jal
f
lh
Exercise
Many
processor
architectures
have
a
load
with
postincrement
instruction
which
updates
the
index
register
to
point
to
the
next
memory
word
after
completing
the
load
lwinc
rt
imm
rs
is
equivalent
to
the
following
two
instructions
lw
rt
imm
rs
addi
rs
rs
Table
Main
decoder
truth
table
to
mark
up
with
changes
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
R
type
lw
sw
X
X
beq
X
X
Repeat
Exercise
for
the
lwinc
instruction
Is
it
possible
to
add
the
instruction
without
modifying
the
register
file
Exercise
Add
a
single
precision
floating
point
unit
to
the
single
cycle
MIPS
processor
to
handle
add
s
sub
s
and
mul
s
Assume
that
you
have
single
precision
floating
point
adder
and
multiplier
units
available
Explain
what
changes
must
be
made
to
the
datapath
and
the
controller
Exercise
Your
friend
is
a
crack
circuit
designer
She
has
offered
to
redesign
one
of
the
units
in
the
single
cycle
MIPS
processor
to
have
half
the
delay
Using
the
delays
from
Table
which
unit
should
she
work
on
to
obtain
the
greatest
speedup
of
the
overall
processor
and
what
would
the
cycle
time
of
the
improved
machine
be
Exercise
Consider
the
delays
given
in
Table
Ben
Bitdiddle
builds
a
prefix
adder
that
reduces
the
ALU
delay
by
ps
If
the
other
element
delays
stay
the
same
find
the
new
cycle
time
of
the
single
cycle
MIPS
processor
and
determine
how
long
it
takes
to
execute
a
benchmark
with
billion
instructions
Exercise
Suppose
one
of
the
following
control
signals
in
the
multicycle
MIPS
processor
has
a
stuck
at
fault
meaning
that
the
signal
is
always
regardless
of
its
intended
value
What
instructions
would
malfunction
Why
a
MemtoReg
b
ALUOp
c
PCSrc
Exercise
Repeat
Exercise
assuming
that
the
signal
has
a
stuck
at
fault
Exercise
Modify
the
HDL
code
for
the
single
cycle
MIPS
processor
given
in
Section
to
handle
one
of
the
new
instructions
from
Exercise
Enhance
the
testbench
given
in
Section
to
test
the
new
instruction
Exercise
Modify
the
multicycle
MIPS
processor
to
implement
one
of
the
following
instructions
See
Appendix
B
for
a
definition
of
the
instructions
Mark
up
a
copy
of
Figure
to
indicate
the
changes
to
the
datapath
Name
any
new
control
signals
Mark
up
a
copy
of
Figure
to
show
the
changes
to
the
controller
FSM
Describe
any
other
changes
that
are
required
a
srlv
b
ori
c
xori
d
jr
e
bne
f
lbu
CHAPTER
SEVEN
Microarchitecture
Exercise
Repeat
Exercise
for
the
multicycle
MIPS
processor
Show
the
changes
to
the
multicycle
datapath
and
control
FSM
Is
it
possible
to
add
the
instruction
without
modifying
the
register
file
Exercise
Repeat
Exercise
for
the
multicycle
MIPS
processor
Exercise
Suppose
that
the
floating
point
adder
and
multiplier
from
Exercise
each
take
two
cycles
to
operate
In
other
words
the
inputs
are
applied
at
the
beginning
of
one
cycle
and
the
output
is
available
in
the
second
cycle
How
does
your
answer
to
Exercise
change
Exercise
Your
friend
the
crack
circuit
designer
has
offered
to
redesign
one
of
the
units
in
the
multicycle
MIPS
processor
to
be
much
faster
Using
the
delays
from
Table
which
unit
should
she
work
on
to
obtain
the
greatest
speedup
of
the
overall
processor
How
fast
should
it
be
Making
it
faster
than
necessary
is
a
waste
of
your
friend
s
effort
What
is
the
cycle
time
of
the
improved
processor
Exercise
Repeat
Exercise
for
the
multicycle
processor
Exercise
Suppose
the
multicycle
MIPS
processor
has
the
component
delays
given
in
Table
Alyssa
P
Hacker
designs
a
new
register
file
that
has
less
power
but
twice
as
much
delay
Should
she
switch
to
the
slower
but
lower
power
register
file
for
her
multicycle
processor
design
Exercise
Goliath
Corp
claims
to
have
a
patent
on
a
three
ported
register
file
Rather
than
fighting
Goliath
in
court
Ben
Bitdiddle
designs
a
new
register
file
that
has
only
a
single
read
write
port
like
the
combined
instruction
and
data
memory
Redesign
the
MIPS
multicycle
datapath
and
controller
to
use
his
new
register
file
Exercise
What
is
the
CPI
of
the
redesigned
multicycle
MIPS
processor
from
Exercise
Use
the
instruction
mix
from
Example
Exercise
How
many
cycles
are
required
to
run
the
following
program
on
the
multicycle
MIPS
processor
What
is
the
CPI
of
this
program
addi
s
sum
while
beq
s
done
if
result
execute
the
while
block
addi
s
s
while
block
result
result
j
while
done
Exercises
Exercise
Repeat
Exercise
for
the
following
program
add
s
i
add
s
sum
addi
t
t
loop
slt
t
s
t
if
i
t
else
t
beq
t
done
if
t
i
branch
to
done
add
s
s
s
sum
sum
i
addi
s
s
increment
i
j
loop
done
Exercise
Write
HDL
code
for
the
multicycle
MIPS
processor
The
processor
should
be
compatible
with
the
following
top
level
module
The
mem
module
is
used
to
hold
both
instructions
and
data
Test
your
processor
using
the
testbench
from
Section
module
top
input
clk
reset
output
writedata
adr
output
memwrite
wire
readdata
instantiate
processor
and
memories
mips
mips
clk
reset
adr
writedata
memwrite
readdata
mem
mem
clk
memwrite
adr
writedata
readdata
endmodule
module
mem
input
clk
we
input
a
wd
output
rd
reg
RAM
initial
begin
readmemh
memfile
dat
RAM
end
assign
rd
RAM
a
word
aligned
always
posedge
clk
if
we
RAM
a
wd
endmodule
Exercise
Extend
your
HDL
code
for
the
multicycle
MIPS
processor
from
Exercise
to
handle
one
of
the
new
instructions
from
Exercise
Enhance
the
testbench
to
test
the
new
instruction
CHAPTER
SEVEN
Microarchitecture
Exercise
The
pipelined
MIPS
processor
is
running
the
following
program
Which
registers
are
being
written
and
which
are
being
read
on
the
fifth
cycle
add
s
t
t
sub
s
t
t
and
s
s
s
or
s
t
t
slt
s
s
s
Exercise
Using
a
diagram
similar
to
Figure
show
the
forwarding
and
stalls
needed
to
execute
the
following
instructions
on
the
pipelined
MIPS
processor
add
t
s
s
sub
t
t
s
lw
t
t
and
t
t
t
Exercise
Repeat
Exercise
for
the
following
instructions
add
t
s
s
lw
t
s
sub
t
t
s
and
t
t
t
Exercise
How
many
cycles
are
required
for
the
pipelined
MIPS
processor
to
issue
all
of
the
instructions
for
the
program
in
Exercise
What
is
the
CPI
of
the
processor
on
this
program
Exercise
Explain
how
to
extend
the
pipelined
MIPS
processor
to
handle
the
addi
instruction
Exercise
Explain
how
to
extend
the
pipelined
processor
to
handle
the
j
instruction
Give
particular
attention
to
how
the
pipeline
is
flushed
when
a
jump
takes
place
Exercise
Examples
and
point
out
that
the
pipelined
MIPS
processor
performance
might
be
better
if
branches
take
place
during
the
Execute
stage
rather
than
the
Decode
stage
Show
how
to
modify
the
pipelined
processor
from
Figure
to
branch
in
the
Execute
stage
How
do
the
stall
and
flush
signals
change
Redo
Examples
and
to
find
the
new
CPI
cycle
time
and
overall
time
to
execute
the
program
Exercise
Your
friend
the
crack
circuit
designer
has
offered
to
redesign
one
of
the
units
in
the
pipelined
MIPS
processor
to
be
much
faster
Using
the
delays
from
Table
and
Example
which
unit
should
she
work
on
to
obtain
the
greatest
speedup
of
the
overall
processor
How
fast
should
it
be
Making
it
faster
than
necessary
is
a
waste
of
your
friend
s
effort
What
is
the
cycle
time
of
the
improved
processor
Exercises
Exercise
Consider
the
delays
from
Table
and
Example
Now
suppose
that
the
ALU
were
faster
Would
the
cycle
time
of
the
pipelined
MIPS
processor
change
What
if
the
ALU
were
slower
Exercise
Write
HDL
code
for
the
pipelined
MIPS
processor
The
processor
should
be
compatible
with
the
top
level
module
from
HDL
Example
It
should
support
all
of
the
instructions
described
in
this
chapter
including
addi
and
j
see
Exercises
and
Test
your
design
using
the
testbench
from
HDL
Example
Exercise
Design
the
hazard
unit
shown
in
Figure
for
the
pipelined
MIPS
processor
Use
an
HDL
to
implement
your
design
Sketch
the
hardware
that
a
synthesis
tool
might
generate
from
your
HDL
Exercise
A
nonmaskable
interrupt
NMI
is
triggered
by
an
input
pin
to
the
processor
When
the
pin
is
asserted
the
current
instruction
should
finish
then
the
processor
should
set
the
Cause
register
to
and
take
an
exception
Show
how
to
modify
the
multicycle
processor
in
Figures
and
to
handle
nonmaskable
interrupts
CHAPTER
SEVEN
Microarchitecture
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Explain
the
advantages
of
pipelined
microprocessors
Question
If
additional
pipeline
stages
allow
a
processor
to
go
faster
why
don
t
processors
have
pipeline
stages
Question
Describe
what
a
hazard
is
in
a
microprocessor
and
explain
ways
in
which
it
can
be
resolved
What
are
the
pros
and
cons
of
each
way
Question
Describe
the
concept
of
a
superscalar
processor
and
its
pros
and
cons
Interview
Questions
Introduction
Memory
System
Performance
Analysis
Caches
Virtual
Memory
Memory
Mapped
I
O
Real
World
Perspective
IA
Memory
and
I
O
Systems
Summary
Exercises
Interview
Questions
Memory
Systems
INTRODUCTION
Computer
system
performance
depends
on
the
memory
system
as
well
as
the
processor
microarchitecture
Chapter
assumed
an
ideal
memory
system
that
could
be
accessed
in
a
single
clock
cycle
However
this
would
be
true
only
for
a
very
small
memory
or
a
very
slow
processor
Early
processors
were
relatively
slow
so
memory
was
able
to
keep
up
But
processor
speed
has
increased
at
a
faster
rate
than
memory
speeds
DRAM
memories
are
currently
to
times
slower
than
processors
The
increasing
gap
between
processor
and
DRAM
memory
speeds
demands
increasingly
ingenious
memory
systems
to
try
to
approximate
a
memory
that
is
as
fast
as
the
processor
This
chapter
investigates
practical
memory
systems
and
considers
trade
offs
of
speed
capacity
and
cost
The
processor
communicates
with
the
memory
system
over
a
memory
interface
Figure
shows
the
simple
memory
interface
used
in
our
multicycle
MIPS
processor
The
processor
sends
an
address
over
the
Address
bus
to
the
memory
system
For
a
read
MemWrite
is
and
the
memory
returns
the
data
on
the
ReadData
bus
For
a
write
MemWrite
is
and
the
processor
sends
data
to
memory
on
the
WriteData
bus
The
major
issues
in
memory
system
design
can
be
broadly
explained
using
a
metaphor
of
books
in
a
library
A
library
contains
many
books
on
the
shelves
If
you
were
writing
a
term
paper
on
the
meaning
of
dreams
you
might
go
to
the
library
and
pull
Freud
s
The
Interpretation
of
Dreams
off
the
shelf
and
bring
it
to
your
cubicle
After
skimming
it
you
might
put
it
back
and
pull
out
Jung
s
The
Psychology
of
the
We
realize
that
library
usage
is
plummeting
among
college
students
because
of
the
Internet
But
we
also
believe
that
libraries
contain
vast
troves
of
hard
won
human
knowledge
that
are
not
electronically
available
We
hope
that
Web
searching
does
not
completely
displace
the
art
of
library
research
Unconscious
You
might
then
go
back
for
another
quote
from
Interpretation
of
Dreams
followed
by
yet
another
trip
to
the
stacks
for
Freud
s
The
Ego
and
the
Id
Pretty
soon
you
would
get
tired
of
walking
from
your
cubicle
to
the
stacks
If
you
are
clever
you
would
save
time
by
keeping
the
books
in
your
cubicle
rather
than
schlepping
them
back
and
forth
Furthermore
when
you
pull
a
book
by
Freud
you
could
also
pull
several
of
his
other
books
from
the
same
shelf
This
metaphor
emphasizes
the
principle
introduced
in
Section
of
making
the
common
case
fast
By
keeping
books
that
you
have
recently
used
or
might
likely
use
in
the
future
at
your
cubicle
you
reduce
the
number
of
time
consuming
trips
to
the
stacks
In
particular
you
use
the
principles
of
temporal
and
spatial
locality
Temporal
locality
means
that
if
you
have
used
a
book
recently
you
are
likely
to
use
it
again
soon
Spatial
locality
means
that
when
you
use
one
particular
book
you
are
likely
to
be
interested
in
other
books
on
the
same
shelf
The
library
itself
makes
the
common
case
fast
by
using
these
principles
of
locality
The
library
has
neither
the
shelf
space
nor
the
budget
to
accommodate
all
of
the
books
in
the
world
Instead
it
keeps
some
of
the
lesser
used
books
in
deep
storage
in
the
basement
Also
it
may
have
an
interlibrary
loan
agreement
with
nearby
libraries
so
that
it
can
offer
more
books
than
it
physically
carries
In
summary
you
obtain
the
benefits
of
both
a
large
collection
and
quick
access
to
the
most
commonly
used
books
through
a
hierarchy
of
storage
The
most
commonly
used
books
are
in
your
cubicle
A
larger
collection
is
on
the
shelves
And
an
even
larger
collection
is
available
with
advanced
notice
from
the
basement
and
other
libraries
Similarly
memory
systems
use
a
hierarchy
of
storage
to
quickly
access
the
most
commonly
used
data
while
still
having
the
capacity
to
store
large
amounts
of
data
Memory
subsystems
used
to
build
this
hierarchy
were
introduced
in
Section
Computer
memories
are
primarily
built
from
dynamic
RAM
DRAM
and
static
RAM
SRAM
Ideally
the
computer
memory
system
is
fast
large
and
cheap
In
practice
a
single
memory
only
has
two
of
these
three
attributes
it
is
either
slow
small
or
expensive
But
computer
systems
can
approximate
the
ideal
by
combining
a
fast
small
cheap
memory
and
a
slow
large
cheap
memory
The
fast
memory
stores
the
most
commonly
used
data
and
instructions
so
on
average
the
memory
CHAPTER
EIGHT
Memory
Systems
Processor
Memory
Address
MemWrite
WriteData
ReadData
WE
CLK
Figure
The
memory
interface
system
appears
fast
The
large
memory
stores
the
remainder
of
the
data
and
instructions
so
the
overall
capacity
is
large
The
combination
of
two
cheap
memories
is
much
less
expensive
than
a
single
large
fast
memory
These
principles
extend
to
using
an
entire
hierarchy
of
memories
of
increasing
capacity
and
decreasing
speed
Computer
memory
is
generally
built
from
DRAM
chips
In
a
typical
PC
had
a
main
memory
consisting
of
MB
to
GB
of
DRAM
and
DRAM
cost
about
per
gigabyte
GB
DRAM
prices
have
declined
at
about
per
year
for
the
last
three
decades
and
memory
capacity
has
grown
at
the
same
rate
so
the
total
cost
of
the
memory
in
a
PC
has
remained
roughly
constant
Unfortunately
DRAM
speed
has
improved
by
only
about
per
year
whereas
processor
performance
has
improved
at
a
rate
of
to
per
year
as
shown
in
Figure
The
plot
shows
memory
and
processor
speeds
with
the
speeds
as
a
baseline
In
about
processor
and
memory
speeds
were
the
same
But
performance
has
diverged
since
then
with
memories
badly
lagging
DRAM
could
keep
up
with
processors
in
the
s
and
early
s
but
it
is
now
woefully
too
slow
The
DRAM
access
time
is
one
to
two
orders
of
magnitude
longer
than
the
processor
cycle
time
tens
of
nanoseconds
compared
to
less
than
one
nanosecond
To
counteract
this
trend
computers
store
the
most
commonly
used
instructions
and
data
in
a
faster
but
smaller
memory
called
a
cache
The
cache
is
usually
built
out
of
SRAM
on
the
same
chip
as
the
processor
The
cache
speed
is
comparable
to
the
processor
speed
because
SRAM
is
inherently
faster
than
DRAM
and
because
the
on
chip
memory
eliminates
lengthy
delays
caused
by
traveling
to
and
from
a
separate
chip
In
on
chip
SRAM
costs
were
on
the
order
of
GB
but
the
cache
is
relatively
small
kilobytes
to
a
few
megabytes
so
the
overall
Introduction
Year
CPU
Performance
Memory
Figure
Diverging
processor
and
memory
performance
Adapted
with
permission
from
Hennessy
and
Patterson
Computer
Architecture
A
Quantitative
Approach
rd
ed
Morgan
Kaufmann
cost
is
low
Caches
can
store
both
instructions
and
data
but
we
will
refer
to
their
contents
generically
as
data
If
the
processor
requests
data
that
is
available
in
the
cache
it
is
returned
quickly
This
is
called
a
cache
hit
Otherwise
the
processor
retrieves
the
data
from
main
memory
DRAM
This
is
called
a
cache
miss
If
the
cache
hits
most
of
the
time
then
the
processor
seldom
has
to
wait
for
the
slow
main
memory
and
the
average
access
time
is
low
The
third
level
in
the
memory
hierarchy
is
the
hard
disk
or
hard
drive
In
the
same
way
that
a
library
uses
the
basement
to
store
books
that
do
not
fit
in
the
stacks
computer
systems
use
the
hard
disk
to
store
data
that
does
not
fit
in
main
memory
In
a
hard
disk
cost
less
than
GB
and
had
an
access
time
of
about
ms
Hard
disk
costs
have
decreased
at
year
but
access
times
scarcely
improved
The
hard
disk
provides
an
illusion
of
more
capacity
than
actually
exists
in
the
main
memory
It
is
thus
called
virtual
memory
Like
books
in
the
basement
data
in
virtual
memory
takes
a
long
time
to
access
Main
memory
also
called
physical
memory
holds
a
subset
of
the
virtual
memory
Hence
the
main
memory
can
be
viewed
as
a
cache
for
the
most
commonly
used
data
from
the
hard
disk
Figure
summarizes
the
memory
hierarchy
of
the
computer
system
discussed
in
the
rest
of
this
chapter
The
processor
first
seeks
data
in
a
small
but
fast
cache
that
is
usually
located
on
the
same
chip
If
the
data
is
not
available
in
the
cache
the
processor
then
looks
in
main
memory
If
the
data
is
not
there
either
the
processor
fetches
the
data
from
virtual
memory
on
the
large
but
slow
hard
disk
Figure
illustrates
this
capacity
and
speed
trade
off
in
the
memory
hierarchy
and
lists
typical
costs
and
access
times
in
technology
As
access
time
decreases
speed
increases
Section
introduces
memory
system
performance
analysis
Section
explores
several
cache
organizations
and
Section
delves
into
virtual
memory
systems
To
conclude
this
chapter
explores
how
processors
can
access
input
and
output
devices
such
as
keyboards
and
monitors
in
much
the
same
way
as
they
access
memory
Section
investigates
such
memory
mapped
I
O
CHAPTER
EIGHT
Memory
Systems
CPU
Cache
Main
Memory
Processor
Chip
CLK
Hard
Disk
Figure
A
typical
memory
hierarchy
MEMORY
SYSTEM
PERFORMANCE
ANALYSIS
Designers
and
computer
buyers
need
quantitative
ways
to
measure
the
performance
of
memory
systems
to
evaluate
the
cost
benefit
trade
offs
of
various
alternatives
Memory
system
performance
metrics
are
miss
rate
or
hit
rate
and
average
memory
access
time
Miss
and
hit
rates
are
calculated
as
Number
of
misses
Miss
Rate
Hit
Rate
Number
of
total
memory
accesses
Number
of
hits
Hit
Rate
Miss
Rate
Number
of
total
memory
accesses
Example
CALCULATING
CACHE
PERFORMANCE
Suppose
a
program
has
data
access
instructions
loads
or
stores
and
of
these
requested
data
values
are
found
in
the
cache
The
other
data
values
are
supplied
to
the
processor
by
main
memory
or
disk
memory
What
are
the
miss
and
hit
rates
for
the
cache
Solution
The
miss
rate
is
The
hit
rate
is
Average
memory
access
time
AMAT
is
the
average
time
a
processor
must
wait
for
memory
per
load
or
store
instruction
In
the
typical
computer
system
from
Figure
the
processor
first
looks
for
the
data
in
the
cache
If
the
cache
misses
the
processor
then
looks
in
main
memory
If
the
main
memory
misses
the
processor
accesses
virtual
memory
on
the
hard
disk
Thus
AMAT
is
calculated
as
AMAT
tcache
MRcache
tMM
MRMMtVM
Memory
System
Performance
Analysis
Cache
Main
Memory
Speed
Virtual
Memory
Capacity
Technology
Cost
GB
Access
Time
SRAM
ns
DRAM
ns
Hard
Disk
ns
Figure
Memory
hierarchy
components
with
typical
characteristics
in
Chapter
qx
where
tcache
tMM
and
tVM
are
the
access
times
of
the
cache
main
memory
and
virtual
memory
and
MRcache
and
MRMM
are
the
cache
and
main
memory
miss
rates
respectively
Example
CALCULATING
AVERAGE
MEMORY
ACCESS
TIME
Suppose
a
computer
system
has
a
memory
organization
with
only
two
levels
of
hierarchy
a
cache
and
main
memory
What
is
the
average
memory
access
time
given
the
access
times
and
miss
rates
given
in
Table
Solution
The
average
memory
access
time
is
cycles
CHAPTER
EIGHT
Memory
Systems
Table
Access
times
and
miss
rates
Memory
Access
Time
Miss
Level
Cycles
Rate
Cache
Main
Memory
Gene
Amdahl
Most
famous
for
Amdahl
s
Law
an
observation
he
made
in
While
in
graduate
school
he
began
designing
computers
in
his
free
time
This
side
work
earned
him
his
Ph
D
in
theoretical
physics
in
He
joined
IBM
immediately
after
graduation
and
later
went
on
to
found
three
companies
including
one
called
Amdahl
Corporation
in
Example
IMPROVING
ACCESS
TIME
An
cycle
average
memory
access
time
means
that
the
processor
spends
ten
cycles
waiting
for
data
for
every
one
cycle
actually
using
that
data
What
cache
miss
rate
is
needed
to
reduce
the
average
memory
access
time
to
cycles
given
the
access
times
in
Table
Solution
If
the
miss
rate
is
m
the
average
access
time
is
m
Setting
this
time
to
and
solving
for
m
requires
a
cache
miss
rate
of
As
a
word
of
caution
performance
improvements
might
not
always
be
as
good
as
they
sound
For
example
making
the
memory
system
ten
times
faster
will
not
necessarily
make
a
computer
program
run
ten
times
as
fast
If
of
a
program
s
instructions
are
loads
and
stores
a
tenfold
memory
system
improvement
only
means
a
fold
improvement
in
program
performance
This
general
principle
is
called
Amdahl
s
Law
which
says
that
the
effort
spent
on
increasing
the
performance
of
a
subsystem
is
worthwhile
only
if
the
subsystem
affects
a
large
percentage
of
the
overall
performance
CACHES
A
cache
holds
commonly
used
memory
data
The
number
of
data
words
that
it
can
hold
is
called
the
capacity
C
Because
the
capacity
C
of
the
cache
is
smaller
than
that
of
main
memory
the
computer
system
designer
must
choose
what
subset
of
the
main
memory
is
kept
in
the
cache
When
the
processor
attempts
to
access
data
it
first
checks
the
cache
for
the
data
If
the
cache
hits
the
data
is
available
immediately
If
the
cache
misses
the
processor
fetches
the
data
from
main
memory
and
places
it
in
the
cache
for
future
use
To
accommodate
the
new
data
the
cache
must
replace
old
data
This
section
investigates
these
issues
in
cache
design
by
answering
the
following
questions
What
data
is
held
in
the
cache
How
is
the
data
found
and
What
data
is
replaced
to
make
room
for
new
data
when
the
cache
is
full
When
reading
the
next
sections
keep
in
mind
that
the
driving
force
in
answering
these
questions
is
the
inherent
spatial
and
temporal
locality
of
data
accesses
in
most
applications
Caches
use
spatial
and
temporal
locality
to
predict
what
data
will
be
needed
next
If
a
program
accesses
data
in
a
random
order
it
would
not
benefit
from
a
cache
As
we
explain
in
the
following
sections
caches
are
specified
by
their
capacity
C
number
of
sets
S
block
size
b
number
of
blocks
B
and
degree
of
associativity
N
Although
we
focus
on
data
cache
loads
the
same
principles
apply
for
fetches
from
an
instruction
cache
Data
cache
store
operations
are
similar
and
are
discussed
further
in
Section
What
Data
Is
Held
in
the
Cache
An
ideal
cache
would
anticipate
all
of
the
data
needed
by
the
processor
and
fetch
it
from
main
memory
ahead
of
time
so
that
the
cache
has
a
zero
miss
rate
Because
it
is
impossible
to
predict
the
future
with
perfect
accuracy
the
cache
must
guess
what
data
will
be
needed
based
on
the
past
pattern
of
memory
accesses
In
particular
the
cache
exploits
temporal
and
spatial
locality
to
achieve
a
low
miss
rate
Recall
that
temporal
locality
means
that
the
processor
is
likely
to
access
a
piece
of
data
again
soon
if
it
has
accessed
that
data
recently
Therefore
when
the
processor
loads
or
stores
data
that
is
not
in
the
cache
the
data
is
copied
from
main
memory
into
the
cache
Subsequent
requests
for
that
data
hit
in
the
cache
Recall
that
spatial
locality
means
that
when
the
processor
accesses
a
piece
of
data
it
is
also
likely
to
access
data
in
nearby
memory
locations
Therefore
when
the
cache
fetches
one
word
from
memory
it
may
also
fetch
several
adjacent
words
This
group
of
words
is
called
a
cache
block
The
number
of
words
in
the
cache
block
b
is
called
the
block
size
A
cache
of
capacity
C
contains
B
C
b
blocks
The
principles
of
temporal
and
spatial
locality
have
been
experimentally
verified
in
real
programs
If
a
variable
is
used
in
a
program
the
Caches
Cache
a
hiding
place
especially
for
concealing
and
preserving
provisions
or
implements
Merriam
Webster
Online
Dictionary
http
www
merriam
webster
com
C
same
variable
is
likely
to
be
used
again
creating
temporal
locality
If
an
element
in
an
array
is
used
other
elements
in
the
same
array
are
also
likely
to
be
used
creating
spatial
locality
How
Is
the
Data
Found
A
cache
is
organized
into
S
sets
each
of
which
holds
one
or
more
blocks
of
data
The
relationship
between
the
address
of
data
in
main
memory
and
the
location
of
that
data
in
the
cache
is
called
the
mapping
Each
memory
address
maps
to
exactly
one
set
in
the
cache
Some
of
the
address
bits
are
used
to
determine
which
cache
set
contains
the
data
If
the
set
contains
more
than
one
block
the
data
may
be
kept
in
any
of
the
blocks
in
the
set
Caches
are
categorized
based
on
the
number
of
blocks
in
a
set
In
a
direct
mapped
cache
each
set
contains
exactly
one
block
so
the
cache
has
S
B
sets
Thus
a
particular
main
memory
address
maps
to
a
unique
block
in
the
cache
In
an
N
way
set
associative
cache
each
set
contains
N
blocks
The
address
still
maps
to
a
unique
set
with
S
B
N
sets
But
the
data
from
that
address
can
go
in
any
of
the
N
blocks
in
that
set
A
fully
associative
cache
has
only
S
set
Data
can
go
in
any
of
the
B
blocks
in
the
set
Hence
a
fully
associative
cache
is
another
name
for
a
B
way
set
associative
cache
To
illustrate
these
cache
organizations
we
will
consider
a
MIPS
memory
system
with
bit
addresses
and
bit
words
The
memory
is
byte
addressable
and
each
word
is
four
bytes
so
the
memory
consists
of
words
aligned
on
word
boundaries
We
analyze
caches
with
an
eightword
capacity
C
for
the
sake
of
simplicity
We
begin
with
a
one
word
block
size
b
then
generalize
later
to
larger
blocks
Direct
Mapped
Cache
A
direct
mapped
cache
has
one
block
in
each
set
so
it
is
organized
into
S
B
sets
To
understand
the
mapping
of
memory
addresses
onto
cache
blocks
imagine
main
memory
as
being
mapped
into
b
word
blocks
just
as
the
cache
is
An
address
in
block
of
main
memory
maps
to
set
of
the
cache
An
address
in
block
of
main
memory
maps
to
set
of
the
cache
and
so
forth
until
an
address
in
block
B
of
main
memory
maps
to
block
B
of
the
cache
There
are
no
more
blocks
of
the
cache
so
the
mapping
wraps
around
such
that
block
B
of
main
memory
maps
to
block
of
the
cache
This
mapping
is
illustrated
in
Figure
for
a
direct
mapped
cache
with
a
capacity
of
eight
words
and
a
block
size
of
one
word
The
cache
has
eight
sets
each
of
which
contains
a
one
word
block
The
bottom
two
bits
of
the
address
are
always
because
they
are
word
aligned
The
next
log
bits
indicate
the
set
onto
which
the
memory
address
maps
Thus
the
data
at
addresses
x
x
CHAPTER
EIGHT
Memory
Systems
Chapter
xFFFFFFE
all
map
to
set
as
shown
in
blue
Likewise
data
at
addresses
x
xFFFFFFF
all
map
to
set
and
so
forth
Each
main
memory
address
maps
to
exactly
one
set
in
the
cache
Example
CACHE
FIELDS
To
what
cache
set
in
Figure
does
the
word
at
address
x
map
Name
another
address
that
maps
to
the
same
set
Solution
The
two
least
significant
bits
of
the
address
are
because
the
address
is
word
aligned
The
next
three
bits
are
so
the
word
maps
to
set
Words
at
addresses
x
x
x
xFFFFFFF
all
map
to
this
same
set
Because
many
addresses
map
to
a
single
set
the
cache
must
also
keep
track
of
the
address
of
the
data
actually
contained
in
each
set
The
least
significant
bits
of
the
address
specify
which
set
holds
the
data
The
remaining
most
significant
bits
are
called
the
tag
and
indicate
which
of
the
many
possible
addresses
is
held
in
that
set
In
our
previous
example
the
two
least
significant
bits
of
the
bit
address
are
called
the
byte
offset
because
they
indicate
the
byte
within
the
word
The
next
three
bits
are
called
the
set
bits
because
they
indicate
the
set
to
which
the
address
maps
In
general
the
number
of
set
bits
is
log
S
The
remaining
tag
bits
indicate
the
memory
address
of
the
data
stored
in
a
given
cache
set
Figure
shows
the
cache
fields
for
address
xFFFFFFE
It
maps
to
set
and
its
tag
is
all
s
Caches
Word
Main
Memory
mem
x
mem
x
mem
x
mem
x
C
mem
x
mem
x
mem
x
mem
x
C
mem
x
mem
x
mem
xFFFFFFE
mem
xFFFFFFE
mem
xFFFFFFE
mem
xFFFFFFEC
mem
xFFFFFFF
mem
xFFFFFFF
mem
xFFFFFFF
mem
xFFFFFFFC
Word
Cache
Address
Set
Set
Set
Set
Set
Set
Set
Set
Data
Figure
Mapping
of
main
memory
to
a
direct
mapped
cache
Example
CACHE
FIELDS
Find
the
number
of
set
and
tag
bits
for
a
direct
mapped
cache
with
sets
and
a
one
word
block
size
The
address
size
is
bits
Solution
A
cache
with
sets
requires
log
set
bits
The
two
least
significant
bits
of
the
address
are
the
byte
offset
and
the
remaining
bits
form
the
tag
Sometimes
such
as
when
the
computer
first
starts
up
the
cache
sets
contain
no
data
at
all
The
cache
uses
a
valid
bit
for
each
set
to
indicate
whether
the
set
holds
meaningful
data
If
the
valid
bit
is
the
contents
are
meaningless
Figure
shows
the
hardware
for
the
direct
mapped
cache
of
Figure
The
cache
is
constructed
as
an
eight
entry
SRAM
Each
entry
or
set
contains
one
line
consisting
of
bits
of
data
bits
of
tag
and
valid
bit
The
cache
is
accessed
using
the
bit
address
The
two
least
significant
bits
the
byte
offset
bits
are
ignored
for
word
accesses
The
next
three
bits
the
set
bits
specify
the
entry
or
set
in
the
cache
A
load
instruction
reads
the
specified
entry
from
the
cache
and
checks
the
tag
and
valid
bits
If
the
tag
matches
the
most
significant
CHAPTER
EIGHT
Memory
Systems
Tag
Set
Byte
Offset
Memory
Address
FFFFFF
E
Figure
Cache
fields
for
address
xFFFFFFE
when
mapping
to
the
cache
in
Figure
Tag
Data
Tag
Set
Byte
Offset
Memory
Address
Hit
Data
V
entry
x
bit
SRAM
Set
Set
Set
Set
Set
Set
Set
Set
Figure
Direct
mapped
cache
with
sets
Chap
bits
of
the
address
and
the
valid
bit
is
the
cache
hits
and
the
data
is
returned
to
the
processor
Otherwise
the
cache
misses
and
the
memory
system
must
fetch
the
data
from
main
memory
Example
TEMPORAL
LOCALITY
WITH
A
DIRECT
MAPPED
CACHE
Loops
are
a
common
source
of
temporal
and
spatial
locality
in
applications
Using
the
eight
entry
cache
of
Figure
show
the
contents
of
the
cache
after
executing
the
following
silly
loop
in
MIPS
assembly
code
Assume
that
the
cache
is
initially
empty
What
is
the
miss
rate
addi
t
loop
beq
t
done
lw
t
x
lw
t
xC
lw
t
x
addi
t
t
j
loop
done
Solution
The
program
contains
a
loop
that
repeats
for
five
iterations
Each
iteration
involves
three
memory
accesses
loads
resulting
in
total
memory
accesses
The
first
time
the
loop
executes
the
cache
is
empty
and
the
data
must
be
fetched
from
main
memory
locations
x
xC
and
x
into
cache
sets
and
respectively
However
the
next
four
times
the
loop
executes
the
data
is
found
in
the
cache
Figure
shows
the
contents
of
the
cache
during
the
last
request
to
memory
address
x
The
tags
are
all
because
the
upper
bits
of
the
addresses
are
The
miss
rate
is
Caches
V
Tag
Data
mem
x
Tag
Set
Byte
Offset
Memory
Address
V
mem
x
C
mem
x
Set
Set
Set
Set
Set
Set
Set
Set
Figure
Direct
mapped
cache
contents
When
two
recently
accessed
addresses
map
to
the
same
cache
block
a
conflict
occurs
and
the
most
recently
accessed
address
evicts
the
previous
one
from
the
block
Direct
mapped
caches
have
only
one
block
in
each
set
so
two
addresses
that
map
to
the
same
set
always
cause
a
conflict
The
example
on
the
next
page
illustrates
conflicts
Ch
Example
CACHE
BLOCK
CONFLICT
What
is
the
miss
rate
when
the
following
loop
is
executed
on
the
eight
word
direct
mapped
cache
from
Figure
Assume
that
the
cache
is
initially
empty
addi
t
loop
beq
t
done
lw
t
x
lw
t
x
addi
t
t
j
loop
done
Solution
Memory
addresses
x
and
x
both
map
to
set
During
the
initial
execution
of
the
loop
data
at
address
x
is
loaded
into
set
of
the
cache
Then
data
at
address
x
is
loaded
into
set
evicting
the
data
from
address
x
Upon
the
second
execution
of
the
loop
the
pattern
repeats
and
the
cache
must
refetch
data
at
address
x
evicting
data
from
address
x
The
two
addresses
conflict
and
the
miss
rate
is
Multi
way
Set
Associative
Cache
An
N
way
set
associative
cache
reduces
conflicts
by
providing
N
blocks
in
each
set
where
data
mapping
to
that
set
might
be
found
Each
memory
address
still
maps
to
a
specific
set
but
it
can
map
to
any
one
of
the
N
blocks
in
the
set
Hence
a
direct
mapped
cache
is
another
name
for
a
one
way
set
associative
cache
N
is
also
called
the
degree
of
associativity
of
the
cache
Figure
shows
the
hardware
for
a
C
word
N
way
set
associative
cache
The
cache
now
has
only
S
sets
rather
than
CHAPTER
EIGHT
Memory
Systems
Tag
Data
Tag
Set
Byte
Offset
Memory
Address
Data
Hit
V
V
Tag
Data
Hit
Hit
Hit
Way
Way
Set
Set
Set
Set
Figure
Two
way
set
associative
cache
Chap
Thus
only
log
set
bits
rather
than
are
used
to
select
the
set
The
tag
increases
from
to
bits
Each
set
contains
two
ways
or
degrees
of
associativity
Each
way
consists
of
a
data
block
and
the
valid
and
tag
bits
The
cache
reads
blocks
from
both
ways
in
the
selected
set
and
checks
the
tags
and
valid
bits
for
a
hit
If
a
hit
occurs
in
one
of
the
ways
a
multiplexer
selects
data
from
that
way
Set
associative
caches
generally
have
lower
miss
rates
than
direct
mapped
caches
of
the
same
capacity
because
they
have
fewer
conflicts
However
set
associative
caches
are
usually
slower
and
somewhat
more
expensive
to
build
because
of
the
output
multiplexer
and
additional
comparators
They
also
raise
the
question
of
which
way
to
replace
when
both
ways
are
full
this
is
addressed
further
in
Section
Most
commercial
systems
use
set
associative
caches
Example
SET
ASSOCIATIVE
CACHE
MISS
RATE
Repeat
Example
using
the
eight
word
two
way
set
associative
cache
from
Figure
Solution
Both
memory
accesses
to
addresses
x
and
x
map
to
set
However
the
cache
has
two
ways
so
it
can
accommodate
data
from
both
addresses
During
the
first
loop
iteration
the
empty
cache
misses
both
addresses
and
loads
both
words
of
data
into
the
two
ways
of
set
as
shown
in
Figure
On
the
next
four
iterations
the
cache
hits
Hence
the
miss
rate
is
Recall
that
the
direct
mapped
cache
of
the
same
size
from
Example
had
a
miss
rate
of
Caches
V
Tag
Data
V
Tag
Data
mem
x
mem
x
Way
Way
Set
Set
Set
Set
Figure
Two
way
set
associative
cache
contents
Fully
Associative
Cache
A
fully
associative
cache
contains
a
single
set
with
B
ways
where
B
is
the
number
of
blocks
A
memory
address
can
map
to
a
block
in
any
of
these
ways
A
fully
associative
cache
is
another
name
for
a
B
way
set
associative
cache
with
one
set
Figure
shows
the
SRAM
array
of
a
fully
associative
cache
with
eight
blocks
Upon
a
data
request
eight
tag
comparisons
not
shown
must
be
made
because
the
data
could
be
in
any
block
Similarly
an
multiplexer
chooses
the
proper
data
if
a
hit
occurs
Fully
associative
caches
tend
Ch
to
have
the
fewest
conflict
misses
for
a
given
cache
capacity
but
they
require
more
hardware
for
additional
tag
comparisons
They
are
best
suited
to
relatively
small
caches
because
of
the
large
number
of
comparators
Block
Size
The
previous
examples
were
able
to
take
advantage
only
of
temporal
locality
because
the
block
size
was
one
word
To
exploit
spatial
locality
a
cache
uses
larger
blocks
to
hold
several
consecutive
words
The
advantage
of
a
block
size
greater
than
one
is
that
when
a
miss
occurs
and
the
word
is
fetched
into
the
cache
the
adjacent
words
in
the
block
are
also
fetched
Therefore
subsequent
accesses
are
more
likely
to
hit
because
of
spatial
locality
However
a
large
block
size
means
that
a
fixed
size
cache
will
have
fewer
blocks
This
may
lead
to
more
conflicts
increasing
the
miss
rate
Moreover
it
takes
more
time
to
fetch
the
missing
cache
block
after
a
miss
because
more
than
one
data
word
is
fetched
from
main
memory
The
time
required
to
load
the
missing
block
into
the
cache
is
called
the
miss
penalty
If
the
adjacent
words
in
the
block
are
not
accessed
later
the
effort
of
fetching
them
is
wasted
Nevertheless
most
real
programs
benefit
from
larger
block
sizes
Figure
shows
the
hardware
for
a
C
word
direct
mapped
cache
with
a
b
word
block
size
The
cache
now
has
only
B
C
b
blocks
A
direct
mapped
cache
has
one
block
in
each
set
so
this
cache
is
CHAPTER
EIGHT
Memory
Systems
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
Way
Way
Way
Way
Way
Way
Way
Way
Tag
Data
Tag
Byte
Offset
Memory
Address
Data
V
Block
Offset
Hit
Set
Set
Set
Figure
Eight
block
fully
associative
cache
Figure
Direct
mapped
cache
with
two
sets
and
a
four
word
block
size
Chap
organized
as
two
sets
Thus
only
log
bit
is
used
to
select
the
set
A
multiplexer
is
now
needed
to
select
the
word
within
the
block
The
multiplexer
is
controlled
by
the
log
block
offset
bits
of
the
address
The
most
significant
address
bits
form
the
tag
Only
one
tag
is
needed
for
the
entire
block
because
the
words
in
the
block
are
at
consecutive
addresses
Figure
shows
the
cache
fields
for
address
x
C
when
it
maps
to
the
direct
mapped
cache
of
Figure
The
byte
offset
bits
are
always
for
word
accesses
The
next
log
b
block
offset
bits
indicate
the
word
within
the
block
And
the
next
bit
indicates
the
set
The
remaining
bits
are
the
tag
Therefore
word
x
C
maps
to
set
word
in
the
cache
The
principle
of
using
larger
block
sizes
to
exploit
spatial
locality
also
applies
to
associative
caches
Example
SPATIAL
LOCALITY
WITH
A
DIRECT
MAPPED
CACHE
Repeat
Example
for
the
eight
word
direct
mapped
cache
with
a
four
word
block
size
Solution
Figure
shows
the
contents
of
the
cache
after
the
first
memory
access
On
the
first
loop
iteration
the
cache
misses
on
the
access
to
memory
address
x
This
access
loads
data
at
addresses
x
through
xC
into
the
cache
block
All
subsequent
accesses
as
shown
for
address
xC
hit
in
the
cache
Hence
the
miss
rate
is
Caches
Tag
Byte
Offset
Memory
Address
Block
Offset
C
Set
Figure
Cache
fields
for
address
x
C
when
mapping
to
the
cache
of
Figure
Set
V
Tag
Data
mem
x
C
Set
mem
x
mem
x
mem
x
Tag
Byte
Offset
Memory
Address
V
Block
Set
Offset
Figure
Cache
contents
with
a
block
size
b
of
four
words
Putting
It
All
Together
Caches
are
organized
as
two
dimensional
arrays
The
rows
are
called
sets
and
the
columns
are
called
ways
Each
entry
in
the
array
consists
of
Chap
a
data
block
and
its
associated
valid
and
tag
bits
Caches
are
characterized
by
capacity
C
block
size
b
and
number
of
blocks
B
C
b
number
of
blocks
in
a
set
N
Table
summarizes
the
various
cache
organizations
Each
address
in
memory
maps
to
only
one
set
but
can
be
stored
in
any
of
the
ways
Cache
capacity
associativity
set
size
and
block
size
are
typically
powers
of
This
makes
the
cache
fields
tag
set
and
block
offset
bits
subsets
of
the
address
bits
Increasing
the
associativity
N
usually
reduces
the
miss
rate
caused
by
conflicts
But
higher
associativity
requires
more
tag
comparators
Increasing
the
block
size
b
takes
advantage
of
spatial
locality
to
reduce
the
miss
rate
However
it
decreases
the
number
of
sets
in
a
fixed
sized
cache
and
therefore
could
lead
to
more
conflicts
It
also
increases
the
miss
penalty
What
Data
Is
Replaced
In
a
direct
mapped
cache
each
address
maps
to
a
unique
block
and
set
If
a
set
is
full
when
new
data
must
be
loaded
the
block
in
that
set
is
replaced
with
the
new
data
In
set
associative
and
fully
associative
caches
the
cache
must
choose
which
block
to
evict
when
a
cache
set
is
full
The
principle
of
temporal
locality
suggests
that
the
best
choice
is
to
evict
the
least
recently
used
block
because
it
is
least
likely
to
be
used
again
soon
Hence
most
associative
caches
have
a
least
recently
used
LRU
replacement
policy
In
a
two
way
set
associative
cache
a
use
bit
U
indicates
which
way
within
a
set
was
least
recently
used
Each
time
one
of
the
ways
is
used
U
is
adjusted
to
indicate
the
other
way
For
set
associative
caches
with
more
than
two
ways
tracking
the
least
recently
used
way
becomes
complicated
To
simplify
the
problem
the
ways
are
often
divided
into
two
groups
and
U
indicates
which
group
of
ways
was
least
recently
used
CHAPTER
EIGHT
Memory
Systems
Table
Cache
organizations
Number
of
Ways
Number
of
Sets
Organization
N
S
direct
mapped
B
set
associative
N
B
B
N
fully
associative
B
Chap
Upon
replacement
the
new
block
replaces
a
random
block
within
the
least
recently
used
group
Such
a
policy
is
called
pseudo
LRU
and
is
good
enough
in
practice
Example
LRU
REPLACEMENT
Show
the
contents
of
an
eight
word
two
way
set
associative
cache
after
executing
the
following
code
Assume
LRU
replacement
a
block
size
of
one
word
and
an
initially
empty
cache
lw
t
x
lw
t
x
lw
t
x
Solution
The
first
two
instructions
load
data
from
memory
addresses
x
and
x
into
set
of
the
cache
shown
in
Figure
a
U
indicates
that
data
in
way
was
the
least
recently
used
The
next
memory
access
to
address
x
also
maps
to
set
and
replaces
the
least
recently
used
data
in
way
as
shown
in
Figure
b
The
use
bit
U
is
set
to
to
indicate
that
data
in
way
was
the
least
recently
used
Caches
V
Tag
Data
V
Tag
Data
U
mem
x
mem
x
a
Way
Way
Set
Set
Set
Set
V
Tag
Data
V
Tag
Data
U
mem
x
mem
x
b
Way
Way
Set
Set
Set
Set
Figure
Two
way
associative
cache
with
LRU
replacement
Advanced
Cache
Design
Modern
systems
use
multiple
levels
of
caches
to
decrease
memory
access
time
This
section
explores
the
performance
of
a
two
level
caching
system
and
examines
how
block
size
associativity
and
cache
capacity
affect
miss
rate
The
section
also
describes
how
caches
handle
stores
or
writes
by
using
a
write
through
or
write
back
policy
C
Multiple
Level
Caches
Large
caches
are
beneficial
because
they
are
more
likely
to
hold
data
of
interest
and
therefore
have
lower
miss
rates
However
large
caches
tend
to
be
slower
than
small
ones
Modern
systems
often
use
two
levels
of
caches
as
shown
in
Figure
The
first
level
L
cache
is
small
enough
to
provide
a
one
or
two
cycle
access
time
The
second
level
L
cache
is
also
built
from
SRAM
but
is
larger
and
therefore
slower
than
the
L
cache
The
processor
first
looks
for
the
data
in
the
L
cache
If
the
L
cache
misses
the
processor
looks
in
the
L
cache
If
the
L
cache
misses
the
processor
fetches
the
data
from
main
memory
Some
modern
systems
add
even
more
levels
of
cache
to
the
memory
hierarchy
because
accessing
main
memory
is
so
slow
Example
SYSTEM
WITH
AN
L
CACHE
Use
the
system
of
Figure
with
access
times
of
and
cycles
for
the
L
cache
L
cache
and
main
memory
respectively
Assume
that
the
L
and
L
caches
have
miss
rates
of
and
respectively
Specifically
of
the
of
accesses
that
miss
the
L
cache
of
those
also
miss
the
L
cache
What
is
the
average
memory
access
time
AMAT
Solution
Each
memory
access
checks
the
L
cache
When
the
L
cache
misses
of
the
time
the
processor
checks
the
L
cache
When
the
L
cache
misses
of
the
time
the
processor
fetches
the
data
from
main
memory
Using
Equation
we
calculate
the
average
memory
access
time
as
follows
cycle
cycles
cycles
cycles
The
L
miss
rate
is
high
because
it
receives
only
the
hard
memory
accesses
those
that
miss
in
the
L
cache
If
all
accesses
went
directly
to
the
L
cache
the
L
miss
rate
would
be
about
CHAPTER
EIGHT
Memory
Systems
L
Cache
L
Cache
Main
Memory
Capacity
Virtual
Memory
Speed
Figure
Memory
hierarchy
with
two
levels
of
cache
C
Reducing
Miss
Rate
Cache
misses
can
be
reduced
by
changing
capacity
block
size
and
or
associativity
The
first
step
to
reducing
the
miss
rate
is
to
understand
the
causes
of
the
misses
The
misses
can
be
classified
as
compulsory
capacity
and
conflict
The
first
request
to
a
cache
block
is
called
a
compulsory
miss
because
the
block
must
be
read
from
memory
regardless
of
the
cache
design
Capacity
misses
occur
when
the
cache
is
too
small
to
hold
all
concurrently
used
data
Conflict
misses
are
caused
when
several
addresses
map
to
the
same
set
and
evict
blocks
that
are
still
needed
Changing
cache
parameters
can
affect
one
or
more
type
of
cache
miss
For
example
increasing
cache
capacity
can
reduce
conflict
and
capacity
misses
but
it
does
not
affect
compulsory
misses
On
the
other
hand
increasing
block
size
could
reduce
compulsory
misses
due
to
spatial
locality
but
might
actually
increase
conflict
misses
because
more
addresses
would
map
to
the
same
set
and
could
conflict
Memory
systems
are
complicated
enough
that
the
best
way
to
evaluate
their
performance
is
by
running
benchmarks
while
varying
cache
parameters
Figure
plots
miss
rate
versus
cache
size
and
degree
of
associativity
for
the
SPEC
benchmark
This
benchmark
has
a
small
number
of
compulsory
misses
shown
by
the
dark
region
near
the
x
axis
As
expected
when
cache
size
increases
capacity
misses
decrease
Increased
associativity
especially
for
small
caches
decreases
the
number
of
conflict
misses
shown
along
the
top
of
the
curve
Caches
way
way
way
way
Capacity
Compulsory
Cache
Size
KB
Miss
Rate
per
Type
Figure
Miss
rate
versus
cache
size
and
associativity
on
SPEC
benchmark
Adapted
with
permission
from
Hennessy
and
Patterson
Computer
Architecture
A
Quantitative
Approach
rd
ed
Morgan
Kaufmann
Increasing
associativity
beyond
four
or
eight
ways
provides
only
small
decreases
in
miss
rate
As
mentioned
miss
rate
can
also
be
decreased
by
using
larger
block
sizes
that
take
advantage
of
spatial
locality
But
as
block
size
increases
the
number
of
sets
in
a
fixed
size
cache
decreases
increasing
the
probability
of
conflicts
Figure
plots
miss
rate
versus
block
size
in
number
of
bytes
for
caches
of
varying
capacity
For
small
caches
such
as
the
KB
cache
increasing
the
block
size
beyond
bytes
increases
the
miss
rate
because
of
conflicts
For
larger
caches
increasing
the
block
size
does
not
change
the
miss
rate
However
large
block
sizes
might
still
increase
execution
time
because
of
the
larger
miss
penalty
the
time
required
to
fetch
the
missing
cache
block
from
main
memory
on
a
miss
Write
Policy
The
previous
sections
focused
on
memory
loads
Memory
stores
or
writes
follow
a
similar
procedure
as
loads
Upon
a
memory
store
the
processor
checks
the
cache
If
the
cache
misses
the
cache
block
is
fetched
from
main
memory
into
the
cache
and
then
the
appropriate
word
in
the
cache
block
is
written
If
the
cache
hits
the
word
is
simply
written
to
the
cache
block
Caches
are
classified
as
either
write
through
or
write
back
In
a
write
through
cache
the
data
written
to
a
cache
block
is
simultaneously
written
to
main
memory
In
a
write
back
cache
a
dirty
bit
D
is
associated
with
each
cache
block
D
is
when
the
cache
block
has
been
written
and
otherwise
Dirty
cache
blocks
are
written
back
to
main
memory
only
when
they
are
evicted
from
the
cache
A
write
through
CHAPTER
EIGHT
Memory
Systems
Miss
Rate
Block
Size
K
K
K
K
Figure
Miss
rate
versus
block
size
and
cache
size
on
SPEC
benchmark
Adapted
with
permission
from
Hennessy
and
Patterson
Computer
Architecture
A
Quantitative
Approach
rd
ed
Morgan
Kaufmann
cache
requires
no
dirty
bit
but
usually
requires
more
main
memory
writes
than
a
write
back
cache
Modern
caches
are
usually
write
back
because
main
memory
access
time
is
so
large
Example
WRITE
THROUGH
VERSUS
WRITE
BACK
Suppose
a
cache
has
a
block
size
of
four
words
How
many
main
memory
accesses
are
required
by
the
following
code
when
using
each
write
policy
writethrough
or
write
back
sw
t
x
sw
t
xC
sw
t
x
sw
t
x
Solution
All
four
store
instructions
write
to
the
same
cache
block
With
a
writethrough
cache
each
store
instruction
writes
a
word
to
main
memory
requiring
four
main
memory
writes
A
write
back
policy
requires
only
one
main
memory
access
when
the
dirty
cache
block
is
evicted
The
Evolution
of
MIPS
Caches
Table
traces
the
evolution
of
cache
organizations
used
by
the
MIPS
processor
from
to
The
major
trends
are
the
introduction
of
multiple
levels
of
cache
larger
cache
capacity
and
increased
associativity
These
trends
are
driven
by
the
growing
disparity
between
CPU
frequency
and
main
memory
speed
and
the
decreasing
cost
of
transistors
The
growing
difference
between
CPU
and
memory
speeds
necessitates
a
lower
miss
rate
to
avoid
the
main
memory
bottleneck
and
the
decreasing
cost
of
transistors
allows
larger
cache
sizes
Caches
Table
MIPS
cache
evolution
Year
CPU
MHz
L
Cache
L
Cache
R
none
none
R
KB
direct
mapped
none
R
KB
direct
mapped
MB
direct
mapped
R
KB
two
way
MB
two
way
R
KB
two
way
MB
two
way
R
A
KB
two
way
MB
two
way
Adapted
from
D
Sweetman
See
MIPS
Run
Morgan
Kaufmann
VIRTUAL
MEMORY
Most
modern
computer
systems
use
a
hard
disk
also
called
a
hard
drive
as
the
lowest
level
in
the
memory
hierarchy
see
Figure
Compared
with
the
ideal
large
fast
cheap
memory
a
hard
disk
is
large
and
cheap
but
terribly
slow
The
disk
provides
a
much
larger
capacity
than
is
possible
with
a
cost
effective
main
memory
DRAM
However
if
a
significant
fraction
of
memory
accesses
involve
the
disk
performance
is
dismal
You
may
have
encountered
this
on
a
PC
when
running
too
many
programs
at
once
Figure
shows
a
hard
disk
with
the
lid
of
its
case
removed
As
the
name
implies
the
hard
disk
contains
one
or
more
rigid
disks
or
platters
each
of
which
has
a
read
write
head
on
the
end
of
a
long
triangular
arm
The
head
moves
to
the
correct
location
on
the
disk
and
reads
or
writes
data
magnetically
as
the
disk
rotates
beneath
it
The
head
takes
several
milliseconds
to
seek
the
correct
location
on
the
disk
which
is
fast
from
a
human
perspective
but
millions
of
times
slower
than
the
processor
CHAPTER
EIGHT
Memory
Systems
Figure
Hard
disk
The
objective
of
adding
a
hard
disk
to
the
memory
hierarchy
is
to
inexpensively
give
the
illusion
of
a
very
large
memory
while
still
providing
the
speed
of
faster
memory
for
most
accesses
A
computer
with
only
MB
of
DRAM
for
example
could
effectively
provide
GB
of
memory
using
the
hard
disk
This
larger
GB
memory
is
called
virtual
memory
and
the
smaller
MB
main
memory
is
called
physical
memory
We
will
use
the
term
physical
memory
to
refer
to
main
memory
throughout
this
section
Programs
can
access
data
anywhere
in
virtual
memory
so
they
must
use
virtual
addresses
that
specify
the
location
in
virtual
memory
The
physical
memory
holds
a
subset
of
most
recently
accessed
virtual
memory
In
this
way
physical
memory
acts
as
a
cache
for
virtual
memory
Thus
most
accesses
hit
in
physical
memory
at
the
speed
of
DRAM
yet
the
program
enjoys
the
capacity
of
the
larger
virtual
memory
Virtual
memory
systems
use
different
terminologies
for
the
same
caching
principles
discussed
in
Section
Table
summarizes
the
analogous
terms
Virtual
memory
is
divided
into
virtual
pages
typically
KB
in
size
Physical
memory
is
likewise
divided
into
physical
pages
of
the
same
size
A
virtual
page
may
be
located
in
physical
memory
DRAM
or
on
the
disk
For
example
Figure
shows
a
virtual
memory
that
is
larger
than
physical
memory
The
rectangles
indicate
pages
Some
virtual
pages
are
present
in
physical
memory
and
some
are
located
on
the
disk
The
process
of
determining
the
physical
address
from
the
virtual
address
is
called
address
translation
If
the
processor
attempts
to
access
a
virtual
address
that
is
not
in
physical
memory
a
page
fault
occurs
and
the
operating
system
loads
the
page
from
the
hard
disk
into
physical
memory
To
avoid
page
faults
caused
by
conflicts
any
virtual
page
can
map
to
any
physical
page
In
other
words
physical
memory
behaves
as
a
fully
associative
cache
for
virtual
memory
In
a
conventional
fully
associative
cache
every
cache
block
has
a
comparator
that
checks
the
most
significant
address
bits
against
a
tag
to
determine
whether
the
request
hits
in
Virtual
Memory
A
computer
with
bit
addresses
can
access
a
maximum
of
bytes
GB
of
memory
This
is
one
of
the
motivations
for
moving
to
bit
computers
which
can
access
far
more
memory
Table
Analogous
cache
and
virtual
memory
terms
Cache
Virtual
Memory
Block
Page
Block
size
Page
size
Block
offset
Page
offset
Miss
Page
fault
Tag
Virtual
page
number
C
the
block
In
an
analogous
virtual
memory
system
each
physical
page
would
need
a
comparator
to
check
the
most
significant
virtual
address
bits
against
a
tag
to
determine
whether
the
virtual
page
maps
to
that
physical
page
A
realistic
virtual
memory
system
has
so
many
physical
pages
that
providing
a
comparator
for
each
page
would
be
excessively
expensive
Instead
the
virtual
memory
system
uses
a
page
table
to
perform
address
translation
A
page
table
contains
an
entry
for
each
virtual
page
indicating
its
location
in
physical
memory
or
that
it
is
on
the
disk
Each
load
or
store
instruction
requires
a
page
table
access
followed
by
a
physical
memory
access
The
page
table
access
translates
the
virtual
address
used
by
the
program
to
a
physical
address
The
physical
address
is
then
used
to
actually
read
or
write
the
data
The
page
table
is
usually
so
large
that
it
is
located
in
physical
memory
Hence
each
load
or
store
involves
two
physical
memory
accesses
a
page
table
access
and
a
data
access
To
speed
up
address
translation
a
translation
lookaside
buffer
TLB
caches
the
most
commonly
used
page
table
entries
The
remainder
of
this
section
elaborates
on
address
translation
page
tables
and
TLBs
Address
Translation
In
a
system
with
virtual
memory
programs
use
virtual
addresses
so
that
they
can
access
a
large
memory
The
computer
must
translate
these
virtual
addresses
to
either
find
the
address
in
physical
memory
or
take
a
page
fault
and
fetch
the
data
from
the
hard
disk
Recall
that
virtual
memory
and
physical
memory
are
divided
into
pages
The
most
significant
bits
of
the
virtual
or
physical
address
specify
the
virtual
or
physical
page
number
The
least
significant
bits
specify
the
word
within
the
page
and
are
called
the
page
offset
CHAPTER
EIGHT
Memory
Systems
Physical
Memory
Physical
Addresses
Virtual
Addresses
Hard
Disk
Address
Translation
Figure
Virtual
and
physical
pages
Figure
illustrates
the
page
organization
of
a
virtual
memory
system
with
GB
of
virtual
memory
and
MB
of
physical
memory
divided
into
KB
pages
MIPS
accommodates
bit
addresses
With
a
GB
byte
virtual
memory
only
the
least
significant
virtual
address
bits
are
used
the
nd
bit
is
always
Similarly
with
a
MB
byte
physical
memory
only
the
least
significant
physical
address
bits
are
used
the
upper
bits
are
always
Because
the
page
size
is
KB
bytes
there
are
virtual
pages
and
physical
pages
Thus
the
virtual
and
physical
page
numbers
are
and
bits
respectively
Physical
memory
can
only
hold
up
to
th
of
the
virtual
pages
at
any
given
time
The
rest
of
the
virtual
pages
are
kept
on
disk
Figure
shows
virtual
page
mapping
to
physical
page
virtual
page
x
FFFC
mapping
to
physical
page
x
FFE
and
so
forth
For
example
virtual
address
x
F
an
offset
of
x
F
within
virtual
page
maps
to
physical
address
x
F
an
offset
of
x
F
within
physical
page
The
least
significant
bits
of
the
virtual
and
physical
addresses
are
the
same
x
F
and
specify
the
page
offset
within
the
virtual
and
physical
pages
Only
the
page
number
needs
to
be
translated
to
obtain
the
physical
address
from
the
virtual
address
Figure
illustrates
the
translation
of
a
virtual
address
to
a
physical
address
The
least
significant
bits
indicate
the
page
offset
and
require
no
translation
The
upper
bits
of
the
virtual
address
specify
the
virtual
page
number
VPN
and
are
translated
to
a
bit
physical
page
number
PPN
The
next
two
sections
describe
how
page
tables
and
TLBs
are
used
to
perform
this
address
translation
Virtual
Memory
Physical
Memory
Physical
Page
Number
Physical
Addresses
Virtual
Memory
Virtual
Page
Number
Virtual
Addresses
FFF
x
FFF
x
FFFFFF
x
FFE
x
FFEFFF
x
x
FFF
x
x
FFF
FFE
FFFA
FFF
FFFC
FFFB
FFFE
FFFD
FFFF
x
FFFF
x
FFFFFFF
x
FFFE
x
FFFEFFF
x
FFFD
x
FFFDFFF
x
FFFC
x
FFFCFFF
x
FFFB
x
FFFBFFF
x
FFFA
x
FFFAFFF
x
x
FFF
x
x
FFF
x
x
FFF
x
FFF
x
FFF
FFF
x
x
FFF
x
x
FFF
x
x
FFF
x
x
FFF
Figure
Physical
and
virtual
pages
Chapt
Example
VIRTUAL
ADDRESS
TO
PHYSICAL
ADDRESS
TRANSLATION
Find
the
physical
address
of
virtual
address
x
C
using
the
virtual
memory
system
shown
in
Figure
Solution
The
bit
page
offset
x
C
requires
no
translation
The
remaining
bits
of
the
virtual
address
give
the
virtual
page
number
so
virtual
address
x
C
is
found
in
virtual
page
x
In
Figure
virtual
page
x
maps
to
physical
page
x
FFF
Thus
virtual
address
x
C
maps
to
physical
address
x
FFF
C
The
Page
Table
The
processor
uses
a
page
table
to
translate
virtual
addresses
to
physical
addresses
Recall
that
the
page
table
contains
an
entry
for
each
virtual
page
This
entry
contains
a
physical
page
number
and
a
valid
bit
If
the
valid
bit
is
the
virtual
page
maps
to
the
physical
page
specified
in
the
entry
Otherwise
the
virtual
page
is
found
on
disk
Because
the
page
table
is
so
large
it
is
stored
in
physical
memory
Let
us
assume
for
now
that
it
is
stored
as
a
contiguous
array
as
shown
in
Figure
This
page
table
contains
the
mapping
of
the
memory
system
of
Figure
The
page
table
is
indexed
with
the
virtual
page
number
VPN
For
example
entry
specifies
that
virtual
page
maps
to
physical
page
Entry
is
invalid
V
so
virtual
page
is
located
on
disk
Example
USING
THE
PAGE
TABLE
TO
PERFORM
ADDRESS
TRANSLATION
Find
the
physical
address
of
virtual
address
x
C
using
the
page
table
shown
in
Figure
Solution
Figure
shows
the
virtual
address
to
physical
address
translation
for
virtual
address
x
C
The
bit
page
offset
requires
no
translation
The
remaining
bits
of
the
virtual
address
are
the
virtual
page
number
x
and
CHAPTER
EIGHT
Memory
Systems
PPN
Page
Offset
VPN
Page
Offset
Virtual
Address
Physical
Address
Translation
Figure
Translation
from
virtual
address
to
physical
address
Page
Table
Virtual
Page
Number
FFFA
FFFC
FFFB
FFFE
FFFD
FFFF
V
Physical
Page
Number
x
x
FFE
x
x
FFF
Figure
The
page
table
for
Figure
C
The
page
table
can
be
stored
anywhere
in
physical
memory
at
the
discretion
of
the
OS
The
processor
typically
uses
a
dedicated
register
called
the
page
table
register
to
store
the
base
address
of
the
page
table
in
physical
memory
To
perform
a
load
or
store
the
processor
must
first
translate
the
virtual
address
to
a
physical
address
and
then
access
the
data
at
that
physical
address
The
processor
extracts
the
virtual
page
number
from
the
virtual
address
and
adds
it
to
the
page
table
register
to
find
the
physical
address
of
the
page
table
entry
The
processor
then
reads
this
page
table
entry
from
physical
memory
to
obtain
the
physical
page
number
If
the
entry
is
valid
it
merges
this
physical
page
number
with
the
page
offset
to
create
the
physical
address
Finally
it
reads
or
writes
data
at
this
physical
address
Because
the
page
table
is
stored
in
physical
memory
each
load
or
store
involves
two
physical
memory
accesses
Virtual
Memory
x
x
FFE
x
Page
Table
x
FFF
V
Virtual
Address
x
C
Hit
Physical
Page
Number
Virtual
Page
Number
Page
Offset
Physical
Address
x
FFF
C
Figure
Address
translation
using
the
page
table
give
the
index
into
the
page
table
The
page
table
maps
virtual
page
x
to
physical
page
x
FFF
So
virtual
address
x
C
maps
to
physical
address
x
FFF
C
The
least
significant
bits
are
the
same
in
both
the
physical
and
the
virtual
address
The
Translation
Lookaside
Buffer
Virtual
memory
would
have
a
severe
performance
impact
if
it
required
a
page
table
read
on
every
load
or
store
doubling
the
delay
of
loads
and
stores
Fortunately
page
table
accesses
have
great
temporal
locality
The
temporal
and
spatial
locality
of
data
accesses
and
the
large
page
size
mean
that
many
consecutive
loads
or
stores
are
likely
to
reference
the
same
page
Therefore
if
the
processor
remembers
the
last
page
table
entry
that
it
read
it
can
probably
reuse
this
translation
without
rereading
the
page
table
In
general
the
processor
can
keep
the
last
several
page
table
entries
in
a
small
cache
called
a
translation
lookaside
buffer
TLB
The
processor
looks
aside
to
find
the
translation
in
the
TLB
before
having
to
access
the
page
table
in
physical
memory
In
real
programs
the
vast
majority
of
accesses
hit
in
the
TLB
avoiding
the
time
consuming
page
table
reads
from
physical
memory
A
TLB
is
organized
as
a
fully
associative
cache
and
typically
holds
to
entries
Each
TLB
entry
holds
a
virtual
page
number
and
its
corresponding
physical
page
number
The
TLB
is
accessed
using
the
virtual
page
number
If
the
TLB
hits
it
returns
the
corresponding
physical
page
number
Otherwise
the
processor
must
read
the
page
table
in
physical
memory
The
TLB
is
designed
to
be
small
enough
that
it
can
be
accessed
in
less
than
one
cycle
Even
so
TLBs
typically
have
a
hit
rate
of
greater
than
The
TLB
decreases
the
number
of
memory
accesses
required
for
most
load
or
store
instructions
from
two
to
one
Example
USING
THE
TLB
TO
PERFORM
ADDRESS
TRANSLATION
Consider
the
virtual
memory
system
of
Figure
Use
a
two
entry
TLB
or
explain
why
a
page
table
access
is
necessary
to
translate
virtual
addresses
x
C
and
x
FB
to
physical
addresses
Suppose
the
TLB
currently
holds
valid
translations
of
virtual
pages
x
and
x
FFFD
Solution
Figure
shows
the
two
entry
TLB
with
the
request
for
virtual
address
x
C
The
TLB
receives
the
virtual
page
number
of
the
incoming
address
x
and
compares
it
to
the
virtual
page
number
of
each
entry
Entry
matches
and
is
valid
so
the
request
hits
The
translated
physical
address
is
the
physical
page
number
of
the
matching
entry
x
FFF
concatenated
with
the
page
offset
of
the
virtual
address
As
always
the
page
offset
requires
no
translation
The
request
for
virtual
address
x
FB
misses
in
the
TLB
So
the
request
is
forwarded
to
the
page
table
for
translation
CHAPTER
EIGHT
Memory
Systems
Memory
Protection
So
far
this
section
has
focused
on
using
virtual
memory
to
provide
a
fast
inexpensive
large
memory
An
equally
important
reason
to
use
virtual
memory
is
to
provide
protection
between
concurrently
running
programs
As
you
probably
know
modern
computers
typically
run
several
programs
or
processes
at
the
same
time
All
of
the
programs
are
simultaneously
present
in
physical
memory
In
a
well
designed
computer
system
the
programs
should
be
protected
from
each
other
so
that
no
program
can
crash
or
hijack
another
program
Specifically
no
program
should
be
able
to
access
another
program
s
memory
without
permission
This
is
called
memory
protection
Virtual
memory
systems
provide
memory
protection
by
giving
each
program
its
own
virtual
address
space
Each
program
can
use
as
much
memory
as
it
wants
in
that
virtual
address
space
but
only
a
portion
of
the
virtual
address
space
is
in
physical
memory
at
any
given
time
Each
program
can
use
its
entire
virtual
address
space
without
having
to
worry
about
where
other
programs
are
physically
located
However
a
program
can
access
only
those
physical
pages
that
are
mapped
in
its
page
table
In
this
way
a
program
cannot
accidentally
or
maliciously
access
another
program
s
physical
pages
because
they
are
not
mapped
in
its
page
table
In
some
cases
multiple
programs
access
common
instructions
or
data
The
operating
system
adds
control
bits
to
each
page
table
entry
to
determine
which
programs
if
any
can
write
to
the
shared
physical
pages
Virtual
Memory
Hit
V
Hit
Hit
Hit
Virtual
Page
Number
Physical
Page
Number
Entry
x
FFFD
x
x
x
FFF
Virtual
Address
x
C
Virtual
Page
Number
Page
Offset
V
Virtual
Page
Number
Physical
Page
Number
Entry
Physical
Address
x
FFF
C
TLB
Figure
Address
translation
using
a
two
entry
TLB
Replacement
Policies
Virtual
memory
systems
use
write
back
and
an
approximate
least
recently
used
LRU
replacement
policy
A
write
through
policy
where
each
write
to
physical
memory
initiates
a
write
to
disk
would
be
impractical
Store
instructions
would
operate
at
the
speed
of
the
disk
instead
of
the
speed
of
the
processor
milliseconds
instead
of
nanoseconds
Under
the
writeback
policy
the
physical
page
is
written
back
to
disk
only
when
it
is
evicted
from
physical
memory
Writing
the
physical
page
back
to
disk
and
reloading
it
with
a
different
virtual
page
is
called
swapping
so
the
disk
in
a
virtual
memory
system
is
sometimes
called
swap
space
The
processor
swaps
out
one
of
the
least
recently
used
physical
pages
when
a
page
fault
occurs
then
replaces
that
page
with
the
missing
virtual
page
To
support
these
replacement
policies
each
page
table
entry
contains
two
additional
status
bits
a
dirty
bit
D
and
a
use
bit
U
The
dirty
bit
is
if
any
store
instructions
have
changed
the
physical
page
since
it
was
read
from
disk
When
a
physical
page
is
swapped
out
it
needs
to
be
written
back
to
disk
only
if
its
dirty
bit
is
otherwise
the
disk
already
holds
an
exact
copy
of
the
page
The
use
bit
is
if
the
physical
page
has
been
accessed
recently
As
in
a
cache
system
exact
LRU
replacement
would
be
impractically
complicated
Instead
the
OS
approximates
LRU
replacement
by
periodically
resetting
all
the
use
bits
in
the
page
table
When
a
page
is
accessed
its
use
bit
is
set
to
Upon
a
page
fault
the
OS
finds
a
page
with
U
to
swap
out
of
physical
memory
Thus
it
does
not
necessarily
replace
the
least
recently
used
page
just
one
of
the
least
recently
used
pages
Multilevel
Page
Tables
Page
tables
can
occupy
a
large
amount
of
physical
memory
For
example
the
page
table
from
the
previous
sections
for
a
GB
virtual
memory
with
KB
pages
would
need
entries
If
each
entry
is
bytes
the
page
table
is
bytes
bytes
MB
To
conserve
physical
memory
page
tables
can
be
broken
up
into
multiple
usually
two
levels
The
first
level
page
table
is
always
kept
in
physical
memory
It
indicates
where
small
second
level
page
tables
are
stored
in
virtual
memory
The
second
level
page
tables
each
contain
the
actual
translations
for
a
range
of
virtual
pages
If
a
particular
range
of
translations
is
not
actively
used
the
corresponding
second
level
page
table
can
be
swapped
out
to
the
hard
disk
so
it
does
not
waste
physical
memory
In
a
two
level
page
table
the
virtual
page
number
is
split
into
two
parts
the
page
table
number
and
the
page
table
offset
as
shown
in
Figure
The
page
table
number
indexes
the
first
level
page
table
which
must
reside
in
physical
memory
The
first
level
page
table
entry
gives
the
base
address
of
the
second
level
page
table
or
indicates
that
CHAPTER
EIGHT
Memory
Systems
Cha
it
must
be
fetched
from
disk
when
V
is
The
page
table
offset
indexes
the
second
level
page
table
The
remaining
bits
of
the
virtual
address
are
the
page
offset
as
before
for
a
page
size
of
KB
In
Figure
the
bit
virtual
page
number
is
broken
into
and
bits
to
indicate
the
page
table
number
and
the
page
table
offset
respectively
Thus
the
first
level
page
table
has
entries
Each
of
these
second
level
page
tables
has
K
entries
If
each
of
the
first
and
second
level
page
table
entries
is
bits
bytes
and
only
two
secondlevel
page
tables
are
present
in
physical
memory
at
once
the
hierarchical
page
table
uses
only
bytes
K
bytes
KB
of
physical
memory
The
two
level
page
table
requires
a
fraction
of
the
physical
memory
needed
to
store
the
entire
page
table
MB
The
drawback
of
a
two
level
page
table
is
that
it
adds
yet
another
memory
access
for
translation
when
the
TLB
misses
Example
USING
A
MULTILEVEL
PAGE
TABLE
FOR
ADDRESS
TRANSLATION
Figure
shows
the
possible
contents
of
the
two
level
page
table
from
Figure
The
contents
of
only
one
second
level
page
table
are
shown
Using
this
two
level
page
table
describe
what
happens
on
an
access
to
virtual
address
x
FEFB
Virtual
Memory
First
Level
Page
Table
Page
Table
Address
K
entries
entries
Page
Table
Number
Page
Table
Offset
Virtual
Address
V
Physical
Page
V
Number
Second
Level
Page
Tables
Page
Offset
Figure
Hierarchical
page
tables
Chap
MEMORY
MAPPED
I
O
Processors
also
use
the
memory
interface
to
communicate
with
input
output
I
O
devices
such
as
keyboards
monitors
and
printers
A
processor
accesses
an
I
O
device
using
the
address
and
data
busses
in
the
same
way
that
it
accesses
memory
A
portion
of
the
address
space
is
dedicated
to
I
O
devices
rather
than
memory
For
example
suppose
that
addresses
in
the
range
CHAPTER
EIGHT
Memory
Systems
Page
Table
Address
Page
Table
Number
Page
Table
Offset
Virtual
Address
V
Physical
Page
V
Number
Page
Offset
x
Valid
x
First
Level
Page
Table
Second
Level
Page
Tables
x
FE
FB
Valid
Physical
Address
x
F
FB
x
FFE
x
x
FC
x
C
x
x
F
Figure
Address
translation
using
a
two
level
page
table
Solution
As
always
only
the
virtual
page
number
requires
translation
The
most
significant
nine
bits
of
the
virtual
address
x
give
the
page
table
number
the
index
into
the
first
level
page
table
The
first
level
page
table
at
entry
x
indicates
that
the
second
level
page
table
is
resident
in
memory
V
and
its
physical
address
is
x
The
next
ten
bits
of
the
virtual
address
x
FE
are
the
page
table
offset
which
gives
the
index
into
the
second
level
page
table
Entry
is
at
the
bottom
of
the
second
level
page
table
and
entry
x
FF
is
at
the
top
Entry
x
FE
in
the
secondlevel
page
table
indicates
that
the
virtual
page
is
resident
in
physical
memory
V
and
that
the
physical
page
number
is
x
F
The
physical
page
number
is
concatenated
with
the
page
offset
to
form
the
physical
address
x
F
FB
Ch
xFFFF
to
xFFFFFFFF
are
used
for
I
O
Recall
from
Section
that
these
addresses
are
in
a
reserved
portion
of
the
memory
map
Each
I
O
device
is
assigned
one
or
more
memory
addresses
in
this
range
A
store
to
the
specified
address
sends
data
to
the
device
A
load
receives
data
from
the
device
This
method
of
communicating
with
I
O
devices
is
called
memory
mapped
I
O
In
a
system
with
memory
mapped
I
O
a
load
or
store
may
access
either
memory
or
an
I
O
device
Figure
shows
the
hardware
needed
to
support
two
memory
mapped
I
O
devices
An
address
decoder
determines
which
device
communicates
with
the
processor
It
uses
the
Address
and
MemWrite
signals
to
generate
control
signals
for
the
rest
of
the
hardware
The
ReadData
multiplexer
selects
between
memory
and
the
various
I
O
devices
Write
enabled
registers
hold
the
values
written
to
the
I
O
devices
Example
COMMUNICATING
WITH
I
O
DEVICES
Suppose
I
O
Device
in
Figure
is
assigned
the
memory
address
xFFFFFFF
Show
the
MIPS
assembly
code
for
writing
the
value
to
I
O
Device
and
for
reading
the
output
value
from
I
O
Device
Solution
The
following
MIPS
assembly
code
writes
the
value
to
I
O
Device
addi
t
sw
t
xFFF
The
address
decoder
asserts
WE
because
the
address
is
xFFFFFFF
and
MemWrite
is
TRUE
The
value
on
the
WriteData
bus
is
written
into
the
register
connected
to
the
input
pins
of
I
O
Device
Memory
Mapped
I
O
Processor
Memory
Address
MemWrite
WriteData
I
O
ReadData
Device
I
O
Device
CLK
EN
EN
Address
Decoder
WE
WEM
RDsel
WE
WE
CLK
CLK
Figure
Support
hardware
for
memory
mapped
I
O
Some
architectures
notably
IA
use
specialized
instructions
instead
of
memorymapped
I
O
to
communicate
with
I
O
devices
These
instructions
are
of
the
following
form
where
devicel
and
device
are
the
unique
ID
of
the
peripheral
device
lwio
t
device
swio
t
device
This
type
of
communication
with
I
O
devices
is
called
programmed
I
O
Recall
that
the
bit
immediate
xFFF
is
sign
extended
to
the
bit
value
xFFFFFFF
To
read
from
I
O
Device
the
processor
performs
the
following
MIPS
assembly
code
lw
t
xFFF
The
address
decoder
sets
RDsel
to
because
it
detects
the
address
xFFFFFFF
and
MemWrite
is
FALSE
The
output
of
I
O
Device
passes
through
the
multiplexer
onto
the
ReadData
bus
and
is
loaded
into
t
in
the
processor
Software
that
communicates
with
an
I
O
device
is
called
a
device
driver
You
have
probably
downloaded
or
installed
device
drivers
for
your
printer
or
other
I
O
device
Writing
a
device
driver
requires
detailed
knowledge
about
the
I
O
device
hardware
Other
programs
call
functions
in
the
device
driver
to
access
the
device
without
having
to
understand
the
low
level
device
hardware
To
illustrate
memory
mapped
I
O
hardware
and
software
the
rest
of
this
section
describes
interfacing
a
commercial
speech
synthesizer
chip
to
a
MIPS
processor
Speech
Synthesizer
Hardware
The
Radio
Shack
SP
speech
synthesizer
chip
generates
robot
like
speech
Words
are
composed
of
one
or
more
allophones
the
fundamental
units
of
sound
For
example
the
word
hello
uses
five
allophones
represented
by
the
following
symbols
in
the
SP
speech
chip
HH
EH
LL
AX
OW
The
speech
synthesizer
uses
bit
codes
to
represent
different
allophones
that
appear
in
the
English
language
For
example
the
five
allophones
for
the
word
hello
correspond
to
the
hexadecimal
values
x
B
x
x
D
x
F
x
respectively
The
processor
sends
a
series
of
allophones
to
the
speech
synthesizer
which
drives
a
speaker
to
blabber
the
sounds
Figure
shows
the
pinout
of
the
SP
speech
chip
The
I
O
pins
highlighted
in
blue
are
used
to
interface
with
the
MIPS
processor
to
produce
speech
Pins
A
receive
the
bit
allophone
encoding
from
the
processor
The
allophone
sound
is
produced
on
the
Digital
Out
pin
The
Digital
Out
signal
is
first
amplified
and
then
sent
to
a
speaker
The
other
two
highlighted
pins
SBY
and
ALD
are
status
and
control
pins
When
the
SBY
output
is
the
speech
chip
is
standing
by
and
is
ready
to
receive
a
new
allophone
On
the
falling
edge
of
the
address
load
input
ALD
the
speech
chip
reads
the
allophone
specified
by
A
Other
pins
such
as
power
and
ground
VDD
and
VSS
and
the
clock
OSC
must
be
connected
as
shown
but
are
not
driven
by
the
processor
Figure
shows
the
speech
synthesizer
interfaced
to
the
MIPS
processor
The
processor
uses
three
memory
mapped
I
O
addresses
to
communicate
with
the
speech
synthesizer
We
arbitrarily
have
chosen
CHAPTER
EIGHT
Memory
Systems
See
www
speechchips
com
for
more
information
about
the
SP
and
the
allophone
encodings
that
the
A
port
is
mapped
to
address
xFFFFFF
ALD
to
xFFFFFF
and
SBY
to
xFFFFFF
Although
the
WriteData
bus
is
bits
only
the
least
significant
bits
are
used
for
A
and
the
least
significant
bit
is
used
for
ALD
the
other
bits
are
ignored
Similarly
SBY
is
read
on
the
least
significant
bit
of
the
ReadData
bus
the
other
bits
are
Memory
Mapped
I
O
OSC
OSC
ROM
Clock
SBY
Reset
Digital
Out
VD
Test
Ser
In
ALD
SE
A
A
A
A
VSS
Reset
ROM
Disable
C
C
C
VDD
SBY
To
Processor
From
Processor
From
Processor
From
Processor
LRQ
A
A
Ser
Out
A
A
SPO
MHz
A
Amplifier
Speaker
Processor
Memory
Address
MemWrite
WriteData
ReadData
SP
CLK
EN
EN
Address
Decoder
WE
CLK
CLK
A
ALD
SBY
WE
WE
WEM
RDsel
Figure
SPO
speech
synthesizer
chip
pinout
Figure
Hardware
for
driving
the
SPO
speech
synthesizer
Speech
Synthesizer
Device
Driver
The
device
driver
controls
the
speech
synthesizer
by
sending
an
appropriate
series
of
allophones
over
the
memory
mapped
I
O
interface
It
follows
the
protocol
expected
by
the
SPO
chip
given
below
Set
ALD
to
Wait
until
the
chip
asserts
SBY
to
indicate
that
it
is
finished
speaking
the
previous
allophone
and
is
ready
for
the
next
Write
a
bit
allophone
to
A
Reset
ALD
to
to
initiate
speech
This
sequence
can
be
repeated
for
any
number
of
allophones
The
MIPS
assembly
in
Code
Example
writes
five
allophones
to
the
speech
chip
The
allophone
encodings
are
stored
as
bit
values
in
a
five
entry
array
starting
at
memory
address
x
The
assembly
code
in
Code
Example
polls
or
repeatedly
checks
the
SBY
signal
to
determine
when
the
speech
chip
is
ready
to
receive
a
new
allophone
The
code
functions
correctly
but
wastes
valuable
processor
cycles
that
could
be
used
to
perform
useful
work
Instead
of
polling
the
processor
could
use
an
interrupt
connected
to
SBY
When
SBY
rises
the
processor
stops
what
it
is
doing
and
jumps
to
code
that
handles
the
interrupt
In
the
case
of
the
speech
synthesizer
the
interrupt
handler
CHAPTER
EIGHT
Memory
Systems
init
addi
t
t
value
to
write
to
ALD
addi
t
t
array
size
lui
t
x
t
array
base
address
addi
t
t
array
index
start
sw
t
xFF
ALD
loop
lw
t
xFF
t
SBY
beq
t
loop
loop
until
SBY
add
t
t
t
t
address
of
allophone
lw
t
t
t
allophone
sw
t
xFF
A
allophone
sw
xFF
ALD
to
initiate
speech
addi
t
t
increment
array
index
beq
t
t
done
last
allophone
in
array
j
start
repeat
done
Code
Example
SPEECH
CHIP
DEVICE
DRIVER
Chapter
qxd
would
send
the
next
allophone
then
let
the
processor
resume
what
it
was
doing
before
the
interrupt
As
described
in
Section
the
processor
handles
interrupts
like
any
other
exception
REAL
WORLD
PERSPECTIVE
IA
MEMORY
AND
I
O
SYSTEMS
As
processors
get
faster
they
need
ever
more
elaborate
memory
hierarchies
to
keep
a
steady
supply
of
data
and
instructions
flowing
This
section
describes
the
memory
systems
of
IA
processors
to
illustrate
the
progression
Section
contained
photographs
of
the
processors
highlighting
the
on
chip
caches
IA
also
has
an
unusual
programmed
I
O
system
that
differs
from
the
more
common
memory
mapped
I
O
IA
Cache
Systems
The
initially
produced
in
operated
at
MHz
It
lacked
a
cache
so
it
directly
accessed
main
memory
for
all
instructions
and
data
Depending
on
the
speed
of
the
memory
the
processor
might
get
an
immediate
response
or
it
might
have
to
pause
for
one
or
more
cycles
for
the
memory
to
react
These
cycles
are
called
wait
states
and
they
increase
the
CPI
of
the
processor
Microprocessor
clock
frequencies
have
increased
by
at
least
per
year
since
then
whereas
memory
latency
has
scarcely
diminished
The
delay
from
when
the
processor
sends
an
address
to
main
memory
until
the
memory
returns
the
data
can
now
exceed
processor
clock
cycles
Therefore
caches
with
a
low
miss
rate
are
essential
to
good
performance
Table
summarizes
the
evolution
of
cache
systems
on
Intel
IA
processors
The
introduced
a
unified
write
through
cache
to
hold
both
instructions
and
data
Most
high
performance
computer
systems
also
provided
a
larger
second
level
cache
on
the
motherboard
using
commercially
available
SRAM
chips
that
were
substantially
faster
than
main
memory
The
Pentium
processor
introduced
separate
instruction
and
data
caches
to
avoid
contention
during
simultaneous
requests
for
data
and
instructions
The
caches
used
a
write
back
policy
reducing
the
communication
with
main
memory
Again
a
larger
second
level
cache
typically
KB
was
usually
offered
on
the
motherboard
The
P
series
of
processors
Pentium
Pro
Pentium
II
and
Pentium
III
were
designed
for
much
higher
clock
frequencies
The
second
level
cache
on
the
motherboard
could
not
keep
up
so
it
was
moved
closer
to
the
processor
to
improve
its
latency
and
throughput
The
Pentium
Pro
was
packaged
in
a
multichip
module
MCM
containing
both
the
processor
chip
and
a
second
level
cache
chip
as
shown
in
Figure
Like
the
Pentium
the
processor
had
separate
KB
level
instruction
and
data
Real
World
Perspective
IA
Memory
and
I
O
Systems
caches
However
these
caches
were
nonblocking
so
that
the
out
oforder
processor
could
continue
executing
subsequent
cache
accesses
even
if
the
cache
missed
a
particular
access
and
had
to
fetch
data
from
main
memory
The
second
level
cache
was
KB
KB
or
MB
in
size
and
could
operate
at
the
same
speed
as
the
processor
Unfortunately
the
MCM
packaging
proved
too
expensive
for
high
volume
manufacturing
Therefore
the
Pentium
II
was
sold
in
a
lower
cost
cartridge
containing
the
processor
and
the
second
level
cache
The
level
caches
were
doubled
in
size
to
compensate
for
the
fact
that
the
second
level
cache
operated
at
half
the
processor
s
speed
The
Pentium
III
integrated
a
full
speed
second
level
cache
directly
onto
the
same
chip
as
the
processor
A
cache
on
the
same
chip
can
operate
at
better
latency
and
throughput
so
it
is
substantially
more
effective
than
an
off
chip
cache
of
the
same
size
The
Pentium
offered
a
nonblocking
level
data
cache
It
switched
to
a
trace
cache
to
store
instructions
after
they
had
been
decoded
into
micro
ops
avoiding
the
delay
of
redecoding
each
time
instructions
were
fetched
from
the
cache
The
Pentium
M
design
was
adapted
from
the
Pentium
III
It
further
increased
the
level
caches
to
KB
each
and
featured
a
to
MB
level
cache
The
Core
Duo
contains
two
modified
Pentium
M
processors
and
CHAPTER
EIGHT
Memory
Systems
Table
Evolution
of
Intel
IA
microprocessor
memory
systems
Frequency
Level
Level
Processor
Year
MHz
Data
Cache
Instruction
Cache
Level
Cache
none
none
none
KB
unified
none
on
chip
Pentium
KB
KB
none
on
chip
Pentium
Pro
KB
KB
KB
MB
on
MCM
Pentium
II
KB
KB
KB
on
cartridge
Pentium
III
KB
KB
KB
on
chip
Pentium
KB
K
op
KB
MB
trace
cache
on
chip
Pentium
M
KB
KB
MB
on
chip
Core
Duo
KB
core
KB
core
MB
shared
on
chip
a
shared
MB
cache
on
one
chip
The
shared
cache
is
used
for
communication
between
the
processors
one
can
write
data
to
the
cache
and
the
other
can
read
it
IA
Virtual
Memory
IA
processors
operate
in
either
real
mode
or
protected
mode
Real
mode
is
backward
compatible
with
the
original
It
only
uses
bits
of
addresses
limiting
memory
to
MB
and
it
does
not
allow
virtual
memory
Protected
mode
was
introduced
with
the
and
extended
to
bit
addresses
with
the
It
supports
virtual
memory
with
KB
pages
It
also
provides
memory
protection
so
that
one
program
cannot
access
the
pages
belonging
to
other
programs
Hence
a
buggy
or
malicious
program
cannot
crash
or
corrupt
other
programs
All
modern
operating
systems
now
use
protected
mode
A
bit
address
permits
up
to
GB
of
memory
Processors
since
the
Pentium
Pro
have
bumped
the
memory
capacity
to
GB
using
a
Real
World
Perspective
IA
Memory
and
I
O
Systems
Figure
Pentium
Pro
multichip
module
with
processor
left
and
KB
cache
right
in
a
pin
grid
array
PGA
package
Courtesy
Intel
Although
memory
protection
became
available
in
the
hardware
in
the
early
s
Microsoft
Windows
took
almost
years
to
take
advantage
of
the
feature
and
prevent
bad
programs
from
crashing
the
entire
computer
Until
the
release
of
Windows
consumer
versions
of
Windows
were
notoriously
unstable
The
lag
between
hardware
features
and
software
support
can
be
extremely
long
technique
called
physical
address
extension
Each
process
uses
bit
addresses
The
virtual
memory
system
maps
these
addresses
onto
a
larger
bit
virtual
memory
space
It
uses
different
page
tables
for
each
process
so
that
each
process
can
have
its
own
address
space
of
up
to
GB
IA
Programmed
I
O
Most
architectures
use
memory
mapped
I
O
described
in
Section
in
which
programs
access
I
O
devices
by
reading
and
writing
memory
locations
IA
uses
programmed
I
O
in
which
special
IN
and
OUT
instructions
are
used
to
read
and
write
I
O
devices
IA
defines
I
O
ports
The
IN
instruction
reads
one
two
or
four
bytes
from
the
port
specified
by
DX
into
AL
AX
or
EAX
OUT
is
similar
but
writes
the
port
Connecting
a
peripheral
device
to
a
programmed
I
O
system
is
similar
to
connecting
it
to
a
memory
mapped
system
When
accessing
an
I
O
port
the
processor
sends
the
port
number
rather
than
the
memory
address
on
the
least
significant
bits
of
the
address
bus
The
device
reads
or
writes
data
from
the
data
bus
The
major
difference
is
that
the
processor
also
produces
an
M
IO
signal
When
M
IO
the
processor
is
accessing
memory
When
it
is
the
process
is
accessing
one
of
the
I
O
devices
The
address
decoder
must
also
look
at
M
IO
to
generate
the
appropriate
enables
for
main
memory
and
for
the
I
O
devices
I
O
devices
can
also
send
interrupts
to
the
processor
to
indicate
that
they
are
ready
to
communicate
SUMMARY
Memory
system
organization
is
a
major
factor
in
determining
computer
performance
Different
memory
technologies
such
as
DRAM
SRAM
and
hard
disks
offer
trade
offs
in
capacity
speed
and
cost
This
chapter
introduced
cache
and
virtual
memory
organizations
that
use
a
hierarchy
of
memories
to
approximate
an
ideal
large
fast
inexpensive
memory
Main
memory
is
typically
built
from
DRAM
which
is
significantly
slower
than
the
processor
A
cache
reduces
access
time
by
keeping
commonly
used
data
in
fast
SRAM
Virtual
memory
increases
the
memory
capacity
by
using
a
hard
disk
to
store
data
that
does
not
fit
in
the
main
memory
Caches
and
virtual
memory
add
complexity
and
hardware
to
a
computer
system
but
the
benefits
usually
outweigh
the
costs
All
modern
personal
computers
use
caches
and
virtual
memory
Most
processors
also
use
the
memory
interface
to
communicate
with
I
O
devices
This
is
called
memory
mapped
I
O
Programs
use
load
and
store
operations
to
access
the
I
O
devices
CHAPTER
EIGHT
Memory
Systems
C
EPILOGUE
This
chapter
brings
us
to
the
end
of
our
journey
together
into
the
realm
of
digital
systems
We
hope
this
book
has
conveyed
the
beauty
and
thrill
of
the
art
as
well
as
the
engineering
knowledge
You
have
learned
to
design
combinational
and
sequential
logic
using
schematics
and
hardware
description
languages
You
are
familiar
with
larger
building
blocks
such
as
multiplexers
ALUs
and
memories
Computers
are
one
of
the
most
fascinating
applications
of
digital
systems
You
have
learned
how
to
program
a
MIPS
processor
in
its
native
assembly
language
and
how
to
build
the
processor
and
memory
system
using
digital
building
blocks
Throughout
you
have
seen
the
application
of
abstraction
discipline
hierarchy
modularity
and
regularity
With
these
techniques
we
have
pieced
together
the
puzzle
of
a
microprocessor
s
inner
workings
From
cell
phones
to
digital
television
to
Mars
rovers
to
medical
imaging
systems
our
world
is
an
increasingly
digital
place
Imagine
what
Faustian
bargain
Charles
Babbage
would
have
made
to
take
a
similar
journey
a
century
and
a
half
ago
He
merely
aspired
to
calculate
mathematical
tables
with
mechanical
precision
Today
s
digital
systems
are
yesterday
s
science
fiction
Might
Dick
Tracy
have
listened
to
iTunes
on
his
cell
phone
Would
Jules
Verne
have
launched
a
constellation
of
global
positioning
satellites
into
space
Could
Hippocrates
have
cured
illness
using
high
resolution
digital
images
of
the
brain
But
at
the
same
time
George
Orwell
s
nightmare
of
ubiquitous
government
surveillance
becomes
closer
to
reality
each
day
And
rogue
states
develop
nuclear
weapons
using
laptop
computers
more
powerful
than
the
room
sized
supercomputers
that
simulated
Cold
War
bombs
The
microprocessor
revolution
continues
to
accelerate
The
changes
in
the
coming
decades
will
surpass
those
of
the
past
You
now
have
the
tools
to
design
and
build
these
new
systems
that
will
shape
our
future
With
your
newfound
power
comes
profound
responsibility
We
hope
that
you
will
use
it
not
just
for
fun
and
riches
but
also
for
the
benefit
of
humanity
Epilogue
Exercises
Exercise
In
less
than
one
page
describe
four
everyday
activities
that
exhibit
temporal
or
spatial
locality
List
two
activities
for
each
type
of
locality
and
be
specific
Exercise
In
one
paragraph
describe
two
short
computer
applications
that
exhibit
temporal
and
or
spatial
locality
Describe
how
Be
specific
Exercise
Come
up
with
a
sequence
of
addresses
for
which
a
direct
mapped
cache
with
a
size
capacity
of
words
and
block
size
of
words
outperforms
a
fully
associative
cache
with
least
recently
used
LRU
replacement
that
has
the
same
capacity
and
block
size
Exercise
Repeat
Exercise
for
the
case
when
the
fully
associative
cache
outperforms
the
direct
mapped
cache
Exercise
Describe
the
trade
offs
of
increasing
each
of
the
following
cache
parameters
while
keeping
the
others
the
same
a
block
size
b
associativity
c
cache
size
Exercise
Is
the
miss
rate
of
a
two
way
set
associative
cache
always
usually
occasionally
or
never
better
than
that
of
a
direct
mapped
cache
of
the
same
capacity
and
block
size
Explain
Exercise
Each
of
the
following
statements
pertains
to
the
miss
rate
of
caches
Mark
each
statement
as
true
or
false
Briefly
explain
your
reasoning
present
a
counterexample
if
the
statement
is
false
a
A
two
way
set
associative
cache
always
has
a
lower
miss
rate
than
a
direct
mapped
cache
with
the
same
block
size
and
total
capacity
b
A
KB
direct
mapped
cache
always
has
a
lower
miss
rate
than
an
KB
direct
mapped
cache
with
the
same
block
size
c
An
instruction
cache
with
a
byte
block
size
usually
has
a
lower
miss
rate
than
an
instruction
cache
with
an
byte
block
size
given
the
same
degree
of
associativity
and
total
capacity
CHAPTER
EIGHT
Memory
Systems
Exercise
A
cache
has
the
following
parameters
b
block
size
given
in
numbers
of
words
S
number
of
sets
N
number
of
ways
and
A
number
of
address
bits
a
In
terms
of
the
parameters
described
what
is
the
cache
capacity
C
b
In
terms
of
the
parameters
described
what
is
the
total
number
of
bits
required
to
store
the
tags
c
What
are
S
and
N
for
a
fully
associative
cache
of
capacity
C
words
with
block
size
b
d
What
is
S
for
a
direct
mapped
cache
of
size
C
words
and
block
size
b
Exercise
A
word
cache
has
the
parameters
given
in
Exercise
Consider
the
following
repeating
sequence
of
lw
addresses
given
in
hexadecimal
C
C
C
C
C
C
Assuming
least
recently
used
LRU
replacement
for
associative
caches
determine
the
effective
miss
rate
if
the
sequence
is
input
to
the
following
caches
ignoring
startup
effects
i
e
compulsory
misses
a
direct
mapped
cache
S
b
word
b
fully
associative
cache
N
b
word
c
two
way
set
associative
cache
S
b
word
d
direct
mapped
cache
S
b
words
Exercise
Suppose
you
are
running
a
program
with
the
following
data
access
pattern
The
pattern
is
executed
only
once
x
x
x
x
x
x
a
If
you
use
a
direct
mapped
cache
with
a
cache
size
of
KB
and
a
block
size
of
bytes
words
how
many
sets
are
in
the
cache
b
With
the
same
cache
and
block
size
as
in
part
a
what
is
the
miss
rate
of
the
direct
mapped
cache
for
the
given
memory
access
pattern
c
For
the
given
memory
access
pattern
which
of
the
following
would
decrease
the
miss
rate
the
most
Cache
capacity
is
kept
constant
Circle
one
i
Increasing
the
degree
of
associativity
to
ii
Increasing
the
block
size
to
bytes
Exercises
Chapter
iii
Either
i
or
ii
iv
Neither
i
nor
ii
Exercise
You
are
building
an
instruction
cache
for
a
MIPS
processor
It
has
a
total
capacity
of
C
c
bytes
It
is
N
n
way
set
associative
N
with
a
block
size
of
b
b
bytes
b
Give
your
answers
to
the
following
questions
in
terms
of
these
parameters
a
Which
bits
of
the
address
are
used
to
select
a
word
within
a
block
b
Which
bits
of
the
address
are
used
to
select
the
set
within
the
cache
c
How
many
bits
are
in
each
tag
d
How
many
tag
bits
are
in
the
entire
cache
Exercise
Consider
a
cache
with
the
following
parameters
N
associativity
b
block
size
words
W
word
size
bits
C
cache
size
K
words
A
address
size
bits
You
need
consider
only
word
addresses
a
Show
the
tag
set
block
offset
and
byte
offset
bits
of
the
address
State
how
many
bits
are
needed
for
each
field
b
What
is
the
size
of
all
the
cache
tags
in
bits
c
Suppose
each
cache
block
also
has
a
valid
bit
V
and
a
dirty
bit
D
What
is
the
size
of
each
cache
set
including
data
tag
and
status
bits
d
Design
the
cache
using
the
building
blocks
in
Figure
and
a
small
number
of
two
input
logic
gates
The
cache
design
must
include
tag
storage
data
storage
address
comparison
data
output
selection
and
any
other
parts
you
feel
are
relevant
Note
that
the
multiplexer
and
comparator
blocks
may
be
any
size
n
or
p
bits
wide
respectively
but
the
SRAM
blocks
must
be
K
bits
Be
sure
to
include
a
neatly
labeled
block
diagram
CHAPTER
EIGHT
Memory
Systems
K
SRAM
p
p
n
n
n
Figure
Building
blocks
Chapter
Exercise
You
ve
joined
a
hot
new
Internet
startup
to
build
wrist
watches
with
a
built
in
pager
and
Web
browser
It
uses
an
embedded
processor
with
a
multilevel
cache
scheme
depicted
in
Figure
The
processor
includes
a
small
on
chip
cache
in
addition
to
a
large
off
chip
second
level
cache
Yes
the
watch
weighs
pounds
but
you
should
see
it
surf
Assume
that
the
processor
uses
bit
physical
addresses
but
accesses
data
only
on
word
boundaries
The
caches
have
the
characteristics
given
in
Table
The
DRAM
has
an
access
time
of
tm
and
a
size
of
MB
a
For
a
given
word
in
memory
what
is
the
total
number
of
locations
in
which
it
might
be
found
in
the
on
chip
cache
and
in
the
second
level
cache
b
What
is
the
size
in
bits
of
each
tag
for
the
on
chip
cache
and
the
secondlevel
cache
c
Give
an
expression
for
the
average
memory
read
access
time
The
caches
are
accessed
in
sequence
d
Measurements
show
that
for
a
particular
problem
of
interest
the
on
chip
cache
hit
rate
is
and
the
second
level
cache
hit
rate
is
However
when
the
on
chip
cache
is
disabled
the
second
level
cache
hit
rate
shoots
up
to
Give
a
brief
explanation
of
this
behavior
Exercises
Figure
Computer
system
CPU
Level
Cache
Level
Cache
Main
Memory
Processor
Chip
Table
Memory
characteristics
Characteristic
On
chip
Cache
Off
chip
Cache
organization
four
way
set
associative
direct
mapped
hit
rate
A
B
access
time
ta
tb
block
size
bytes
bytes
number
of
blocks
K
Exercise
This
chapter
described
the
least
recently
used
LRU
replacement
policy
for
multiway
associative
caches
Other
less
common
replacement
policies
include
first
in
first
out
FIFO
and
random
policies
FIFO
replacement
evicts
the
block
that
has
been
there
the
longest
regardless
of
how
recently
it
was
accessed
Random
replacement
randomly
picks
a
block
to
evict
a
Discuss
the
advantages
and
disadvantages
of
each
of
these
replacement
policies
b
Describe
a
data
access
pattern
for
which
FIFO
would
perform
better
than
LRU
Exercise
You
are
building
a
computer
with
a
hierarchical
memory
system
that
consists
of
separate
instruction
and
data
caches
followed
by
main
memory
You
are
using
the
MIPS
multicycle
processor
from
Figure
running
at
GHz
a
Suppose
the
instruction
cache
is
perfect
i
e
always
hits
but
the
data
cache
has
a
miss
rate
On
a
cache
miss
the
processor
stalls
for
ns
to
access
main
memory
then
resumes
normal
operation
Taking
cache
misses
into
account
what
is
the
average
memory
access
time
b
How
many
clock
cycles
per
instruction
CPI
on
average
are
required
for
load
and
store
word
instructions
considering
the
non
ideal
memory
system
c
Consider
the
benchmark
application
of
Example
that
has
loads
stores
branches
jumps
and
R
type
instructions
Taking
the
non
ideal
memory
system
into
account
what
is
the
average
CPI
for
this
benchmark
d
Now
suppose
that
the
instruction
cache
is
also
non
ideal
and
has
a
miss
rate
What
is
the
average
CPI
for
the
benchmark
in
part
c
Take
into
account
both
instruction
and
data
cache
misses
Exercise
If
a
computer
uses
bit
virtual
addresses
how
much
virtual
memory
can
it
access
Note
that
bytes
terabyte
bytes
petabyte
and
bytes
exabyte
Exercise
A
supercomputer
designer
chooses
to
spend
million
on
DRAM
and
the
same
amount
on
hard
disks
for
virtual
memory
Using
the
prices
from
Figure
how
much
physical
and
virtual
memory
will
the
computer
have
How
many
bits
of
physical
and
virtual
addresses
are
necessary
to
access
this
memory
CHAPTER
EIGHT
Memory
Systems
Data
from
Patterson
and
Hennessy
Computer
Organization
and
Design
rd
Edition
Morgan
Kaufmann
Used
with
permission
Cha
Exercise
Consider
a
virtual
memory
system
that
can
address
a
total
of
bytes
You
have
unlimited
hard
disk
space
but
are
limited
to
only
MB
of
semiconductor
physical
memory
Assume
that
virtual
and
physical
pages
are
each
KB
in
size
a
How
many
bits
is
the
physical
address
b
What
is
the
maximum
number
of
virtual
pages
in
the
system
c
How
many
physical
pages
are
in
the
system
d
How
many
bits
are
the
virtual
and
physical
page
numbers
e
Suppose
that
you
come
up
with
a
direct
mapped
scheme
that
maps
virtual
pages
to
physical
pages
The
mapping
uses
the
least
significant
bits
of
the
virtual
page
number
to
determine
the
physical
page
number
How
many
virtual
pages
are
mapped
to
each
physical
page
Why
is
this
direct
mapping
a
bad
plan
f
Clearly
a
more
flexible
and
dynamic
scheme
for
translating
virtual
addresses
into
physical
addresses
is
required
than
the
one
described
in
part
d
Suppose
you
use
a
page
table
to
store
mappings
translations
from
virtual
page
number
to
physical
page
number
How
many
page
table
entries
will
the
page
table
contain
g
Assume
that
in
addition
to
the
physical
page
number
each
page
table
entry
also
contains
some
status
information
in
the
form
of
a
valid
bit
V
and
a
dirty
bit
D
How
many
bytes
long
is
each
page
table
entry
Round
up
to
an
integer
number
of
bytes
h
Sketch
the
layout
of
the
page
table
What
is
the
total
size
of
the
page
table
in
bytes
Exercise
You
decide
to
speed
up
the
virtual
memory
system
of
Exercise
by
using
a
translation
lookaside
buffer
TLB
Suppose
your
memory
system
has
the
characteristics
shown
in
Table
The
TLB
and
cache
miss
rates
indicate
how
often
the
requested
entry
is
not
found
The
main
memory
miss
rate
indicates
how
often
page
faults
occur
Exercises
Table
Memory
characteristics
Memory
Unit
Access
Time
Cycles
Miss
Rate
TLB
cache
main
memory
disk
a
What
is
the
average
memory
access
time
of
the
virtual
memory
system
before
and
after
adding
the
TLB
Assume
that
the
page
table
is
always
resident
in
physical
memory
and
is
never
held
in
the
data
cache
b
If
the
TLB
has
entries
how
big
in
bits
is
the
TLB
Give
numbers
for
data
physical
page
number
tag
virtual
page
number
and
valid
bits
of
each
entry
Show
your
work
clearly
c
Sketch
the
TLB
Clearly
label
all
fields
and
dimensions
d
What
size
SRAM
would
you
need
to
build
the
TLB
described
in
part
c
Give
your
answer
in
terms
of
depth
width
Exercise
Suppose
the
MIPS
multicycle
processor
described
in
Section
uses
a
virtual
memory
system
a
Sketch
the
location
of
the
TLB
in
the
multicycle
processor
schematic
b
Describe
how
adding
a
TLB
affects
processor
performance
Exercise
The
virtual
memory
system
you
are
designing
uses
a
single
level
page
table
built
from
dedicated
hardware
SRAM
and
associated
logic
It
supports
bit
virtual
addresses
bit
physical
addresses
and
byte
KB
pages
Each
page
table
entry
contains
a
physical
page
number
a
valid
bit
V
and
a
dirty
bit
D
a
What
is
the
total
size
of
the
page
table
in
bits
b
The
operating
system
team
proposes
reducing
the
page
size
from
to
KB
but
the
hardware
engineers
on
your
team
object
on
the
grounds
of
added
hardware
cost
Explain
their
objection
c
The
page
table
is
to
be
integrated
on
the
processor
chip
along
with
the
on
chip
cache
The
on
chip
cache
deals
only
with
physical
not
virtual
addresses
Is
it
possible
to
access
the
appropriate
set
of
the
on
chip
cache
concurrently
with
the
page
table
access
for
a
given
memory
access
Explain
briefly
the
relationship
that
is
necessary
for
concurrent
access
to
the
cache
set
and
page
table
entry
d
Is
it
possible
to
perform
the
tag
comparison
in
the
on
chip
cache
concurrently
with
the
page
table
access
for
a
given
memory
access
Explain
briefly
Exercise
Describe
a
scenario
in
which
the
virtual
memory
system
might
affect
how
an
application
is
written
Be
sure
to
include
a
discussion
of
how
the
page
size
and
physical
memory
size
affect
the
performance
of
the
application
CHAPTER
EIGHT
Memory
Systems
Exercise
Suppose
you
own
a
personal
computer
PC
that
uses
bit
virtual
addresses
a
What
is
the
maximum
amount
of
virtual
memory
space
each
program
can
use
b
How
does
the
size
of
your
PC
s
hard
disk
affect
performance
c
How
does
the
size
of
your
PC
s
physical
memory
affect
performance
Exercise
Use
MIPS
memory
mapped
I
O
to
interact
with
a
user
Each
time
the
user
presses
a
button
a
pattern
of
your
choice
displays
on
five
light
emitting
diodes
LEDs
Suppose
the
input
button
is
mapped
to
address
xFFFFFF
and
the
LEDs
are
mapped
to
address
xFFFFFF
When
the
button
is
pushed
its
output
is
otherwise
it
is
a
Write
MIPS
code
to
implement
this
functionality
b
Draw
a
schematic
similar
to
Figure
for
this
memory
mapped
I
O
system
c
Write
HDL
code
to
implement
the
address
decoder
for
your
memorymapped
I
O
system
Exercise
Finite
state
machines
FSMs
like
the
ones
you
built
in
Chapter
can
also
be
implemented
in
software
a
Implement
the
traffic
light
FSM
from
Figure
using
MIPS
assembly
code
The
inputs
TA
and
TB
are
memory
mapped
to
bit
and
bit
respectively
of
address
xFFFFF
The
two
bit
outputs
LA
and
LB
are
mapped
to
bits
and
bits
respectively
of
address
xFFFFF
Assume
one
hot
output
encodings
for
each
light
LA
and
LB
red
is
yellow
is
and
green
is
b
Draw
a
schematic
similar
to
Figure
for
this
memory
mapped
I
O
system
c
Write
HDL
code
to
implement
the
address
decoder
for
your
memorymapped
I
O
system
Exercises
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
on
interviews
Question
Explain
the
difference
between
direct
mapped
set
associative
and
fully
associative
caches
For
each
cache
type
describe
an
application
for
which
that
cache
type
will
perform
better
than
the
other
two
Question
Explain
how
virtual
memory
systems
work
Question
Explain
the
advantages
and
disadvantages
of
using
a
virtual
memory
system
Question
Explain
how
cache
performance
might
be
affected
by
the
virtual
page
size
of
a
memory
system
Question
Can
addresses
used
for
memory
mapped
I
O
be
cached
Explain
why
or
why
not
CHAPTER
EIGHT
Memory
Systems
A
A
Introduction
A
xx
Logic
A
Programmable
Logic
A
Application
Specific
Integrated
Circuits
A
Data
Sheets
A
Logic
Families
A
Packaging
and
Assembly
A
Transmission
Lines
A
Economics
Digital
System
Implementation
A
INTRODUCTION
This
appendix
introduces
practical
issues
in
the
design
of
digital
systems
The
material
in
this
appendix
is
not
necessary
for
understanding
the
rest
of
the
book
However
it
seeks
to
demystify
the
process
of
building
real
digital
systems
Moreover
we
believe
that
the
best
way
to
understand
digital
systems
is
to
build
and
debug
them
yourself
in
the
laboratory
Digital
systems
are
usually
built
using
one
or
more
chips
One
strategy
is
to
connect
together
chips
containing
individual
logic
gates
or
larger
elements
such
as
arithmetic
logical
units
ALUs
or
memories
Another
is
to
use
programmable
logic
which
contains
generic
arrays
of
circuitry
that
can
be
programmed
to
perform
specific
logic
functions
Yet
a
third
is
to
design
a
custom
integrated
circuit
containing
the
specific
logic
necessary
for
the
system
These
three
strategies
offer
trade
offs
in
cost
speed
power
consumption
and
design
time
that
are
explored
in
the
following
sections
This
appendix
also
examines
the
physical
packaging
and
assembly
of
circuits
the
transmission
lines
that
connect
the
chips
and
the
economics
of
digital
systems
A
XX
LOGIC
In
the
s
and
s
many
digital
systems
were
built
from
simple
chips
each
containing
a
handful
of
logic
gates
For
example
the
chip
contains
six
NOT
gates
the
contains
four
AND
gates
and
the
contains
two
flip
flops
These
chips
are
collectively
referred
to
as
xx
series
logic
They
were
sold
by
many
manufacturers
typically
for
to
cents
per
chip
These
chips
are
now
largely
obsolete
but
they
are
still
handy
for
simple
digital
systems
or
class
projects
because
they
are
so
inexpensive
and
easy
to
use
xx
series
chips
are
commonly
sold
in
pin
dual
inline
packages
DIPs
LS
inverter
chip
in
a
pin
dual
inline
package
The
part
number
is
on
the
first
line
LS
indicates
the
logic
family
see
Section
A
The
N
suffix
indicates
a
DIP
package
The
large
S
is
the
logo
of
the
manufacturer
Signetics
The
bottom
two
lines
of
gibberish
are
codes
indicating
the
batch
in
which
the
chip
was
manufactured
A
Logic
Gates
Figure
A
shows
the
pinout
diagrams
for
a
variety
of
popular
xx
series
chips
containing
basic
logic
gates
These
are
sometimes
called
small
scale
integration
SSI
chips
because
they
are
built
from
a
few
transistors
The
pin
packages
typically
have
a
notch
at
the
top
or
a
dot
on
the
top
left
to
indicate
orientation
Pins
are
numbered
starting
with
in
the
upper
left
and
going
counterclockwise
around
the
package
The
chips
need
to
receive
power
VDD
V
and
ground
GND
V
at
pins
and
respectively
The
number
of
logic
gates
on
the
chip
is
determined
by
the
number
of
pins
Note
that
pins
and
of
the
chip
are
not
connected
NC
to
anything
The
flip
flop
has
the
usual
D
CLK
and
Q
terminals
It
also
has
a
complementary
output
Moreover
it
receives
asynchronous
set
also
called
preset
or
PRE
and
reset
also
called
clear
or
CLR
signals
These
are
active
low
in
other
words
the
flop
sets
when
resets
when
and
operates
normally
when
A
Other
Functions
The
xx
series
also
includes
somewhat
more
complex
logic
functions
including
those
shown
in
Figures
A
and
A
These
are
called
mediumscale
integration
MSI
chips
Most
use
larger
packages
to
accommodate
more
inputs
and
outputs
Power
and
ground
are
still
provided
at
the
upper
right
and
lower
left
respectively
of
each
chip
A
general
functional
description
is
provided
for
each
chip
See
the
manufacturer
s
data
sheets
for
complete
descriptions
A
PROGRAMMABLE
LOGIC
Programmable
logic
consists
of
arrays
of
circuitry
that
can
be
configured
to
perform
specific
logic
functions
We
have
already
introduced
three
forms
of
programmable
logic
programmable
read
only
memories
PROMs
programmable
logic
arrays
PLAs
and
field
programmable
gate
arrays
FPGAs
This
section
shows
chip
implementations
for
each
of
these
Configuration
of
these
chips
may
be
performed
by
blowing
on
chip
fuses
to
connect
or
disconnect
circuit
elements
This
is
called
one
time
programmable
OTP
logic
because
once
a
fuse
is
blown
it
cannot
be
restored
Alternatively
the
configuration
may
be
stored
in
a
memory
that
can
be
reprogrammed
at
will
Reprogrammable
logic
is
convenient
in
the
laboratory
because
the
same
chip
can
be
reused
during
development
A
PROMs
As
discussed
in
Section
PROMs
can
be
used
as
lookup
tables
A
N
word
M
bit
PROM
can
be
programmed
to
perform
any
combinational
function
of
N
inputs
and
M
outputs
Design
changes
simply
CLR
PRE
CLR
PRE
Q
APPENDIX
A
Digital
System
Implementation
Appendi
A
Programmable
Logic
Q
D
Q
GND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
NAND
GND
Y
VDD
A
B
Y
A
B
Y
B
A
Y
B
A
NOR
GND
A
VDD
Y
A
Y
A
Y
A
Y
A
Y
A
Y
NOT
GND
A
VDD
B
A
B
C
Y
C
Y
C
B
A
Y
AND
GND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
AND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
XOR
GND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
OR
GND
GND
A
VDD
B
C
D
Y
D
C
NC
B
A
Y
AND
NC
GND
VDD
D
Q
FLOP
CLK
CLR
PRE
Q
CLR
D
PRE
Q
Q
CLK
reset
set
D
Q
Q
reset
set
Figure
A
Common
xx
series
logic
gates
APPENDIX
A
Digital
System
Implementation
CLR
CLK
D
D
D
D
ENP
GND
Counter
VDD
Q
RCO
LOAD
ENT
Q
Q
Q
always
posedge
CLK
if
CLRb
Q
b
else
if
LOADb
Q
D
else
if
ENP
ENT
Q
Q
assign
RCO
Q
b
ENT
bit
Counter
CLK
clock
Q
counter
output
D
parallel
input
CLRb
async
reset
sync
reset
LOADb
load
Q
from
D
ENP
ENT
enables
RCO
ripple
carry
out
G
S
D
D
D
D
Y
GND
Mux
VDD
G
S
D
D
D
D
Y
always
Gb
S
D
if
Gb
Y
else
Y
D
S
always
Gb
S
D
if
Gb
Y
else
Y
D
S
Two
Multiplexers
D
data
S
select
Y
output
Gb
enable
A
A
A
G
A
G
B
G
Y
GND
Decoder
VDD
Y
Y
Y
Y
Y
Y
Y
Decoder
A
address
output
G
active
high
enable
G
active
low
enables
G
G
A
G
B
A
Y
x
x
xxx
x
xxx
xxx
Y
S
D
D
D
D
Y
GND
Mux
VDD
G
D
D
Y
D
D
Y
always
if
Gb
Y
else
Y
S
D
D
if
Gb
Y
else
Y
S
D
D
if
Gb
Y
else
Y
S
D
D
if
Gb
Y
else
Y
S
D
D
Four
Multiplexers
D
data
S
select
Y
output
Gb
enable
EN
A
Y
A
Y
GND
Tristate
Buffer
VDD
EN
Y
A
Y
A
Y
A
A
Y
A
Y
Y
A
assign
Y
ENb
bzzzz
A
assign
Y
ENB
bzzzz
A
bit
Tristate
Buffer
A
input
Y
output
ENb
enable
always
posedge
clk
if
ENb
Q
D
EN
Q
D
D
Q
GND
Register
VDD
Q
D
D
Q
Q
D
D
Q
D
D
Q
Q
CLK
bit
Enableable
Register
CLK
clock
D
data
Q
output
ENb
enable
Yb
Figure
A
Medium
scale
integration
chips
Note
Verilog
variable
names
cannot
start
with
numbers
but
the
names
in
the
example
code
in
Figure
A
are
chosen
to
match
the
manufacturer
s
data
sheet
A
Programmable
Logic
bit
ALU
A
B
Y
output
F
function
select
M
mode
select
Cbn
carry
in
Cbnplus
carry
out
AeqB
equality
in
some
modes
X
Y
carry
lookahead
adder
outputs
B
A
S
S
S
S
Cn
M
F
F
F
GND
ALU
VDD
A
B
A
B
A
B
Y
Cn
X
A
B
F
D
D
LT
RBO
RBI
D
D
GND
Segment
Decoder
VDD
f
g
a
b
c
d
e
segment
Display
Decoder
D
data
a
f
segments
low
ON
LTb
light
test
RBIb
ripple
blanking
in
RBOb
ripple
blanking
out
RBO
LT
RBI
D
a
b
c
d
e
f
g
x
x
x
x
x
x
a
b
c
d
e
f
g
B
AltBin
AeqBin
AgtBin
AgtBout
AeqBout
AltBout
GND
Comparator
VDD
A
B
A
A
B
A
B
always
if
A
B
A
B
AgtBin
begin
AgtBout
AeqBout
AltBout
end
else
if
A
B
A
B
AltBin
begin
AgtBout
AeqBout
AltBout
end
else
begin
AgtBout
AeqBout
AltBout
end
bit
Comparator
A
B
data
relin
input
relation
relout
output
relation
always
case
F
Y
M
A
A
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
b
b
Cbn
Y
M
A
B
A
A
B
Cbn
Y
M
B
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
B
A
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
B
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
A
Cbn
Y
M
A
B
A
B
A
Cbn
Y
M
A
B
A
B
A
Cbn
Y
M
A
A
Cbn
endcase
inputs
Figure
A
More
medium
scale
integration
MSI
chips
involve
replacing
the
contents
of
the
PROM
rather
than
rewiring
connections
between
chips
Lookup
tables
are
useful
for
small
functions
but
become
prohibitively
expensive
as
the
number
of
inputs
grows
For
example
the
classic
KB
Kb
erasable
PROM
EPROM
is
shown
in
Figure
A
The
EPROM
has
address
lines
to
specify
one
of
the
K
words
and
data
lines
to
read
the
byte
of
data
at
that
word
The
chip
enable
and
output
enable
must
both
be
asserted
for
data
to
be
read
The
maximum
propagation
delay
is
ps
In
normal
operation
and
VPP
is
not
used
The
EPROM
is
usually
programmed
on
a
special
programmer
that
sets
applies
V
to
VPP
and
uses
a
special
sequence
of
inputs
to
configure
the
memory
Modern
PROMs
are
similar
in
concept
but
have
much
larger
capacities
and
more
pins
Flash
memory
is
the
cheapest
type
of
PROM
selling
for
about
per
gigabyte
in
Prices
have
historically
declined
by
to
per
year
A
PLAs
As
discussed
in
Section
PLAs
contain
AND
and
OR
planes
to
compute
any
combinational
function
written
in
sum
of
products
form
The
AND
and
OR
planes
can
be
programmed
using
the
same
techniques
for
PROMs
A
PLA
has
two
columns
for
each
input
and
one
column
for
each
output
It
has
one
row
for
each
minterm
This
organization
is
more
efficient
than
a
PROM
for
many
functions
but
the
array
still
grows
excessively
large
for
functions
with
numerous
I
Os
and
minterms
Many
different
manufacturers
have
extended
the
basic
PLA
concept
to
build
programmable
logic
devices
PLDs
that
include
registers
PGM
PGM
APPENDIX
A
Digital
System
Implementation
A
VPP
A
A
A
A
A
A
VDD
PGM
NC
A
A
A
OE
A
A
A
D
D
D
GND
CE
D
D
D
D
D
assign
D
CEb
OEb
ROM
A
bZ
KB
EPROM
A
address
input
D
data
output
CEb
chip
enable
OEb
output
enable
PGMb
program
VPP
program
voltage
NC
no
connection
Figure
A
KB
EPROM
Ap
The
V
is
one
of
the
most
popular
classic
PLDs
It
has
dedicated
input
pins
and
outputs
The
outputs
can
come
directly
from
the
PLA
or
from
clocked
registers
on
the
chip
The
outputs
can
also
be
fed
back
into
the
PLA
Thus
the
V
can
directly
implement
FSMs
with
up
to
inputs
outputs
and
bits
of
state
The
V
costs
about
in
quantities
of
PLDs
have
been
rendered
mostly
obsolete
by
the
rapid
improvements
in
capacity
and
cost
of
FPGAs
A
FPGAs
As
discussed
in
Section
FPGAs
consist
of
arrays
of
configurable
logic
blocks
CLBs
connected
together
with
programmable
wires
The
CLBs
contain
small
lookup
tables
and
flip
flops
FPGAs
scale
gracefully
to
extremely
large
capacities
with
thousands
of
lookup
tables
Xilinx
and
Altera
are
two
of
the
leading
FPGA
manufacturers
Lookup
tables
and
programmable
wires
are
flexible
enough
to
implement
any
logic
function
However
they
are
an
order
of
magnitude
less
efficient
in
speed
and
cost
chip
area
than
hard
wired
versions
of
the
same
functions
Thus
FPGAs
often
include
specialized
blocks
such
as
memories
multipliers
and
even
entire
microprocessors
Figure
A
shows
the
design
process
for
a
digital
system
on
an
FPGA
The
design
is
usually
specified
with
a
hardware
description
language
HDL
although
some
FPGA
tools
also
support
schematics
The
design
is
then
simulated
Inputs
are
applied
and
compared
against
expected
outputs
to
verify
that
the
logic
is
correct
Usually
some
debugging
is
required
Next
logic
synthesis
converts
the
HDL
into
Boolean
functions
Good
synthesis
tools
produce
a
schematic
of
the
functions
and
the
prudent
designer
examines
these
schematics
as
well
as
any
warnings
produced
during
synthesis
to
ensure
that
the
desired
logic
was
produced
Sometimes
sloppy
coding
leads
to
circuits
that
are
much
larger
than
intended
or
to
circuits
with
asynchronous
logic
When
the
synthesis
results
are
good
the
FPGA
tool
maps
the
functions
onto
the
CLBs
of
a
specific
chip
The
place
and
route
tool
determines
which
functions
go
in
which
lookup
tables
and
how
they
are
wired
together
Wire
delay
increases
with
length
so
critical
circuits
should
be
placed
close
together
If
the
design
is
too
big
to
fit
on
the
chip
it
must
be
reengineered
Timing
analysis
compares
the
timing
constraints
e
g
an
intended
clock
speed
of
MHz
against
the
actual
circuit
delays
and
reports
any
errors
If
the
logic
is
too
slow
it
may
have
to
be
redesigned
or
pipelined
differently
When
the
design
is
correct
a
file
is
generated
specifying
the
contents
of
all
the
CLBs
and
the
programming
of
all
the
wires
on
the
FPGA
Many
FPGAs
store
this
configuration
information
in
static
RAM
that
must
be
reloaded
each
time
the
FPGA
is
turned
on
The
FPGA
can
download
this
information
from
a
computer
in
the
A
Programmable
Logic
Design
Entry
Synthesis
Logic
Verification
Mapping
Timing
Analysis
Configure
FPGA
Place
and
Route
Debug
Debug
Too
big
Too
slow
Figure
A
FPGA
design
flow
laboratory
or
can
read
it
from
a
nonvolatile
ROM
when
power
is
first
applied
Example
A
FPGA
TIMING
ANALYSIS
Alyssa
P
Hacker
is
using
an
FPGA
to
implement
an
M
M
sorter
with
a
color
sensor
and
motors
to
put
red
candy
in
one
jar
and
green
candy
in
another
Her
design
is
implemented
as
an
FSM
and
she
is
using
a
Spartan
XC
S
FPGA
a
chip
from
the
Spartan
series
family
According
to
the
data
sheet
the
FPGA
has
the
timing
characteristics
shown
in
Table
A
Assume
that
the
design
is
small
enough
that
wire
delay
is
negligible
Alyssa
would
like
her
FSM
to
run
at
MHz
What
is
the
maximum
number
of
CLBs
on
the
critical
path
What
is
the
fastest
speed
at
which
her
FSM
could
possibly
run
SOLUTION
At
MHz
the
cycle
time
Tc
is
ns
Alyssa
uses
Equation
figure
to
out
the
minimum
combinational
propagation
delay
tpd
at
this
cycle
time
tpd
ns
ns
ns
ns
A
Alyssa
s
FSM
can
use
at
most
consecutive
CLBs
to
implement
the
next
state
logic
The
fastest
speed
at
which
an
FSM
will
run
on
a
Spartan
FPGA
is
when
it
is
using
a
single
CLB
for
the
next
state
logic
The
minimum
cycle
time
is
Tc
ns
ns
ns
ns
A
Therefore
the
maximum
frequency
is
MHz
APPENDIX
A
Digital
System
Implementation
Table
A
Spartan
XC
S
timing
name
value
ns
tpcq
tsetup
thold
tpd
per
CLB
tskew
Xilinx
advertises
the
XC
S
E
FPGA
with
lookup
tables
and
flip
flops
for
in
quantities
of
in
In
more
modest
quantities
medium
sized
FPGAs
typically
cost
about
and
the
largest
Ap
FPGAs
cost
hundreds
or
even
thousands
of
dollars
The
cost
has
declined
at
approximately
per
year
so
FPGAs
are
becoming
extremely
popular
A
APPLICATION
SPECIFIC
INTEGRATED
CIRCUITS
Application
specific
integrated
circuits
ASICs
are
chips
designed
for
a
particular
purpose
Graphics
accelerators
network
interface
chips
and
cell
phone
chips
are
common
examples
of
ASICS
The
ASIC
designer
places
transistors
to
form
logic
gates
and
wires
the
gates
together
Because
the
ASIC
is
hardwired
for
a
specific
function
it
is
typically
several
times
faster
than
an
FPGA
and
occupies
an
order
of
magnitude
less
chip
area
and
hence
cost
than
an
FPGA
with
the
same
function
However
the
masks
specifying
where
transistors
and
wires
are
located
on
the
chip
cost
hundreds
of
thousands
of
dollars
to
produce
The
fabrication
process
usually
requires
to
weeks
to
manufacture
package
and
test
the
ASICs
If
errors
are
discovered
after
the
ASIC
is
manufactured
the
designer
must
correct
the
problem
generate
new
masks
and
wait
for
another
batch
of
chips
to
be
fabricated
Hence
ASICs
are
suitable
only
for
products
that
will
be
produced
in
large
quantities
and
whose
function
is
well
defined
in
advance
Figure
A
shows
the
ASIC
design
process
which
is
similar
to
the
FPGA
design
process
of
Figure
A
Logic
verification
is
especially
important
because
correction
of
errors
after
the
masks
are
produced
is
expensive
Synthesis
produces
a
netlist
consisting
of
logic
gates
and
connections
between
the
gates
the
gates
in
this
netlist
are
placed
and
the
wires
are
routed
between
gates
When
the
design
is
satisfactory
masks
are
generated
and
used
to
fabricate
the
ASIC
A
single
speck
of
dust
can
ruin
an
ASIC
so
the
chips
must
be
tested
after
fabrication
The
fraction
of
manufactured
chips
that
work
is
called
the
yield
it
is
typically
to
depending
on
the
size
of
the
chip
and
the
maturity
of
the
manufacturing
process
Finally
the
working
chips
are
placed
in
packages
as
will
be
discussed
in
Section
A
A
DATA
SHEETS
Integrated
circuit
manufacturers
publish
data
sheets
that
describe
the
functions
and
performance
of
their
chips
It
is
essential
to
read
and
understand
the
data
sheets
One
of
the
leading
sources
of
errors
in
digital
systems
comes
from
misunderstanding
the
operation
of
a
chip
Data
sheets
are
usually
available
from
the
manufacturer
s
Web
site
If
you
cannot
locate
the
data
sheet
for
a
part
and
do
not
have
clear
documentation
from
another
source
don
t
use
the
part
Some
of
the
entries
in
the
data
sheet
may
be
cryptic
Often
the
manufacturer
publishes
data
books
containing
data
sheets
for
many
related
parts
The
A
Data
Sheets
Design
Entry
Synthesis
Logic
Verification
Timing
Analysis
Generate
Masks
Place
and
Route
Debug
Debug
Too
big
Too
slow
Fabricate
ASIC
Test
ASIC
Package
ASIC
Defective
Figure
A
ASIC
design
flow
beginning
of
the
data
book
has
additional
explanatory
information
This
information
can
usually
be
found
on
the
Web
with
a
careful
search
This
section
dissects
the
Texas
Instruments
TI
data
sheet
for
a
HC
inverter
chip
The
data
sheet
is
relatively
simple
but
illustrates
many
of
the
major
elements
TI
still
manufacturers
a
wide
variety
of
xx
series
chips
In
the
past
many
other
companies
built
these
chips
too
but
the
market
is
consolidating
as
the
sales
decline
Figure
A
shows
the
first
page
of
the
data
sheet
Some
of
the
key
sections
are
highlighted
in
blue
The
title
is
SN
HC
SN
HC
HEX
INVERTERS
HEX
INVERTERS
means
that
the
chip
contains
six
inverters
SN
indicates
that
TI
is
the
manufacturer
Other
manufacture
codes
include
MC
for
Motorola
and
DM
for
National
Semiconductor
You
can
generally
ignore
these
codes
because
all
of
the
manufacturers
build
compatible
xxseries
logic
HC
is
the
logic
family
high
speed
CMOS
The
logic
family
determines
the
speed
and
power
consumption
of
the
chip
but
not
the
function
For
example
the
HC
and
LS
chips
all
contain
six
inverters
but
they
differ
in
performance
and
cost
Other
logic
families
are
discussed
in
Section
A
The
xx
chips
operate
across
the
commercial
or
industrial
temperature
range
to
C
or
to
C
respectively
whereas
the
xx
chips
operate
across
the
military
temperature
range
to
C
and
sell
for
a
higher
price
but
are
otherwise
compatible
The
is
available
in
many
different
packages
and
it
is
important
to
order
the
one
you
intended
when
you
make
a
purchase
The
packages
are
distinguished
by
a
suffix
on
the
part
number
N
indicates
a
plastic
dual
inline
package
PDIP
which
fits
in
a
breadboard
or
can
be
soldered
in
through
holes
in
a
printed
circuit
board
Other
packages
are
discussed
in
Section
A
The
function
table
shows
that
each
gate
inverts
its
input
If
A
is
HIGH
H
Y
is
LOW
L
and
vice
versa
The
table
is
trivial
in
this
case
but
is
more
interesting
for
more
complex
chips
Figure
A
shows
the
second
page
of
the
data
sheet
The
logic
diagram
indicates
that
the
chip
contains
inverters
The
absolute
maximum
section
indicates
conditions
beyond
which
the
chip
could
be
destroyed
In
particular
the
power
supply
voltage
VCC
also
called
VDD
in
this
book
should
not
exceed
V
The
continuous
output
current
should
not
exceed
mA
The
thermal
resistance
or
impedance
JA
is
used
to
calculate
the
temperature
rise
caused
by
the
chip
s
dissipating
power
If
the
ambient
temperature
in
the
vicinity
of
the
chip
is
TA
and
the
chip
dissipates
Pchip
then
the
temperature
on
the
chip
itself
at
its
junction
with
the
package
is
TJ
TA
Pchip
JA
A
For
example
if
a
chip
in
a
plastic
DIP
package
is
operating
in
a
hot
box
at
C
and
consumes
mW
the
junction
temperature
will
APPENDIX
A
Digital
System
Implementation
A
A
Data
Sheets
SCLS
D
DECEMBER
REVISED
JULY
POST
OFFICE
BOX
DALLAS
TEXAS
Wide
Operating
Voltage
Range
of
V
to
V
Outputs
Can
Drive
Up
To
LSTTL
Loads
Low
Power
Consumption
A
Max
ICC
Typical
tpd
ns
mA
Output
Drive
at
V
Low
Input
Current
of
A
Max
A
Y
A
Y
A
Y
GND
VCC
A
Y
A
Y
A
Y
SN
HC
J
OR
W
PACKAGE
SN
HC
D
N
NS
OR
PW
PACKAGE
TOPVIEW
Y
NC
A
NC
Y
A
NC
Y
NC
A
Y
A
CN
Y
A
VCC
A
Y
DNG
CN
SN
HC
FK
PACKAGE
TOPVIEW
NC
No
internal
connection
description
ordering
information
ORDERING
INFORMATION
TA
PACKAGE
ORDERABLE
PARTNUMBER
TOP
SIDE
MARKING
PDIP
N
Tube
of
SN
HC
N
SN
HC
N
Tube
of
SN
HC
D
SOIC
D
Reel
of
SN
HC
DR
HC
Reel
of
SN
HC
DT
C
to
C
SOP
NS
Reel
of
SN
HC
NSR
HC
Tube
of
SN
HC
PW
TSSOP
PW
Reel
of
SN
HC
PWR
HC
Reel
of
SN
HC
PWT
CDIP
J
Tube
of
SNJ
HC
J
SNJ
HC
J
C
to
C
CFP
W
Tube
of
SNJ
HC
W
SNJ
HC
W
LCCC
FK
Tube
of
SNJ
HC
FK
SNJ
HC
FK
Package
drawings
standard
packing
quantities
thermal
data
symbolization
and
PCB
design
guidelines
are
available
at
www
ti
com
sc
package
FUNCTION
TABLE
each
inverter
INPUT
A
OUTPUT
Y
H
L
L
H
Please
be
aware
that
an
important
notice
concerning
availability
standard
warranty
and
use
in
critical
applications
of
Texas
Instruments
semiconductor
products
and
disclaimers
there
to
appears
at
the
end
of
this
data
sheet
PRODUCTION
DATA
information
is
current
as
of
publication
date
Copyright
c
Texas
Instruments
Incorporated
Products
conform
to
specifications
per
the
terms
of
Texas
Instruments
standard
warranty
Production
processing
does
not
necessarily
include
testing
of
all
parameters
On
products
compliant
to
MIL
PRF
all
parameters
are
tested
unless
otherwise
noted
On
all
other
products
production
processing
does
not
necessarily
include
testing
of
all
parameters
SN
HC
SN
HC
HEX
INVERTERS
The
HC
devices
contain
six
independent
inverters
They
perform
the
Boolean
function
Y
A
in
positive
logic
Figure
A
data
sheet
page
APPENDIX
A
Digital
System
Implementation
SN
HC
SN
HC
HEX
INVERTERS
SCLS
D
DECEMBER
REVISED
JULY
POST
OFFICE
BOX
DALLAS
TEXAS
logic
diagram
positive
logic
A
Y
absolute
maximum
ratings
over
operating
free
air
temperature
range
unless
otherwise
noted
Supply
voltage
range
VCC
V
to
V
Input
clamp
current
IIK
VI
or
VI
VCC
see
Note
mA
Output
clamp
current
IOK
VO
or
VO
VCC
see
Note
mA
Continuous
output
current
IO
VO
to
VCC
mA
Continuous
current
through
VCC
or
GND
mA
Package
thermal
impedance
JA
egakcapD
etoNees
C
W
N
egakcap
C
W
N
egakcapS
C
W
P
egakcapW
C
W
Storage
temperature
range
Tstg
C
to
C
Stresses
beyond
those
listed
under
absolute
maximum
ratings
may
cause
permanent
damage
to
the
device
These
are
stress
ratings
only
and
functional
operation
of
the
device
at
these
or
any
other
conditions
beyond
those
indicated
under
recommended
operating
conditions
is
not
implied
Exposure
to
absolute
maximum
rated
conditions
for
extended
periods
may
affect
device
reliability
NOTES
The
input
and
output
voltage
ratings
may
be
exceeded
if
the
input
and
output
current
ratings
are
observed
The
package
thermal
impedance
is
calculated
in
accordance
with
JESD
recommended
operating
conditions
see
Note
SN
HC
SN
HC
UNIT
MIN
NOM
MAX
MIN
NOM
MAX
UNIT
VCC
Supply
voltage
V
VCC
V
VIH
High
level
input
voltage
VCC
V
V
VCC
V
VCC
V
VIL
Low
level
input
voltage
VCC
V
V
VCC
V
VI
Input
voltage
VCC
VCC
V
VO
Output
voltage
VCC
VCC
V
VCC
V
t
v
Input
transition
rise
fall
time
VCC
V
ns
VCC
V
TA
Operating
free
air
temperature
C
NOTE
All
unused
inputs
of
the
device
must
be
held
at
VCC
or
GND
to
ensure
proper
device
operation
Refer
to
the
TI
application
report
Implications
of
Slow
or
Floating
CMOS
Inputs
literature
number
SCBA
TEXAS
INSTRUMENTS
Figure
A
datasheet
page
climb
to
C
W
C
W
C
Internal
power
dissipation
is
seldom
important
for
xx
series
chips
but
it
becomes
important
for
modern
chips
that
dissipate
tens
of
watts
or
more
The
recommended
operating
conditions
define
the
environment
in
which
the
chip
should
be
used
Within
these
conditions
the
chip
should
meet
specifications
These
conditions
are
more
stringent
than
the
absolute
maximums
For
example
the
power
supply
voltage
should
be
between
and
V
The
input
logic
levels
for
the
HC
logic
family
depend
on
VDD
Use
the
V
entries
when
VDD
V
to
allow
for
a
droop
in
the
power
supply
caused
by
noise
in
the
system
Figure
A
shows
the
third
page
of
the
data
sheet
The
electrical
characteristics
describe
how
the
device
performs
when
used
within
the
recommended
operating
conditions
if
the
inputs
are
held
constant
For
example
if
VCC
V
and
droops
to
V
and
the
output
current
IOH
IOL
does
not
exceed
A
VOH
V
and
VOL
V
in
the
worst
case
If
the
output
current
increases
the
output
voltages
become
less
ideal
because
the
transistors
on
the
chip
struggle
to
provide
the
current
The
HC
logic
family
uses
CMOS
transistors
that
draw
very
little
current
The
current
into
each
input
is
guaranteed
to
be
less
than
nA
and
is
typically
only
nA
at
room
temperature
The
quiescent
power
supply
current
IDD
drawn
while
the
chip
is
idle
is
less
than
A
Each
input
has
less
than
pF
of
capacitance
The
switching
characteristics
define
how
the
device
performs
when
used
within
the
recommended
operating
conditions
if
the
inputs
change
The
propagation
delay
tpd
is
measured
from
when
the
input
passes
through
VCC
to
when
the
output
passes
through
VCC
If
VCC
is
nominally
V
and
the
chip
drives
a
capacitance
of
less
than
pF
the
propagation
delay
will
not
exceed
ns
and
typically
will
be
much
faster
Recall
that
each
input
may
present
pF
so
the
chip
cannot
drive
more
than
five
identical
chips
at
full
speed
Indeed
stray
capacitance
from
the
wires
connecting
chips
cuts
further
into
the
useful
load
The
transition
time
also
called
the
rise
fall
time
is
measured
as
the
output
transitions
between
VCC
and
VCC
Recall
from
Section
that
chips
consume
both
static
and
dynamic
power
Static
power
is
low
for
HC
circuits
At
C
the
maximum
quiescent
supply
current
is
A
At
V
this
gives
a
static
power
consumption
of
mW
The
dynamic
power
depends
on
the
capacitance
being
driven
and
the
switching
frequency
The
has
an
internal
power
dissipation
capacitance
of
pF
per
inverter
If
all
six
inverters
on
the
switch
at
MHz
and
drive
external
loads
of
pF
then
the
dynamic
power
given
by
Equation
is
pF
pF
MHz
mW
and
the
maximum
total
power
is
mW
A
Data
Sheets
Appendi
APPENDIX
A
Digital
System
Implementation
SN
HC
SN
HC
HEX
INVERTERS
SCLS
D
DECEMBER
REVISED
JULY
POST
OFFICE
BOX
DALLAS
TEXAS
TEXAS
INSTRUMENTS
electrical
characteristics
over
recommended
operating
free
air
temperature
range
unless
otherwise
noted
PARAMETER
TEST
CONDITIONS
VCC
TA
C
SN
HC
SN
HC
UNIT
MIN
TYP
MAX
MIN
MAX
MIN
MAX
UNIT
V
I
OH
A
I
OL
A
V
VOH
VOL
VI
VIH
or
VIL
VI
VIH
or
VIL
VI
VCC
or
VI
VCC
or
V
V
I
OH
mA
I
OL
mA
V
I
OH
mA
I
OL
mA
V
V
V
V
V
V
V
I
I
V
nA
I
CC
I
O
V
A
Ci
Vto
V
pF
switching
characteristics
over
recommended
operating
free
air
temperature
range
CL
pF
unless
otherwise
noted
see
Figure
FROM
TO
VCC
TA
C
SN
HC
SN
HC
PARAMETER
UNIT
INPUT
OUTPUT
MIN
TYP
MAX
MIN
MAX
MIN
MAX
UNIT
V
t
pd
A
Y
V
ns
V
V
t
t
Y
V
ns
V
operating
characteristics
TA
C
PARAMETER
TEST
CONDITIONS
TYP
UNIT
Cpd
Power
dissipation
capacitance
per
inverter
No
load
pF
Figure
A
datasheet
page
A
LOGIC
FAMILIES
The
xx
series
logic
chips
have
been
manufactured
using
many
different
technologies
called
logic
families
that
offer
different
speed
power
and
logic
level
trade
offs
Other
chips
are
usually
designed
to
be
compatible
with
some
of
these
logic
families
The
original
chips
such
as
the
were
built
using
bipolar
transistors
in
a
technology
called
TransistorTransistor
Logic
TTL
Newer
technologies
add
one
or
more
letters
after
the
to
indicate
the
logic
family
such
as
LS
HC
or
AHCT
Table
A
summarizes
the
most
common
V
logic
families
Advances
in
bipolar
circuits
and
process
technology
led
to
the
Schottky
S
and
Low
Power
Schottky
LS
families
Both
are
faster
than
TTL
Schottky
draws
more
power
whereas
Low
power
Schottky
draws
less
Advanced
Schottky
AS
and
Advanced
Low
Power
Schottky
ALS
have
improved
speed
and
power
compared
to
S
and
LS
Fast
F
logic
is
faster
and
draws
less
power
than
AS
All
of
these
families
provide
more
current
for
LOW
outputs
than
for
HIGH
outputs
and
hence
have
asymmetric
logic
levels
They
conform
to
the
TTL
logic
levels
VIH
V
VIL
V
VOH
V
and
VOL
V
A
Logic
Families
Table
A
Typical
specifications
for
V
logic
families
CMOS
TTL
Bipolar
TTL
CMOS
Compatible
Characteristic
TTL
S
LS
AS
ALS
F
HC
AHC
HCT
AHCT
tpd
ns
VIH
V
VIL
V
VOH
V
VOL
V
IOH
mA
IOL
mA
IIL
mA
IIH
mA
IDD
mA
Cpd
pF
n
a
cost
US
obsolete
Per
unit
in
quantities
of
for
the
from
Texas
Instruments
in
Ap
As
CMOS
circuits
matured
in
the
s
and
s
they
became
popular
because
they
draw
very
little
power
supply
or
input
current
The
High
Speed
CMOS
HC
and
Advanced
High
Speed
CMOS
AHC
families
draw
almost
no
static
power
They
also
deliver
the
same
current
for
HIGH
and
LOW
outputs
They
conform
to
the
CMOS
logic
levels
VIH
V
VIL
V
VOH
V
and
VOL
V
Unfortunately
these
levels
are
incompatible
with
TTL
circuits
because
a
TTL
HIGH
output
of
V
may
not
be
recognized
as
a
legal
CMOS
HIGH
input
This
motivates
the
use
of
High
Speed
TTL
compatible
CMOS
HCT
and
Advanced
High
Speed
TTLcompatible
CMOS
AHCT
which
accept
TTL
input
logic
levels
and
generate
valid
CMOS
output
logic
levels
These
families
are
slightly
slower
than
their
pure
CMOS
counterparts
All
CMOS
chips
are
sensitive
to
electrostatic
discharge
ESD
caused
by
static
electricity
Ground
yourself
by
touching
a
large
metal
object
before
handling
CMOS
chips
lest
you
zap
them
The
xx
series
logic
is
inexpensive
The
newer
logic
families
are
often
cheaper
than
the
obsolete
ones
The
LS
family
is
widely
available
and
robust
and
is
a
popular
choice
for
laboratory
or
hobby
projects
that
have
no
special
performance
requirements
The
V
standard
collapsed
in
the
mid
s
when
transistors
became
too
small
to
withstand
the
voltage
Moreover
lower
voltage
offers
lower
power
consumption
Now
and
even
lower
voltages
are
commonly
used
The
plethora
of
voltages
raises
challenges
in
communicating
between
chips
with
different
power
supplies
Table
A
lists
some
of
the
low
voltage
logic
families
Not
all
xx
parts
are
available
in
all
of
these
logic
families
All
of
the
low
voltage
logic
families
use
CMOS
transistors
the
workhorse
of
modern
integrated
circuits
They
operate
over
a
wide
range
of
VDD
but
the
speed
degrades
at
lower
voltage
Low
Voltage
CMOS
LVC
logic
and
Advanced
Low
Voltage
CMOS
ALVC
logic
are
commonly
used
at
or
V
LVC
withstands
inputs
up
to
V
so
it
can
receive
inputs
from
V
CMOS
or
TTL
circuits
Advanced
Ultra
Low
Voltage
CMOS
AUC
is
commonly
used
at
or
V
and
is
exceptionally
fast
Both
ALVC
and
AUC
withstand
inputs
up
to
V
so
they
can
receive
inputs
from
V
circuits
FPGAs
often
offer
separate
voltage
supplies
for
the
internal
logic
called
the
core
and
for
the
input
output
I
O
pins
As
FPGAs
have
advanced
the
core
voltage
has
dropped
from
to
and
V
to
save
power
and
avoid
damaging
the
very
small
transistors
FPGAs
have
configurable
I
Os
that
can
operate
at
many
different
voltages
so
as
to
be
compatible
with
the
rest
of
the
system
APPENDIX
A
Digital
System
Implementation
Ap
A
PACKAGING
AND
ASSEMBLY
Integrated
circuits
are
typically
placed
in
packages
made
of
plastic
or
ceramic
The
packages
serve
a
number
of
functions
including
connecting
the
tiny
metal
I
O
pads
of
the
chip
to
larger
pins
in
the
package
for
ease
of
connection
protecting
the
chip
from
physical
damage
and
spreading
the
heat
generated
by
the
chip
over
a
larger
area
to
help
with
cooling
The
packages
are
placed
on
a
breadboard
or
printed
circuit
board
and
wired
together
to
assemble
the
system
Packages
Figure
A
shows
a
variety
of
integrated
circuit
packages
Packages
can
be
generally
categorized
as
through
hole
or
surface
mount
SMT
Through
hole
packages
as
their
name
implies
have
pins
that
can
be
inserted
through
holes
in
a
printed
circuit
board
or
into
a
socket
Dual
inline
packages
DIPs
have
two
rows
of
pins
with
inch
spacing
between
pins
Pin
grid
arrays
PGAs
support
more
pins
in
a
smaller
package
by
placing
the
pins
under
the
package
SMT
packages
are
soldered
directly
to
the
surface
of
a
printed
circuit
board
without
using
holes
Pins
on
SMT
parts
are
called
leads
The
thin
small
outline
package
TSOP
has
two
rows
of
closely
spaced
leads
typically
inch
spacing
Plastic
leaded
chip
carriers
PLCCs
have
J
shaped
leads
on
all
four
A
Packaging
and
Assembly
Table
A
Typical
specifications
for
low
voltage
logic
families
LVC
ALVC
AUC
Vdd
V
tpd
ns
VIH
V
VIL
V
VOH
V
VOL
V
IO
mA
II
mA
IDD
mA
Cpd
pF
cost
US
not
available
Delay
and
capacitance
not
available
at
the
time
of
writing
sides
with
inch
spacing
They
can
be
soldered
directly
to
a
board
or
placed
in
special
sockets
Quad
flat
packs
QFPs
accommodate
a
large
number
of
pins
using
closely
spaced
legs
on
all
four
sides
Ball
grid
arrays
BGAs
eliminate
the
legs
altogether
Instead
they
have
hundreds
of
tiny
solder
balls
on
the
underside
of
the
package
They
are
carefully
placed
over
matching
pads
on
a
printed
circuit
board
then
heated
so
that
the
solder
melts
and
joins
the
package
to
the
underlying
board
Breadboards
DIPs
are
easy
to
use
for
prototyping
because
they
can
be
placed
in
a
breadboard
A
breadboard
is
a
plastic
board
containing
rows
of
sockets
as
shown
in
Figure
A
All
five
holes
in
a
row
are
connected
together
Each
pin
of
the
package
is
placed
in
a
hole
in
a
separate
row
Wires
can
be
placed
in
adjacent
holes
in
the
same
row
to
make
connections
to
the
pin
Breadboards
often
provide
separate
columns
of
connected
holes
running
the
height
of
the
board
to
distribute
power
and
ground
Figure
A
shows
a
breadboard
containing
a
majority
gate
built
with
a
LS
AND
chip
and
a
LS
OR
chip
The
schematic
of
the
circuit
is
shown
in
Figure
A
Each
gate
in
the
schematic
is
labeled
with
the
chip
or
and
the
pin
numbers
of
the
inputs
and
outputs
see
Figure
A
Observe
that
the
same
connections
are
made
on
the
breadboard
The
inputs
are
connected
to
pins
and
of
the
chip
and
the
output
is
measured
at
pin
of
the
chip
Power
and
ground
are
connected
to
pins
and
respectively
of
each
chip
from
the
vertical
power
and
ground
columns
that
are
attached
to
the
banana
plug
receptacles
Vb
and
Va
Labeling
the
schematic
in
this
way
and
checking
off
connections
as
they
are
made
is
a
good
way
to
reduce
the
number
of
mistakes
made
during
breadboarding
Unfortunately
it
is
easy
to
accidentally
plug
a
wire
in
the
wrong
hole
or
have
a
wire
fall
out
so
breadboarding
requires
a
great
deal
of
care
and
usually
some
debugging
in
the
laboratory
Breadboards
are
suited
only
to
prototyping
not
production
APPENDIX
A
Digital
System
Implementation
Figure
A
Integrated
circuit
packages
Printed
Circuit
Boards
Instead
of
breadboarding
chip
packages
may
be
soldered
to
a
printed
circuit
board
PCB
The
PCB
is
formed
of
alternating
layers
of
conducting
copper
and
insulating
epoxy
The
copper
is
etched
to
form
wires
called
traces
Holes
called
vias
are
drilled
through
the
board
and
plated
with
metal
to
connect
between
layers
PCBs
are
usually
designed
with
computer
aided
design
CAD
tools
You
can
etch
and
drill
your
own
simple
boards
in
the
laboratory
or
you
can
send
the
board
design
to
a
specialized
factory
for
inexpensive
mass
production
Factories
have
turnaround
times
of
days
or
weeks
for
cheap
mass
production
runs
and
typically
charge
a
few
hundred
dollars
in
setup
fees
and
a
few
dollars
per
board
for
moderately
complex
boards
built
in
large
quantities
A
Packaging
and
Assembly
BC
Y
A
Figure
A
Majority
gate
schematic
with
chips
and
pins
identified
GND
Rows
of
pins
are
internally
connected
VDD
Figure
A
Majority
circuit
on
breadboard
APPENDIX
A
Digital
System
Implementation
Signal
Layer
Signal
Layer
Power
Plane
Ground
Plane
Copper
Trace
Insulator
Figure
A
Printed
circuit
board
cross
section
PCB
traces
are
normally
made
of
copper
because
of
its
low
resistance
The
traces
are
embedded
in
an
insulating
material
usually
a
green
fireresistant
plastic
called
FR
A
PCB
also
typically
has
copper
power
and
ground
layers
called
planes
between
signal
layers
Figure
A
shows
a
cross
section
of
a
PCB
The
signal
layers
are
on
the
top
and
bottom
and
the
power
and
ground
planes
are
embedded
in
the
center
of
the
board
The
power
and
ground
planes
have
low
resistance
so
they
distribute
stable
power
to
components
on
the
board
They
also
make
the
capacitance
and
inductance
of
the
traces
uniform
and
predictable
Figure
A
shows
a
PCB
for
a
s
vintage
Apple
II
computer
At
the
top
is
a
Motorola
microprocessor
Beneath
are
six
Kb
ROM
chips
forming
KB
of
ROM
containing
the
operating
system
Three
rows
of
eight
Kb
DRAM
chips
provide
KB
of
RAM
On
the
right
are
several
rows
of
xx
series
logic
for
memory
address
decoding
and
other
functions
The
lines
between
chips
are
traces
that
wire
the
chips
together
The
dots
at
the
ends
of
some
of
the
traces
are
vias
filled
with
metal
Putting
It
All
Together
Most
modern
chips
with
large
numbers
of
inputs
and
outputs
use
SMT
packages
especially
QFPs
and
BGAs
These
packages
require
a
printed
circuit
board
rather
than
a
breadboard
Working
with
BGAs
is
especially
challenging
because
they
require
specialized
assembly
equipment
Moreover
the
balls
cannot
be
probed
with
a
voltmeter
or
oscilloscope
during
debugging
in
the
laboratory
because
they
are
hidden
under
the
package
In
summary
the
designer
needs
to
consider
packaging
early
on
to
determine
whether
a
breadboard
can
be
used
during
prototyping
and
whether
BGA
parts
will
be
required
Professional
engineers
rarely
use
breadboards
when
they
are
confident
of
connecting
chips
together
correctly
without
experimentation
A
TRANSMISSION
LINES
We
have
assumed
so
far
that
wires
are
equipotential
connections
that
have
a
single
voltage
along
their
entire
length
Signals
actually
propagate
along
wires
at
the
speed
of
light
in
the
form
of
electromagnetic
waves
If
the
wires
are
short
enough
or
the
signals
change
slowly
the
equipotential
assumption
is
good
enough
When
the
wire
is
long
or
the
signal
is
very
fast
the
transmission
time
along
the
wire
becomes
important
to
accurately
determine
the
circuit
delay
We
must
model
such
wires
as
transmission
lines
in
which
a
wave
of
voltage
and
current
propagates
at
the
speed
of
light
When
the
wave
reaches
the
end
of
the
line
it
may
reflect
back
along
the
line
The
reflection
may
cause
noise
and
odd
behaviors
unless
steps
are
taken
to
limit
it
Hence
the
digital
designer
must
consider
transmission
line
behavior
to
accurately
account
for
the
delay
and
noise
effects
in
long
wires
Electromagnetic
waves
travel
at
the
speed
of
light
in
a
given
medium
which
is
fast
but
not
instantaneous
The
speed
of
light
depends
on
the
permittivity
and
permeability
of
the
medium
v
s
sLC
A
Transmission
Lines
Figure
A
Apple
II
circuit
board
The
capacitance
C
and
inductance
L
of
a
wire
are
related
to
the
permittivity
and
permeability
of
the
physical
medium
in
which
the
wire
is
located
Ap
The
speed
of
light
in
free
space
is
c
m
s
Signals
in
a
PCB
travel
at
about
half
this
speed
because
the
FR
insulator
has
four
times
the
permittivity
of
air
Thus
PCB
signals
travel
at
about
m
s
or
cm
ns
The
time
delay
for
a
signal
to
travel
along
a
transmission
line
of
length
l
is
td
l
v
A
The
characteristic
impedance
of
a
transmission
line
Z
pronounced
Z
naught
is
the
ratio
of
voltage
to
current
in
a
wave
traveling
along
the
line
Z
V
I
It
is
not
the
resistance
of
the
wire
a
good
transmission
line
in
a
digital
system
typically
has
negligible
resistance
Z
depends
on
the
inductance
and
capacitance
of
the
line
see
the
derivation
in
Section
A
and
typically
has
a
value
of
to
A
Figure
A
shows
the
symbol
for
a
transmission
line
The
symbol
resembles
a
coaxial
cable
with
an
inner
signal
conductor
and
an
outer
grounded
conductor
like
that
used
in
television
cable
wiring
The
key
to
understanding
the
behavior
of
transmission
lines
is
to
visualize
the
wave
of
voltage
propagating
along
the
line
at
the
speed
of
light
When
the
wave
reaches
the
end
of
the
line
it
may
be
absorbed
or
reflected
depending
on
the
termination
or
load
at
the
end
Reflections
travel
back
along
the
line
adding
to
the
voltage
already
on
the
line
Terminations
are
classified
as
matched
open
short
or
mismatched
The
following
subsections
explore
how
a
wave
propagates
along
the
line
and
what
happens
to
the
wave
when
it
reaches
the
termination
A
Matched
Termination
Figure
A
shows
a
transmission
line
of
length
l
with
a
matched
termination
which
means
that
the
load
impedance
ZL
is
equal
to
the
characteristic
impedance
Z
The
transmission
line
has
a
characteristic
impedance
of
One
end
of
the
line
is
connected
to
a
voltage
Z
v
L
C
APPENDIX
A
Digital
System
Implementation
I
V
Z
C
L
Figure
A
Transmission
line
symbol
Appendi
A
Transmission
Lines
VS
t
Z
length
l
td
l
v
A
C
B
l
ZL
Figure
A
Transmission
line
with
matched
termination
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
ZL
A
B
C
Figure
A
Equivalent
circuit
of
Figure
A
at
steady
state
source
through
a
switch
that
closes
at
time
t
The
other
end
is
connected
to
the
matched
load
This
section
analyzes
the
voltages
and
currents
at
points
A
B
and
C
at
the
beginning
of
the
line
one
third
of
the
length
along
the
line
and
at
the
end
of
the
line
respectively
Figure
A
shows
the
voltages
at
points
A
B
and
C
over
time
Initially
there
is
no
voltage
or
current
flowing
in
the
transmission
line
because
the
switch
is
open
At
time
t
the
switch
closes
and
the
voltage
source
launches
a
wave
with
voltage
V
VS
along
the
line
Because
the
characteristic
impedance
is
Z
the
wave
has
current
I
VS
Z
The
voltage
reaches
the
beginning
of
the
line
point
A
immediately
as
shown
in
Figure
A
a
The
wave
propagates
along
the
line
at
the
speed
of
light
At
time
td
the
wave
reaches
point
B
The
voltage
at
this
point
abruptly
rises
from
to
VS
as
shown
in
Figure
A
b
At
time
td
the
incident
wave
reaches
point
C
at
the
end
of
the
line
and
the
voltage
rises
there
too
All
of
the
current
I
flows
into
the
resistor
ZL
producing
a
voltage
across
the
resistor
of
ZLI
ZL
VS
Z
VS
because
ZL
Z
This
voltage
is
consistent
with
the
wave
flowing
along
the
transmission
line
Thus
the
wave
is
absorbed
by
the
load
impedance
and
the
transmission
line
reaches
its
steady
state
In
steady
state
the
transmission
line
behaves
like
an
ideal
equipotential
wire
because
it
is
after
all
just
a
wire
The
voltage
at
all
points
along
the
line
must
be
identical
Figure
A
shows
the
steady
state
equivalent
model
of
the
circuit
in
Figure
A
The
voltage
is
VS
everywhere
along
the
wire
Example
A
TRANSMISSION
LINE
WITH
MATCHED
SOURCE
AND
LOAD
TERMINATIONS
Figure
A
shows
a
transmission
line
with
matched
source
and
load
impedances
ZS
and
ZL
Plot
the
voltage
at
nodes
A
B
and
C
versus
time
When
does
the
system
reach
steady
state
and
what
is
the
equivalent
circuit
at
steady
state
SOLUTION
When
the
voltage
source
has
a
source
impedance
ZS
in
series
with
the
transmission
line
part
of
the
voltage
drops
across
ZS
and
the
remainder
a
t
t
d
VA
VS
c
t
VS
t
d
VC
b
t
VS
t
d
t
d
VB
Appendi
propagates
down
the
transmission
line
At
first
the
transmission
line
behaves
as
an
impedance
Z
because
the
load
at
the
end
of
the
line
cannot
possibly
influence
the
behavior
of
the
line
until
a
speed
of
light
delay
has
elapsed
Hence
by
the
voltage
divider
equation
the
incident
voltage
flowing
down
the
line
is
A
Thus
at
t
a
wave
of
voltage
is
sent
down
the
line
from
point
A
Again
the
signal
reaches
point
B
at
time
td
and
point
C
at
td
as
shown
in
Figure
A
All
of
the
current
is
absorbed
by
the
load
impedance
ZL
so
the
circuit
enters
steady
state
at
t
td
In
steady
state
the
entire
line
is
at
VS
just
as
the
steady
state
equivalent
circuit
in
Figure
A
would
predict
A
Open
Termination
When
the
load
impedance
is
not
equal
to
Z
the
termination
cannot
absorb
all
of
the
current
and
some
of
the
wave
must
be
reflected
Figure
A
shows
a
transmission
line
with
an
open
load
termination
No
current
can
flow
through
an
open
termination
so
the
current
at
point
C
must
always
be
The
voltage
on
the
line
is
initially
zero
At
t
the
switch
closes
and
a
wave
of
voltage
begins
propagating
down
the
line
Notice
that
this
initial
wave
is
the
same
as
that
of
Example
A
and
is
independent
of
the
termination
because
the
load
at
the
end
of
the
line
cannot
influence
the
behavior
at
the
beginning
until
at
least
td
has
elapsed
This
wave
reaches
point
B
at
td
and
point
C
at
td
as
shown
in
Figure
A
When
the
incident
wave
reaches
point
C
it
cannot
continue
forward
because
the
wire
is
open
It
must
instead
reflect
back
toward
the
source
The
reflected
wave
also
has
voltage
because
the
open
termination
reflects
the
entire
wave
The
voltage
at
any
point
is
the
sum
of
the
incident
and
reflected
waves
At
time
t
td
the
voltage
at
point
C
is
The
reflected
wave
reaches
point
B
at
td
and
point
A
at
td
When
it
reaches
point
A
the
wave
is
absorbed
by
the
source
termination
V
VS
VS
VS
V
VS
VVS
Z
Z
ZS
VS
V
VS
V
VS
Z
Z
ZS
VS
APPENDIX
A
Digital
System
Implementation
t
VA
a
VS
t
d
t
b
t
d
VS
VB
t
c
t
d
VC
VS
t
d
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
ZL
ZS
A
B
C
VS
Figure
A
Equivalent
circuit
of
Figure
A
at
steady
state
VS
t
Z
ZL
ZS
length
l
td
l
v
A
B
C
l
Figure
A
Transmission
line
with
matched
source
and
load
impedances
Appendix
A
qxd
A
Transmission
Lines
impedance
that
matches
the
characteristic
impedance
of
the
line
Thus
the
system
reaches
steady
state
at
time
t
td
and
the
transmission
line
becomes
equivalent
to
an
equipotential
wire
with
voltage
VS
and
current
I
A
Short
Termination
Figure
A
shows
a
transmission
line
terminated
with
a
short
circuit
to
ground
Thus
the
voltage
at
point
C
must
always
be
As
in
the
previous
examples
the
voltages
on
the
line
are
initially
When
the
switch
closes
a
wave
of
voltage
begins
propagating
down
the
line
Figure
A
When
it
reaches
the
end
of
the
line
it
must
reflect
with
opposite
polarity
The
reflected
wave
with
voltage
adds
to
the
incident
wave
ensuring
that
the
voltage
at
point
C
remains
The
reflected
wave
reaches
the
source
at
time
t
td
and
is
absorbed
by
the
source
impedance
At
this
point
the
system
reaches
steady
state
and
the
transmission
line
is
equivalent
to
an
equipotential
wire
with
voltage
V
V
VS
V
VS
t
VS
VS
t
d
t
d
t
d
VB
t
VS
t
d
VC
c
b
a
t
t
d
VA
VS
VS
t
d
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
t
Z
length
l
td
l
v
A
C
B
l
ZS
Figure
A
Transmission
line
with
open
load
termination
VS
t
Z
length
l
td
l
v
A
C
B
l
ZS
Figure
A
Transmission
line
with
short
termination
A
Mismatched
Termination
The
termination
impedance
is
said
to
be
mismatched
when
it
does
not
equal
the
characteristic
impedance
of
the
line
In
general
when
an
incident
wave
reaches
a
mismatched
termination
part
of
the
wave
is
absorbed
and
part
is
reflected
The
reflection
coefficient
kr
indicates
the
fraction
of
the
incident
wave
Vi
that
is
reflected
Vr
krVi
Section
A
derives
the
reflection
coefficient
using
conservation
of
current
arguments
It
shows
that
when
an
incident
wave
flowing
along
a
Appendi
transmission
line
of
characteristic
impedance
Z
reaches
a
termination
impedance
ZT
at
the
end
of
the
line
the
reflection
coefficient
is
A
Note
a
few
special
cases
If
the
termination
is
an
open
circuit
ZT
kr
because
the
incident
wave
is
entirely
reflected
so
the
current
out
the
end
of
the
line
remains
zero
If
the
termination
is
a
short
circuit
ZT
kr
because
the
incident
wave
is
reflected
with
negative
polarity
so
the
voltage
at
the
end
of
the
line
remains
zero
If
the
termination
is
a
matched
load
ZT
Z
kr
because
the
incident
wave
is
absorbed
Figure
A
illustrates
reflections
in
a
transmission
line
with
a
mismatched
load
termination
of
ZT
ZL
and
Z
so
kr
As
in
previous
examples
the
voltage
on
the
line
is
initially
When
the
switch
closes
a
wave
of
voltage
propagates
down
the
line
reaching
the
end
at
t
td
When
the
incident
wave
reaches
the
termination
at
the
end
of
the
line
one
fifth
of
the
wave
is
reflected
and
the
remaining
four
fifths
flows
into
the
load
impedance
Thus
the
reflected
wave
has
a
voltage
The
total
voltage
at
point
C
is
the
sum
of
the
incoming
and
reflected
voltages
At
t
td
the
reflected
wave
reaches
point
A
where
it
is
absorbed
by
the
matched
termination
ZS
Figure
A
plots
the
voltages
and
currents
along
the
line
Again
note
that
in
steady
state
in
this
case
at
time
t
td
the
transmission
line
is
equivalent
to
an
equipotential
wire
as
shown
in
Figure
A
At
steady
state
the
system
acts
like
a
voltage
divider
so
Reflections
can
occur
at
both
ends
of
the
transmission
line
Figure
A
shows
a
transmission
line
with
a
source
impedance
ZS
of
and
an
open
termination
at
the
load
The
reflection
coefficients
at
the
load
and
source
krL
and
krS
are
and
respectively
In
this
case
waves
reflect
off
both
ends
of
the
transmission
line
until
a
steady
state
is
reached
The
bounce
diagram
shown
in
Figure
A
helps
visualize
reflections
off
both
ends
of
the
transmission
line
The
horizontal
axis
represents
VA
VB
VC
VS
ZL
ZL
ZS
VS
VS
VC
VS
VS
VS
V
VS
VS
V
VS
kr
ZT
Z
ZT
Z
APPENDIX
A
Digital
System
Implementation
t
t
d
VA
VS
t
d
a
t
VS
t
d
t
d
t
d
VB
b
c
t
VS
t
d
VC
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
t
Z
length
l
td
l
v
A
C
B
l
ZL
ZS
Figure
A
Transmission
line
with
mismatched
termination
Appendix
A
qxd
distance
along
the
transmission
line
and
the
vertical
axis
represents
time
increasing
downward
The
two
sides
of
the
bounce
diagram
represent
the
source
and
load
ends
of
the
transmission
line
points
A
and
C
The
incoming
and
reflected
signal
waves
are
drawn
as
diagonal
lines
between
points
A
and
C
At
time
t
the
source
impedance
and
transmission
line
behave
as
a
voltage
divider
launching
a
voltage
wave
of
from
point
A
toward
point
C
At
time
t
td
the
signal
reaches
point
C
and
is
completely
reflected
krL
At
time
t
td
the
reflected
wave
of
reaches
point
A
and
is
reflected
with
a
reflection
coefficient
krS
to
produce
a
wave
of
traveling
toward
point
C
and
so
forth
The
voltage
at
a
given
time
at
any
point
on
the
transmission
line
is
the
sum
of
all
the
incident
and
reflected
waves
Thus
at
time
t
td
the
voltage
at
point
C
is
At
time
t
td
the
voltage
at
point
C
is
and
so
forth
Figure
A
plots
the
voltages
against
time
As
t
approaches
infinity
the
voltages
approach
steady
state
with
VA
VB
VC
VS
VS
VS
VS
VS
VS
VS
VS
VS
VS
VS
VS
A
Transmission
Lines
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
t
length
l
td
l
v
A
C
B
l
ZS
Z
Figure
A
Transmission
line
with
mismatched
source
and
load
terminations
A
B
C
VS
ZS
ZL
Figure
A
Equivalent
circuit
of
Figure
A
at
steady
state
Voltage
A
C
VS
t
td
krS
B
VS
VS
VS
VS
VS
t
td
t
td
t
td
t
td
t
td
t
td
krL
Figure
A
Bounce
diagram
for
Figure
A
t
t
d
VA
VS
VS
t
d
a
t
VB
VS
VS
t
d
t
d
t
d
b
t
t
d
VC
c
VS
Appendix
A
q
A
When
to
Use
Transmission
Line
Models
Transmission
line
models
for
wires
are
needed
whenever
the
wire
delay
td
is
longer
than
a
fraction
e
g
of
the
edge
rates
rise
or
fall
times
of
a
signal
If
the
wire
delay
is
shorter
it
has
an
insignificant
effect
on
the
propagation
delay
of
the
signal
and
the
reflections
dissipate
while
the
signal
is
transitioning
If
the
wire
delay
is
longer
it
must
be
considered
in
order
to
accurately
predict
the
propagation
delay
and
waveform
of
the
signal
In
particular
reflections
may
distort
the
digital
characteristic
of
a
waveform
resulting
in
incorrect
logic
operations
Recall
that
signals
travel
on
a
PCB
at
about
cm
ns
For
TTL
logic
with
edge
rates
of
ns
wires
must
be
modeled
as
transmission
lines
only
if
they
are
longer
than
cm
ns
cm
ns
PCB
traces
are
usually
less
than
cm
so
most
traces
can
be
modeled
as
ideal
equipotential
wires
In
contrast
many
modern
chips
have
edge
rates
of
ns
or
less
so
traces
longer
than
about
cm
about
inches
must
be
modeled
as
transmission
lines
Clearly
use
of
edge
rates
that
are
crisper
than
necessary
just
causes
difficulties
for
the
designer
Breadboards
lack
a
ground
plane
so
the
electromagnetic
fields
of
each
signal
are
nonuniform
and
difficult
to
model
Moreover
the
fields
interact
with
other
signals
This
can
cause
strange
reflections
and
crosstalk
between
signals
Thus
breadboards
are
unreliable
above
a
few
megahertz
In
contrast
PCBs
have
good
transmission
lines
with
consistent
characteristic
impedance
and
velocity
along
the
entire
line
As
long
as
they
are
terminated
with
a
source
or
load
impedance
that
is
matched
to
the
impedance
of
the
line
PCB
traces
do
not
suffer
from
reflections
A
Proper
Transmission
Line
Terminations
There
are
two
common
ways
to
properly
terminate
a
transmission
line
shown
in
Figure
A
In
parallel
termination
Figure
A
a
the
driver
has
a
low
impedance
ZS
A
load
resistor
ZL
with
impedance
Z
is
placed
in
parallel
with
the
load
between
the
input
of
the
receiver
gate
and
APPENDIX
A
Digital
System
Implementation
Figure
A
Voltage
and
current
waveforms
for
Figure
A
t
VS
VA
td
VS
VS
td
td
a
t
VB
VS
VS
VS
VS
VS
td
td
td
td
td
td
b
t
VC
c
td
td
td
VS
VS
VS
Ap
ground
When
the
driver
switches
from
to
VDD
it
sends
a
wave
with
voltage
VDD
down
the
line
The
wave
is
absorbed
by
the
matched
load
termination
and
no
reflections
take
place
In
series
termination
Figure
A
b
a
source
resistor
Z
is
placed
in
series
with
the
driver
to
raise
the
source
impedance
to
Z
The
load
has
a
high
impedance
ZL
When
the
driver
switches
it
sends
a
wave
with
voltage
VDD
down
the
line
The
wave
reflects
at
the
open
circuit
load
and
returns
bringing
the
voltage
on
the
line
up
to
VDD
The
wave
is
absorbed
at
the
source
termination
Both
schemes
are
similar
in
that
the
voltage
at
the
receiver
transitions
from
to
VDD
at
t
td
just
as
one
would
desire
They
differ
in
power
consumption
and
in
the
waveforms
that
appear
elsewhere
along
the
line
Parallel
termination
dissipates
power
continuously
through
the
load
resistor
when
the
line
is
at
a
high
voltage
Series
termination
dissipates
no
DC
power
because
the
load
is
an
open
circuit
However
in
series
terminated
lines
points
near
the
middle
of
the
transmission
line
initially
see
a
voltage
of
VDD
until
the
reflection
returns
If
other
gates
are
attached
to
the
A
Transmission
Lines
driver
A
C
B
gate
b
receiver
gate
series
termination
resistor
ZL
t
t
td
t
VDD
VA
VB
VC
t
t
t
V
DD
VDD
VB
VC
A
C
B
a
driver
gate
receiver
gate
parallel
termination
resistor
ZS
Z
ZL
Z
ZS
Z
Z
VA
VDD
VDD
td
td
VDD
td
td
td
td
td
td
td
VDD
VDD
Figure
A
Termination
schemes
a
parallel
b
series
A
middle
of
the
line
they
will
momentarily
see
an
illegal
logic
level
Therefore
series
termination
works
best
for
point
to
point
communication
with
a
single
driver
and
a
single
receiver
Parallel
termination
is
better
for
a
bus
with
multiple
receivers
because
receivers
at
the
middle
of
the
line
never
see
an
illegal
logic
level
A
Derivation
of
Z
Z
is
the
ratio
of
voltage
to
current
in
a
wave
propagating
along
a
transmission
line
This
section
derives
Z
it
assumes
some
previous
knowledge
of
resistor
inductor
capacitor
RLC
circuit
analysis
Imagine
applying
a
step
voltage
to
the
input
of
a
semi
infinite
transmission
line
so
that
there
are
no
reflections
Figure
A
shows
the
semi
infinite
line
and
a
model
of
a
segment
of
the
line
of
length
dx
R
L
and
C
are
the
values
of
resistance
inductance
and
capacitance
per
unit
length
Figure
A
b
shows
the
transmission
line
model
with
a
resistive
component
R
This
is
called
a
lossy
transmission
line
model
because
energy
is
dissipated
or
lost
in
the
resistance
of
the
wire
However
this
loss
is
often
negligible
and
we
can
simplify
analysis
by
ignoring
the
resistive
component
and
treating
the
transmission
line
as
an
ideal
transmission
line
as
shown
in
Figure
A
c
Voltage
and
current
are
functions
of
time
and
space
throughout
the
transmission
line
as
given
by
Equations
A
and
A
A
A
Taking
the
space
derivative
of
Equation
A
and
the
time
derivative
of
Equation
A
and
substituting
gives
Equation
A
the
wave
equation
A
Z
is
the
ratio
of
voltage
to
current
in
the
transmission
line
as
illustrated
in
Figure
A
a
Z
must
be
independent
of
the
length
of
the
line
because
the
behavior
of
the
wave
cannot
depend
on
things
at
a
distance
Because
it
is
independent
of
length
the
impedance
must
still
equal
Z
after
the
addition
of
a
small
amount
of
transmission
line
dx
as
shown
in
Figure
A
b
x
V
x
t
LC
t
I
x
t
x
I
x
t
C
t
V
x
t
xV
x
t
L
t
I
x
t
APPENDIX
A
Digital
System
Implementation
dx
x
a
Cdx
Rdx
dx
Ldx
b
Ldx
Cdx
dx
c
Figure
A
Transmission
line
models
a
semi
infinite
cable
b
lossy
c
ideal
App
Using
the
impedances
of
an
inductor
and
a
capacitor
we
rewrite
the
relationship
of
Figure
A
in
equation
form
Z
j
Ldx
Z
j
Cdx
A
Rearranging
we
get
Z
j
C
j
L
Z
LCdx
A
Taking
the
limit
as
dx
approaches
the
last
term
vanishes
and
we
find
that
A
A
Derivation
of
the
Reflection
Coefficient
The
reflection
coefficient
kr
is
derived
using
conservation
of
current
Figure
A
shows
a
transmission
line
with
characteristic
impedance
Z
and
load
impedance
ZL
Imagine
an
incident
wave
of
voltage
Vi
and
current
Ii
When
the
wave
reaches
the
termination
some
current
IL
flows
through
the
load
impedance
causing
a
voltage
drop
VL
The
remainder
of
the
current
reflects
back
down
the
line
in
a
wave
of
voltage
Vr
and
current
Ir
Z
is
the
ratio
of
voltage
to
current
in
waves
propagating
along
the
line
so
The
voltage
on
the
line
is
the
sum
of
the
voltages
of
the
incident
and
reflected
waves
The
current
flowing
in
the
positive
direction
on
the
line
is
the
difference
between
the
currents
of
the
incident
and
reflected
waves
VL
Vi
Vr
A
IL
Ii
Ir
A
Vi
Ii
Vr
Ir
Z
Z
v
L
C
A
Transmission
Lines
j
Ldx
dx
j
Cdx
V
Z
I
V
I
a
Z
b
Figure
A
Transmission
line
model
a
for
entire
line
and
b
with
additional
length
dx
Figure
A
Transmission
line
showing
incoming
reflected
and
load
voltages
and
currents
Z
ZL
Ii
Vi
IL
Ir
Vr
VL
Appendi
APPENDIX
A
Digital
System
Implementation
Using
Ohm
s
law
and
substituting
for
IL
Ii
and
Ir
in
Equation
A
we
get
A
Rearranging
we
solve
for
the
reflection
coefficient
kr
A
A
Putting
It
All
Together
Transmission
lines
model
the
fact
that
signals
take
time
to
propagate
down
long
wires
because
the
speed
of
light
is
finite
An
ideal
transmission
line
has
uniform
inductance
L
and
capacitance
C
per
unit
length
and
zero
resistance
The
transmission
line
is
characterized
by
its
characteristic
impedance
Z
and
delay
td
which
can
be
derived
from
the
inductance
capacitance
and
wire
length
The
transmission
line
has
significant
delay
and
noise
effects
on
signals
whose
rise
fall
times
are
less
than
about
td
This
means
that
for
systems
with
ns
rise
fall
times
PCB
traces
longer
than
about
cm
must
be
analyzed
as
transmission
lines
to
accurately
understand
their
behavior
A
digital
system
consisting
of
a
gate
driving
a
long
wire
attached
to
the
input
of
a
second
gate
can
be
modeled
with
a
transmission
line
as
shown
in
Figure
A
The
voltage
source
source
impedance
ZS
and
switch
model
the
first
gate
switching
from
to
at
time
The
driver
gate
cannot
supply
infinite
current
this
is
modeled
by
ZS
ZS
is
usually
small
for
a
logic
gate
but
a
designer
may
choose
to
add
a
resistor
in
series
with
the
gate
to
raise
ZS
and
match
the
impedance
of
the
line
The
input
to
the
second
gate
is
modeled
as
ZL
CMOS
circuits
usually
have
little
input
current
so
ZL
may
be
close
to
infinity
The
designer
may
also
choose
to
add
a
resistor
in
parallel
with
the
second
gate
between
the
gate
input
and
ground
so
that
ZL
matches
the
impedance
of
the
line
Vr
Vi
ZL
Z
ZL
Z
kr
Vi
Vr
ZL
Vi
Z
Vr
Z
a
long
wire
driver
gate
receiver
gate
VS
td
Z
Z
S
b
receiver
gate
long
wire
driver
gate
ZL
t
Figure
A
Digital
system
modeled
with
transmission
line
App
When
the
first
gate
switches
a
wave
of
voltage
is
driven
onto
the
transmission
line
The
source
impedance
and
transmission
line
form
a
voltage
divider
so
the
voltage
of
the
incident
wave
is
A
At
time
td
the
wave
reaches
the
end
of
the
line
Part
is
absorbed
by
the
load
impedance
and
part
is
reflected
The
reflection
coefficient
kr
indicates
the
portion
that
is
reflected
kr
Vr
Vi
where
Vr
is
the
voltage
of
the
reflected
wave
and
Vi
is
the
voltage
of
the
incident
wave
A
The
reflected
wave
adds
to
the
voltage
already
on
the
line
It
reaches
the
source
at
time
td
where
part
is
absorbed
and
part
is
again
reflected
The
reflections
continue
back
and
forth
and
the
voltage
on
the
line
eventually
approaches
the
value
that
would
be
expected
if
the
line
were
a
simple
equipotential
wire
A
ECONOMICS
Although
digital
design
is
so
much
fun
that
some
of
us
would
do
it
for
free
most
designers
and
companies
intend
to
make
money
Therefore
economic
considerations
are
a
major
factor
in
design
decisions
The
cost
of
a
digital
system
can
be
divided
into
nonrecurring
engineering
costs
NRE
and
recurring
costs
NRE
accounts
for
the
cost
of
designing
the
system
It
includes
the
salaries
of
the
design
team
computer
and
software
costs
and
the
costs
of
producing
the
first
working
unit
The
fully
loaded
cost
of
a
designer
in
the
United
States
in
including
salary
health
insurance
retirement
plan
and
a
computer
with
design
tools
is
roughly
per
year
so
design
costs
can
be
significant
Recurring
costs
are
the
cost
of
each
additional
unit
this
includes
components
manufacturing
marketing
technical
support
and
shipping
The
sales
price
must
cover
not
only
the
cost
of
the
system
but
also
other
costs
such
as
office
rental
taxes
and
salaries
of
staff
who
do
not
directly
contribute
to
the
design
such
as
the
janitor
and
the
CEO
After
all
of
these
expenses
the
company
should
still
make
a
profit
Example
A
BEN
TRIES
TO
MAKE
SOME
MONEY
Ben
Bitdiddle
has
designed
a
crafty
circuit
for
counting
raindrops
He
decides
to
sell
the
device
and
try
to
make
some
money
but
he
needs
help
deciding
what
implementation
to
use
He
decides
to
use
either
an
FPGA
or
an
ASIC
The
kr
ZL
Z
ZL
Z
Vi
VS
Z
Z
ZS
A
Economics
App
development
kit
to
design
and
test
the
FPGA
costs
Each
FPGA
costs
The
ASIC
costs
for
a
mask
set
and
per
chip
Regardless
of
what
chip
implementation
he
chooses
Ben
needs
to
mount
the
packaged
chip
on
a
printed
circuit
board
PCB
which
will
cost
him
per
board
He
thinks
he
can
sell
devices
per
month
Ben
has
coerced
a
team
of
bright
undergraduates
into
designing
the
chip
for
their
senior
project
so
it
doesn
t
cost
him
anything
to
design
If
the
sales
price
has
to
be
twice
the
cost
profit
margin
and
the
product
life
is
years
which
implementation
is
the
better
choice
SOLUTION
Ben
figures
out
the
total
cost
for
each
implementation
over
years
as
shown
in
Table
A
Over
years
Ben
plans
on
selling
devices
and
the
total
cost
is
given
in
Table
A
for
each
option
If
the
product
life
is
only
two
years
the
FPGA
option
is
clearly
superior
The
per
unit
cost
is
and
the
sales
price
is
per
unit
to
give
a
profit
margin
The
ASIC
option
would
have
cost
and
would
have
sold
for
per
unit
Example
A
BEN
GETS
GREEDY
After
seeing
the
marketing
ads
for
his
product
Ben
thinks
he
can
sell
even
more
chips
per
month
than
originally
expected
If
he
were
to
choose
the
ASIC
option
how
many
devices
per
month
would
he
have
to
sell
to
make
the
ASIC
option
more
profitable
than
the
FPGA
option
SOLUTION
Ben
solves
for
the
minimum
number
of
units
N
that
he
would
need
to
sell
in
years
N
N
APPENDIX
A
Digital
System
Implementation
Table
A
ASIC
vs
FPGA
costs
Cost
ASIC
FPGA
NRE
chip
PCB
TOTAL
per
unit
Appendix
A
Economics
Solving
the
equation
gives
N
units
or
units
per
month
He
would
need
to
almost
double
his
monthly
sales
to
benefit
from
the
ASIC
solution
Example
A
BEN
GETS
LESS
GREEDY
Ben
realizes
that
his
eyes
have
gotten
too
big
for
his
stomach
and
he
doesn
t
think
he
can
sell
more
than
devices
per
month
But
he
does
think
the
product
life
can
be
longer
than
years
At
a
sales
volume
of
devices
per
month
how
long
would
the
product
life
have
to
be
to
make
the
ASIC
option
worthwhile
SOLUTION
If
Ben
sells
more
than
units
in
total
the
ASIC
option
is
the
best
choice
So
Ben
would
need
to
sell
at
a
volume
of
per
month
for
at
least
months
rounding
up
which
is
almost
years
By
then
his
product
is
likely
to
be
obsolete
Chips
are
usually
purchased
from
a
distributor
rather
than
directly
from
the
manufacturer
unless
you
are
ordering
tens
of
thousands
of
units
Digikey
www
digikey
com
is
a
leading
distributor
that
sells
a
wide
variety
of
electronics
Jameco
www
jameco
com
and
All
Electronics
www
allelectronics
com
have
eclectic
catalogs
that
are
competitively
priced
and
well
suited
to
hobbyists
A
MIPS
Instructions
B
This
appendix
summarizes
MIPS
instructions
used
in
this
book
Tables
B
B
define
the
opcode
and
funct
fields
for
each
instruction
along
with
a
short
description
of
what
the
instruction
does
The
following
notations
are
used
reg
contents
of
the
register
imm
bit
immediate
field
of
the
I
type
instruction
addr
bit
address
field
of
the
J
type
instruction
SignImm
sign
extended
immediate
imm
imm
ZeroImm
zero
extended
immediate
b
imm
Address
rs
SignImm
Address
contents
of
memory
location
Address
BTA
branch
target
address
PC
SignImm
JTA
jump
target
address
PC
addr
b
The
SPIM
simulator
has
no
branch
delay
slot
so
BTA
is
PC
SignImm
Thus
if
you
use
the
SPIM
assembler
to
create
machine
code
for
a
real
MIPS
processor
you
must
decrement
the
immediate
field
by
to
compensate
Appendix
B
qxd
Table
B
Instructions
sorted
by
opcode
APPENDIX
B
MIPS
Instructions
Opcode
Name
Description
Operation
R
type
all
R
type
instructions
see
Table
B
bltz
bgez
branch
less
than
zero
if
rs
PC
BTA
rt
branch
greater
than
or
if
rs
PC
BTA
equal
to
zero
j
jump
PC
JTA
jal
jump
and
link
ra
PC
PC
JTA
beq
branch
if
equal
if
rs
rt
PC
BTA
bne
branch
if
not
equal
if
rs
rt
PC
BTA
blez
branch
if
less
than
or
equal
to
zero
if
rs
PC
BTA
bgtz
branch
if
greater
than
zero
if
rs
PC
BTA
addi
add
immediate
rt
rs
SignImm
addiu
add
immediate
unsigned
rt
rs
SignImm
slti
set
less
than
immediate
rs
SignImm
rt
rt
sltiu
set
less
than
immediate
unsigned
rs
SignImm
rt
rt
andi
and
immediate
rt
rs
ZeroImm
ori
or
immediate
rt
rs
ZeroImm
xori
xor
immediate
rt
rs
ZeroImm
lui
load
upper
immediate
rt
Imm
b
mfc
move
from
to
coprocessor
rt
rd
rd
rt
rs
mtc
rd
is
in
coprocessor
F
type
fop
F
type
instructions
see
Table
B
bc
f
bc
t
fop
branch
if
fpcond
is
if
fpcond
PC
BTA
rt
FALSE
TRUE
if
fpcond
PC
BTA
lb
load
byte
rt
SignExt
Address
lh
load
halfword
rt
SignExt
Address
lw
load
word
rt
Address
lbu
load
byte
unsigned
rt
ZeroExt
Address
lhu
load
halfword
unsigned
rt
ZeroExt
Address
sb
store
byte
Address
rt
continued
Appendix
B
qxd
PM
Page
Table
B
Instructions
sorted
by
opcode
Cont
d
Table
B
R
type
instructions
sorted
by
funct
field
Appendix
B
Funct
Name
Description
Operation
sll
shift
left
logical
rd
rt
shamt
srl
shift
right
logical
rd
rt
shamt
sra
shift
right
arithmetic
rd
rt
shamt
sllv
shift
left
logical
rd
rt
rs
variable
assembly
sllv
rd
rt
rs
srlv
shift
right
logical
rd
rt
rs
variable
assembly
srlv
rd
rt
rs
srav
shift
right
arithmetic
rd
rt
rs
variable
assembly
srav
rd
rt
rs
jr
jump
register
PC
rs
jalr
jump
and
link
register
ra
PC
PC
rs
syscall
system
call
system
call
exception
break
break
break
exception
mfhi
move
from
hi
rd
hi
mthi
move
to
hi
hi
rs
mflo
move
from
lo
rd
lo
mtlo
move
to
lo
lo
rs
mult
multiply
hi
lo
rs
rt
multu
multiply
unsigned
hi
lo
rs
rt
div
divide
lo
rs
rt
hi
rs
rt
sh
store
halfword
Address
rt
sw
store
word
Address
rt
lwc
load
word
to
FP
coprocessor
ft
Address
swc
store
word
to
FP
coprocessor
Address
ft
continued
Opcode
Name
Description
Operation
Appendix
B
qxd
APPENDIX
B
MIPS
Instructions
Table
B
R
type
instructions
sorted
by
funct
field
Cont
d
Table
B
F
type
instructions
fop
Funct
Name
Description
Operation
divu
divide
unsigned
lo
rs
rt
hi
rs
rt
add
add
rd
rs
rt
addu
add
unsigned
rd
rs
rt
sub
subtract
rd
rs
rt
subu
subtract
unsigned
rd
rs
rt
and
and
rd
rs
rt
or
or
rd
rs
rt
xor
xor
rd
rs
rt
nor
nor
rd
rs
rt
slt
set
less
than
rs
rt
rd
rd
sltu
set
less
than
unsigned
rs
rt
rd
rd
Funct
Name
Description
Operation
add
s
add
d
FP
add
fd
fs
ft
sub
s
sub
d
FP
subtract
fd
fs
ft
mul
s
mul
d
FP
multiply
fd
fs
ft
div
s
div
d
FP
divide
fd
fs
ft
abs
s
abs
d
FP
absolute
value
fd
fs
fs
fs
neg
s
neg
d
FP
negation
fd
fs
c
seq
s
c
seq
d
FP
equality
comparison
fpcond
fs
ft
c
lt
s
c
lt
d
FP
less
than
comparison
fpcond
fs
ft
c
le
s
c
le
d
FP
less
than
or
equal
comparison
fpcond
fs
ft
Appendix
B
qxd
Further
Reading
Berlin
L
The
Man
Behind
the
Microchip
Robert
Noyce
and
the
Invention
of
Silicon
Valley
Oxford
University
Press
The
fascinating
biography
of
Robert
Noyce
an
inventor
of
the
microchip
and
founder
of
Fairchild
and
Intel
For
anyone
thinking
of
working
in
Silicon
Valley
this
book
gives
insights
into
the
culture
of
the
region
a
culture
influenced
more
heavily
by
Noyce
than
any
other
individual
Colwell
R
The
Pentium
Chronicles
The
People
Passion
and
Politics
Behind
Intel
s
Landmark
Chips
Wiley
An
insider
s
tale
of
the
development
of
several
generations
of
Intel
s
Pentium
chips
told
by
one
of
the
leaders
of
the
project
For
those
considering
a
career
in
the
field
this
book
offers
views
into
the
managment
of
huge
design
projects
and
a
behind
the
scenes
look
at
one
of
the
most
significant
commercial
microprocessor
lines
Ercegovac
M
and
Lang
T
Digital
Arithmetic
Morgan
Kaufmann
The
most
complete
text
on
computer
arithmetic
systems
An
excellent
resource
for
building
high
quality
arithmetic
units
for
computers
Hennessy
J
and
Patterson
D
Computer
Architecture
A
Quantitative
Approach
th
ed
Morgan
Kaufmann
The
authoritative
text
on
advanced
computer
architecture
If
you
are
intrigued
about
the
inner
workings
of
cutting
edge
microprocessors
this
is
the
book
for
you
Kidder
T
The
Soul
of
a
New
Machine
Back
Bay
Books
A
classic
story
of
the
design
of
a
computer
system
Three
decades
later
the
story
is
still
a
page
turner
and
the
insights
on
project
managment
and
technology
still
ring
true
Pedroni
V
Circuit
Design
with
VHDL
MIT
Press
A
reference
showing
how
to
design
circuits
with
VHDL
Thomas
D
and
Moorby
P
The
Verilog
Hardware
Description
Language
th
ed
Kluwer
Academic
Publishers
Ciletti
M
Advanced
Digital
Design
with
the
Verilog
HDL
Prentice
Hall
Both
excellent
references
covering
Verilog
in
more
detail
Further
Reading
Verilog
IEEE
Standard
IEEE
STD
The
IEEE
standard
for
the
Verilog
Hardware
Description
Language
last
updated
in
Available
at
ieeexplore
ieee
org
VHDL
IEEE
Standard
IEEE
STD
The
IEEE
standard
for
VHDL
last
updated
in
Available
from
IEEE
Available
at
ieeexplore
ieee
org
Wakerly
J
Digital
Design
Principles
and
Practices
th
ed
Prentice
Hall
A
comprehensive
and
readable
text
on
digital
design
and
an
excellent
reference
book
Weste
N
and
Harris
D
CMOS
VLSI
Design
rd
ed
Addison
Wesley
Very
Large
Scale
Integration
VLSI
Design
is
the
art
and
science
of
building
chips
containing
oodles
of
transistors
This
book
coauthored
by
one
of
our
favorite
writers
spans
the
field
from
the
beginning
through
the
most
advanced
techniques
used
in
commercial
products
Index
See
also
LOW
OFF
See
also
HIGH
ON
xx
series
logic
parts
Mux
Decoder
Mux
AND
AND
AND
Counter
FLOP
NAND
NOR
NOT
OR
XOR
Register
Tristate
buffer
schematics
of
A
add
Adders
carry
lookahead
See
Carrylookahead
adder
carry
propagate
CPA
See
Carrypropagate
adder
prefix
See
Prefix
adder
ripple
carry
See
Ripple
carry
adder
add
immediate
addi
add
immediate
unsigned
addiu
add
unsigned
addu
Addition
floating
point
overflow
See
Overflow
two
s
complement
underflow
See
Underflow
unsigned
binary
Address
physical
translation
virtual
word
alignment
Addressing
modes
base
immediate
MIPS
PC
relative
pseudo
direct
register
only
Advanced
microarchitecture
branch
prediction
See
Branch
prediction
deep
pipelines
See
Deep
pipelines
multiprocessors
See
Multiprocessors
multithreading
See
Multithreading
out
of
order
processor
See
Out
of
order
processor
register
renaming
See
Register
renaming
single
instruction
multiple
data
See
Single
instruction
multiple
data
SIMD
units
superscalar
processor
See
Superscalar
processor
vector
processor
See
Single
instruction
multiple
data
SIMD
units
Alignment
See
Word
alignment
ALU
See
Arithmetic
logical
unit
ALU
decoder
ALU
decoder
truth
table
ALUControl
ALUOp
ALUOut
ALUResult
AMAT
See
Average
memory
access
time
Amdahl
Gene
Amdahl
s
Law
Anodes
and
immediate
andi
AND
gate
Application
specific
integrated
circuits
ASICs
Architectural
state
Architecture
See
Instruction
Set
Architecture
Arithmetic
See
also
Adders
Addition
Comparator
Divider
Multiplier
adders
See
Adders
addition
See
Addition
ALU
See
Arithmetic
logical
unit
circuits
comparators
See
Comparators
divider
See
Divider
division
See
Division
fixed
point
floating
point
logical
instructions
multiplier
See
Multiplier
packed
rotators
See
Rotators
shifters
See
Shifters
signed
and
unsigned
subtraction
See
Subtraction
Arithmetic
Continued
subtractor
See
Subtractor
underflow
See
Addition
Underflow
Arithmetic
logical
unit
ALU
bit
adder
See
Adders
ALUOp
ALUResult
ALUOut
comparator
See
Comparators
control
subtractor
See
Subtractor
Arrays
accessing
bytes
and
characters
FPGA
See
Field
programmable
gate
array
logic
See
Logic
arrays
memory
See
Memory
RAM
ROM
ASCII
American
Standard
Code
for
Information
Interchange
codes
table
of
Assembler
directives
ASICs
See
Application
specific
integrated
circuits
Assembly
language
MIPS
See
MIPS
assembly
language
Associativity
Average
memory
access
time
AMAT
Asynchronous
circuits
Asynchronous
inputs
Asynchronous
resettable
registers
HDL
for
Axioms
See
Boolean
axioms
B
Babbage
Charles
Base
address
Base
register
Base
number
representations
See
Binary
numbers
Base
number
representations
See
Octal
numbers
Base
number
representations
See
Hexadecimal
numbers
Block
digital
building
See
Digital
building
blocks
in
code
else
block
if
block
Base
addressing
Baudot
Jean
Maurice
Emile
Behavioral
modeling
Benchmarks
SPEC
Biased
numbers
See
also
Floating
point
Big
Endian
Binary
numbers
See
also
Arithmetic
ASCII
binary
coded
decimal
conversion
See
Number
conversion
fixed
point
floating
point
See
Floating
point
numbers
signed
sign
magnitude
See
Sign
magnitude
numbers
two
s
complement
See
Two
s
complement
numbers
unsigned
Binary
to
decimal
conversion
Binary
to
hexadecimal
conversion
Bit
dirty
least
significant
most
significant
sign
use
valid
Bit
cells
Bit
swizzling
Bitwise
operators
Boole
George
Boolean
algebra
axioms
equation
simplification
theorems
Boolean
axioms
Boolean
equations
product
of
sums
POS
canonical
form
sum
of
products
SOP
canonical
form
terminology
Boolean
theorems
DeMorgan
s
complement
consensus
covering
combining
Branching
calculating
address
conditional
prediction
target
address
BTA
unconditional
jump
Branch
equal
beq
Branch
not
equal
bne
Branch
control
hazards
See
also
Hazards
Branch
prediction
Breadboards
Bubble
pushing
Buffer
tristate
Bugs
Bus
tristate
Bypassing
Bytes
Byte
addressable
memory
Byte
order
Big
Endian
Little
Endian
C
C
programming
language
overflows
strings
Caches
See
also
Memory
accessing
advanced
design
associativity
block
size
blocks
capacity
data
placement
data
replacement
Index
definition
direct
mapped
dirty
bit
entry
fields
evolution
of
MIPS
fully
associative
hit
hit
rate
IA
systems
level
L
mapping
miss
capacity
compulsory
conflict
miss
rate
miss
rate
versus
cache
parameters
miss
penalty
multiway
set
associative
nonblocking
organizations
performance
set
associative
tag
use
bit
valid
bit
write
policy
CAD
See
Computer
aided
design
Canonical
form
Capacitors
Capacity
miss
Carry
lookahead
adder
Carry
propagate
adder
CPA
Cathodes
Cause
register
Chips
xx
series
logic
See
xx
series
logic
Circuits
xx
series
logic
See
xx
series
logic
application
specific
integrated
ASIC
arithmetic
See
Arithmetic
asynchronous
bistable
device
combinational
definition
delays
calculating
dynamic
multiple
output
pipelining
priority
synchronous
sequential
synthesized
timing
timing
analysis
types
without
glitch
CISC
See
Complex
instruction
set
computers
CLBs
See
Configurable
logic
blocks
Clock
cycle
See
Clock
period
Clock
period
Clock
rate
See
Clock
period
Clock
cycles
per
instruction
CPI
Clustered
computers
Clock
skew
CMOS
See
Complementary
MetalOxide
Semiconductor
Logic
Code
Code
size
Combinational
composition
Combinational
logic
design
Boolean
algebra
Boolean
equations
building
blocks
delays
don
t
cares
HDLs
and
See
Hardware
description
languages
Karnaugh
maps
logic
multilevel
overview
precedence
timing
two
level
X
s
contention
See
Contention
X
s
don
t
cares
See
Don
t
cares
Z
s
floating
See
Floating
Comparators
equality
magnitude
Compiler
Complementary
Metal
OxideSemiconductor
Logic
CMOS
bubble
pushing
logic
gates
NAND
gate
NOR
gate
NOT
gate
transistors
Complex
instruction
set
computers
CISC
Compulsory
miss
Computer
aided
design
CAD
Computer
Organization
and
Design
Patterson
and
Hennessy
Complexity
management
abstraction
discipline
hierarchy
modularity
regularity
Conditional
assignment
Conditional
branches
Condition
codes
Conflict
misses
Conditional
statements
Constants
See
also
Immediates
Contamination
delay
Contention
X
Context
switch
Control
signals
Control
unit
multicycle
MIPS
processor
FSM
pipelined
MIPS
processor
single
cycle
MIPS
processor
Configurable
logic
blocks
CLBs
Control
hazards
See
Branch
control
hazards
Pipelining
Coprocessor
Counters
Covalent
bond
CPA
See
Carry
propagate
adder
CPI
See
Clock
cycles
per
instruction
Critical
path
Cyclic
paths
Cycle
time
See
Clock
Period
D
Data
hazards
See
Hazards
Data
memory
Data
sheets
Datapath
See
also
MIPS
microprocessors
Index
Datapath
Continued
elements
multicycle
MIPS
processor
pipelined
MIPS
processor
single
cycle
MIPS
processor
Data
segments
See
Memory
map
Data
types
See
Hardware
description
languages
DC
See
Direct
current
Decimal
numbers
conversion
to
binary
and
hexadecimal
See
Number
conversion
scientific
notation
Decimal
to
Binary
conversion
Decimal
to
hexadecimal
conversion
Decoders
implementation
logic
parameterized
Deep
pipelines
Delay
DeMorgan
Augustus
DeMorgan
s
theorem
Dennard
Robert
Destination
register
Device
under
test
DUT
See
also
Unit
under
test
Device
driver
Dice
Digital
abstraction
DC
transfer
characteristics
logic
levels
noise
margins
supply
voltage
Digital
design
abstraction
discipline
hierarchy
modularity
regularity
Digital
system
implementation
xx
series
logic
See
xx
series
logic
application
specific
integrated
circuits
ASICs
data
sheets
economics
logic
families
overview
packaging
and
assembly
breadboards
packages
printed
circuit
boards
programmable
logic
transmission
lines
See
Transmission
lines
Diodes
DIP
See
Dual
inline
package
Direct
current
DC
transfer
characteristics
Direct
mapped
cache
Dirty
bit
Discipline
Disk
See
Hard
disk
divide
div
divide
unsigned
divu
Divider
Division
floating
point
instructions
Divisor
Don
t
care
X
Dopant
atoms
Double
See
Double
precision
floatingpoint
numbers
Double
precision
floating
point
numbers
DRAM
See
Dynamic
random
access
memory
Driver
See
Device
driver
Dual
inline
package
DIP
DUT
See
Device
under
test
Dynamic
discipline
Dynamic
data
segment
Dynamic
random
access
memory
DRAM
E
Edison
Thomas
Edge
triggered
digital
systems
Equations
simplification
Electrically
erasable
programmable
read
only
memory
EEPROM
Enabled
registers
HDL
for
EPC
See
Exception
Program
Counter
register
Erasable
programmable
read
only
memory
EPROM
Exceptions
Exception
handler
Exception
program
counter
EPC
Exclusive
or
See
XOR
Executable
file
Execution
time
Exponent
F
Failures
FDIV
bug
FET
See
Field
effect
transistors
Field
programmable
gate
array
FPGA
Field
effect
transistors
FIFO
See
First
in
first
out
queue
See
also
Queue
Finite
state
machines
FSMs
design
example
divide
by
factoring
HDLs
and
Mealy
machines
See
Mealy
machines
Moore
machines
See
Moore
machines
Moore
versus
Mealy
machines
state
encodings
state
transition
diagram
First
in
first
out
FIFO
queue
Fixed
point
numbers
Flash
memory
Flip
flops
See
also
Registers
comparison
with
latches
D
enabled
register
resettable
asynchronous
synchronous
transistor
count
transistor
level
Floating
Z
Floating
point
numbers
Index
addition
See
also
Addition
converting
binary
or
decimal
to
See
Number
conversions
division
See
also
Division
double
precision
FDIV
bug
See
FDIV
bug
floating
point
unit
FPU
instructions
rounding
single
precision
special
cases
infinity
not
a
number
NaN
Forwarding
Fractional
numbers
FPGA
See
Field
programmable
gate
array
Frequency
See
also
Clock
period
FSM
See
Finite
state
machines
Full
adder
See
also
Adder
Addition
HDL
using
always
process
using
nonblocking
assignments
Fully
associative
cache
Funct
field
Functional
specification
Functions
See
Procedure
calls
Fuse
G
Gates
AND
NAND
NOR
OR
transistor
level
implementation
XOR
XNOR
Gedanken
Experiments
on
Sequential
Machines
Moore
Generate
signal
Glitches
Global
pointer
gp
See
also
Static
data
segment
Gray
Frank
Gray
codes
Gulliver
s
Travels
Swift
H
Half
word
Half
adder
Hard
disk
Hard
drive
See
Hard
disk
Hardware
reduction
Hardware
description
languages
HDLs
assignment
statements
behavioral
modeling
combinational
logic
bit
swizzling
bitwise
operators
blocking
and
nonblocking
assignments
case
statements
conditional
assignment
delays
if
statements
internal
variables
numbers
precedence
reduction
operators
synthesis
tools
See
Synthesis
Tools
Verilog
VHDL
libraries
and
types
Z
s
and
X
s
finite
state
machines
generate
statement
generic
building
blocks
invalid
logic
level
language
origins
modules
origins
of
overview
parameterized
modules
representation
sequential
logic
enabled
registers
latches
multiple
registers
registers
See
also
Registers
resettable
registers
simulation
and
synthesis
single
cycle
processor
structural
modeling
testbenches
Hazards
See
also
Glitches
control
hazards
data
hazards
solving
forwarding
stalls
WAR
See
Write
after
read
WAW
See
Write
after
write
Hazard
unit
Heap
HDLs
See
Hardware
description
languages
Hennessy
John
Hexadecimal
numbers
to
binary
conversion
table
Hierarchy
HIGH
See
also
ON
High
level
programming
languages
translating
into
assembly
compiling
linking
and
launching
Hit
High
impedance
See
Floating
Z
High
Z
See
Floating
High
impedance
Z
Hold
time
I
I
type
instruction
IA
microprocessor
branch
conditions
cache
systems
encoding
evolution
instructions
memory
and
input
output
I
O
systems
operands
programmed
I
O
registers
status
flags
virtual
memory
IEEE
floating
point
standard
Idempotency
Idioms
IEEE
Index
If
statements
If
else
statements
ILP
See
Instruction
level
parallelism
Immediates
Immediate
addressing
Information
amount
of
IorD
I
O
input
output
communicating
with
device
driver
devices
See
also
Peripheral
devices
memory
interface
memory
mapped
I
O
Inputs
asynchronous
Instruction
encoding
See
Machine
Language
Instruction
register
IR
Instruction
set
architecture
ISA
See
also
MIPS
instruction
set
architecture
Input
output
blocks
IOBs
Input
terminals
Institute
of
Electrical
and
Electronics
Engineers
Instruction
decode
Instruction
encoding
See
Instruction
format
Instruction
format
F
type
I
type
J
type
R
type
Instruction
level
parallelism
ILP
Instruction
memory
Instruction
set
See
Instruction
set
architecture
Instructions
See
also
Language
arithmetic
logical
floating
point
IA
I
type
J
type
loads
See
Loads
multiplication
and
division
pseudoinstructions
R
type
set
less
than
shift
signed
and
unsigned
Intel
Inverter
See
NOT
gate
Integrated
circuits
ICs
costs
manufacturing
process
Intel
See
IA
microprocessors
Interrupts
See
Exceptions
An
Investigation
of
the
Laws
of
Thought
Boole
Involution
IOBs
See
Input
output
blocks
I
type
instructions
J
Java
See
also
Language
JTA
See
Jump
target
address
J
type
instructions
Jump
See
also
Branch
unconditional
Programming
Jump
target
address
JTA
K
K
maps
See
Karnaugh
maps
Karnaugh
Maurice
Karnaugh
maps
K
maps
logic
minimization
using
prime
implicants
seven
segment
display
decoder
with
don
t
cares
without
glitches
Kilby
Jack
Kilobyte
K
maps
See
Karnaugh
maps
L
Labels
Language
See
also
Instructions
assembly
high
level
machine
mnemonic
translating
assembly
to
machine
Last
in
first
out
LIFO
queue
See
also
Stack
Queue
Latches
comparison
with
flip
flops
D
SR
transistor
level
Latency
Lattice
Leaf
procedures
Least
recently
used
LRU
replacement
Least
significant
bit
lsb
Least
significant
byte
LSB
LIFO
See
Last
in
first
out
queue
Literal
Little
Endian
Load
byte
lb
byte
unsigned
lbu
half
lh
immediate
li
upper
immediate
lui
word
lw
Loading
Locality
Local
variables
Logic
See
also
Multilevel
combinational
logic
Sequential
logic
design
bubble
pushing
See
Bubble
pushing
combinational
See
Combinational
logic
families
gates
See
Logic
gates
hardware
reduction
See
Hardware
reduction
Equation
simplification
multilevel
See
Multilevel
combinational
logic
programmable
sequential
See
Sequential
logic
synthesis
two
level
using
memory
arrays
See
also
Logic
arrays
Logic
arrays
field
programmable
gate
array
programmable
logic
array
transistor
level
implementations
Logic
families
Index
compatibility
specifications
Logic
gates
buffer
delays
multiple
input
gates
two
input
gates
types
AND
See
AND
gate
AOI
and
or
invert
See
And
or
invert
gate
NAND
See
NAND
gate
NOR
See
NOR
gate
NOT
See
NOT
gate
OAI
or
and
invert
See
Or
and
invert
gate
OR
See
OR
gate
XOR
See
XOR
gate
XNOR
See
XNOR
gate
Logic
levels
Logical
operations
Lookup
tables
LUTs
Loops
for
while
LOW
See
also
OFF
Low
Voltage
CMOS
Logic
LVCMOS
Low
Voltage
TTL
Logic
LVTTL
LRU
See
Least
recently
used
replacement
LSB
See
Least
significant
byte
LUTs
See
Lookup
tables
LVCMOS
See
Low
Voltage
CMOS
Logic
LVTTL
See
Low
Voltage
TTL
Logic
M
Machine
language
function
fields
interpreting
code
I
type
instructions
J
type
instructions
opcodes
R
type
instructions
stored
programs
translating
from
assembly
language
translating
into
assembly
language
Main
decoder
Main
memory
Mapping
Mantissa
See
also
Floating
point
numbers
Masuoka
Fujio
MCM
See
Multichip
module
Mealy
George
H
Mealy
machines
combined
state
transition
and
output
table
state
transition
diagram
timing
diagram
Mean
time
between
failures
MTBF
Memory
access
average
memory
access
time
AMAT
cache
See
Caches
DRAM
See
Dynamic
random
access
memory
hierarchy
interface
main
map
dynamic
data
segment
global
data
segment
reserved
segment
text
segment
nonvolatile
performance
physical
protection
See
also
Virtual
memory
RAM
ROM
separate
data
and
instruction
shared
stack
See
Stack
types
flip
flops
latches
DRAM
registers
register
file
SRAM
virtual
See
also
Virtual
memory
volatile
word
addressable
See
Wordaddressable
memory
Memory
arrays
area
bit
cells
delay
DRAM
See
Dynamic
random
access
memory
HDL
code
for
logic
implementation
using
See
also
Logic
arrays
organization
overview
ports
register
files
built
using
types
DRAM
See
Dynamic
random
access
memory
ROM
See
Read
only
memory
SRAM
See
Static
random
access
memory
Memory
mapped
I
O
input
output
address
decoder
communicating
with
I
O
devices
hardware
speech
synthesizer
device
driver
speech
synthesizer
hardware
SP
Memory
protection
Memory
systems
caches
See
Caches
IA
MIPS
overview
performance
analysis
virtual
memory
See
Virtual
memory
Mercedes
Benz
Metal
oxide
semiconductor
field
effect
transistors
MOSFETs
See
also
CMOS
nMOS
pMOS
transistors
Metastability
MTBF
See
Mean
time
between
failures
metastable
state
probability
of
failure
resolution
time
synchronizers
A
Method
of
Synthesizing
Sequential
Circuits
Mealy
Index
Microarchitecture
advanced
See
Advanced
microarchitecture
architectural
state
See
Architectural
State
See
also
Architecture
design
process
exception
handling
HDL
representation
IA
See
IA
microprocessor
instruction
set
See
Instruction
set
MIPS
See
MIPS
microprocessor
overview
performance
analysis
types
advanced
See
Advanced
microarchitecture
multicycle
See
Multicycle
MIPS
processor
pipelined
See
Pipelined
MIPS
processor
single
cycle
See
Single
cycle
MIPS
processor
Microprocessors
advanced
See
Advanced
microarchitecture
chips
See
Chips
clock
frequencies
IA
See
IA
microprocessor
instructions
See
MIPS
instructions
IA
instructions
MIPS
See
MIPS
microprocessor
Microsoft
Windows
Minterms
Maxterms
MIPS
Millions
of
instructions
per
second
See
Millions
of
instructions
per
second
MIPS
architecture
See
MIPS
instruction
set
architecture
ISA
MIPS
assembly
language
See
also
MIPS
instruction
set
architecture
addressing
modes
assembler
directives
instructions
logical
instructions
mnemonic
operands
procedure
calls
table
of
instructions
translating
machine
language
to
translating
to
machine
language
MIPS
instruction
set
architecture
ISA
addressing
modes
assembly
language
compiling
assembling
and
loading
exceptions
floating
point
instructions
IA
instructions
machine
language
MIPS
instructions
overview
programming
pseudoinstructions
signed
and
unsigned
instructions
SPARC
translating
and
starting
a
program
See
Translating
and
starting
a
program
MIPS
instructions
formats
F
type
I
type
J
type
R
type
tables
of
opcodes
R
type
funct
fields
types
arithmetic
branching
See
Branching
division
floating
point
logical
multiplication
pseudoinstructions
MIPS
microprocessor
ALU
multicycle
See
Multicycle
MIPS
processor
pipelined
See
Pipelined
MIPS
processor
single
cycle
See
Single
cycle
MIPS
processor
MIPS
processor
See
MIPS
microprocessor
MIPS
registers
nonpreserved
preserved
table
of
MIPS
single
cycle
HDL
implementation
building
blocks
controller
datapath
testbench
top
level
module
Misses
AMAT
See
Average
memory
access
time
cache
capacity
compulsory
conflict
page
fault
Miss
penalty
Miss
rate
Mnemonic
Modeling
structural
See
Structural
modeling
Modeling
behavioral
See
Behavioral
modeling
Modularity
Modules
in
HDL
See
also
Hardware
description
languages
behavioral
parameterized
structural
Moore
Edward
F
Moore
Gordon
Moore
machine
output
table
state
transition
diagram
state
transition
table
timing
diagram
Moore
s
law
MOSFETs
See
Metal
oxide
semiconductor
field
effect
transistors
Most
significant
bit
msb
Most
significant
byte
MSB
Move
from
hi
mfhi
Move
from
lo
mflo
Move
from
coprocessor
mfc
msb
See
Most
significant
bit
MSB
See
Most
significant
byte
MTBF
See
Mean
time
before
failure
Multichip
module
MCM
Multicycle
MIPS
processor
control
control
FSM
datapath
performance
analysis
Multilevel
combinational
logic
See
also
Logic
Multilevel
page
table
Multiplexers
Index
instance
logic
parameterized
See
also
Hardware
description
languages
symbol
and
truth
table
timing
with
type
conversion
wide
Multiplicand
Multiplication
See
also
Arithmetic
Multiplier
architecture
instructions
Multiplier
multiply
mult
multiply
unsigned
multu
Multiprocessors
chip
Multithreading
Mux
See
Multiplexers
N
NaN
See
Not
a
number
Negation
See
also
Taking
the
two
s
complement
Not
a
number
NaN
NAND
gate
nor
NOR
gate
NOT
gate
See
also
Inverter
in
HDL
using
always
process
Nested
procedure
calls
Netlist
Next
state
Nibbles
nMOS
nMOS
transistors
No
operation
See
nop
Noise
margins
nop
Noyce
Robert
Number
conversion
See
also
Number
systems
Binary
numbers
binary
to
decimal
binary
to
hexadecimal
decimal
to
binary
decimal
to
hexadecimal
taking
the
two
s
complement
Number
systems
addition
See
Addition
binary
numbers
See
Binary
numbers
comparison
of
conversion
of
See
Number
conversions
decimal
numbers
See
Decimal
numbers
estimating
powers
of
two
fixed
point
See
Fixedpoint
numbers
floating
point
See
Floating
point
numbers
in
hardware
description
languages
hexadecimal
numbers
See
Hexadecimal
numbers
negative
and
positive
rounding
See
also
Floatingpoint
numbers
sign
bit
signed
See
also
Signed
binary
numbers
sign
magnitude
numbers
See
Sign
magnitude
numbers
two
s
complement
numbers
See
Two
s
complement
numbers
unsigned
O
OAI
gate
See
Or
and
invert
gate
Object
files
Octal
numbers
OFF
See
also
LOW
Offset
ON
See
also
HIGH
Asserted
One
cold
One
hot
Opcode
Operands
Operators
bitwise
or
immediate
ori
OR
gate
Or
and
invert
OAI
gate
precedence
table
of
reduction
Out
of
order
execution
Out
of
order
processor
Output
devices
Output
terminals
Overflow
detecting
with
addition
P
Page
size
Page
fault
Page
offset
Page
table
Parity
Parallelism
pipelining
See
Pipelining
Pipelined
MIPS
processor
SIMD
See
Single
instruction
multiple
data
unit
spatial
and
temporal
vector
processor
See
Vector
processor
Patterson
David
PCBs
See
Printed
circuit
boards
PC
relative
addressing
See
also
Addressing
modes
PCSrc
PCWrite
Pentium
processors
See
also
Intel
IA
Pentium
Pentium
II
Pentium
III
Pentium
M
Pentium
Pro
Perfect
induction
Performance
Peripheral
devices
See
I
O
devices
Perl
programming
language
Physical
memory
Physical
pages
Physical
page
number
Pipelined
MIPS
processor
See
also
MIPS
Architecture
Microarchitecture
control
datapath
forwarding
hazards
See
Hazards
performance
analysis
processor
performance
comparison
stalls
See
also
Hazards
timing
Index
Pipelining
hazards
See
Hazards
PLAs
See
Programmable
logic
arrays
Plastic
leaded
chip
carriers
PLCCs
PLCCs
See
Plastic
leaded
chip
carriers
PLDs
See
Programmable
logic
devices
pMOS
pMOS
transistors
Pointer
global
stack
Pop
See
also
Stack
Ports
POS
See
Product
of
sums
form
Power
consumption
Prediction
See
Branch
prediction
Prefix
adder
Preserved
registers
Prime
implicants
Printed
circuit
boards
PCBs
Procedure
calls
arguments
and
variables
nested
preserved
versus
nonpreserved
registers
returns
return
values
stack
frame
stack
usage
Processors
See
Microprocessors
Product
of
sums
POS
canonical
form
Program
counter
PC
Programmable
logic
arrays
PLAs
Programmable
logic
devices
PLDs
Programmable
read
only
memories
PROMs
See
also
Read
only
memories
Programming
arithmetic
logical
instructions
See
Arithmetic
arrays
See
Arrays
branching
See
Branching
conditional
statements
See
Conditional
statements
constants
immediates
loops
See
Loops
procedure
calls
See
Procedure
calls
shift
instructions
translating
and
starting
a
program
Programming
languages
Propagation
delay
Protection
memory
See
Memory
protection
Proving
Boolean
theorems
See
Perfect
induction
PROMs
See
Programmable
read
only
memories
Pseudo
direct
addressing
See
also
Addressing
modes
Pseudo
nMOS
logic
See
also
Transistors
Pseudoinstructions
Push
See
also
Stack
Q
Queue
FIFO
See
First
in
first
out
queue
LIFO
See
Last
in
first
out
queue
Q
output
See
Sequential
logic
design
R
R
type
instructions
RAM
See
Random
access
memory
Random
access
memory
RAM
See
also
Memory
arrays
synthesized
Read
only
memory
See
also
Memory
arrays
EPROM
See
Erasable
programmable
read
only
memory
EEPROM
See
Electrically
erasable
programmable
read
only
memory
flash
memory
See
Flash
memory
PROM
See
Programmable
read
only
memory
transistor
level
implementation
Read
write
head
Recursive
procedure
calls
See
also
Procedure
calls
Reduced
instruction
set
computer
RISC
Reduction
operators
See
also
Hardware
description
languages
Verilog
RegDst
Register
only
addressing
See
also
Addressing
modes
Register
renaming
See
also
Advanced
microarchitecture
Register
s
See
also
Flip
flops
Register
file
arguments
a
a
assembler
temporary
at
enabled
See
Enabled
registers
file
global
pointer
gp
multiple
program
counter
PC
preserved
and
nonpreserved
renaming
resettable
See
Resettable
registers
asynchronous
synchronous
return
address
ra
Register
set
See
also
Register
file
MIPS
registers
IA
registers
Regularity
RegWrite
Replacement
policies
See
also
Caches
Virtual
memory
Resettable
registers
asynchronous
See
Asynchronous
resettable
registers
synchronous
See
Synchronous
resettable
registers
Return
address
ra
Ripple
carry
adder
RISC
See
Reduced
instruction
set
computer
ROM
See
Read
Only
Memory
Rotators
Rounding
S
Scalar
processor
Scan
chains
See
also
Shift
registers
Schematic
Scientific
notation
Index
Seek
time
Segments
memory
Semiconductor
industry
sales
Semiconductors
See
also
Transistors
CMOS
See
Complementary
metal
oxide
silicon
diodes
See
Diodes
transistors
See
Transistors
MOSFET
See
Metal
oxide
silicon
field
effect
transistors
nMOS
See
nMOS
transistors
pMOS
See
pMOS
transistors
pseudo
nMOS
See
Pseudo
nMOS
Sensitivity
list
Sequential
building
blocks
See
Sequential
logic
Sequential
logic
enabled
registers
latches
multiple
registers
overview
registers
shift
registers
synchronous
timing
of
See
also
Timing
set
if
less
than
slt
set
if
less
than
immediate
slti
set
if
less
than
immediate
unsigned
sltiu
set
if
less
than
unsigned
sltu
Setup
time
Seven
segment
display
decoder
with
don
t
cares
Shared
memory
Shift
amount
shamt
shift
left
logical
sll
shift
left
logical
variable
sllv
shift
right
arithmetic
sra
shift
right
arithmetic
variable
srav
shift
right
logical
srl
shift
right
logical
variable
srlv
Shifters
arithmetic
logical
Shift
instructions
Shift
registers
Short
path
Sign
magnitude
numbers
Sign
bit
Sign
extension
Significand
See
Mantissa
Silicon
Si
Silicon
dioxide
SO
Silicon
Valley
Simplicity
SIMD
See
Single
instruction
multiple
data
units
Single
cycle
MIPS
processor
See
also
MIPS
microprocessor
MIPS
architecture
MIPS
microarchitecture
control
ALU
decoder
truth
table
See
ALU
decoder
Main
decoder
truth
table
See
Main
decoder
datapath
HDL
representation
operation
performance
analysis
timing
Single
instruction
multiple
data
SIMD
units
Slash
notation
SPARC
architecture
SRAM
See
Static
random
access
memory
SP
Spatial
locality
Speech
synthesis
device
driver
SP
Stack
See
also
Memory
map
Procedure
calls
Queue
dynamic
data
segment
frame
LIFO
See
Last
in
first
out
queue
pointer
Stalls
See
also
Hazards
Static
discipline
Static
random
access
memory
SRAM
Status
flags
Stored
program
concept
Stores
store
byte
sb
store
half
sh
store
word
sw
Strings
Structural
modeling
subtract
sub
subtract
unsigned
subu
Subtraction
Subtractor
Sum
of
products
SOP
canonical
form
Sun
Microsystems
Superscalar
processor
Supply
voltage
Swap
space
Swift
Jonathan
Switch
case
statements
Symbol
table
Synchronizers
asynchronous
inputs
MTBF
See
Mean
time
before
failure
probability
of
failure
See
Probability
of
failure
Synchronous
resettable
registers
HDL
for
Synchronous
sequential
logic
problematic
circuits
Synthesis
Synthesis
Tools
T
Taking
the
two
s
complement
Temporal
locality
Testbenches
self
checking
with
test
vector
file
Text
segment
Theorems
See
Boolean
Theorems
Thin
small
outline
package
TSOP
Threshold
voltage
Throughput
Timing
analysis
delay
glitches
of
sequential
logic
clock
skew
dynamic
discipline
hold
time
See
Hold
time
hold
time
constraint
See
Hold
time
constraint
Hold
time
violation
hold
time
violation
See
Hold
time
violation
Hold
time
constraint
Index
Timing
Continued
metastability
setup
time
See
Setup
time
setup
time
constraint
setup
time
violations
resolution
time
See
also
Metastability
synchronizers
system
timing
specification
TLB
See
Translation
lookaside
buffer
Token
Transistors
CMOS
See
Complement
metal
oxide
silicon
nMOS
See
nMOS
pMOS
See
pMOS
Transistor
Transistor
Logic
TTL
Translation
lookaside
buffer
TLB
Translating
and
starting
a
program
Transmission
gates
See
also
Transistors
Transmission
lines
reflection
coefficient
Z
matched
termination
mismatched
termination
open
termination
proper
terminations
short
termination
when
to
use
Tristate
buffer
Truth
tables
ALU
decoder
don
t
care
main
decoder
multiplexer
seven
segment
display
decoder
SR
latch
with
undefined
and
floating
inputs
TSOP
See
Thin
small
outline
package
TTL
See
Transistor
Transistor
Logic
Two
level
logic
Two
s
complement
numbers
See
also
Binary
numbers
U
Unconditional
branches
See
also
Jumps
Unicode
See
also
ASCII
Unit
under
test
UUT
Unity
gain
points
Unsigned
numbers
Use
bit
UUT
See
Unit
under
test
V
Valid
bit
See
also
Caches
Virtual
memory
Vanity
Fair
Carroll
VCC
VDD
Vector
processors
See
also
Advanced
microarchitecture
Verilog
decoder
accessing
parts
of
busses
adder
ALU
decoder
AND
architecture
body
assign
statement
asynchronous
reset
bad
synchronizer
with
blocking
assignments
bit
swizzling
blocking
assignment
case
sensitivity
casez
combinational
logic
comments
comparators
continuous
assignment
statement
controller
counter
datapath
default
divide
by
finite
state
machine
D
latch
eight
input
AND
entity
declaration
full
adder
using
always
process
using
nonblocking
assignments
IEEE
STD
LOGIC
IEEE
STD
LOGIC
SIGNED
IEEE
STD
LOGIC
UNSIGNED
inverters
using
always
process
left
shift
library
use
clause
logic
gates
with
delays
main
decoder
MIPS
testbench
MIPS
top
level
module
multiplexers
multiplier
nonblocking
assignment
NOT
numbers
operator
precedence
OR
parameterized
N
N
decoder
N
bit
multiplexer
N
input
AND
gate
pattern
recognizer
Mealy
FSM
Moore
FSM
priority
circuit
RAM
reg
register
register
file
resettable
flip
flop
resettable
enabled
register
resettable
register
ROM
self
checking
testbench
seven
segment
display
decoder
shift
register
sign
extension
single
cycle
MIPS
processor
STD
LOGIC
statement
structural
models
Index
multiplexer
multiplexer
subtractor
synchronizer
synchronous
reset
testbench
with
test
vector
file
tristate
buffer
truth
tables
with
undefined
and
floating
inputs
type
declaration
wires
VHDL
libraries
and
types
decoder
accessing
parts
of
busses
adder
architecture
asynchronous
reset
bad
synchronizer
with
blocking
assignments
bit
swizzling
boolean
case
sensitivity
clk
event
combinational
logic
comments
comparators
concurrent
signal
assignment
controller
CONV
STD
LOGIC
VECTOR
counter
datapath
decoder
divide
by
finite
state
machine
D
latch
eight
input
AND
expression
full
adder
using
always
process
using
nonblocking
assignments
generic
statement
inverters
using
always
process
left
shift
logic
gates
with
delays
main
decoder
MIPS
testbench
MIPS
top
level
module
multiplexers
multiplier
numbers
operand
operator
precedence
others
parameterized
N
bit
multiplexer
N
input
AND
gate
N
N
decoder
pattern
recognizer
Mealy
FSM
Moore
FSM
priority
circuit
process
RAM
register
register
file
resettable
enabled
register
resettable
flip
flop
resettable
register
RISING
EDGE
ROM
selected
signal
assignment
statements
self
checking
testbench
seven
segment
display
decoder
shift
register
sign
extension
signals
simulation
waveforms
with
delays
single
cycle
MIPS
processor
STD
LOGIC
ARITH
structural
models
multiplexer
multiplexer
subtractor
synchronizer
synchronous
reset
testbench
with
test
vector
file
tristate
buffer
truth
tables
with
undefined
and
floating
inputs
VHSIC
Virtual
address
Virtual
memory
address
translation
IA
memory
protection
pages
page
faults
page
offset
page
table
multilevel
page
tables
replacement
policies
translation
lookaside
buffer
TLB
See
Translation
lookaside
buffer
write
policies
Virtual
pages
Virtual
page
number
Volatile
memory
See
also
DRAM
SRAM
Flip
flops
Voltage
threshold
VSS
W
Wafers
Wall
Larry
WAR
See
Write
after
read
WAW
See
Write
after
write
While
loop
White
space
Whitmore
Georgiana
Wire
Word
addressable
memory
Write
policies
Write
after
read
WAR
hazard
See
also
Hazards
Write
after
write
WAW
hazard
See
also
Hazards
X
XOR
gate
XNOR
gate
Xilinx
FPGA
X
See
Contention
Don
t
care
Z
Z
See
Floating
Zero
extension
Index
In
Praise
of
Digital
Design
and
Computer
Architecture
Harris
and
Harris
have
taken
the
popular
pedagogy
from
Computer
Organization
and
Design
to
the
next
level
of
refinement
showing
in
detail
how
to
build
a
MIPS
microprocessor
in
both
Verilog
and
VHDL
Given
the
exciting
opportunity
that
students
have
to
run
large
digital
designs
on
modern
FGPAs
the
approach
the
authors
take
in
this
book
is
both
informative
and
enlightening
David
A
Patterson
University
of
California
Berkeley
Digital
Design
and
Computer
Architecture
brings
a
fresh
perspective
to
an
old
discipline
Many
textbooks
tend
to
resemble
overgrown
shrubs
but
Harris
and
Harris
have
managed
to
prune
away
the
deadwood
while
preserving
the
fundamentals
and
presenting
them
in
a
contemporary
context
In
doing
so
they
offer
a
text
that
will
benefit
students
interested
in
designing
solutions
for
tomorrow
s
challenges
Jim
Frenzel
University
of
Idaho
Harris
and
Harris
have
a
pleasant
and
informative
writing
style
Their
treatment
of
the
material
is
at
a
good
level
for
introducing
students
to
computer
engineering
with
plenty
of
helpful
diagrams
Combinational
circuits
microarchitecture
and
memory
systems
are
handled
particularly
well
James
Pinter
Lucke
Claremont
McKenna
College
Harris
and
Harris
have
written
a
book
that
is
very
clear
and
easy
to
understand
The
exercises
are
well
designed
and
the
real
world
examples
are
a
nice
touch
The
lengthy
and
confusing
explanations
often
found
in
similar
textbooks
are
not
seen
here
It
s
obvious
that
the
authors
have
devoted
a
great
deal
of
time
and
effort
to
create
an
accessible
text
I
strongly
recommend
Digital
Design
and
Computer
Architecture
Peiyi
Zhao
Chapman
University
Harris
and
Harris
have
created
the
first
book
that
successfully
combines
digital
system
design
with
computer
architecture
Digital
Design
and
Computer
Architecture
is
a
much
welcomed
text
that
extensively
explores
digital
systems
designs
and
explains
the
MIPS
architecture
in
fantastic
detail
I
highly
recommend
this
book
James
E
Stine
Jr
Oklahoma
State
University
Digital
Design
and
Computer
Architecture
is
a
brilliant
book
Harris
and
Harris
seamlessly
tie
together
all
the
important
elements
in
microprocessor
design
transistors
circuits
logic
gates
finite
state
machines
memories
arithmetic
units
and
conclude
with
computer
architecture
This
text
is
an
excellent
guide
for
understanding
how
complex
systems
can
be
flawlessly
designed
Jaeha
Kim
Rambus
Inc
Digital
Design
and
Computer
Architecture
is
a
very
well
written
book
that
will
appeal
to
both
young
engineers
who
are
learning
these
subjects
for
the
first
time
and
also
to
the
experienced
engineers
who
want
to
use
this
book
as
a
reference
I
highly
recommend
it
A
Utku
Diril
Nvidia
Corporation
Digital
Design
and
Computer
Architecture
About
the
Authors
David
Money
Harris
is
an
associate
professor
of
engineering
at
Harvey
Mudd
College
He
received
his
Ph
D
in
electrical
engineering
from
Stanford
University
and
his
M
Eng
in
electrical
engineering
and
computer
science
from
MIT
Before
attending
Stanford
he
worked
at
Intel
as
a
logic
and
circuit
designer
on
the
Itanium
and
Pentium
II
processors
Since
then
he
has
consulted
at
Sun
Microsystems
Hewlett
Packard
Evans
Sutherland
and
other
design
companies
David
s
passions
include
teaching
building
chips
and
exploring
the
outdoors
When
he
is
not
at
work
he
can
usually
be
found
hiking
mountaineering
or
rock
climbing
He
particularly
enjoys
hiking
with
his
son
Abraham
who
was
born
at
the
start
of
this
book
project
David
holds
about
a
dozen
patents
and
is
the
author
of
three
other
textbooks
on
chip
design
as
well
as
two
guidebooks
to
the
Southern
California
mountains
Sarah
L
Harris
is
an
assistant
professor
of
engineering
at
Harvey
Mudd
College
She
received
her
Ph
D
and
M
S
in
electrical
engineering
from
Stanford
University
Before
attending
Stanford
she
received
a
B
S
in
electrical
and
computer
engineering
from
Brigham
Young
University
Sarah
has
also
worked
with
Hewlett
Packard
the
San
Diego
Supercomputer
Center
Nvidia
and
Microsoft
Research
in
Beijing
Sarah
loves
teaching
exploring
and
developing
new
technologies
traveling
wind
surfing
rock
climbing
and
playing
the
guitar
Her
recent
exploits
include
researching
sketching
interfaces
for
digital
circuit
design
acting
as
a
science
correspondent
for
a
National
Public
Radio
affiliate
and
learning
how
to
kite
surf
She
speaks
four
languages
and
looks
forward
to
adding
a
few
more
to
the
list
in
the
near
future
Digital
Design
and
Computer
Architecture
David
Money
Harris
Sarah
L
Harris
AMSTERDAM
BOSTON
HEIDELBERG
LONDON
NEW
YORK
OXFORD
PARIS
SAN
DIEGO
SAN
FRANCISCO
SINGAPORE
SYDNEY
TOKYO
Morgan
Kaufmann
Publishers
is
an
imprint
of
Elsevier
Publisher
Denise
E
M
Penrose
Senior
Developmental
Editor
Nate
McFadden
Publishing
Services
Manager
George
Morrison
Project
Manager
Marilyn
E
Rash
Assistant
Editor
Mary
E
James
Editorial
Assistant
Kimberlee
Honjo
Cover
and
Editorial
Illustrations
Duane
Bibby
Interior
Design
Frances
Baca
Design
Composition
Integra
Technical
Illustrations
Harris
and
Harris
Integra
Production
Services
Graphic
World
Inc
Interior
Printer
Courier
Westford
Cover
Printer
Phoenix
Color
Corp
Morgan
Kaufmann
Publishers
is
an
imprint
of
Elsevier
Sansome
Street
Suite
San
Francisco
CA
This
book
is
printed
on
acid
free
paper
by
Elsevier
Inc
All
rights
reserved
Designations
used
by
companies
to
distinguish
their
products
are
often
claimed
as
trademarks
or
registered
trademarks
In
all
instances
in
which
Morgan
Kaufmann
Publishers
is
aware
of
a
claim
the
product
names
appear
in
initial
capital
or
all
capital
letters
Readers
however
should
contact
the
appropriate
companies
for
more
complete
information
regarding
trademarks
and
registration
Permissions
may
be
sought
directly
from
Elsevier
s
Science
Technology
Rights
Department
in
Oxford
UK
phone
fax
e
mail
permissions
elsevier
co
uk
You
may
also
complete
your
request
on
line
via
the
Elsevier
homepage
http
elsevier
com
by
selecting
Customer
Support
and
then
Obtaining
Permissions
Library
of
Congress
Cataloging
in
Publication
Data
Harris
David
Money
Digital
design
and
computer
architecture
David
Money
Harris
and
Sarah
L
Harris
st
ed
p
cm
Includes
bibliographical
references
and
index
ISBN
alk
paper
ISBN
Digital
electronics
Logic
design
Computer
architecture
I
Harris
Sarah
L
II
Title
TK
D
H
dc
For
information
on
all
Morgan
Kaufmann
publications
visit
our
Web
site
at
www
mkp
com
Printed
in
the
United
States
of
America
Pr
To
my
family
Jennifer
and
Abraham
DMH
To
my
sisters
Lara
and
Jenny
SLH
Contents
Preface
xvii
Features
xviii
Online
Supplements
xix
How
to
Use
the
Software
Tools
in
a
Course
xix
Labs
xx
Bugs
xxi
Acknowledgments
xxi
Chapter
From
Zero
to
One
The
Game
Plan
The
Art
of
Managing
Complexity
Abstraction
Discipline
The
Three
Y
s
The
Digital
Abstraction
Number
Systems
Decimal
Numbers
Binary
Numbers
Hexadecimal
Numbers
Bytes
Nibbles
and
All
That
Jazz
Binary
Addition
Signed
Binary
Numbers
Logic
Gates
NOT
Gate
Buffer
AND
Gate
OR
Gate
Other
Two
Input
Gates
Multiple
Input
Gates
Beneath
the
Digital
Abstraction
Supply
Voltage
Logic
Levels
Noise
Margins
DC
Transfer
Characteristics
The
Static
Discipline
ix
CMOS
Transistors
Semiconductors
Diodes
Capacitors
nMOS
and
pMOS
Transistors
CMOS
NOT
Gate
Other
CMOS
Logic
Gates
Transmission
Gates
Pseudo
nMOS
Logic
Power
Consumption
Summary
and
a
Look
Ahead
Exercises
Interview
Questions
Chapter
Combinational
Logic
Design
Introduction
Boolean
Equations
Terminology
Sum
of
Products
Form
Product
of
Sums
Form
Boolean
Algebra
Axioms
Theorems
of
One
Variable
Theorems
of
Several
Variables
The
Truth
Behind
It
All
Simplifying
Equations
From
Logic
to
Gates
Multilevel
Combinational
Logic
Hardware
Reduction
Bubble
Pushing
X
s
and
Z
s
Oh
My
Illegal
Value
X
Floating
Value
Z
Karnaugh
Maps
Circular
Thinking
Logic
Minimization
with
K
Maps
Don
t
Cares
The
Big
Picture
Combinational
Building
Blocks
Multiplexers
Decoders
Timing
Propagation
and
Contamination
Delay
Glitches
x
CONTENTS
Summary
Exercises
Interview
Questions
Chapter
Sequential
Logic
Design
Introduction
Latches
and
Flip
Flops
SR
Latch
D
Latch
D
Flip
Flop
Register
Enabled
Flip
Flop
Resettable
Flip
Flop
Transistor
Level
Latch
and
Flip
Flop
Designs
Putting
It
All
Together
Synchronous
Logic
Design
Some
Problematic
Circuits
Synchronous
Sequential
Circuits
Synchronous
and
Asynchronous
Circuits
Finite
State
Machines
FSM
Design
Example
State
Encodings
Moore
and
Mealy
Machines
Factoring
State
Machines
FSM
Review
Timing
of
Sequential
Logic
The
Dynamic
Discipline
System
Timing
Clock
Skew
Metastability
Synchronizers
Derivation
of
Resolution
Time
Parallelism
Summary
Exercises
Interview
Questions
Chapter
Hardware
Description
Languages
Introduction
Modules
Language
Origins
Simulation
and
Synthesis
CONTENTS
xi
Combinational
Logic
Bitwise
Operators
Comments
and
White
Space
Reduction
Operators
Conditional
Assignment
Internal
Variables
Precedence
Numbers
Z
s
and
X
s
Bit
Swizzling
Delays
VHDL
Libraries
and
Types
Structural
Modeling
Sequential
Logic
Registers
Resettable
Registers
Enabled
Registers
Multiple
Registers
Latches
More
Combinational
Logic
Case
Statements
If
Statements
Verilog
casez
Blocking
and
Nonblocking
Assignments
Finite
State
Machines
Parameterized
Modules
Testbenches
Summary
Exercises
Interview
Questions
Chapter
Digital
Building
Blocks
Introduction
Arithmetic
Circuits
Addition
Subtraction
Comparators
ALU
Shifters
and
Rotators
Multiplication
Division
Further
Reading
xii
CONTENTS
Number
Systems
Fixed
Point
Number
Systems
Floating
Point
Number
Systems
Sequential
Building
Blocks
Counters
Shift
Registers
Memory
Arrays
Overview
Dynamic
Random
Access
Memory
Static
Random
Access
Memory
Area
and
Delay
Register
Files
Read
Only
Memory
Logic
Using
Memory
Arrays
Memory
HDL
Logic
Arrays
Programmable
Logic
Array
Field
Programmable
Gate
Array
Array
Implementations
Summary
Exercises
Interview
Questions
Chapter
Architecture
Introduction
Assembly
Language
Instructions
Operands
Registers
Memory
and
Constants
Machine
Language
R
type
Instructions
I
type
Instructions
J
type
Instructions
Interpreting
Machine
Language
Code
The
Power
of
the
Stored
Program
Programming
Arithmetic
Logical
Instructions
Branching
Conditional
Statements
Getting
Loopy
Arrays
Procedure
Calls
Addressing
Modes
CONTENTS
xiii
Lights
Camera
Action
Compiling
Assembling
and
Loading
The
Memory
Map
Translating
and
Starting
a
Program
Odds
and
Ends
Pseudoinstructions
Exceptions
Signed
and
Unsigned
Instructions
Floating
Point
Instructions
Real
World
Perspective
IA
Architecture
IA
Registers
IA
Operands
Status
Flags
IA
Instructions
IA
Instruction
Encoding
Other
IA
Peculiarities
The
Big
Picture
Summary
Exercises
Interview
Questions
Chapter
Microarchitecture
Introduction
Architectural
State
and
Instruction
Set
Design
Process
MIPS
Microarchitectures
Performance
Analysis
Single
Cycle
Processor
Single
Cycle
Datapath
Single
Cycle
Control
More
Instructions
Performance
Analysis
Multicycle
Processor
Multicycle
Datapath
Multicycle
Control
More
Instructions
Performance
Analysis
Pipelined
Processor
Pipelined
Datapath
Pipelined
Control
Hazards
More
Instructions
Performance
Analysis
xiv
CONTENTS
HDL
Representation
Single
Cycle
Processor
Generic
Building
Blocks
Testbench
Exceptions
Advanced
Microarchitecture
Deep
Pipelines
Branch
Prediction
Superscalar
Processor
Out
of
Order
Processor
Register
Renaming
Single
Instruction
Multiple
Data
Multithreading
Multiprocessors
Real
World
Perspective
IA
Microarchitecture
Summary
Exercises
Interview
Questions
Chapter
Memory
Systems
Introduction
Memory
System
Performance
Analysis
Caches
What
Data
Is
Held
in
the
Cache
How
Is
the
Data
Found
What
Data
Is
Replaced
Advanced
Cache
Design
The
Evolution
of
MIPS
Caches
Virtual
Memory
Address
Translation
The
Page
Table
The
Translation
Lookaside
Buffer
Memory
Protection
Replacement
Policies
Multilevel
Page
Tables
Memory
Mapped
I
O
Real
World
Perspective
IA
Memory
and
I
O
Systems
IA
Cache
Systems
IA
Virtual
Memory
IA
Programmed
I
O
Summary
Exercises
Interview
Questions
CONTENTS
xv
Appendix
A
Digital
System
Implementation
A
Introduction
A
xx
Logic
A
Logic
Gates
A
Other
Functions
A
Programmable
Logic
A
PROMs
A
PLAs
A
FPGAs
A
Application
Specific
Integrated
Circuits
A
Data
Sheets
A
Logic
Families
A
Packaging
and
Assembly
A
Transmission
lines
A
Matched
Termination
A
Open
Termination
A
Short
Termination
A
Mismatched
Termination
A
When
to
Use
Transmission
Line
Models
A
Proper
Transmission
Line
Terminations
A
Derivation
of
Z
A
Derivation
of
the
Reflection
Coefficient
A
Putting
It
All
Together
A
Economics
Appendix
B
MIPS
Instructions
Further
Reading
Index
xvi
CONTENTS
xvii
Preface
Why
publish
yet
another
book
on
digital
design
and
computer
architecture
There
are
dozens
of
good
books
in
print
on
digital
design
There
are
also
several
good
books
about
computer
architecture
especially
the
classic
texts
of
Patterson
and
Hennessy
This
book
is
unique
in
its
treatment
in
that
it
presents
digital
logic
design
from
the
perspective
of
computer
architecture
starting
at
the
beginning
with
s
and
s
and
leading
students
through
the
design
of
a
MIPS
microprocessor
We
have
used
several
editions
of
Patterson
and
Hennessy
s
Computer
Organization
and
Design
COD
for
many
years
at
Harvey
Mudd
College
We
particularly
like
their
coverage
of
the
MIPS
architecture
and
microarchitecture
because
MIPS
is
a
commercially
successful
microprocessor
architecture
yet
it
is
simple
enough
to
clearly
explain
and
build
in
an
introductory
class
Because
our
class
has
no
prerequisites
the
first
half
of
the
semester
is
dedicated
to
digital
design
which
is
not
covered
by
COD
Other
universities
have
indicated
a
need
for
a
book
that
combines
digital
design
and
computer
architecture
We
have
undertaken
to
prepare
such
a
book
We
believe
that
building
a
microprocessor
is
a
special
rite
of
passage
for
engineering
and
computer
science
students
The
inner
workings
of
a
processor
seem
almost
magical
to
the
uninitiated
yet
prove
to
be
straightforward
when
carefully
explained
Digital
design
in
itself
is
a
powerful
and
exciting
subject
Assembly
language
programming
unveils
the
inner
language
spoken
by
the
processor
Microarchitecture
is
the
link
that
brings
it
all
together
This
book
is
suitable
for
a
rapid
paced
single
semester
introduction
to
digital
design
and
computer
architecture
or
for
a
two
quarter
or
two
semester
sequence
giving
more
time
to
digest
the
material
and
experiment
in
the
lab
The
only
prerequisite
is
basic
familiarity
with
a
high
level
programming
language
such
as
C
C
or
Java
The
material
is
usually
taught
at
the
sophomore
or
junior
year
level
but
may
also
be
accessible
to
bright
freshmen
who
have
some
programming
experience
FEATURES
This
book
offers
a
number
of
special
features
Side
by
Side
Coverage
of
Verilog
and
VHDL
Hardware
description
languages
HDLs
are
at
the
center
of
modern
digital
design
practices
Unfortunately
designers
are
evenly
split
between
the
two
dominant
languages
Verilog
and
VHDL
This
book
introduces
HDLs
in
Chapter
as
soon
as
combinational
and
sequential
logic
design
has
been
covered
HDLs
are
then
used
in
Chapters
and
to
design
larger
building
blocks
and
entire
processors
Nevertheless
Chapter
can
be
skipped
and
the
later
chapters
are
still
accessible
for
courses
that
choose
not
to
cover
HDLs
This
book
is
unique
in
its
side
by
side
presentation
of
Verilog
and
VHDL
enabling
the
reader
to
quickly
compare
and
contrast
the
two
languages
Chapter
describes
principles
applying
to
both
HDLs
then
provides
language
specific
syntax
and
examples
in
adjacent
columns
This
side
by
side
treatment
makes
it
easy
for
an
instructor
to
choose
either
HDL
and
for
the
reader
to
transition
from
one
to
the
other
either
in
a
class
or
in
professional
practice
Classic
MIPS
Architecture
and
Microarchitecture
Chapters
and
focus
on
the
MIPS
architecture
adapted
from
the
treatment
of
Patterson
and
Hennessy
MIPS
is
an
ideal
architecture
because
it
is
a
real
architecture
shipped
in
millions
of
products
yearly
yet
it
is
streamlined
and
easy
to
learn
Moreover
hundreds
of
universities
around
the
world
have
developed
pedagogy
labs
and
tools
around
the
MIPS
architecture
Real
World
Perspectives
Chapters
and
illustrate
the
architecture
microarchitecture
and
memory
hierarchy
of
Intel
IA
processors
These
real
world
perspective
chapters
show
how
the
concepts
in
the
chapter
relate
to
the
chips
found
in
most
PCs
Accessible
Overview
of
Advanced
Microarchitecture
Chapter
includes
an
overview
of
modern
high
performance
microarchitectural
features
including
branch
prediction
superscalar
and
out
oforder
operation
multithreading
and
multicore
processors
The
treatment
is
accessible
to
a
student
in
a
first
course
and
shows
how
the
microarchitectures
in
the
book
can
be
extended
to
modern
processors
End
of
Chapter
Exercises
and
Interview
Questions
The
best
way
to
learn
digital
design
is
to
do
it
Each
chapter
ends
with
numerous
exercises
to
practice
the
material
The
exercises
are
followed
by
a
set
of
interview
questions
that
our
industrial
colleagues
have
asked
students
applying
for
work
in
the
field
These
questions
provide
a
helpful
xviii
PREFACE
glimpse
into
the
types
of
problems
job
applicants
will
typically
encounter
during
the
interview
process
Exercise
solutions
are
available
via
the
book
s
companion
and
instructor
Web
pages
For
more
details
see
the
next
section
Online
Supplements
ONLINE
SUPPLEMENTS
Supplementary
materials
are
available
online
at
textbooks
elsevier
com
This
companion
site
accessible
to
all
readers
includes
Solutions
to
odd
numbered
exercises
Links
to
professional
strength
computer
aided
design
CAD
tools
from
Xilinx
and
Synplicity
Link
to
PCSPIM
a
Windows
based
MIPS
simulator
Hardware
description
language
HDL
code
for
the
MIPS
processor
Xilinx
Project
Navigator
helpful
hints
Lecture
slides
in
PowerPoint
PPT
format
Sample
course
and
lab
materials
List
of
errata
The
instructor
site
linked
to
the
companion
site
and
accessible
to
adopters
who
register
at
textbooks
elsevier
com
includes
Solutions
to
even
numbered
exercises
Links
to
professional
strength
computer
aided
design
CAD
tools
from
Xilinx
and
Synplicity
Instructors
from
qualified
universities
can
access
free
Synplicity
tools
for
use
in
their
classroom
and
laboratories
More
details
are
available
at
the
instructor
site
Figures
from
the
text
in
JPG
and
PPT
formats
Additional
details
on
using
the
Xilinx
Synplicity
and
PCSPIM
tools
in
your
course
are
provided
in
the
next
section
Details
on
the
sample
lab
materials
are
also
provided
here
HOW
TO
USE
THE
SOFTWARE
TOOLS
IN
A
COURSE
Xilinx
ISE
WebPACK
Xilinx
ISE
WebPACK
is
a
free
version
of
the
professional
strength
Xilinx
ISE
Foundation
FPGA
design
tools
It
allows
students
to
enter
their
digital
designs
in
schematic
or
using
either
the
Verilog
or
VHDL
hardware
description
language
HDL
After
entering
the
design
students
can
PREFACE
xix
Prelims
qxd
simulate
their
circuits
using
ModelSim
MXE
III
Starter
which
is
included
in
the
Xilinx
WebPACK
Xilinx
WebPACK
also
includes
XST
a
logic
synthesis
tool
supporting
both
Verilog
and
VHDL
The
difference
between
WebPACK
and
Foundation
is
that
WebPACK
supports
a
subset
of
the
most
common
Xilinx
FPGAs
The
difference
between
ModelSim
MXE
III
Starter
and
ModelSim
commercial
versions
is
that
Starter
degrades
performance
for
simulations
with
more
than
lines
of
HDL
Synplify
Pro
Synplify
Pro
is
a
high
performance
sophisticated
logic
synthesis
engine
for
FPGA
and
CPLD
designs
Synplify
Pro
also
contains
HDL
Analyst
a
graphical
interface
tool
that
generates
schematic
views
of
the
HDL
source
code
We
have
found
that
this
is
immensely
useful
in
the
learning
and
debugging
process
Synplicity
has
generously
agreed
to
donate
Synplify
Pro
to
qualified
universities
and
will
provide
as
many
licenses
as
needed
to
fill
university
labs
Instructors
should
visit
the
instructor
Web
page
for
this
text
for
more
information
on
how
to
request
Synplify
Pro
licenses
For
additional
information
on
Synplicity
and
its
other
software
visit
www
synplicity
com
university
PCSPIM
PCSPIM
also
called
simply
SPIM
is
a
Windows
based
MIPS
simulator
that
runs
MIPS
assembly
code
Students
enter
their
MIPS
assembly
code
into
a
text
file
and
run
it
using
PCSPIM
PCSPIM
displays
the
instructions
memory
and
register
values
Links
to
the
user
s
manual
and
an
example
file
are
available
at
the
companion
site
textbooks
elsevier
com
LABS
The
companion
site
includes
links
to
a
series
of
labs
that
cover
topics
from
digital
design
through
computer
architecture
The
labs
teach
students
how
to
use
the
Xilinx
WebPACK
or
Foundation
tools
to
enter
simulate
synthesize
and
implement
their
designs
The
labs
also
include
topics
on
assembly
language
programming
using
the
PCSPIM
simulator
After
synthesis
students
can
implement
their
designs
using
the
Digilent
Spartan
Starter
Board
or
the
XUP
Virtex
Pro
V
Pro
Board
Both
of
these
powerful
and
competitively
priced
boards
are
available
from
www
digilentinc
com
The
boards
contain
FPGAs
that
can
be
programmed
to
implement
student
designs
We
provide
labs
that
describe
how
to
implement
a
selection
of
designs
using
Digilent
s
Spartan
Board
using
xx
PREFACE
PREFACE
xxi
WebPACK
Unfortunately
Xilinx
WebPACK
does
not
support
the
huge
FPGA
on
the
V
Pro
board
Qualified
universities
may
contact
the
Xilinx
University
Program
to
request
a
donation
of
the
full
Foundation
tools
To
run
the
labs
students
will
need
to
download
and
install
the
Xilinx
WebPACK
PCSPIM
and
possibly
Synplify
Pro
Instructors
may
also
choose
to
install
the
tools
on
lab
machines
The
labs
include
instructions
on
how
to
implement
the
projects
on
the
Digilent
s
Spartan
Starter
Board
The
implementation
step
may
be
skipped
but
we
have
found
it
of
great
value
The
labs
will
also
work
with
the
XST
synthesis
tool
but
we
recommend
using
Synplify
Pro
because
the
schematics
it
produces
give
students
invaluable
feedback
We
have
tested
the
labs
on
Windows
but
the
tools
are
also
available
for
Linux
BUGS
As
all
experienced
programmers
know
any
program
of
significant
complexity
undoubtedly
contains
bugs
So
too
do
books
We
have
taken
great
care
to
find
and
squash
the
bugs
in
this
book
However
some
errors
undoubtedly
do
remain
We
will
maintain
a
list
of
errata
on
the
book
s
Web
page
Please
send
your
bug
reports
to
ddcabugs
onehotlogic
com
The
first
person
to
report
a
substantive
bug
with
a
fix
that
we
use
in
a
future
printing
will
be
rewarded
with
a
bounty
Be
sure
to
include
your
mailing
address
ACKNOWLEDGMENTS
First
and
foremost
we
thank
David
Patterson
and
John
Hennessy
for
their
pioneering
MIPS
microarchitectures
described
in
their
Computer
Organization
and
Design
textbook
We
have
taught
from
various
editions
of
their
book
for
many
years
We
appreciate
their
gracious
support
of
this
book
and
their
permission
to
build
on
their
microarchitectures
Duane
Bibby
our
favorite
cartoonist
labored
long
and
hard
to
illustrate
the
fun
and
adventure
of
digital
design
We
also
appreciate
the
enthusiasm
of
Denise
Penrose
Nate
McFadden
and
the
rest
of
the
team
at
Morgan
Kaufmann
who
made
this
book
happen
Jeff
Somers
at
Graphic
World
Publishing
Services
has
ably
guided
the
book
through
production
Numerous
reviewers
have
substantially
improved
the
book
They
include
John
Barr
Ithaca
College
Jack
V
Briner
Charleston
Southern
University
Andrew
C
Brown
SK
Communications
Carl
Baumgaertner
Harvey
Mudd
College
A
Utku
Diril
Nvidia
Corporation
Jim
Frenzel
University
of
Idaho
Jaeha
Kim
Rambus
Inc
Phillip
King
xxii
PREFACE
ShotSpotter
Inc
James
Pinter
Lucke
Claremont
McKenna
College
Amir
Roth
Z
Jerry
Shi
University
of
Connecticut
James
E
Stine
Oklahoma
State
University
Luke
Teyssier
Peiyi
Zhao
Chapman
University
and
an
anonymous
reviewer
Simon
Moore
was
a
wonderful
host
during
David
s
sabbatical
visit
to
Cambridge
University
where
major
sections
of
this
book
were
written
We
also
appreciate
the
students
in
our
course
at
Harvey
Mudd
College
who
have
given
us
helpful
feedback
on
drafts
of
this
textbook
Of
special
note
are
Casey
Schilling
Alice
Clifton
Chris
Acon
and
Stephen
Brawner
I
David
particularly
thank
my
wife
Jennifer
who
gave
birth
to
our
son
Abraham
at
the
beginning
of
the
project
I
appreciate
her
patience
and
loving
support
through
yet
another
project
at
a
busy
time
in
our
lives
The
Game
Plan
The
Art
of
Managing
Complexity
The
Digital
Abstraction
Number
Systems
Logic
Gates
Beneath
the
Digital
Abstraction
CMOS
Transistors
Power
Consumption
Summary
and
a
Look
Ahead
Exercises
Interview
Questions
From
Zero
to
One
THE
GAME
PLAN
Microprocessors
have
revolutionized
our
world
during
the
past
three
decades
A
laptop
computer
today
has
far
more
capability
than
a
room
sized
mainframe
of
yesteryear
A
luxury
automobile
contains
about
microprocessors
Advances
in
microprocessors
have
made
cell
phones
and
the
Internet
possible
have
vastly
improved
medicine
and
have
transformed
how
war
is
waged
Worldwide
semiconductor
industry
sales
have
grown
from
US
billion
in
to
billion
in
and
microprocessors
are
a
major
segment
of
these
sales
We
believe
that
microprocessors
are
not
only
technically
economically
and
socially
important
but
are
also
an
intrinsically
fascinating
human
invention
By
the
time
you
finish
reading
this
book
you
will
know
how
to
design
and
build
your
own
microprocessor
The
skills
you
learn
along
the
way
will
prepare
you
to
design
many
other
digital
systems
We
assume
that
you
have
a
basic
familiarity
with
electricity
some
prior
programming
experience
and
a
genuine
interest
in
understanding
what
goes
on
under
the
hood
of
a
computer
This
book
focuses
on
the
design
of
digital
systems
which
operate
on
s
and
s
We
begin
with
digital
logic
gates
that
accept
s
and
s
as
inputs
and
produce
s
and
s
as
outputs
We
then
explore
how
to
combine
logic
gates
into
more
complicated
modules
such
as
adders
and
memories
Then
we
shift
gears
to
programming
in
assembly
language
the
native
tongue
of
the
microprocessor
Finally
we
put
gates
together
to
build
a
microprocessor
that
runs
these
assembly
language
programs
A
great
advantage
of
digital
systems
is
that
the
building
blocks
are
quite
simple
just
s
and
s
They
do
not
require
grungy
mathematics
or
a
profound
knowledge
of
physics
Instead
the
designer
s
challenge
is
to
combine
these
simple
blocks
into
complicated
systems
A
microprocessor
may
be
the
first
system
that
you
build
that
is
too
complex
to
fit
in
your
head
all
at
once
One
of
the
major
themes
weaved
through
this
book
is
how
to
manage
complexity
THE
ART
OF
MANAGING
COMPLEXITY
One
of
the
characteristics
that
separates
an
engineer
or
computer
scientist
from
a
layperson
is
a
systematic
approach
to
managing
complexity
Modern
digital
systems
are
built
from
millions
or
billions
of
transistors
No
human
being
could
understand
these
systems
by
writing
equations
describing
the
movement
of
electrons
in
each
transistor
and
solving
all
of
the
equations
simultaneously
You
will
need
to
learn
to
manage
complexity
to
understand
how
to
build
a
microprocessor
without
getting
mired
in
a
morass
of
detail
Abstraction
The
critical
technique
for
managing
complexity
is
abstraction
hiding
details
when
they
are
not
important
A
system
can
be
viewed
from
many
different
levels
of
abstraction
For
example
American
politicians
abstract
the
world
into
cities
counties
states
and
countries
A
county
contains
multiple
cities
and
a
state
contains
many
counties
When
a
politician
is
running
for
president
the
politician
is
mostly
interested
in
how
the
state
as
a
whole
will
vote
rather
than
how
each
county
votes
so
the
state
is
the
most
useful
level
of
abstraction
On
the
other
hand
the
Census
Bureau
measures
the
population
of
every
city
so
the
agency
must
consider
the
details
of
a
lower
level
of
abstraction
Figure
illustrates
levels
of
abstraction
for
an
electronic
computer
system
along
with
typical
building
blocks
at
each
level
At
the
lowest
level
of
abstraction
is
the
physics
the
motion
of
electrons
The
behavior
of
electrons
is
described
by
quantum
mechanics
and
Maxwell
s
equations
Our
system
is
constructed
from
electronic
devices
such
as
transistors
or
vacuum
tubes
once
upon
a
time
These
devices
have
well
defined
connection
points
called
terminals
and
can
be
modeled
by
the
relationship
between
voltage
and
current
as
measured
at
each
terminal
By
abstracting
to
this
device
level
we
can
ignore
the
individual
electrons
The
next
level
of
abstraction
is
analog
circuits
in
which
devices
are
assembled
to
create
components
such
as
amplifiers
Analog
circuits
input
and
output
a
continuous
range
of
voltages
Digital
circuits
such
as
logic
gates
restrict
the
voltages
to
discrete
ranges
which
we
will
use
to
indicate
and
In
logic
design
we
build
more
complex
structures
such
as
adders
or
memories
from
digital
circuits
Microarchitecture
links
the
logic
and
architecture
levels
of
abstraction
The
architecture
level
of
abstraction
describes
a
computer
from
the
programmer
s
perspective
For
example
the
Intel
IA
architecture
used
by
microprocessors
in
most
personal
computers
PCs
is
defined
by
a
set
of
CHAPTER
ONE
From
Zero
to
One
Physics
Devices
Analog
Circuits
Digital
Circuits
Logic
Microarchitecture
Architecture
Operating
Systems
Application
Software
Electrons
Transistors
Diodes
Amplifiers
Filters
AND
gates
NOT
gates
Adders
Memories
Datapaths
Controllers
Instructions
Registers
Device
Drivers
Programs
Figure
Levels
of
abstraction
for
electronic
computing
system
instructions
and
registers
memory
for
temporarily
storing
variables
that
the
programmer
is
allowed
to
use
Microarchitecture
involves
combining
logic
elements
to
execute
the
instructions
defined
by
the
architecture
A
particular
architecture
can
be
implemented
by
one
of
many
different
microarchitectures
with
different
price
performance
power
trade
offs
For
example
the
Intel
Core
Duo
the
Intel
and
the
AMD
Athlon
all
implement
the
IA
architecture
with
different
microarchitectures
Moving
into
the
software
realm
the
operating
system
handles
lowlevel
details
such
as
accessing
a
hard
drive
or
managing
memory
Finally
the
application
software
uses
these
facilities
provided
by
the
operating
system
to
solve
a
problem
for
the
user
Thanks
to
the
power
of
abstraction
your
grandmother
can
surf
the
Web
without
any
regard
for
the
quantum
vibrations
of
electrons
or
the
organization
of
the
memory
in
her
computer
This
book
focuses
on
the
levels
of
abstraction
from
digital
circuits
through
computer
architecture
When
you
are
working
at
one
level
of
abstraction
it
is
good
to
know
something
about
the
levels
of
abstraction
immediately
above
and
below
where
you
are
working
For
example
a
computer
scientist
cannot
fully
optimize
code
without
understanding
the
architecture
for
which
the
program
is
being
written
A
device
engineer
cannot
make
wise
trade
offs
in
transistor
design
without
understanding
the
circuits
in
which
the
transistors
will
be
used
We
hope
that
by
the
time
you
finish
reading
this
book
you
can
pick
the
level
of
abstraction
appropriate
to
solving
your
problem
and
evaluate
the
impact
of
your
design
choices
on
other
levels
of
abstraction
Discipline
Discipline
is
the
act
of
intentionally
restricting
your
design
choices
so
that
you
can
work
more
productively
at
a
higher
level
of
abstraction
Using
interchangeable
parts
is
a
familiar
application
of
discipline
One
of
the
first
examples
of
interchangeable
parts
was
in
flintlock
rifle
manufacturing
Until
the
early
th
century
rifles
were
individually
crafted
by
hand
Components
purchased
from
many
different
craftsmen
were
carefully
filed
and
fit
together
by
a
highly
skilled
gunmaker
The
discipline
of
interchangeable
parts
revolutionized
the
industry
By
limiting
the
components
to
a
standardized
set
with
well
defined
tolerances
rifles
could
be
assembled
and
repaired
much
faster
and
with
less
skill
The
gunmaker
no
longer
concerned
himself
with
lower
levels
of
abstraction
such
as
the
specific
shape
of
an
individual
barrel
or
gunstock
In
the
context
of
this
book
the
digital
discipline
will
be
very
important
Digital
circuits
use
discrete
voltages
whereas
analog
circuits
use
continuous
voltages
Therefore
digital
circuits
are
a
subset
of
analog
circuits
and
in
some
sense
must
be
capable
of
less
than
the
broader
class
of
analog
circuits
However
digital
circuits
are
much
simpler
to
design
By
limiting
The
Art
of
Managing
Complexity
ourselves
to
digital
circuits
we
can
easily
combine
components
into
sophisticated
systems
that
ultimately
outperform
those
built
from
analog
components
in
many
applications
For
example
digital
televisions
compact
disks
CDs
and
cell
phones
are
replacing
their
analog
predecessors
The
Three
Y
s
In
addition
to
abstraction
and
discipline
designers
use
the
three
y
s
to
manage
complexity
hierarchy
modularity
and
regularity
These
principles
apply
to
both
software
and
hardware
systems
Hierarchy
involves
dividing
a
system
into
modules
then
further
subdividing
each
of
these
modules
until
the
pieces
are
easy
to
understand
Modularity
states
that
the
modules
have
well
defined
functions
and
interfaces
so
that
they
connect
together
easily
without
unanticipated
side
effects
Regularity
seeks
uniformity
among
the
modules
Common
modules
are
reused
many
times
reducing
the
number
of
distinct
modules
that
must
be
designed
To
illustrate
these
y
s
we
return
to
the
example
of
rifle
manufacturing
A
flintlock
rifle
was
one
of
the
most
intricate
objects
in
common
use
in
the
early
th
century
Using
the
principle
of
hierarchy
we
can
break
it
into
components
shown
in
Figure
the
lock
stock
and
barrel
The
barrel
is
the
long
metal
tube
through
which
the
bullet
is
fired
The
lock
is
the
firing
mechanism
And
the
stock
is
the
wooden
body
that
holds
the
parts
together
and
provides
a
secure
grip
for
the
user
In
turn
the
lock
contains
the
trigger
hammer
flint
frizzen
and
pan
Each
of
these
components
could
be
hierarchically
described
in
further
detail
Modularity
teaches
that
each
component
should
have
a
well
defined
function
and
interface
A
function
of
the
stock
is
to
mount
the
barrel
and
lock
Its
interface
consists
of
its
length
and
the
location
of
its
mounting
pins
In
a
modular
rifle
design
stocks
from
many
different
manufacturers
can
be
used
with
a
particular
barrel
as
long
as
the
stock
and
barrel
are
of
the
correct
length
and
have
the
proper
mounting
mechanism
A
function
of
the
barrel
is
to
impart
spin
to
the
bullet
so
that
it
travels
more
accurately
Modularity
dictates
that
there
should
be
no
side
effects
the
design
of
the
stock
should
not
impede
the
function
of
the
barrel
Regularity
teaches
that
interchangeable
parts
are
a
good
idea
With
regularity
a
damaged
barrel
can
be
replaced
by
an
identical
part
The
barrels
can
be
efficiently
built
on
an
assembly
line
instead
of
being
painstakingly
hand
crafted
We
will
return
to
these
principles
of
hierarchy
modularity
and
regularity
throughout
the
book
CHAPTER
ONE
From
Zero
to
One
Captain
Meriwether
Lewis
of
the
Lewis
and
Clark
Expedition
was
one
of
the
early
advocates
of
interchangeable
parts
for
rifles
In
he
explained
The
guns
of
Drewyer
and
Sergt
Pryor
were
both
out
of
order
The
first
was
repared
with
a
new
lock
the
old
one
having
become
unfit
for
use
the
second
had
the
cock
screw
broken
which
was
replaced
by
a
duplicate
which
had
been
prepared
for
the
lock
at
Harpers
Ferry
where
she
was
manufactured
But
for
the
precaution
taken
in
bringing
on
those
extra
locks
and
parts
of
locks
in
addition
to
the
ingenuity
of
John
Shields
most
of
our
guns
would
at
this
moment
been
entirely
unfit
for
use
but
fortunately
for
us
I
have
it
in
my
power
here
to
record
that
they
are
all
in
good
order
See
Elliott
Coues
ed
The
History
of
the
Lewis
and
Clark
Expedition
vols
New
York
Harper
reprint
vols
New
York
Dover
Cha
THE
DIGITAL
ABSTRACTION
Most
physical
variables
are
continuous
For
example
the
voltage
on
a
wire
the
frequency
of
an
oscillation
or
the
position
of
a
mass
are
all
continuous
quantities
Digital
systems
on
the
other
hand
represent
information
with
discrete
valued
variables
that
is
variables
with
a
finite
number
of
distinct
values
An
early
digital
system
using
variables
with
ten
discrete
values
was
Charles
Babbage
s
Analytical
Engine
Babbage
labored
from
to
designing
and
attempting
to
build
this
mechanical
computer
The
Analytical
Engine
used
gears
with
ten
positions
labeled
through
much
like
a
mechanical
odometer
in
a
car
Figure
shows
a
prototype
The
Digital
Abstraction
Barrel
Stoc
Lock
Expanded
view
of
Lock
k
Flint
Cock
Pan
Spring
String
Figure
Flintlock
rifle
with
a
close
up
view
of
the
lock
Image
by
Euroams
Italia
www
euroarms
net
And
we
thought
graduate
school
was
long
Charles
Babbage
Attended
Cambridge
University
and
married
Georgiana
Whitmore
in
Invented
the
Analytical
Engine
the
world
s
first
mechanical
computer
Also
invented
the
cowcatcher
and
the
universal
postage
rate
Interested
in
lock
picking
but
abhorred
street
musicians
image
courtesy
of
Fourmilab
Switzerland
www
fourmilab
ch
of
the
Analytical
Engine
in
which
each
row
processes
one
digit
Babbage
chose
rows
of
gears
so
the
machine
has
digit
precision
Unlike
Babbage
s
machine
most
electronic
computers
use
a
binary
two
valued
representation
in
which
a
high
voltage
indicates
a
and
a
low
voltage
indicates
a
because
it
is
easier
to
distinguish
between
two
voltages
than
ten
The
amount
of
information
D
in
a
discrete
valued
variable
with
N
distinct
states
is
measured
in
units
of
bits
as
D
log
N
bits
A
binary
variable
conveys
log
bit
of
information
Indeed
the
word
bit
is
short
for
binary
digit
Each
of
Babbage
s
gears
carried
log
bits
of
information
because
it
could
be
in
one
of
unique
positions
A
continuous
signal
theoretically
contains
an
infinite
amount
of
information
because
it
can
take
on
an
infinite
number
of
values
In
practice
noise
and
measurement
error
limit
the
information
to
only
to
bits
for
most
continuous
signals
If
the
measurement
must
be
made
rapidly
the
information
content
is
lower
e
g
bits
This
book
focuses
on
digital
circuits
using
binary
variables
s
and
s
George
Boole
developed
a
system
of
logic
operating
on
binary
variables
that
is
now
known
as
Boolean
logic
Each
of
Boole
s
variables
could
be
TRUE
or
FALSE
Electronic
computers
commonly
use
a
positive
voltage
to
represent
and
zero
volts
to
represent
In
this
book
we
will
use
the
terms
TRUE
and
HIGH
synonymously
Similarly
we
will
use
FALSE
and
LOW
interchangeably
The
beauty
of
the
digital
abstraction
is
that
digital
designers
can
focus
on
s
and
s
ignoring
whether
the
Boolean
variables
are
physically
represented
with
specific
voltages
rotating
gears
or
even
hydraulic
CHAPTER
ONE
From
Zero
to
One
Figure
Babbage
s
Analytical
Engine
under
construction
at
the
time
of
his
death
in
image
courtesy
of
Science
Museum
Science
and
Society
Picture
Library
George
Boole
Born
to
working
class
parents
and
unable
to
afford
a
formal
education
Boole
taught
himself
mathematics
and
joined
the
faculty
of
Queen
s
College
in
Ireland
He
wrote
An
Investigation
of
the
Laws
of
Thought
which
introduced
binary
variables
and
the
three
fundamental
logic
operations
AND
OR
and
NOT
image
courtesy
of
xxx
Chap
fluid
levels
A
computer
programmer
can
work
without
needing
to
know
the
intimate
details
of
the
computer
hardware
On
the
other
hand
understanding
the
details
of
the
hardware
allows
the
programmer
to
optimize
the
software
better
for
that
specific
computer
An
individual
bit
doesn
t
carry
much
information
In
the
next
section
we
examine
how
groups
of
bits
can
be
used
to
represent
numbers
In
later
chapters
we
will
also
use
groups
of
bits
to
represent
letters
and
programs
NUMBER
SYSTEMS
You
are
accustomed
to
working
with
decimal
numbers
In
digital
systems
consisting
of
s
and
s
binary
or
hexadecimal
numbers
are
often
more
convenient
This
section
introduces
the
various
number
systems
that
will
be
used
throughout
the
rest
of
the
book
Decimal
Numbers
In
elementary
school
you
learned
to
count
and
do
arithmetic
in
decimal
Just
as
you
probably
have
ten
fingers
there
are
ten
decimal
digits
Decimal
digits
are
joined
together
to
form
longer
decimal
numbers
Each
column
of
a
decimal
number
has
ten
times
the
weight
of
the
previous
column
From
right
to
left
the
column
weights
are
and
so
on
Decimal
numbers
are
referred
to
as
base
The
base
is
indicated
by
a
subscript
after
the
number
to
prevent
confusion
when
working
in
more
than
one
base
For
example
Figure
shows
how
the
decimal
number
is
written
as
the
sum
of
each
of
its
digits
multiplied
by
the
weight
of
the
corresponding
column
An
N
digit
decimal
number
represents
one
of
N
possibilities
N
This
is
called
the
range
of
the
number
For
example
a
three
digit
decimal
number
represents
one
of
possibilities
in
the
range
of
to
Binary
Numbers
Bits
represent
one
of
two
values
or
and
are
joined
together
to
form
binary
numbers
Each
column
of
a
binary
number
has
twice
the
weight
Number
Systems
nine
thousands
s
column
s
column
s
column
seven
hundreds
four
tens
two
ones
s
column
Figure
Representation
of
a
decimal
number
C
of
the
previous
column
so
binary
numbers
are
base
In
binary
the
column
weights
again
from
right
to
left
are
and
so
on
If
you
work
with
binary
numbers
often
you
ll
save
time
if
you
remember
these
powers
of
two
up
to
An
N
bit
binary
number
represents
one
of
N
possibilities
N
Table
shows
and
bit
binary
numbers
and
their
decimal
equivalents
Example
BINARY
TO
DECIMAL
CONVERSION
Convert
the
binary
number
to
decimal
Solution
Figure
shows
the
conversion
CHAPTER
ONE
From
Zero
to
One
Bit
Bit
Bit
Bit
Binary
Binnary
Binary
Binary
Decimal
Numbers
Numbers
Numbers
Numbers
Equivalents
Table
Binary
numbers
and
their
decimal
equivalent
C
Example
DECIMAL
TO
BINARY
CONVERSION
Convert
the
decimal
number
to
binary
Solution
Determine
whether
each
column
of
the
binary
result
has
a
or
a
We
can
do
this
starting
at
either
the
left
or
the
right
column
Working
from
the
left
start
with
the
largest
power
of
less
than
the
number
in
this
case
so
there
is
a
in
the
s
column
leaving
so
there
is
a
in
the
s
column
so
there
is
a
in
the
s
column
leaving
so
there
is
a
in
the
s
column
so
there
is
a
in
the
s
column
leaving
Thus
there
must
be
s
in
the
s
and
s
column
Putting
this
all
together
Working
from
the
right
repeatedly
divide
the
number
by
The
remainder
goes
in
each
column
so
goes
in
the
s
column
so
goes
in
the
s
column
with
a
remainder
of
going
in
the
s
column
so
goes
in
the
s
column
with
a
remainder
of
going
in
the
s
column
so
goes
in
the
s
column
Finally
with
a
remainder
of
going
in
the
s
column
Again
Hexadecimal
Numbers
Writing
long
binary
numbers
becomes
tedious
and
prone
to
error
A
group
of
four
bits
represents
one
of
possibilities
Hence
it
is
sometimes
more
convenient
to
work
in
base
called
hexadecimal
Hexadecimal
numbers
use
the
digits
to
along
with
the
letters
A
to
F
as
shown
in
Table
Columns
in
base
have
weights
of
or
or
and
so
on
Example
HEXADECIMAL
TO
BINARY
AND
DECIMAL
CONVERSION
Convert
the
hexadecimal
number
ED
to
binary
and
to
decimal
Solution
Conversion
between
hexadecimal
and
binary
is
easy
because
each
hexadecimal
digit
directly
corresponds
to
four
binary
digits
E
and
D
so
ED
Conversion
to
decimal
requires
the
arithmetic
shown
in
Figure
Number
Systems
one
sixteen
s
column
no
eight
one
four
one
two
no
one
s
column
s
column
s
column
s
column
Figure
Conversion
of
a
binary
number
to
decimal
Hexadecimal
a
term
coined
by
IBM
in
derives
from
the
Greek
hexi
six
and
Latin
decem
ten
A
more
proper
term
would
use
the
Latin
sexa
six
but
sexidecimal
sounded
too
risqu
Chapter
qxd
Example
BINARY
TO
HEXADECIMAL
CONVERSION
Convert
the
binary
number
to
hexadecimal
Solution
Again
conversion
is
easy
Start
reading
from
the
right
The
four
least
significant
bits
are
A
The
next
bits
are
Hence
A
CHAPTER
ONE
From
Zero
to
One
Table
Hexadecimal
number
system
Hexadecimal
Digit
Decimal
Equivalent
Binary
Equivalent
A
B
C
D
E
F
ED
E
D
two
two
hundred
fifty
six
s
s
column
fourteen
sixteens
thirteen
ones
s
column
s
column
Figure
Conversion
of
hexadecimal
number
to
decimal
Cha
Example
DECIMAL
TO
HEXADECIMAL
AND
BINARY
CONVERSION
Convert
the
decimal
number
to
hexadecimal
and
binary
Solution
Like
decimal
to
binary
conversion
decimal
to
hexadecimal
conversion
can
be
done
from
the
left
or
the
right
Working
from
the
left
start
with
the
largest
power
of
less
than
the
number
in
this
case
goes
into
once
so
there
is
a
in
the
s
column
leaving
goes
into
four
times
so
there
is
a
in
the
s
column
leaving
D
so
there
is
a
D
in
the
s
column
In
summary
D
Now
it
is
easy
to
convert
from
hexadecimal
to
binary
as
in
Example
D
Working
from
the
right
repeatedly
divide
the
number
by
The
remainder
goes
in
each
column
with
a
remainder
of
D
going
in
the
s
column
with
a
remainder
of
going
in
the
s
column
with
a
remainder
of
going
in
the
s
column
Again
the
result
is
D
Bytes
Nibbles
and
All
That
Jazz
A
group
of
eight
bits
is
called
a
byte
It
represents
one
of
possibilities
The
size
of
objects
stored
in
computer
memories
is
customarily
measured
in
bytes
rather
than
bits
A
group
of
four
bits
or
half
a
byte
is
called
a
nibble
It
represents
one
of
possibilities
One
hexadecimal
digit
stores
one
nibble
and
two
hexadecimal
digits
store
one
full
byte
Nibbles
are
no
longer
a
commonly
used
unit
but
the
term
is
cute
Microprocessors
handle
data
in
chunks
called
words
The
size
of
a
word
depends
on
the
architecture
of
the
microprocessor
When
this
chapter
was
written
in
most
computers
had
bit
processors
indicating
that
they
operate
on
bit
words
At
the
time
computers
handling
bit
words
were
on
the
verge
of
becoming
widely
available
Simpler
microprocessors
especially
those
used
in
gadgets
such
as
toasters
use
or
bit
words
Within
a
group
of
bits
the
bit
in
the
s
column
is
called
the
least
significant
bit
lsb
and
the
bit
at
the
other
end
is
called
the
most
significant
bit
msb
as
shown
in
Figure
a
for
a
bit
binary
number
Similarly
within
a
word
the
bytes
are
identified
as
least
significant
byte
LSB
through
most
significant
byte
MSB
as
shown
in
Figure
b
for
a
four
byte
number
written
with
eight
hexadecimal
digits
Number
Systems
A
microprocessor
is
a
processor
built
on
a
single
chip
Until
the
s
processors
were
too
complicated
to
fit
on
one
chip
so
mainframe
processors
were
built
from
boards
containing
many
chips
Intel
introduced
the
first
bit
microprocessor
called
the
in
Now
even
the
most
sophisticated
supercomputers
are
built
using
microprocessors
We
will
use
the
terms
microprocessor
and
processor
interchangeably
throughout
this
book
Chapter
qx
By
handy
coincidence
Hence
the
term
kilo
Greek
for
thousand
indicates
For
example
bytes
is
one
kilobyte
KB
Similarly
mega
million
indicates
and
giga
billion
indicates
If
you
know
thousand
million
billion
and
remember
the
powers
of
two
up
to
it
is
easy
to
estimate
any
power
of
two
in
your
head
Example
ESTIMATING
POWERS
OF
TWO
Find
the
approximate
value
of
without
using
a
calculator
Solution
Split
the
exponent
into
a
multiple
of
ten
and
the
remainder
million
So
million
Technically
but
million
is
close
enough
for
marketing
purposes
bytes
is
called
a
kilobyte
KB
bits
is
called
a
kilobit
Kb
or
Kbit
Similarly
MB
Mb
GB
and
Gb
are
used
for
millions
and
billions
of
bytes
and
bits
Memory
capacity
is
usually
measured
in
bytes
Communication
speed
is
usually
measured
in
bits
sec
For
example
the
maximum
speed
of
a
dial
up
modem
is
usually
Kbits
sec
Binary
Addition
Binary
addition
is
much
like
decimal
addition
but
easier
as
shown
in
Figure
As
in
decimal
addition
if
the
sum
of
two
numbers
is
greater
than
what
fits
in
a
single
digit
we
carry
a
into
the
next
column
Figure
compares
addition
of
decimal
and
binary
numbers
In
the
right
most
column
of
Figure
a
which
cannot
fit
in
a
single
digit
because
it
is
greater
than
So
we
record
the
s
digit
and
carry
the
s
digit
over
to
the
next
column
Likewise
in
binary
if
the
sum
of
two
numbers
is
greater
than
we
carry
the
s
digit
over
to
the
next
column
For
example
in
the
right
most
column
of
Figure
b
the
sum
CHAPTER
ONE
From
Zero
to
One
least
significant
bit
most
significant
bit
a
b
DEAFDAD
least
significant
byte
most
significant
byte
carries
a
b
Figure
Least
and
most
significant
bits
and
bytes
Figure
Addition
examples
showing
carries
a
decimal
b
binary
Chapter
qx
cannot
fit
in
a
single
binary
digit
So
we
record
the
s
digit
and
carry
the
s
digit
of
the
result
to
the
next
column
In
the
second
column
the
sum
is
Again
we
record
the
s
digit
and
carry
the
s
digit
to
the
next
column
For
obvious
reasons
the
bit
that
is
carried
over
to
the
neighboring
column
is
called
the
carry
bit
Example
BINARY
ADDITION
Compute
Solution
Figure
shows
that
the
sum
is
The
carries
are
indicated
in
blue
We
can
check
our
work
by
repeating
the
computation
in
decimal
The
sum
is
Digital
systems
usually
operate
on
a
fixed
number
of
digits
Addition
is
said
to
overflow
if
the
result
is
too
big
to
fit
in
the
available
digits
A
bit
number
for
example
has
the
range
bit
binary
addition
overflows
if
the
result
exceeds
The
fifth
bit
is
discarded
producing
an
incorrect
result
in
the
remaining
four
bits
Overflow
can
be
detected
by
checking
for
a
carry
out
of
the
most
significant
column
Example
ADDITION
WITH
OVERFLOW
Compute
Does
overflow
occur
Solution
Figure
shows
the
sum
is
This
result
overflows
the
range
of
a
bit
binary
number
If
it
must
be
stored
as
four
bits
the
most
significant
bit
is
discarded
leaving
the
incorrect
result
of
If
the
computation
had
been
done
using
numbers
with
five
or
more
bits
the
result
would
have
been
correct
Signed
Binary
Numbers
So
far
we
have
considered
only
unsigned
binary
numbers
that
represent
positive
quantities
We
will
often
want
to
represent
both
positive
and
negative
numbers
requiring
a
different
binary
number
system
Several
schemes
exist
to
represent
signed
binary
numbers
the
two
most
widely
employed
are
called
sign
magnitude
and
two
s
complement
Sign
Magnitude
Numbers
Sign
magnitude
numbers
are
intuitively
appealing
because
they
match
our
custom
of
writing
negative
numbers
with
a
minus
sign
followed
by
the
magnitude
An
N
bit
sign
magnitude
number
uses
the
most
significant
bit
Number
Systems
Figure
Binary
addition
example
Figure
Binary
addition
example
with
overflow
Chapter
as
the
sign
and
the
remaining
N
bits
as
the
magnitude
absolute
value
A
sign
bit
of
indicates
positive
and
a
sign
bit
of
indicates
negative
Example
SIGN
MAGNITUDE
NUMBERS
Write
and
as
bit
sign
magnitude
numbers
Solution
Both
numbers
have
a
magnitude
of
Thus
and
Unfortunately
ordinary
binary
addition
does
not
work
for
sign
magnitude
numbers
For
example
using
ordinary
addition
on
gives
which
is
nonsense
An
N
bit
sign
magnitude
number
spans
the
range
N
N
Sign
magnitude
numbers
are
slightly
odd
in
that
both
and
exist
Both
indicate
zero
As
you
may
expect
it
can
be
troublesome
to
have
two
different
representations
for
the
same
number
Two
s
Complement
Numbers
Two
s
complement
numbers
are
identical
to
unsigned
binary
numbers
except
that
the
most
significant
bit
position
has
a
weight
of
N
instead
of
N
They
overcome
the
shortcomings
of
sign
magnitude
numbers
zero
has
a
single
representation
and
ordinary
addition
works
In
two
s
complement
representation
zero
is
written
as
all
zeros
The
most
positive
number
has
a
in
the
most
significant
position
and
s
elsewhere
N
The
most
negative
number
has
a
in
the
most
significant
position
and
s
elsewhere
N
And
is
written
as
all
ones
Notice
that
positive
numbers
have
a
in
the
most
significant
position
and
negative
numbers
have
a
in
this
position
so
the
most
significant
bit
can
be
viewed
as
the
sign
bit
However
the
remaining
bits
are
interpreted
differently
for
two
s
complement
numbers
than
for
sign
magnitude
numbers
The
sign
of
a
two
s
complement
number
is
reversed
in
a
process
called
taking
the
two
s
complement
The
process
consists
of
inverting
all
of
the
bits
in
the
number
then
adding
to
the
least
significant
bit
position
This
is
useful
to
find
the
representation
of
a
negative
number
or
to
determine
the
magnitude
of
a
negative
number
Example
TWO
S
COMPLEMENT
REPRESENTATION
OF
A
NEGATIVE
NUMBER
Find
the
representation
of
as
a
bit
two
s
complement
number
CHAPTER
ONE
From
Zero
to
One
The
billion
Ariane
rocket
launched
on
June
veered
off
course
seconds
after
launch
broke
up
and
exploded
The
failure
was
caused
when
the
computer
controlling
the
rocket
overflowed
its
bit
range
and
crashed
The
code
had
been
extensively
tested
on
the
Ariane
rocket
However
the
Ariane
had
a
faster
engine
that
produced
larger
values
for
the
control
computer
leading
to
the
overflow
Photograph
courtesy
ESA
CNES
ARIANESPACEService
Optique
CS
Chapter
qxd
Solution
Start
with
To
get
invert
the
bits
and
add
Inverting
produces
So
is
Example
VALUE
OF
NEGATIVE
TWO
S
COMPLEMENT
NUMBERS
Find
the
decimal
value
of
the
two
s
complement
number
Solution
has
a
leading
so
it
must
be
negative
To
find
its
magnitude
invert
the
bits
and
add
Inverting
Hence
Two
s
complement
numbers
have
the
compelling
advantage
that
addition
works
properly
for
both
positive
and
negative
numbers
Recall
that
when
adding
N
bit
numbers
the
carry
out
of
the
Nth
bit
i
e
the
N
th
result
bit
is
discarded
Example
ADDING
TWO
S
COMPLEMENT
NUMBERS
Compute
a
and
b
using
two
s
complement
numbers
Solution
a
b
The
fifth
bit
is
discarded
leaving
the
correct
bit
result
Subtraction
is
performed
by
taking
the
two
s
complement
of
the
second
number
then
adding
Example
SUBTRACTING
TWO
S
COMPLEMENT
NUMBERS
Compute
a
and
b
using
bit
two
s
complement
numbers
Solution
a
Take
its
two
s
complement
to
obtain
Now
add
Note
that
the
carry
out
of
the
most
significant
position
is
discarded
because
the
result
is
stored
in
four
bits
b
Take
the
two
s
complement
of
to
obtain
Now
add
The
two
s
complement
of
is
found
by
inverting
all
the
bits
producing
and
adding
which
produces
all
s
disregarding
the
carry
out
of
the
most
significant
bit
position
Hence
zero
is
always
represented
with
all
s
Unlike
the
sign
magnitude
system
the
two
s
complement
system
has
no
separate
Zero
is
considered
positive
because
its
sign
bit
is
Number
Systems
Chapter
qxd
AM
Page
Like
unsigned
numbers
N
bit
two
s
complement
numbers
represent
one
of
N
possible
values
However
the
values
are
split
between
positive
and
negative
numbers
For
example
a
bit
unsigned
number
represents
values
to
A
bit
two
s
complement
number
also
represents
values
to
In
general
the
range
of
an
N
bit
two
s
complement
number
spans
N
N
It
should
make
sense
that
there
is
one
more
negative
number
than
positive
number
because
there
is
no
The
most
negative
number
N
is
sometimes
called
the
weird
number
Its
two
s
complement
is
found
by
inverting
the
bits
producing
and
adding
which
produces
the
weird
number
again
Hence
this
negative
number
has
no
positive
counterpart
Adding
two
N
bit
positive
numbers
or
negative
numbers
may
cause
overflow
if
the
result
is
greater
than
N
or
less
than
N
Adding
a
positive
number
to
a
negative
number
never
causes
overflow
Unlike
unsigned
numbers
a
carry
out
of
the
most
significant
column
does
not
indicate
overflow
Instead
overflow
occurs
if
the
two
numbers
being
added
have
the
same
sign
bit
and
the
result
has
the
opposite
sign
bit
Example
ADDING
TWO
S
COMPLEMENT
NUMBERS
WITH
OVERFLOW
Compute
a
using
bit
two
s
complement
numbers
Does
the
result
overflow
Solution
a
The
result
overflows
the
range
of
bit
positive
two
s
complement
numbers
producing
an
incorrect
negative
result
If
the
computation
had
been
done
using
five
or
more
bits
the
result
would
have
been
correct
When
a
two
s
complement
number
is
extended
to
more
bits
the
sign
bit
must
be
copied
into
the
most
significant
bit
positions
This
process
is
called
sign
extension
For
example
the
numbers
and
are
written
as
bit
two
s
complement
numbers
and
respectively
They
are
sign
extended
to
seven
bits
by
copying
the
sign
bit
into
the
three
new
upper
bits
to
form
and
respectively
Comparison
of
Number
Systems
The
three
most
commonly
used
binary
number
systems
are
unsigned
two
s
complement
and
sign
magnitude
Table
compares
the
range
of
N
bit
numbers
in
each
of
these
three
systems
Two
s
complement
numbers
are
convenient
because
they
represent
both
positive
and
negative
integers
and
because
ordinary
addition
works
for
all
numbers
CHAPTER
ONE
From
Zero
to
One
Chapter
qxd
Subtraction
is
performed
by
negating
the
second
number
i
e
taking
the
two
s
complement
and
then
adding
Unless
stated
otherwise
assume
that
all
signed
binary
numbers
use
two
s
complement
representation
Figure
shows
a
number
line
indicating
the
values
of
bit
numbers
in
each
system
Unsigned
numbers
span
the
range
in
regular
binary
order
Two
s
complement
numbers
span
the
range
The
nonnegative
numbers
share
the
same
encodings
as
unsigned
numbers
The
negative
numbers
are
encoded
such
that
a
larger
unsigned
binary
value
represents
a
number
closer
to
Notice
that
the
weird
number
represents
and
has
no
positive
counterpart
Sign
magnitude
numbers
span
the
range
The
most
significant
bit
is
the
sign
bit
The
positive
numbers
share
the
same
encodings
as
unsigned
numbers
The
negative
numbers
are
symmetric
but
have
the
sign
bit
set
is
represented
by
both
and
Thus
N
bit
sign
magnitude
numbers
represent
only
N
integers
because
of
the
two
representations
for
LOGIC
GATES
Now
that
we
know
how
to
use
binary
variables
to
represent
information
we
explore
digital
systems
that
perform
operations
on
these
binary
variables
Logic
gates
are
simple
digital
circuits
that
take
one
or
more
binary
inputs
and
produce
a
binary
output
Logic
gates
are
drawn
with
a
symbol
showing
the
input
or
inputs
and
the
output
Inputs
are
Logic
Gates
Two
s
Complement
Sign
Magnitude
Unsigned
Table
Range
of
N
bit
numbers
System
Range
Unsigned
N
Sign
Magnitude
N
N
Two
s
Complement
N
N
Figure
Number
line
and
bit
binary
encodings
Chapter
qxd
usually
drawn
on
the
left
or
top
and
outputs
on
the
right
or
bottom
Digital
designers
typically
use
letters
near
the
beginning
of
the
alphabet
for
gate
inputs
and
the
letter
Y
for
the
gate
output
The
relationship
between
the
inputs
and
the
output
can
be
described
with
a
truth
table
or
a
Boolean
equation
A
truth
table
lists
inputs
on
the
left
and
the
corresponding
output
on
the
right
It
has
one
row
for
each
possible
combination
of
inputs
A
Boolean
equation
is
a
mathematical
expression
using
binary
variables
NOT
Gate
A
NOT
gate
has
one
input
A
and
one
output
Y
as
shown
in
Figure
The
NOT
gate
s
output
is
the
inverse
of
its
input
If
A
is
FALSE
then
Y
is
TRUE
If
A
is
TRUE
then
Y
is
FALSE
This
relationship
is
summarized
by
the
truth
table
and
Boolean
equation
in
the
figure
The
line
over
A
in
the
Boolean
equation
is
pronounced
NOT
so
Y
A
is
read
Y
equals
NOT
A
The
NOT
gate
is
also
called
an
inverter
Other
texts
use
a
variety
of
notations
for
NOT
including
Y
A
Y
A
Y
A
or
Y
A
We
will
use
Y
A
exclusively
but
don
t
be
puzzled
if
you
encounter
another
notation
elsewhere
Buffer
The
other
one
input
logic
gate
is
called
a
buffer
and
is
shown
in
Figure
It
simply
copies
the
input
to
the
output
From
the
logical
point
of
view
a
buffer
is
no
different
from
a
wire
so
it
might
seem
useless
However
from
the
analog
point
of
view
the
buffer
might
have
desirable
characteristics
such
as
the
ability
to
deliver
large
amounts
of
current
to
a
motor
or
the
ability
to
quickly
send
its
output
to
many
gates
This
is
an
example
of
why
we
need
to
consider
multiple
levels
of
abstraction
to
fully
understand
a
system
the
digital
abstraction
hides
the
real
purpose
of
a
buffer
The
triangle
symbol
indicates
a
buffer
A
circle
on
the
output
is
called
a
bubble
and
indicates
inversion
as
was
seen
in
the
NOT
gate
symbol
of
Figure
AND
Gate
Two
input
logic
gates
are
more
interesting
The
AND
gate
shown
in
Figure
produces
a
TRUE
output
Y
if
and
only
if
both
A
and
B
are
TRUE
Otherwise
the
output
is
FALSE
By
convention
the
inputs
are
listed
in
the
order
as
if
you
were
counting
in
binary
The
Boolean
equation
for
an
AND
gate
can
be
written
in
several
ways
Y
A
B
Y
AB
or
Y
A
B
The
symbol
is
pronounced
intersection
and
is
preferred
by
logicians
We
prefer
Y
AB
read
Y
equals
A
and
B
because
we
are
lazy
CHAPTER
ONE
From
Zero
to
One
NOT
Y
A
A
Y
A
Y
BUF
Y
A
A
Y
A
Y
AND
Y
AB
ABY
A
B
Y
Figure
NOT
gate
Figure
AND
gate
Figure
Buffer
According
to
Larry
Wall
inventor
of
the
Perl
programming
language
the
three
principal
virtues
of
a
programmer
are
Laziness
Impatience
and
Hubris
Chapter
q
OR
Gate
The
OR
gate
shown
in
Figure
produces
a
TRUE
output
Y
if
either
A
or
B
or
both
are
TRUE
The
Boolean
equation
for
an
OR
gate
is
written
as
Y
A
B
or
Y
A
B
The
symbol
is
pronounced
union
and
is
preferred
by
logicians
Digital
designers
normally
use
the
notation
Y
A
B
is
pronounced
Y
equals
A
or
B
Other
Two
Input
Gates
Figure
shows
other
common
two
input
logic
gates
XOR
exclusive
OR
pronounced
ex
OR
is
TRUE
if
A
or
B
but
not
both
are
TRUE
Any
gate
can
be
followed
by
a
bubble
to
invert
its
operation
The
NAND
gate
performs
NOT
AND
Its
output
is
TRUE
unless
both
inputs
are
TRUE
The
NOR
gate
performs
NOT
OR
Its
output
is
TRUE
if
neither
A
nor
B
is
TRUE
Example
XNOR
GATE
Figure
shows
the
symbol
and
Boolean
equation
for
a
two
input
XNOR
gate
that
performs
the
inverse
of
an
XOR
Complete
the
truth
table
Solution
Figure
shows
the
truth
table
The
XNOR
output
is
TRUE
if
both
inputs
are
FALSE
or
both
inputs
are
TRUE
The
two
input
XNOR
gate
is
sometimes
called
an
equality
gate
because
its
output
is
TRUE
when
the
inputs
are
equal
Multiple
Input
Gates
Many
Boolean
functions
of
three
or
more
inputs
exist
The
most
common
are
AND
OR
XOR
NAND
NOR
and
XNOR
An
N
input
AND
gate
produces
a
TRUE
output
when
all
N
inputs
are
TRUE
An
N
input
OR
gate
produces
a
TRUE
output
when
at
least
one
input
is
TRUE
Logic
Gates
OR
Y
A
B
ABY
A
B
Y
Figure
OR
gate
XOR
NAND
NOR
Y
A
B
Y
AB
Y
A
B
ABY
ABY
ABY
A
B
Y
A
B
Y
A
B
Y
Figure
More
two
input
logic
gates
A
silly
way
to
remember
the
OR
symbol
is
that
it
s
input
side
is
curved
like
Pacman
s
mouth
so
the
gate
is
hungry
and
willing
to
eat
any
TRUE
inputs
it
can
find
Figure
XNOR
gate
XNOR
Y
A
B
ABY
A
B
Y
Chapt
An
N
input
XOR
gate
is
sometimes
called
a
parity
gate
and
produces
a
TRUE
output
if
an
odd
number
of
inputs
are
TRUE
As
with
two
input
gates
the
input
combinations
in
the
truth
table
are
listed
in
counting
order
Example
THREE
INPUT
NOR
GATE
Figure
shows
the
symbol
and
Boolean
equation
for
a
three
input
NOR
gate
Complete
the
truth
table
Solution
Figure
shows
the
truth
table
The
output
is
TRUE
only
if
none
of
the
inputs
are
TRUE
Example
FOUR
INPUT
AND
GATE
Figure
shows
the
symbol
and
Boolean
equation
for
a
four
input
AND
gate
Create
a
truth
table
Solution
Figure
shows
the
truth
table
The
output
is
TRUE
only
if
all
of
the
inputs
are
TRUE
BENEATH
THE
DIGITAL
ABSTRACTION
A
digital
system
uses
discrete
valued
variables
However
the
variables
are
represented
by
continuous
physical
quantities
such
as
the
voltage
on
a
wire
the
position
of
a
gear
or
the
level
of
fluid
in
a
cylinder
Hence
the
designer
must
choose
a
way
to
relate
the
continuous
value
to
the
discrete
value
For
example
consider
representing
a
binary
signal
A
with
a
voltage
on
a
wire
Let
volts
V
indicate
A
and
V
indicate
A
Any
real
system
must
tolerate
some
noise
so
V
probably
ought
to
be
interpreted
as
A
as
well
But
what
about
V
Or
V
Or
V
Supply
Voltage
Suppose
the
lowest
voltage
in
the
system
is
V
also
called
ground
or
GND
The
highest
voltage
in
the
system
comes
from
the
power
supply
and
is
usually
called
VDD
In
s
and
s
technology
VDD
was
generally
V
As
chips
have
progressed
to
smaller
transistors
VDD
has
dropped
to
V
V
V
V
V
or
even
lower
to
save
power
and
avoid
overloading
the
transistors
Logic
Levels
The
mapping
of
a
continuous
variable
onto
a
discrete
binary
variable
is
done
by
defining
logic
levels
as
shown
in
Figure
The
first
gate
is
called
the
driver
and
the
second
gate
is
called
the
receiver
The
output
of
CHAPTER
ONE
From
Zero
to
One
Figure
Three
input
NOR
gate
Figure
Three
input
NOR
truth
table
Figure
Four
input
AND
gate
NOR
Y
A
B
C
BCY
A
B
Y
C
A
BCY
A
AND
Y
ABCD
A
B
C
Y
D
Figure
XNOR
truth
table
ABY
Cha
the
driver
is
connected
to
the
input
of
the
receiver
The
driver
produces
a
LOW
output
in
the
range
of
to
VOL
or
a
HIGH
output
in
the
range
of
VOH
to
VDD
If
the
receiver
gets
an
input
in
the
range
of
to
VIL
it
will
consider
the
input
to
be
LOW
If
the
receiver
gets
an
input
in
the
range
of
VIH
to
VDD
it
will
consider
the
input
to
be
HIGH
If
for
some
reason
such
as
noise
or
faulty
components
the
receiver
s
input
should
fall
in
the
forbidden
zone
between
VIL
and
VIH
the
behavior
of
the
gate
is
unpredictable
VOH
VOL
VIH
and
VIL
are
called
the
output
and
input
high
and
low
logic
levels
Noise
Margins
If
the
output
of
the
driver
is
to
be
correctly
interpreted
at
the
input
of
the
receiver
we
must
choose
VOL
VIL
and
VOH
VIH
Thus
even
if
the
output
of
the
driver
is
contaminated
by
some
noise
the
input
of
the
receiver
will
still
detect
the
correct
logic
level
The
noise
margin
is
the
amount
of
noise
that
could
be
added
to
a
worst
case
output
such
that
the
signal
can
still
be
interpreted
as
a
valid
input
As
can
be
seen
in
Figure
the
low
and
high
noise
margins
are
respectively
NML
VIL
VOL
NMH
VOH
VIH
Example
Consider
the
inverter
circuit
of
Figure
VO
is
the
output
voltage
of
inverter
I
and
VI
is
the
input
voltage
of
inverter
I
Both
inverters
have
the
following
characteristics
VDD
V
VIL
V
VIH
V
VOL
V
and
VOH
V
What
are
the
inverter
low
and
high
noise
margins
Can
the
circuit
tolerate
V
of
noise
between
VO
and
VI
Solution
The
inverter
noise
margins
are
NML
VIL
VOL
V
V
V
NMH
VOH
VIH
V
V
V
The
circuit
can
tolerate
V
of
noise
when
the
output
is
LOW
NML
V
but
not
when
the
output
is
HIGH
NMH
V
For
example
suppose
the
driver
I
outputs
its
worst
case
HIGH
value
VO
VOH
V
If
noise
causes
the
voltage
to
droop
by
V
before
reaching
the
input
of
the
receiver
VI
V
V
V
This
is
less
than
the
acceptable
input
HIGH
value
VIH
V
so
the
receiver
may
not
sense
a
proper
HIGH
input
DC
Transfer
Characteristics
To
understand
the
limits
of
the
digital
abstraction
we
must
delve
into
the
analog
behavior
of
a
gate
The
DC
transfer
characteristics
of
a
gate
describe
the
output
voltage
as
a
function
of
the
input
voltage
when
the
Beneath
the
Digital
Abstraction
Figure
Four
input
AND
truth
table
BDY
C
A
VDD
stands
for
the
voltage
on
the
drain
of
a
metal
oxidesemiconductor
transistor
used
to
build
most
modern
chips
The
power
supply
voltage
is
also
sometimes
called
VCC
standing
for
the
voltage
on
the
collector
of
a
bipolar
transistor
used
to
build
chips
in
an
older
technology
Ground
is
sometimes
called
VSS
because
it
is
the
voltage
on
the
source
of
a
metal
oxidesemiconductor
transistor
See
Section
for
more
information
on
transistors
Chapter
qxd
input
is
changed
slowly
enough
that
the
output
can
keep
up
They
are
called
transfer
characteristics
because
they
describe
the
relationship
between
input
and
output
voltages
An
ideal
inverter
would
have
an
abrupt
switching
threshold
at
VDD
as
shown
in
Figure
a
For
V
A
VDD
V
Y
VDD
For
V
A
VDD
V
Y
In
such
a
case
VIH
VIL
VDD
VOH
VDD
and
VOL
A
real
inverter
changes
more
gradually
between
the
extremes
as
shown
in
Figure
b
When
the
input
voltage
V
A
is
the
output
voltage
V
Y
VDD
When
V
A
VDD
V
Y
However
the
transition
between
these
endpoints
is
smooth
and
may
not
be
centered
at
exactly
VDD
This
raises
the
question
of
how
to
define
the
logic
levels
A
reasonable
place
to
choose
the
logic
levels
is
where
the
slope
of
the
transfer
characteristic
dV
Y
dV
A
is
These
two
points
are
called
the
unity
gain
points
Choosing
logic
levels
at
the
unity
gain
points
usually
maximizes
the
noise
margins
If
VIL
were
reduced
VOH
would
only
increase
by
a
small
amount
But
if
VIL
were
increased
VOH
would
drop
precipitously
The
Static
Discipline
To
avoid
inputs
falling
into
the
forbidden
zone
digital
logic
gates
are
designed
to
conform
to
the
static
discipline
The
static
discipline
requires
that
given
logically
valid
inputs
every
circuit
element
will
produce
logically
valid
outputs
By
conforming
to
the
static
discipline
digital
designers
sacrifice
the
freedom
of
using
arbitrary
analog
circuit
elements
in
return
for
the
CHAPTER
ONE
From
Zero
to
One
I
I
Noise
VO
VI
Figure
Inverter
circuit
DC
indicates
behavior
when
an
input
voltage
is
held
constant
or
changes
slowly
enough
for
the
rest
of
the
system
to
keep
up
The
term
s
historical
root
comes
from
direct
current
a
method
of
transmitting
power
across
a
line
with
a
constant
voltage
In
contrast
the
transient
response
of
a
circuit
is
the
behavior
when
an
input
voltage
changes
rapidly
Section
explores
transient
response
further
Forbidden
Zone
NML
NMH
Output
Characteristics
Input
Characteristics
VOH
VDD
VOL
GND
VIH
VIL
Logic
High
Input
Range
Logic
Low
Input
Range
Logic
High
Output
Range
Logic
Low
Output
Range
Driver
Receiver
Figure
Logic
levels
and
noise
margins
Chapter
simplicity
and
robustness
of
digital
circuits
They
raise
the
level
of
abstraction
from
analog
to
digital
increasing
design
productivity
by
hiding
needless
detail
The
choice
of
VDD
and
logic
levels
is
arbitrary
but
all
gates
that
communicate
must
have
compatible
logic
levels
Therefore
gates
are
grouped
into
logic
families
such
that
all
gates
in
a
logic
family
obey
the
static
discipline
when
used
with
other
gates
in
the
family
Logic
gates
in
the
same
logic
family
snap
together
like
Legos
in
that
they
use
consistent
power
supply
voltages
and
logic
levels
Four
major
logic
families
that
predominated
from
the
s
through
the
s
are
Transistor
Transistor
Logic
TTL
Complementary
MetalOxide
Semiconductor
Logic
CMOS
pronounced
sea
moss
Low
Voltage
TTL
Logic
LVTTL
and
Low
Voltage
CMOS
Logic
LVCMOS
Their
logic
levels
are
compared
in
Table
Since
then
logic
families
have
balkanized
with
a
proliferation
of
even
lower
power
supply
voltages
Appendix
A
revisits
popular
logic
families
in
more
detail
Beneath
the
Digital
Abstraction
VDD
V
A
V
Y
VOH
VDD
VOL
VIL
VIH
A
Y
VDD
V
A
V
Y
VOH
VDD
VOL
VIL
VIH
Unity
Gain
Points
Slope
a
b
VDD
Figure
DC
transfer
characteristics
and
logic
levels
Table
Logic
levels
of
V
and
V
logic
families
Logic
Family
VDD
VIL
VIH
VOL
VOH
TTL
CMOS
LVTTL
LVCMOS
Example
LOGIC
FAMILY
COMPATIBILITY
Which
of
the
logic
families
in
Table
can
communicate
with
each
other
reliably
Solution
Table
lists
which
logic
families
have
compatible
logic
levels
Note
that
a
V
logic
family
such
as
TTL
or
CMOS
may
produce
an
output
voltage
as
HIGH
as
V
If
this
V
signal
drives
the
input
of
a
V
logic
family
such
as
LVTTL
or
LVCMOS
it
can
damage
the
receiver
unless
the
receiver
is
specially
designed
to
be
volt
compatible
CMOS
TRANSISTORS
This
section
and
other
sections
marked
with
a
are
optional
and
are
not
necessary
to
understand
the
main
flow
of
the
book
Babbage
s
Analytical
Engine
was
built
from
gears
and
early
electrical
computers
used
relays
or
vacuum
tubes
Modern
computers
use
transistors
because
they
are
cheap
small
and
reliable
Transistors
are
electrically
controlled
switches
that
turn
ON
or
OFF
when
a
voltage
or
current
is
applied
to
a
control
terminal
The
two
main
types
of
transistors
are
bipolar
transistors
and
metal
oxide
semiconductor
field
effect
transistors
MOSFETs
or
MOS
transistors
pronounced
moss
fets
or
M
O
S
respectively
In
Jack
Kilby
at
Texas
Instruments
built
the
first
integrated
circuit
containing
two
transistors
In
Robert
Noyce
at
Fairchild
Semiconductor
patented
a
method
of
interconnecting
multiple
transistors
on
a
single
silicon
chip
At
the
time
transistors
cost
about
each
Thanks
to
more
than
three
decades
of
unprecedented
manufacturing
advances
engineers
can
now
pack
roughly
one
billion
MOSFETs
onto
a
cm
chip
of
silicon
and
these
transistors
cost
less
than
microcents
apiece
The
capacity
and
cost
continue
to
improve
by
an
order
of
magnitude
every
years
or
so
MOSFETs
are
now
the
building
blocks
of
CHAPTER
ONE
From
Zero
to
One
Table
Compatibility
of
logic
families
Receiver
TTL
CMOS
LVTTL
LVCMOS
Driver
TTL
OK
NO
VOH
VIH
MAYBEa
MAYBEa
CMOS
OK
OK
MAYBEa
MAYBEa
LVTTL
OK
NO
VOH
VIH
OK
OK
LVCMOS
OK
NO
VOH
VIH
OK
OK
a
As
long
as
a
V
HIGH
level
does
not
damage
the
receiver
input
Robert
Noyce
Born
in
Burlington
Iowa
Received
a
B
A
in
physics
from
Grinnell
College
and
a
Ph
D
in
physics
from
MIT
Nicknamed
Mayor
of
Silicon
Valley
for
his
profound
influence
on
the
industry
Cofounded
Fairchild
Semiconductor
in
and
Intel
in
Coinvented
the
integrated
circuit
Many
engineers
from
his
teams
went
on
to
found
other
seminal
semiconductor
companies
Intel
Corporation
Reproduced
by
permission
almost
all
digital
systems
In
this
section
we
will
peer
beneath
the
digital
abstraction
to
see
how
logic
gates
are
built
from
MOSFETs
Semiconductors
MOS
transistors
are
built
from
silicon
the
predominant
atom
in
rock
and
sand
Silicon
Si
is
a
group
IV
atom
so
it
has
four
electrons
in
its
valence
shell
and
forms
bonds
with
four
adjacent
atoms
resulting
in
a
crystalline
lattice
Figure
a
shows
the
lattice
in
two
dimensions
for
ease
of
drawing
but
remember
that
the
lattice
actually
forms
a
cubic
crystal
In
the
figure
a
line
represents
a
covalent
bond
By
itself
silicon
is
a
poor
conductor
because
all
the
electrons
are
tied
up
in
covalent
bonds
However
it
becomes
a
better
conductor
when
small
amounts
of
impurities
called
dopant
atoms
are
carefully
added
If
a
group
V
dopant
such
as
arsenic
As
is
added
the
dopant
atoms
have
an
extra
electron
that
is
not
involved
in
the
bonds
The
electron
can
easily
move
about
the
lattice
leaving
an
ionized
dopant
atom
As
behind
as
shown
in
Figure
b
The
electron
carries
a
negative
charge
so
we
call
arsenic
an
n
type
dopant
On
the
other
hand
if
a
group
III
dopant
such
as
boron
B
is
added
the
dopant
atoms
are
missing
an
electron
as
shown
in
Figure
c
This
missing
electron
is
called
a
hole
An
electron
from
a
neighboring
silicon
atom
may
move
over
to
fill
the
missing
bond
forming
an
ionized
dopant
atom
B
and
leaving
a
hole
at
the
neighboring
silicon
atom
In
a
similar
fashion
the
hole
can
migrate
around
the
lattice
The
hole
is
a
lack
of
negative
charge
so
it
acts
like
a
positively
charged
particle
Hence
we
call
boron
a
p
type
dopant
Because
the
conductivity
of
silicon
changes
over
many
orders
of
magnitude
depending
on
the
concentration
of
dopants
silicon
is
called
a
semiconductor
Diodes
The
junction
between
p
type
and
n
type
silicon
is
called
a
diode
The
p
type
region
is
called
the
anode
and
the
n
type
region
is
called
the
cathode
as
illustrated
in
Figure
When
the
voltage
on
the
anode
rises
above
the
voltage
on
the
cathode
the
diode
is
forward
biased
and
CMOS
Transistors
Si
Si
Si
Si
Si
Si
Si
Si
Si
a
Si
As
Si
Si
Si
Si
Si
Si
Si
b
Free
electron
Si
B
Si
Si
Si
Si
Si
Si
Si
c
Free
hole
Figure
Silicon
lattice
and
dopant
atoms
C
current
flows
through
the
diode
from
the
anode
to
the
cathode
But
when
the
anode
voltage
is
lower
than
the
voltage
on
the
cathode
the
diode
is
reverse
biased
and
no
current
flows
The
diode
symbol
intuitively
shows
that
current
only
flows
in
one
direction
Capacitors
A
capacitor
consists
of
two
conductors
separated
by
an
insulator
When
a
voltage
V
is
applied
to
one
of
the
conductors
the
conductor
accumulates
electric
charge
Q
and
the
other
conductor
accumulates
the
opposite
charge
Q
The
capacitance
C
of
the
capacitor
is
the
ratio
of
charge
to
voltage
C
Q
V
The
capacitance
is
proportional
to
the
size
of
the
conductors
and
inversely
proportional
the
distance
between
them
The
symbol
for
a
capacitor
is
shown
in
Figure
Capacitance
is
important
because
charging
or
discharging
a
conductor
takes
time
and
energy
More
capacitance
means
that
a
circuit
will
be
slower
and
require
more
energy
to
operate
Speed
and
energy
will
be
discussed
throughout
this
book
nMOS
and
pMOS
Transistors
A
MOSFET
is
a
sandwich
of
several
layers
of
conducting
and
insulating
materials
MOSFETs
are
built
on
thin
flat
wafers
of
silicon
of
about
to
cm
in
diameter
The
manufacturing
process
begins
with
a
bare
wafer
The
process
involves
a
sequence
of
steps
in
which
dopants
are
implanted
into
the
silicon
thin
films
of
silicon
dioxide
and
silicon
are
grown
and
metal
is
deposited
Between
each
step
the
wafer
is
patterned
so
that
the
materials
appear
only
where
they
are
desired
Because
transistors
are
a
fraction
of
a
micron
in
length
and
the
entire
wafer
is
processed
at
once
it
is
inexpensive
to
manufacture
billions
of
transistors
at
a
time
Once
processing
is
complete
the
wafer
is
cut
into
rectangles
called
chips
or
dice
that
contain
thousands
millions
or
even
billions
of
transistors
The
chip
is
tested
then
placed
in
a
plastic
or
ceramic
package
with
metal
pins
to
connect
it
to
a
circuit
board
The
MOSFET
sandwich
consists
of
a
conducting
layer
called
the
gate
on
top
of
an
insulating
layer
of
silicon
dioxide
SiO
on
top
of
the
silicon
wafer
called
the
substrate
Historically
the
gate
was
constructed
from
metal
hence
the
name
metal
oxide
semiconductor
Modern
manufacturing
processes
use
polycrystalline
silicon
for
the
gate
because
it
does
not
melt
during
subsequent
high
temperature
processing
steps
Silicon
dioxide
is
better
known
as
glass
and
is
often
simply
called
oxide
in
the
semiconductor
industry
The
metal
oxide
semiconductor
sandwich
forms
a
capacitor
in
which
a
thin
layer
of
insulating
oxide
called
a
dielectric
separates
the
metal
and
semiconductor
plates
CHAPTER
ONE
From
Zero
to
One
p
type
n
type
anode
cathode
Figure
The
p
n
junction
diode
structure
and
symbol
C
Figure
Capacitor
symbol
Technicians
in
an
Intel
clean
room
wear
Gore
Tex
bunny
suits
to
prevent
particulates
from
their
hair
skin
and
clothing
from
contaminating
the
microscopic
transistors
on
silicon
wafers
Intel
Corporation
Reproduced
by
permission
A
pin
dual
inline
package
DIP
contains
a
small
chip
scarcely
visible
in
the
center
that
is
connected
to
metal
pins
on
a
side
by
gold
wires
thinner
than
a
strand
of
hair
photograph
by
Kevin
Mapp
Harvey
Mudd
College
m
micron
m
Chapt
There
are
two
flavors
of
MOSFETs
nMOS
and
pMOS
pronounced
n
moss
and
p
moss
Figure
shows
cross
sections
of
each
type
made
by
sawing
through
a
wafer
and
looking
at
it
from
the
side
The
n
type
transistors
called
nMOS
have
regions
of
n
type
dopants
adjacent
to
the
gate
called
the
source
and
the
drain
and
are
built
on
a
p
type
semiconductor
substrate
The
pMOS
transistors
are
just
the
opposite
consisting
of
p
type
source
and
drain
regions
in
an
n
type
substrate
A
MOSFET
behaves
as
a
voltage
controlled
switch
in
which
the
gate
voltage
creates
an
electric
field
that
turns
ON
or
OFF
a
connection
between
the
source
and
drain
The
term
field
effect
transistor
comes
from
this
principle
of
operation
Let
us
start
by
exploring
the
operation
of
an
nMOS
transistor
The
substrate
of
an
nMOS
transistor
is
normally
tied
to
GND
the
lowest
voltage
in
the
system
First
consider
the
situation
when
the
gate
is
also
at
V
as
shown
in
Figure
a
The
diodes
between
the
source
or
drain
and
the
substrate
are
reverse
biased
because
the
source
or
drain
voltage
is
nonnegative
Hence
there
is
no
path
for
current
to
flow
between
the
source
and
drain
so
the
transistor
is
OFF
Now
consider
when
the
gate
is
raised
to
VDD
as
shown
in
Figure
b
When
a
positive
voltage
is
applied
to
the
top
plate
of
a
capacitor
it
establishes
an
electric
field
that
attracts
positive
charge
on
the
top
plate
and
negative
charge
to
the
bottom
plate
If
the
voltage
is
sufficiently
large
so
much
negative
charge
is
attracted
to
the
underside
of
the
gate
that
the
region
inverts
from
p
type
to
effectively
become
n
type
This
inverted
region
is
called
the
channel
Now
the
transistor
has
a
continuous
path
from
the
n
type
source
through
the
n
type
channel
to
the
n
type
drain
so
electrons
can
flow
from
source
to
drain
The
transistor
is
ON
The
gate
voltage
required
to
turn
on
a
transistor
is
called
the
threshold
voltage
Vt
and
is
typically
to
V
CMOS
Transistors
n
p
source
gate
drain
substrate
SiO
n
source
gate
drain
Polysilicon
n
p
p
gate
source
drain
gate
source
drain
substrate
a
nMOS
b
pMOS
Figure
nMOS
and
pMOS
transistors
The
source
and
drain
terminals
are
physically
symmetric
However
we
say
that
charge
flows
from
the
source
to
the
drain
In
an
nMOS
transistor
the
charge
is
carried
by
electrons
which
flow
from
negative
voltage
to
positive
voltage
In
a
pMOS
transistor
the
charge
is
carried
by
holes
which
flow
from
positive
voltage
to
negative
voltage
If
we
draw
schematics
with
the
most
positive
voltage
at
the
top
and
the
most
negative
at
the
bottom
the
source
of
negative
charges
in
an
nMOS
transistor
is
the
bottom
terminal
and
the
source
of
positive
charges
in
a
pMOS
transistor
is
the
top
terminal
A
technician
holds
a
inch
wafer
containing
hundreds
of
microprocessor
chips
Intel
Corporation
Reproduced
by
permission
pMOS
transistors
work
in
just
the
opposite
fashion
as
might
be
guessed
from
the
bubble
on
their
symbol
The
substrate
is
tied
to
VDD
When
the
gate
is
also
at
VDD
the
pMOS
transistor
is
OFF
When
the
gate
is
at
GND
the
channel
inverts
to
p
type
and
the
pMOS
transistor
is
ON
Unfortunately
MOSFETs
are
not
perfect
switches
In
particular
nMOS
transistors
pass
s
well
but
pass
s
poorly
Specifically
when
the
gate
of
an
nMOS
transistor
is
at
VDD
the
drain
will
only
swing
between
and
VDD
Vt
Similarly
pMOS
transistors
pass
s
well
but
s
poorly
However
we
will
see
that
it
is
possible
to
build
logic
gates
that
use
transistors
only
in
their
good
mode
nMOS
transistors
need
a
p
type
substrate
and
pMOS
transistors
need
an
n
type
substrate
To
build
both
flavors
of
transistors
on
the
same
chip
manufacturing
processes
typically
start
with
a
p
type
wafer
then
implant
n
type
regions
called
wells
where
the
pMOS
transistors
should
go
These
processes
that
provide
both
flavors
of
transistors
are
called
Complementary
MOS
or
CMOS
CMOS
processes
are
used
to
build
the
vast
majority
of
all
transistors
fabricated
today
In
summary
CMOS
processes
give
us
two
types
of
electrically
controlled
switches
as
shown
in
Figure
The
voltage
at
the
gate
g
regulates
the
flow
of
current
between
the
source
s
and
drain
d
nMOS
transistors
are
OFF
when
the
gate
is
and
ON
when
the
gate
is
CHAPTER
ONE
From
Zero
to
One
n
p
gate
source
drain
substrate
n
a
GND
GND
n
p
source
gate
drain
substrate
n
b
VDD
GND
channel
Figure
nMOS
transistor
operation
Figure
Switch
models
of
MOSFETs
g
s
d
g
d
s
nMOS
pMOS
g
s
d
d
s
OFF
ON
g
s
d
d
s
ON
OFF
Gordon
Moore
Born
in
San
Francisco
Received
a
B
S
in
chemistry
from
UC
Berkeley
and
a
Ph
D
in
chemistry
and
physics
from
Caltech
Cofounded
Intel
in
with
Robert
Noyce
Observed
in
that
the
number
of
transistors
on
a
computer
chip
doubles
every
year
This
trend
has
become
known
as
Moore
s
Law
Since
transistor
counts
have
doubled
every
two
years
A
corollary
of
Moore
s
Law
is
that
microprocessor
performance
doubles
every
to
months
Semiconductor
sales
have
also
increased
exponentially
Unfortunately
power
consumption
has
increased
exponentially
as
well
Intel
Corporation
Reproduced
by
permission
C
pMOS
transistors
are
just
the
opposite
ON
when
the
gate
is
and
OFF
when
the
gate
is
CMOS
NOT
Gate
Figure
shows
a
schematic
of
a
NOT
gate
built
with
CMOS
transistors
The
triangle
indicates
GND
and
the
flat
bar
indicates
VDD
these
labels
will
be
omitted
from
future
schematics
The
nMOS
transistor
N
is
connected
between
GND
and
the
Y
output
The
pMOS
transistor
P
is
connected
between
VDD
and
the
Y
output
Both
transistor
gates
are
controlled
by
the
input
A
If
A
N
is
OFF
and
P
is
ON
Hence
Y
is
connected
to
VDD
but
not
to
GND
and
is
pulled
up
to
a
logic
P
passes
a
good
If
A
N
is
ON
and
P
is
OFF
and
Y
is
pulled
down
to
a
logic
N
passes
a
good
Checking
against
the
truth
table
in
Figure
we
see
that
the
circuit
is
indeed
a
NOT
gate
Other
CMOS
Logic
Gates
Figure
shows
a
schematic
of
a
two
input
NAND
gate
In
schematic
diagrams
wires
are
always
joined
at
three
way
junctions
They
are
joined
at
four
way
junctions
only
if
a
dot
is
shown
The
nMOS
transistors
N
and
N
are
connected
in
series
both
nMOS
transistors
must
be
ON
to
pull
the
output
down
to
GND
The
pMOS
transistors
P
and
P
are
in
parallel
only
one
pMOS
transistor
must
be
ON
to
pull
the
output
up
to
VDD
Table
lists
the
operation
of
the
pull
down
and
pull
up
networks
and
the
state
of
the
output
demonstrating
that
the
gate
does
function
as
a
NAND
For
example
when
A
and
B
N
is
ON
but
N
is
OFF
blocking
the
path
from
Y
to
GND
P
is
OFF
but
P
is
ON
creating
a
path
from
VDD
to
Y
Therefore
Y
is
pulled
up
to
Figure
shows
the
general
form
used
to
construct
any
inverting
logic
gate
such
as
NOT
NAND
or
NOR
nMOS
transistors
are
good
at
passing
s
so
a
pull
down
network
of
nMOS
transistors
is
placed
between
the
output
and
GND
to
pull
the
output
down
to
pMOS
transistors
are
CMOS
Transistors
Figure
NOT
gate
schematic
Figure
Two
input
NAND
gate
schematic
VDD
A
Y
GND
N
P
A
B
Y
N
N
P
P
Table
NAND
gate
operation
A
B
Pull
Down
Network
Pull
Up
Network
Y
OFF
ON
OFF
ON
OFF
ON
ON
OFF
pMOS
pull
up
network
output
inputs
nMOS
pull
down
network
Figure
General
form
of
an
inverting
logic
gate
Chap
good
at
passing
s
so
a
pull
up
network
of
pMOS
transistors
is
placed
between
the
output
and
VDD
to
pull
the
output
up
to
The
networks
may
consist
of
transistors
in
series
or
in
parallel
When
transistors
are
in
parallel
the
network
is
ON
if
either
transistor
is
ON
When
transistors
are
in
series
the
network
is
ON
only
if
both
transistors
are
ON
The
slash
across
the
input
wire
indicates
that
the
gate
may
receive
multiple
inputs
If
both
the
pull
up
and
pull
down
networks
were
ON
simultaneously
a
short
circuit
would
exist
between
VDD
and
GND
The
output
of
the
gate
might
be
in
the
forbidden
zone
and
the
transistors
would
consume
large
amounts
of
power
possibly
enough
to
burn
out
On
the
other
hand
if
both
the
pull
up
and
pull
down
networks
were
OFF
simultaneously
the
output
would
be
connected
to
neither
VDD
nor
GND
We
say
that
the
output
floats
Its
value
is
again
undefined
Floating
outputs
are
usually
undesirable
but
in
Section
we
will
see
how
they
can
occasionally
be
used
to
the
designer
s
advantage
In
a
properly
functioning
logic
gate
one
of
the
networks
should
be
ON
and
the
other
OFF
at
any
given
time
so
that
the
output
is
pulled
HIGH
or
LOW
but
not
shorted
or
floating
We
can
guarantee
this
by
using
the
rule
of
conduction
complements
When
nMOS
transistors
are
in
series
the
pMOS
transistors
must
be
in
parallel
When
nMOS
transistors
are
in
parallel
the
pMOS
transistors
must
be
in
series
Example
THREE
INPUT
NAND
SCHEMATIC
Draw
a
schematic
for
a
three
input
NAND
gate
using
CMOS
transistors
Solution
The
NAND
gate
should
produce
a
output
only
when
all
three
inputs
are
Hence
the
pull
down
network
should
have
three
nMOS
transistors
in
series
By
the
conduction
complements
rule
the
pMOS
transistors
must
be
in
parallel
Such
a
gate
is
shown
in
Figure
you
can
verify
the
function
by
checking
that
it
has
the
correct
truth
table
Example
TWO
INPUT
NOR
SCHEMATIC
Draw
a
schematic
for
a
two
input
NOR
gate
using
CMOS
transistors
Solution
The
NOR
gate
should
produce
a
output
if
either
input
is
Hence
the
pull
down
network
should
have
two
nMOS
transistors
in
parallel
By
the
conduction
complements
rule
the
pMOS
transistors
must
be
in
series
Such
a
gate
is
shown
in
Figure
Example
TWO
INPUT
AND
SCHEMATIC
Draw
a
schematic
for
a
two
input
AND
gate
CHAPTER
ONE
From
Zero
to
One
A
B
Y
C
A
B
Y
Figure
Three
input
NAND
gate
schematic
Figure
Two
input
NOR
gate
schematic
Experienced
designers
claim
that
electronic
devices
operate
because
they
contain
magic
smoke
They
confirm
this
theory
with
the
observation
that
if
the
magic
smoke
is
ever
let
out
of
the
device
it
ceases
to
work
Solution
It
is
impossible
to
build
an
AND
gate
with
a
single
CMOS
gate
However
building
NAND
and
NOT
gates
is
easy
Thus
the
best
way
to
build
an
AND
gate
using
CMOS
transistors
is
to
use
a
NAND
followed
by
a
NOT
as
shown
in
Figure
Transmission
Gates
At
times
designers
find
it
convenient
to
use
an
ideal
switch
that
can
pass
both
and
well
Recall
that
nMOS
transistors
are
good
at
passing
and
pMOS
transistors
are
good
at
passing
so
the
parallel
combination
of
the
two
passes
both
values
well
Figure
shows
such
a
circuit
called
a
transmission
gate
or
pass
gate
The
two
sides
of
the
switch
are
called
A
and
B
because
a
switch
is
bidirectional
and
has
no
preferred
input
or
output
side
The
control
signals
are
called
enables
EN
and
EN
When
EN
and
EN
both
transistors
are
OFF
Hence
the
transmission
gate
is
OFF
or
disabled
so
A
and
B
are
not
connected
When
EN
and
EN
the
transmission
gate
is
ON
or
enabled
and
any
logic
value
can
flow
between
A
and
B
Pseudo
nMOS
Logic
An
N
input
CMOS
NOR
gate
uses
N
nMOS
transistors
in
parallel
and
N
pMOS
transistors
in
series
Transistors
in
series
are
slower
than
transistors
in
parallel
just
as
resistors
in
series
have
more
resistance
than
resistors
in
parallel
Moreover
pMOS
transistors
are
slower
than
nMOS
transistors
because
holes
cannot
move
around
the
silicon
lattice
as
fast
as
electrons
Therefore
the
parallel
nMOS
transistors
are
fast
and
the
series
pMOS
transistors
are
slow
especially
when
many
are
in
series
Pseudo
nMOS
logic
replaces
the
slow
stack
of
pMOS
transistors
with
a
single
weak
pMOS
transistor
that
is
always
ON
as
shown
in
Figure
This
pMOS
transistor
is
often
called
a
weak
pull
up
The
physical
dimensions
of
the
pMOS
transistor
are
selected
so
that
the
pMOS
transistor
will
pull
the
output
Y
HIGH
weakly
that
is
only
if
none
of
the
nMOS
transistors
are
ON
But
if
any
nMOS
transistor
is
ON
it
overpowers
the
weak
pull
up
and
pulls
Y
down
close
enough
to
GND
to
produce
a
logic
The
advantage
of
pseudo
nMOS
logic
is
that
it
can
be
used
to
build
fast
NOR
gates
with
many
inputs
For
example
Figure
shows
a
pseudo
nMOS
four
input
NOR
Pseudo
nMOS
gates
are
useful
for
certain
memory
and
logic
arrays
discussed
in
Chapter
The
disadvantage
is
that
a
short
circuit
exists
between
VDD
and
GND
when
the
output
is
LOW
the
weak
pMOS
and
nMOS
transistors
are
both
ON
The
short
circuit
draws
continuous
power
so
pseudo
nMOS
logic
must
be
used
sparingly
CMOS
Transistors
Figure
Pseudo
nMOS
fourinput
NOR
gate
Figure
Two
input
AND
gate
schematic
Figure
Transmission
gate
Figure
Generic
pseudonMOS
gate
A
B
Y
A
B
EN
EN
Y
inputs
nMOS
pull
down
network
weak
A
B
Y
weak
C
D
Chap
Pseudo
nMOS
gates
got
their
name
from
the
s
when
manufacturing
processes
only
had
nMOS
transistors
A
weak
nMOS
transistor
was
used
to
pull
the
output
HIGH
because
pMOS
transistors
were
not
available
POWER
CONSUMPTION
Power
consumption
is
the
amount
of
energy
used
per
unit
time
Power
consumption
is
of
great
importance
in
digital
systems
The
battery
life
of
portable
systems
such
as
cell
phones
and
laptop
computers
is
limited
by
power
consumption
Power
is
also
significant
for
systems
that
are
plugged
in
because
electricity
costs
money
and
because
the
system
will
overheat
if
it
draws
too
much
power
Digital
systems
draw
both
dynamic
and
static
power
Dynamic
power
is
the
power
used
to
charge
capacitance
as
signals
change
between
and
Static
power
is
the
power
used
even
when
signals
do
not
change
and
the
system
is
idle
Logic
gates
and
the
wires
that
connect
them
have
capacitance
The
energy
drawn
from
the
power
supply
to
charge
a
capacitance
C
to
voltage
VDD
is
CVDD
If
the
voltage
on
the
capacitor
switches
at
frequency
f
i
e
f
times
per
second
it
charges
the
capacitor
f
times
and
discharges
it
f
times
per
second
Discharging
does
not
draw
energy
from
the
power
supply
so
the
dynamic
power
consumption
is
Pdynamic
CV
DDf
Electrical
systems
draw
some
current
even
when
they
are
idle
When
transistors
are
OFF
they
leak
a
small
amount
of
current
Some
circuits
such
as
the
pseudo
nMOS
gate
discussed
in
Section
have
a
path
from
VDD
to
GND
through
which
current
flows
continuously
The
total
static
current
IDD
is
also
called
the
leakage
current
or
the
quiescent
supply
current
flowing
between
VDD
and
GND
The
static
power
consumption
is
proportional
to
this
static
current
Pstatic
IDDVDD
Example
POWER
CONSUMPTION
A
particular
cell
phone
has
a
watt
hour
W
hr
battery
and
operates
at
V
Suppose
that
when
it
is
in
use
the
cell
phone
operates
at
MHz
and
the
average
amount
of
capacitance
in
the
chip
switching
at
any
given
time
is
nF
Farads
When
in
use
it
also
broadcasts
W
of
power
out
of
its
antenna
When
the
phone
is
not
in
use
the
dynamic
power
drops
to
almost
zero
because
the
signal
processing
is
turned
off
But
the
phone
also
draws
mA
of
quiescent
current
whether
it
is
in
use
or
not
Determine
the
battery
life
of
the
phone
a
if
it
is
not
being
used
and
b
if
it
is
being
used
continuously
CHAPTER
ONE
From
Zero
to
One
Cha
Solution
The
static
power
is
Pstatic
A
V
mW
If
the
phone
is
not
being
used
this
is
the
only
power
consumption
so
the
battery
life
is
Whr
W
hours
about
days
If
the
phone
is
being
used
the
dynamic
power
is
Pdynamic
F
V
Hz
W
Together
with
the
static
and
broadcast
power
the
total
active
power
is
W
W
W
W
so
the
battery
life
is
W
hr
W
hours
This
example
somewhat
oversimplifies
the
actual
operation
of
a
cell
phone
but
it
illustrates
the
key
ideas
of
power
consumption
SUMMARY
AND
A
LOOK
AHEAD
There
are
kinds
of
people
in
this
world
those
who
can
count
in
binary
and
those
who
can
t
This
chapter
has
introduced
principles
for
understanding
and
designing
complex
systems
Although
the
real
world
is
analog
digital
designers
discipline
themselves
to
use
a
discrete
subset
of
possible
signals
In
particular
binary
variables
have
just
two
states
and
also
called
FALSE
and
TRUE
or
LOW
and
HIGH
Logic
gates
compute
a
binary
output
from
one
or
more
binary
inputs
Some
of
the
common
logic
gates
are
NOT
TRUE
when
input
is
FALSE
AND
TRUE
when
all
inputs
are
TRUE
OR
TRUE
when
any
inputs
are
TRUE
XOR
TRUE
when
an
odd
number
of
inputs
are
TRUE
Logic
gates
are
commonly
built
from
CMOS
transistors
which
behave
as
electrically
controlled
switches
nMOS
transistors
turn
ON
when
the
gate
is
pMOS
transistors
turn
ON
when
the
gate
is
In
Chapters
through
we
continue
the
study
of
digital
logic
Chapter
addresses
combinational
logic
in
which
the
outputs
depend
only
on
the
current
inputs
The
logic
gates
introduced
already
are
examples
of
combinational
logic
You
will
learn
to
design
circuits
involving
multiple
gates
to
implement
a
relationship
between
inputs
and
outputs
specified
by
a
truth
table
or
Boolean
equation
Chapter
addresses
sequential
logic
in
which
the
outputs
depend
on
both
current
and
past
inputs
Registers
are
common
sequential
elements
that
remember
their
previous
input
Finite
state
machines
built
from
registers
and
combinational
logic
are
a
powerful
way
to
build
complicated
systems
in
a
systematic
fashion
We
also
study
timing
of
digital
systems
to
analyze
how
fast
the
systems
can
operate
Chapter
describes
hardware
description
languages
HDLs
HDLs
are
related
to
conventional
programming
languages
but
are
used
to
simulate
and
build
hardware
rather
than
software
Most
digital
systems
today
are
designed
with
HDLs
Verilog
Summary
and
a
Look
Ahead
Chapter
q
and
VHDL
are
the
two
prevalent
languages
and
they
are
covered
sideby
side
in
this
book
Chapter
studies
other
combinational
and
sequential
building
blocks
such
as
adders
multipliers
and
memories
Chapter
shifts
to
computer
architecture
It
describes
the
MIPS
processor
an
industry
standard
microprocessor
used
in
consumer
electronics
some
Silicon
Graphics
workstations
and
many
communications
systems
such
as
televisions
networking
hardware
and
wireless
links
The
MIPS
architecture
is
defined
by
its
registers
and
assembly
language
instruction
set
You
will
learn
to
write
programs
in
assembly
language
for
the
MIPS
processor
so
that
you
can
communicate
with
the
processor
in
its
native
language
Chapters
and
bridge
the
gap
between
digital
logic
and
computer
architecture
Chapter
investigates
microarchitecture
the
arrangement
of
digital
building
blocks
such
as
adders
and
registers
needed
to
construct
a
processor
In
that
chapter
you
learn
to
build
your
own
MIPS
processor
Indeed
you
learn
three
microarchitectures
illustrating
different
trade
offs
of
performance
and
cost
Processor
performance
has
increased
exponentially
requiring
ever
more
sophisticated
memory
systems
to
feed
the
insatiable
demand
for
data
Chapter
delves
into
memory
system
architecture
and
also
describes
how
computers
communicate
with
peripheral
devices
such
as
keyboards
and
printers
CHAPTER
ONE
From
Zero
to
One
Exercises
Exercises
Exercise
Explain
in
one
paragraph
at
least
three
levels
of
abstraction
that
are
used
by
a
biologists
studying
the
operation
of
cells
b
chemists
studying
the
composition
of
matter
Exercise
Explain
in
one
paragraph
how
the
techniques
of
hierarchy
modularity
and
regularity
may
be
used
by
a
automobile
designers
b
businesses
to
manage
their
operations
Exercise
Ben
Bitdiddle
is
building
a
house
Explain
how
he
can
use
the
principles
of
hierarchy
modularity
and
regularity
to
save
time
and
money
during
construction
Exercise
An
analog
voltage
is
in
the
range
of
V
If
it
can
be
measured
with
an
accuracy
of
mV
at
most
how
many
bits
of
information
does
it
convey
Exercise
A
classroom
has
an
old
clock
on
the
wall
whose
minute
hand
broke
off
a
If
you
can
read
the
hour
hand
to
the
nearest
minutes
how
many
bits
of
information
does
the
clock
convey
about
the
time
b
If
you
know
whether
it
is
before
or
after
noon
how
many
additional
bits
of
information
do
you
know
about
the
time
Exercise
The
Babylonians
developed
the
sexagesimal
base
number
system
about
years
ago
How
many
bits
of
information
is
conveyed
with
one
sexagesimal
digit
How
do
you
write
the
number
in
sexagesimal
Exercise
How
many
different
numbers
can
be
represented
with
bits
Exercise
What
is
the
largest
unsigned
bit
binary
number
Exercise
What
is
the
largest
bit
binary
number
that
can
be
represented
with
a
unsigned
numbers
b
two
s
complement
numbers
c
sign
magnitude
numbers
CHAPTER
ONE
From
Zero
to
One
Exercise
What
is
the
smallest
most
negative
bit
binary
number
that
can
be
represented
with
a
unsigned
numbers
b
two
s
complement
numbers
c
sign
magnitude
numbers
Exercise
Convert
the
following
unsigned
binary
numbers
to
decimal
a
b
c
d
Exercise
Repeat
Exercise
but
convert
to
hexadecimal
Exercise
Convert
the
following
hexadecimal
numbers
to
decimal
a
A
b
B
c
FFFF
d
D
Exercise
Repeat
Exercise
but
convert
to
unsigned
binary
Exercise
Convert
the
following
two
s
complement
binary
numbers
to
decimal
a
b
c
d
Exercise
Repeat
Exercise
assuming
the
binary
numbers
are
in
sign
magnitude
form
rather
than
two
s
complement
representation
Exercise
Convert
the
following
decimal
numbers
to
unsigned
binary
numbers
a
b
c
d
Exercise
Repeat
Exercise
but
convert
to
hexadecimal
Exercise
Convert
the
following
decimal
numbers
to
bit
two
s
complement
numbers
or
indicate
that
the
decimal
number
would
overflow
the
range
a
b
c
d
e
Exercise
Repeat
Exercise
but
convert
to
bit
sign
magnitude
numbers
Exercise
Convert
the
following
bit
two
s
complement
numbers
to
bit
two
s
complement
numbers
a
b
Exercise
Repeat
Exercise
if
the
numbers
are
unsigned
rather
than
two
s
complement
Exercise
Base
is
referred
to
as
octal
Convert
each
of
the
numbers
from
Exercise
to
octal
Exercise
Convert
each
of
the
following
octal
numbers
to
binary
hexadecimal
and
decimal
a
b
c
d
Exercise
How
many
bit
two
s
complement
numbers
are
greater
than
How
many
are
less
than
How
would
your
answers
differ
for
sign
magnitude
numbers
Exercises
Ch
Exercise
How
many
bytes
are
in
a
bit
word
How
many
nibbles
are
in
the
word
Exercise
How
many
bytes
are
in
a
bit
word
Exercise
A
particular
DSL
modem
operates
at
kbits
sec
How
many
bytes
can
it
receive
in
minute
Exercise
Hard
disk
manufacturers
use
the
term
megabyte
to
mean
bytes
and
gigabyte
to
mean
bytes
How
many
real
GBs
of
music
can
you
store
on
a
GB
hard
disk
Exercise
Estimate
the
value
of
without
using
a
calculator
Exercise
A
memory
on
the
Pentium
II
microprocessor
is
organized
as
a
rectangular
array
of
bits
with
rows
and
columns
Estimate
how
many
bits
it
has
without
using
a
calculator
Exercise
Draw
a
number
line
analogous
to
Figure
for
bit
unsigned
two
s
complement
and
sign
magnitude
numbers
Exercise
Perform
the
following
additions
of
unsigned
binary
numbers
Indicate
whether
or
not
the
sum
overflows
a
bit
result
a
b
Exercise
Perform
the
following
additions
of
unsigned
binary
numbers
Indicate
whether
or
not
the
sum
overflows
an
bit
result
a
b
Exercise
Repeat
Exercise
assuming
that
the
binary
numbers
are
in
two
s
complement
form
Exercise
Convert
the
following
decimal
numbers
to
bit
two
s
complement
binary
numbers
and
add
them
Indicate
whether
or
not
the
sum
overflows
a
bit
result
a
b
c
CHAPTER
ONE
From
Zero
to
One
C
d
e
f
Exercise
Perform
the
following
additions
of
unsigned
hexadecimal
numbers
Indicate
whether
or
not
the
sum
overflows
an
bit
two
hex
digit
result
a
b
c
AB
E
d
F
AD
Exercise
Convert
the
following
decimal
numbers
to
bit
two
s
complement
binary
numbers
and
subtract
them
Indicate
whether
or
not
the
difference
overflows
a
bit
result
a
b
c
d
Exercise
In
a
biased
N
bit
binary
number
system
with
bias
B
positive
and
negative
numbers
are
represented
as
their
value
plus
the
bias
B
For
example
for
bit
numbers
with
a
bias
of
the
number
is
represented
as
as
and
so
forth
Biased
number
systems
are
sometimes
used
in
floating
point
mathematics
which
will
be
discussed
in
Chapter
Consider
a
biased
bit
binary
number
system
with
a
bias
of
a
What
decimal
value
does
the
binary
number
represent
b
What
binary
number
represents
the
value
c
What
is
the
representation
and
value
of
the
most
negative
number
d
What
is
the
representation
and
value
of
the
most
positive
number
Exercise
Draw
a
number
line
analogous
to
Figure
for
bit
biased
numbers
with
a
bias
of
see
Exercise
for
a
definition
of
biased
numbers
Exercise
In
a
binary
coded
decimal
BCD
system
bits
are
used
to
represent
a
decimal
digit
from
to
For
example
is
written
as
BCD
Exercises
Chapter
a
Write
in
BCD
b
Convert
BCD
to
decimal
c
Convert
BCD
to
binary
d
Explain
why
BCD
might
be
a
useful
way
to
represent
numbers
Exercise
A
flying
saucer
crashes
in
a
Nebraska
cornfield
The
FBI
investigates
the
wreckage
and
finds
an
engineering
manual
containing
an
equation
in
the
Martian
number
system
If
this
equation
is
correct
how
many
fingers
would
you
expect
Martians
have
Exercise
Ben
Bitdiddle
and
Alyssa
P
Hacker
are
having
an
argument
Ben
says
All
integers
greater
than
zero
and
exactly
divisible
by
six
have
exactly
two
s
in
their
binary
representation
Alyssa
disagrees
She
says
No
but
all
such
numbers
have
an
even
number
of
s
in
their
representation
Do
you
agree
with
Ben
or
Alyssa
or
both
or
neither
Explain
Exercise
Ben
Bitdiddle
and
Alyssa
P
Hacker
are
having
another
argument
Ben
says
I
can
get
the
two
s
complement
of
a
number
by
subtracting
then
inverting
all
the
bits
of
the
result
Alyssa
says
No
I
can
do
it
by
examining
each
bit
of
the
number
starting
with
the
least
significant
bit
When
the
first
is
found
invert
each
subsequent
bit
Do
you
agree
with
Ben
or
Alyssa
or
both
or
neither
Explain
Exercise
Write
a
program
in
your
favorite
language
e
g
C
Java
Perl
to
convert
numbers
from
binary
to
decimal
The
user
should
type
in
an
unsigned
binary
number
The
program
should
print
the
decimal
equivalent
Exercise
Repeat
Exercise
but
convert
from
decimal
to
hexadecimal
Exercise
Repeat
Exercise
but
convert
from
an
arbitrary
base
b
to
another
base
b
as
specified
by
the
user
Support
bases
up
to
using
the
letters
of
the
alphabet
for
digits
greater
than
The
user
should
enter
b
b
and
then
the
number
to
convert
in
base
b
The
program
should
print
the
equivalent
number
in
base
b
Exercise
Draw
the
symbol
Boolean
equation
and
truth
table
for
a
a
three
input
OR
gate
b
a
three
input
exclusive
OR
XOR
gate
c
a
four
input
XNOR
gate
CHAPTER
ONE
From
Zero
to
One
C
Exercise
A
majority
gate
produces
a
TRUE
output
if
and
only
if
more
than
half
of
its
inputs
are
TRUE
Complete
a
truth
table
for
the
three
input
majority
gate
shown
in
Figure
Exercise
A
three
input
AND
OR
AO
gate
shown
in
Figure
produces
a
TRUE
output
if
both
A
and
B
are
TRUE
or
if
C
is
TRUE
Complete
a
truth
table
for
the
gate
Exercise
A
three
input
OR
AND
INVERT
OAI
gate
shown
in
Figure
produces
a
FALSE
input
if
C
is
TRUE
and
A
or
B
is
TRUE
Otherwise
it
produces
a
TRUE
output
Complete
a
truth
table
for
the
gate
Exercise
There
are
different
truth
tables
for
Boolean
functions
of
two
variables
List
each
truth
table
Give
each
one
a
short
descriptive
name
such
as
OR
NAND
and
so
on
Exercise
How
many
different
truth
tables
exist
for
Boolean
functions
of
N
variables
Exercise
Is
it
possible
to
assign
logic
levels
so
that
a
device
with
the
transfer
characteristics
shown
in
Figure
would
serve
as
an
inverter
If
so
what
are
the
input
and
output
low
and
high
levels
VIL
VOL
VIH
and
VOH
and
noise
margins
NML
and
NMH
If
not
explain
why
not
Exercises
A
B
Y
C
MAJ
A
B
Y
C
A
B
Y
C
Figure
Three
input
majority
gate
Figure
Three
input
AND
OR
gate
Figure
Three
input
OR
AND
INVERT
gate
Exercise
Repeat
Exercise
for
the
transfer
characteristics
shown
in
Figure
Exercise
Is
it
possible
to
assign
logic
levels
so
that
a
device
with
the
transfer
characteristics
shown
in
Figure
would
serve
as
a
buffer
If
so
what
are
the
input
and
output
low
and
high
levels
VIL
VOL
VIH
and
VOH
and
noise
margins
NML
and
NMH
If
not
explain
why
not
CHAPTER
ONE
From
Zero
to
One
Figure
DC
transfer
characteristics
Vin
Vout
Figure
DC
transfer
characteristics
Vin
Vout
Exercise
Ben
Bitdiddle
has
invented
a
circuit
with
the
transfer
characteristics
shown
in
Figure
that
he
would
like
to
use
as
a
buffer
Will
it
work
Why
or
why
not
He
would
like
to
advertise
that
it
is
compatible
with
LVCMOS
and
LVTTL
logic
Can
Ben
s
buffer
correctly
receive
inputs
from
those
logic
families
Can
its
output
properly
drive
those
logic
families
Explain
Exercise
While
walking
down
a
dark
alley
Ben
Bitdiddle
encounters
a
twoinput
gate
with
the
transfer
function
shown
in
Figure
The
inputs
are
A
and
B
and
the
output
is
Y
a
What
kind
of
logic
gate
did
he
find
b
What
are
the
approximate
high
and
low
logic
levels
Exercises
Vin
Vout
Vin
Vout
Figure
DC
transfer
characteristics
Figure
Ben
s
buffer
DC
transfer
characteristics
Exercise
Repeat
Exercise
for
Figure
Exercise
Sketch
a
transistor
level
circuit
for
the
following
CMOS
gates
Use
a
minimum
number
of
transistors
a
A
four
input
NAND
gate
b
A
three
input
OR
AND
INVERT
gate
see
Exercise
c
A
three
input
AND
OR
gate
see
Exercise
Exercise
A
minority
gate
produces
a
TRUE
output
if
and
only
if
fewer
than
half
of
its
inputs
are
TRUE
Otherwise
it
produces
a
FALSE
output
Sketch
a
transistor
level
circuit
for
a
CMOS
minority
gate
Use
a
minimum
number
of
transistors
CHAPTER
ONE
From
Zero
to
One
Figure
Two
input
DC
transfer
characteristics
Figure
Two
input
DC
transfer
characteristics
A
B
Y
A
B
Y
Exercise
Write
a
truth
table
for
the
function
performed
by
the
gate
in
Figure
The
truth
table
should
have
two
inputs
A
and
B
What
is
the
name
of
this
function
Exercise
Write
a
truth
table
for
the
function
performed
by
the
gate
in
Figure
The
truth
table
should
have
three
inputs
A
B
and
C
Exercise
Implement
the
following
three
input
gates
using
only
pseudonMOS
logic
gates
Your
gates
receive
three
inputs
A
B
and
C
Use
a
minimum
number
of
transistors
a
three
input
NOR
gate
b
three
input
NAND
gate
a
three
input
AND
gate
Exercise
Resistor
Transistor
Logic
RTL
uses
nMOS
transistors
to
pull
the
gate
output
LOW
and
a
weak
resistor
to
pull
the
output
HIGH
when
none
of
the
paths
to
ground
are
active
A
NOT
gate
built
using
RTL
is
shown
in
Figure
Sketch
a
three
input
RTL
NOR
gate
Use
a
minimum
number
of
transistors
Exercises
A
B
A
B
A
A
B
B
Y
A
B
C
C
A
B
Y
A
Y
weak
Figure
Mystery
schematic
Figure
Mystery
schematic
Figure
RTL
NOT
gate
Interview
Questions
These
questions
have
been
asked
at
interviews
for
digital
design
jobs
Question
Sketch
a
transistor
level
circuit
for
a
CMOS
four
input
NOR
gate
Question
The
king
receives
gold
coins
in
taxes
but
has
reason
to
believe
that
one
is
counterfeit
He
summons
you
to
identify
the
fake
coin
You
have
a
balance
that
can
hold
coins
on
each
side
How
many
times
do
you
need
to
use
the
balance
to
find
the
lighter
fake
coin
Question
The
professor
the
teaching
assistant
the
digital
design
student
and
the
freshman
track
star
need
to
cross
a
rickety
bridge
on
a
dark
night
The
bridge
is
so
shakey
that
only
two
people
can
cross
at
a
time
They
have
only
one
flashlight
among
them
and
the
span
is
too
long
to
throw
the
flashlight
so
somebody
must
carry
it
back
to
the
other
people
The
freshman
track
star
can
cross
the
bridge
in
minute
The
digital
design
student
can
cross
the
bridge
in
minutes
The
teaching
assistant
can
cross
the
bridge
in
minutes
The
professor
always
gets
distracted
and
takes
minutes
to
cross
the
bridge
What
is
the
fastest
time
to
get
everyone
across
the
bridge
CHAPTER
ONE
From
Zero
to
One
Introduction
Boolean
Equations
Boolean
Algebra
From
Logic
to
Gates
Multilevel
Combinational
Logic
X
s
and
Z
s
Oh
My
Karnaugh
Maps
Combinational
Building
Blocks
Timing
Summary
Exercises
Interview
Questions
Combinational
Logic
Design
INTRODUCTION
In
digital
electronics
a
circuit
is
a
network
that
processes
discretevalued
variables
A
circuit
can
be
viewed
as
a
black
box
shown
in
Figure
with
one
or
more
discrete
valued
input
terminals
one
or
more
discrete
valued
output
terminals
a
functional
specification
describing
the
relationship
between
inputs
and
outputs
a
timing
specification
describing
the
delay
between
inputs
changing
and
outputs
responding
Peering
inside
the
black
box
circuits
are
composed
of
nodes
and
elements
An
element
is
itself
a
circuit
with
inputs
outputs
and
a
specification
A
node
is
a
wire
whose
voltage
conveys
a
discrete
valued
variable
Nodes
are
classified
as
input
output
or
internal
Inputs
receive
values
from
the
external
world
Outputs
deliver
values
to
the
external
world
Wires
that
are
not
inputs
or
outputs
are
called
internal
nodes
Figure
inputs
outputs
functional
spec
timing
spec
Figure
Circuit
as
a
black
box
with
inputs
outputs
and
specifications
Figure
Elements
and
nodes
A
E
E
B
E
C
n
Y
Z
Chap
illustrates
a
circuit
with
three
elements
E
E
and
E
and
six
nodes
Nodes
A
B
and
C
are
inputs
Y
and
Z
are
outputs
n
is
an
internal
node
between
E
and
E
Digital
circuits
are
classified
as
combinational
or
sequential
A
combinational
circuit
s
outputs
depend
only
on
the
current
values
of
the
inputs
in
other
words
it
combines
the
current
input
values
to
compute
the
output
For
example
a
logic
gate
is
a
combinational
circuit
A
sequential
circuit
s
outputs
depend
on
both
current
and
previous
values
of
the
inputs
in
other
words
it
depends
on
the
input
sequence
A
combinational
circuit
is
memoryless
but
a
sequential
circuit
has
memory
This
chapter
focuses
on
combinational
circuits
and
Chapter
examines
sequential
circuits
The
functional
specification
of
a
combinational
circuit
expresses
the
output
values
in
terms
of
the
current
input
values
The
timing
specification
of
a
combinational
circuit
consists
of
lower
and
upper
bounds
on
the
delay
from
input
to
output
We
will
initially
concentrate
on
the
functional
specification
then
return
to
the
timing
specification
later
in
this
chapter
Figure
shows
a
combinational
circuit
with
two
inputs
and
one
output
On
the
left
of
the
figure
are
the
inputs
A
and
B
and
on
the
right
is
the
output
Y
The
symbol
CL
inside
the
box
indicates
that
it
is
implemented
using
only
combinational
logic
In
this
example
the
function
F
is
specified
to
be
OR
Y
F
A
B
A
B
In
words
we
say
the
output
Y
is
a
function
of
the
two
inputs
A
and
B
namely
Y
A
OR
B
Figure
shows
two
possible
implementations
for
the
combinational
logic
circuit
in
Figure
As
we
will
see
repeatedly
throughout
the
book
there
are
often
many
implementations
for
a
single
function
You
choose
which
to
use
given
the
building
blocks
at
your
disposal
and
your
design
constraints
These
constraints
often
include
area
speed
power
and
design
time
Figure
shows
a
combinational
circuit
with
multiple
outputs
This
particular
combinational
circuit
is
called
a
full
adder
and
we
will
revisit
it
in
Section
The
two
equations
specify
the
function
of
the
outputs
S
and
Cout
in
terms
of
the
inputs
A
B
and
Cin
To
simplify
drawings
we
often
use
a
single
line
with
a
slash
through
it
and
a
number
next
to
it
to
indicate
a
bus
a
bundle
of
multiple
signals
The
number
specifies
how
many
signals
are
in
the
bus
For
example
Figure
a
represents
a
block
of
combinational
logic
with
three
inputs
and
two
outputs
If
the
number
of
bits
is
unimportant
or
obvious
from
the
context
the
slash
may
be
shown
without
a
number
Figure
b
indicates
two
blocks
of
combinational
logic
with
an
arbitrary
number
of
outputs
from
one
block
serving
as
inputs
to
the
second
block
The
rules
of
combinational
composition
tell
us
how
we
can
build
a
large
combinational
circuit
from
smaller
combinational
circuit
CHAPTER
TWO
Combinational
Logic
Design
Figure
Combinational
logic
circuit
Figure
Two
OR
implementations
A
B
Y
Y
F
A
B
A
B
CL
A
B
Y
a
Y
b
A
B
Figure
Multiple
output
combinational
circuit
Figure
Slash
notation
for
multiple
signals
A
S
S
A
B
Cin
Cout
AB
ACin
BCin
B
Cin
CL
Cout
CL
a
CL
CL
b
Chap
elements
A
circuit
is
combinational
if
it
consists
of
interconnected
circuit
elements
such
that
Every
circuit
element
is
itself
combinational
Every
node
of
the
circuit
is
either
designated
as
an
input
to
the
circuit
or
connects
to
exactly
one
output
terminal
of
a
circuit
element
The
circuit
contains
no
cyclic
paths
every
path
through
the
circuit
visits
each
circuit
node
at
most
once
Example
COMBINATIONAL
CIRCUITS
Which
of
the
circuits
in
Figure
are
combinational
circuits
according
to
the
rules
of
combinational
composition
Solution
Circuit
a
is
combinational
It
is
constructed
from
two
combinational
circuit
elements
inverters
I
and
I
It
has
three
nodes
n
n
and
n
n
is
an
input
to
the
circuit
and
to
I
n
is
an
internal
node
which
is
the
output
of
I
and
the
input
to
I
n
is
the
output
of
the
circuit
and
of
I
b
is
not
combinational
because
there
is
a
cyclic
path
the
output
of
the
XOR
feeds
back
to
one
of
its
inputs
Hence
a
cyclic
path
starting
at
n
passes
through
the
XOR
to
n
which
returns
to
n
c
is
combinational
d
is
not
combinational
because
node
n
connects
to
the
output
terminals
of
both
I
and
I
e
is
combinational
illustrating
two
combinational
circuits
connected
to
form
a
larger
combinational
circuit
f
does
not
obey
the
rules
of
combinational
composition
because
it
has
a
cyclic
path
through
the
two
elements
Depending
on
the
functions
of
the
elements
it
may
or
may
not
be
a
combinational
circuit
Large
circuits
such
as
microprocessors
can
be
very
complicated
so
we
use
the
principles
from
Chapter
to
manage
the
complexity
Viewing
a
circuit
as
a
black
box
with
a
well
defined
interface
and
function
is
an
Introduction
The
rules
of
combinational
composition
are
sufficient
but
not
strictly
necessary
Certain
circuits
that
disobey
these
rules
are
still
combinational
so
long
as
the
outputs
depend
only
on
the
current
values
of
the
inputs
However
determining
whether
oddball
circuits
are
combinational
is
more
difficult
so
we
will
usually
restrict
ourselves
to
combinational
composition
as
a
way
to
build
combinational
circuits
Figure
Example
circuits
a
n
n
n
I
I
c
CL
CL
e
n
n
b
n
I
I
d
CL
CL
f
Cha
application
of
abstraction
and
modularity
Building
the
circuit
out
of
smaller
circuit
elements
is
an
application
of
hierarchy
The
rules
of
combinational
composition
are
an
application
of
discipline
The
functional
specification
of
a
combinational
circuit
is
usually
expressed
as
a
truth
table
or
a
Boolean
equation
In
the
next
sections
we
describe
how
to
derive
a
Boolean
equation
from
any
truth
table
and
how
to
use
Boolean
algebra
and
Karnaugh
maps
to
simplify
equations
We
show
how
to
implement
these
equations
using
logic
gates
and
how
to
analyze
the
speed
of
these
circuits
BOOLEAN
EQUATIONS
Boolean
equations
deal
with
variables
that
are
either
TRUE
or
FALSE
so
they
are
perfect
for
describing
digital
logic
This
section
defines
some
terminology
commonly
used
in
Boolean
equations
then
shows
how
to
write
a
Boolean
equation
for
any
logic
function
given
its
truth
table
Terminology
The
complement
of
a
variable
A
is
its
inverse
The
variable
or
its
complement
is
called
a
literal
For
example
A
B
and
are
literals
We
call
A
the
true
form
of
the
variable
and
the
complementary
form
true
form
does
not
mean
that
A
is
TRUE
but
merely
that
A
does
not
have
a
line
over
it
The
AND
of
one
or
more
literals
is
called
a
product
or
an
implicant
and
B
are
all
implicants
for
a
function
of
three
variables
A
minterm
is
a
product
involving
all
of
the
inputs
to
the
function
is
a
minterm
for
a
function
of
the
three
variables
A
B
and
C
but
is
not
because
it
does
not
involve
C
Similarly
the
OR
of
one
or
more
literals
is
called
a
sum
A
maxterm
is
a
sum
involving
all
of
the
inputs
to
the
function
is
a
maxterm
for
a
function
of
the
three
variables
A
B
and
C
The
order
of
operations
is
important
when
interpreting
Boolean
equations
Does
Y
A
BC
mean
Y
A
OR
B
AND
C
or
Y
A
OR
B
AND
C
In
Boolean
equations
NOT
has
the
highest
precedence
followed
by
AND
then
OR
Just
as
in
ordinary
equations
products
are
performed
before
sums
Therefore
the
equation
is
read
as
Y
A
OR
B
AND
C
Equation
gives
another
example
of
order
of
operations
Sum
of
Products
Form
A
truth
table
of
N
inputs
contains
N
rows
one
for
each
possible
value
of
the
inputs
Each
row
in
a
truth
table
is
associated
with
a
minterm
AB
BCD
A
B
BC
D
A
B
C
AB
ABC
AB
ABC
A
A
B
A
CHAPTER
TWO
Combinational
Logic
Design
Chapter
that
is
TRUE
for
that
row
Figure
shows
a
truth
table
of
two
inputs
A
and
B
Each
row
shows
its
corresponding
minterm
For
example
the
minterm
for
the
first
row
is
because
is
TRUE
when
A
B
We
can
write
a
Boolean
equation
for
any
truth
table
by
summing
each
of
the
minterms
for
which
the
output
Y
is
TRUE
For
example
in
Figure
there
is
only
one
row
or
minterm
for
which
the
output
Y
is
TRUE
shown
circled
in
blue
Thus
Figure
shows
a
truth
table
with
more
than
one
row
in
which
the
output
is
TRUE
Taking
the
sum
of
each
of
the
circled
minterms
gives
This
is
called
the
sum
of
products
canonical
form
of
a
function
because
it
is
the
sum
OR
of
products
ANDs
forming
minterms
Although
there
are
many
ways
to
write
the
same
function
such
as
we
will
sort
the
minterms
in
the
same
order
that
they
appear
in
the
truth
table
so
that
we
always
write
the
same
Boolean
expression
for
the
same
truth
table
Example
SUM
OF
PRODUCTS
FORM
Ben
Bitdiddle
is
having
a
picnic
He
won
t
enjoy
it
if
it
rains
or
if
there
are
ants
Design
a
circuit
that
will
output
TRUE
only
if
Ben
enjoys
the
picnic
Solution
First
define
the
inputs
and
outputs
The
inputs
are
A
and
R
which
indicate
if
there
are
ants
and
if
it
rains
A
is
TRUE
when
there
are
ants
and
FALSE
when
there
are
no
ants
Likewise
R
is
TRUE
when
it
rains
and
FALSE
when
the
sun
smiles
on
Ben
The
output
is
E
Ben
s
enjoyment
of
the
picnic
E
is
TRUE
if
Ben
enjoys
the
picnic
and
FALSE
if
he
suffers
Figure
shows
the
truth
table
for
Ben
s
picnic
experience
Using
sum
of
products
form
we
write
the
equation
as
We
can
build
the
equation
using
two
inverters
and
a
two
input
AND
gate
shown
in
Figure
a
You
may
recognize
this
truth
table
as
the
NOR
function
from
Section
E
A
NOR
Figure
b
shows
the
NOR
implementation
In
Section
we
show
that
the
two
equations
and
are
equivalent
The
sum
of
products
form
provides
a
Boolean
equation
for
any
truth
table
with
any
number
of
variables
Figure
shows
a
random
threeinput
truth
table
The
sum
of
products
form
of
the
logic
function
is
Unfortunately
sum
of
products
form
does
not
necessarily
generate
the
simplest
equation
In
Section
we
show
how
to
write
the
same
function
using
fewer
terms
Y
ABC
ABC
ABC
AR
A
R
R
A
R
E
AR
Y
BA
BA
Y
AB
AB
Y
AB
AB
AB
Boolean
Equations
Figure
Truth
table
and
minterms
Figure
Truth
table
with
multiple
TRUE
minterms
ABY
minterm
A
B
A
B
A
B
A
B
ABY
minterm
A
B
A
B
A
B
A
B
Canonical
form
is
just
a
fancy
word
for
standard
form
You
can
use
the
term
to
impress
your
friends
and
scare
your
enemies
Chapter
qxd
Product
of
Sums
Form
An
alternative
way
of
expressing
Boolean
functions
is
the
product
ofsums
canonical
form
Each
row
of
a
truth
table
corresponds
to
a
maxterm
that
is
FALSE
for
that
row
For
example
the
maxterm
for
the
first
row
of
a
two
input
truth
table
is
A
B
because
A
B
is
FALSE
when
A
B
We
can
write
a
Boolean
equation
for
any
circuit
directly
from
the
truth
table
as
the
AND
of
each
of
the
maxterms
for
which
the
output
is
FALSE
Example
PRODUCT
OF
SUMS
FORM
Write
an
equation
in
product
of
sums
form
for
the
truth
table
in
Figure
Solution
The
truth
table
has
two
rows
in
which
the
output
is
FALSE
Hence
the
function
can
be
written
in
product
of
sums
form
as
The
first
maxterm
A
B
guarantees
that
Y
for
A
B
because
any
value
AND
is
Likewise
the
second
maxterm
guarantees
that
Y
for
A
B
Figure
is
the
same
truth
table
as
Figure
showing
that
the
same
function
can
be
written
in
more
than
one
way
Similarly
a
Boolean
equation
for
Ben
s
picnic
from
Figure
can
be
written
in
product
of
sums
form
by
circling
the
three
rows
of
s
to
obtain
This
is
uglier
than
the
sumof
products
equation
but
the
two
equations
are
logically
equivalent
Sum
of
products
produces
the
shortest
equations
when
the
output
is
TRUE
on
only
a
few
rows
of
a
truth
table
product
of
sums
is
simpler
when
the
output
is
FALSE
on
only
a
few
rows
of
a
truth
table
BOOLEAN
ALGEBRA
In
the
previous
section
we
learned
how
to
write
a
Boolean
expression
given
a
truth
table
However
that
expression
does
not
necessarily
lead
to
the
simplest
set
of
logic
gates
Just
as
you
use
algebra
to
simplify
mathematical
equations
you
can
use
Boolean
algebra
to
simplify
Boolean
equations
The
rules
of
Boolean
algebra
are
much
like
those
of
ordinary
algebra
but
are
in
some
cases
simpler
because
variables
have
only
two
possible
values
or
Boolean
algebra
is
based
on
a
set
of
axioms
that
we
assume
are
correct
Axioms
are
unprovable
in
the
sense
that
a
definition
cannot
be
proved
From
these
axioms
we
prove
all
the
theorems
of
Boolean
algebra
These
theorems
have
great
practical
significance
because
they
teach
us
how
to
simplify
logic
to
produce
smaller
and
less
costly
circuits
E
AR
E
A
R
A
R
A
R
A
B
Y
A
B
A
B
CHAPTER
TWO
Combinational
Logic
Design
Figure
Truth
table
with
multiple
FALSE
maxterms
A
B
ABY
maxterm
A
B
A
B
A
B
Figure
Random
three
input
truth
table
BCY
A
Figure
Ben
s
circuit
A
R
E
a
A
R
E
b
Figure
Ben
s
truth
table
ARE
Chapter
qxd
Axioms
and
theorems
of
Boolean
algebra
obey
the
principle
of
duality
If
the
symbols
and
and
the
operators
AND
and
OR
are
interchanged
the
statement
will
still
be
correct
We
use
the
prime
symbol
to
denote
the
dual
of
a
statement
Axioms
Table
states
the
axioms
of
Boolean
algebra
These
five
axioms
and
their
duals
define
Boolean
variables
and
the
meanings
of
NOT
AND
and
OR
Axiom
A
states
that
a
Boolean
variable
B
is
if
it
is
not
The
axiom
s
dual
A
states
that
the
variable
is
if
it
is
not
Together
A
and
A
tell
us
that
we
are
working
in
a
Boolean
or
binary
field
of
s
and
s
Axioms
A
and
A
define
the
NOT
operation
Axioms
A
to
A
define
AND
their
duals
A
to
A
define
OR
Theorems
of
One
Variable
Theorems
T
to
T
in
Table
describe
how
to
simplify
equations
involving
one
variable
The
identity
theorem
T
states
that
for
any
Boolean
variable
B
B
AND
B
Its
dual
states
that
B
OR
B
In
hardware
as
shown
in
Figure
T
means
that
if
one
input
of
a
two
input
AND
gate
is
always
we
can
remove
the
AND
gate
and
replace
it
with
a
wire
connected
to
the
variable
input
B
Likewise
T
means
that
if
one
input
of
a
two
input
OR
gate
is
always
we
can
replace
the
OR
gate
with
a
wire
connected
to
B
In
general
gates
cost
money
power
and
delay
so
replacing
a
gate
with
a
wire
is
beneficial
The
null
element
theorem
T
says
that
B
AND
is
always
equal
to
Therefore
is
called
the
null
element
for
the
AND
operation
because
it
nullifies
the
effect
of
any
other
input
The
dual
states
that
B
OR
is
always
equal
to
Hence
is
the
null
element
for
the
OR
operation
In
hardware
as
shown
in
Figure
if
one
input
of
an
AND
gate
is
we
can
replace
the
AND
gate
with
a
wire
that
is
tied
Boolean
Algebra
Axiom
Dual
Name
A
B
if
B
A
B
if
B
Binary
field
A
A
NOT
A
A
AND
OR
A
A
AND
OR
A
A
AND
OR
Table
Axioms
of
Boolean
algebra
The
null
element
theorem
leads
to
some
outlandish
statements
that
are
actually
true
It
is
particularly
dangerous
when
left
in
the
hands
of
advertisers
YOU
WILL
GET
A
MILLION
DOLLARS
or
we
ll
send
you
a
toothbrush
in
the
mail
You
ll
most
likely
be
receiving
a
toothbrush
in
the
mail
Figure
Identity
theorem
in
hardware
a
T
b
T
a
B
B
B
B
b
Chapter
qxd
LOW
to
Likewise
if
one
input
of
an
OR
gate
is
we
can
replace
the
OR
gate
with
a
wire
that
is
tied
HIGH
to
Idempotency
T
says
that
a
variable
AND
itself
is
equal
to
just
itself
Likewise
a
variable
OR
itself
is
equal
to
itself
The
theorem
gets
its
name
from
the
Latin
roots
idem
same
and
potent
power
The
operations
return
the
same
thing
you
put
into
them
Figure
shows
that
idempotency
again
permits
replacing
a
gate
with
a
wire
Involution
T
is
a
fancy
way
of
saying
that
complementing
a
variable
twice
results
in
the
original
variable
In
digital
electronics
two
wrongs
make
a
right
Two
inverters
in
series
logically
cancel
each
other
out
and
are
logically
equivalent
to
a
wire
as
shown
in
Figure
The
dual
of
T
is
itself
The
complement
theorem
T
Figure
states
that
a
variable
AND
its
complement
is
because
one
of
them
has
to
be
And
by
duality
a
variable
OR
its
complement
is
because
one
of
them
has
to
be
Theorems
of
Several
Variables
Theorems
T
to
T
in
Table
describe
how
to
simplify
equations
involving
more
than
one
Boolean
variable
Commutativity
and
associativity
T
and
T
work
the
same
as
in
traditional
algebra
By
commutativity
the
order
of
inputs
for
an
AND
or
OR
function
does
not
affect
the
value
of
the
output
By
associativity
the
specific
groupings
of
inputs
do
not
affect
the
value
of
the
output
The
distributivity
theorem
T
is
the
same
as
in
traditional
algebra
but
its
dual
T
is
not
By
T
AND
distributes
over
OR
and
by
T
OR
distributes
over
AND
In
traditional
algebra
multiplication
distributes
over
addition
but
addition
does
not
distribute
over
multiplication
so
that
B
C
B
D
B
C
D
The
covering
combining
and
consensus
theorems
T
to
T
permit
us
to
eliminate
redundant
variables
With
some
thought
you
should
be
able
to
convince
yourself
that
these
theorems
are
correct
CHAPTER
TWO
Combinational
Logic
Design
Theorem
Dual
Name
T
B
B
T
B
B
Identity
T
B
T
B
Null
Element
T
B
B
B
T
B
B
B
Idempotency
T
B
B
Involution
T
B
B
T
B
B
Complements
Table
Boolean
theorems
of
one
variable
Figure
Idempotency
theorem
in
hardware
a
T
b
T
B
a
B
B
B
B
B
b
Figure
Involution
theorem
in
hardware
T
Figure
Complement
theorem
in
hardware
a
T
b
T
B
B
B
a
B
B
B
b
Figure
Null
element
theorem
in
hardware
a
T
b
T
a
B
B
b
Chapter
qxd
De
Morgan
s
Theorem
T
is
a
particularly
powerful
tool
in
digital
design
The
theorem
explains
that
the
complement
of
the
product
of
all
the
terms
is
equal
to
the
sum
of
the
complement
of
each
term
Likewise
the
complement
of
the
sum
of
all
the
terms
is
equal
to
the
product
of
the
complement
of
each
term
According
to
De
Morgan
s
theorem
a
NAND
gate
is
equivalent
to
an
OR
gate
with
inverted
inputs
Similarly
a
NOR
gate
is
equivalent
to
an
AND
gate
with
inverted
inputs
Figure
shows
these
De
Morgan
equivalent
gates
for
NAND
and
NOR
gates
The
two
symbols
shown
for
each
function
are
called
duals
They
are
logically
equivalent
and
can
be
used
interchangeably
Boolean
Algebra
Theorem
Dual
Name
T
B
C
C
B
T
B
C
C
B
Commutativity
T
B
C
D
B
C
D
T
B
C
D
B
C
D
Associativity
T
B
C
B
D
B
C
D
T
B
C
B
D
B
C
D
Distributivity
T
B
B
C
B
T
B
B
C
B
Covering
T
T
Combining
T
T
Consensus
T
T
De
Morgan
s
B
B
B
B
B
B
Theorem
B
B
B
B
B
B
B
C
B
D
B
C
B
D
B
C
B
D
C
D
B
C
B
D
C
D
B
C
B
C
B
B
C
B
C
B
Table
Boolean
theorems
of
several
variables
Augustus
De
Morgan
died
A
British
mathematician
born
in
India
Blind
in
one
eye
His
father
died
when
he
was
Attended
Trinity
College
Cambridge
at
age
and
was
appointed
Professor
of
Mathematics
at
the
newly
founded
London
University
at
age
Wrote
widely
on
many
mathematical
subjects
including
logic
algebra
and
paradoxes
De
Morgan
s
crater
on
the
moon
is
named
for
him
He
proposed
a
riddle
for
the
year
of
his
birth
I
was
x
years
of
age
in
the
year
x
Figure
De
Morgan
equivalent
gates
ABY
NAND
A
B
Y
A
B
Y
NOR
A
B
Y
A
B
Y
ABY
Y
AB
A
B
Y
A
B
A
B
Chapter
qxd
PM
Page
The
inversion
circle
is
called
a
bubble
Intuitively
you
can
imagine
that
pushing
a
bubble
through
the
gate
causes
it
to
come
out
at
the
other
side
and
flips
the
body
of
the
gate
from
AND
to
OR
or
vice
versa
For
example
the
NAND
gate
in
Figure
consists
of
an
AND
body
with
a
bubble
on
the
output
Pushing
the
bubble
to
the
left
results
in
an
OR
body
with
bubbles
on
the
inputs
The
underlying
rules
for
bubble
pushing
are
Pushing
bubbles
backward
from
the
output
or
forward
from
the
inputs
changes
the
body
of
the
gate
from
AND
to
OR
or
vice
versa
Pushing
a
bubble
from
the
output
back
to
the
inputs
puts
bubbles
on
all
gate
inputs
Pushing
bubbles
on
all
gate
inputs
forward
toward
the
output
puts
a
bubble
on
the
output
Section
uses
bubble
pushing
to
help
analyze
circuits
Example
DERIVE
THE
PRODUCT
OF
SUMS
FORM
Figure
shows
the
truth
table
for
a
Boolean
function
Y
and
its
complement
Using
De
Morgan
s
Theorem
derive
the
product
of
sums
canonical
form
of
Y
from
the
sum
of
products
form
of
Solution
Figure
shows
the
minterms
circled
contained
in
The
sum
ofproducts
canonical
form
of
is
Taking
the
complement
of
both
sides
and
applying
De
Morgan
s
Theorem
twice
we
get
The
Truth
Behind
It
All
The
curious
reader
might
wonder
how
to
prove
that
a
theorem
is
true
In
Boolean
algebra
proofs
of
theorems
with
a
finite
number
of
variables
are
easy
just
show
that
the
theorem
holds
for
all
possible
values
of
these
variables
This
method
is
called
perfect
induction
and
can
be
done
with
a
truth
table
Example
PROVING
THE
CONSENSUS
THEOREM
Prove
the
consensus
theorem
T
from
Table
Solution
Check
both
sides
of
the
equation
for
all
eight
combinations
of
B
C
and
D
The
truth
table
in
Figure
illustrates
these
combinations
Because
BC
BD
CD
BC
BD
for
all
cases
the
theorem
is
proved
Y
Y
AB
AB
AB
AB
A
B
A
B
Y
AB
AB
Y
Y
Y
Y
CHAPTER
TWO
Combinational
Logic
Design
FIGURE
Truth
table
showing
Y
and
Y
Figure
Truth
table
showing
minterms
for
Y
ABY
Y
ABY
Y
minterm
A
B
A
B
A
B
A
B
Chapter
qxd
Simplifying
Equations
The
theorems
of
Boolean
algebra
help
us
simplify
Boolean
equations
For
example
consider
the
sum
of
products
expression
from
the
truth
table
of
Figure
By
Theorem
T
the
equation
simplifies
to
This
may
have
been
obvious
looking
at
the
truth
table
In
general
multiple
steps
may
be
necessary
to
simplify
more
complex
equations
The
basic
principle
of
simplifying
sum
of
products
equations
is
to
combine
terms
using
the
relationship
where
P
may
be
any
implicant
How
far
can
an
equation
be
simplified
We
define
an
equation
in
sum
of
products
form
to
be
minimized
if
it
uses
the
fewest
possible
implicants
If
there
are
several
equations
with
the
same
number
of
implicants
the
minimal
one
is
the
one
with
the
fewest
literals
An
implicant
is
called
a
prime
implicant
if
it
cannot
be
combined
with
any
other
implicants
to
form
a
new
implicant
with
fewer
literals
The
implicants
in
a
minimal
equation
must
all
be
prime
implicants
Otherwise
they
could
be
combined
to
reduce
the
number
of
literals
Example
EQUATION
MINIMIZATION
Minimize
Equation
Solution
We
start
with
the
original
equation
and
apply
Boolean
theorems
step
by
step
as
shown
in
Table
Have
we
simplified
the
equation
completely
at
this
point
Let
s
take
a
closer
look
From
the
original
equation
the
minterms
and
differ
only
in
the
variable
A
So
we
combined
the
minterms
to
form
However
if
we
look
at
the
original
equation
we
note
that
the
last
two
minterms
and
also
differ
by
a
single
literal
C
and
Thus
using
the
same
method
we
could
have
combined
these
two
minterms
to
form
the
minterm
We
say
that
implicants
and
share
the
minterm
So
are
we
stuck
with
simplifying
only
one
of
the
minterm
pairs
or
can
we
simplify
both
Using
the
idempotency
theorem
we
can
duplicate
terms
as
many
times
as
we
want
B
B
B
B
B
Using
this
principle
we
simplify
the
equation
completely
to
its
two
prime
implicants
as
shown
in
Table
BC
AB
BC
AB
ABC
AB
C
ABC
ABC
BC
ABC
ABC
ABC
ABC
AB
C
PA
PA
P
Y
B
Y
AB
AB
Boolean
Algebra
Figure
Truth
table
proving
T
BCD
BC
BD
CD
BC
BD
Chapter
q
Although
it
is
a
bit
counterintuitive
expanding
an
implicant
for
example
turning
AB
into
is
sometimes
useful
in
minimizing
equations
By
doing
this
you
can
repeat
one
of
the
expanded
minterms
to
be
combined
shared
with
another
minterm
You
may
have
noticed
that
completely
simplifying
a
Boolean
equation
with
the
theorems
of
Boolean
algebra
can
take
some
trial
and
error
Section
describes
a
methodical
technique
called
Karnaugh
maps
that
makes
the
process
easier
Why
bother
simplifying
a
Boolean
equation
if
it
remains
logically
equivalent
Simplifying
reduces
the
number
of
gates
used
to
physically
implement
the
function
thus
making
it
smaller
cheaper
and
possibly
faster
The
next
section
describes
how
to
implement
Boolean
equations
with
logic
gates
FROM
LOGIC
TO
GATES
A
schematic
is
a
diagram
of
a
digital
circuit
showing
the
elements
and
the
wires
that
connect
them
together
For
example
the
schematic
in
Figure
shows
a
possible
hardware
implementation
of
our
favorite
logic
function
Equation
Y
ABC
ABC
ABC
ABC
ABC
CHAPTER
TWO
Combinational
Logic
Design
Step
Equation
Justification
T
Idempotency
T
Distributivity
T
Complements
BC
AB
T
Identity
BC
AB
BC
A
A
AB
C
C
ABC
ABC
ABC
ABC
ABC
ABC
ABC
Table
Improved
equation
minimization
Step
Equation
Justification
T
Distributivity
T
Complements
BC
ABC
T
Identity
BC
ABC
BC
A
A
ABC
ABC
ABC
ABC
Table
Equation
minimization
Chapter
qxd
By
drawing
schematics
in
a
consistent
fashion
we
make
them
easier
to
read
and
debug
We
will
generally
obey
the
following
guidelines
Inputs
are
on
the
left
or
top
side
of
a
schematic
Outputs
are
on
the
right
or
bottom
side
of
a
schematic
Whenever
possible
gates
should
flow
from
left
to
right
Straight
wires
are
better
to
use
than
wires
with
multiple
corners
jagged
wires
waste
mental
effort
following
the
wire
rather
than
thinking
of
what
the
circuit
does
Wires
always
connect
at
a
T
junction
A
dot
where
wires
cross
indicates
a
connection
between
the
wires
Wires
crossing
without
a
dot
make
no
connection
The
last
three
guidelines
are
illustrated
in
Figure
Any
Boolean
equation
in
sum
of
products
form
can
be
drawn
as
a
schematic
in
a
systematic
way
similar
to
Figure
First
draw
columns
for
the
inputs
Place
inverters
in
adjacent
columns
to
provide
the
complementary
inputs
if
necessary
Draw
rows
of
AND
gates
for
each
of
the
minterms
Then
for
each
output
draw
an
OR
gate
connected
to
the
minterms
related
to
that
output
This
style
is
called
a
programmable
logic
array
PLA
because
the
inverters
AND
gates
and
OR
gates
are
arrayed
in
a
systematic
fashion
PLAs
will
be
discussed
further
in
Section
Figure
shows
an
implementation
of
the
simplified
equation
we
found
using
Boolean
algebra
in
Example
Notice
that
the
simplified
circuit
has
significantly
less
hardware
than
that
of
Figure
It
may
also
be
faster
because
it
uses
gates
with
fewer
inputs
We
can
reduce
the
number
of
gates
even
further
albeit
by
a
single
inverter
by
taking
advantage
of
inverting
gates
Observe
that
is
an
BC
From
Logic
to
Gates
Figure
Schematic
of
Y
A
B
C
AB
C
AB
C
A
C
B
Y
minterm
ABC
minterm
ABC
minterm
ABC
ABC
Figure
Wire
connections
wires
connect
at
a
T
junction
wires
connect
at
a
dot
wires
crossing
without
a
dot
do
not
connect
A
B
C
Y
Figure
Schematic
of
Y
B
C
AB
Chapter
q
Black
light
twinkies
and
beer
AND
with
inverted
inputs
Figure
shows
a
schematic
using
this
optimization
to
eliminate
the
inverter
on
C
Recall
that
by
De
Morgan
s
theorem
the
AND
with
inverted
inputs
is
equivalent
to
a
NOR
gate
Depending
on
the
implementation
technology
it
may
be
cheaper
to
use
the
fewest
gates
or
to
use
certain
types
of
gates
in
preference
to
others
For
example
NANDs
and
NORs
are
preferred
over
ANDs
and
ORs
in
CMOS
implementations
Many
circuits
have
multiple
outputs
each
of
which
computes
a
separate
Boolean
function
of
the
inputs
We
can
write
a
separate
truth
table
for
each
output
but
it
is
often
convenient
to
write
all
of
the
outputs
on
a
single
truth
table
and
sketch
one
schematic
with
all
of
the
outputs
Example
MULTIPLE
OUTPUT
CIRCUITS
The
dean
the
department
chair
the
teaching
assistant
and
the
dorm
social
chair
each
use
the
auditorium
from
time
to
time
Unfortunately
they
occasionally
conflict
leading
to
disasters
such
as
the
one
that
occurred
when
the
dean
s
fundraising
meeting
with
crusty
trustees
happened
at
the
same
time
as
the
dorm
s
BTB
party
Alyssa
P
Hacker
has
been
called
in
to
design
a
room
reservation
system
The
system
has
four
inputs
A
A
and
four
outputs
Y
Y
These
signals
can
also
be
written
as
A
and
Y
Each
user
asserts
her
input
when
she
requests
the
auditorium
for
the
next
day
The
system
asserts
at
most
one
output
granting
the
auditorium
to
the
highest
priority
user
The
dean
who
is
paying
for
the
system
demands
highest
priority
The
department
chair
teaching
assistant
and
dorm
social
chair
have
decreasing
priority
Write
a
truth
table
and
Boolean
equations
for
the
system
Sketch
a
circuit
that
performs
this
function
Solution
This
function
is
called
a
four
input
priority
circuit
Its
symbol
and
truth
table
are
shown
in
Figure
We
could
write
each
output
in
sum
of
products
form
and
reduce
the
equations
using
Boolean
algebra
However
the
simplified
equations
are
clear
by
inspection
from
the
functional
description
and
the
truth
table
Y
is
TRUE
whenever
A
is
asserted
so
Y
A
Y
is
TRUE
if
A
is
asserted
and
A
is
not
asserted
so
Y
is
TRUE
if
A
is
asserted
and
neither
of
the
higher
priority
inputs
is
asserted
And
Y
is
TRUE
whenever
A
and
no
other
input
is
asserted
The
schematic
is
shown
in
Figure
An
experienced
designer
can
often
implement
a
logic
circuit
by
inspection
Given
a
clear
specification
simply
turn
the
words
into
equations
and
the
equations
into
gates
Y
A
A
A
A
Y
A
A
A
Y
A
A
CHAPTER
TWO
Combinational
Logic
Design
Y
A
B
C
Figure
Schematic
using
fewer
gates
Chap
Notice
that
if
A
is
asserted
in
the
priority
circuit
the
outputs
don
t
care
what
the
other
inputs
are
We
use
the
symbol
X
to
describe
inputs
that
the
output
doesn
t
care
about
Figure
shows
that
the
four
input
priority
circuit
truth
table
becomes
much
smaller
with
don
t
cares
From
this
truth
table
we
can
easily
read
the
Boolean
equations
in
sum
ofproducts
form
by
ignoring
inputs
with
X
s
Don
t
cares
can
also
appear
in
truth
table
outputs
as
we
will
see
in
Section
MULTILEVEL
COMBINATIONAL
LOGIC
Logic
in
sum
of
products
form
is
called
two
level
logic
because
it
consists
of
literals
connected
to
a
level
of
AND
gates
connected
to
a
level
of
Multilevel
Combinational
Logic
A
A
Priority
Circuit
A
A
Y
Y
Y
Y
A
A
A
A
Y
Y
Y
Y
Figure
Priority
circuit
Figure
Priority
circuit
schematic
A
A
A
A
Y
Y
Y
Y
Figure
Priority
circuit
truth
table
with
don
t
cares
X
s
A
A
X
X
X
A
A
X
X
X
Y
Y
Y
Y
X
is
an
overloaded
symbol
that
means
don
t
care
in
truth
tables
and
contention
in
logic
simulation
see
Section
Think
about
the
context
so
you
don
t
mix
up
the
meanings
Some
authors
use
D
or
instead
for
don
t
care
to
avoid
this
ambiguity
OR
gates
Designers
often
build
circuits
with
more
than
two
levels
of
logic
gates
These
multilevel
combinational
circuits
may
use
less
hardware
than
their
two
level
counterparts
Bubble
pushing
is
especially
helpful
in
analyzing
and
designing
multilevel
circuits
Hardware
Reduction
Some
logic
functions
require
an
enormous
amount
of
hardware
when
built
using
two
level
logic
A
notable
example
is
the
XOR
function
of
multiple
variables
For
example
consider
building
a
three
input
XOR
using
the
two
level
techniques
we
have
studied
so
far
Recall
that
an
N
input
XOR
produces
a
TRUE
output
when
an
odd
number
of
inputs
are
TRUE
Figure
shows
the
truth
table
for
a
three
input
XOR
with
the
rows
circled
that
produce
TRUE
outputs
From
the
truth
table
we
read
off
a
Boolean
equation
in
sum
of
products
form
in
Equation
Unfortunately
there
is
no
way
to
simplify
this
equation
into
fewer
implicants
On
the
other
hand
A
B
C
A
B
C
prove
this
to
yourself
by
perfect
induction
if
you
are
in
doubt
Therefore
the
three
input
XOR
can
be
built
out
of
a
cascade
of
two
input
XORs
as
shown
in
Figure
Similarly
an
eight
input
XOR
would
require
eight
input
AND
gates
and
one
input
OR
gate
for
a
two
level
sum
of
products
implementation
A
much
better
option
is
to
use
a
tree
of
two
input
XOR
gates
as
shown
in
Figure
Y
ABC
ABC
ABC
ABC
CHAPTER
TWO
Combinational
Logic
Design
Figure
Three
input
XOR
a
functional
specification
and
b
two
level
logic
implementation
B
C
A
Y
XOR
Y
A
B
C
A
B
Y
C
A
B
C
Y
a
b
Chapter
Selecting
the
best
multilevel
implementation
of
a
specific
logic
function
is
not
a
simple
process
Moreover
best
has
many
meanings
fewest
gates
fastest
shortest
design
time
least
cost
least
power
consumption
In
Chapter
you
will
see
that
the
best
circuit
in
one
technology
is
not
necessarily
the
best
in
another
For
example
we
have
been
using
ANDs
and
ORs
but
in
CMOS
NANDs
and
NORs
are
more
efficient
With
some
experience
you
will
find
that
you
can
create
a
good
multilevel
design
by
inspection
for
most
circuits
You
will
develop
some
of
this
experience
as
you
study
circuit
examples
through
the
rest
of
this
book
As
you
are
learning
explore
various
design
options
and
think
about
the
trade
offs
Computer
aided
design
CAD
tools
are
also
available
to
search
a
vast
space
of
possible
multilevel
designs
and
seek
the
one
that
best
fits
your
constraints
given
the
available
building
blocks
Bubble
Pushing
You
may
recall
from
Section
that
CMOS
circuits
prefer
NANDs
and
NORs
over
ANDs
and
ORs
But
reading
the
equation
by
inspection
from
a
multilevel
circuit
with
NANDs
and
NORs
can
get
pretty
hairy
Figure
shows
a
multilevel
circuit
whose
function
is
not
immediately
clear
by
inspection
Bubble
pushing
is
a
helpful
way
to
redraw
these
circuits
so
that
the
bubbles
cancel
out
and
the
function
can
be
more
easily
determined
Building
on
the
principles
from
Section
the
guidelines
for
bubble
pushing
are
as
follows
Begin
at
the
output
of
the
circuit
and
work
toward
the
inputs
Push
any
bubbles
on
the
final
output
back
toward
the
inputs
so
that
you
can
read
an
equation
in
terms
of
the
output
for
example
Y
instead
of
the
complement
of
the
output
Working
backward
draw
each
gate
in
a
form
so
that
bubbles
cancel
If
the
current
gate
has
an
input
bubble
draw
the
preceding
gate
with
an
output
bubble
If
the
current
gate
does
not
have
an
input
bubble
draw
the
preceding
gate
without
an
output
bubble
Figure
shows
how
to
redraw
Figure
according
to
the
bubble
pushing
guidelines
Starting
at
the
output
Y
the
NAND
gate
has
a
bubble
on
the
output
that
we
wish
to
eliminate
We
push
the
output
bubble
back
to
form
an
OR
with
inverted
inputs
shown
in
Y
Multilevel
Combinational
Logic
Figure
Three
input
XOR
using
two
two
input
XORs
A
B
Y
C
Figure
Eight
input
XOR
using
seven
two
input
XORs
Figure
Multilevel
circuit
using
NANDs
and
NORs
A
B
C
D
Y
Cha
Figure
a
Working
to
the
left
the
rightmost
gate
has
an
input
bubble
that
cancels
with
the
output
bubble
of
the
middle
NAND
gate
so
no
change
is
necessary
as
shown
in
Figure
b
The
middle
gate
has
no
input
bubble
so
we
transform
the
leftmost
gate
to
have
no
output
bubble
as
shown
in
Figure
c
Now
all
of
the
bubbles
in
the
circuit
cancel
except
at
the
inputs
so
the
function
can
be
read
by
inspection
in
terms
of
ANDs
and
ORs
of
true
or
complementary
inputs
For
emphasis
of
this
last
point
Figure
shows
a
circuit
logically
equivalent
to
the
one
in
Figure
The
functions
of
internal
nodes
are
labeled
in
blue
Because
bubbles
in
series
cancel
we
can
ignore
the
bubble
on
the
output
of
the
middle
gate
and
the
input
of
the
rightmost
gate
to
produce
the
logically
equivalent
circuit
of
Figure
Y
ABC
D
CHAPTER
TWO
Combinational
Logic
Design
A
B
C
Y
D
a
no
output
bubble
bubble
on
A
input
and
output
B
C
D
Y
b
A
B
C
D
Y
c
Y
ABC
D
no
bubble
on
input
and
output
Figure
Bubble
pushed
circuit
Figure
Logically
equivalent
bubble
pushed
circuit
A
B
C
D
Y
AB
ABC
Y
ABC
D
Ch
Example
BUBBLE
PUSHING
FOR
CMOS
LOGIC
Most
designers
think
in
terms
of
AND
and
OR
gates
but
suppose
you
would
like
to
implement
the
circuit
in
Figure
in
CMOS
logic
which
favors
NAND
and
NOR
gates
Use
bubble
pushing
to
convert
the
circuit
to
NANDs
NORs
and
inverters
Solution
A
brute
force
solution
is
to
just
replace
each
AND
gate
with
a
NAND
and
an
inverter
and
each
OR
gate
with
a
NOR
and
an
inverter
as
shown
in
Figure
This
requires
eight
gates
Notice
that
the
inverter
is
drawn
with
the
bubble
on
the
front
rather
than
back
to
emphasize
how
the
bubble
can
cancel
with
the
preceding
inverting
gate
For
a
better
solution
observe
that
bubbles
can
be
added
to
the
output
of
a
gate
and
the
input
of
the
next
gate
without
changing
the
function
as
shown
in
Figure
a
The
final
AND
is
converted
to
a
NAND
and
an
inverter
as
shown
in
Figure
b
This
solution
requires
only
five
gates
X
S
AND
Z
S
OH
MY
Boolean
algebra
is
limited
to
s
and
s
However
real
circuits
can
also
have
illegal
and
floating
values
represented
symbolically
by
X
and
Z
Illegal
Value
X
The
symbol
X
indicates
that
the
circuit
node
has
an
unknown
or
illegal
value
This
commonly
happens
if
it
is
being
driven
to
both
and
at
the
same
time
Figure
shows
a
case
where
node
Y
is
driven
both
HIGH
and
LOW
This
situation
called
contention
is
considered
to
be
an
error
X
s
and
Z
s
Oh
My
Figure
Circuit
using
ANDs
and
ORs
Figure
Poor
circuit
using
NANDs
and
NORs
a
b
Figure
Better
circuit
using
NANDs
and
NORs
Figure
Circuit
with
contention
A
Y
X
B
and
must
be
avoided
The
actual
voltage
on
a
node
with
contention
may
be
somewhere
between
and
VDD
depending
on
the
relative
strengths
of
the
gates
driving
HIGH
and
LOW
It
is
often
but
not
always
in
the
forbidden
zone
Contention
also
can
cause
large
amounts
of
power
to
flow
between
the
fighting
gates
resulting
in
the
circuit
getting
hot
and
possibly
damaged
X
values
are
also
sometimes
used
by
circuit
simulators
to
indicate
an
uninitialized
value
For
example
if
you
forget
to
specify
the
value
of
an
input
the
simulator
may
assume
it
is
an
X
to
warn
you
of
the
problem
As
mentioned
in
Section
digital
designers
also
use
the
symbol
X
to
indicate
don
t
care
values
in
truth
tables
Be
sure
not
to
mix
up
the
two
meanings
When
X
appears
in
a
truth
table
it
indicates
that
the
value
of
the
variable
in
the
truth
table
is
unimportant
When
X
appears
in
a
circuit
it
means
that
the
circuit
node
has
an
unknown
or
illegal
value
Floating
Value
Z
The
symbol
Z
indicates
that
a
node
is
being
driven
neither
HIGH
nor
LOW
The
node
is
said
to
be
floating
high
impedance
or
high
Z
A
typical
misconception
is
that
a
floating
or
undriven
node
is
the
same
as
a
logic
In
reality
a
floating
node
might
be
might
be
or
might
be
at
some
voltage
in
between
depending
on
the
history
of
the
system
A
floating
node
does
not
always
mean
there
is
an
error
in
the
circuit
so
long
as
some
other
circuit
element
does
drive
the
node
to
a
valid
logic
level
when
the
value
of
the
node
is
relevant
to
circuit
operation
One
common
way
to
produce
a
floating
node
is
to
forget
to
connect
a
voltage
to
a
circuit
input
or
to
assume
that
an
unconnected
input
is
the
same
as
an
input
with
the
value
of
This
mistake
may
cause
the
circuit
to
behave
erratically
as
the
floating
input
randomly
changes
from
to
Indeed
touching
the
circuit
may
be
enough
to
trigger
the
change
by
means
of
static
electricity
from
the
body
We
have
seen
circuits
that
operate
correctly
only
as
long
as
the
student
keeps
a
finger
pressed
on
a
chip
The
tristate
buffer
shown
in
Figure
has
three
possible
output
states
HIGH
LOW
and
floating
Z
The
tristate
buffer
has
an
input
A
an
output
Y
and
an
enable
E
When
the
enable
is
TRUE
the
tristate
buffer
acts
as
a
simple
buffer
transferring
the
input
value
to
the
output
When
the
enable
is
FALSE
the
output
is
allowed
to
float
Z
The
tristate
buffer
in
Figure
has
an
active
high
enable
That
is
when
the
enable
is
HIGH
the
buffer
is
enabled
Figure
shows
a
tristate
buffer
with
an
active
low
enable
When
the
enable
is
LOW
CHAPTER
TWO
Combinational
Logic
Design
Figure
Tristate
buffer
Figure
Tristate
buffer
with
active
low
enable
EAY
Z
Z
A
E
Y
EAY
Z
Z
A
E
Y
the
buffer
is
enabled
We
show
that
the
signal
is
active
low
by
putting
a
bubble
on
its
input
wire
We
often
indicate
an
active
low
input
by
drawing
a
bar
over
its
name
or
appending
the
word
bar
after
its
name
Ebar
Tristate
buffers
are
commonly
used
on
busses
that
connect
multiple
chips
For
example
a
microprocessor
a
video
controller
and
an
Ethernet
controller
might
all
need
to
communicate
with
the
memory
system
in
a
personal
computer
Each
chip
can
connect
to
a
shared
memory
bus
using
tristate
buffers
as
shown
in
Figure
Only
one
chip
at
a
time
is
allowed
to
assert
its
enable
signal
to
drive
a
value
onto
the
bus
The
other
chips
must
produce
floating
outputs
so
that
they
do
not
cause
contention
with
the
chip
talking
to
the
memory
Any
chip
can
read
the
information
from
the
shared
bus
at
any
time
Such
tristate
busses
were
once
common
However
in
modern
computers
higher
speeds
are
possible
with
point
to
point
links
in
which
chips
are
connected
to
each
other
directly
rather
than
over
a
shared
bus
KARNAUGH
MAPS
After
working
through
several
minimizations
of
Boolean
equations
using
Boolean
algebra
you
will
realize
that
if
you
re
not
careful
you
sometimes
end
up
with
a
completely
different
equation
instead
of
a
simplified
equation
Karnaugh
maps
K
maps
are
a
graphical
method
for
simplifying
Boolean
equations
They
were
invented
in
by
Maurice
Karnaugh
a
telecommunications
engineer
at
Bell
Labs
K
maps
work
well
for
problems
with
up
to
four
variables
More
important
they
give
insight
into
manipulating
Boolean
equations
E
Karnaugh
Maps
Figure
Tristate
bus
connecting
multiple
chips
en
to
bus
from
bus
en
to
bus
from
bus
en
to
bus
from
bus
en
to
bus
from
bus
Processor
Video
Ethernet
shared
bus
Memory
Maurice
Karnaugh
Graduated
with
a
bachelor
s
degree
in
physics
from
the
City
College
of
New
York
in
and
earned
a
Ph
D
in
physics
from
Yale
in
Worked
at
Bell
Labs
and
IBM
from
to
and
as
a
computer
science
professor
at
the
Polytechnic
University
of
New
York
from
to
Recall
that
logic
minimization
involves
combining
terms
Two
terms
containing
an
implicant
P
and
the
true
and
complementary
forms
of
some
variable
A
are
combined
to
eliminate
A
Karnaugh
maps
make
these
combinable
terms
easy
to
see
by
putting
them
next
to
each
other
in
a
grid
Figure
shows
the
truth
table
and
K
map
for
a
three
input
function
The
top
row
of
the
K
map
gives
the
four
possible
values
for
the
A
and
B
inputs
The
left
column
gives
the
two
possible
values
for
the
C
input
Each
square
in
the
K
map
corresponds
to
a
row
in
the
truth
table
and
contains
the
value
of
the
output
Y
for
that
row
For
example
the
top
left
square
corresponds
to
the
first
row
in
the
truth
table
and
indicates
that
the
output
value
Y
when
ABC
Just
like
each
row
in
a
truth
table
each
square
in
a
K
map
represents
a
single
minterm
For
the
purpose
of
explanation
Figure
c
shows
the
minterm
corresponding
to
each
square
in
the
K
map
Each
square
or
minterm
differs
from
an
adjacent
square
by
a
change
in
a
single
variable
This
means
that
adjacent
squares
share
all
the
same
literals
except
one
which
appears
in
true
form
in
one
square
and
in
complementary
form
in
the
other
For
example
the
squares
representing
the
minterms
and
are
adjacent
and
differ
only
in
the
variable
C
You
may
have
noticed
that
the
A
and
B
combinations
in
the
top
row
are
in
a
peculiar
order
This
order
is
called
a
Gray
code
It
differs
from
ordinary
binary
order
in
that
adjacent
entries
differ
only
in
a
single
variable
For
example
only
changes
A
from
to
while
would
change
A
from
to
and
B
from
to
Hence
writing
the
combinations
in
binary
order
would
not
have
produced
our
desired
property
of
adjacent
squares
differing
only
in
one
variable
The
K
map
also
wraps
around
The
squares
on
the
far
right
are
effectively
adjacent
to
the
squares
on
the
far
left
in
that
they
differ
only
in
one
variable
A
In
other
words
you
could
take
the
map
and
roll
it
into
a
cylinder
then
join
the
ends
of
the
cylinder
to
form
a
torus
i
e
a
donut
and
still
guarantee
that
adjacent
squares
would
differ
only
in
one
variable
Circular
Thinking
In
the
K
map
in
Figure
only
two
minterms
are
present
in
the
equation
and
as
indicated
by
the
s
in
the
left
column
Reading
the
minterms
from
the
K
map
is
exactly
equivalent
to
reading
equations
in
sum
of
products
form
directly
from
the
truth
table
ABC
ABC
ABC
ABC
PA
PA
P
CHAPTER
TWO
Combinational
Logic
Design
Gray
codes
were
patented
U
S
Patent
by
Frank
Gray
a
Bell
Labs
researcher
in
They
are
especially
useful
in
mechanical
encoders
because
a
slight
misalignment
causes
an
error
in
only
one
bit
Gray
codes
generalize
to
any
number
of
bits
For
example
a
bit
Gray
code
sequence
is
Lewis
Carroll
posed
a
related
puzzle
in
Vanity
Fair
in
The
rules
of
the
Puzzle
are
simple
enough
Two
words
are
proposed
of
the
same
length
and
the
puzzle
consists
of
linking
these
together
by
interposing
other
words
each
of
which
shall
differ
from
the
next
word
in
one
letter
only
That
is
to
say
one
letter
may
be
changed
in
one
of
the
given
words
then
one
letter
in
the
word
so
obtained
and
so
on
till
we
arrive
at
the
other
given
word
For
example
SHIP
to
DOCK
SHIP
SLIP
SLOP
SLOT
SOOT
LOOT
LOOK
LOCK
DOCK
Can
you
find
a
shorter
sequence
Chap
As
before
we
can
use
Boolean
algebra
to
minimize
equations
in
sum
ofproducts
form
K
maps
help
us
do
this
simplification
graphically
by
circling
s
in
adjacent
squares
as
shown
in
Figure
For
each
circle
we
write
the
corresponding
implicant
Remember
from
Section
that
an
implicant
is
the
product
of
one
or
more
literals
Variables
whose
true
and
complementary
forms
are
both
in
the
circle
are
excluded
from
the
implicant
In
this
case
the
variable
C
has
both
its
true
form
and
its
complementary
form
in
the
circle
so
we
do
not
include
it
in
the
implicant
In
other
words
Y
is
TRUE
when
A
B
independent
of
C
So
the
implicant
is
This
K
map
gives
the
same
answer
we
reached
using
Boolean
algebra
Logic
Minimization
with
K
Maps
K
maps
provide
an
easy
visual
way
to
minimize
logic
Simply
circle
all
the
rectangular
blocks
of
s
in
the
map
using
the
fewest
possible
number
of
circles
Each
circle
should
be
as
large
as
possible
Then
read
off
the
implicants
that
were
circled
More
formally
recall
that
a
Boolean
equation
is
minimized
when
it
is
written
as
a
sum
of
the
fewest
number
of
prime
implicants
Each
circle
on
the
K
map
represents
an
implicant
The
largest
possible
circles
are
prime
implicants
AB
Y
ABC
ABC
AB
C
C
AB
Karnaugh
Maps
Figure
Three
input
function
a
truth
table
b
K
map
c
K
map
showing
minterms
B
C
A
Y
a
C
Y
AB
b
C
Y
AB
ABC
ABC
ABC
ABC
ABC
ABC
ABC
ABC
c
C
Y
AB
Figure
K
map
minimization
Chapter
For
example
in
the
K
map
of
Figure
and
are
implicants
but
not
prime
implicants
Only
is
a
prime
implicant
in
that
K
map
Rules
for
finding
a
minimized
equation
from
a
K
map
are
as
follows
Use
the
fewest
circles
necessary
to
cover
all
the
s
All
the
squares
in
each
circle
must
contain
s
Each
circle
must
span
a
rectangular
block
that
is
a
power
of
i
e
or
squares
in
each
direction
Each
circle
should
be
as
large
as
possible
A
circle
may
wrap
around
the
edges
of
the
K
map
A
in
a
K
map
may
be
circled
multiple
times
if
doing
so
allows
fewer
circles
to
be
used
Example
MINIMIZATION
OF
A
THREE
VARIABLE
FUNCTION
USING
A
K
MAP
Suppose
we
have
the
function
Y
F
A
B
C
with
the
K
map
shown
in
Figure
Minimize
the
equation
using
the
K
map
Solution
Circle
the
s
in
the
K
map
using
as
few
circles
as
possible
as
shown
in
Figure
Each
circle
in
the
K
map
represents
a
prime
implicant
and
the
dimension
of
each
circle
is
a
power
of
two
and
We
form
the
prime
implicant
for
each
circle
by
writing
those
variables
that
appear
in
the
circle
only
in
true
or
only
in
complementary
form
For
example
in
the
circle
the
true
and
complementary
forms
of
B
are
included
in
the
circle
so
we
do
not
include
B
in
the
prime
implicant
However
only
the
true
form
of
A
A
and
complementary
form
of
C
are
in
this
circle
so
we
include
these
variables
in
the
prime
implicant
Similarly
the
circle
covers
all
squares
where
B
so
the
prime
implicant
is
Notice
how
the
top
right
square
minterm
is
covered
twice
to
make
the
prime
implicant
circles
as
large
as
possible
As
we
saw
with
Boolean
algebra
techniques
this
is
equivalent
to
sharing
a
minterm
to
reduce
the
size
of
the
B
AC
C
AB
ABC
ABC
CHAPTER
TWO
Combinational
Logic
Design
Figure
K
map
for
Example
Y
AB
C
Chapter
implicant
Also
notice
how
the
circle
covering
four
squares
wraps
around
the
sides
of
the
K
map
Example
SEVEN
SEGMENT
DISPLAY
DECODER
A
seven
segment
display
decoder
takes
a
bit
data
input
D
and
produces
seven
outputs
to
control
light
emitting
diodes
to
display
a
digit
from
to
The
seven
outputs
are
often
called
segments
a
through
g
or
Sa
Sg
as
defined
in
Figure
The
digits
are
shown
in
Figure
Write
a
truth
table
for
the
outputs
and
use
K
maps
to
find
Boolean
equations
for
outputs
Sa
and
Sb
Assume
that
illegal
input
values
produce
a
blank
readout
Solution
The
truth
table
is
given
in
Table
For
example
an
input
of
should
turn
on
all
segments
except
Sg
Each
of
the
seven
outputs
is
an
independent
function
of
four
variables
The
K
maps
for
outputs
Sa
and
Sb
are
shown
in
Figure
Remember
that
adjacent
squares
may
differ
in
only
a
single
variable
so
we
label
the
rows
and
columns
in
Gray
code
order
Be
careful
to
also
remember
this
ordering
when
entering
the
output
values
into
the
squares
Next
circle
the
prime
implicants
Use
the
fewest
number
of
circles
necessary
to
cover
all
the
s
A
circle
can
wrap
around
the
edges
vertical
and
horizontal
and
a
may
be
circled
more
than
once
Figure
shows
the
prime
implicants
and
the
simplified
Boolean
equations
Note
that
the
minimal
set
of
prime
implicants
is
not
unique
For
example
the
entry
in
the
Sa
K
map
was
circled
along
with
the
entry
to
produce
the
minterm
The
circle
could
have
included
the
entry
instead
producing
a
minterm
as
shown
with
dashed
lines
in
Figure
Figure
illustrates
see
page
a
common
error
in
which
a
nonprime
implicant
was
chosen
to
cover
the
in
the
upper
left
corner
This
minterm
gives
a
sum
of
products
equation
that
is
not
minimal
The
minterm
could
have
been
combined
with
either
of
the
adjacent
ones
to
form
a
larger
circle
as
was
done
in
the
previous
two
figures
D
D
D
D
D
D
D
D
D
D
Karnaugh
Maps
Figure
Solution
for
Example
Figure
Seven
segment
display
decoder
icon
Y
AB
C
AC
Y
AC
B
B
segment
display
decoder
a
b
c
d
g
e
f
D
S
CHAPTER
TWO
Combinational
Logic
Design
Figure
Karnaugh
maps
for
Sa
and
Sb
D
D
D
D
Sa
Sb
D
Sa
Sb
Sc
Sd
Se
Sf
Sg
others
Table
Seven
segment
display
decoder
truth
table
Figure
Seven
segment
display
digits
Don
t
Cares
Recall
that
don
t
care
entries
for
truth
table
inputs
were
introduced
in
Section
to
reduce
the
number
of
rows
in
the
table
when
some
variables
do
not
affect
the
output
They
are
indicated
by
the
symbol
X
which
means
that
the
entry
can
be
either
or
Don
t
cares
also
appear
in
truth
table
outputs
where
the
output
value
is
unimportant
or
the
corresponding
input
combination
can
never
happen
Such
outputs
can
be
treated
as
either
s
or
s
at
the
designer
s
discretion
Karnaugh
Maps
Figure
K
map
solution
for
Example
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
Sa
D
D
D
D
D
Sb
D
D
D
D
D
D
D
D
D
D
D
D
D
D
Sb
D
D
D
D
D
D
Figure
Alternative
K
map
for
Sa
showing
different
set
of
prime
implicants
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
D
Sa
In
a
K
map
X
s
allow
for
even
more
logic
minimization
They
can
be
circled
if
they
help
cover
the
s
with
fewer
or
larger
circles
but
they
do
not
have
to
be
circled
if
they
are
not
helpful
Example
SEVEN
SEGMENT
DISPLAY
DECODER
WITH
DON
T
CARES
Repeat
Example
if
we
don
t
care
about
the
output
values
for
illegal
input
values
of
to
Solution
The
K
map
is
shown
in
Figure
with
X
entries
representing
don
t
care
Because
don
t
cares
can
be
or
we
circle
a
don
t
care
if
it
allows
us
to
cover
the
s
with
fewer
or
bigger
circles
Circled
don
t
cares
are
treated
as
s
whereas
uncircled
don
t
cares
are
s
Observe
how
a
square
wrapping
around
all
four
corners
is
circled
for
segment
Sa
Use
of
don
t
cares
simplifies
the
logic
substantially
The
Big
Picture
Boolean
algebra
and
Karnaugh
maps
are
two
methods
for
logic
simplification
Ultimately
the
goal
is
to
find
a
low
cost
method
of
implementing
a
particular
logic
function
In
modern
engineering
practice
computer
programs
called
logic
synthesizers
produce
simplified
circuits
from
a
description
of
the
logic
function
as
we
will
see
in
Chapter
For
large
problems
logic
synthesizers
are
much
more
efficient
than
humans
For
small
problems
a
human
with
a
bit
of
experience
can
find
a
good
solution
by
inspection
Neither
of
the
authors
has
ever
used
a
Karnaugh
map
in
real
life
to
CHAPTER
TWO
Combinational
Logic
Design
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
Sa
D
D
D
D
D
D
D
D
D
D
D
D
Figure
Alternative
K
map
for
Sa
showing
incorrect
nonprime
implicant
solve
a
practical
problem
But
the
insight
gained
from
the
principles
underlying
Karnaugh
maps
is
valuable
And
Karnaugh
maps
often
appear
at
job
interviews
COMBINATIONAL
BUILDING
BLOCKS
Combinational
logic
is
often
grouped
into
larger
building
blocks
to
build
more
complex
systems
This
is
an
application
of
the
principle
of
abstraction
hiding
the
unnecessary
gate
level
details
to
emphasize
the
function
of
the
building
block
We
have
already
studied
three
such
building
blocks
full
adders
from
Section
priority
circuits
from
Section
and
seven
segment
display
decoders
from
Section
This
section
introduces
two
more
commonly
used
building
blocks
multiplexers
and
decoders
Chapter
covers
other
combinational
building
blocks
Multiplexers
Multiplexers
are
among
the
most
commonly
used
combinational
circuits
They
choose
an
output
from
among
several
possible
inputs
based
on
the
value
of
a
select
signal
A
multiplexer
is
sometimes
affectionately
called
a
mux
Multiplexer
Figure
shows
the
schematic
and
truth
table
for
a
multiplexer
with
two
data
inputs
D
and
D
a
select
input
S
and
one
output
Y
The
multiplexer
chooses
between
the
two
data
inputs
based
on
the
select
if
S
Y
D
and
if
S
Y
D
S
is
also
called
a
control
signal
because
it
controls
what
the
multiplexer
does
Combinational
Building
Blocks
Figure
K
map
solution
with
don
t
cares
X
X
X
X
X
X
X
X
X
X
X
X
D
D
Sa
D
D
Sb
Sa
D
D
D
D
D
D
Sb
D
D
D
D
D
D
D
Figure
multiplexer
symbol
and
truth
table
Y
S
D
Y
D
S
D
D
Chap
A
multiplexer
can
be
built
from
sum
of
products
logic
as
shown
in
Figure
The
Boolean
equation
for
the
multiplexer
may
be
derived
with
a
Karnaugh
map
or
read
off
by
inspection
Y
is
if
S
AND
D
is
OR
if
S
AND
D
is
Alternatively
multiplexers
can
be
built
from
tristate
buffers
as
shown
in
Figure
The
tristate
enables
are
arranged
such
that
at
all
times
exactly
one
tristate
buffer
is
active
When
S
tristate
T
is
enabled
allowing
D
to
flow
to
Y
When
S
tristate
T
is
enabled
allowing
D
to
flow
to
Y
Wider
Multiplexers
A
multiplexer
has
four
data
inputs
and
one
output
as
shown
in
Figure
Two
select
signals
are
needed
to
choose
among
the
four
data
inputs
The
multiplexer
can
be
built
using
sum
of
products
logic
tristates
or
multiple
multiplexers
as
shown
in
Figure
The
product
terms
enabling
the
tristates
can
be
formed
using
AND
gates
and
inverters
They
can
also
be
formed
using
a
decoder
which
we
will
introduce
in
Section
Wider
multiplexers
such
as
and
multiplexers
can
be
built
by
expanding
the
methods
shown
in
Figure
In
general
an
N
multiplexer
needs
log
N
select
lines
Again
the
best
implementation
choice
depends
on
the
target
technology
Multiplexer
Logic
Multiplexers
can
be
used
as
lookup
tables
to
perform
logic
functions
Figure
shows
a
multiplexer
used
to
implement
a
two
input
CHAPTER
TWO
Combinational
Logic
Design
D
Y
D
S
S
Y
D
Y
D
S
D
S
Figure
multiplexer
implementation
using
two
level
logic
Figure
Multiplexer
using
tristate
buffers
Figure
multiplexer
Y
D
S
T
T
Y
D
S
D
S
D
S
D
D
Y
D
D
Shorting
together
the
outputs
of
multiple
gates
technically
violates
the
rules
for
combinational
circuits
given
in
Section
But
because
exactly
one
of
the
outputs
is
driven
at
any
time
this
exception
is
allowed
Chap
AND
gate
The
inputs
A
and
B
serve
as
select
lines
The
multiplexer
data
inputs
are
connected
to
or
according
to
the
corresponding
row
of
the
truth
table
In
general
a
N
input
multiplexer
can
be
programmed
to
perform
any
N
input
logic
function
by
applying
s
and
s
to
the
appropriate
data
inputs
Indeed
by
changing
the
data
inputs
the
multiplexer
can
be
reprogrammed
to
perform
a
different
function
With
a
little
cleverness
we
can
cut
the
multiplexer
size
in
half
using
only
a
N
input
multiplexer
to
perform
any
N
input
logic
function
The
strategy
is
to
provide
one
of
the
literals
as
well
as
s
and
s
to
the
multiplexer
data
inputs
To
illustrate
this
principle
Figure
shows
two
input
AND
and
XOR
functions
implemented
with
multiplexers
We
start
with
an
ordinary
truth
table
and
then
combine
pairs
of
rows
to
eliminate
the
rightmost
input
variable
by
expressing
the
output
in
terms
of
this
variable
For
example
in
the
case
of
AND
when
A
Y
regardless
of
B
When
A
Y
if
B
and
Y
if
B
so
Y
B
We
then
use
the
multiplexer
as
a
lookup
table
according
to
the
new
smaller
truth
table
Example
LOGIC
WITH
MULTIPLEXERS
Alyssa
P
Hacker
needs
to
implement
the
function
to
finish
her
senior
project
but
when
she
looks
in
her
lab
kit
the
only
part
she
has
left
is
an
multiplexer
How
does
she
implement
the
function
Solution
Figure
shows
Alyssa
s
implementation
using
a
single
multiplexer
The
multiplexer
acts
as
a
lookup
table
where
each
row
in
the
truth
table
corresponds
to
a
multiplexer
input
Y
AB
BC
ABC
Combinational
Building
Blocks
Figure
multiplexer
implementations
a
twolevel
logic
b
tristates
c
hierarchical
a
Y
D
D
D
D
b
c
S
Y
S
D
D
D
D
Y
S
S
S
S
S
S
S
S
D
D
D
D
S
S
ABY
Y
AB
Y
A
B
Figure
multiplexer
implementation
of
two
input
AND
function
Chapter
Example
LOGIC
WITH
MULTIPLEXERS
REPRISED
Alyssa
turns
on
her
circuit
one
more
time
before
the
final
presentation
and
blows
up
the
multiplexer
She
accidently
powered
it
with
V
instead
of
V
after
not
sleeping
all
night
She
begs
her
friends
for
spare
parts
and
they
give
her
a
multiplexer
and
an
inverter
Can
she
build
her
circuit
with
only
these
parts
Solution
Alyssa
reduces
her
truth
table
to
four
rows
by
letting
the
output
depend
on
C
She
could
also
have
chosen
to
rearrange
the
columns
of
the
truth
table
to
let
the
output
depend
on
A
or
B
Figure
shows
the
new
design
Decoders
A
decoder
has
N
inputs
and
N
outputs
It
asserts
exactly
one
of
its
outputs
depending
on
the
input
combination
Figure
shows
a
decoder
When
A
Y
is
When
A
Y
is
And
so
forth
The
outputs
are
called
one
hot
because
exactly
one
is
hot
HIGH
at
a
given
time
CHAPTER
TWO
Combinational
Logic
Design
Figure
Alyssa
s
circuit
a
truth
table
b
multiplexer
implementation
AB
Y
C
a
Y
AB
BC
ABC
A
B
C
b
Y
Figure
Multiplexer
logic
using
variable
inputs
ABY
A
Y
A
B
Y
B
Y
A
B
B
B
b
ABY
Y
AB
A
Y
A
B
Y
B
a
Ch
Example
DECODER
IMPLEMENTATION
Implement
a
decoder
with
AND
OR
and
NOT
gates
Solution
Figure
shows
an
implementation
for
the
decoder
using
four
AND
gates
Each
gate
depends
on
either
the
true
or
the
complementary
form
of
each
input
In
general
an
N
N
decoder
can
be
constructed
from
N
N
input
AND
gates
that
accept
the
various
combinations
of
true
or
complementary
inputs
Each
output
in
a
decoder
represents
a
single
minterm
For
example
Y
represents
the
minterm
This
fact
will
be
handy
when
using
decoders
with
other
digital
building
blocks
A
A
Decoder
Logic
Decoders
can
be
combined
with
OR
gates
to
build
logic
functions
Figure
shows
the
two
input
XNOR
function
using
a
decoder
and
a
single
OR
gate
Because
each
output
of
a
decoder
represents
a
single
minterm
the
function
is
built
as
the
OR
of
all
the
minterms
in
the
function
In
Figure
Y
AB
AB
AB
Combinational
Building
Blocks
AB
Y
C
a
A
Y
B
C
C
Y
A
B
C
b
c
Figure
Alyssa
s
new
circuit
Figure
decoder
Decoder
A
A
Y
Y
Y
Y
A
A
Y
Y
Y
Y
Figure
decoder
implementation
A
A
Y
Y
Y
Y
Figure
Logic
function
using
decoder
Decoder
A
B
Y
A
B
Y
AB
AB
AB
AB
Minterm
Chap
When
using
decoders
to
build
logic
it
is
easiest
to
express
functions
as
a
truth
table
or
in
canonical
sum
of
products
form
An
N
input
function
with
M
s
in
the
truth
table
can
be
built
with
an
N
N
decoder
and
an
M
input
OR
gate
attached
to
all
of
the
minterms
containing
s
in
the
truth
table
This
concept
will
be
applied
to
the
building
of
Read
Only
Memories
ROMs
in
Section
TIMING
In
previous
sections
we
have
been
concerned
primarily
with
whether
the
circuit
works
ideally
using
the
fewest
gates
However
as
any
seasoned
circuit
designer
will
attest
one
of
the
most
challenging
issues
in
circuit
design
is
timing
making
a
circuit
run
fast
An
output
takes
time
to
change
in
response
to
an
input
change
Figure
shows
the
delay
between
an
input
change
and
the
subsequent
output
change
for
a
buffer
The
figure
is
called
a
timing
diagram
it
portrays
the
transient
response
of
the
buffer
circuit
when
an
input
changes
The
transition
from
LOW
to
HIGH
is
called
the
rising
edge
Similarly
the
transition
from
HIGH
to
LOW
not
shown
in
the
figure
is
called
the
falling
edge
The
blue
arrow
indicates
that
the
rising
edge
of
Y
is
caused
by
the
rising
edge
of
A
We
measure
delay
from
the
point
of
the
input
signal
A
to
the
point
of
the
output
signal
Y
The
point
is
the
point
at
which
the
signal
is
half
way
between
its
LOW
and
HIGH
values
as
it
transitions
Propagation
and
Contamination
Delay
Combinational
logic
is
characterized
by
its
propagation
delay
and
contamination
delay
The
propagation
delay
tpd
is
the
maximum
time
from
when
an
input
changes
until
the
output
or
outputs
reach
their
final
value
The
contamination
delay
tcd
is
the
minimum
time
from
when
an
input
changes
until
any
output
starts
to
change
its
value
CHAPTER
TWO
Combinational
Logic
Design
Figure
Circuit
delay
A
Y
Time
delay
A
Y
When
designers
speak
of
calculating
the
delay
of
a
circuit
they
generally
are
referring
to
the
worst
case
value
the
propagation
delay
unless
it
is
clear
otherwise
from
the
context
Figure
illustrates
a
buffer
s
propagation
delay
and
contamination
delay
in
blue
and
gray
respectively
The
figure
shows
that
A
is
initially
either
HIGH
or
LOW
and
changes
to
the
other
state
at
a
particular
time
we
are
interested
only
in
the
fact
that
it
changes
not
what
value
it
has
In
response
Y
changes
some
time
later
The
arcs
indicate
that
Y
may
start
to
change
tcd
after
A
transitions
and
that
Y
definitely
settles
to
its
new
value
within
tpd
The
underlying
causes
of
delay
in
circuits
include
the
time
required
to
charge
the
capacitance
in
a
circuit
and
the
speed
of
light
tpd
and
tcd
may
be
different
for
many
reasons
including
different
rising
and
falling
delays
multiple
inputs
and
outputs
some
of
which
are
faster
than
others
circuits
slowing
down
when
hot
and
speeding
up
when
cold
Calculating
tpd
and
tcd
requires
delving
into
the
lower
levels
of
abstraction
beyond
the
scope
of
this
book
However
manufacturers
normally
supply
data
sheets
specifying
these
delays
for
each
gate
Along
with
the
factors
already
listed
propagation
and
contamination
delays
are
also
determined
by
the
path
a
signal
takes
from
input
to
output
Figure
shows
a
four
input
logic
circuit
The
critical
path
shown
in
blue
is
the
path
from
input
A
or
B
to
output
Y
It
is
the
longest
and
therefore
the
slowest
path
because
the
input
travels
Timing
Figure
Propagation
and
contamination
delay
A
Y
A
Y
Time
tpd
tcd
A
B
C
D
Y
Critical
Path
Short
Path
n
n
Figure
Short
path
and
critical
path
Circuit
delays
are
ordinarily
on
the
order
of
picoseconds
ps
seconds
to
nanoseconds
ns
seconds
Trillions
of
picoseconds
have
elapsed
in
the
time
you
spent
reading
this
sidebar
Chapt
through
three
gates
to
the
output
This
path
is
critical
because
it
limits
the
speed
at
which
the
circuit
operates
The
short
path
through
the
circuit
shown
in
gray
is
from
input
D
to
output
Y
This
is
the
shortest
and
therefore
the
fastest
path
through
the
circuit
because
the
input
travels
through
only
a
single
gate
to
the
output
The
propagation
delay
of
a
combinational
circuit
is
the
sum
of
the
propagation
delays
through
each
element
on
the
critical
path
The
contamination
delay
is
the
sum
of
the
contamination
delays
through
each
element
on
the
short
path
These
delays
are
illustrated
in
Figure
and
are
described
by
the
following
equations
Example
FINDING
DELAYS
Ben
Bitdiddle
needs
to
find
the
propagation
delay
and
contamination
delay
of
the
circuit
shown
in
Figure
According
to
his
data
book
each
gate
has
a
propagation
delay
of
picoseconds
ps
and
a
contamination
delay
of
ps
Solution
Ben
begins
by
finding
the
critical
path
and
the
shortest
path
through
the
circuit
The
critical
path
highlighted
in
blue
in
Figure
is
from
input
A
tcd
tcd
AND
tpd
tpd
AND
tpd
OR
CHAPTER
TWO
Combinational
Logic
Design
A
Y
D
Y
delay
Time
A
Y
delay
A
B
C
D
Y
Short
Path
Critical
Path
Time
n
n
n
n
n
n
B
C
D
Figure
Critical
and
short
path
waveforms
Although
we
are
ignoring
wire
delay
in
this
analysis
digital
circuits
are
now
so
fast
that
the
delay
of
long
wires
can
be
as
important
as
the
delay
of
the
gates
The
speed
of
light
delay
in
wires
is
covered
in
Appendix
A
Cha
or
B
through
three
gates
to
the
output
Y
Hence
tpd
is
three
times
the
propagation
delay
of
a
single
gate
or
ps
The
shortest
path
shown
in
gray
in
Figure
is
from
input
C
D
or
E
through
two
gates
to
the
output
Y
There
are
only
two
gates
in
the
shortest
path
so
tcd
is
ps
Example
MULTIPLEXER
TIMING
CONTROL
CRITICAL
VS
DATA
CRITICAL
Compare
the
worst
case
timing
of
the
three
four
input
multiplexer
designs
shown
in
Figure
in
Section
Table
lists
the
propagation
delays
for
the
components
What
is
the
critical
path
for
each
design
Given
your
timing
analysis
why
might
you
choose
one
design
over
the
other
Solution
One
of
the
critical
paths
for
each
of
the
three
design
options
is
highlighted
in
blue
in
Figures
and
tpd
sy
indicates
the
propagation
delay
from
input
S
to
output
Y
tpd
dy
indicates
the
propagation
delay
from
input
D
to
output
Y
tpd
is
the
worst
of
the
two
max
tpd
sy
tpd
dy
For
both
the
two
level
logic
and
tristate
implementations
in
Figure
the
critical
path
is
from
one
of
the
control
signals
S
to
the
output
Y
tpd
tpd
sy
These
circuits
are
control
critical
because
the
critical
path
is
from
the
control
signals
to
the
output
Any
additional
delay
in
the
control
signals
will
add
directly
to
the
worst
case
delay
The
delay
from
D
to
Y
in
Figure
b
is
only
ps
compared
with
the
delay
from
S
to
Y
of
ps
Timing
A
B
C
D
E
Y
Figure
Ben
s
circuit
Figure
Ben
s
critical
path
A
B
C
D
E
Y
Figure
Ben
s
shortest
path
A
B
C
D
E
Y
C
Figure
shows
the
hierarchical
implementation
of
the
multiplexer
using
two
stages
of
multiplexers
The
critical
path
is
from
any
of
the
D
inputs
to
the
output
This
circuit
is
data
critical
because
the
critical
path
is
from
the
data
input
to
the
output
tpd
tpd
dy
If
data
inputs
arrive
well
before
the
control
inputs
we
would
prefer
the
design
with
the
shortest
control
to
output
delay
the
hierarchical
design
in
Figure
Similarly
if
the
control
inputs
arrive
well
before
the
data
inputs
we
would
prefer
the
design
with
the
shortest
data
to
output
delay
the
tristate
design
in
Figure
b
The
best
choice
depends
not
only
on
the
critical
path
through
the
circuit
and
the
input
arrival
times
but
also
on
the
power
cost
and
availability
of
parts
Glitches
So
far
we
have
discussed
the
case
where
a
single
input
transition
causes
a
single
output
transition
However
it
is
possible
that
a
single
input
transition
can
cause
multiple
output
transitions
These
are
called
glitches
or
hazards
Although
glitches
usually
don
t
cause
problems
it
is
important
to
realize
that
they
exist
and
recognize
them
when
looking
at
timing
diagrams
Figure
shows
a
circuit
with
a
glitch
and
the
Karnaugh
map
of
the
circuit
The
Boolean
equation
is
correctly
minimized
but
let
s
look
at
what
happens
when
A
C
and
B
transitions
from
to
Figure
see
page
illustrates
this
scenario
The
short
path
shown
in
gray
goes
through
two
gates
the
AND
and
OR
gates
The
critical
path
shown
in
blue
goes
through
an
inverter
and
two
gates
the
AND
and
OR
gates
CHAPTER
TWO
Combinational
Logic
Design
Gate
tpd
ps
NOT
input
AND
input
AND
input
OR
tristate
A
to
Y
tristate
enable
to
Y
Table
Timing
specifications
for
multiplexer
circuit
elements
Hazards
have
another
meaning
related
to
microarchitecture
in
Chapter
so
we
will
stick
with
the
term
glitches
for
multiple
output
transitions
to
avoid
confusion
Cha
Timing
Figure
multiplexer
propagation
delays
a
two
level
logic
b
tristate
tpd
sy
tpd
INV
tpd
AND
tpd
OR
ps
ps
ps
ps
S
D
D
D
D
Out
S
a
tpd
dy
tpd
AND
tpd
OR
ps
D
D
Out
S
S
tpd
sy
tpd
INV
tpd
AND
tpd
TRI
SY
ps
ps
ps
ps
b
tpd
dy
tpd
TRI
AY
ps
D
D
S
D
D
D
D
S
Y
tpd
s
y
tpd
TRLSY
tpd
TRI
AY
ns
mux
mux
mux
tpd
dy
tpd
TRI
AY
ns
Figure
multiplexer
propagation
delays
hierarchical
using
multiplexers
As
B
transitions
from
to
n
on
the
short
path
falls
before
n
on
the
critical
path
can
rise
Until
n
rises
the
two
inputs
to
the
OR
gate
are
and
the
output
Y
drops
to
When
n
eventually
rises
Y
returns
to
As
shown
in
the
timing
diagram
of
Figure
Y
starts
at
and
ends
at
but
momentarily
glitches
to
A
B
C
Y
Y
AB
C
Y
AB
BC
Figure
Circuit
with
a
glitch
CHAPTER
TWO
Combinational
Logic
Design
Figure
Input
change
crosses
implicant
boundary
Y
AB
C
Y
AB
BC
Figure
Timing
of
a
glitch
A
C
B
Y
Short
Path
Critical
Path
B
Y
Time
glitch
n
n
n
n
As
long
as
we
wait
for
the
propagation
delay
to
elapse
before
we
depend
on
the
output
glitches
are
not
a
problem
because
the
output
eventually
settles
to
the
right
answer
If
we
choose
to
we
can
avoid
this
glitch
by
adding
another
gate
to
the
implementation
This
is
easiest
to
understand
in
terms
of
the
K
map
Figure
shows
how
an
input
transition
on
B
from
ABC
to
ABC
moves
from
one
prime
implicant
circle
to
another
The
transition
across
the
boundary
of
two
prime
implicants
in
the
K
map
indicates
a
possible
glitch
As
we
saw
from
the
timing
diagram
in
Figure
if
the
circuitry
implementing
one
of
the
prime
implicants
turns
off
before
the
circuitry
of
the
other
prime
implicant
can
turn
on
there
is
a
glitch
To
fix
this
we
add
another
circle
that
covers
that
prime
implicant
boundary
as
shown
in
Figure
You
might
recognize
this
as
the
consensus
theorem
where
the
added
term
is
the
consensus
or
redundant
term
AC
Ch
Summary
Y
AB
C
AC
Y
AB
BC
AC
B
Y
A
C
Figure
K
map
without
glitch
Figure
Circuit
without
glitch
Figure
shows
the
glitch
proof
circuit
The
added
AND
gate
is
highlighted
in
blue
Now
a
transition
on
B
when
A
and
C
does
not
cause
a
glitch
on
the
output
because
the
blue
AND
gate
outputs
throughout
the
transition
In
general
a
glitch
can
occur
when
a
change
in
a
single
variable
crosses
the
boundary
between
two
prime
implicants
in
a
K
map
We
can
eliminate
the
glitch
by
adding
redundant
implicants
to
the
K
map
to
cover
these
boundaries
This
of
course
comes
at
the
cost
of
extra
hardware
However
simultaneous
transitions
on
multiple
variables
can
also
cause
glitches
These
glitches
cannot
be
fixed
by
adding
hardware
Because
the
vast
majority
of
interesting
systems
have
simultaneous
or
near
simultaneous
transitions
on
multiple
variables
glitches
are
a
fact
of
life
in
most
circuits
Although
we
have
shown
how
to
eliminate
one
kind
of
glitch
the
point
of
discussing
glitches
is
not
to
eliminate
them
but
to
be
aware
that
they
exist
This
is
especially
important
when
looking
at
timing
diagrams
on
a
simulator
or
oscilloscope
SUMMARY
A
digital
circuit
is
a
module
with
discrete
valued
inputs
and
outputs
and
a
specification
describing
the
function
and
timing
of
the
module
This
chapter
has
focused
on
combinational
circuits
circuits
whose
outputs
depend
only
on
the
current
values
of
the
inputs
Ch
The
function
of
a
combinational
circuit
can
be
given
by
a
truth
table
or
a
Boolean
equation
The
Boolean
equation
for
any
truth
table
can
be
obtained
systematically
using
sum
of
products
or
product
ofsums
form
In
sum
of
products
form
the
function
is
written
as
the
sum
OR
of
one
or
more
implicants
Implicants
are
the
product
AND
of
literals
Literals
are
the
true
or
complementary
forms
of
the
input
variables
Boolean
equations
can
be
simplified
using
the
rules
of
Boolean
algebra
In
particular
they
can
be
simplified
into
minimal
sum
of
products
form
by
combining
implicants
that
differ
only
in
the
true
and
complementary
forms
of
one
of
the
literals
Karnaugh
maps
are
a
visual
tool
for
minimizing
functions
of
up
to
four
variables
With
practice
designers
can
usually
simplify
functions
of
a
few
variables
by
inspection
Computer
aided
design
tools
are
used
for
more
complicated
functions
such
methods
and
tools
are
discussed
in
Chapter
Logic
gates
are
connected
to
create
combinational
circuits
that
perform
the
desired
function
Any
function
in
sum
of
products
form
can
be
built
using
two
level
logic
with
the
literals
as
inputs
NOT
gates
form
the
complementary
literals
AND
gates
form
the
products
and
OR
gates
form
the
sum
Depending
on
the
function
and
the
building
blocks
available
multilevel
logic
implementations
with
various
types
of
gates
may
be
more
efficient
For
example
CMOS
circuits
favor
NAND
and
NOR
gates
because
these
gates
can
be
built
directly
from
CMOS
transistors
without
requiring
extra
NOT
gates
When
using
NAND
and
NOR
gates
bubble
pushing
is
helpful
to
keep
track
of
the
inversions
Logic
gates
are
combined
to
produce
larger
circuits
such
as
multiplexers
decoders
and
priority
circuits
A
multiplexer
chooses
one
of
the
data
inputs
based
on
the
select
input
A
decoder
sets
one
of
the
outputs
HIGH
according
to
the
input
A
priority
circuit
produces
an
output
indicating
the
highest
priority
input
These
circuits
are
all
examples
of
combinational
building
blocks
Chapter
will
introduce
more
building
blocks
including
other
arithmetic
circuits
These
building
blocks
will
be
used
extensively
to
build
a
microprocessor
in
Chapter
The
timing
specification
of
a
combinational
circuit
consists
of
the
propagation
and
contamination
delays
through
the
circuit
These
indicate
the
longest
and
shortest
times
between
an
input
change
and
the
consequent
output
change
Calculating
the
propagation
delay
of
a
circuit
involves
identifying
the
critical
path
through
the
circuit
then
adding
up
the
propagation
delays
of
each
element
along
that
path
There
are
many
different
ways
to
implement
complicated
combinational
circuits
these
ways
offer
trade
offs
between
speed
and
cost
The
next
chapter
will
move
to
sequential
circuits
whose
outputs
depend
on
previous
as
well
as
current
values
of
the
inputs
In
other
words
sequential
circuits
have
memory
of
the
past
PA
PA
P
CHAPTER
TWO
Combinational
Logic
Design
Ch
Exercises
Exercises
Exercise
Write
a
Boolean
equation
in
sum
of
products
canonical
form
for
each
of
the
truth
tables
in
Figure
Exercise
Write
a
Boolean
equation
in
product
of
sums
canonical
form
for
the
truth
tables
in
Figure
Exercise
Minimize
each
of
the
Boolean
equations
from
Exercise
Exercise
Sketch
a
reasonably
simple
combinational
circuit
implementing
each
of
the
functions
from
Exercise
Reasonably
simple
means
that
you
are
not
wasteful
of
gates
but
you
don
t
waste
vast
amounts
of
time
checking
every
possible
implementation
of
the
circuit
either
Exercise
Repeat
Exercise
using
only
NOT
gates
and
AND
and
OR
gates
Exercise
Repeat
Exercise
using
only
NOT
gates
and
NAND
and
NOR
gates
Exercise
Simplify
the
following
Boolean
equations
using
Boolean
theorems
Check
for
correctness
using
a
truth
table
or
K
map
a
b
c
Y
ABCD
ABC
ABCD
ABD
ABCD
BCD
A
Y
AB
ABC
A
C
Y
AC
ABC
BCY
A
BCY
A
ABY
CDY
B
A
CDY
B
A
a
b
c
d
e
Figure
Truth
tables
Chapter
qx
CHAPTER
TWO
Combinational
Logic
Design
Exercise
Sketch
a
reasonably
simple
combinational
circuit
implementing
each
of
the
functions
from
Exercise
Exercise
Simplify
each
of
the
following
Boolean
equations
Sketch
a
reasonably
simple
combinational
circuit
implementing
the
simplified
equation
a
b
c
Y
ABC
ABD
ABE
ACD
ACE
Exercise
Give
an
example
of
a
truth
table
requiring
between
billion
and
billion
rows
that
can
be
constructed
using
fewer
than
but
at
least
two
input
gates
Exercise
Give
an
example
of
a
circuit
with
a
cyclic
path
that
is
nevertheless
combinational
Exercise
Alyssa
P
Hacker
says
that
any
Boolean
function
can
be
written
in
minimal
sum
of
products
form
as
the
sum
of
all
of
the
prime
implicants
of
the
function
Ben
Bitdiddle
says
that
there
are
some
functions
whose
minimal
equation
does
not
involve
all
of
the
prime
implicants
Explain
why
Alyssa
is
right
or
provide
a
counterexample
demonstrating
Ben
s
point
Exercise
Prove
that
the
following
theorems
are
true
using
perfect
induction
You
need
not
prove
their
duals
a
The
idempotency
theorem
T
b
The
distributivity
theorem
T
c
The
combining
theorem
T
Exercise
Prove
De
Morgan
s
Theorem
T
for
three
variables
B
B
B
using
perfect
induction
Exercise
Write
Boolean
equations
for
the
circuit
in
Figure
You
need
not
minimize
the
equations
BCE
BDE
CDE
A
D
E
BCD
Y
A
AB
AB
A
B
Y
BC
ABC
BC
Chapter
qxd
Exercises
Exercise
Minimize
the
Boolean
equations
from
Exercise
and
sketch
an
improved
circuit
with
the
same
function
Exercise
Using
De
Morgan
equivalent
gates
and
bubble
pushing
methods
redraw
the
circuit
in
Figure
so
that
you
can
find
the
Boolean
equation
by
inspection
Write
the
Boolean
equation
Exercise
Repeat
Exercise
for
the
circuit
in
Figure
A
B
C
D
Y
Z
A
B
C
D
E
Y
Figure
Circuit
schematic
Figure
Circuit
schematic
CHAPTER
TWO
Combinational
Logic
Design
Exercise
Find
a
minimal
Boolean
equation
for
the
function
in
Figure
Remember
to
take
advantage
of
the
don
t
care
entries
Exercise
Sketch
a
circuit
for
the
function
from
Exercise
Exercise
Does
your
circuit
from
Exercise
have
any
potential
glitches
when
one
of
the
inputs
changes
If
not
explain
why
not
If
so
show
how
to
modify
the
circuit
to
eliminate
the
glitches
Exercise
Ben
Bitdiddle
will
enjoy
his
picnic
on
sunny
days
that
have
no
ants
He
will
also
enjoy
his
picnic
any
day
he
sees
a
hummingbird
as
well
as
on
days
where
there
are
ants
and
ladybugs
Write
a
Boolean
equation
for
his
enjoyment
E
in
terms
of
sun
S
ants
A
hummingbirds
H
and
ladybugs
L
CDY
X
X
X
B
X
X
A
X
X
Figure
Truth
table
A
B
C
D
E
F
G
Y
Figure
Circuit
schematic
Exercises
Exercise
Complete
the
design
of
the
seven
segment
decoder
segments
Sc
through
Sg
see
Example
a
Derive
Boolean
equations
for
the
outputs
Sc
through
Sg
assuming
that
inputs
greater
than
must
produce
blank
outputs
b
Derive
Boolean
equations
for
the
outputs
Sc
through
Sg
assuming
that
inputs
greater
than
are
don
t
cares
c
Sketch
a
reasonably
simple
gate
level
implementation
of
part
b
Multiple
outputs
can
share
gates
where
appropriate
Exercise
A
circuit
has
four
inputs
and
two
outputs
The
inputs
A
represent
a
number
from
to
Output
P
should
be
TRUE
if
the
number
is
prime
and
are
not
prime
but
and
so
on
are
prime
Output
D
should
be
TRUE
if
the
number
is
divisible
by
Give
simplified
Boolean
equations
for
each
output
and
sketch
a
circuit
Exercise
A
priority
encoder
has
N
inputs
It
produces
an
N
bit
binary
output
indicating
the
most
significant
bit
of
the
input
that
is
TRUE
or
if
none
of
the
inputs
are
TRUE
It
also
produces
an
output
NONE
that
is
TRUE
if
none
of
the
input
bits
are
TRUE
Design
an
eight
input
priority
encoder
with
inputs
A
and
outputs
Y
and
NONE
For
example
if
the
input
is
the
output
Y
should
be
and
NONE
should
be
Give
a
simplified
Boolean
equation
for
each
output
and
sketch
a
schematic
Exercise
Design
a
modified
priority
encoder
see
Exercise
that
receives
an
bit
input
A
and
produces
two
bit
outputs
Y
and
Z
Y
indicates
the
most
significant
bit
of
the
input
that
is
TRUE
Z
indicates
the
second
most
significant
bit
of
the
input
that
is
TRUE
Y
should
be
if
none
of
the
inputs
are
TRUE
Z
should
be
if
no
more
than
one
of
the
inputs
is
TRUE
Give
a
simplified
Boolean
equation
for
each
output
and
sketch
a
schematic
Exercise
An
M
bit
thermometer
code
for
the
number
k
consists
of
k
s
in
the
least
significant
bit
positions
and
M
k
s
in
all
the
more
significant
bit
positions
A
binary
to
thermometer
code
converter
has
N
inputs
and
N
outputs
It
produces
a
N
bit
thermometer
code
for
the
number
specified
by
the
input
For
example
if
the
input
is
the
output
should
be
Design
a
binary
to
thermometer
code
converter
Give
a
simplified
Boolean
equation
for
each
output
and
sketch
a
schematic
Exercise
Write
a
minimized
Boolean
equation
for
the
function
performed
by
the
circuit
in
Figure
CHAPTER
TWO
Combinational
Logic
Design
Exercise
Implement
the
function
from
Figure
b
using
a
an
multiplexer
b
a
multiplexer
and
one
inverter
c
a
multiplexer
and
two
other
logic
gates
Exercise
Implement
the
function
from
Exercise
a
using
a
an
multiplexer
b
a
multiplexer
and
no
other
gates
c
a
multiplexer
one
OR
gate
and
an
inverter
Exercise
Determine
the
propagation
delay
and
contamination
delay
of
the
circuit
in
Figure
Use
the
gate
delays
given
in
Table
Exercise
Write
a
minimized
Boolean
equation
for
the
function
performed
by
the
circuit
in
Figure
Figure
Multiplexer
circuit
C
D
A
Y
Figure
Multiplexer
circuit
C
D
Y
A
B
Exercises
Exercise
Sketch
a
schematic
for
a
fast
decoder
Suppose
gate
delays
are
given
in
Table
and
only
the
gates
in
that
table
are
available
Design
your
decoder
to
have
the
shortest
possible
critical
path
and
indicate
what
that
path
is
What
are
its
propagation
delay
and
contamination
delay
Exercise
Redesign
the
circuit
from
Exercise
to
be
as
fast
as
possible
Use
only
the
gates
from
Table
Sketch
the
new
circuit
and
indicate
the
critical
path
What
are
its
propagation
delay
and
contamination
delay
Exercise
Redesign
the
priority
encoder
from
Exercise
to
be
as
fast
as
possible
You
may
use
any
of
the
gates
from
Table
Sketch
the
new
circuit
and
indicate
the
critical
path
What
are
its
propagation
delay
and
contamination
delay
Exercise
Design
an
multiplexer
with
the
shortest
possible
delay
from
the
data
inputs
to
the
output
You
may
use
any
of
the
gates
from
Table
on
page
Sketch
a
schematic
Using
the
gate
delays
from
the
table
determine
this
delay
Gate
tpd
ps
tcd
ps
NOT
input
NAND
input
NAND
input
NOR
input
NOR
input
AND
input
AND
input
OR
input
OR
input
XOR
Table
Gate
delays
for
Exercises
CHAPTER
TWO
Combinational
Logic
Design
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Sketch
a
schematic
for
the
two
input
XOR
function
using
only
NAND
gates
How
few
can
you
use
Question
Design
a
circuit
that
will
tell
whether
a
given
month
has
days
in
it
The
month
is
specified
by
a
bit
input
A
For
example
if
the
inputs
are
the
month
is
January
and
if
the
inputs
are
the
month
is
December
The
circuit
output
Y
should
be
HIGH
only
when
the
month
specified
by
the
inputs
has
days
in
it
Write
the
simplified
equation
and
draw
the
circuit
diagram
using
a
minimum
number
of
gates
Hint
Remember
to
take
advantage
of
don
t
cares
Question
What
is
a
tristate
buffer
How
and
why
is
it
used
Question
A
gate
or
set
of
gates
is
universal
if
it
can
be
used
to
construct
any
Boolean
function
For
example
the
set
AND
OR
NOT
is
universal
a
Is
an
AND
gate
by
itself
universal
Why
or
why
not
b
Is
the
set
OR
NOT
universal
Why
or
why
not
c
Is
a
NAND
gate
by
itself
universal
Why
or
why
not
Question
Explain
why
a
circuit
s
contamination
delay
might
be
less
than
instead
of
equal
to
its
propagation
delay
Introduction
Latches
and
Flip
Flops
Synchronous
Logic
Design
Finite
State
Machines
Timing
of
Sequential
Logic
Parallelism
Summary
Exercises
Interview
Questions
Sequential
Logic
Design
INTRODUCTION
In
the
last
chapter
we
showed
how
to
analyze
and
design
combinational
logic
The
output
of
combinational
logic
depends
only
on
current
input
values
Given
a
specification
in
the
form
of
a
truth
table
or
Boolean
equation
we
can
create
an
optimized
circuit
to
meet
the
specification
In
this
chapter
we
will
analyze
and
design
sequential
logic
The
outputs
of
sequential
logic
depend
on
both
current
and
prior
input
values
Hence
sequential
logic
has
memory
Sequential
logic
might
explicitly
remember
certain
previous
inputs
or
it
might
distill
the
prior
inputs
into
a
smaller
amount
of
information
called
the
state
of
the
system
The
state
of
a
digital
sequential
circuit
is
a
set
of
bits
called
state
variables
that
contain
all
the
information
about
the
past
necessary
to
explain
the
future
behavior
of
the
circuit
The
chapter
begins
by
studying
latches
and
flip
flops
which
are
simple
sequential
circuits
that
store
one
bit
of
state
In
general
sequential
circuits
are
complicated
to
analyze
To
simplify
design
we
discipline
ourselves
to
build
only
synchronous
sequential
circuits
consisting
of
combinational
logic
and
banks
of
flip
flops
containing
the
state
of
the
circuit
The
chapter
describes
finite
state
machines
which
are
an
easy
way
to
design
sequential
circuits
Finally
we
analyze
the
speed
of
sequential
circuits
and
discuss
parallelism
as
a
way
to
increase
clock
speed
LATCHES
AND
FLIP
FLOPS
The
fundamental
building
block
of
memory
is
a
bistable
element
an
element
with
two
stable
states
Figure
a
shows
a
simple
bistable
element
consisting
of
a
pair
of
inverters
connected
in
a
loop
Figure
b
shows
the
same
circuit
redrawn
to
emphasize
the
symmetry
The
inverters
are
cross
coupled
meaning
that
the
input
of
I
is
the
output
of
I
and
vice
versa
The
circuit
has
no
inputs
but
it
does
have
two
outputs
Q
and
Analyzing
this
circuit
is
different
from
analyzing
a
combinational
circuit
because
it
is
cyclic
Q
depends
on
and
depends
on
Q
Consider
the
two
cases
Q
is
or
Q
is
Working
through
the
consequences
of
each
case
we
have
Case
I
Q
As
shown
in
Figure
a
I
receives
a
FALSE
input
Q
so
it
produces
a
TRUE
output
on
I
receives
a
TRUE
input
so
it
produces
a
FALSE
output
on
Q
This
is
consistent
with
the
original
assumption
that
Q
so
the
case
is
said
to
be
stable
Case
II
Q
As
shown
in
Figure
b
I
receives
a
TRUE
input
and
produces
a
FALSE
output
on
I
receives
a
FALSE
input
and
produces
a
TRUE
output
on
Q
This
is
again
stable
Because
the
cross
coupled
inverters
have
two
stable
states
Q
and
Q
the
circuit
is
said
to
be
bistable
A
subtle
point
is
that
the
circuit
has
a
third
possible
state
with
both
outputs
approximately
halfway
between
and
This
is
called
a
metastable
state
and
will
be
discussed
in
Section
An
element
with
N
stable
states
conveys
log
N
bits
of
information
so
a
bistable
element
stores
one
bit
The
state
of
the
cross
coupled
inverters
is
contained
in
one
binary
state
variable
Q
The
value
of
Q
tells
us
everything
about
the
past
that
is
necessary
to
explain
the
future
behavior
of
the
circuit
Specifically
if
Q
it
will
remain
forever
and
if
Q
it
will
remain
forever
The
circuit
does
have
another
node
but
does
not
contain
any
additional
information
because
if
Q
is
known
is
also
known
On
the
other
hand
is
also
an
acceptable
choice
for
the
state
variable
Q
Q
Q
Q
Q
Q
Q
Q
Q
Q
CHAPTER
THREE
Sequential
Logic
Design
b
Q
Q
I
I
Q
Q
a
I
I
Figure
Cross
coupled
inverter
pair
b
Q
Q
I
I
a
I
Q
I
Q
Figure
Bistable
operation
of
cross
coupled
inverters
Just
as
Y
is
commonly
used
for
the
output
of
combinational
logic
Q
is
commonly
used
for
the
output
of
sequential
logic
Chapter
When
power
is
first
applied
to
a
sequential
circuit
the
initial
state
is
unknown
and
usually
unpredictable
It
may
differ
each
time
the
circuit
is
turned
on
Although
the
cross
coupled
inverters
can
store
a
bit
of
information
they
are
not
practical
because
the
user
has
no
inputs
to
control
the
state
However
other
bistable
elements
such
as
latches
and
flip
flops
provide
inputs
to
control
the
value
of
the
state
variable
The
remainder
of
this
section
considers
these
circuits
SR
Latch
One
of
the
simplest
sequential
circuits
is
the
SR
latch
which
is
composed
of
two
cross
coupled
NOR
gates
as
shown
in
Figure
The
latch
has
two
inputs
S
and
R
and
two
outputs
Q
and
The
SR
latch
is
similar
to
the
cross
coupled
inverters
but
its
state
can
be
controlled
through
the
S
and
R
inputs
which
set
and
reset
the
output
Q
A
good
way
to
understand
an
unfamiliar
circuit
is
to
work
out
its
truth
table
so
that
is
where
we
begin
Recall
that
a
NOR
gate
produces
a
FALSE
output
when
either
input
is
TRUE
Consider
the
four
possible
combinations
of
R
and
S
Case
I
R
S
N
sees
at
least
one
TRUE
input
R
so
it
produces
a
FALSE
output
on
Q
N
sees
both
Q
and
S
FALSE
so
it
produces
a
TRUE
output
on
Case
II
R
S
N
receives
inputs
of
and
Because
we
don
t
yet
know
we
can
t
determine
the
output
Q
N
receives
at
least
one
TRUE
input
S
so
it
produces
a
FALSE
output
on
Now
we
can
revisit
N
knowing
that
both
inputs
are
FALSE
so
the
output
Q
is
TRUE
Case
III
R
S
N
and
N
both
see
at
least
one
TRUE
input
R
or
S
so
each
produces
a
FALSE
output
Hence
Q
and
are
both
FALSE
Case
IV
R
S
N
receives
inputs
of
and
Because
we
don
t
yet
know
we
can
t
determine
the
output
N
receives
inputs
of
and
Q
Because
we
don
t
yet
know
Q
we
can
t
determine
the
output
Now
we
are
stuck
This
is
reminiscent
of
the
cross
coupled
inverters
But
we
know
that
Q
must
either
be
or
So
we
can
solve
the
problem
by
checking
what
happens
in
each
of
these
subcases
Q
Q
Q
Q
Q
Q
Q
Q
Latches
and
Flip
Flops
R
S
N
Q
N
Q
Figure
SR
latch
schematic
Chapter
q
Case
IVa
Q
Because
S
and
Q
are
FALSE
N
produces
a
TRUE
output
on
as
shown
in
Figure
a
Now
N
receives
one
TRUE
input
so
its
output
Q
is
FALSE
just
as
we
had
assumed
Case
IVb
Q
Because
Q
is
TRUE
N
produces
a
FALSE
output
on
as
shown
in
Figure
b
Now
N
receives
two
FALSE
inputs
R
and
so
its
output
Q
is
TRUE
just
as
we
had
assumed
Putting
this
all
together
suppose
Q
has
some
known
prior
value
which
we
will
call
Qprev
before
we
enter
Case
IV
Qprev
is
either
or
and
represents
the
state
of
the
system
When
R
and
S
are
Q
will
remember
this
old
value
Qprev
and
will
be
its
complement
This
circuit
has
memory
The
truth
table
in
Figure
summarizes
these
four
cases
The
inputs
S
and
R
stand
for
Set
and
Reset
To
set
a
bit
means
to
make
it
TRUE
To
reset
a
bit
means
to
make
it
FALSE
The
outputs
Q
and
are
normally
complementary
When
R
is
asserted
Q
is
reset
to
and
does
the
opposite
When
S
is
asserted
Q
is
set
to
and
does
the
opposite
When
neither
input
is
asserted
Q
remembers
its
old
value
Qprev
Asserting
both
S
and
R
simultaneously
doesn
t
make
much
sense
because
it
means
the
latch
should
be
set
and
reset
at
the
same
time
which
is
impossible
The
poor
confused
circuit
responds
by
making
both
outputs
The
SR
latch
is
represented
by
the
symbol
in
Figure
Using
the
symbol
is
an
application
of
abstraction
and
modularity
There
are
various
ways
to
build
an
SR
latch
such
as
using
different
logic
gates
or
transistors
Nevertheless
any
circuit
element
with
the
relationship
specified
by
the
truth
table
in
Figure
and
the
symbol
in
Figure
is
called
an
SR
latch
Like
the
cross
coupled
inverters
the
SR
latch
is
a
bistable
element
with
one
bit
of
state
stored
in
Q
However
the
state
can
be
controlled
through
the
S
and
R
inputs
When
R
is
asserted
the
state
is
reset
to
When
S
is
asserted
the
state
is
set
to
When
neither
is
asserted
the
state
retains
its
old
value
Notice
that
the
entire
history
of
inputs
can
be
Q
Q
Q
Q
Qprev
Q
Q
Q
Q
CHAPTER
THREE
Sequential
Logic
Design
R
S
N
Q
N
b
Q
R
S
N
Q
N
a
Q
Figure
Bistable
states
of
SR
latch
SRQ
Qprev
Case
IV
I
II
III
Q
Qprev
Figure
SR
latch
truth
table
S
R
Q
Q
Figure
SR
latch
symbol
Cha
accounted
for
by
the
single
state
variable
Q
No
matter
what
pattern
of
setting
and
resetting
occurred
in
the
past
all
that
is
needed
to
predict
the
future
behavior
of
the
SR
latch
is
whether
it
was
most
recently
set
or
reset
D
Latch
The
SR
latch
is
awkward
because
it
behaves
strangely
when
both
S
and
R
are
simultaneously
asserted
Moreover
the
S
and
R
inputs
conflate
the
issues
of
what
and
when
Asserting
one
of
the
inputs
determines
not
only
what
the
state
should
be
but
also
when
it
should
change
Designing
circuits
becomes
easier
when
these
questions
of
what
and
when
are
separated
The
D
latch
in
Figure
a
solves
these
problems
It
has
two
inputs
The
data
input
D
controls
what
the
next
state
should
be
The
clock
input
CLK
controls
when
the
state
should
change
Again
we
analyze
the
latch
by
writing
the
truth
table
given
in
Figure
b
For
convenience
we
first
consider
the
internal
nodes
S
and
R
If
CLK
both
S
and
R
are
FALSE
regardless
of
the
value
of
D
If
CLK
one
AND
gate
will
produce
TRUE
and
the
other
FALSE
depending
on
the
value
of
D
Given
S
and
R
Q
and
are
determined
using
Figure
Observe
that
when
CLK
Q
remembers
its
old
value
Qprev
When
CLK
Q
D
In
all
cases
is
the
complement
of
Q
as
would
seem
logical
The
D
latch
avoids
the
strange
case
of
simultaneously
asserted
R
and
S
inputs
Putting
it
all
together
we
see
that
the
clock
controls
when
data
flows
through
the
latch
When
CLK
the
latch
is
transparent
The
data
at
D
flows
through
to
Q
as
if
the
latch
were
just
a
buffer
When
CLK
the
latch
is
opaque
It
blocks
the
new
data
from
flowing
through
to
Q
and
Q
retains
the
old
value
Hence
the
D
latch
is
sometimes
called
a
transparent
latch
or
a
level
sensitive
latch
The
D
latch
symbol
is
given
in
Figure
c
The
D
latch
updates
its
state
continuously
while
CLK
We
shall
see
later
in
this
chapter
that
it
is
useful
to
update
the
state
only
at
a
specific
instant
in
time
The
D
flip
flop
described
in
the
next
section
does
just
that
Q
Q
D
Latches
and
Flip
Flops
S
R
Q
Q
D
CLK
D
R
S
a
Q
Q
CLK
D
Q
c
Q
S
R
Q
Qprev
CLK
D
X
D
X
b
Qprev
Q
Figure
D
latch
a
schematic
b
truth
table
c
symbol
Some
people
call
a
latch
open
or
closed
rather
than
transparent
or
opaque
However
we
think
those
terms
are
ambiguous
does
open
mean
transparent
like
an
open
door
or
opaque
like
an
open
circuit
Chapter
D
Flip
Flop
A
D
flip
flop
can
be
built
from
two
back
to
back
D
latches
controlled
by
complementary
clocks
as
shown
in
Figure
a
The
first
latch
L
is
called
the
master
The
second
latch
L
is
called
the
slave
The
node
between
them
is
named
N
A
symbol
for
the
D
flip
flop
is
given
in
Figure
b
When
the
output
is
not
needed
the
symbol
is
often
condensed
as
in
Figure
c
When
CLK
the
master
latch
is
transparent
and
the
slave
is
opaque
Therefore
whatever
value
was
at
D
propagates
through
to
N
When
CLK
the
master
goes
opaque
and
the
slave
becomes
transparent
The
value
at
N
propagates
through
to
Q
but
N
is
cut
off
from
D
Hence
whatever
value
was
at
D
immediately
before
the
clock
rises
from
to
gets
copied
to
Q
immediately
after
the
clock
rises
At
all
other
times
Q
retains
its
old
value
because
there
is
always
an
opaque
latch
blocking
the
path
between
D
and
Q
In
other
words
a
D
flip
flop
copies
D
to
Q
on
the
rising
edge
of
the
clock
and
remembers
its
state
at
all
other
times
Reread
this
definition
until
you
have
it
memorized
one
of
the
most
common
problems
for
beginning
digital
designers
is
to
forget
what
a
flip
flop
does
The
rising
edge
of
the
clock
is
often
just
called
the
clock
edge
for
brevity
The
D
input
specifies
what
the
new
state
will
be
The
clock
edge
indicates
when
the
state
should
be
updated
A
D
flip
flop
is
also
known
as
a
master
slave
flip
flop
an
edge
triggered
flip
flop
or
a
positive
edge
triggered
flip
flop
The
triangle
in
the
symbols
denotes
an
edge
triggered
clock
input
The
output
is
often
omitted
when
it
is
not
needed
Example
FLIP
FLOP
TRANSISTOR
COUNT
How
many
transistors
are
needed
to
build
the
D
flip
flop
described
in
this
section
Solution
A
NAND
or
NOR
gate
uses
four
transistors
A
NOT
gate
uses
two
transistors
An
AND
gate
is
built
from
a
NAND
and
a
NOT
so
it
uses
six
transistors
The
SR
latch
uses
two
NOR
gates
or
eight
transistors
The
D
latch
uses
an
SR
latch
two
AND
gates
and
a
NOT
gate
or
transistors
The
D
flip
flop
uses
two
D
latches
and
a
NOT
gate
or
transistors
Section
describes
a
more
efficient
CMOS
implementation
using
transmission
gates
Register
An
N
bit
register
is
a
bank
of
N
flip
flops
that
share
a
common
CLK
input
so
that
all
bits
of
the
register
are
updated
at
the
same
time
Q
Q
CHAPTER
THREE
Sequential
Logic
Design
a
CLK
D
Q
CLK
D
D
Q
Q
N
CLK
L
L
master
slave
b
D
Q
c
Q
Q
Q
Q
Figure
D
flip
flop
a
schematic
b
symbol
c
condensed
symbol
The
precise
distinction
between
flip
flops
and
latches
is
somewhat
muddled
and
has
evolved
over
time
In
common
industry
usage
a
flip
flop
is
edge
triggered
In
other
words
it
is
a
bistable
element
with
a
clock
input
The
state
of
the
flip
flop
changes
only
in
response
to
a
clock
edge
such
as
when
the
clock
rises
from
to
Bistable
elements
without
an
edge
triggered
clock
are
commonly
called
latches
The
term
flip
flop
or
latch
by
itself
usually
refers
to
a
D
flip
flop
or
D
latch
respectively
because
these
are
the
types
most
commonly
used
in
practice
Ch
Registers
are
the
key
building
block
of
most
sequential
circuits
Figure
shows
the
schematic
and
symbol
for
a
four
bit
register
with
inputs
D
and
outputs
Q
D
and
Q
are
both
bit
busses
Enabled
Flip
Flop
An
enabled
flip
flop
adds
another
input
called
EN
or
ENABLE
to
determine
whether
data
is
loaded
on
the
clock
edge
When
EN
is
TRUE
the
enabled
flip
flop
behaves
like
an
ordinary
D
flip
flop
When
EN
is
FALSE
the
enabled
flip
flop
ignores
the
clock
and
retains
its
state
Enabled
flip
flops
are
useful
when
we
wish
to
load
a
new
value
into
a
flip
flop
only
some
of
the
time
rather
than
on
every
clock
edge
Figure
shows
two
ways
to
construct
an
enabled
flip
flop
from
a
D
flip
flop
and
an
extra
gate
In
Figure
a
an
input
multiplexer
chooses
whether
to
pass
the
value
at
D
if
EN
is
TRUE
or
to
recycle
the
old
state
from
Q
if
EN
is
FALSE
In
Figure
b
the
clock
is
gated
Latches
and
Flip
Flops
CLK
D
Q
D
Q
D
Q
D
Q
D
D
D
D
Q
Q
Q
Q
a
D
Q
CLK
b
Figure
A
bit
register
a
schematic
and
b
symbol
b
D
Q
CLK
EN
D
Q
D
Q
EN
a
c
D
Q
EN
CLK
D
Q
Figure
Enabled
flip
flop
a
b
schematics
c
symbol
If
EN
is
TRUE
the
CLK
input
to
the
flip
flop
toggles
normally
If
EN
is
FALSE
the
CLK
input
is
also
FALSE
and
the
flip
flop
retains
its
old
value
Notice
that
EN
must
not
change
while
CLK
lest
the
flip
flop
see
a
clock
glitch
switch
at
an
incorrect
time
Generally
performing
logic
on
the
clock
is
a
bad
idea
Clock
gating
delays
the
clock
and
can
cause
timing
errors
as
we
will
see
in
Section
so
do
it
only
if
you
are
sure
you
know
what
you
are
doing
The
symbol
for
an
enabled
flipflop
is
given
in
Figure
c
Resettable
Flip
Flop
A
resettable
flip
flop
adds
another
input
called
RESET
When
RESET
is
FALSE
the
resettable
flip
flop
behaves
like
an
ordinary
D
flip
flop
When
RESET
is
TRUE
the
resettable
flip
flop
ignores
D
and
resets
the
output
to
Resettable
flip
flops
are
useful
when
we
want
to
force
a
known
state
i
e
into
all
the
flip
flops
in
a
system
when
we
first
turn
it
on
Such
flip
flops
may
be
synchronously
or
asynchronously
resettable
Synchronously
resettable
flip
flops
reset
themselves
only
on
the
rising
edge
of
CLK
Asynchronously
resettable
flip
flops
reset
themselves
as
soon
as
RESET
becomes
TRUE
independent
of
CLK
Figure
a
shows
how
to
construct
a
synchronously
resettable
flip
flop
from
an
ordinary
D
flip
flop
and
an
AND
gate
When
is
FALSE
the
AND
gate
forces
a
into
the
input
of
the
flip
flop
When
is
TRUE
the
AND
gate
passes
D
to
the
flip
flop
In
this
example
is
an
active
low
signal
meaning
that
the
reset
signal
performs
its
function
when
it
is
not
By
adding
an
inverter
the
circuit
could
have
accepted
an
active
high
RESET
signal
instead
Figures
b
and
c
show
symbols
for
the
resettable
flip
flop
with
active
high
RESET
Asynchronously
resettable
flip
flops
require
modifying
the
internal
structure
of
the
flip
flop
and
are
left
to
you
to
design
in
Exercise
however
they
are
frequently
available
to
the
designer
as
a
standard
component
As
you
might
imagine
settable
flip
flops
are
also
occasionally
used
They
load
a
into
the
flip
flop
when
SET
is
asserted
and
they
too
come
in
synchronous
and
asynchronous
flavors
Resettable
and
settable
flip
flops
may
also
have
an
enable
input
and
may
be
grouped
into
N
bit
registers
Transistor
Level
Latch
and
Flip
Flop
Designs
Example
showed
that
latches
and
flip
flops
require
a
large
number
of
transistors
when
built
from
logic
gates
But
the
fundamental
role
of
a
RESET
RESET
RESET
CHAPTER
THREE
Sequential
Logic
Design
a
D
Q
CLK
D
Q
RESET
D
Q
RESET
b
c
r
Figure
Synchronously
resettable
flip
flop
a
schematic
b
c
symbols
C
latch
is
to
be
transparent
or
opaque
much
like
a
switch
Recall
from
Section
that
a
transmission
gate
is
an
efficient
way
to
build
a
CMOS
switch
so
we
might
expect
that
we
could
take
advantage
of
transmission
gates
to
reduce
the
transistor
count
A
compact
D
latch
can
be
constructed
from
a
single
transmission
gate
as
shown
in
Figure
a
When
CLK
and
the
transmission
gate
is
ON
so
D
flows
to
Q
and
the
latch
is
transparent
When
CLK
and
the
transmission
gate
is
OFF
so
Q
is
isolated
from
D
and
the
latch
is
opaque
This
latch
suffers
from
two
major
limitations
Floating
output
node
When
the
latch
is
opaque
Q
is
not
held
at
its
value
by
any
gates
Thus
Q
is
called
a
floating
or
dynamic
node
After
some
time
noise
and
charge
leakage
may
disturb
the
value
of
Q
No
buffers
The
lack
of
buffers
has
caused
malfunctions
on
several
commercial
chips
A
spike
of
noise
that
pulls
D
to
a
negative
voltage
can
turn
on
the
nMOS
transistor
making
the
latch
transparent
even
when
CLK
Likewise
a
spike
on
D
above
VDD
can
turn
on
the
pMOS
transistor
even
when
CLK
And
the
transmission
gate
is
symmetric
so
it
could
be
driven
backward
with
noise
on
Q
affecting
the
input
D
The
general
rule
is
that
neither
the
input
of
a
transmission
gate
nor
the
state
node
of
a
sequential
circuit
should
ever
be
exposed
to
the
outside
world
where
noise
is
likely
Figure
b
shows
a
more
robust
transistor
D
latch
used
on
modern
commercial
chips
It
is
still
built
around
a
clocked
transmission
gate
but
it
adds
inverters
I
and
I
to
buffer
the
input
and
output
The
state
of
the
latch
is
held
on
node
N
Inverter
I
and
the
tristate
buffer
T
provide
feedback
to
turn
N
into
a
static
node
If
a
small
amount
of
noise
occurs
on
N
while
CLK
T
will
drive
N
back
to
a
valid
logic
value
Figure
shows
a
D
flip
flop
constructed
from
two
static
latches
controlled
by
and
CLK
Some
redundant
internal
inverters
have
been
removed
so
the
flip
flop
requires
only
transistors
CLK
CLK
CLK
Latches
and
Flip
Flops
a
CLK
D
Q
CLK
CLK
D
Q
N
b
CLK
CLK
CLK
I
I
I
T
Figure
D
latch
schematic
This
circuit
assumes
CLK
and
are
both
available
If
not
two
more
transistors
are
needed
for
a
CLK
inverter
CLK
CLK
D
N
CLK
CLK
CLK
I
I
T
I
CLK
CLK
T
CLK
CLK
I
Q
N
Figure
D
flip
flop
schematic
Chapter
Putting
It
All
Together
Latches
and
flip
flops
are
the
fundamental
building
blocks
of
sequential
circuits
Remember
that
a
D
latch
is
level
sensitive
whereas
a
D
flip
flop
is
edge
triggered
The
D
latch
is
transparent
when
CLK
allowing
the
input
D
to
flow
through
to
the
output
Q
The
D
flip
flop
copies
D
to
Q
on
the
rising
edge
of
CLK
At
all
other
times
latches
and
flip
flops
retain
their
old
state
A
register
is
a
bank
of
several
D
flip
flops
that
share
a
common
CLK
signal
Example
FLIP
FLOP
AND
LATCH
COMPARISON
Ben
Bitdiddle
applies
the
D
and
CLK
inputs
shown
in
Figure
to
a
D
latch
and
a
D
flip
flop
Help
him
determine
the
output
Q
of
each
device
Solution
Figure
shows
the
output
waveforms
assuming
a
small
delay
for
Q
to
respond
to
input
changes
The
arrows
indicate
the
cause
of
an
output
change
The
initial
value
of
Q
is
unknown
and
could
be
or
as
indicated
by
the
pair
of
horizontal
lines
First
consider
the
latch
On
the
first
rising
edge
of
CLK
D
so
Q
definitely
becomes
Each
time
D
changes
while
CLK
Q
also
follows
When
D
changes
while
CLK
it
is
ignored
Now
consider
the
flip
flop
On
each
rising
edge
of
CLK
D
is
copied
to
Q
At
all
other
times
Q
retains
its
state
CHAPTER
THREE
Sequential
Logic
Design
CLK
D
Q
latch
Q
flop
Figure
Example
waveforms
CLK
D
Q
latch
Q
flop
Figure
Solution
waveforms
Chap
SYNCHRONOUS
LOGIC
DESIGN
In
general
sequential
circuits
include
all
circuits
that
are
not
combinational
that
is
those
whose
output
cannot
be
determined
simply
by
looking
at
the
current
inputs
Some
sequential
circuits
are
just
plain
kooky
This
section
begins
by
examining
some
of
those
curious
circuits
It
then
introduces
the
notion
of
synchronous
sequential
circuits
and
the
dynamic
discipline
By
disciplining
ourselves
to
synchronous
sequential
circuits
we
can
develop
easy
systematic
ways
to
analyze
and
design
sequential
systems
Some
Problematic
Circuits
Example
ASTABLE
CIRCUITS
Alyssa
P
Hacker
encounters
three
misbegotten
inverters
who
have
tied
themselves
in
a
loop
as
shown
in
Figure
The
output
of
the
third
inverter
is
fed
back
to
the
first
inverter
Each
inverter
has
a
propagation
delay
of
ns
Determine
what
the
circuit
does
Solution
Suppose
node
X
is
initially
Then
Y
Z
and
hence
X
which
is
inconsistent
with
our
original
assumption
The
circuit
has
no
stable
states
and
is
said
to
be
unstable
or
astable
Figure
shows
the
behavior
of
the
circuit
If
X
rises
at
time
Y
will
fall
at
ns
Z
will
rise
at
ns
and
X
will
fall
again
at
ns
In
turn
Y
will
rise
at
ns
Z
will
fall
at
ns
and
X
will
rise
again
at
ns
and
then
the
pattern
will
repeat
Each
node
oscillates
between
and
with
a
period
repetition
time
of
ns
This
circuit
is
called
a
ring
oscillator
The
period
of
the
ring
oscillator
depends
on
the
propagation
delay
of
each
inverter
This
delay
depends
on
how
the
inverter
was
manufactured
the
power
supply
voltage
and
even
the
temperature
Therefore
the
ring
oscillator
period
is
difficult
to
accurately
predict
In
short
the
ring
oscillator
is
a
sequential
circuit
with
zero
inputs
and
one
output
that
changes
periodically
Synchronous
Logic
Design
X
Y
Z
Figure
Three
inverter
loop
X
Y
Z
time
ns
Figure
Ring
oscillator
waveforms
Example
RACE
CONDITIONS
Ben
Bitdiddle
designed
a
new
D
latch
that
he
claims
is
better
than
the
one
in
Figure
because
it
uses
fewer
gates
He
has
written
the
truth
table
to
find
Cha
the
output
Q
given
the
two
inputs
D
and
CLK
and
the
old
state
of
the
latch
Qprev
Based
on
this
truth
table
he
has
derived
Boolean
equations
He
obtains
Qprev
by
feeding
back
the
output
Q
His
design
is
shown
in
Figure
Does
his
latch
work
correctly
independent
of
the
delays
of
each
gate
Solution
Figure
shows
that
the
circuit
has
a
race
condition
that
causes
it
to
fail
when
certain
gates
are
slower
than
others
Suppose
CLK
D
The
latch
is
transparent
and
passes
D
through
to
make
Q
Now
CLK
falls
The
latch
should
remember
its
old
value
keeping
Q
However
suppose
the
delay
through
the
inverter
from
CLK
to
is
rather
long
compared
to
the
delays
of
the
AND
and
OR
gates
Then
nodes
N
and
Q
may
both
fall
before
rises
In
such
a
case
N
will
never
rise
and
Q
becomes
stuck
at
This
is
an
example
of
asynchronous
circuit
design
in
which
outputs
are
directly
fed
back
to
inputs
Asynchronous
circuits
are
infamous
for
having
race
conditions
where
the
behavior
of
the
circuit
depends
on
which
of
two
paths
through
logic
gates
is
fastest
One
circuit
may
work
while
a
seemingly
identical
one
built
from
gates
with
slightly
different
delays
may
not
work
Or
the
circuit
may
work
only
at
certain
temperatures
or
voltages
at
which
the
delays
are
just
right
These
mistakes
are
extremely
difficult
to
track
down
Synchronous
Sequential
Circuits
The
previous
two
examples
contain
loops
called
cyclic
paths
in
which
outputs
are
fed
directly
back
to
inputs
They
are
sequential
rather
than
combinational
circuits
Combinational
logic
has
no
cyclic
paths
and
no
races
If
inputs
are
applied
to
combinational
logic
the
outputs
will
always
settle
to
the
correct
value
within
a
propagation
delay
However
sequential
circuits
with
cyclic
paths
can
have
undesirable
races
or
unstable
behavior
Analyzing
such
circuits
for
problems
is
time
consuming
and
many
bright
people
have
made
mistakes
CLK
CLK
CHAPTER
THREE
Sequential
Logic
Design
CLK
N
N
Q
CLK
Figure
Latch
waveforms
illustrating
race
condition
Q
CLK
D
Qprev
CLK
D
CLK
Qprev
Q
N
CLK
D
N
CLK
Qprev
Q
CLK
D
CLK
Qprev
Figure
An
improved
D
latch
Chap
To
avoid
these
problems
designers
break
the
cyclic
paths
by
inserting
registers
somewhere
in
the
path
This
transforms
the
circuit
into
a
collection
of
combinational
logic
and
registers
The
registers
contain
the
state
of
the
system
which
changes
only
at
the
clock
edge
so
we
say
the
state
is
synchronized
to
the
clock
If
the
clock
is
sufficiently
slow
so
that
the
inputs
to
all
registers
settle
before
the
next
clock
edge
all
races
are
eliminated
Adopting
this
discipline
of
always
using
registers
in
the
feedback
path
leads
us
to
the
formal
definition
of
a
synchronous
sequential
circuit
Recall
that
a
circuit
is
defined
by
its
input
and
output
terminals
and
its
functional
and
timing
specifications
A
sequential
circuit
has
a
finite
set
of
discrete
states
S
S
Sk
A
synchronous
sequential
circuit
has
a
clock
input
whose
rising
edges
indicate
a
sequence
of
times
at
which
state
transitions
occur
We
often
use
the
terms
current
state
and
next
state
to
distinguish
the
state
of
the
system
at
the
present
from
the
state
to
which
it
will
enter
on
the
next
clock
edge
The
functional
specification
details
the
next
state
and
the
value
of
each
output
for
each
possible
combination
of
current
state
and
input
values
The
timing
specification
consists
of
an
upper
bound
tpcq
and
a
lower
bound
tccq
on
the
time
from
the
rising
edge
of
the
clock
until
the
output
changes
as
well
as
setup
and
hold
times
tsetup
and
thold
that
indicate
when
the
inputs
must
be
stable
relative
to
the
rising
edge
of
the
clock
The
rules
of
synchronous
sequential
circuit
composition
teach
us
that
a
circuit
is
a
synchronous
sequential
circuit
if
it
consists
of
interconnected
circuit
elements
such
that
Every
circuit
element
is
either
a
register
or
a
combinational
circuit
At
least
one
circuit
element
is
a
register
All
registers
receive
the
same
clock
signal
Every
cyclic
path
contains
at
least
one
register
Sequential
circuits
that
are
not
synchronous
are
called
asynchronous
A
flip
flop
is
the
simplest
synchronous
sequential
circuit
It
has
one
input
D
one
clock
CLK
one
output
Q
and
two
states
The
functional
specification
for
a
flip
flop
is
that
the
next
state
is
D
and
that
the
output
Q
is
the
current
state
as
shown
in
Figure
We
often
call
the
current
state
variable
S
and
the
next
state
variable
S
In
this
case
the
prime
after
S
indicates
next
state
not
inversion
The
timing
of
sequential
circuits
will
be
analyzed
in
Section
Two
other
common
types
of
synchronous
sequential
circuits
are
called
finite
state
machines
and
pipelines
These
will
be
covered
later
in
this
chapter
Synchronous
Logic
Design
tpcq
stands
for
the
time
of
propagation
from
clock
to
Q
where
Q
indicates
the
output
of
a
synchronous
sequential
circuit
tccq
stands
for
the
time
of
contamination
from
clock
to
Q
These
are
analogous
to
tpd
and
tcd
in
combinational
logic
D
Q
Next
State
Current
State
S
S
CLK
Figure
Flip
flop
current
state
and
next
state
This
definition
of
a
synchronous
sequential
circuit
is
sufficient
but
more
restrictive
than
necessary
For
example
in
high
performance
microprocessors
some
registers
may
receive
delayed
or
gated
clocks
to
squeeze
out
the
last
bit
of
performance
or
power
Similarly
some
microprocessors
use
latches
instead
of
registers
However
the
definition
is
adequate
for
all
of
the
synchronous
sequential
circuits
covered
in
this
book
and
for
most
commercial
digital
systems
Chapt
Example
SYNCHRONOUS
SEQUENTIAL
CIRCUITS
Which
of
the
circuits
in
Figure
are
synchronous
sequential
circuits
Solution
Circuit
a
is
combinational
not
sequential
because
it
has
no
registers
b
is
a
simple
sequential
circuit
with
no
feedback
c
is
neither
a
combinational
circuit
nor
a
synchronous
sequential
circuit
because
it
has
a
latch
that
is
neither
a
register
nor
a
combinational
circuit
d
and
e
are
synchronous
sequential
logic
they
are
two
forms
of
finite
state
machines
which
are
discussed
in
Section
f
is
neither
combinational
nor
synchronous
sequential
because
it
has
a
cyclic
path
from
the
output
of
the
combinational
logic
back
to
the
input
of
the
same
logic
but
no
register
in
the
path
g
is
synchronous
sequential
logic
in
the
form
of
a
pipeline
which
we
will
study
in
Section
h
is
not
strictly
speaking
a
synchronous
sequential
circuit
because
the
second
register
receives
a
different
clock
signal
than
the
first
delayed
by
two
inverter
delays
Synchronous
and
Asynchronous
Circuits
Asynchronous
design
in
theory
is
more
general
than
synchronous
design
because
the
timing
of
the
system
is
not
limited
by
clocked
registers
Just
as
analog
circuits
are
more
general
than
digital
circuits
because
analog
circuits
can
use
any
voltage
asynchronous
circuits
are
more
general
than
synchronous
circuits
because
they
can
use
any
kind
of
feedback
However
synchronous
circuits
have
proved
to
be
easier
to
design
and
use
than
asynchronous
circuits
just
as
digital
are
easier
than
analog
circuits
Despite
decades
of
research
on
asynchronous
circuits
virtually
all
digital
systems
are
essentially
synchronous
CHAPTER
THREE
Sequential
Logic
Design
CL
CLK
CL
a
CL
b
CLK
CL
CLK
c
d
CL
CLK
CL
e
CL
f
CLK
g
CL
CLK
CL
CLK
CLK
CLK
h
CL
Figure
Example
circuits
Of
course
asynchronous
circuits
are
occasionally
necessary
when
communicating
between
systems
with
different
clocks
or
when
receiving
inputs
at
arbitrary
times
just
as
analog
circuits
are
necessary
when
communicating
with
the
real
world
of
continuous
voltages
Furthermore
research
in
asynchronous
circuits
continues
to
generate
interesting
insights
some
of
which
can
improve
synchronous
circuits
too
FINITE
STATE
MACHINES
Synchronous
sequential
circuits
can
be
drawn
in
the
forms
shown
in
Figure
These
forms
are
called
finite
state
machines
FSMs
They
get
their
name
because
a
circuit
with
k
registers
can
be
in
one
of
a
finite
number
k
of
unique
states
An
FSM
has
M
inputs
N
outputs
and
k
bits
of
state
It
also
receives
a
clock
and
optionally
a
reset
signal
An
FSM
consists
of
two
blocks
of
combinational
logic
next
state
logic
and
output
logic
and
a
register
that
stores
the
state
On
each
clock
edge
the
FSM
advances
to
the
next
state
which
was
computed
based
on
the
current
state
and
inputs
There
are
two
general
classes
of
finite
state
machines
characterized
by
their
functional
specifications
In
Moore
machines
the
outputs
depend
only
on
the
current
state
of
the
machine
In
Mealy
machines
the
outputs
depend
on
both
the
current
state
and
the
current
inputs
Finite
state
machines
provide
a
systematic
way
to
design
synchronous
sequential
circuits
given
a
functional
specification
This
method
will
be
explained
in
the
remainder
of
this
section
starting
with
an
example
FSM
Design
Example
To
illustrate
the
design
of
FSMs
consider
the
problem
of
inventing
a
controller
for
a
traffic
light
at
a
busy
intersection
on
campus
Engineering
students
are
moseying
between
their
dorms
and
the
labs
on
Academic
Ave
They
are
busy
reading
about
FSMs
in
their
favorite
Finite
State
Machines
CLK
M
next
k
N
state
logic
output
logic
a
inputs
outputs
state
next
state
k
b
CLK
M
next
k
N
state
logic
output
logic
inputs
outputs
state
next
state
k
Figure
Finite
state
machines
a
Moore
machine
b
Mealy
machine
Moore
and
Mealy
machines
are
named
after
their
promoters
researchers
who
developed
automata
theory
the
mathematical
underpinnings
of
state
machines
at
Bell
Labs
Edward
F
Moore
not
to
be
confused
with
Intel
founder
Gordon
Moore
published
his
seminal
article
Gedankenexperiments
on
Sequential
Machines
in
He
subsequently
became
a
professor
of
mathematics
and
computer
science
at
the
University
of
Wisconsin
George
H
Mealy
published
A
Method
of
Synthesizing
Sequential
Circuits
in
He
subsequently
wrote
the
first
Bell
Labs
operating
system
for
the
IBM
computer
He
later
joined
Harvard
University
textbook
and
aren
t
looking
where
they
are
going
Football
players
are
hustling
between
the
athletic
fields
and
the
dining
hall
on
Bravado
Boulevard
They
are
tossing
the
ball
back
and
forth
and
aren
t
looking
where
they
are
going
either
Several
serious
injuries
have
already
occurred
at
the
intersection
of
these
two
roads
and
the
Dean
of
Students
asks
Ben
Bitdiddle
to
install
a
traffic
light
before
there
are
fatalities
Ben
decides
to
solve
the
problem
with
an
FSM
He
installs
two
traffic
sensors
TA
and
TB
on
Academic
Ave
and
Bravado
Blvd
respectively
Each
sensor
indicates
TRUE
if
students
are
present
and
FALSE
if
the
street
is
empty
He
also
installs
two
traffic
lights
LA
and
LB
to
control
traffic
Each
light
receives
digital
inputs
specifying
whether
it
should
be
green
yellow
or
red
Hence
his
FSM
has
two
inputs
TA
and
TB
and
two
outputs
LA
and
LB
The
intersection
with
lights
and
sensors
is
shown
in
Figure
Ben
provides
a
clock
with
a
second
period
On
each
clock
tick
rising
edge
the
lights
may
change
based
on
the
traffic
sensors
He
also
provides
a
reset
button
so
that
Physical
Plant
technicians
can
put
the
controller
in
a
known
initial
state
when
they
turn
it
on
Figure
shows
a
black
box
view
of
the
state
machine
Ben
s
next
step
is
to
sketch
the
state
transition
diagram
shown
in
Figure
to
indicate
all
the
possible
states
of
the
system
and
the
transitions
between
these
states
When
the
system
is
reset
the
lights
are
green
on
Academic
Ave
and
red
on
Bravado
Blvd
Every
seconds
the
controller
examines
the
traffic
pattern
and
decides
what
to
do
next
As
long
CHAPTER
THREE
Sequential
Logic
Design
TA
TB
LA
LB
CLK
Reset
Traffic
Light
Controller
Figure
Black
box
view
of
finite
state
machine
TA
LA
TA
LB
TB
TB
LA
LB
Academic
Ave
Bravado
Blvd
Dorms
Fields
Athletic
Dining
Hall
Labs
Figure
Campus
map
as
traffic
is
present
on
Academic
Ave
the
lights
do
not
change
When
there
is
no
longer
traffic
on
Academic
Ave
the
light
on
Academic
Ave
becomes
yellow
for
seconds
before
it
turns
red
and
Bravado
Blvd
s
light
turns
green
Similarly
the
Bravado
Blvd
light
remains
green
as
long
as
traffic
is
present
on
the
boulevard
then
turns
yellow
and
eventually
red
In
a
state
transition
diagram
circles
represent
states
and
arcs
represent
transitions
between
states
The
transitions
take
place
on
the
rising
edge
of
the
clock
we
do
not
bother
to
show
the
clock
on
the
diagram
because
it
is
always
present
in
a
synchronous
sequential
circuit
Moreover
the
clock
simply
controls
when
the
transitions
should
occur
whereas
the
diagram
indicates
which
transitions
occur
The
arc
labeled
Reset
pointing
from
outer
space
into
state
S
indicates
that
the
system
should
enter
that
state
upon
reset
regardless
of
what
previous
state
it
was
in
If
a
state
has
multiple
arcs
leaving
it
the
arcs
are
labeled
to
show
what
input
triggers
each
transition
For
example
when
in
state
S
the
system
will
remain
in
that
state
if
TA
is
TRUE
and
move
to
S
if
TA
is
FALSE
If
a
state
has
a
single
arc
leaving
it
that
transition
always
occurs
regardless
of
the
inputs
For
example
when
in
state
S
the
system
will
always
move
to
S
The
value
that
the
outputs
have
while
in
a
particular
state
are
indicated
in
the
state
For
example
while
in
state
S
LA
is
red
and
LB
is
green
Ben
rewrites
the
state
transition
diagram
as
a
state
transition
table
Table
which
indicates
for
each
state
and
input
what
the
next
state
S
should
be
Note
that
the
table
uses
don
t
care
symbols
X
whenever
the
next
state
does
not
depend
on
a
particular
input
Also
note
that
Reset
is
omitted
from
the
table
Instead
we
use
resettable
flip
flops
that
always
go
to
state
S
on
reset
independent
of
the
inputs
The
state
transition
diagram
is
abstract
in
that
it
uses
states
labeled
S
S
S
S
and
outputs
labeled
red
yellow
green
To
build
a
real
circuit
the
states
and
outputs
must
be
assigned
binary
encodings
Ben
chooses
the
simple
encodings
given
in
Tables
and
Each
state
and
each
output
is
encoded
with
two
bits
S
LA
and
LB
Finite
State
Machines
S
LA
green
LB
red
LA
red
LB
yellow
LA
yellow
LB
red
LA
red
LB
green
S
S
S
TA
TA
TB
TB
Reset
Figure
State
transition
diagram
Ben
updates
the
state
transition
table
to
use
these
binary
encodings
as
shown
in
Table
The
revised
state
transition
table
is
a
truth
table
specifying
the
next
state
logic
It
defines
next
state
S
as
a
function
of
the
current
state
S
and
the
inputs
The
revised
output
table
is
a
truth
table
specifying
the
output
logic
It
defines
the
outputs
LA
and
LB
as
functions
of
the
current
state
S
From
this
table
it
is
straightforward
to
read
off
the
Boolean
equations
for
the
next
state
in
sum
of
products
form
The
equations
can
be
simplified
using
Karnaugh
maps
but
often
doing
it
by
inspection
is
easier
For
example
the
TB
and
terms
in
the
S
equation
are
clearly
redundant
Thus
S
reduces
to
an
XOR
operation
Equation
gives
the
next
state
equations
TB
S
S
S
TA
S
S
TB
S
S
S
S
S
TB
S
S
TB
CHAPTER
THREE
Sequential
Logic
Design
Table
Output
encoding
Output
Encoding
L
green
yellow
red
Table
State
transition
table
with
binary
encodings
Current
State
Inputs
Next
State
S
S
TA
TB
S
S
X
X
XX
X
X
XX
Table
State
transition
table
Current
Inputs
Next
State
State
S
TA
TB
S
S
X
S
S
X
S
S
X
X
S
S
X
S
S
X
S
S
X
X
S
Table
State
encoding
State
Encoding
S
S
S
S
S
Ch
Similarly
Ben
writes
an
output
table
Table
indicating
for
each
state
what
the
output
should
be
in
that
state
Again
it
is
straightforward
to
read
off
and
simplify
the
Boolean
equations
for
the
outputs
For
example
observe
that
LA
is
TRUE
only
on
the
rows
where
S
is
TRUE
Finally
Ben
sketches
his
Moore
FSM
in
the
form
of
Figure
a
First
he
draws
the
bit
state
register
as
shown
in
Figure
a
On
each
clock
edge
the
state
register
copies
the
next
state
S
to
become
the
state
S
The
state
register
receives
a
synchronous
or
asynchronous
reset
to
initialize
the
FSM
at
startup
Then
he
draws
the
next
state
logic
based
on
Equation
which
computes
the
next
state
based
on
the
current
state
and
inputs
as
shown
in
Figure
b
Finally
he
draws
the
output
logic
based
on
Equation
which
computes
the
outputs
based
on
the
current
state
as
shown
in
Figure
c
Figure
shows
a
timing
diagram
illustrating
the
traffic
light
controller
going
through
a
sequence
of
states
The
diagram
shows
CLK
Reset
the
inputs
TA
and
TB
next
state
S
state
S
and
outputs
LA
and
LB
Arrows
indicate
causality
for
example
changing
the
state
causes
the
outputs
to
change
and
changing
the
inputs
causes
the
next
state
to
change
Dashed
lines
indicate
the
rising
edge
of
CLK
when
the
state
changes
The
clock
has
a
second
period
so
the
traffic
lights
change
at
most
once
every
seconds
When
the
finite
state
machine
is
first
turned
on
its
state
is
unknown
as
indicated
by
the
question
marks
Therefore
the
system
should
be
reset
to
put
it
into
a
known
state
In
this
timing
diagram
LB
S
S
LB
S
LA
S
S
LA
S
S
S
S
TA
S
S
TB
S
S
S
Finite
State
Machines
Table
Output
table
Current
State
Outputs
S
S
LA
LA
LB
LB
Chapte
CHAPTER
THREE
Sequential
Logic
Design
a
S
S
S
S
CLK
state
register
Reset
r
S
S
S
S
CLK
next
state
logic
state
register
Reset
TA
TB
inputs
b
S
S
r
S
S
S
S
CLK
next
state
logic
output
logic
state
register
Reset
LA
LB
LB
LA
TA
TB
inputs
outputs
c
S
S
r
Figure
State
machine
circuit
for
traffic
light
controller
CLK
Reset
TA
TB
S
S
LA
LB
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
S
S
S
S
t
sec
S
S
S
S
S
S
Green
Red
S
Yellow
Red
Green
Green
Yellow
Red
Figure
Timing
diagram
for
traffic
light
controller
This
schematic
uses
some
AND
gates
with
bubbles
on
the
inputs
They
might
be
constructed
with
AND
gates
and
input
inverters
with
NOR
gates
and
inverters
for
the
non
bubbled
inputs
or
with
some
other
combination
of
gates
The
best
choice
depends
on
the
particular
implementation
technology
S
immediately
resets
to
S
indicating
that
asynchronously
resettable
flipflops
are
being
used
In
state
S
light
LA
is
green
and
light
LB
is
red
In
this
example
traffic
arrives
immediately
on
Academic
Ave
Therefore
the
controller
remains
in
state
S
keeping
LA
green
even
though
traffic
arrives
on
Bravado
Blvd
and
starts
waiting
After
seconds
the
traffic
on
Academic
Ave
has
all
passed
through
and
TA
falls
At
the
following
clock
edge
the
controller
moves
to
state
S
turning
LA
yellow
In
another
seconds
the
controller
proceeds
to
state
S
in
which
LA
turns
red
and
LB
turns
green
The
controller
waits
in
state
S
until
all
the
traffic
on
Bravado
Blvd
has
passed
through
It
then
proceeds
to
state
S
turning
LB
yellow
seconds
later
the
controller
enters
state
S
turning
LB
red
and
LA
green
The
process
repeats
State
Encodings
In
the
previous
example
the
state
and
output
encodings
were
selected
arbitrarily
A
different
choice
would
have
resulted
in
a
different
circuit
A
natural
question
is
how
to
determine
the
encoding
that
produces
the
circuit
with
the
fewest
logic
gates
or
the
shortest
propagation
delay
Unfortunately
there
is
no
simple
way
to
find
the
best
encoding
except
to
try
all
possibilities
which
is
infeasible
when
the
number
of
states
is
large
However
it
is
often
possible
to
choose
a
good
encoding
by
inspection
so
that
related
states
or
outputs
share
bits
Computer
aided
design
CAD
tools
are
also
good
at
searching
the
set
of
possible
encodings
and
selecting
a
reasonable
one
One
important
decision
in
state
encoding
is
the
choice
between
binary
encoding
and
one
hot
encoding
With
binary
encoding
as
was
used
in
the
traffic
light
controller
example
each
state
is
represented
as
a
binary
number
Because
K
binary
numbers
can
be
represented
by
log
K
bits
a
system
with
K
states
only
needs
log
K
bits
of
state
In
one
hot
encoding
a
separate
bit
of
state
is
used
for
each
state
It
is
called
one
hot
because
only
one
bit
is
hot
or
TRUE
at
any
time
For
example
a
one
hot
encoded
FSM
with
three
states
would
have
state
encodings
of
and
Each
bit
of
state
is
stored
in
a
flip
flop
so
onehot
encoding
requires
more
flip
flops
than
binary
encoding
However
with
one
hot
encoding
the
next
state
and
output
logic
is
often
simpler
so
fewer
gates
are
required
The
best
encoding
choice
depends
on
the
specific
FSM
Example
FSM
STATE
ENCODING
A
divide
by
N
counter
has
one
output
and
no
inputs
The
output
Y
is
HIGH
for
one
clock
cycle
out
of
every
N
In
other
words
the
output
divides
the
frequency
of
the
clock
by
N
The
waveform
and
state
transition
diagram
for
a
divide
by
counter
is
shown
in
Figure
Sketch
circuit
designs
for
such
a
counter
using
binary
and
one
hot
state
encodings
Finite
State
Machines
Despite
Ben
s
best
efforts
students
don
t
pay
attention
to
traffic
lights
and
collisions
continue
to
occur
The
Dean
of
Students
next
asks
him
to
design
a
catapult
to
throw
engineering
students
directly
from
their
dorm
roofs
through
the
open
windows
of
the
lab
bypassing
the
troublesome
intersection
all
together
But
that
is
the
subject
of
another
textbook
Solution
Tables
and
show
the
abstract
state
transition
and
output
tables
before
encoding
Table
compares
binary
and
one
hot
encodings
for
the
three
states
The
binary
encoding
uses
two
bits
of
state
Using
this
encoding
the
state
transition
table
is
shown
in
Table
Note
that
there
are
no
inputs
the
next
state
depends
only
on
the
current
state
The
output
table
is
left
as
an
exercise
to
the
reader
The
next
state
and
output
equations
are
The
one
hot
encoding
uses
three
bits
of
state
The
state
transition
table
for
this
encoding
is
shown
in
Table
and
the
output
table
is
again
left
as
an
exercise
to
the
reader
The
next
state
and
output
equations
are
as
follows
Figure
shows
schematics
for
each
of
these
designs
Note
that
the
hardware
for
the
binary
encoded
design
could
be
optimized
to
share
the
same
gate
for
Y
and
S
Also
observe
that
the
one
hot
encoding
requires
both
settable
s
and
resettable
r
flip
flops
to
initialize
the
machine
to
S
on
reset
The
best
implementation
choice
depends
on
the
relative
cost
of
gates
and
flip
flops
but
the
one
hot
design
is
usually
preferable
for
this
specific
example
A
related
encoding
is
the
one
cold
encoding
in
which
K
states
are
represented
with
K
bits
exactly
one
of
which
is
FALSE
Y
S
S
S
S
S
S
S
Y
S
S
S
S
S
S
S
S
CHAPTER
THREE
Sequential
Logic
Design
CLK
Y
a
S
Y
S
Y
S
Y
Reset
b
Figure
Divide
by
counter
a
waveform
and
b
state
transition
diagram
Table
Divide
by
counter
state
transition
table
Current
Next
State
State
S
S
S
S
S
S
Table
Divide
by
counter
output
table
Current
State
Output
S
S
S
Chapter
Finite
State
Machines
Table
Binary
and
one
hot
encodings
for
divide
by
counter
State
Binary
Encoding
One
Hot
Encoding
S
S
S
S
S
S
S
S
Table
State
transition
table
with
binary
encoding
Current
State
Next
State
S
S
S
S
Table
State
transition
table
with
one
hot
encoding
Current
State
Next
State
S
S
S
S
S
S
S
S
S
S
CLK
Reset
Y
next
state
logic
state
register
output
logic
output
a
Reset
CLK
rrs
S
S
S
Y
b
S
S
r
Figure
Divide
by
circuits
for
a
binary
and
b
one
hot
encodings
Moore
and
Mealy
Machines
So
far
we
have
shown
examples
of
Moore
machines
in
which
the
output
depends
only
on
the
state
of
the
system
Hence
in
state
transition
diagrams
for
Moore
machines
the
outputs
are
labeled
in
the
circles
Recall
that
Mealy
machines
are
much
like
Moore
machines
but
the
outputs
can
depend
on
inputs
as
well
as
the
current
state
Hence
in
state
transition
diagrams
for
Mealy
machines
the
outputs
are
labeled
on
the
arcs
instead
of
in
the
circles
The
block
of
combinational
logic
that
computes
the
outputs
uses
the
current
state
and
inputs
as
was
shown
in
Figure
b
Example
MOORE
VERSUS
MEALY
MACHINES
Alyssa
P
Hacker
owns
a
pet
robotic
snail
with
an
FSM
brain
The
snail
crawls
from
left
to
right
along
a
paper
tape
containing
a
sequence
of
s
and
s
On
each
clock
cycle
the
snail
crawls
to
the
next
bit
The
snail
smiles
when
the
last
four
bits
that
it
has
crawled
over
are
from
left
to
right
Design
the
FSM
to
compute
when
the
snail
should
smile
The
input
A
is
the
bit
underneath
the
snail
s
antennae
The
output
Y
is
TRUE
when
the
snail
smiles
Compare
Moore
and
Mealy
state
machine
designs
Sketch
a
timing
diagram
for
each
machine
showing
the
input
states
and
output
as
your
snail
crawls
along
the
sequence
Solution
The
Moore
machine
requires
five
states
as
shown
in
Figure
a
Convince
yourself
that
the
state
transition
diagram
is
correct
In
particular
why
is
there
an
arc
from
S
to
S
when
the
input
is
In
comparison
the
Mealy
machine
requires
only
four
states
as
shown
in
Figure
b
Each
arc
is
labeled
as
A
Y
A
is
the
value
of
the
input
that
causes
that
transition
and
Y
is
the
corresponding
output
Tables
and
show
the
state
transition
and
output
tables
for
the
Moore
machine
The
Moore
machine
requires
at
least
three
bits
of
state
Consider
using
a
binary
state
encoding
S
S
S
S
and
S
Tables
and
rewrite
the
state
transition
and
output
tables
with
these
encodings
These
four
tables
follow
on
page
From
these
tables
we
find
the
next
state
and
output
equations
by
inspection
Note
that
these
equations
are
simplified
using
the
fact
that
states
and
do
not
exist
Thus
the
corresponding
next
state
and
output
for
the
nonexistent
states
are
don
t
cares
not
shown
in
the
tables
We
use
the
don
t
cares
to
minimize
our
equations
S
S
S
S
A
S
S
A
S
S
S
A
S
S
S
A
S
S
S
A
CHAPTER
THREE
Sequential
Logic
Design
An
easy
way
to
remember
the
difference
between
the
two
types
of
finite
state
machines
is
that
a
Moore
machine
typically
has
more
states
than
a
Mealy
machine
for
a
given
problem
Chapter
Table
shows
the
combined
state
transition
and
output
table
for
the
Mealy
machine
The
Mealy
machine
requires
at
least
two
bits
of
state
Consider
using
a
binary
state
encoding
S
S
S
and
S
Table
rewrites
the
state
transition
and
output
table
with
these
encodings
From
these
tables
we
find
the
next
state
and
output
equations
by
inspection
The
Moore
and
Mealy
machine
schematics
are
shown
in
Figure
a
and
b
respectively
The
timing
diagrams
for
the
Moore
and
Mealy
machines
are
shown
in
Figure
see
page
The
two
machines
follow
a
different
sequence
of
states
Moreover
the
Mealy
machine
s
output
rises
a
cycle
sooner
because
it
responds
to
the
input
rather
than
waiting
for
the
state
change
If
the
Mealy
output
were
delayed
through
a
flip
flop
it
would
match
the
Moore
output
When
choosing
your
FSM
design
style
consider
when
you
want
your
outputs
to
respond
Y
S
S
A
S
S
S
A
S
S
A
S
S
A
S
S
S
S
S
A
Y
S
Finite
State
Machines
Reset
a
S
S
S
S
S
Reset
b
S
S
S
S
Figure
FSM
state
transition
diagrams
a
Moore
machine
b
Mealy
machine
Chapter
CHAPTER
THREE
Sequential
Logic
Design
Table
Moore
output
table
with
state
encodings
Current
State
Output
S
S
S
Y
Table
Moore
state
transition
table
with
state
encodings
Current
State
Input
Next
State
S
S
S
A
S
S
S
Table
Moore
state
transition
table
Current
State
Input
Next
State
S
AS
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
Table
Moore
output
table
Current
Output
State
S
Y
S
S
S
S
S
Factoring
State
Machines
Designing
complex
FSMs
is
often
easier
if
they
can
be
broken
down
into
multiple
interacting
simpler
state
machines
such
that
the
output
of
some
machines
is
the
input
of
others
This
application
of
hierarchy
and
modularity
is
called
factoring
of
state
machines
Finite
State
Machines
Table
Mealy
state
transition
and
output
table
Current
State
Input
Next
State
Output
S
AS
Y
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
Table
Mealy
state
transition
and
output
table
with
state
encodings
Current
State
Input
Next
State
Output
S
S
A
S
S
Y
CHAPTER
THREE
Sequential
Logic
Design
Example
UNFACTORED
AND
FACTORED
STATE
MACHINES
Modify
the
traffic
light
controller
from
Section
to
have
a
parade
mode
which
keeps
the
Bravado
Boulevard
light
green
while
spectators
and
the
band
march
to
football
games
in
scattered
groups
The
controller
receives
two
more
inputs
P
and
R
Asserting
P
for
at
least
one
cycle
enters
parade
mode
Asserting
R
for
at
least
one
cycle
leaves
parade
mode
When
in
parade
mode
the
controller
proceeds
through
its
usual
sequence
until
LB
turns
green
then
remains
in
that
state
with
LB
green
until
parade
mode
ends
First
sketch
a
state
transition
diagram
for
a
single
FSM
as
shown
in
Figure
a
Then
sketch
the
state
transition
diagrams
for
two
interacting
FSMs
as
S
S
S
S
S
S
Y
CLK
Reset
A
a
S
S
S
b
S
S
CLK
Reset
S
S
A
Y
S
S
Figure
FSM
schematics
for
a
Moore
and
b
Mealy
machines
Finite
State
Machines
Mealy
Machine
Moore
Machine
CLK
Reset
A
S
Y
S
Y
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
Cycle
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
Figure
Timing
diagrams
for
Moore
and
Mealy
machines
Controller
TA
FSM
a
TB
LA
LB
b
Mode
FSM
Lights
FSM
P
M
Controller
FSM
LA
LB
TA
TB
R
P
R
Figure
a
single
and
b
factored
designs
for
modified
traffic
light
controller
FSM
shown
in
Figure
b
The
Mode
FSM
asserts
the
output
M
when
it
is
in
parade
mode
The
Lights
FSM
controls
the
lights
based
on
M
and
the
traffic
sensors
TA
and
TB
Solution
Figure
a
shows
the
single
FSM
design
States
S
to
S
handle
normal
mode
States
S
to
S
handle
parade
mode
The
two
halves
of
the
diagram
are
almost
identical
but
in
parade
mode
the
FSM
remains
in
S
with
a
green
light
on
Bravado
Blvd
The
P
and
R
inputs
control
movement
between
these
two
halves
The
FSM
is
messy
and
tedious
to
design
Figure
b
shows
the
factored
FSM
design
The
mode
FSM
has
two
states
to
track
whether
the
lights
are
in
normal
or
parade
mode
The
Lights
FSM
is
modified
to
remain
in
S
while
M
is
TRUE
FSM
Review
Finite
state
machines
are
a
powerful
way
to
systematically
design
sequential
circuits
from
a
written
specification
Use
the
following
procedure
to
design
an
FSM
Identify
the
inputs
and
outputs
Sketch
a
state
transition
diagram
CHAPTER
THREE
Sequential
Logic
Design
S
LA
green
LB
red
LA
green
LB
red
LA
yellow
LB
red
LA
yellow
LB
red
LA
red
LB
green
LA
red
LB
green
LA
red
LB
yellow
LA
red
LB
yellow
S
S
S
TA
TA
TB
TB
Reset
S
S
S
S
TA
TA
P
P
P
P
P
P
R
R
R
R
R
P
R
P
PTA
PTA
P
RTA
RTA
R
RTB
RTB
a
LA
green
LB
red
LA
yellow
LB
red
LA
red
LB
green
LA
red
LB
yellow
S
S
S
S
TA
TA
M
TB
MTB
Reset
Lights
FSM
b
S
M
S
M
P
Reset
P
Mode
FSM
R
R
Figure
State
transition
diagrams
a
unfactored
b
factored
Ch
For
a
Moore
machine
Write
a
state
transition
table
Write
an
output
table
For
a
Mealy
machine
Write
a
combined
state
transition
and
output
table
Select
state
encodings
your
selection
affects
the
hardware
design
Write
Boolean
equations
for
the
next
state
and
output
logic
Sketch
the
circuit
schematic
We
will
repeatedly
use
FSMs
to
design
complex
digital
systems
throughout
this
book
TIMING
OF
SEQUENTIAL
LOGIC
Recall
that
a
flip
flop
copies
the
input
D
to
the
output
Q
on
the
rising
edge
of
the
clock
This
process
is
called
sampling
D
on
the
clock
edge
If
D
is
stable
at
either
or
when
the
clock
rises
this
behavior
is
clearly
defined
But
what
happens
if
D
is
changing
at
the
same
time
the
clock
rises
This
problem
is
similar
to
that
faced
by
a
camera
when
snapping
a
picture
Imagine
photographing
a
frog
jumping
from
a
lily
pad
into
the
lake
If
you
take
the
picture
before
the
jump
you
will
see
a
frog
on
a
lily
pad
If
you
take
the
picture
after
the
jump
you
will
see
ripples
in
the
water
But
if
you
take
it
just
as
the
frog
jumps
you
may
see
a
blurred
image
of
the
frog
stretching
from
the
lily
pad
into
the
water
A
camera
is
characterized
by
its
aperture
time
during
which
the
object
must
remain
still
for
a
sharp
image
to
be
captured
Similarly
a
sequential
element
has
an
aperture
time
around
the
clock
edge
during
which
the
input
must
be
stable
for
the
flip
flop
to
produce
a
well
defined
output
The
aperture
of
a
sequential
element
is
defined
by
a
setup
time
and
a
hold
time
before
and
after
the
clock
edge
respectively
Just
as
the
static
discipline
limited
us
to
using
logic
levels
outside
the
forbidden
zone
the
dynamic
discipline
limits
us
to
using
signals
that
change
outside
the
aperture
time
By
taking
advantage
of
the
dynamic
discipline
we
can
think
of
time
in
discrete
units
called
clock
cycles
just
as
we
think
of
signal
levels
as
discrete
s
and
s
A
signal
may
glitch
and
oscillate
wildly
for
some
bounded
amount
of
time
Under
the
dynamic
discipline
we
are
concerned
only
about
its
final
value
at
the
end
of
the
clock
cycle
after
it
has
settled
to
a
stable
value
Hence
we
can
simply
write
A
n
the
value
of
signal
A
at
the
end
of
the
nth
clock
cycle
where
n
is
an
integer
rather
than
A
t
the
value
of
A
at
some
instant
t
where
t
is
any
real
number
The
clock
period
has
to
be
long
enough
for
all
signals
to
settle
This
sets
a
limit
on
the
speed
of
the
system
In
real
systems
the
clock
does
not
Timing
of
Sequential
Logic
Chap
reach
all
flip
flops
at
precisely
the
same
time
This
variation
in
time
called
clock
skew
further
increases
the
necessary
clock
period
Sometimes
it
is
impossible
to
satisfy
the
dynamic
discipline
especially
when
interfacing
with
the
real
world
For
example
consider
a
circuit
with
an
input
coming
from
a
button
A
monkey
might
press
the
button
just
as
the
clock
rises
This
can
result
in
a
phenomenon
called
metastability
where
the
flip
flop
captures
a
value
partway
between
and
that
can
take
an
unlimited
amount
of
time
to
resolve
into
a
good
logic
value
The
solution
to
such
asynchronous
inputs
is
to
use
a
synchronizer
which
has
a
very
small
but
nonzero
probability
of
producing
an
illegal
logic
value
We
expand
on
all
of
these
ideas
in
the
rest
of
this
section
The
Dynamic
Discipline
So
far
we
have
focused
on
the
functional
specification
of
sequential
circuits
Recall
that
a
synchronous
sequential
circuit
such
as
a
flip
flop
or
FSM
also
has
a
timing
specification
as
illustrated
in
Figure
When
the
clock
rises
the
output
or
outputs
may
start
to
change
after
the
clock
to
Q
contamination
delay
tccq
and
must
definitely
settle
to
the
final
value
within
the
clock
to
Q
propagation
delay
tpcq
These
represent
the
fastest
and
slowest
delays
through
the
circuit
respectively
For
the
circuit
to
sample
its
input
correctly
the
input
or
inputs
must
have
stabilized
at
least
some
setup
time
tsetup
before
the
rising
edge
of
the
clock
and
must
remain
stable
for
at
least
some
hold
time
thold
after
the
rising
edge
of
the
clock
The
sum
of
the
setup
and
hold
times
is
called
the
aperture
time
of
the
circuit
because
it
is
the
total
time
for
which
the
input
must
remain
stable
The
dynamic
discipline
states
that
the
inputs
of
a
synchronous
sequential
circuit
must
be
stable
during
the
setup
and
hold
aperture
time
around
the
clock
edge
By
imposing
this
requirement
we
guarantee
that
the
flip
flops
sample
signals
while
they
are
not
changing
Because
we
are
concerned
only
about
the
final
values
of
the
inputs
at
the
time
they
are
sampled
we
can
treat
signals
as
discrete
in
time
as
well
as
in
logic
levels
CHAPTER
THREE
Sequential
Logic
Design
CLK
tccq
tpcq
tsetup
output
s
input
s
thold
Figure
Timing
specification
for
synchronous
sequential
circuit
System
Timing
The
clock
period
or
cycle
time
Tc
is
the
time
between
rising
edges
of
a
repetitive
clock
signal
Its
reciprocal
fc
Tc
is
the
clock
frequency
All
else
being
the
same
increasing
the
clock
frequency
increases
the
work
that
a
digital
system
can
accomplish
per
unit
time
Frequency
is
measured
in
units
of
Hertz
Hz
or
cycles
per
second
megahertz
MHz
Hz
and
gigahertz
GHz
Hz
Figure
a
illustrates
a
generic
path
in
a
synchronous
sequential
circuit
whose
clock
period
we
wish
to
calculate
On
the
rising
edge
of
the
clock
register
R
produces
output
or
outputs
Q
These
signals
enter
a
block
of
combinational
logic
producing
D
the
input
or
inputs
to
register
R
The
timing
diagram
in
Figure
b
shows
that
each
output
signal
may
start
to
change
a
contamination
delay
after
its
input
change
and
settles
to
the
final
value
within
a
propagation
delay
after
its
input
settles
The
gray
arrows
represent
the
contamination
delay
through
R
and
the
combinational
logic
and
the
blue
arrows
represent
the
propagation
delay
through
R
and
the
combinational
logic
We
analyze
the
timing
constraints
with
respect
to
the
setup
and
hold
time
of
the
second
register
R
Setup
Time
Constraint
Figure
is
the
timing
diagram
showing
only
the
maximum
delay
through
the
path
indicated
by
the
blue
arrows
To
satisfy
the
setup
time
of
R
D
must
settle
no
later
than
the
setup
time
before
the
next
clock
edge
Timing
of
Sequential
Logic
CL
CLK
CLK
R
R
Q
D
a
CLK
Q
D
b
Tc
Figure
Path
between
registers
and
timing
diagram
CLK
Q
D
Tc
tpcq
tpd
tsetup
CL
CLK
CLK
Q
D
R
R
Figure
Maximum
delay
for
setup
time
constraint
In
the
three
decades
from
when
one
of
the
authors
families
bought
an
Apple
II
computer
to
the
present
time
of
writing
microprocessor
clock
frequencies
have
increased
from
MHz
to
several
GHz
a
factor
of
more
than
This
speedup
partially
explains
the
revolutionary
changes
computers
have
made
in
society
Cha
Hence
we
find
an
equation
for
the
minimum
clock
period
In
commercial
designs
the
clock
period
is
often
dictated
by
the
Director
of
Engineering
or
by
the
marketing
department
to
ensure
a
competitive
product
Moreover
the
flip
flop
clock
to
Q
propagation
delay
and
setup
time
tpcq
and
tsetup
are
specified
by
the
manufacturer
Hence
we
rearrange
Equation
to
solve
for
the
maximum
propagation
delay
through
the
combinational
logic
which
is
usually
the
only
variable
under
the
control
of
the
individual
designer
The
term
in
parentheses
tpcq
tsetup
is
called
the
sequencing
overhead
Ideally
the
entire
cycle
time
Tc
would
be
available
for
useful
computation
in
the
combinational
logic
tpd
However
the
sequencing
overhead
of
the
flip
flop
cuts
into
this
time
Equation
is
called
the
setup
time
constraint
or
max
delay
constraint
because
it
depends
on
the
setup
time
and
limits
the
maximum
delay
through
combinational
logic
If
the
propagation
delay
through
the
combinational
logic
is
too
great
D
may
not
have
settled
to
its
final
value
by
the
time
R
needs
it
to
be
stable
and
samples
it
Hence
R
may
sample
an
incorrect
result
or
even
an
illegal
logic
level
a
level
in
the
forbidden
region
In
such
a
case
the
circuit
will
malfunction
The
problem
can
be
solved
by
increasing
the
clock
period
or
by
redesigning
the
combinational
logic
to
have
a
shorter
propagation
delay
Hold
Time
Constraint
The
register
R
in
Figure
a
also
has
a
hold
time
constraint
Its
input
D
must
not
change
until
some
time
thold
after
the
rising
edge
of
the
clock
According
to
Figure
D
might
change
as
soon
as
tccq
tcd
after
the
rising
edge
of
the
clock
tpd
Tc
tpcq
tsetup
Tc
tpcq
tpd
tsetup
CHAPTER
THREE
Sequential
Logic
Design
CLK
Q
D
tccq
tcd
thold
CL
CLK
CLK
Q
D
R
R
Figure
Minimum
delay
for
hold
time
constraint
C
Hence
we
find
Again
tccq
and
thold
are
characteristics
of
the
flip
flop
that
are
usually
outside
the
designer
s
control
Rearranging
we
can
solve
for
the
minimum
contamination
delay
through
the
combinational
logic
Equation
is
also
called
the
min
delay
constraint
because
it
limits
the
minimum
delay
through
combinational
logic
We
have
assumed
that
any
logic
elements
can
be
connected
to
each
other
without
introducing
timing
problems
In
particular
we
would
expect
that
two
flip
flops
may
be
directly
cascaded
as
in
Figure
without
causing
hold
time
problems
In
such
a
case
tcd
because
there
is
no
combinational
logic
between
flip
flops
Substituting
into
Equation
yields
the
requirement
that
In
other
words
a
reliable
flip
flop
must
have
a
hold
time
shorter
than
its
contamination
delay
Often
flip
flops
are
designed
with
thold
so
that
Equation
is
always
satisfied
Unless
noted
otherwise
we
will
usually
make
that
assumption
and
ignore
the
hold
time
constraint
in
this
book
Nevertheless
hold
time
constraints
are
critically
important
If
they
are
violated
the
only
solution
is
to
increase
the
contamination
delay
through
the
logic
which
requires
redesigning
the
circuit
Unlike
setup
time
constraints
they
cannot
be
fixed
by
adjusting
the
clock
period
Redesigning
an
integrated
circuit
and
manufacturing
the
corrected
design
takes
months
and
millions
of
dollars
in
today
s
advanced
technologies
so
hold
time
violations
must
be
taken
extremely
seriously
Putting
It
All
Together
Sequential
circuits
have
setup
and
hold
time
constraints
that
dictate
the
maximum
and
minimum
delays
of
the
combinational
logic
between
flipflops
Modern
flip
flops
are
usually
designed
so
that
the
minimum
delay
through
the
combinational
logic
is
that
is
flip
flops
can
be
placed
back
to
back
The
maximum
delay
constraint
limits
the
number
of
consecutive
gates
on
the
critical
path
of
a
high
speed
circuit
because
a
high
clock
frequency
means
a
short
clock
period
Example
TIMING
ANALYSIS
Ben
Bitdiddle
designed
the
circuit
in
Figure
According
to
the
data
sheets
for
the
components
he
is
using
flip
flops
have
a
clock
to
Q
contamination
delay
thold
tccq
tcd
thold
tccq
tccq
tcd
thold
Timing
of
Sequential
Logic
CLK
Figure
Back
to
back
flip
flops
Cha
of
ps
and
a
propagation
delay
of
ps
They
have
a
setup
time
of
ps
and
a
hold
time
of
ps
Each
logic
gate
has
a
propagation
delay
of
ps
and
a
contamination
delay
of
ps
Help
Ben
determine
the
maximum
clock
frequency
and
whether
any
hold
time
violations
could
occur
This
process
is
called
timing
analysis
Solution
Figure
a
shows
waveforms
illustrating
when
the
signals
might
change
The
inputs
A
to
D
are
registered
so
they
change
shortly
after
CLK
rises
only
The
critical
path
occurs
when
B
C
D
and
A
rises
from
to
triggering
n
to
rise
X
to
rise
and
Y
to
fall
as
shown
in
Figure
b
This
path
involves
three
gate
delays
For
the
critical
path
we
assume
that
each
gate
requires
its
full
propagation
delay
Y
must
setup
before
the
next
rising
edge
of
the
CLK
Hence
the
minimum
cycle
time
is
The
maximum
clock
frequency
is
fc
Tc
GHz
A
short
path
occurs
when
A
and
C
rises
causing
X
to
rise
as
shown
in
Figure
c
For
the
short
path
we
assume
that
each
gate
switches
after
only
a
contamination
delay
This
path
involves
only
one
gate
delay
so
it
may
occur
after
tccq
tcd
ps
But
recall
that
the
flip
flop
has
a
hold
time
of
ps
meaning
that
X
must
remain
stable
for
ps
after
the
rising
edge
of
CLK
for
the
flip
flop
to
reliably
sample
its
value
In
this
case
X
at
the
first
rising
edge
of
CLK
so
we
want
the
flip
flop
to
capture
X
Because
X
did
not
hold
stable
long
enough
the
actual
value
of
X
is
unpredictable
The
circuit
has
a
hold
time
violation
and
may
behave
erratically
at
any
clock
frequency
Tc
tpcq
tpd
tsetup
ps
CHAPTER
THREE
Sequential
Logic
Design
CLK
CLK
A
B
C
D
n
X
Y
X
Y
Figure
Sample
circuit
for
timing
analysis
Chapter
q
Example
FIXING
HOLD
TIME
VIOLATIONS
Alyssa
P
Hacker
proposes
to
fix
Ben
s
circuit
by
adding
buffers
to
slow
down
the
short
paths
as
shown
in
Figure
The
buffers
have
the
same
delays
as
other
gates
Determine
the
maximum
clock
frequency
and
whether
any
hold
time
problems
could
occur
Solution
Figure
shows
waveforms
illustrating
when
the
signals
might
change
The
critical
path
from
A
to
Y
is
unaffected
because
it
does
not
pass
through
any
buffers
Therefore
the
maximum
clock
frequency
is
still
GHz
However
the
short
paths
are
slowed
by
the
contamination
delay
of
the
buffer
Now
X
will
not
change
until
tccq
tcd
ps
This
is
after
the
ps
hold
time
has
elapsed
so
the
circuit
now
operates
correctly
This
example
had
an
unusually
long
hold
time
to
illustrate
the
point
of
hold
time
problems
Most
flip
flops
are
designed
with
thold
tccq
to
avoid
such
problems
However
several
high
performance
microprocessors
including
the
Pentium
use
an
element
called
a
pulsed
latch
in
place
of
a
flip
flop
The
pulsed
latch
behaves
like
a
flip
flop
but
has
a
short
clock
to
Q
delay
and
a
long
hold
time
In
general
adding
buffers
can
usually
but
not
always
solve
hold
time
problems
without
slowing
the
critical
path
Timing
of
Sequential
Logic
A
D
CLK
n
X
Y
t
ps
a
A
n
X
Y
tpcq
tpd
tpd
tpd
tsetup
C
X
tccq
tcd
thold
b
c
Figure
Timing
diagram
a
general
case
b
critical
path
c
short
path
Ch
Clock
Skew
In
the
previous
analysis
we
assumed
that
the
clock
reaches
all
registers
at
exactly
the
same
time
In
reality
there
is
some
variation
in
this
time
This
variation
in
clock
edges
is
called
clock
skew
For
example
the
wires
from
the
clock
source
to
different
registers
may
be
of
different
lengths
resulting
in
slightly
different
delays
as
shown
in
Figure
Noise
also
results
in
different
delays
Clock
gating
described
in
Section
further
delays
the
clock
If
some
clocks
are
gated
and
others
are
not
there
will
be
substantial
skew
between
the
gated
and
ungated
clocks
In
CHAPTER
THREE
Sequential
Logic
Design
CLK
CLK
A
B
C
D
n
X
Y
X
Y
Buffers
added
to
fix
hold
time
violation
n
n
Figure
Corrected
circuit
to
fix
hold
time
problem
A
D
CLK
n
n
X
Y
t
ps
Figure
Timing
diagram
with
buffers
to
fix
hold
time
problem
t
skew
CLK
CLK
CL
CLK
CLK
R
R
Q
D
CLK
delay
CLK
Figure
Clock
skew
caused
by
wire
delay
Figure
CLK
is
early
with
respect
to
CLK
because
the
clock
wire
between
the
two
registers
follows
a
scenic
route
If
the
clock
had
been
routed
differently
CLK
might
have
been
early
instead
When
doing
timing
analysis
we
consider
the
worst
case
scenario
so
that
we
can
guarantee
that
the
circuit
will
work
under
all
circumstances
Figure
adds
skew
to
the
timing
diagram
from
Figure
The
heavy
clock
line
indicates
the
latest
time
at
which
the
clock
signal
might
reach
any
register
the
hashed
lines
show
that
the
clock
might
arrive
up
to
tskew
earlier
First
consider
the
setup
time
constraint
shown
in
Figure
In
the
worst
case
R
receives
the
latest
skewed
clock
and
R
receives
the
earliest
skewed
clock
leaving
as
little
time
as
possible
for
data
to
propagate
between
the
registers
The
data
propagates
through
the
register
and
combinational
logic
and
must
setup
before
R
samples
it
Hence
we
conclude
that
Next
consider
the
hold
time
constraint
shown
in
Figure
In
the
worst
case
R
receives
an
early
skewed
clock
CLK
and
R
receives
a
late
skewed
clock
CLK
The
data
zips
through
the
register
tpd
Tc
tpcq
tsetup
tskew
Tc
tpcq
tpd
tsetup
tskew
Timing
of
Sequential
Logic
CL
CLK
CLK
R
R
Q
D
a
CLK
Q
D
b
Tc
tskew
Figure
Timing
diagram
with
clock
skew
CLK
Q
D
Tc
tpcq
tpd
tsetuptskew
CL
CLK
CLK
R
R
Q
D
CLK
Figure
Setup
time
constraint
with
clock
skew
C
and
combinational
logic
but
must
not
arrive
until
a
hold
time
after
the
late
clock
Thus
we
find
that
In
summary
clock
skew
effectively
increases
both
the
setup
time
and
the
hold
time
It
adds
to
the
sequencing
overhead
reducing
the
time
available
for
useful
work
in
the
combinational
logic
It
also
increases
the
required
minimum
delay
through
the
combinational
logic
Even
if
thold
a
pair
of
back
to
back
flip
flops
will
violate
Equation
if
tskew
tccq
To
prevent
serious
hold
time
failures
designers
must
not
permit
too
much
clock
skew
Sometimes
flip
flops
are
intentionally
designed
to
be
particularly
slow
i
e
large
tccq
to
prevent
hold
time
problems
even
when
the
clock
skew
is
substantial
Example
TIMING
ANALYSIS
WITH
CLOCK
SKEW
Revisit
Example
and
assume
that
the
system
has
ps
of
clock
skew
Solution
The
critical
path
remains
the
same
but
the
setup
time
is
effectively
increased
by
the
skew
Hence
the
minimum
cycle
time
is
The
maximum
clock
frequency
is
fc
Tc
GHz
The
short
path
also
remains
the
same
at
ps
The
hold
time
is
effectively
increased
by
the
skew
to
ps
which
is
much
greater
than
ps
Hence
the
circuit
will
violate
the
hold
time
and
malfunction
at
any
frequency
The
circuit
violated
the
hold
time
constraint
even
without
skew
Skew
in
the
system
just
makes
the
violation
worse
ps
Tc
tpcq
tpd
tsetup
tskew
tcd
thold
tskew
tccq
tccq
tcd
thold
tskew
CHAPTER
THREE
Sequential
Logic
Design
tcd
thold
Q
D
tskew
CL
CLK
CLK
R
R
Q
D
CLK
CLK
tccq
Figure
Hold
time
constraint
with
clock
skew
Chapter
Example
FIXING
HOLD
TIME
VIOLATIONS
Revisit
Example
and
assume
that
the
system
has
ps
of
clock
skew
Solution
The
critical
path
is
unaffected
so
the
maximum
clock
frequency
remains
GHz
The
short
path
increases
to
ps
This
is
still
less
than
thold
tskew
ps
so
the
circuit
still
violates
its
hold
time
constraint
To
fix
the
problem
even
more
buffers
could
be
inserted
Buffers
would
need
to
be
added
on
the
critical
path
as
well
reducing
the
clock
frequency
Alternatively
a
better
flip
flop
with
a
shorter
hold
time
might
be
used
Metastability
As
noted
earlier
it
is
not
always
possible
to
guarantee
that
the
input
to
a
sequential
circuit
is
stable
during
the
aperture
time
especially
when
the
input
arrives
from
the
external
world
Consider
a
button
connected
to
the
input
of
a
flip
flop
as
shown
in
Figure
When
the
button
is
not
pressed
D
When
the
button
is
pressed
D
A
monkey
presses
the
button
at
some
random
time
relative
to
the
rising
edge
of
CLK
We
want
to
know
the
output
Q
after
the
rising
edge
of
CLK
In
Case
I
when
the
button
is
pressed
much
before
CLK
Q
In
Case
II
when
the
button
is
not
pressed
until
long
after
CLK
Q
But
in
Case
III
when
the
button
is
pressed
sometime
between
tsetup
before
CLK
and
thold
after
CLK
the
input
violates
the
dynamic
discipline
and
the
output
is
undefined
Metastable
State
In
reality
when
a
flip
flop
samples
an
input
that
is
changing
during
its
aperture
the
output
Q
may
momentarily
take
on
a
voltage
between
and
VDD
that
is
in
the
forbidden
zone
This
is
called
a
metastable
state
Eventually
the
flip
flop
will
resolve
the
output
to
a
stable
state
of
either
or
However
the
resolution
time
required
to
reach
the
stable
state
is
unbounded
The
metastable
state
of
a
flip
flop
is
analogous
to
a
ball
on
the
summit
of
a
hill
between
two
valleys
as
shown
in
Figure
The
two
valleys
are
stable
states
because
a
ball
in
the
valley
will
remain
there
as
long
as
it
is
not
disturbed
The
top
of
the
hill
is
called
metastable
because
the
ball
would
remain
there
if
it
were
perfectly
balanced
But
because
nothing
is
perfect
the
ball
will
eventually
roll
to
one
side
or
the
other
The
time
required
for
this
change
to
occur
depends
on
how
nearly
well
balanced
the
ball
originally
was
Every
bistable
device
has
a
metastable
state
between
the
two
stable
states
Timing
of
Sequential
Logic
D
Q
CLK
button
CLK
tsetup
thold
aperture
D
Q
D
Q
D
Q
Case
I
Case
II
Case
III
Figure
Input
changing
before
after
or
during
aperture
Chapt
Resolution
Time
If
a
flip
flop
input
changes
at
a
random
time
during
the
clock
cycle
the
resolution
time
tres
required
to
resolve
to
a
stable
state
is
also
a
random
variable
If
the
input
changes
outside
the
aperture
then
tres
tpcq
But
if
the
input
happens
to
change
within
the
aperture
tres
can
be
substantially
longer
Theoretical
and
experimental
analyses
see
Section
have
shown
that
the
probability
that
the
resolution
time
tres
exceeds
some
arbitrary
time
t
decreases
exponentially
with
t
where
Tc
is
the
clock
period
and
T
and
are
characteristic
of
the
flipflop
The
equation
is
valid
only
for
t
substantially
longer
than
tpcq
Intuitively
T
Tc
describes
the
probability
that
the
input
changes
at
a
bad
time
i
e
during
the
aperture
time
this
probability
decreases
with
the
cycle
time
Tc
is
a
time
constant
indicating
how
fast
the
flip
flop
moves
away
from
the
metastable
state
it
is
related
to
the
delay
through
the
cross
coupled
gates
in
the
flip
flop
In
summary
if
the
input
to
a
bistable
device
such
as
a
flip
flop
changes
during
the
aperture
time
the
output
may
take
on
a
metastable
value
for
some
time
before
resolving
to
a
stable
or
The
amount
of
time
required
to
resolve
is
unbounded
because
for
any
finite
time
t
the
probability
that
the
flip
flop
is
still
metastable
is
nonzero
However
this
probability
drops
off
exponentially
as
t
increases
Therefore
if
we
wait
long
enough
much
longer
than
tpcq
we
can
expect
with
exceedingly
high
probability
that
the
flip
flop
will
reach
a
valid
logic
level
Synchronizers
Asynchronous
inputs
to
digital
systems
from
the
real
world
are
inevitable
Human
input
is
asynchronous
for
example
If
handled
carelessly
these
asynchronous
inputs
can
lead
to
metastable
voltages
within
the
system
causing
erratic
system
failures
that
are
extremely
difficult
to
track
down
and
correct
The
goal
of
a
digital
system
designer
should
be
to
ensure
that
given
asynchronous
inputs
the
probability
of
encountering
a
metastable
voltage
is
sufficiently
small
Sufficiently
depends
on
the
context
For
a
digital
cell
phone
perhaps
one
failure
in
years
is
acceptable
because
the
user
can
always
turn
the
phone
off
and
back
on
if
it
locks
up
For
a
medical
device
one
failure
in
the
expected
life
of
the
universe
years
is
a
better
target
To
guarantee
good
logic
levels
all
asynchronous
inputs
should
be
passed
through
synchronizers
P
tres
t
T
Tc
et
CHAPTER
THREE
Sequential
Logic
Design
metastable
stable
stable
Figure
Stable
and
metastable
states
Cha
A
synchronizer
shown
in
Figure
is
a
device
that
receives
an
asynchronous
input
D
and
a
clock
CLK
It
produces
an
output
Q
within
a
bounded
amount
of
time
the
output
has
a
valid
logic
level
with
extremely
high
probability
If
D
is
stable
during
the
aperture
Q
should
take
on
the
same
value
as
D
If
D
changes
during
the
aperture
Q
may
take
on
either
a
HIGH
or
LOW
value
but
must
not
be
metastable
Figure
shows
a
simple
way
to
build
a
synchronizer
out
of
two
flip
flops
F
samples
D
on
the
rising
edge
of
CLK
If
D
is
changing
at
that
time
the
output
D
may
be
momentarily
metastable
If
the
clock
period
is
long
enough
D
will
with
high
probability
resolve
to
a
valid
logic
level
before
the
end
of
the
period
F
then
samples
D
which
is
now
stable
producing
a
good
output
Q
We
say
that
a
synchronizer
fails
if
Q
the
output
of
the
synchronizer
becomes
metastable
This
may
happen
if
D
has
not
resolved
to
a
valid
level
by
the
time
it
must
setup
at
F
that
is
if
tres
Tctsetup
According
to
Equation
the
probability
of
failure
for
a
single
input
change
at
a
random
time
is
The
probability
of
failure
P
failure
is
the
probability
that
the
output
Q
will
be
metastable
upon
a
single
change
in
D
If
D
changes
once
per
second
the
probability
of
failure
per
second
is
just
P
failure
However
if
D
changes
N
times
per
second
the
probability
of
failure
per
second
is
N
times
as
great
P
failure
sec
N
T
Tc
e
Tctsetup
P
failure
T
Tc
e
Tctsetup
Timing
of
Sequential
Logic
D
Q
CLKSYNC
Figure
Synchronizer
symbol
D
Q
D
Q
D
Tc
tsetup
tpcq
CLK
CLK
CLK
tres
metastable
F
F
Figure
Simple
synchronizer
Chapter
System
reliability
is
usually
measured
in
mean
time
between
failures
MTBF
As
the
name
might
suggest
MTBF
is
the
average
amount
of
time
between
failures
of
the
system
It
is
the
reciprocal
of
the
probability
that
the
system
will
fail
in
any
given
second
Equation
shows
that
the
MTBF
improves
exponentially
as
the
synchronizer
waits
for
a
longer
time
Tc
For
most
systems
a
synchronizer
that
waits
for
one
clock
cycle
provides
a
safe
MTBF
In
exceptionally
high
speed
systems
waiting
for
more
cycles
may
be
necessary
Example
SYNCHRONIZER
FOR
FSM
INPUT
The
traffic
light
controller
FSM
from
Section
receives
asynchronous
inputs
from
the
traffic
sensors
Suppose
that
a
synchronizer
is
used
to
guarantee
stable
inputs
to
the
controller
Traffic
arrives
on
average
times
per
second
The
flipflops
in
the
synchronizer
have
the
following
characteristics
ps
T
ps
and
tsetup
ps
How
long
must
the
synchronizer
clock
period
be
for
the
MTBF
to
exceed
year
Solution
year
seconds
Solve
Equation
This
equation
has
no
closed
form
solution
However
it
is
easy
enough
to
solve
by
guess
and
check
In
a
spreadsheet
try
a
few
values
of
Tc
and
calculate
the
MTBF
until
discovering
the
value
of
Tc
that
gives
an
MTBF
of
year
Tc
ns
Derivation
of
Resolution
Time
Equation
can
be
derived
using
a
basic
knowledge
of
circuit
theory
differential
equations
and
probability
This
section
can
be
skipped
if
you
are
not
interested
in
the
derivation
or
if
you
are
unfamiliar
with
the
mathematics
A
flip
flop
output
will
be
metastable
after
some
time
t
if
the
flipflop
samples
a
changing
input
causing
a
metastable
condition
and
the
output
does
not
resolve
to
a
valid
level
within
that
time
after
the
clock
edge
Symbolically
this
can
be
expressed
as
P
t
res
t
P
samples
changing
input
P
unresolved
Tc
e
Tc
MTBF
P
failure
sec
Tc
e
Tc
tsetup
NT
CHAPTER
THREE
Sequential
Logic
Design
Chapter
qxd
We
consider
each
probability
term
individually
The
asynchronous
input
signal
switches
between
and
in
some
time
tswitch
as
shown
in
Figure
The
probability
that
the
input
changes
during
the
aperture
around
the
clock
edge
is
If
the
flip
flop
does
enter
metastability
that
is
with
probability
P
samples
changing
input
the
time
to
resolve
from
metastability
depends
on
the
inner
workings
of
the
circuit
This
resolution
time
determines
P
unresolved
the
probability
that
the
flip
flop
has
not
yet
resolved
to
a
valid
logic
level
after
a
time
t
The
remainder
of
this
section
analyzes
a
simple
model
of
a
bistable
device
to
estimate
this
probability
A
bistable
device
uses
storage
with
positive
feedback
Figure
a
shows
this
feedback
implemented
with
a
pair
of
inverters
this
circuit
s
behavior
is
representative
of
most
bistable
elements
A
pair
of
inverters
behaves
like
a
buffer
Let
us
model
it
as
having
the
symmetric
DC
transfer
characteristics
shown
in
Figure
b
with
a
slope
of
G
The
buffer
can
deliver
only
a
finite
amount
of
output
current
we
can
model
this
as
an
output
resistance
R
All
real
circuits
also
have
some
capacitance
C
that
must
be
charged
up
Charging
the
capacitor
through
the
resistor
causes
an
RC
delay
preventing
the
buffer
from
switching
instantaneously
Hence
the
complete
circuit
model
is
shown
in
Figure
c
where
vout
t
is
the
voltage
of
interest
conveying
the
state
of
the
bistable
device
The
metastable
point
for
this
circuit
is
vout
t
vin
t
VDD
if
the
circuit
began
at
exactly
that
point
it
would
remain
there
indefinitely
in
the
P
samples
changing
input
tswitch
tsetup
thold
Tc
Timing
of
Sequential
Logic
CLK
D
Tc
tswitch
tsetup
thold
Figure
Input
timing
a
v
t
vout
t
R
C
c
vin
t
i
t
G
b
vin
vout
slope
G
VDD
VDD
VDD
VDD
V
Figure
Circuit
model
of
bistable
device
Cha
absence
of
noise
Because
voltages
are
continuous
variables
the
chance
that
the
circuit
will
begin
at
exactly
the
metastable
point
is
vanishingly
small
However
the
circuit
might
begin
at
time
near
metastability
at
vout
VDD
V
for
some
small
offset
V
In
such
a
case
the
positive
feedback
will
eventually
drive
vout
t
to
VDD
if
V
and
to
if
V
The
time
required
to
reach
VDD
or
is
the
resolution
time
of
the
bistable
device
The
DC
transfer
characteristic
is
nonlinear
but
it
appears
linear
near
the
metastable
point
which
is
the
region
of
interest
to
us
Specifically
if
vin
t
VDD
V
G
then
vout
t
VDD
V
for
small
V
The
current
through
the
resistor
is
i
t
vout
t
vin
t
R
The
capacitor
charges
at
a
rate
dvin
t
dt
i
t
C
Putting
these
facts
together
we
find
the
governing
equation
for
the
output
voltage
This
is
a
linear
first
order
differential
equation
Solving
it
with
the
initial
condition
vout
VDD
V
gives
Figure
plots
trajectories
for
vout
t
given
various
starting
points
vout
t
moves
exponentially
away
from
the
metastable
point
VDD
until
it
saturates
at
VDD
or
The
output
voltage
eventually
resolves
to
or
The
amount
of
time
this
takes
depends
on
the
initial
voltage
offset
V
from
the
metastable
point
VDD
Solving
Equation
for
the
resolution
time
tres
such
that
vout
tres
VDD
or
gives
t
res
RC
G
ln
VDD
V
V
e
G
tres
RC
VDD
vout
t
VDD
Ve
G
t
RC
dvout
t
dt
G
RC
vout
t
VDD
CHAPTER
THREE
Sequential
Logic
Design
VDD
VDD
t
vout
t
Figure
Resolution
trajectories
Chapter
qxd
In
summary
the
resolution
time
increases
if
the
bistable
device
has
high
resistance
or
capacitance
that
causes
the
output
to
change
slowly
It
decreases
if
the
bistable
device
has
high
gain
G
The
resolution
time
also
increases
logarithmically
as
the
circuit
starts
closer
to
the
metastable
point
V
Define
as
Solving
Equation
for
V
finds
the
initial
offset
Vres
that
gives
a
particular
resolution
time
tres
Suppose
that
the
bistable
device
samples
the
input
while
it
is
changing
It
measures
a
voltage
vin
which
we
will
assume
is
uniformly
distributed
between
and
VDD
The
probability
that
the
output
has
not
resolved
to
a
legal
value
after
time
tres
depends
on
the
probability
that
the
initial
offset
is
sufficiently
small
Specifically
the
initial
offset
on
vout
must
be
less
than
Vres
so
the
initial
offset
on
vin
must
be
less
than
Vres
G
Then
the
probability
that
the
bistable
device
samples
the
input
at
a
time
to
obtain
a
sufficiently
small
initial
offset
is
Putting
this
all
together
the
probability
that
the
resolution
time
exceeds
some
time
t
is
given
by
the
following
equation
Observe
that
Equation
is
in
the
form
of
Equation
where
T
tswitch
tsetup
thold
G
and
RC
G
In
summary
we
have
derived
Equation
and
shown
how
T
and
depend
on
physical
properties
of
the
bistable
device
PARALLELISM
The
speed
of
a
system
is
measured
in
latency
and
throughput
of
tokens
moving
through
a
system
We
define
a
token
to
be
a
group
of
inputs
that
are
processed
to
produce
a
group
of
outputs
The
term
conjures
up
the
notion
of
placing
subway
tokens
on
a
circuit
diagram
and
moving
them
around
to
visualize
data
moving
through
the
circuit
The
latency
of
a
system
is
the
time
required
for
one
token
to
pass
through
the
system
from
start
to
end
The
throughput
is
the
number
of
tokens
that
can
be
produced
per
unit
time
P
tres
t
tswitch
tsetup
thold
GTc
e
t
P
unresolved
P
vin
VDD
Vres
G
Vres
GVDD
Vres
VDD
etres
RC
G
Parallelism
Chapter
Example
COOKIE
THROUGHPUT
AND
LATENCY
Ben
Bitdiddle
is
throwing
a
milk
and
cookies
party
to
celebrate
the
installation
of
his
traffic
light
controller
It
takes
him
minutes
to
roll
cookies
and
place
them
on
his
tray
It
then
takes
minutes
for
the
cookies
to
bake
in
the
oven
Once
the
cookies
are
baked
he
starts
another
tray
What
is
Ben
s
throughput
and
latency
for
a
tray
of
cookies
Solution
In
this
example
a
tray
of
cookies
is
a
token
The
latency
is
hour
per
tray
The
throughput
is
trays
hour
As
you
might
imagine
the
throughput
can
be
improved
by
processing
several
tokens
at
the
same
time
This
is
called
parallelism
and
it
comes
in
two
forms
spatial
and
temporal
With
spatial
parallelism
multiple
copies
of
the
hardware
are
provided
so
that
multiple
tasks
can
be
done
at
the
same
time
With
temporal
parallelism
a
task
is
broken
into
stages
like
an
assembly
line
Multiple
tasks
can
be
spread
across
the
stages
Although
each
task
must
pass
through
all
stages
a
different
task
will
be
in
each
stage
at
any
given
time
so
multiple
tasks
can
overlap
Temporal
parallelism
is
commonly
called
pipelining
Spatial
parallelism
is
sometimes
just
called
parallelism
but
we
will
avoid
that
naming
convention
because
it
is
ambiguous
Example
COOKIE
PARALLELISM
Ben
Bitdiddle
has
hundreds
of
friends
coming
to
his
party
and
needs
to
bake
cookies
faster
He
is
considering
using
spatial
and
or
temporal
parallelism
Spatial
Parallelism
Ben
asks
Alyssa
P
Hacker
to
help
out
She
has
her
own
cookie
tray
and
oven
Temporal
Parallelism
Ben
gets
a
second
cookie
tray
Once
he
puts
one
cookie
tray
in
the
oven
he
starts
rolling
cookies
on
the
other
tray
rather
than
waiting
for
the
first
tray
to
bake
What
is
the
throughput
and
latency
using
spatial
parallelism
Using
temporal
parallelism
Using
both
Solution
The
latency
is
the
time
required
to
complete
one
task
from
start
to
finish
In
all
cases
the
latency
is
hour
If
Ben
starts
with
no
cookies
the
latency
is
the
time
needed
for
him
to
produce
the
first
cookie
tray
The
throughput
is
the
number
of
cookie
trays
per
hour
With
spatial
parallelism
Ben
and
Alyssa
each
complete
one
tray
every
minutes
Hence
the
throughput
doubles
to
trays
hour
With
temporal
parallelism
Ben
puts
a
new
tray
in
the
oven
every
minutes
for
a
throughput
of
trays
hour
These
are
illustrated
in
Figure
If
Ben
and
Alyssa
use
both
techniques
they
can
bake
trays
hour
CHAPTER
THREE
Sequential
Logic
Design
Consider
a
task
with
latency
L
In
a
system
with
no
parallelism
the
throughput
is
L
In
a
spatially
parallel
system
with
N
copies
of
the
hardware
the
throughput
is
N
L
In
a
temporally
parallel
system
the
task
is
ideally
broken
into
N
steps
or
stages
of
equal
length
In
such
a
case
the
throughput
is
also
N
L
and
only
one
copy
of
the
hardware
is
required
However
as
the
cookie
example
showed
finding
N
steps
of
equal
length
is
often
impractical
If
the
longest
step
has
a
latency
L
the
pipelined
throughput
is
L
Pipelining
temporal
parallelism
is
particularly
attractive
because
it
speeds
up
a
circuit
without
duplicating
the
hardware
Instead
registers
are
placed
between
blocks
of
combinational
logic
to
divide
the
logic
into
shorter
stages
that
can
run
with
a
faster
clock
The
registers
prevent
a
token
in
one
pipeline
stage
from
catching
up
with
and
corrupting
the
token
in
the
next
stage
Figure
shows
an
example
of
a
circuit
with
no
pipelining
It
contains
four
blocks
of
logic
between
the
registers
The
critical
path
passes
through
blocks
and
Assume
that
the
register
has
a
clock
to
Q
propagation
delay
of
ns
and
a
setup
time
of
ns
Then
the
cycle
time
is
Tc
ns
The
circuit
has
a
latency
of
ns
and
a
throughput
of
ns
MHz
Figure
shows
the
same
circuit
partitioned
into
a
two
stage
pipeline
by
adding
a
register
between
blocks
and
The
first
stage
has
a
minimum
clock
period
of
ns
The
second
Parallelism
Spatial
Parallelism
Temporal
Parallelism
Ben
Ben
Roll
Bake
Ben
Ben
Ben
Ben
Ben
Ben
Alyssa
Alyssa
Ben
Ben
Alyssa
Alyssa
Time
Tray
Tray
Tray
Tray
Latency
time
to
first
tray
Legend
Tray
Tray
Tray
Figure
Spatial
and
temporal
parallelism
in
the
cookie
kitchen
Chap
stage
has
a
minimum
clock
period
of
ns
The
clock
must
be
slow
enough
for
all
stages
to
work
Hence
Tc
ns
The
latency
is
two
clock
cycles
or
ns
The
throughput
is
ns
MHz
This
example
shows
that
in
a
real
circuit
pipelining
with
two
stages
almost
doubles
the
throughput
and
slightly
increases
the
latency
In
comparison
ideal
pipelining
would
exactly
double
the
throughput
at
no
penalty
in
latency
The
discrepancy
comes
about
because
the
circuit
cannot
be
divided
into
two
exactly
equal
halves
and
because
the
registers
introduce
more
sequencing
overhead
Figure
shows
the
same
circuit
partitioned
into
a
three
stage
pipeline
Note
that
two
more
registers
are
needed
to
store
the
results
of
blocks
and
at
the
end
of
the
first
pipeline
stage
The
cycle
time
is
now
limited
by
the
third
stage
to
ns
The
latency
is
three
cycles
or
ns
The
throughput
is
ns
MHz
Again
adding
a
pipeline
stage
improves
throughput
at
the
expense
of
some
latency
Although
these
techniques
are
powerful
they
do
not
apply
to
all
situations
The
bane
of
parallelism
is
dependencies
If
a
current
task
is
CHAPTER
THREE
Sequential
Logic
Design
CL
CL
CL
CL
CLK
CLK
tpd
ns
tpd
ns
tpd
ns
tpd
ns
Stage
ns
Stage
ns
CLK
Figure
Circuit
with
twostage
pipeline
CL
CL
CL
CL
CLK
CLK
tpd
ns
tpd
ns
tpd
ns
tpd
ns
Stage
ns
Stage
ns
CLK
CLK
Stage
ns
Figure
Circuit
with
threestage
pipeline
CL
CL
CL
CL
CLK
CLK
tpd
ns
tpd
ns
tpd
ns
tpd
ns
Tc
ns
Figure
Circuit
with
no
pipelining
Chap
dependent
on
the
result
of
a
prior
task
rather
than
just
prior
steps
in
the
current
task
the
task
cannot
start
until
the
prior
task
has
completed
For
example
if
Ben
wants
to
check
that
the
first
tray
of
cookies
tastes
good
before
he
starts
preparing
the
second
he
has
a
dependency
that
prevents
pipelining
or
parallel
operation
Parallelism
is
one
of
the
most
important
techniques
for
designing
high
performance
microprocessors
Chapter
discusses
pipelining
further
and
shows
examples
of
handling
dependencies
SUMMARY
This
chapter
has
described
the
analysis
and
design
of
sequential
logic
In
contrast
to
combinational
logic
whose
outputs
depend
only
on
the
current
inputs
sequential
logic
outputs
depend
on
both
current
and
prior
inputs
In
other
words
sequential
logic
remembers
information
about
prior
inputs
This
memory
is
called
the
state
of
the
logic
Sequential
circuits
can
be
difficult
to
analyze
and
are
easy
to
design
incorrectly
so
we
limit
ourselves
to
a
small
set
of
carefully
designed
building
blocks
The
most
important
element
for
our
purposes
is
the
flip
flop
which
receives
a
clock
and
an
input
D
and
produces
an
output
Q
The
flip
flop
copies
D
to
Q
on
the
rising
edge
of
the
clock
and
otherwise
remembers
the
old
state
of
Q
A
group
of
flip
flops
sharing
a
common
clock
is
called
a
register
Flip
flops
may
also
receive
reset
or
enable
control
signals
Although
many
forms
of
sequential
logic
exist
we
discipline
ourselves
to
use
synchronous
sequential
circuits
because
they
are
easy
to
design
Synchronous
sequential
circuits
consist
of
blocks
of
combinational
logic
separated
by
clocked
registers
The
state
of
the
circuit
is
stored
in
the
registers
and
updated
only
on
clock
edges
Finite
state
machines
are
a
powerful
technique
for
designing
sequential
circuits
To
design
an
FSM
first
identify
the
inputs
and
outputs
of
the
machine
and
sketch
a
state
transition
diagram
indicating
the
states
and
the
transitions
between
them
Select
an
encoding
for
the
states
and
rewrite
the
diagram
as
a
state
transition
table
and
output
table
indicating
the
next
state
and
output
given
the
current
state
and
input
From
these
tables
design
the
combinational
logic
to
compute
the
next
state
and
output
and
sketch
the
circuit
Synchronous
sequential
circuits
have
a
timing
specification
including
the
clock
to
Q
propagation
and
contamination
delays
tpcq
and
tccq
and
the
setup
and
hold
times
tsetup
and
thold
For
correct
operation
their
inputs
must
be
stable
during
an
aperture
time
that
starts
a
setup
time
before
the
rising
edge
of
the
clock
and
ends
a
hold
time
after
the
rising
edge
of
the
clock
The
minimum
cycle
time
Tc
of
the
system
is
equal
to
the
propagation
delay
tpd
through
the
combinational
logic
plus
Summary
Anyone
who
could
invent
logic
whose
outputs
depend
on
future
inputs
would
be
fabulously
wealthy
CHAPTER
THREE
Sequential
Logic
Design
tpcq
tsetup
of
the
register
For
correct
operation
the
contamination
delay
through
the
register
and
combinational
logic
must
be
greater
than
thold
Despite
the
common
misconception
to
the
contrary
hold
time
does
not
affect
the
cycle
time
Overall
system
performance
is
measured
in
latency
and
throughput
The
latency
is
the
time
required
for
a
token
to
pass
from
start
to
end
The
throughput
is
the
number
of
tokens
that
the
system
can
process
per
unit
time
Parallelism
improves
the
system
throughput
Exercises
Exercises
Exercise
Given
the
input
waveforms
shown
in
Figure
sketch
the
output
Q
of
an
SR
latch
S
R
Figure
Input
waveform
of
SR
latch
Exercise
Given
the
input
waveforms
shown
in
Figure
sketch
the
output
Q
of
a
D
latch
Exercise
Given
the
input
waveforms
shown
in
Figure
sketch
the
output
Q
of
a
D
flip
flop
Exercise
Is
the
circuit
in
Figure
combinational
logic
or
sequential
logic
Explain
in
a
simple
fashion
what
the
relationship
is
between
the
inputs
and
outputs
What
would
you
call
this
circuit
Exercise
Is
the
circuit
in
Figure
combinational
logic
or
sequential
logic
Explain
in
a
simple
fashion
what
the
relationship
is
between
the
inputs
and
outputs
What
would
you
call
this
circuit
CLK
D
Figure
Input
waveform
of
D
latch
or
flip
flop
S
R
Q
Q
Figure
Mystery
circuit
CHAPTER
THREE
Sequential
Logic
Design
Exercise
The
toggle
T
flip
flop
has
one
input
CLK
and
one
output
Q
On
each
rising
edge
of
CLK
Q
toggles
to
the
complement
of
its
previous
value
Draw
a
schematic
for
a
T
flip
flop
using
a
D
flip
flop
and
an
inverter
Exercise
A
JK
flip
flop
receives
a
clock
and
two
inputs
J
and
K
On
the
rising
edge
of
the
clock
it
updates
the
output
Q
If
J
and
K
are
both
Q
retains
its
old
value
If
only
J
is
Q
becomes
If
only
K
is
Q
becomes
If
both
J
and
K
are
Q
becomes
the
opposite
of
its
present
state
a
Construct
a
JK
flip
flop
using
a
D
flip
flop
and
some
combinational
logic
b
Construct
a
D
flip
flop
using
a
JK
flip
flop
and
some
combinational
logic
c
Construct
a
T
flip
flop
see
Exercise
using
a
JK
flip
flop
Exercise
The
circuit
in
Figure
is
called
a
Muller
C
element
Explain
in
a
simple
fashion
what
the
relationship
is
between
the
inputs
and
output
Q
Q
S
R
S
R
D
R
CLK
Figure
Mystery
circuit
A
B
A
B
C
weak
Figure
Muller
C
element
Exercise
Design
an
asynchronously
resettable
D
latch
using
logic
gates
Exercise
Design
an
asynchronously
resettable
D
flip
flop
using
logic
gates
Exercise
Design
a
synchronously
settable
D
flip
flop
using
logic
gates
Exercises
Exercise
Design
an
asynchronously
settable
D
flip
flop
using
logic
gates
Exercise
Suppose
a
ring
oscillator
is
built
from
N
inverters
connected
in
a
loop
Each
inverter
has
a
minimum
delay
of
tcd
and
a
maximum
delay
of
tpd
If
N
is
odd
determine
the
range
of
frequencies
at
which
the
oscillator
might
operate
Exercise
Why
must
N
be
odd
in
Exercise
Exercise
Which
of
the
circuits
in
Figure
are
synchronous
sequential
circuits
Explain
Exercise
You
are
designing
an
elevator
controller
for
a
building
with
floors
The
controller
has
two
inputs
UP
and
DOWN
It
produces
an
output
indicating
the
floor
that
the
elevator
is
on
There
is
no
floor
What
is
the
minimum
number
of
bits
of
state
in
the
controller
Exercise
You
are
designing
an
FSM
to
keep
track
of
the
mood
of
four
students
working
in
the
digital
design
lab
Each
student
s
mood
is
either
HAPPY
the
circuit
works
SAD
the
circuit
blew
up
BUSY
working
on
the
circuit
CLUELESS
confused
about
the
circuit
or
ASLEEP
face
down
on
the
circuit
board
How
many
states
does
the
FSM
have
What
is
the
minimum
number
of
bits
necessary
to
represent
these
states
Exercise
How
would
you
factor
the
FSM
from
Exercise
into
multiple
simpler
machines
How
many
states
does
each
simpler
machine
have
What
is
the
minimum
total
number
of
bits
necessary
in
this
factored
design
a
CL
CL
CLK
CL
CL
CL
CL
CLK
CL
b
c
d
CL
CL
CLK
Figure
Circuits
Exercise
Describe
in
words
what
the
state
machine
in
Figure
does
Using
binary
state
encodings
complete
a
state
transition
table
and
output
table
for
the
FSM
Write
Boolean
equations
for
the
next
state
and
output
and
sketch
a
schematic
of
the
FSM
Exercise
Describe
in
words
what
the
state
machine
in
Figure
does
Using
binary
state
encodings
complete
a
state
transition
table
and
output
table
for
the
FSM
Write
Boolean
equations
for
the
next
state
and
output
and
sketch
a
schematic
of
the
FSM
Exercise
Accidents
are
still
occurring
at
the
intersection
of
Academic
Avenue
and
Bravado
Boulevard
The
football
team
is
rushing
into
the
intersection
the
moment
light
B
turns
green
They
are
colliding
with
sleep
deprived
CS
majors
who
stagger
into
the
intersection
just
before
light
A
turns
red
Extend
the
traffic
light
controller
from
Section
so
that
both
lights
are
red
for
seconds
before
either
light
turns
green
again
Sketch
your
improved
Moore
machine
state
transition
diagram
state
encodings
state
transition
table
output
table
next
state
and
output
equations
and
your
FSM
schematic
Exercise
Alyssa
P
Hacker
s
snail
from
Section
has
a
daughter
with
a
Mealy
machine
FSM
brain
The
daughter
snail
smiles
whenever
she
slides
over
the
pattern
or
the
pattern
Sketch
the
state
transition
diagram
for
this
happy
snail
using
as
few
states
as
possible
Choose
state
encodings
and
write
a
CHAPTER
THREE
Sequential
Logic
Design
S
Q
S
Q
S
Q
Reset
A
B
A
B
Figure
State
transition
diagram
S
S
S
Reset
A
B
A
B
AB
A
B
Figure
State
transition
diagram
Exercises
combined
state
transition
and
output
table
using
your
encodings
Write
the
next
state
and
output
equations
and
sketch
your
FSM
schematic
Exercise
You
have
been
enlisted
to
design
a
soda
machine
dispenser
for
your
department
lounge
Sodas
are
partially
subsidized
by
the
student
chapter
of
the
IEEE
so
they
cost
only
cents
The
machine
accepts
nickels
dimes
and
quarters
When
enough
coins
have
been
inserted
it
dispenses
the
soda
and
returns
any
necessary
change
Design
an
FSM
controller
for
the
soda
machine
The
FSM
inputs
are
Nickel
Dime
and
Quarter
indicating
which
coin
was
inserted
Assume
that
exactly
one
coin
is
inserted
on
each
cycle
The
outputs
are
Dispense
ReturnNickel
ReturnDime
and
ReturnTwoDimes
When
the
FSM
reaches
cents
it
asserts
Dispense
and
the
necessary
Return
outputs
required
to
deliver
the
appropriate
change
Then
it
should
be
ready
to
start
accepting
coins
for
another
soda
Exercise
Gray
codes
have
a
useful
property
in
that
consecutive
numbers
differ
in
only
a
single
bit
position
Table
lists
a
bit
Gray
code
representing
the
numbers
to
Design
a
bit
modulo
Gray
code
counter
FSM
with
no
inputs
and
three
outputs
A
modulo
N
counter
counts
from
to
N
then
repeats
For
example
a
watch
uses
a
modulo
counter
for
the
minutes
and
seconds
that
counts
from
to
When
reset
the
output
should
be
On
each
clock
edge
the
output
should
advance
to
the
next
Gray
code
After
reaching
it
should
repeat
with
Table
bit
Gray
code
Number
Gray
code
Exercise
Extend
your
modulo
Gray
code
counter
from
Exercise
to
be
an
UP
DOWN
counter
by
adding
an
UP
input
If
UP
the
counter
advances
to
the
next
number
If
UP
the
counter
retreats
to
the
previous
number
Cha
Exercise
Your
company
Detect
o
rama
would
like
to
design
an
FSM
that
takes
two
inputs
A
and
B
and
generates
one
output
Z
The
output
in
cycle
n
Zn
is
either
the
Boolean
AND
or
OR
of
the
corresponding
input
An
and
the
previous
input
An
depending
on
the
other
input
Bn
a
Sketch
the
waveform
for
Z
given
the
inputs
shown
in
Figure
b
Is
this
FSM
a
Moore
or
a
Mealy
machine
c
Design
the
FSM
Show
your
state
transition
diagram
encoded
state
transition
table
next
state
and
output
equations
and
schematic
ZnAn
An
if
Bn
Zn
An
An
if
Bn
CHAPTER
THREE
Sequential
Logic
Design
CLK
A
B
Figure
FSM
input
waveforms
Exercise
Design
an
FSM
with
one
input
A
and
two
outputs
X
and
Y
X
should
be
if
A
has
been
for
at
least
three
cycles
altogether
not
necessarily
consecutively
Y
should
be
if
A
has
been
for
at
least
two
consecutive
cycles
Show
your
state
transition
diagram
encoded
state
transition
table
next
state
and
output
equations
and
schematic
Exercise
Analyze
the
FSM
shown
in
Figure
Write
the
state
transition
and
output
tables
and
sketch
the
state
transition
diagram
Describe
in
words
what
the
FSM
does
CLK
CLK
X
Q
Figure
FSM
schematic
Chapter
Exercises
Exercise
Repeat
Exercise
for
the
FSM
shown
in
Figure
Recall
that
the
r
and
s
register
inputs
indicate
set
and
reset
respectively
Exercise
Ben
Bitdiddle
has
designed
the
circuit
in
Figure
to
compute
a
registered
four
input
XOR
function
Each
two
input
XOR
gate
has
a
propagation
delay
of
ps
and
a
contamination
delay
of
ps
Each
flip
flop
has
a
setup
time
of
ps
a
hold
time
of
ps
a
clock
to
Q
maximum
delay
of
ps
and
a
clock
to
Q
minimum
delay
of
ps
a
If
there
is
no
clock
skew
what
is
the
maximum
operating
frequency
of
the
circuit
b
How
much
clock
skew
can
the
circuit
tolerate
if
it
must
operate
at
GHz
c
How
much
clock
skew
can
the
circuit
tolerate
before
it
might
experience
a
hold
time
violation
d
Alyssa
P
Hacker
points
out
that
she
can
redesign
the
combinational
logic
between
the
registers
to
be
faster
and
tolerate
more
clock
skew
Her
improved
circuit
also
uses
three
two
input
XORs
but
they
are
arranged
differently
What
is
her
circuit
What
is
its
maximum
frequency
if
there
is
no
clock
skew
How
much
clock
skew
can
the
circuit
tolerate
before
it
might
experience
a
hold
time
violation
CLK
A
CLK
CLK
Q
reset
r
r
s
Figure
FSM
schematic
CLK
CLK
Figure
Registered
four
input
XOR
circuit
Exercise
You
are
designing
an
adder
for
the
blindingly
fast
bit
RePentium
Processor
The
adder
is
built
from
two
full
adders
such
that
the
carry
out
of
the
first
adder
is
the
carry
in
to
the
second
adder
as
shown
in
Figure
Your
adder
has
input
and
output
registers
and
must
complete
the
addition
in
one
clock
cycle
Each
full
adder
has
the
following
propagation
delays
ps
from
Cin
to
Cout
or
to
Sum
S
ps
from
A
or
B
to
Cout
and
ps
from
A
or
B
to
S
The
adder
has
a
contamination
delay
of
ps
from
Cin
to
either
output
and
ps
from
A
or
B
to
either
output
Each
flip
flop
has
a
setup
time
of
ps
a
hold
time
of
ps
a
clock
to
Q
propagation
delay
of
ps
and
a
clock
to
Q
contamination
delay
of
ps
a
If
there
is
no
clock
skew
what
is
the
maximum
operating
frequency
of
the
circuit
b
How
much
clock
skew
can
the
circuit
tolerate
if
it
must
operate
at
GHz
c
How
much
clock
skew
can
the
circuit
tolerate
before
it
might
experience
a
hold
time
violation
Exercise
A
field
programmable
gate
array
FPGA
uses
configurable
logic
blocks
CLBs
rather
than
logic
gates
to
implement
combinational
logic
The
Xilinx
Spartan
FPGA
has
propagation
and
contamination
delays
of
and
ns
respectively
for
each
CLB
It
also
contains
flip
flops
with
propagation
and
contamination
delays
of
and
ns
and
setup
and
hold
times
of
and
ns
respectively
a
If
you
are
building
a
system
that
needs
to
run
at
MHz
how
many
consecutive
CLBs
can
you
use
between
two
flip
flops
Assume
there
is
no
clock
skew
and
no
delay
through
wires
between
CLBs
b
Suppose
that
all
paths
between
flip
flops
pass
through
at
least
one
CLB
How
much
clock
skew
can
the
FPGA
have
without
violating
the
hold
time
Exercise
A
synchronizer
is
built
from
a
pair
of
flip
flops
with
tsetup
ps
T
ps
and
ps
It
samples
an
asynchronous
input
that
changes
times
per
second
What
is
the
minimum
clock
period
of
the
synchronizer
to
achieve
a
mean
time
between
failures
MTBF
of
years
CHAPTER
THREE
Sequential
Logic
Design
A
B
Cin
Cout
S
A
B
Cin
Cout
S
CLK
C
A
B
A
B
S
S
CLK
Figure
bit
adder
schematic
Cha
Exercises
Exercise
You
would
like
to
build
a
synchronizer
that
can
receive
asynchronous
inputs
with
an
MTBF
of
years
Your
system
is
running
at
GHz
and
you
use
sampling
flip
flops
with
ps
T
ps
and
tsetup
ps
The
synchronizer
receives
a
new
asynchronous
input
on
average
times
per
second
i
e
once
every
seconds
What
is
the
required
probability
of
failure
to
satisfy
this
MTBF
How
many
clock
cycles
would
you
have
to
wait
before
reading
the
sampled
input
signal
to
give
that
probability
of
error
Exercise
You
are
walking
down
the
hallway
when
you
run
into
your
lab
partner
walking
in
the
other
direction
The
two
of
you
first
step
one
way
and
are
still
in
each
other
s
way
Then
you
both
step
the
other
way
and
are
still
in
each
other
s
way
Then
you
both
wait
a
bit
hoping
the
other
person
will
step
aside
You
can
model
this
situation
as
a
metastable
point
and
apply
the
same
theory
that
has
been
applied
to
synchronizers
and
flip
flops
Suppose
you
create
a
mathematical
model
for
yourself
and
your
lab
partner
You
start
the
unfortunate
encounter
in
the
metastable
state
The
probability
that
you
remain
in
this
state
after
t
seconds
is
indicates
your
response
rate
today
your
brain
has
been
blurred
by
lack
of
sleep
and
has
seconds
a
How
long
will
it
be
until
you
have
certainty
that
you
will
have
resolved
from
metastability
i
e
figured
out
how
to
pass
one
another
b
You
are
not
only
sleepy
but
also
ravenously
hungry
In
fact
you
will
starve
to
death
if
you
don
t
get
going
to
the
cafeteria
within
minutes
What
is
the
probability
that
your
lab
partner
will
have
to
drag
you
to
the
morgue
Exercise
You
have
built
a
synchronizer
using
flip
flops
with
T
ps
and
ps
Your
boss
tells
you
that
you
need
to
increase
the
MTBF
by
a
factor
of
By
how
much
do
you
need
to
increase
the
clock
period
Exercise
Ben
Bitdiddle
invents
a
new
and
improved
synchronizer
in
Figure
that
he
claims
eliminates
metastability
in
a
single
cycle
He
explains
that
the
circuit
in
box
M
is
an
analog
metastability
detector
that
produces
a
HIGH
output
if
the
input
voltage
is
in
the
forbidden
zone
between
VIL
and
VIH
The
metastability
detector
checks
to
determine
e
t
CLK
D
r
CLK
Q
M
D
Figure
New
and
improved
synchronizer
Chapter
CHAPTER
THREE
Sequential
Logic
Design
whether
the
first
flip
flop
has
produced
a
metastable
output
on
D
If
so
it
asynchronously
resets
the
flip
flop
to
produce
a
good
at
D
The
second
flip
flop
then
samples
D
always
producing
a
valid
logic
level
on
Q
Alyssa
P
Hacker
tells
Ben
that
there
must
be
a
bug
in
the
circuit
because
eliminating
metastability
is
just
as
impossible
as
building
a
perpetual
motion
machine
Who
is
right
Explain
showing
Ben
s
error
or
showing
why
Alyssa
is
wrong
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Draw
a
state
machine
that
can
detect
when
it
has
received
the
serial
input
sequence
Question
Design
a
serial
one
bit
at
a
time
two
s
complementer
FSM
with
two
inputs
Start
and
A
and
one
output
Q
A
binary
number
of
arbitrary
length
is
provided
to
input
A
starting
with
the
least
significant
bit
The
corresponding
bit
of
the
output
appears
at
Q
on
the
same
cycle
Start
is
asserted
for
one
cycle
to
initialize
the
FSM
before
the
least
significant
bit
is
provided
Question
What
is
the
difference
between
a
latch
and
a
flip
flop
Under
what
circumstances
is
each
one
preferable
Question
Design
a
bit
counter
finite
state
machine
Question
Design
an
edge
detector
circuit
The
output
should
go
HIGH
for
one
cycle
after
the
input
makes
a
transition
Question
Describe
the
concept
of
pipelining
and
why
it
is
used
Question
Describe
what
it
means
for
a
flip
flop
to
have
a
negative
hold
time
Question
Given
signal
A
shown
in
Figure
design
a
circuit
that
produces
signal
B
Interview
Questions
A
B
Figure
Signal
waveforms
Question
Consider
a
block
of
logic
between
two
registers
Explain
the
timing
constraints
If
you
add
a
buffer
on
the
clock
input
of
the
receiver
the
second
flip
flop
does
the
setup
time
constraint
get
better
or
worse
Introduction
Combinational
Logic
Structural
Modeling
Sequential
Logic
More
Combinational
Logic
Finite
State
Machines
Parameterized
Modules
Testbenches
Summary
Exercises
Interview
Questions
Hardware
Description
Languages
INTRODUCTION
Thus
far
we
have
focused
on
designing
combinational
and
sequential
digital
circuits
at
the
schematic
level
The
process
of
finding
an
efficient
set
of
logic
gates
to
perform
a
given
function
is
labor
intensive
and
error
prone
requiring
manual
simplification
of
truth
tables
or
Boolean
equations
and
manual
translation
of
finite
state
machines
FSMs
into
gates
In
the
s
designers
discovered
that
they
were
far
more
productive
if
they
worked
at
a
higher
level
of
abstraction
specifying
just
the
logical
function
and
allowing
a
computer
aided
design
CAD
tool
to
produce
the
optimized
gates
The
specifications
are
generally
given
in
a
hardware
description
language
HDL
The
two
leading
hardware
description
languages
are
Verilog
and
VHDL
Verilog
and
VHDL
are
built
on
similar
principles
but
have
different
syntax
Discussion
of
these
languages
in
this
chapter
is
divided
into
two
columns
for
literal
side
by
side
comparison
with
Verilog
on
the
left
and
VHDL
on
the
right
When
you
read
the
chapter
for
the
first
time
focus
on
one
language
or
the
other
Once
you
know
one
you
ll
quickly
master
the
other
if
you
need
it
Subsequent
chapters
show
hardware
in
both
schematic
and
HDL
form
If
you
choose
to
skip
this
chapter
and
not
learn
one
of
the
HDLs
you
will
still
be
able
to
master
the
principles
of
computer
organization
from
the
schematics
However
the
vast
majority
of
commercial
systems
are
now
built
using
HDLs
rather
than
schematics
If
you
expect
to
do
digital
design
at
any
point
in
your
professional
life
we
urge
you
to
learn
one
of
the
HDLs
Modules
A
block
of
hardware
with
inputs
and
outputs
is
called
a
module
An
AND
gate
a
multiplexer
and
a
priority
circuit
are
all
examples
of
hardware
modules
The
two
general
styles
for
describing
module
functionality
are
behavioral
and
structural
Behavioral
models
describe
what
a
module
does
Structural
models
describe
how
a
module
is
built
from
simpler
pieces
it
is
an
application
of
hierarchy
The
Verilog
and
VHDL
code
in
HDL
Example
illustrate
behavioral
descriptions
of
a
module
that
computes
the
Boolean
function
from
Example
In
both
languages
the
module
is
named
sillyfunction
and
has
three
inputs
a
b
and
c
and
one
output
y
y
abcabc
abc
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
sillyfunction
input
a
b
c
output
y
assign
y
a
b
c
a
b
c
a
b
c
endmodule
A
Verilog
module
begins
with
the
module
name
and
a
listing
of
the
inputs
and
outputs
The
assign
statement
describes
combinational
logic
indicates
NOT
indicates
AND
and
indicates
OR
Verilog
signals
such
as
the
inputs
and
outputs
are
Boolean
variables
or
They
may
also
have
floating
and
undefined
values
as
discussed
in
Section
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
sillyfunction
is
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
sillyfunction
is
begin
y
not
a
and
not
b
and
not
c
or
a
and
not
b
and
not
c
or
a
and
not
b
and
c
end
VHDL
code
has
three
parts
the
library
use
clause
the
entity
declaration
and
the
architecture
body
The
library
use
clause
is
required
and
will
be
discussed
in
Section
The
entity
declaration
lists
the
module
name
and
its
inputs
and
outputs
The
architecture
body
defines
what
the
module
does
VHDL
signals
such
as
inputs
and
outputs
must
have
a
type
declaration
Digital
signals
should
be
declared
to
be
STD
LOGIC
type
STD
LOGIC
signals
can
have
a
value
of
or
as
well
as
floating
and
undefined
values
that
will
be
described
in
Section
The
STD
LOGIC
type
is
defined
in
the
IEEE
STD
LOGIC
library
which
is
why
the
library
must
be
used
VHDL
lacks
a
good
default
order
of
operations
so
Boolean
equations
should
be
parenthesized
HDL
Example
COMBINATIONAL
LOGIC
A
module
as
you
might
expect
is
a
good
application
of
modularity
It
has
a
well
defined
interface
consisting
of
its
inputs
and
outputs
and
it
performs
a
specific
function
The
particular
way
in
which
it
is
coded
is
unimportant
to
others
that
might
use
the
module
as
long
as
it
performs
its
function
Language
Origins
Universities
are
almost
evenly
split
on
which
of
these
languages
is
taught
in
a
first
course
and
industry
is
similarly
split
on
which
language
is
preferred
Compared
to
Verilog
VHDL
is
more
verbose
and
cumbersome
Chapt
as
you
might
expect
of
a
language
developed
by
committee
U
S
military
contractors
the
European
Space
Agency
and
telecommunications
companies
use
VHDL
extensively
Both
languages
are
fully
capable
of
describing
any
hardware
system
and
both
have
their
quirks
The
best
language
to
use
is
the
one
that
is
already
being
used
at
your
site
or
the
one
that
your
customers
demand
Most
CAD
tools
today
allow
the
two
languages
to
be
mixed
so
that
different
modules
can
be
described
in
different
languages
Simulation
and
Synthesis
The
two
major
purposes
of
HDLs
are
logic
simulation
and
synthesis
During
simulation
inputs
are
applied
to
a
module
and
the
outputs
are
checked
to
verify
that
the
module
operates
correctly
During
synthesis
the
textual
description
of
a
module
is
transformed
into
logic
gates
Simulation
Humans
routinely
make
mistakes
Such
errors
in
hardware
designs
are
called
bugs
Eliminating
the
bugs
from
a
digital
system
is
obviously
important
especially
when
customers
are
paying
money
and
lives
depend
on
the
correct
operation
Testing
a
system
in
the
laboratory
is
time
consuming
Discovering
the
cause
of
errors
in
the
lab
can
be
extremely
difficult
because
only
signals
routed
to
the
chip
pins
can
be
observed
There
is
no
way
to
directly
observe
what
is
happening
inside
a
chip
Correcting
errors
after
the
system
is
built
can
be
devastatingly
expensive
For
example
correcting
a
mistake
in
a
cutting
edge
integrated
circuit
costs
more
than
a
million
dollars
and
takes
several
months
Intel
s
infamous
FDIV
floating
point
division
bug
in
the
Pentium
processor
forced
the
company
to
recall
chips
after
they
had
shipped
at
a
total
cost
of
million
Logic
simulation
is
essential
to
test
a
system
before
it
is
built
Introduction
The
term
bug
predates
the
invention
of
the
computer
Thomas
Edison
called
the
little
faults
and
difficulties
with
his
inventions
bugs
in
The
first
real
computer
bug
was
a
moth
which
got
caught
between
the
relays
of
the
Harvard
Mark
II
electromechanical
computer
in
It
was
found
by
Grace
Hopper
who
logged
the
incident
along
with
the
moth
itself
and
the
comment
first
actual
case
of
bug
being
found
Source
Notebook
entry
courtesy
Naval
Historical
Center
US
Navy
photo
No
NII
KN
The
Institute
of
Electrical
and
Electronics
Engineers
IEEE
is
a
professional
society
responsible
for
many
computing
standards
including
WiFi
Ethernet
and
floating
point
numbers
see
Chapter
VHDL
VHDL
is
an
acronym
for
the
VHSIC
Hardware
Description
Language
VHSIC
is
in
turn
an
acronym
for
the
Very
High
Speed
Integrated
Circuits
program
of
the
US
Department
of
Defense
VHDL
was
originally
developed
in
by
the
Department
of
Defense
to
describe
the
structure
and
function
of
hardware
Its
roots
draw
from
the
Ada
programming
language
The
IEEE
standardized
it
in
IEEE
STD
and
has
updated
the
standard
several
times
since
The
language
was
first
envisioned
for
documentation
but
was
quickly
adopted
for
simulation
and
synthesis
Verilog
Verilog
was
developed
by
Gateway
Design
Automation
as
a
proprietary
language
for
logic
simulation
in
Gateway
was
acquired
by
Cadence
in
and
Verilog
was
made
an
open
standard
in
under
the
control
of
Open
Verilog
International
The
language
became
an
IEEE
standard
in
IEEE
STD
and
was
updated
in
Figure
shows
waveforms
from
a
simulation
of
the
previous
sillyfunction
module
demonstrating
that
the
module
works
correctly
y
is
TRUE
when
a
b
and
c
are
or
as
specified
by
the
Boolean
equation
Synthesis
Logic
synthesis
transforms
HDL
code
into
a
netlist
describing
the
hardware
e
g
the
logic
gates
and
the
wires
connecting
them
The
logic
synthesizer
might
perform
optimizations
to
reduce
the
amount
of
hardware
required
The
netlist
may
be
a
text
file
or
it
may
be
drawn
as
a
schematic
to
help
visualize
the
circuit
Figure
shows
the
results
of
synthesizing
the
sillyfunction
module
Notice
how
the
three
threeinput
AND
gates
are
simplified
into
two
two
input
AND
gates
as
we
discovered
in
Example
using
Boolean
algebra
Circuit
descriptions
in
HDL
resemble
code
in
a
programming
language
However
you
must
remember
that
the
code
is
intended
to
represent
hardware
Verilog
and
VHDL
are
rich
languages
with
many
commands
Not
all
of
these
commands
can
be
synthesized
into
hardware
For
example
a
command
to
print
results
on
the
screen
during
simulation
does
not
translate
into
hardware
Because
our
primary
interest
is
CHAPTER
FOUR
Hardware
Description
Languages
ns
a
Now
ns
b
c
y
ns
ns
Figure
Simulation
waveforms
un
y
un
y
y
c
y
b
a
Figure
Synthesized
circuit
The
simulation
was
performed
with
the
Xilinx
ISE
Simulator
which
is
part
of
the
Xilinx
ISE
software
The
simulator
was
selected
because
it
is
used
commercially
yet
is
freely
available
to
universities
Synthesis
was
performed
with
Synplify
Pro
from
Synplicity
The
tool
was
selected
because
it
is
the
leading
commercial
tool
for
synthesizing
HDL
to
field
programmable
gate
arrays
see
Section
and
because
it
is
available
inexpensively
for
universities
to
build
hardware
we
will
emphasize
a
synthesizable
subset
of
the
languages
Specifically
we
will
divide
HDL
code
into
synthesizable
modules
and
a
testbench
The
synthesizable
modules
describe
the
hardware
The
testbench
contains
code
to
apply
inputs
to
a
module
check
whether
the
output
results
are
correct
and
print
discrepancies
between
expected
and
actual
outputs
Testbench
code
is
intended
only
for
simulation
and
cannot
be
synthesized
One
of
the
most
common
mistakes
for
beginners
is
to
think
of
HDL
as
a
computer
program
rather
than
as
a
shorthand
for
describing
digital
hardware
If
you
don
t
know
approximately
what
hardware
your
HDL
should
synthesize
into
you
probably
won
t
like
what
you
get
You
might
create
far
more
hardware
than
is
necessary
or
you
might
write
code
that
simulates
correctly
but
cannot
be
implemented
in
hardware
Instead
think
of
your
system
in
terms
of
blocks
of
combinational
logic
registers
and
finite
state
machines
Sketch
these
blocks
on
paper
and
show
how
they
are
connected
before
you
start
writing
code
In
our
experience
the
best
way
to
learn
an
HDL
is
by
example
HDLs
have
specific
ways
of
describing
various
classes
of
logic
these
ways
are
called
idioms
This
chapter
will
teach
you
how
to
write
the
proper
HDL
idioms
for
each
type
of
block
and
then
how
to
put
the
blocks
together
to
produce
a
working
system
When
you
need
to
describe
a
particular
kind
of
hardware
look
for
a
similar
example
and
adapt
it
to
your
purpose
We
do
not
attempt
to
rigorously
define
all
the
syntax
of
the
HDLs
because
that
is
deathly
boring
and
because
it
tends
to
encourage
thinking
of
HDLs
as
programming
languages
not
shorthand
for
hardware
The
IEEE
Verilog
and
VHDL
specifications
and
numerous
dry
but
exhaustive
textbooks
contain
all
of
the
details
should
you
find
yourself
needing
more
information
on
a
particular
topic
See
Further
Readings
section
at
back
of
the
book
COMBINATIONAL
LOGIC
Recall
that
we
are
disciplining
ourselves
to
design
synchronous
sequential
circuits
which
consist
of
combinational
logic
and
registers
The
outputs
of
combinational
logic
depend
only
on
the
current
inputs
This
section
describes
how
to
write
behavioral
models
of
combinational
logic
with
HDLs
Bitwise
Operators
Bitwise
operators
act
on
single
bit
signals
or
on
multi
bit
busses
For
example
the
inv
module
in
HDL
Example
describes
four
inverters
connected
to
bit
busses
Combinational
Logic
The
endianness
of
a
bus
is
purely
arbitrary
See
the
sidebar
in
Section
for
the
origin
of
the
term
Indeed
endianness
is
also
irrelevant
to
this
example
because
a
bank
of
inverters
doesn
t
care
what
the
order
of
the
bits
are
Endianness
matters
only
for
operators
such
as
addition
where
the
sum
of
one
column
carries
over
into
the
next
Either
ordering
is
acceptable
as
long
as
it
is
used
consistently
We
will
consistently
use
the
little
endian
order
N
in
Verilog
and
N
downto
in
VHDL
for
an
N
bit
bus
After
each
code
example
in
this
chapter
is
a
schematic
produced
from
the
Verilog
code
by
the
Synplify
Pro
synthesis
tool
Figure
shows
that
the
inv
module
synthesizes
to
a
bank
of
four
inverters
indicated
by
the
inverter
symbol
labeled
y
The
bank
of
inverters
connects
to
bit
input
and
output
busses
Similar
hardware
is
produced
from
the
synthesized
VHDL
code
The
gates
module
in
HDL
Example
demonstrates
bitwise
operations
acting
on
bit
busses
for
other
basic
logic
functions
CHAPTER
FOUR
Hardware
Description
Languages
y
a
y
Figure
inv
synthesized
circuit
Verilog
module
inv
input
a
output
y
assign
y
a
endmodule
a
represents
a
bit
bus
The
bits
from
most
significant
to
least
significant
are
a
a
a
and
a
This
is
called
little
endian
order
because
the
least
significant
bit
has
the
smallest
bit
number
We
could
have
named
the
bus
a
in
which
case
a
would
have
been
the
most
significant
Or
we
could
have
used
a
in
which
case
the
bits
from
most
significant
to
least
significant
would
be
a
a
a
and
a
This
is
called
big
endian
order
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
inv
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
inv
is
begin
y
not
a
end
VHDL
uses
STD
LOGIC
VECTOR
to
indicate
busses
of
STD
LOGIC
STD
LOGIC
VECTOR
downto
represents
a
bit
bus
The
bits
from
most
significant
to
least
significant
are
and
This
is
called
little
endian
order
because
the
least
significant
bit
has
the
smallest
bit
number
We
could
have
declared
the
bus
to
be
STD
LOGIC
VECTOR
downto
in
which
case
bit
would
have
been
the
most
significant
Or
we
could
have
written
STD
LOGIC
VECTOR
to
in
which
case
the
bits
from
most
significant
to
least
significant
would
be
and
This
is
called
big
endian
order
HDL
Example
INVERTERS
Ch
Combinational
Logic
y
y
y
y
y
y
y
y
y
y
b
a
Figure
gates
synthesized
circuit
Verilog
module
gates
input
a
b
output
y
y
y
y
y
Five
different
two
input
logic
gates
acting
on
bit
busses
assign
y
a
b
AND
assign
y
a
b
OR
assign
y
a
b
XOR
assign
y
a
b
NAND
assign
y
a
b
NOR
endmodule
and
are
examples
of
Verilog
operators
whereas
a
b
and
y
are
operands
A
combination
of
operators
and
operands
such
as
a
b
or
a
b
is
called
an
expression
A
complete
command
such
as
assign
y
a
b
is
called
a
statement
assign
out
in
op
in
is
called
a
continuous
assignment
statement
Continuous
assignment
statements
end
with
a
semicolon
Anytime
the
inputs
on
the
right
side
of
the
in
a
continuous
assignment
statement
change
the
output
on
the
left
side
is
recomputed
Thus
continuous
assignment
statements
describe
combinational
logic
HDL
Example
LOGIC
GATES
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
gates
is
port
a
b
in
STD
LOGIC
VECTOR
downto
y
y
y
y
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
gates
is
begin
Five
different
two
input
logic
gates
acting
on
bit
busses
y
a
and
b
y
a
or
b
y
a
xor
b
y
a
nand
b
y
a
nor
b
end
not
xor
and
or
are
examples
of
VHDL
operators
whereas
a
b
and
y
are
operands
A
combination
of
operators
and
operands
such
as
a
and
b
or
a
nor
b
is
called
an
expression
A
complete
command
such
as
y
a
nand
b
is
called
a
statement
out
in
op
in
is
called
a
concurrent
signal
assignment
statement
VHDL
assignment
statements
end
with
a
semicolon
Anytime
the
inputs
on
the
right
side
of
the
in
a
concurrent
signal
assignment
statement
change
the
output
on
the
left
side
is
recomputed
Thus
concurrent
signal
assignment
statements
describe
combinational
logic
Chapter
qxd
Reduction
Operators
Reduction
operators
imply
a
multiple
input
gate
acting
on
a
single
bus
HDL
Example
describes
an
eight
input
AND
gate
with
inputs
a
a
a
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
Verilog
comments
are
just
like
those
in
C
or
Java
Comments
beginning
with
continue
possibly
across
multiple
lines
to
the
next
Comments
beginning
with
continue
to
the
end
of
the
line
Verilog
is
case
sensitive
y
and
Y
are
different
signals
in
Verilog
VHDL
VHDL
comments
begin
with
and
continue
to
the
end
of
the
line
Comments
spanning
multiple
lines
must
use
at
the
beginning
of
each
line
VHDL
is
not
case
sensitive
y
and
Y
are
the
same
signal
in
VHDL
However
other
tools
that
may
read
your
file
might
be
case
sensitive
leading
to
nasty
bugs
if
you
blithely
mix
upper
and
lower
case
Comments
and
White
Space
The
gates
example
showed
how
to
format
comments
Verilog
and
VHDL
are
not
picky
about
the
use
of
white
space
i
e
spaces
tabs
and
line
breaks
Nevertheless
proper
indenting
and
use
of
blank
lines
is
helpful
to
make
nontrivial
designs
readable
Be
consistent
in
your
use
of
capitalization
and
underscores
in
signal
and
module
names
Module
and
signal
names
must
not
begin
with
a
digit
Verilog
module
and
input
a
output
y
assign
y
a
a
is
much
easier
to
write
than
assign
y
a
a
a
a
a
a
a
a
endmodule
As
one
would
expect
and
reduction
operators
are
available
for
OR
XOR
NAND
and
NOR
as
well
Recall
that
a
multi
input
XOR
performs
parity
returning
TRUE
if
an
odd
number
of
inputs
are
TRUE
VHDL
VHDL
does
not
have
reduction
operators
Instead
it
provides
the
generate
command
see
Section
Alternatively
the
operation
can
be
written
explicitly
as
shown
below
library
IEEE
use
IEEE
STD
LOGIC
all
entity
and
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
end
architecture
synth
of
and
is
begin
y
a
and
a
and
a
and
a
and
a
and
a
and
a
and
a
end
HDL
Example
EIGHT
INPUT
AND
Chap
Conditional
Assignment
Conditional
assignments
select
the
output
from
among
alternatives
based
on
an
input
called
the
condition
HDL
Example
illustrates
a
multiplexer
using
conditional
assignment
Combinational
Logic
y
y
a
Figure
and
synthesized
circuit
Verilog
The
conditional
operator
chooses
based
on
a
first
expression
between
a
second
and
third
expression
The
first
expression
is
called
the
condition
If
the
condition
is
the
operator
chooses
the
second
expression
If
the
condition
is
the
operator
chooses
the
third
expression
is
especially
useful
for
describing
a
multiplexer
because
based
on
the
first
input
it
selects
between
two
others
The
following
code
demonstrates
the
idiom
for
a
multiplexer
with
bit
inputs
and
outputs
using
the
conditional
operator
module
mux
input
d
d
input
s
output
y
assign
y
s
d
d
endmodule
If
s
is
then
y
d
If
s
is
then
y
d
is
also
called
a
ternary
operator
because
it
takes
three
inputs
It
is
used
for
the
same
purpose
in
the
C
and
Java
programming
languages
VHDL
Conditional
signal
assignments
perform
different
operations
depending
on
some
condition
They
are
especially
useful
for
describing
a
multiplexer
For
example
a
multiplexer
can
use
conditional
signal
assignment
to
select
one
of
two
bit
inputs
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
mux
is
begin
y
d
when
s
else
d
end
The
conditional
signal
assignment
sets
y
to
d
if
s
is
Otherwise
it
sets
y
to
d
HDL
Example
MULTIPLEXER
y
y
s
d
d
Figure
mux
synthesized
circuit
Chapt
HDL
Example
shows
a
multiplexer
based
on
the
same
principle
as
the
multiplexer
in
HDL
Example
Figure
shows
the
schematic
for
the
multiplexer
produced
by
Synplify
Pro
The
software
uses
a
different
multiplexer
symbol
than
this
text
has
shown
so
far
The
multiplexer
has
multiple
data
d
and
one
hot
enable
e
inputs
When
one
of
the
enables
is
asserted
the
associated
data
is
passed
to
the
output
For
example
when
s
s
the
bottom
AND
gate
un
s
produces
a
enabling
the
bottom
input
of
the
multiplexer
and
causing
it
to
select
d
Internal
Variables
Often
it
is
convenient
to
break
a
complex
function
into
intermediate
steps
For
example
a
full
adder
which
will
be
described
in
Section
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
A
multiplexer
can
select
one
of
four
inputs
using
nested
conditional
operators
module
mux
input
d
d
d
d
input
s
output
y
assign
y
s
s
d
d
s
d
d
endmodule
If
s
is
then
the
multiplexer
chooses
the
first
expression
s
d
d
This
expression
in
turn
chooses
either
d
or
d
based
on
s
y
d
if
s
is
and
d
if
s
is
If
s
is
then
the
multiplexer
similarly
chooses
the
second
expression
which
gives
either
d
or
d
based
on
s
VHDL
A
multiplexer
can
select
one
of
four
inputs
using
multiple
else
clauses
in
the
conditional
signal
assignment
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synthl
of
mux
is
begin
y
d
when
s
else
d
when
s
else
d
when
s
else
d
end
VHDL
also
supports
selected
signal
assignment
statements
to
provide
a
shorthand
when
selecting
from
one
of
several
possibilities
This
is
analogous
to
using
a
case
statement
in
place
of
multiple
if
else
statements
in
some
programming
languages
The
multiplexer
can
be
rewritten
with
selected
signal
assignment
as
follows
architecture
synth
of
mux
is
begin
with
a
select
y
d
when
d
when
d
when
d
when
others
end
HDL
Example
MULTIPLEXER
Chapter
is
a
circuit
with
three
inputs
and
two
outputs
defined
by
the
following
equations
If
we
define
intermediate
signals
P
and
G
we
can
rewrite
the
full
adder
as
follows
P
and
G
are
called
internal
variables
because
they
are
neither
inputs
nor
outputs
but
are
used
only
internal
to
the
module
They
are
similar
to
local
variables
in
programming
languages
HDL
Example
shows
how
they
are
used
in
HDLs
HDL
assignment
statements
assign
in
Verilog
and
in
VHDL
take
place
concurrently
This
is
different
from
conventional
programming
languages
such
as
C
or
Java
in
which
statements
are
evaluated
in
the
order
in
which
they
are
written
In
a
conventional
language
it
is
CoutG
PCin
S
P
Cin
GAB
PA
B
Cout
AB
ACin
BCin
S
A
B
Cin
Combinational
Logic
un
s
un
s
un
s
un
s
y
e
d
e
d
e
d
e
d
y
s
d
d
d
d
Figure
mux
synthesized
circuit
Check
this
by
filling
out
the
truth
table
to
convince
yourself
it
is
correct
Chapter
important
that
S
P
Cin
comes
after
P
A
B
because
statements
are
executed
sequentially
In
an
HDL
the
order
does
not
matter
Like
hardware
HDL
assignment
statements
are
evaluated
any
time
the
inputs
signals
on
the
right
hand
side
change
their
value
regardless
of
the
order
in
which
the
assignment
statements
appear
in
a
module
Precedence
Notice
that
we
parenthesized
the
cout
computation
in
HDL
Example
to
define
the
order
of
operations
as
Cout
G
P
Cin
rather
than
Cout
G
P
Cin
If
we
had
not
used
parentheses
the
default
operation
order
is
defined
by
the
language
HDL
Example
specifies
operator
precedence
from
highest
to
lowest
for
each
language
The
tables
include
arithmetic
shift
and
comparison
operators
that
will
be
defined
in
Chapter
CHAPTER
FOUR
Hardware
Description
Languages
p
g
s
un
cout
cout
cout
s
cin
b
a
Figure
fulladder
synthesized
circuit
Verilog
In
Verilog
wires
are
used
to
represent
internal
variables
whose
values
are
defined
by
assign
statements
such
as
assign
p
a
b
Wires
technically
have
to
be
declared
only
for
multibit
busses
but
it
is
good
practice
to
include
them
for
all
internal
variables
their
declaration
could
have
been
omitted
in
this
example
module
fulladder
input
a
b
cin
output
s
cout
wire
p
g
assign
p
a
b
assign
g
a
b
assign
s
p
cin
assign
cout
g
p
cin
endmodule
VHDL
In
VHDL
signals
are
used
to
represent
internal
variables
whose
values
are
defined
by
concurrent
signal
assignment
statements
such
as
p
a
xor
b
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fulladder
is
port
a
b
cin
in
STD
LOGIC
s
cout
out
STD
LOGIC
end
architecture
synth
of
fulladder
is
signal
p
g
STD
LOGIC
begin
p
a
xor
b
g
a
and
b
s
p
xor
cin
cout
g
or
p
and
cin
end
HDL
Example
FULL
ADDER
Chapter
qxd
Combinational
Logic
Verilog
VHDL
HDL
Example
OPERATOR
PRECEDENCE
Table
VHDL
operator
precedence
Op
Meaning
not
NOT
mod
rem
MUL
DIV
MOD
REM
PLUS
MINUS
CONCATENATE
rol
ror
Rotate
srl
sll
Shift
logical
sra
sla
Shift
arithmetic
Comparison
and
or
nand
Logical
Operations
nor
xor
H
i
g
h
e
s
t
L
o
w
e
s
t
The
operator
precedence
for
Verilog
is
much
like
you
would
expect
in
other
programming
languages
In
particular
AND
has
precedence
over
OR
We
could
take
advantage
of
this
precedence
to
eliminate
the
parentheses
assign
cout
g
p
cin
Multiplication
has
precedence
over
addition
in
VHDL
as
you
would
expect
However
unlike
Verilog
all
of
the
logical
operations
and
or
etc
have
equal
precedence
unlike
what
one
might
expect
in
Boolean
algebra
Thus
parentheses
are
necessary
otherwise
cout
g
or
p
and
cin
would
be
interpreted
from
left
to
right
as
cout
g
or
p
and
cin
Numbers
Numbers
can
be
specified
in
a
variety
of
bases
Underscores
in
numbers
are
ignored
and
can
be
helpful
in
breaking
long
numbers
into
more
readable
chunks
HDL
Example
explains
how
numbers
are
written
in
each
language
Z
s
and
X
s
HDLs
use
z
to
indicate
a
floating
value
z
is
particularly
useful
for
describing
a
tristate
buffer
whose
output
floats
when
the
enable
is
Recall
from
Section
that
a
bus
can
be
driven
by
several
tristate
buffers
exactly
one
of
which
should
be
enabled
HDL
Example
shows
the
idiom
for
a
tristate
buffer
If
the
buffer
is
enabled
the
output
is
the
same
as
the
input
If
the
buffer
is
disabled
the
output
is
assigned
a
floating
value
z
Table
Verilog
operator
precedence
Op
Meaning
NOT
MUL
DIV
MOD
PLUS
MINUS
Logical
Left
Right
Shift
Arithmetic
Left
Right
Shift
Relative
Comparison
Equality
Comparison
AND
NAND
XOR
XNOR
OR
NOR
Conditional
H
i
g
h
e
s
t
L
o
w
e
s
t
Chapter
qxd
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
Verilog
numbers
can
specify
their
base
and
size
the
number
of
bits
used
to
represent
them
The
format
for
declaring
constants
is
N
Bvalue
where
N
is
the
size
in
bits
B
is
the
base
and
value
gives
the
value
For
example
h
indicates
a
bit
number
with
a
value
of
Verilog
supports
b
for
binary
base
o
for
octal
base
d
for
decimal
base
and
h
for
hexadecimal
base
If
the
base
is
omitted
the
base
defaults
to
decimal
If
the
size
is
not
given
the
number
is
assumed
to
have
as
many
bits
as
the
expression
in
which
it
is
being
used
Zeros
are
automatically
padded
on
the
front
of
the
number
to
bring
it
up
to
full
size
For
example
if
w
is
a
bit
bus
assign
w
b
gives
w
the
value
It
is
better
practice
to
explicitly
give
the
size
VHDL
In
VHDL
STD
LOGIC
numbers
are
written
in
binary
and
enclosed
in
single
quotes
and
indicate
logic
and
STD
LOGIC
VECTOR
numbers
are
written
in
binary
or
hexadecimal
and
enclosed
in
double
quotation
marks
The
base
is
binary
by
default
and
can
be
explicitly
defined
with
the
prefix
X
for
hexadecimal
or
B
for
binary
HDL
Example
NUMBERS
Table
Verilog
numbers
Numbers
Bits
Base
Val
Stored
b
b
b
b
d
o
hAB
Table
VHDL
numbers
Numbers
Bits
Base
Val
Stored
B
X
AB
Verilog
module
tristate
input
a
input
en
output
y
assign
y
en
a
bz
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
tristate
is
port
a
in
STD
LOGIC
VECTOR
downto
en
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
tristate
is
begin
y
ZZZZ
when
en
else
a
end
HDL
Example
TRISTATE
BUFFER
Chapte
Combinational
Logic
y
y
en
a
Figure
tristate
synthesized
circuit
Similarly
HDLs
use
x
to
indicate
an
invalid
logic
level
If
a
bus
is
simultaneously
driven
to
and
by
two
enabled
tristate
buffers
or
other
gates
the
result
is
x
indicating
contention
If
all
the
tristate
buffers
driving
a
bus
are
simultaneously
OFF
the
bus
will
float
indicated
by
z
At
the
start
of
simulation
state
nodes
such
as
flip
flop
outputs
are
initialized
to
an
unknown
state
x
in
Verilog
and
u
in
VHDL
This
is
helpful
to
track
errors
caused
by
forgetting
to
reset
a
flip
flop
before
its
output
is
used
If
a
gate
receives
a
floating
input
it
may
produce
an
x
output
when
it
can
t
determine
the
correct
output
value
Similarly
if
it
receives
an
illegal
or
uninitialized
input
it
may
produce
an
x
output
HDL
Example
shows
how
Verilog
and
VHDL
combine
these
different
signal
values
in
logic
gates
Verilog
Verilog
signal
values
are
z
and
x
Verilog
constants
starting
with
z
or
x
are
padded
with
leading
z
s
or
x
s
instead
of
s
to
reach
their
full
length
when
necessary
Table
shows
a
truth
table
for
an
AND
gate
using
all
four
possible
signal
values
Note
that
the
gate
can
sometimes
determine
the
output
despite
some
inputs
being
unknown
For
example
z
returns
because
the
output
of
an
AND
gate
is
always
if
either
input
is
Otherwise
floating
or
invalid
inputs
cause
invalid
outputs
displayed
as
x
in
Verilog
VHDL
VHDL
STD
LOGIC
signals
are
z
x
and
u
Table
shows
a
truth
table
for
an
AND
gate
using
all
five
possible
signal
values
Notice
that
the
gate
can
sometimes
determine
the
output
despite
some
inputs
being
unknown
For
example
and
z
returns
because
the
output
of
an
AND
gate
is
always
if
either
input
is
Otherwise
floating
or
invalid
inputs
cause
invalid
outputs
displayed
as
x
in
VHDL
Uninitialized
inputs
cause
uninitialized
outputs
displayed
as
u
in
VHDL
HDL
Example
TRUTH
TABLES
WITH
UNDEFINED
AND
FLOATING
INPUTS
Table
Verilog
AND
gate
truth
table
with
z
and
x
A
zx
B
xx
z
x
xx
x
x
xx
Table
VHDL
AND
gate
truth
table
with
z
x
and
u
AND
A
zxu
xxu
B
z
xxxu
x
xxxu
u
uuuu
Seeing
x
or
u
values
in
simulation
is
almost
always
an
indication
of
a
bug
or
bad
coding
practice
In
the
synthesized
circuit
this
corresponds
to
a
floating
gate
input
uninitialized
state
or
contention
The
x
or
u
may
be
interpreted
randomly
by
the
circuit
as
or
leading
to
unpredictable
behavior
Bit
Swizzling
Often
it
is
necessary
to
operate
on
a
subset
of
a
bus
or
to
concatenate
join
together
signals
to
form
busses
These
operations
are
collectively
known
as
bit
swizzling
In
HDL
Example
y
is
given
the
bit
value
c
c
d
d
d
c
using
bit
swizzling
operations
Delays
HDL
statements
may
be
associated
with
delays
specified
in
arbitrary
units
They
are
helpful
during
simulation
to
predict
how
fast
a
circuit
will
work
if
you
specify
meaningful
delays
and
also
for
debugging
purposes
to
understand
cause
and
effect
deducing
the
source
of
a
bad
output
is
tricky
if
all
signals
change
simultaneously
in
the
simulation
results
These
delays
are
ignored
during
synthesis
the
delay
of
a
gate
produced
by
the
synthesizer
depends
on
its
tpd
and
tcd
specifications
not
on
numbers
in
HDL
code
HDL
Example
adds
delays
to
the
original
function
from
HDL
Example
It
assumes
that
inverters
have
a
delay
of
ns
three
input
AND
gates
have
a
delay
of
ns
and
three
input
OR
gates
have
a
delay
of
ns
Figure
shows
the
simulation
waveforms
with
y
lagging
ns
after
the
inputs
Note
that
y
is
initially
unknown
at
the
beginning
of
the
simulation
y
abcabc
abc
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
assign
y
c
d
c
b
The
operator
is
used
to
concatenate
busses
d
indicates
three
copies
of
d
Don
t
confuse
the
bit
binary
constant
b
with
a
bus
named
b
Note
that
it
was
critical
to
specify
the
length
of
bits
in
the
constant
otherwise
it
would
have
had
an
unknown
number
of
leading
zeros
that
might
appear
in
the
middle
of
y
If
y
were
wider
than
bits
zeros
would
be
placed
in
the
most
significant
bits
VHDL
y
c
downto
d
d
d
c
The
operator
is
used
to
concatenate
busses
y
must
be
a
bit
STD
LOGIC
VECTOR
Do
not
confuse
with
the
and
operator
in
VHDL
HDL
Example
BIT
SWIZZLING
Chapt
Combinational
Logic
VHDL
Libraries
and
Types
This
section
may
be
skipped
by
Verilog
users
Unlike
Verilog
VHDL
enforces
a
strict
data
typing
system
that
can
protect
the
user
from
some
errors
but
that
is
also
clumsy
at
times
Despite
its
fundamental
importance
the
STD
LOGIC
type
is
not
built
into
VHDL
Instead
it
is
part
of
the
IEEE
STD
LOGIC
library
Thus
every
file
must
contain
the
library
statements
shown
in
the
previous
examples
Verilog
timescale
ns
ps
module
example
input
a
b
c
output
y
wire
ab
bb
cb
n
n
n
assign
ab
bb
cb
a
b
c
assign
n
ab
bb
cb
assign
n
a
bb
cb
assign
n
a
bb
c
assign
y
n
n
n
endmodule
Verilog
files
can
include
a
timescale
directive
that
indicates
the
value
of
each
time
unit
The
statement
is
of
the
form
timescale
unit
precision
In
this
file
each
unit
is
ns
and
the
simulation
has
ps
precision
If
no
timescale
directive
is
given
in
the
file
a
default
unit
and
precision
usually
ns
for
both
is
used
In
Verilog
a
symbol
is
used
to
indicate
the
number
of
units
of
delay
It
can
be
placed
in
assign
statements
as
well
as
non
blocking
and
blocking
assignments
which
will
be
discussed
in
Section
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
example
is
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
example
is
signal
ab
bb
cb
n
n
n
STD
LOGIC
begin
ab
not
a
after
ns
bb
not
b
after
ns
cb
not
c
after
ns
n
ab
and
bb
and
cb
after
ns
n
a
and
bb
and
cb
after
ns
n
a
and
bb
and
c
after
ns
y
n
or
n
or
n
after
ns
end
In
VHDL
the
after
clause
is
used
to
indicate
delay
The
units
in
this
case
are
specified
as
nanoseconds
HDL
Example
LOGIC
GATES
WITH
DELAYS
Figure
Example
simulation
waveforms
with
delays
from
the
ModelSim
simulator
Chapter
qxd
Moreover
IEEE
STD
LOGIC
lacks
basic
operations
such
as
addition
comparison
shifts
and
conversion
to
integers
for
the
STD
LOGIC
VECTOR
data
Most
CAD
vendors
have
adopted
yet
more
libraries
containing
these
functions
IEEE
STD
LOGIC
UNSIGNED
and
IEEE
STD
LOGIC
SIGNED
See
Section
for
a
discussion
of
unsigned
and
signed
numbers
and
examples
of
these
operations
VHDL
also
has
a
BOOLEAN
type
with
two
values
true
and
false
BOOLEAN
values
are
returned
by
comparisons
such
as
the
equality
comparison
s
and
are
used
in
conditional
statements
such
as
when
Despite
the
temptation
to
believe
a
BOOLEAN
true
value
should
be
equivalent
to
a
STD
LOGIC
and
BOOLEAN
false
should
mean
STD
LOGIC
these
types
are
not
interchangeable
Thus
the
following
code
is
illegal
y
d
when
s
else
d
q
state
S
Instead
we
must
write
y
d
when
s
else
d
q
when
state
S
else
Although
we
do
not
declare
any
signals
to
be
BOOLEAN
they
are
automatically
implied
by
comparisons
and
used
by
conditional
statements
Similarly
VHDL
has
an
INTEGER
type
that
represents
both
positive
and
negative
integers
Signals
of
type
INTEGER
span
at
least
the
values
to
Integer
values
are
used
as
indices
of
busses
For
example
in
the
statement
y
a
and
a
and
a
and
a
and
are
integers
serving
as
an
index
to
choose
bits
of
the
a
signal
We
cannot
directly
index
a
bus
with
a
STD
LOGIC
or
STD
LOGIC
VECTOR
signal
Instead
we
must
convert
the
signal
to
an
INTEGER
This
is
demonstrated
in
HDL
Example
for
an
multiplexer
that
selects
one
bit
from
a
vector
using
a
bit
index
The
CONV
INTEGER
function
is
defined
in
the
IEEE
STD
LOGIC
UNSIGNED
library
and
performs
the
conversion
from
STD
LOGIC
VECTOR
to
INTEGER
for
positive
unsigned
values
VHDL
is
also
strict
about
out
ports
being
exclusively
for
output
For
example
the
following
code
for
two
and
three
input
AND
gates
is
illegal
VHDL
because
v
is
an
output
and
is
also
used
to
compute
w
library
IEEE
use
IEEE
STD
LOGIC
all
entity
and
is
port
a
b
c
in
STD
LOGIC
v
w
out
STD
LOGIC
end
CHAPTER
FOUR
Hardware
Description
Languages
Chapter
architecture
synth
of
and
is
begin
v
a
and
b
w
v
and
c
end
VHDL
defines
a
special
port
type
buffer
to
solve
this
problem
A
signal
connected
to
a
buffer
port
behaves
as
an
output
but
may
also
be
used
within
the
module
The
corrected
entity
definition
follows
Verilog
does
not
have
this
limitation
and
does
not
require
buffer
ports
entity
and
is
port
a
b
c
in
STD
LOGIC
v
buffer
STD
LOGIC
w
out
STD
LOGIC
end
VHDL
supports
enumeration
types
as
an
abstract
way
of
representing
information
without
assigning
specific
binary
encodings
For
example
the
divide
by
FSM
described
in
Section
uses
three
states
We
can
give
the
states
names
using
the
enumeration
type
rather
than
referring
to
them
by
binary
values
This
is
powerful
because
it
allows
VHDL
to
search
for
the
best
state
encoding
during
synthesis
rather
than
depending
on
an
arbitrary
encoding
specified
by
the
user
type
statetype
is
S
S
S
signal
state
nextstate
statetype
STRUCTURAL
MODELING
The
previous
section
discussed
behavioral
modeling
describing
a
module
in
terms
of
the
relationships
between
inputs
and
outputs
This
section
Structural
Modeling
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
mux
is
port
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
end
architecture
synth
of
mux
is
begin
y
d
CONV
INTEGER
s
end
HDL
Example
MULTIPLEXER
WITH
TYPE
CONVERSION
Figure
follows
on
next
page
Cha
CHAPTER
FOUR
Hardware
Description
Languages
un
s
un
s
un
s
un
s
un
s
un
s
un
s
un
s
y
e
d
e
d
e
d
e
d
e
d
e
d
e
d
e
d
y
s
d
Figure
mux
synthesized
circuit
examines
structural
modeling
describing
a
module
in
terms
of
how
it
is
composed
of
simpler
modules
For
example
HDL
Example
shows
how
to
assemble
a
multiplexer
from
three
multiplexers
Each
copy
of
the
multiplexer
Structural
Modeling
v
w
w
v
c
b
a
Figure
and
synthesized
circuit
Verilog
module
mux
input
d
d
d
d
input
s
output
y
wire
low
high
mux
lowmux
d
d
s
low
mux
highmux
d
d
s
high
mux
finalmux
low
high
s
y
endmodule
The
three
mux
instances
are
called
lowmux
highmux
and
finalmux
The
mux
module
must
be
defined
elsewhere
in
the
Verilog
code
HDL
Example
STRUCTURAL
MODEL
OF
MULTIPLEXER
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
mux
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
signal
low
high
STD
LOGIC
VECTOR
downto
begin
lowmux
mux
port
map
d
d
s
low
highmux
mux
port
map
d
d
s
high
finalmux
mux
port
map
low
high
s
y
end
The
architecture
must
first
declare
the
mux
ports
using
the
component
declaration
statement
This
allows
VHDL
tools
to
check
that
the
component
you
wish
to
use
has
the
same
ports
as
the
entity
that
was
declared
somewhere
else
in
another
entity
statement
preventing
errors
caused
by
changing
the
entity
but
not
the
instance
However
component
declaration
makes
VHDL
code
rather
cumbersome
Note
that
this
architecture
of
mux
was
named
struct
whereas
architectures
of
modules
with
behavioral
descriptions
from
Section
were
named
synth
VHDL
allows
multiple
architectures
implementations
for
the
same
entity
the
architectures
are
distinguished
by
name
The
names
themselves
have
no
significance
to
the
CAD
tools
but
struct
and
synth
are
common
Synthesizable
VHDL
code
generally
contains
only
one
architecture
for
each
entity
so
we
will
not
discuss
the
VHDL
syntax
to
configure
which
architecture
is
used
when
multiple
architectures
are
defined
CHAPTER
FOUR
Hardware
Description
Languages
mux
lowmux
mux
highmux
mux
finalmux
y
s
d
d
d
d
s
d
d
y
s
d
d
y
s
d
d
y
Figure
mux
synthesized
circuit
Verilog
module
mux
input
d
d
input
s
output
y
tristate
t
d
s
y
tristate
t
d
s
y
endmodule
In
Verilog
expressions
such
as
s
are
permitted
in
the
port
list
for
an
instance
Arbitrarily
complicated
expressions
are
legal
but
discouraged
because
they
make
the
code
difficult
to
read
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
tristate
port
a
in
STD
LOGIC
VECTOR
downto
en
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
signal
sbar
STD
LOGIC
begin
sbar
not
s
t
tristate
port
map
d
sbar
y
t
tristate
port
map
d
s
y
end
In
VHDL
expressions
such
as
not
s
are
not
permitted
in
the
port
map
for
an
instance
Thus
sbar
must
be
defined
as
a
separate
signal
HDL
Example
STRUCTURAL
MODEL
OF
MULTIPLEXER
is
called
an
instance
Multiple
instances
of
the
same
module
are
distinguished
by
distinct
names
in
this
case
lowmux
highmux
and
finalmux
This
is
an
example
of
regularity
in
which
the
multiplexer
is
reused
many
times
HDL
Example
uses
structural
modeling
to
construct
a
multiplexer
from
a
pair
of
tristate
buffers
C
Structural
Modeling
HDL
Example
shows
how
modules
can
access
part
of
a
bus
An
bit
wide
multiplexer
is
built
using
two
of
the
bit
multiplexers
already
defined
operating
on
the
low
and
high
nibbles
of
the
byte
In
general
complex
systems
are
designed
hierarchically
The
overall
system
is
described
structurally
by
instantiating
its
major
components
Each
of
these
components
is
described
structurally
from
its
building
blocks
and
so
forth
recursively
until
the
pieces
are
simple
enough
to
describe
behaviorally
It
is
good
style
to
avoid
or
at
least
to
minimize
mixing
structural
and
behavioral
descriptions
within
a
single
module
tristate
t
tristate
t
y
s
d
d
en
a
y
en
a
y
Figure
mux
synthesized
circuit
Verilog
module
mux
input
d
d
input
s
output
y
mux
lsbmux
d
d
s
y
mux
msbmux
d
d
s
y
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
mux
port
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
begin
lsbmux
mux
port
map
d
downto
d
downto
s
y
downto
msbhmux
mux
port
map
d
downto
d
downto
s
y
downto
end
HDL
Example
ACCESSING
PARTS
OF
BUSSES
SEQUENTIAL
LOGIC
HDL
synthesizers
recognize
certain
idioms
and
turn
them
into
specific
sequential
circuits
Other
coding
styles
may
simulate
correctly
but
synthesize
into
circuits
with
blatant
or
subtle
errors
This
section
presents
the
proper
idioms
to
describe
registers
and
latches
Registers
The
vast
majority
of
modern
commercial
systems
are
built
with
registers
using
positive
edge
triggered
D
flip
flops
HDL
Example
shows
the
idiom
for
such
flip
flops
In
Verilog
always
statements
and
VHDL
process
statements
signals
keep
their
old
value
until
an
event
in
the
sensitivity
list
takes
place
that
explicitly
causes
them
to
change
Hence
such
code
with
appropriate
sensitivity
lists
can
be
used
to
describe
sequential
circuits
with
memory
For
example
the
flip
flop
includes
only
clk
in
the
sensitive
list
It
remembers
its
old
value
of
q
until
the
next
rising
edge
of
the
clk
even
if
d
changes
in
the
interim
In
contrast
Verilog
continuous
assignment
statements
assign
and
VHDL
concurrent
assignment
statements
are
reevaluated
anytime
any
of
the
inputs
on
the
right
hand
side
changes
Therefore
such
code
necessarily
describes
combinational
logic
CHAPTER
FOUR
Hardware
Description
Languages
mux
lsbmux
mux
msbmux
y
s
d
d
s
d
d
y
s
d
d
y
Figure
mux
synthesized
circuit
C
Sequential
Logic
d
q
clk
D
Q
Figure
flop
synthesized
circuit
Verilog
module
flop
input
clk
input
d
output
reg
q
always
posedge
clk
q
d
endmodule
A
Verilog
always
statement
is
written
in
the
form
always
sensitivity
list
statement
The
statement
is
executed
only
when
the
event
specified
in
the
sensitivity
list
occurs
In
this
example
the
statement
is
q
d
pronounced
q
gets
d
Hence
the
flip
flop
copies
d
to
q
on
the
positive
edge
of
the
clock
and
otherwise
remembers
the
old
state
of
q
is
called
a
nonblocking
assignment
Think
of
it
as
a
regular
sign
for
now
we
ll
return
to
the
more
subtle
points
in
Section
Note
that
is
used
instead
of
assign
inside
an
always
statement
All
signals
on
the
left
hand
side
of
or
in
an
always
statement
must
be
declared
as
reg
In
this
example
q
is
both
an
output
and
a
reg
so
it
is
declared
as
output
reg
q
Declaring
a
signal
as
reg
does
not
mean
the
signal
is
actually
the
output
of
a
register
All
it
means
is
that
the
signal
appears
on
the
left
hand
side
of
an
assignment
in
an
always
statement
We
will
see
later
examples
of
always
statements
describing
combinational
logic
in
which
the
output
is
declared
reg
but
does
not
come
from
a
flip
flop
HDL
Example
REGISTER
Resettable
Registers
When
simulation
begins
or
power
is
first
applied
to
a
circuit
the
output
of
a
flop
or
register
is
unknown
This
is
indicated
with
x
in
Verilog
and
u
in
VHDL
Generally
it
is
good
practice
to
use
resettable
registers
so
that
on
powerup
you
can
put
your
system
in
a
known
state
The
reset
may
be
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
flop
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
flop
is
begin
process
clk
begin
if
clk
event
and
clk
then
q
d
end
if
end
process
end
A
VHDL
process
is
written
in
the
form
process
sensitivity
list
begin
statement
end
process
The
statement
is
executed
when
any
of
the
variables
in
the
sensitivity
list
change
In
this
example
the
if
statement
is
executed
when
clk
changes
indicated
by
clk
event
If
the
change
is
a
rising
edge
clk
after
the
event
then
q
d
pronounced
q
gets
d
Hence
the
flip
flop
copies
d
to
q
on
the
positive
edge
of
the
clock
and
otherwise
remembers
the
old
state
of
q
An
alternative
VHDL
idiom
for
a
flip
flop
is
process
clk
begin
if
RISING
EDGE
clk
then
q
d
end
if
end
process
RISING
EDGE
clk
is
synonymous
with
clk
event
and
clk
Chapter
qx
either
asynchronous
or
synchronous
Recall
that
asynchronous
reset
occurs
immediately
whereas
synchronous
reset
clears
the
output
only
on
the
next
rising
edge
of
the
clock
HDL
Example
demonstrates
the
idioms
for
flip
flops
with
asynchronous
and
synchronous
resets
Note
that
distinguishing
synchronous
and
asynchronous
reset
in
a
schematic
can
be
difficult
The
schematic
produced
by
Synplify
Pro
places
asynchronous
reset
at
the
bottom
of
a
flip
flop
and
synchronous
reset
on
the
left
side
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
flopr
input
clk
input
reset
input
d
output
reg
q
asynchronous
reset
always
posedge
clk
posedge
reset
if
reset
q
b
else
q
d
endmodule
module
flopr
input
clk
input
reset
input
d
output
reg
q
synchronous
reset
always
posedge
clk
if
reset
q
b
else
q
d
endmodule
Multiple
signals
in
an
always
statement
sensitivity
list
are
separated
with
a
comma
or
the
word
or
Notice
that
posedge
reset
is
in
the
sensitivity
list
on
the
asynchronously
resettable
flop
but
not
on
the
synchronously
resettable
flop
Thus
the
asynchronously
resettable
flop
immediately
responds
to
a
rising
edge
on
reset
but
the
synchronously
resettable
flop
responds
to
reset
only
on
the
rising
edge
of
the
clock
Because
the
modules
have
the
same
name
flopr
you
may
include
only
one
or
the
other
in
your
design
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
flopr
is
port
clk
reset
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
asynchronous
of
flopr
is
begin
process
clk
reset
begin
if
reset
then
q
elsif
clk
event
and
clk
then
q
d
end
if
end
process
end
architecture
synchronous
of
flopr
is
begin
process
clk
begin
if
clk
event
and
clk
then
if
reset
then
q
else
q
d
end
if
end
if
end
process
end
Multiple
signals
in
a
process
sensitivity
list
are
separated
with
a
comma
Notice
that
reset
is
in
the
sensitivity
list
on
the
asynchronously
resettable
flop
but
not
on
the
synchronously
resettable
flop
Thus
the
asynchronously
resettable
flop
immediately
responds
to
a
rising
edge
on
reset
but
the
synchronously
resettable
flop
responds
to
reset
only
on
the
rising
edge
of
the
clock
Recall
that
the
state
of
a
flop
is
initialized
to
u
at
startup
during
VHDL
simulation
As
mentioned
earlier
the
name
of
the
architecture
asynchronous
or
synchronous
in
this
example
is
ignored
by
the
VHDL
tools
but
may
be
helpful
to
the
human
reading
the
code
Because
both
architectures
describe
the
entity
flopr
you
may
include
only
one
or
the
other
in
your
design
HDL
Example
RESETTABLE
REGISTER
Chapter
q
Sequential
Logic
Enabled
Registers
Enabled
registers
respond
to
the
clock
only
when
the
enable
is
asserted
HDL
Example
shows
an
asynchronously
resettable
enabled
register
that
retains
its
old
value
if
both
reset
and
en
are
FALSE
R
d
q
reset
clk
Q
D
a
d
q
reset
clk
Q
D
R
b
Figure
flopr
synthesized
circuit
a
asynchronous
reset
b
synchronous
reset
Verilog
module
flopenr
input
clk
input
reset
input
en
input
d
output
reg
q
asynchronous
reset
always
posedge
clk
posedge
reset
if
reset
q
b
else
if
en
q
d
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
flopenr
is
port
clk
reset
en
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
asynchronous
of
flopenr
is
asynchronous
reset
begin
process
clk
reset
begin
if
reset
then
q
elsif
clk
event
and
clk
then
if
en
then
q
d
end
if
end
if
end
process
end
HDL
Example
RESETTABLE
ENABLED
REGISTER
Chapter
Multiple
Registers
A
single
always
process
statement
can
be
used
to
describe
multiple
pieces
of
hardware
For
example
consider
the
synchronizer
from
Section
made
of
two
back
to
back
flip
flops
as
shown
in
Figure
HDL
Example
describes
the
synchronizer
On
the
rising
edge
of
clk
d
is
copied
to
n
At
the
same
time
n
is
copied
to
q
CHAPTER
FOUR
Hardware
Description
Languages
R
d
q
en
reset
clk
D
Q
E
Figure
flopenr
synthesized
circuit
CLK
CLK
D
N
Q
Figure
Synchronizer
circuit
Verilog
module
sync
input
clk
input
d
output
reg
q
reg
n
always
posedge
clk
begin
n
d
q
n
end
endmodule
n
must
be
declared
as
a
reg
because
it
is
an
internal
signal
used
on
the
left
hand
side
of
in
an
always
statement
Also
notice
that
the
begin
end
construct
is
necessary
because
multiple
statements
appear
in
the
always
statement
This
is
analogous
to
in
C
or
Java
The
begin
end
was
not
needed
in
the
flopr
example
because
if
else
counts
as
a
single
statement
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
sync
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
q
out
STD
LOGIC
end
architecture
good
of
sync
is
signal
n
STD
LOGIC
begin
process
clk
begin
if
clk
event
and
clk
then
n
d
q
n
end
if
end
process
end
n
must
be
declared
as
a
signal
because
it
is
an
internal
signal
used
in
the
module
HDL
Example
SYNCHRONIZER
n
q
d
q
clk
QD
QD
Figure
sync
synthesized
circuit
Chapte
More
Combinational
Logic
Latches
Recall
from
Section
that
a
D
latch
is
transparent
when
the
clock
is
HIGH
allowing
data
to
flow
from
input
to
output
The
latch
becomes
opaque
when
the
clock
is
LOW
retaining
its
old
state
HDL
Example
shows
the
idiom
for
a
D
latch
Not
all
synthesis
tools
support
latches
well
Unless
you
know
that
your
tool
does
support
latches
and
you
have
a
good
reason
to
use
them
avoid
them
and
use
edge
triggered
flip
flops
instead
Furthermore
take
care
that
your
HDL
does
not
imply
any
unintended
latches
something
that
is
easy
to
do
if
you
aren
t
attentive
Many
synthesis
tools
warn
you
when
a
latch
is
created
if
you
didn
t
expect
one
track
down
the
bug
in
your
HDL
MORE
COMBINATIONAL
LOGIC
In
Section
we
used
assignment
statements
to
describe
combinational
logic
behaviorally
Verilog
always
statements
and
VHDL
process
statements
are
used
to
describe
sequential
circuits
because
they
remember
the
old
state
when
no
new
state
is
prescribed
However
always
process
lat
q
q
d
clk
D
Q
C
Figure
latch
synthesized
circuit
Verilog
module
latch
input
clk
input
d
output
reg
q
always
clk
d
if
clk
q
d
endmodule
The
sensitivity
list
contains
both
clk
and
d
so
the
always
statement
evaluates
any
time
clk
or
d
changes
If
clk
is
HIGH
d
flows
through
to
q
q
must
be
declared
to
be
a
reg
because
it
appears
on
the
left
hand
side
of
in
an
always
statement
This
does
not
always
mean
that
q
is
the
output
of
a
register
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
latch
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
downto
q
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
latch
is
begin
process
clk
d
begin
if
clk
then
q
d
end
if
end
process
end
The
sensitivity
list
contains
both
clk
and
d
so
the
process
evaluates
anytime
clk
or
d
changes
If
clk
is
HIGH
d
flows
through
to
q
HDL
Example
D
LATCH
Chap
statements
can
also
be
used
to
describe
combinational
logic
behaviorally
if
the
sensitivity
list
is
written
to
respond
to
changes
in
all
of
the
inputs
and
the
body
prescribes
the
output
value
for
every
possible
input
combination
HDL
Example
uses
always
process
statements
to
describe
a
bank
of
four
inverters
see
Figure
for
the
synthesized
circuit
HDLs
support
blocking
and
nonblocking
assignments
in
an
always
process
statement
A
group
of
blocking
assignments
are
evaluated
in
the
order
in
which
they
appear
in
the
code
just
as
one
would
expect
in
a
standard
programming
language
A
group
of
nonblocking
assignments
are
evaluated
concurrently
all
of
the
statements
are
evaluated
before
any
of
the
signals
on
the
left
hand
sides
are
updated
HDL
Example
defines
a
full
adder
using
intermediate
signals
p
and
g
to
compute
s
and
cout
It
produces
the
same
circuit
from
Figure
but
uses
always
process
statements
in
place
of
assignment
statements
These
two
examples
are
poor
applications
of
always
process
statements
for
modeling
combinational
logic
because
they
require
more
lines
than
the
equivalent
approach
with
assignment
statements
from
Section
Moreover
they
pose
the
risk
of
inadvertently
implying
sequential
logic
if
the
inputs
are
left
out
of
the
sensitivity
list
However
case
and
if
statements
are
convenient
for
modeling
more
complicated
combinational
logic
case
and
if
statements
must
appear
within
always
process
statements
and
are
examined
in
the
next
sections
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
inv
input
a
output
reg
y
always
y
a
endmodule
always
reevaluates
the
statements
inside
the
always
statement
any
time
any
of
the
signals
on
the
right
hand
side
of
or
inside
the
always
statement
change
Thus
is
a
safe
way
to
model
combinational
logic
In
this
particular
example
a
would
also
have
sufficed
The
in
the
always
statement
is
called
a
blocking
assignment
in
contrast
to
the
nonblocking
assignment
In
Verilog
it
is
good
practice
to
use
blocking
assignments
for
combinational
logic
and
nonblocking
assignments
for
sequential
logic
This
will
be
discussed
further
in
Section
Note
that
y
must
be
declared
as
reg
because
it
appears
on
the
left
hand
side
of
a
or
sign
in
an
always
statement
Nevertheless
y
is
the
output
of
combinational
logic
not
a
register
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
inv
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
proc
of
inv
is
begin
process
a
begin
y
not
a
end
process
end
The
begin
and
end
process
statements
are
required
in
VHDL
even
though
the
process
contains
only
one
assignment
HDL
Example
INVERTER
USING
always
process
Chapter
More
Combinational
Logic
Verilog
In
a
Verilog
always
statement
indicates
a
blocking
assignment
and
indicates
a
nonblocking
assignment
also
called
a
concurrent
assignment
Do
not
confuse
either
type
with
continuous
assignment
using
the
assign
statement
assign
statements
must
be
used
outside
always
statements
and
are
also
evaluated
concurrently
VHDL
In
a
VHDL
process
statement
indicates
a
blocking
assignment
and
indicates
a
nonblocking
assignment
also
called
a
concurrent
assignment
This
is
the
first
section
where
is
introduced
Nonblocking
assignments
are
made
to
outputs
and
to
signals
Blocking
assignments
are
made
to
variables
which
are
declared
in
process
statements
see
HDL
Example
can
also
appear
outside
process
statements
where
it
is
also
evaluated
concurrently
Verilog
module
fulladder
input
a
b
cin
output
reg
s
cout
reg
p
g
always
begin
p
a
b
blocking
g
a
b
blocking
s
p
cin
blocking
cout
g
p
cin
blocking
end
endmodule
In
this
case
an
a
b
cin
would
have
been
equivalent
to
However
is
better
because
it
avoids
common
mistakes
of
missing
signals
in
the
stimulus
list
For
reasons
that
will
be
discussed
in
Section
it
is
best
to
use
blocking
assignments
for
combinational
logic
This
example
uses
blocking
assignments
first
computing
p
then
g
then
s
and
finally
cout
Because
p
and
g
appear
on
the
left
hand
side
of
an
assignment
in
an
always
statement
they
must
be
declared
to
be
reg
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fulladder
is
port
a
b
cin
in
STD
LOGIC
s
cout
out
STD
LOGIC
end
architecture
synth
of
fulladder
is
begin
process
a
b
cin
variable
p
g
STD
LOGIC
begin
p
a
xor
b
blocking
g
a
and
b
blocking
s
p
xor
cin
cout
g
or
p
and
cin
end
process
end
The
process
sensitivity
list
must
include
a
b
and
cin
because
combinational
logic
should
respond
to
changes
of
any
input
For
reasons
that
will
be
discussed
in
Section
it
is
best
to
use
blocking
assignments
for
intermediate
variables
in
combinational
logic
This
example
uses
blocking
assignments
for
p
and
g
so
that
they
get
their
new
values
before
being
used
to
compute
s
and
cout
that
depend
on
them
Because
p
and
g
appear
on
the
left
hand
side
of
a
blocking
assignment
in
a
process
statement
they
must
be
declared
to
be
variable
rather
than
signal
The
variable
declaration
appears
before
the
begin
in
the
process
where
the
variable
is
used
HDL
Example
FULL
ADDER
USING
always
process
Chapter
qxd
Case
Statements
A
better
application
of
using
the
always
process
statement
for
combinational
logic
is
a
seven
segment
display
decoder
that
takes
advantage
of
the
case
statement
that
must
appear
inside
an
always
process
statement
As
you
might
have
noticed
in
Example
the
design
process
for
large
blocks
of
combinational
logic
is
tedious
and
prone
to
error
HDLs
offer
a
great
improvement
allowing
you
to
specify
the
function
at
a
higher
level
of
abstraction
and
then
automatically
synthesize
the
function
into
gates
HDL
Example
uses
case
statements
to
describe
a
seven
segment
display
decoder
based
on
its
truth
table
See
Example
for
a
description
of
the
seven
segment
display
decoder
The
case
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
sevenseg
input
data
output
reg
segments
always
case
data
abc
defg
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
segments
b
default
segments
b
endcase
endmodule
The
case
statement
checks
the
value
of
data
When
data
is
the
statement
performs
the
action
after
the
colon
setting
segments
to
The
case
statement
similarly
checks
other
data
values
up
to
note
the
use
of
the
default
base
base
The
default
clause
is
a
convenient
way
to
define
the
output
for
all
cases
not
explicitly
listed
guaranteeing
combinational
logic
In
Verilog
case
statements
must
appear
inside
always
statements
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
seven
seg
decoder
is
port
data
in
STD
LOGIC
VECTOR
downto
segments
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
seven
seg
decoder
is
begin
process
data
begin
case
data
is
abcdefg
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
X
segments
when
others
segments
end
case
end
process
end
The
case
statement
checks
the
value
of
data
When
data
is
the
statement
performs
the
action
after
the
setting
segments
to
The
case
statement
similarly
checks
other
data
values
up
to
note
the
use
of
X
for
hexadecimal
numbers
The
others
clause
is
a
convenient
way
to
define
the
output
for
all
cases
not
explicitly
listed
guaranteeing
combinational
logic
Unlike
Verilog
VHDL
supports
selected
signal
assignment
statements
see
HDL
Example
which
are
much
like
case
statements
but
can
appear
outside
processes
Thus
there
is
less
reason
to
use
processes
to
describe
combinational
logic
HDL
Example
SEVEN
SEGMENT
DISPLAY
DECODER
Chapter
qxd
PM
Pag
More
Combinational
Logic
statement
performs
different
actions
depending
on
the
value
of
its
input
A
case
statement
implies
combinational
logic
if
all
possible
input
combinations
are
defined
otherwise
it
implies
sequential
logic
because
the
output
will
keep
its
old
value
in
the
undefined
cases
Synplify
Pro
synthesizes
the
seven
segment
display
decoder
into
a
read
only
memory
ROM
containing
the
outputs
for
each
of
the
possible
inputs
ROMs
are
discussed
further
in
Section
If
the
default
or
others
clause
were
left
out
of
the
case
statement
the
decoder
would
have
remembered
its
previous
output
anytime
data
were
in
the
range
of
This
is
strange
behavior
for
hardware
Ordinary
decoders
are
also
commonly
written
with
case
statements
HDL
Example
describes
a
decoder
If
Statements
always
process
statements
may
also
contain
if
statements
The
if
statement
may
be
followed
by
an
else
statement
If
all
possible
input
rom
segments
data
segments
DOUT
A
Figure
sevenseg
synthesized
circuit
Verilog
module
decoder
input
a
output
reg
y
always
case
a
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
b
y
b
endcase
endmodule
No
default
statement
is
needed
because
all
cases
are
covered
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
decoder
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
decoder
is
begin
process
a
begin
case
a
is
when
y
when
y
when
y
when
y
when
y
when
y
when
y
when
y
end
case
end
process
end
No
others
clause
is
needed
because
all
cases
are
covered
HDL
Example
DECODER
Chapter
qxd
CHAPTER
FOUR
Hardware
Description
Languages
y
y
y
y
y
y
y
y
y
a
Figure
decoder
synthesized
circuit
More
Combinational
Logic
combinations
are
handled
the
statement
implies
combinational
logic
otherwise
it
produces
sequential
logic
like
the
latch
in
Section
HDL
Example
uses
if
statements
to
describe
a
priority
circuit
defined
in
Section
Recall
that
an
N
input
priority
circuit
sets
the
output
TRUE
that
corresponds
to
the
most
significant
input
that
is
TRUE
Verilog
casez
This
section
may
be
skipped
by
VHDL
users
Verilog
also
provides
the
casez
statement
to
describe
truth
tables
with
don
t
cares
indicated
with
in
the
casez
statement
HDL
Example
shows
how
to
describe
a
priority
circuit
with
casez
Synplify
Pro
synthesizes
a
slightly
different
circuit
for
this
module
shown
in
Figure
than
it
did
for
the
priority
circuit
in
Figure
However
the
circuits
are
logically
equivalent
Blocking
and
Nonblocking
Assignments
The
guidelines
on
page
explain
when
and
how
to
use
each
type
of
assignment
If
these
guidelines
are
not
followed
it
is
possible
to
write
Verilog
module
priority
input
a
output
reg
y
always
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
y
b
endmodule
In
Verilog
if
statements
must
appear
inside
of
always
statements
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
priority
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
priority
is
begin
process
a
begin
if
a
then
y
elsif
a
then
y
elsif
a
then
y
elsif
a
then
y
else
y
end
if
end
process
end
Unlike
Verilog
VHDL
supports
conditional
signal
assignment
statements
see
HDL
Example
which
are
much
like
if
statements
but
can
appear
outside
processes
Thus
there
is
less
reason
to
use
processes
to
describe
combinational
logic
Figure
follows
on
next
page
HDL
Example
PRIORITY
CIRCUIT
Chapter
qxd
code
that
appears
to
work
in
simulation
but
synthesizes
to
incorrect
hardware
The
optional
remainder
of
this
section
explains
the
principles
behind
the
guidelines
Combinational
Logic
The
full
adder
from
HDL
Example
is
correctly
modeled
using
blocking
assignments
This
section
explores
how
it
operates
and
how
it
would
differ
if
nonblocking
assignments
had
been
used
Imagine
that
a
b
and
cin
are
all
initially
p
g
s
and
cout
are
thus
as
well
At
some
time
a
changes
to
triggering
the
always
process
statement
The
four
blocking
assignments
evaluate
in
CHAPTER
FOUR
Hardware
Description
Languages
module
priority
casez
input
a
output
reg
y
always
casez
a
b
y
b
b
y
b
b
y
b
b
y
b
default
y
b
endcase
endmodule
HDL
Example
PRIORITY
CIRCUIT
USING
casez
y
un
y
un
y
y
y
y
a
Figure
priority
synthesized
circuit
See
figure
Chapt
More
Combinational
Logic
y
y
y
a
y
Figure
priority
casez
synthesized
circuit
Verilog
Use
always
posedge
clk
and
nonblocking
assignments
to
model
synchronous
sequential
logic
always
posedge
clk
begin
nl
d
nonblocking
q
nl
nonblocking
end
Use
continuous
assignments
to
model
simple
combinational
logic
assign
y
s
d
d
Use
always
and
blocking
assignments
to
model
more
complicated
combinational
logic
where
the
always
statement
is
helpful
always
begin
p
a
b
blocking
g
a
b
blocking
s
p
cin
cout
g
p
cin
end
Do
not
make
assignments
to
the
same
signal
in
more
than
one
always
statement
or
continuous
assignment
statement
VHDL
Use
process
clk
and
nonblocking
assignments
to
model
synchronous
sequential
logic
process
clk
begin
if
clk
event
and
clk
then
nl
d
nonblocking
q
nl
nonblocking
end
if
end
process
Use
concurrent
assignments
outside
process
statements
to
model
simple
combinational
logic
y
d
when
s
else
d
Use
process
in
in
to
model
more
complicated
combinational
logic
where
the
process
is
helpful
Use
blocking
assignments
for
internal
variables
process
a
b
cin
variable
p
g
STD
LOGIC
begin
p
a
xor
b
blocking
g
a
and
b
blocking
s
p
xor
cin
cout
g
or
p
and
cin
end
process
Do
not
make
assignments
to
the
same
variable
in
more
than
one
process
or
concurrent
assignment
statement
BLOCKING
AND
NONBLOCKING
ASSIGNMENT
GUIDELINES
Chapter
qxd
the
order
shown
here
In
the
VHDL
code
s
and
cout
are
assigned
concurrently
Note
that
p
and
g
get
their
new
values
before
s
and
cout
are
computed
because
of
the
blocking
assignments
This
is
important
because
we
want
to
compute
s
and
cout
using
the
new
values
of
p
and
g
p
g
s
cout
In
contrast
HDL
Example
illustrates
the
use
of
nonblocking
assignments
Now
consider
the
same
case
of
a
rising
from
to
while
b
and
cin
are
The
four
nonblocking
assignments
evaluate
concurrently
p
g
s
cout
Observe
that
s
is
computed
concurrently
with
p
and
hence
uses
the
old
value
of
p
not
the
new
value
Therefore
s
remains
rather
than
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
nonblocking
assignments
not
recommended
module
fulladder
input
a
b
cin
output
reg
s
cout
reg
p
g
always
begin
p
a
b
nonblocking
g
a
b
nonblocking
s
p
cin
cout
g
p
cin
end
endmodule
Because
p
and
g
appear
on
the
left
hand
side
of
an
assignment
in
an
always
statement
they
must
be
declared
to
be
reg
VHDL
nonblocking
assignments
not
recommended
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fulladder
is
port
a
b
cin
in
STD
LOGIC
s
cout
out
STD
LOGIC
end
architecture
nonblocking
of
fulladder
is
signal
p
g
STD
LOGIC
begin
process
a
b
cin
p
g
begin
p
a
xor
b
nonblocking
g
a
and
b
nonblocking
s
p
xor
cin
cout
g
or
p
and
cin
end
process
end
Because
p
and
g
appear
on
the
left
hand
side
of
a
nonblocking
assignment
in
a
process
statement
they
must
be
declared
to
be
signal
rather
than
variable
The
signal
declaration
appears
before
the
begin
in
the
architecture
not
the
process
HDL
Example
FULL
ADDER
USING
NONBLOCKING
ASSIGNMENTS
Chapter
qxd
More
Combinational
Logic
becoming
However
p
does
change
from
to
This
change
triggers
the
always
process
statement
to
evaluate
a
second
time
as
follows
p
g
s
cout
This
time
p
is
already
so
s
correctly
changes
to
The
nonblocking
assignments
eventually
reach
the
right
answer
but
the
always
process
statement
had
to
evaluate
twice
This
makes
simulation
slower
though
it
synthesizes
to
the
same
hardware
Another
drawback
of
nonblocking
assignments
in
modeling
combinational
logic
is
that
the
HDL
will
produce
the
wrong
result
if
you
forget
to
include
the
intermediate
variables
in
the
sensitivity
list
Worse
yet
some
synthesis
tools
will
synthesize
the
correct
hardware
even
when
a
faulty
sensitivity
list
causes
incorrect
simulation
This
leads
to
a
mismatch
between
the
simulation
results
and
what
the
hardware
actually
does
Sequential
Logic
The
synchronizer
from
HDL
Example
is
correctly
modeled
using
nonblocking
assignments
On
the
rising
edge
of
the
clock
d
is
copied
to
n
at
the
same
time
that
n
is
copied
to
q
so
the
code
properly
describes
two
registers
For
example
suppose
initially
that
d
n
and
q
On
the
rising
edge
of
the
clock
the
following
two
assignments
occur
concurrently
so
that
after
the
clock
edge
n
and
q
nl
d
q
n
HDL
Example
tries
to
describe
the
same
module
using
blocking
assignments
On
the
rising
edge
of
clk
d
is
copied
to
n
Then
this
new
value
of
n
is
copied
to
q
resulting
in
d
improperly
appearing
at
both
n
and
q
The
assignments
occur
one
after
the
other
so
that
after
the
clock
edge
q
n
n
d
q
n
Verilog
If
the
sensitivity
list
of
the
always
statement
in
HDL
Example
were
written
as
always
a
b
cin
rather
than
always
then
the
statement
would
not
reevaluate
when
p
or
g
changes
In
the
previous
example
s
would
be
incorrectly
left
at
not
VHDL
If
the
sensitivity
list
of
the
process
statement
in
HDL
Example
were
written
as
process
a
b
cin
rather
than
process
a
b
cin
p
g
then
the
statement
would
not
reevaluate
when
p
or
g
changes
In
the
previous
example
s
would
be
incorrectly
left
at
not
Chapter
qxd
Because
n
is
invisible
to
the
outside
world
and
does
not
influence
the
behavior
of
q
the
synthesizer
optimizes
it
away
entirely
as
shown
in
Figure
The
moral
of
this
illustration
is
to
exclusively
use
nonblocking
assignment
in
always
statements
when
modeling
sequential
logic
With
sufficient
cleverness
such
as
reversing
the
orders
of
the
assignments
you
could
make
blocking
assignments
work
correctly
but
blocking
assignments
offer
no
advantages
and
only
introduce
the
risk
of
unintended
behavior
Certain
sequential
circuits
will
not
work
with
blocking
assignments
no
matter
what
the
order
FINITE
STATE
MACHINES
Recall
that
a
finite
state
machine
FSM
consists
of
a
state
register
and
two
blocks
of
combinational
logic
to
compute
the
next
state
and
the
output
given
the
current
state
and
the
input
as
was
shown
in
Figure
HDL
descriptions
of
state
machines
are
correspondingly
divided
into
three
parts
to
model
the
state
register
the
next
state
logic
and
the
output
logic
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
Bad
implementation
using
blocking
assignments
module
syncbad
input
clk
input
d
output
reg
q
reg
n
always
posedge
clk
begin
n
d
blocking
q
n
blocking
end
endmodule
VHDL
Bad
implementation
using
blocking
assignment
library
IEEE
use
IEEE
STD
LOGIC
all
entity
syncbad
is
port
clk
in
STD
LOGIC
d
in
STD
LOGIC
q
out
STD
LOGIC
end
architecture
bad
of
syncbad
is
begin
process
clk
variable
n
STD
LOGIC
begin
if
clk
event
and
clk
then
n
d
blocking
q
n
end
if
end
process
end
HDL
Example
BAD
SYNCHRONIZER
WITH
BLOCKING
ASSIGNMENTS
q
d
q
clk
D
Q
Figure
syncbad
synthesized
circuit
Chapt
Finite
State
Machines
HDL
Example
describes
the
divide
by
FSM
from
Section
It
provides
an
asynchronous
reset
to
initialize
the
FSM
The
state
register
uses
the
ordinary
idiom
for
flip
flops
The
next
state
and
output
logic
blocks
are
combinational
The
Synplify
Pro
Synthesis
tool
just
produces
a
block
diagram
and
state
transition
diagram
for
state
machines
it
does
not
show
the
logic
gates
or
the
inputs
and
outputs
on
the
arcs
and
states
Therefore
be
careful
that
you
have
specified
the
FSM
correctly
in
your
HDL
code
The
state
transition
diagram
in
Figure
for
the
divide
by
FSM
is
analogous
to
the
diagram
in
Figure
b
The
double
circle
indicates
that
Verilog
module
divideby
FSM
input
clk
input
reset
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
state
register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
next
state
logic
always
case
state
S
nextstate
S
S
nextstate
S
S
nextstate
S
default
nextstate
S
endcase
output
logic
assign
y
state
S
endmodule
The
parameter
statement
is
used
to
define
constants
within
a
module
Naming
the
states
with
parameters
is
not
required
but
it
makes
changing
state
encodings
much
easier
and
makes
the
code
more
readable
Notice
how
a
case
statement
is
used
to
define
the
state
transition
table
Because
the
next
state
logic
should
be
combinational
a
default
is
necessary
even
though
the
state
b
should
never
arise
The
output
y
is
when
the
state
is
S
The
equality
comparison
a
b
evaluates
to
if
a
equals
b
and
otherwise
The
inequality
comparison
a
b
does
the
inverse
evaluating
to
if
a
does
not
equal
b
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
divideby
FSM
is
port
clk
reset
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
divideby
FSM
is
type
statetype
is
S
S
S
signal
state
nextstate
statetype
begin
state
register
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
next
state
logic
nextstate
S
when
state
S
else
S
when
state
S
else
S
output
logic
y
when
state
S
else
end
This
example
defines
a
new
enumeration
data
type
statetype
with
three
possibilities
S
S
and
S
state
and
nextstate
are
statetype
signals
By
using
an
enumeration
instead
of
choosing
the
state
encoding
VHDL
frees
the
synthesizer
to
explore
various
state
encodings
to
choose
the
best
one
The
output
y
is
when
the
state
is
S
The
inequalitycomparison
uses
To
produce
an
output
of
when
the
state
is
anything
but
S
change
the
comparison
to
state
S
HDL
Example
DIVIDE
BY
FINITE
STATE
MACHINE
Notice
that
the
synthesis
tool
uses
a
bit
encoding
Q
instead
of
the
bit
encoding
suggested
in
the
Verilog
code
Chapter
qxd
S
is
the
reset
state
Gate
level
implementations
of
the
divide
by
FSM
were
shown
in
Section
If
for
some
reason
we
had
wanted
the
output
to
be
HIGH
in
states
S
and
S
the
output
logic
would
be
modified
as
follows
The
next
two
examples
describe
the
snail
pattern
recognizer
FSM
from
Section
The
code
shows
how
to
use
case
and
if
statements
to
handle
next
state
and
output
logic
that
depend
on
the
inputs
as
well
as
the
current
state
We
show
both
Moore
and
Mealy
modules
In
the
Moore
machine
HDL
Example
the
output
depends
only
on
the
current
state
whereas
in
the
Mealy
machine
HDL
Example
the
output
logic
depends
on
both
the
current
state
and
inputs
CHAPTER
FOUR
Hardware
Description
Languages
S
S
S
statemachine
state
y
reset
clk
C
Q
R
Figure
divideby
fsm
synthesized
circuit
Verilog
Output
Logic
assign
y
state
S
state
S
VHDL
output
logic
y
when
state
S
or
state
S
else
Chapter
Verilog
module
patternMoore
input
clk
input
reset
input
a
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
state
register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
next
state
logic
always
case
state
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
default
nextstate
S
endcase
output
logic
assign
y
state
S
endmodule
Note
how
nonblocking
assignments
are
used
in
the
state
register
to
describe
sequential
logic
whereas
blocking
assignments
are
used
in
the
next
state
logic
to
describe
combinational
logic
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
patternMoore
is
port
clk
reset
in
STD
LOGIC
a
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
patternMoore
is
type
statetype
is
S
S
S
S
S
signal
state
nextstate
statetype
begin
state
register
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
next
state
logic
process
state
a
begin
case
state
is
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
others
nextstate
S
end
case
end
process
output
logic
y
when
state
S
else
end
HDL
Example
PATTERN
RECOGNIZER
MOORE
FSM
S
S
S
S
S
statemachine
state
y
a
reset
clk
I
C
Q
R
Figure
patternMoore
synthesized
circuit
Chapter
qxd
PM
Page
CHAPTER
FOUR
Hardware
Description
Languages
S
S
S
S
statemachine
state
y
y
a
reset
clk
I
C
Q
R
Figure
patternMealy
synthesized
circuit
Verilog
module
patternMealy
input
clk
input
reset
input
a
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
state
register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
next
state
logic
always
case
state
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
S
if
a
nextstate
S
else
nextstate
S
default
nextstate
S
endcase
output
logic
assign
y
a
state
S
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
patternMealy
is
port
clk
reset
in
STD
LOGIC
a
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
patternMealy
is
type
statetype
is
S
S
S
S
signal
state
nextstate
statetype
begin
state
register
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
next
state
logic
process
state
a
begin
case
state
is
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
then
nextstate
S
else
nextstate
S
end
if
when
others
nextstate
S
end
case
end
process
output
logic
y
when
a
and
state
S
else
end
HDL
Example
PATTERN
RECOGNIZER
MEALY
FSM
Chapter
qxd
PM
Page
Parameterized
Modules
Verilog
module
mux
parameter
width
input
width
d
d
input
s
output
width
y
assign
y
s
d
d
endmodule
Verilog
allows
a
parameter
statement
before
the
inputs
and
outputs
to
define
parameters
The
parameter
statement
includes
a
default
value
of
the
parameter
width
The
number
of
bits
in
the
inputs
and
outputs
can
depend
on
this
parameter
module
mux
input
d
d
d
d
input
s
output
y
wire
low
hi
mux
lowmux
d
d
s
low
mux
himux
d
d
s
hi
mux
outmux
low
hi
s
y
endmodule
The
bit
multiplexer
instantiates
three
multiplexers
using
their
default
widths
In
contrast
a
bit
multiplexer
mux
would
need
to
override
the
default
width
using
before
the
instance
name
as
shown
below
module
mux
input
d
d
d
d
input
s
output
y
wire
low
hi
mux
lowmux
d
d
s
low
mux
himux
d
d
s
hi
mux
outmux
low
hi
s
y
endmodule
Do
not
confuse
the
use
of
the
sign
indicating
delays
with
the
use
of
in
defining
and
overriding
parameters
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
architecture
synth
of
mux
is
begin
y
d
when
s
else
d
end
The
generic
statement
includes
a
default
value
of
width
The
value
is
an
integer
entity
mux
is
port
d
d
d
d
in
STD
LOGIC
VECTOR
downto
s
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mux
is
component
mux
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
component
signal
low
hi
STD
LOGIC
VECTOR
downto
begin
lowmux
mux
port
map
d
d
s
low
himux
mux
port
map
d
d
s
hi
outmux
mux
port
map
low
hi
s
y
end
The
bit
multiplexer
mux
instantiates
three
multiplexers
using
their
default
widths
In
contrast
a
bit
multiplexer
mux
would
need
to
override
the
default
width
using
generic
map
as
shown
below
lowmux
mux
generic
map
port
map
d
d
s
low
himux
mux
generic
map
port
map
d
d
s
hi
outmux
mux
generic
map
port
map
low
hi
s
y
HDL
Example
PARAMETERIZED
N
BIT
MULTIPLEXERS
PARAMETERIZED
MODULES
So
far
all
of
our
modules
have
had
fixed
width
inputs
and
outputs
For
example
we
had
to
define
separate
modules
for
and
bit
wide
multiplexers
HDLs
permit
variable
bit
widths
using
parameterized
modules
Chapt
HDL
Example
declares
a
parameterized
multiplexer
with
a
default
width
of
then
uses
it
to
create
and
bit
multiplexers
HDL
Example
shows
a
decoder
which
is
an
even
better
application
of
parameterized
modules
A
large
N
N
decoder
is
cumbersome
to
CHAPTER
FOUR
Hardware
Description
Languages
mux
lowmux
mux
himux
mux
outmux
y
s
d
d
d
d
s
d
d
y
s
d
d
y
s
d
d
y
Figure
mux
synthesized
circuit
Verilog
module
decoder
parameter
N
input
N
a
output
reg
N
y
always
begin
y
y
a
end
endmodule
N
indicates
N
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
use
IEEE
STD
LOGIC
ARITH
all
entity
decoder
is
generic
N
integer
port
a
in
STD
LOGIC
VECTOR
N
downto
y
out
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
decoder
is
begin
process
a
variable
tmp
STD
LOGIC
VECTOR
N
downto
begin
tmp
CONV
STD
LOGIC
VECTOR
N
tmp
CONV
INTEGER
a
y
tmp
end
process
end
N
indicates
N
CONV
STD
LOGIC
VECTOR
N
produces
a
STD
LOGIC
VECTOR
of
length
N
containing
all
s
It
requires
the
STD
LOGIC
ARITH
library
The
function
is
useful
in
other
parameterized
functions
such
as
resettable
flip
flops
that
need
to
be
able
to
produce
constants
with
a
parameterized
number
of
bits
The
bit
index
in
VHDL
must
be
an
integer
so
the
CONV
INTEGER
function
is
used
to
convert
a
from
a
STD
LOGIC
VECTOR
to
an
integer
HDL
Example
PARAMETERIZED
N
N
DECODER
Chapter
Parameterized
Modules
specify
with
case
statements
but
easy
using
parameterized
code
that
simply
sets
the
appropriate
output
bit
to
Specifically
the
decoder
uses
blocking
assignments
to
set
all
the
bits
to
then
changes
the
appropriate
bit
to
HDLs
also
provide
generate
statements
to
produce
a
variable
amount
of
hardware
depending
on
the
value
of
a
parameter
generate
supports
for
loops
and
if
statements
to
determine
how
many
of
what
types
of
hardware
to
produce
HDL
Example
demonstrates
how
to
use
generate
statements
to
produce
an
N
input
AND
function
from
a
cascade
of
two
input
AND
gates
Use
generate
statements
with
caution
it
is
easy
to
produce
a
large
amount
of
hardware
unintentionally
Verilog
module
andN
parameter
width
input
width
a
output
y
genvar
i
wire
width
x
generate
for
i
i
width
ii
begin
forloop
if
i
assign
x
a
a
else
assign
x
i
a
i
x
i
end
endgenerate
assign
y
x
width
endmodule
The
for
statement
loops
through
i
width
to
produce
many
consecutive
AND
gates
The
begin
in
a
generate
for
loop
must
be
followed
by
a
and
an
arbitrary
label
forloop
in
this
case
Of
course
writing
assign
y
a
would
be
much
easier
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
andN
is
generic
width
integer
port
a
in
STD
LOGIC
VECTOR
width
downto
y
out
STD
LOGIC
end
architecture
synth
of
andN
is
signal
i
integer
signal
x
STD
LOGIC
VECTOR
width
downto
begin
AllBits
for
i
in
to
width
generate
LowBit
if
i
generate
A
x
a
and
a
end
generate
OtherBits
if
i
generate
Ai
x
i
a
i
and
x
i
end
generate
end
generate
y
x
width
end
HDL
Example
PARAMETERIZED
N
INPUT
AND
GATE
x
x
x
x
x
x
x
y
a
Figure
andN
synthesized
circuit
Chapter
qxd
TESTBENCHES
A
testbench
is
an
HDL
module
that
is
used
to
test
another
module
called
the
device
under
test
DUT
The
testbench
contains
statements
to
apply
inputs
to
the
DUT
and
ideally
to
check
that
the
correct
outputs
are
produced
The
input
and
desired
output
patterns
are
called
test
vectors
Consider
testing
the
sillyfunction
module
from
Section
that
computes
This
is
a
simple
module
so
we
can
perform
exhaustive
testing
by
applying
all
eight
possible
test
vectors
HDL
Example
demonstrates
a
simple
testbench
It
instantiates
the
DUT
then
applies
the
inputs
Blocking
assignments
and
delays
are
used
to
apply
the
inputs
in
the
appropriate
order
The
user
must
view
the
results
of
the
simulation
and
verify
by
inspection
that
the
correct
outputs
are
produced
Testbenches
are
simulated
the
same
as
other
HDL
modules
However
they
are
not
synthesizeable
y
a
b
c
ab
c
abc
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
testbench
reg
a
b
c
wire
y
instantiate
device
under
test
sillyfunction
dut
a
b
c
y
apply
inputs
one
at
a
time
initial
begin
a
b
c
c
b
c
c
a
b
c
c
b
c
c
end
endmodule
The
initial
statement
executes
the
statements
in
its
body
at
the
start
of
simulation
In
this
case
it
first
applies
the
input
pattern
and
waits
for
time
units
It
then
applies
and
waits
more
units
and
so
forth
until
all
eight
possible
inputs
have
been
applied
initial
statements
should
be
used
only
in
testbenches
for
simulation
not
in
modules
intended
to
be
synthesized
into
actual
hardware
Hardware
has
no
way
of
magically
executing
a
sequence
of
special
steps
when
it
is
first
turned
on
Like
signals
in
always
statements
signals
in
initial
statements
must
be
declared
to
be
reg
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
testbench
is
no
inputs
or
outputs
end
architecture
sim
of
testbench
is
component
sillyfunction
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
component
signal
a
b
c
y
STD
LOGIC
begin
instantiate
device
under
test
dut
sillyfunction
port
map
a
b
c
y
apply
inputs
one
at
a
time
process
begin
a
b
c
wait
for
ns
c
wait
for
ns
b
c
wait
for
ns
c
wait
for
ns
a
b
c
wait
for
ns
c
wait
for
ns
b
c
wait
for
ns
c
wait
for
ns
wait
wait
forever
end
process
end
The
process
statement
first
applies
the
input
pattern
and
waits
for
ns
It
then
applies
and
waits
more
ns
and
so
forth
until
all
eight
possible
inputs
have
been
applied
At
the
end
the
process
waits
indefinitely
otherwise
the
process
would
begin
again
repeatedly
applying
the
pattern
of
test
vectors
HDL
Example
TESTBENCH
Some
tools
also
call
the
module
to
be
tested
the
unit
under
test
UUT
Chapter
qxd
PM
Testbenches
Checking
for
correct
outputs
is
tedious
and
error
prone
Moreover
determining
the
correct
outputs
is
much
easier
when
the
design
is
fresh
in
your
mind
if
you
make
minor
changes
and
need
to
retest
weeks
later
determining
the
correct
outputs
becomes
a
hassle
A
much
better
approach
is
to
write
a
self
checking
testbench
shown
in
HDL
Example
Writing
code
for
each
test
vector
also
becomes
tedious
especially
for
modules
that
require
a
large
number
of
vectors
An
even
better
approach
is
to
place
the
test
vectors
in
a
separate
file
The
testbench
simply
reads
the
test
vectors
from
the
file
applies
the
input
test
vector
to
the
DUT
waits
checks
that
the
output
values
from
the
DUT
match
the
output
vector
and
repeats
until
reaching
the
end
of
the
test
vectors
file
Verilog
module
testbench
reg
a
b
c
wire
y
instantiate
device
under
test
sillyfunction
dut
a
b
c
y
apply
inputs
one
at
a
time
checking
results
initial
begin
a
b
c
if
y
display
failed
c
if
y
display
failed
b
c
if
y
display
failed
c
if
y
display
failed
a
b
c
if
y
display
failed
c
if
y
display
failed
b
c
if
y
display
failed
c
if
y
display
failed
end
endmodule
This
module
checks
y
against
expectations
after
each
input
test
vector
is
applied
In
Verilog
comparison
using
or
is
effective
between
signals
that
do
not
take
on
the
values
of
x
and
z
Testbenches
use
the
and
operators
for
comparisons
of
equality
and
inequality
respectively
because
these
operators
work
correctly
with
operands
that
could
be
x
or
z
It
uses
the
display
system
task
to
print
a
message
on
the
simulator
console
if
an
error
occurs
display
is
meaningful
only
in
simulation
not
synthesis
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
testbench
is
no
inputs
or
outputs
end
architecture
sim
of
testbench
is
component
sillyfunction
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
component
signal
a
b
c
y
STD
LOGIC
begin
instantiate
device
under
test
dut
sillyfunction
port
map
a
b
c
y
apply
inputs
one
at
a
time
checking
results
process
begin
a
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
a
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
b
c
wait
for
ns
assert
y
report
failed
c
wait
for
ns
assert
y
report
failed
wait
wait
forever
end
process
end
The
assert
statement
checks
a
condition
and
prints
the
message
given
in
the
report
clause
if
the
condition
is
not
satisfied
assert
is
meaningful
only
in
simulation
not
in
synthesis
HDL
Example
SELF
CHECKING
TESTBENCH
Chapter
qxd
PM
Page
HDL
Example
demonstrates
such
a
testbench
The
testbench
generates
a
clock
using
an
always
process
statement
with
no
stimulus
list
so
that
it
is
continuously
reevaluated
At
the
beginning
of
the
simulation
it
reads
the
test
vectors
from
a
text
file
and
pulses
reset
for
two
cycles
example
tv
is
a
text
file
containing
the
inputs
and
expected
output
written
in
binary
CHAPTER
FOUR
Hardware
Description
Languages
Verilog
module
testbench
reg
clk
reset
reg
a
b
c
yexpected
wire
y
reg
vectornum
errors
reg
testvectors
instantiate
device
under
test
sillyfunction
dut
a
b
c
y
generate
clock
always
begin
clk
clk
end
at
start
of
test
load
vectors
and
pulse
reset
initial
begin
readmemb
example
tv
testvectors
vectornum
errors
reset
reset
end
apply
test
vectors
on
rising
edge
of
clk
always
posedge
clk
begin
a
b
c
yexpected
testvectors
vectornum
end
check
results
on
falling
edge
of
clk
always
negedge
clk
if
reset
begin
skip
during
reset
if
y
yexpected
begin
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
STD
TEXTIO
all
entity
testbench
is
no
inputs
or
outputs
end
architecture
sim
of
testbench
is
component
sillyfunction
port
a
b
c
in
STD
LOGIC
y
out
STD
LOGIC
end
component
signal
a
b
c
y
STD
LOGIC
signal
clk
reset
STD
LOGIC
signal
yexpected
STD
LOGIC
constant
MEMSIZE
integer
type
tvarray
is
array
MEMSIZE
downto
of
STD
LOGIC
VECTOR
downto
signal
testvectors
tvarray
shared
variable
vectornum
errors
integer
begin
instantiate
device
under
test
dut
sillyfunction
port
map
a
b
c
y
generate
clock
process
begin
clk
wait
for
ns
clk
wait
for
ns
end
process
at
start
of
test
load
vectors
and
pulse
reset
process
is
file
tv
TEXT
variable
i
j
integer
variable
L
line
variable
ch
character
HDL
Example
TESTBENCH
WITH
TEST
VECTOR
FILE
Chapter
q
Testbenches
display
Error
inputs
b
a
b
c
display
outputs
b
b
expected
y
yexpected
errors
errors
end
vectornum
vectornum
if
testvectors
vectornum
bx
begin
display
d
tests
completed
with
d
errors
vectornum
errors
finish
end
end
endmodule
readmemb
reads
a
file
of
binary
numbers
into
the
testvectors
array
readmemh
is
similar
but
reads
a
file
of
hexadecimal
numbers
The
next
block
of
code
waits
one
time
unit
after
the
rising
edge
of
the
clock
to
avoid
any
confusion
if
clock
and
data
change
simultaneously
then
sets
the
three
inputs
and
the
expected
output
based
on
the
four
bits
in
the
current
test
vector
The
next
block
of
code
checks
the
output
of
the
DUT
at
the
negative
edge
of
the
clock
after
the
inputs
have
had
time
to
propagate
through
the
DUT
to
produce
the
output
y
The
testbench
compares
the
generated
output
y
with
the
expected
output
yexpected
and
prints
an
error
if
they
don
t
match
b
and
d
indicate
to
print
the
values
in
binary
and
decimal
respectively
For
example
display
b
b
y
yexpected
prints
the
two
values
y
and
yexpected
in
binary
h
prints
a
value
in
hexadecimal
This
process
repeats
until
there
are
no
more
valid
test
vectors
in
the
testvectors
array
finish
terminates
the
simulation
Note
that
even
though
the
Verilog
module
supports
up
to
test
vectors
it
will
terminate
the
simulation
after
executing
the
eight
vectors
in
the
file
begin
read
file
of
test
vectors
i
FILE
OPEN
tv
example
tv
READ
MODE
while
not
endfile
tv
loop
readline
tv
L
for
j
in
to
loop
read
L
ch
if
ch
then
read
L
ch
end
if
if
ch
then
testvectors
i
j
else
testvectors
i
j
end
if
end
loop
i
i
end
loop
vectornum
errors
reset
wait
for
ns
reset
wait
end
process
apply
test
vectors
on
rising
edge
of
clk
process
clk
begin
if
clk
event
and
clk
then
a
testvectors
vectornum
after
ns
b
testvectors
vectornum
after
ns
c
testvectors
vectornum
after
ns
yexpected
testvectors
vectornum
after
ns
end
if
end
process
check
results
on
falling
edge
of
clk
process
clk
begin
if
clk
event
and
clk
and
reset
then
assert
y
yexpected
report
Error
y
STD
LOGIC
image
y
if
y
yexpected
then
errors
errors
end
if
vectornum
vectornum
if
is
x
testvectors
vectornum
then
if
errors
then
report
Just
kidding
integer
image
vectornum
tests
completed
successfully
severity
failure
else
report
integer
image
vectornum
tests
completed
errors
integer
image
errors
severity
failure
end
if
end
if
end
if
end
process
end
The
VHDL
code
is
rather
ungainly
and
uses
file
reading
commands
beyond
the
scope
of
this
chapter
but
it
gives
the
sense
of
what
a
self
checking
testbench
looks
like
Chapter
qxd
PM
Page
CHAPTER
FOUR
Hardware
Description
Languages
New
inputs
are
applied
on
the
rising
edge
of
the
clock
and
the
output
is
checked
on
the
falling
edge
of
the
clock
This
clock
and
reset
would
also
be
provided
to
the
DUT
if
sequential
logic
were
being
tested
Errors
are
reported
as
they
occur
At
the
end
of
the
simulation
the
testbench
prints
the
total
number
of
test
vectors
applied
and
the
number
of
errors
detected
The
testbench
in
HDL
Example
is
overkill
for
such
a
simple
circuit
However
it
can
easily
be
modified
to
test
more
complex
circuits
by
changing
the
example
tv
file
instantiating
the
new
DUT
and
changing
a
few
lines
of
code
to
set
the
inputs
and
check
the
outputs
SUMMARY
Hardware
description
languages
HDLs
are
extremely
important
tools
for
modern
digital
designers
Once
you
have
learned
Verilog
or
VHDL
you
will
be
able
to
specify
digital
systems
much
faster
than
if
you
had
to
draw
the
complete
schematics
The
debug
cycle
is
also
often
much
faster
because
modifications
require
code
changes
instead
of
tedious
schematic
rewiring
However
the
debug
cycle
can
be
much
longer
using
HDLs
if
you
don
t
have
a
good
idea
of
the
hardware
your
code
implies
HDLs
are
used
for
both
simulation
and
synthesis
Logic
simulation
is
a
powerful
way
to
test
a
system
on
a
computer
before
it
is
turned
into
hardware
Simulators
let
you
check
the
values
of
signals
inside
your
system
that
might
be
impossible
to
measure
on
a
physical
piece
of
hardware
Logic
synthesis
converts
the
HDL
code
into
digital
logic
circuits
The
most
important
thing
to
remember
when
you
are
writing
HDL
code
is
that
you
are
describing
real
hardware
not
writing
a
computer
program
The
most
common
beginner
s
mistake
is
to
write
HDL
code
without
thinking
about
the
hardware
you
intend
to
produce
If
you
don
t
know
what
hardware
you
are
implying
you
are
almost
certain
not
to
get
what
you
want
Instead
begin
by
sketching
a
block
diagram
of
your
system
identifying
which
portions
are
combinational
logic
which
portions
are
sequential
circuits
or
finite
state
machines
and
so
forth
Then
write
HDL
code
for
each
portion
using
the
correct
idioms
to
imply
the
kind
of
hardware
you
need
Exercises
Exercises
The
following
exercises
may
be
done
using
your
favorite
HDL
If
you
have
a
simulator
available
test
your
design
Print
the
waveforms
and
explain
how
they
prove
that
it
works
If
you
have
a
synthesizer
available
synthesize
your
code
Print
the
generated
circuit
diagram
and
explain
why
it
matches
your
expectations
Exercise
Sketch
a
schematic
of
the
circuit
described
by
the
following
HDL
code
Simplify
the
schematic
so
that
it
shows
a
minimum
number
of
gates
Exercise
Sketch
a
schematic
of
the
circuit
described
by
the
following
HDL
code
Simplify
the
schematic
so
that
it
shows
a
minimum
number
of
gates
Exercise
Write
an
HDL
module
that
computes
a
four
input
XOR
function
The
input
is
a
and
the
output
is
y
Verilog
module
exercise
input
a
b
c
output
y
z
assign
y
a
b
c
a
b
c
a
b
c
assign
z
a
b
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
exercise
is
port
a
b
c
in
STD
LOGIC
y
z
out
STD
LOGIC
end
architecture
synth
of
exercisel
is
begin
y
a
and
b
and
c
or
a
and
b
and
not
c
or
a
and
not
b
and
c
z
a
and
b
or
not
a
and
not
b
end
Verilog
module
exercise
input
a
output
reg
y
always
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
y
a
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
exercise
is
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
exercise
is
begin
process
a
begin
if
a
then
y
elsif
a
then
y
elsif
a
then
y
elsif
a
then
y
else
y
a
downto
end
if
end
process
end
Chapter
qxd
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Write
a
self
checking
testbench
for
Exercise
Create
a
test
vector
file
containing
all
test
cases
Simulate
the
circuit
and
show
that
it
works
Introduce
an
error
in
the
test
vector
file
and
show
that
the
testbench
reports
a
mismatch
Exercise
Write
an
HDL
module
called
minority
It
receives
three
inputs
a
b
and
c
It
produces
one
output
y
that
is
TRUE
if
at
least
two
of
the
inputs
are
FALSE
Exercise
Write
an
HDL
module
for
a
hexadecimal
seven
segment
display
decoder
The
decoder
should
handle
the
digits
A
B
C
D
E
and
F
as
well
as
Exercise
Write
a
self
checking
testbench
for
Exercise
Create
a
test
vector
file
containing
all
test
cases
Simulate
the
circuit
and
show
that
it
works
Introduce
an
error
in
the
test
vector
file
and
show
that
the
testbench
reports
a
mismatch
Exercise
Write
an
multiplexer
module
called
mux
with
inputs
s
d
d
d
d
d
d
d
d
and
output
y
Exercise
Write
a
structural
module
to
compute
the
logic
function
using
multiplexer
logic
Use
the
multiplexer
from
Exercise
Exercise
Repeat
Exercise
using
a
multiplexer
and
as
many
NOT
gates
as
you
need
Exercise
Section
pointed
out
that
a
synchronizer
could
be
correctly
described
with
blocking
assignments
if
the
assignments
were
given
in
the
proper
order
Think
of
a
simple
sequential
circuit
that
cannot
be
correctly
described
with
blocking
assignments
regardless
of
order
Exercise
Write
an
HDL
module
for
an
eight
input
priority
circuit
Exercise
Write
an
HDL
module
for
a
decoder
Exercise
Write
an
HDL
module
for
a
decoder
using
three
instances
of
the
decoders
from
Exercise
and
a
bunch
of
three
input
AND
gates
Exercise
Write
HDL
modules
that
implement
the
Boolean
equations
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
circuit
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
logic
function
from
Exercise
Pay
careful
attention
to
how
you
handle
don
t
cares
y
abb
cabc
Cha
Exercises
Exercise
Write
an
HDL
module
that
implements
the
functions
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
priority
encoder
from
Exercise
Exercise
Write
an
HDL
module
that
implements
the
binary
to
thermometer
code
converter
from
Exercise
Exercise
Write
an
HDL
module
implementing
the
days
in
month
function
from
Question
Exercise
Sketch
the
state
transition
diagram
for
the
FSM
described
by
the
following
HDL
code
Verilog
module
fsm
input
clk
reset
input
a
b
output
y
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
always
case
state
S
if
a
b
nextstate
S
else
nextstate
S
S
if
a
b
nextstate
S
else
nextstate
S
S
if
a
b
nextstate
S
else
nextstate
S
S
if
a
b
nextstate
S
else
nextstate
S
endcase
assign
y
state
S
state
S
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fsm
is
port
clk
reset
in
STD
LOGIC
a
b
in
STD
LOGIC
y
out
STD
LOGIC
end
architecture
synth
of
fsm
is
type
statetype
is
S
S
S
S
signal
state
nextstate
statetype
begin
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
process
state
a
b
begin
case
state
is
when
S
if
a
xor
b
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
and
b
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
or
b
then
nextstate
S
else
nextstate
S
end
if
when
S
if
a
or
b
then
nextstate
S
else
nextstate
S
end
if
end
case
end
process
y
when
state
S
or
state
S
else
end
Chapter
qxd
PM
Page
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Sketch
the
state
transition
diagram
for
the
FSM
described
by
the
following
HDL
code
An
FSM
of
this
nature
is
used
in
a
branch
predictor
on
some
microprocessors
Verilog
module
fsm
input
clk
reset
input
taken
back
output
predicttaken
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
parameter
S
b
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
always
case
state
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
S
if
taken
nextstate
S
else
nextstate
S
default
nextstate
S
endcase
assign
predicttaken
state
S
state
S
state
S
back
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
fsm
is
port
clk
reset
in
STD
LOGIC
taken
back
in
STD
LOGIC
predicttaken
out
STD
LOGIC
end
architecture
synth
of
fsm
is
type
statetype
is
S
S
S
S
S
signal
state
nextstate
statetype
begin
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
process
state
taken
begin
case
state
is
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
S
if
taken
then
nextstate
S
else
nextstate
S
end
if
when
others
nextstate
S
end
case
end
process
output
logic
predicttaken
when
state
S
or
state
S
or
state
S
and
back
else
end
Chapter
qxd
PM
Page
Exercises
Exercise
Write
an
HDL
module
for
an
SR
latch
Exercise
Write
an
HDL
module
for
a
JK
flip
flop
The
flip
flop
has
inputs
clk
J
and
K
and
output
Q
On
the
rising
edge
of
the
clock
Q
keeps
its
old
value
if
J
K
It
sets
Q
to
if
J
resets
Q
to
if
K
and
inverts
Q
if
J
K
Exercise
Write
an
HDL
module
for
the
latch
from
Figure
Use
one
assignment
statement
for
each
gate
Specify
delays
of
unit
or
ns
to
each
gate
Simulate
the
latch
and
show
that
it
operates
correctly
Then
increase
the
inverter
delay
How
long
does
the
delay
have
to
be
before
a
race
condition
causes
the
latch
to
malfunction
Exercise
Write
an
HDL
module
for
the
traffic
light
controller
from
Section
Exercise
Write
three
HDL
modules
for
the
factored
parade
mode
traffic
light
controller
from
Example
The
modules
should
be
called
controller
mode
and
lights
and
they
should
have
the
inputs
and
outputs
shown
in
Figure
b
Exercise
Write
an
HDL
module
describing
the
circuit
in
Figure
Exercise
Write
an
HDL
module
for
the
FSM
with
the
state
transition
diagram
given
in
Figure
from
Exercise
Exercise
Write
an
HDL
module
for
the
FSM
with
the
state
transition
diagram
given
in
Figure
from
Exercise
Exercise
Write
an
HDL
module
for
the
improved
traffic
light
controller
from
Exercise
Exercise
Write
an
HDL
module
for
the
daughter
snail
from
Exercise
Exercise
Write
an
HDL
module
for
the
soda
machine
dispenser
from
Exercise
Exercise
Write
an
HDL
module
for
the
Gray
code
counter
from
Exercise
Exercise
Write
an
HDL
module
for
the
UP
DOWN
Gray
code
counter
from
Exercise
Exercise
Write
an
HDL
module
for
the
FSM
from
Exercise
Chapte
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Write
an
HDL
module
for
the
FSM
from
Exercise
Exercise
Write
an
HDL
module
for
the
serial
two
s
complementer
from
Question
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
Exercise
Write
an
HDL
module
for
the
circuit
in
Exercise
You
may
use
the
full
adder
from
Section
Verilog
Exercises
The
following
exercises
are
specific
to
Verilog
Exercise
What
does
it
mean
for
a
signal
to
be
declared
reg
in
Verilog
Exercise
Rewrite
the
syncbad
module
from
HDL
Example
Use
nonblocking
assignments
but
change
the
code
to
produce
a
correct
synchronizer
with
two
flip
flops
Exercise
Consider
the
following
two
Verilog
modules
Do
they
have
the
same
function
Sketch
the
hardware
each
one
implies
module
code
input
clk
a
b
c
output
reg
y
reg
x
always
posedge
clk
begin
x
a
b
y
x
c
end
endmodule
module
code
input
a
b
c
clk
output
reg
y
reg
x
always
posedge
clk
begin
y
x
c
x
a
b
end
endmodule
Exercise
Repeat
Exercise
if
the
is
replaced
by
in
every
assignment
Chapte
Exercises
Exercise
The
following
Verilog
modules
show
errors
that
the
authors
have
seen
students
make
in
the
laboratory
Explain
the
error
in
each
module
and
show
how
to
fix
it
a
module
latch
input
clk
input
d
output
reg
q
always
clk
if
clk
q
d
endmodule
b
module
gates
input
a
b
output
reg
y
y
y
y
y
always
a
begin
y
a
b
y
a
b
y
a
b
y
a
b
y
a
b
end
endmodule
c
module
mux
input
d
d
input
s
output
reg
y
always
posedge
s
if
s
y
d
else
y
d
endmodule
d
module
twoflops
input
clk
input
d
d
output
reg
q
q
always
posedge
clk
q
d
q
d
endmodule
Chapter
CHAPTER
FOUR
Hardware
Description
Languages
e
module
FSM
input
clk
input
a
output
reg
out
out
reg
state
next
state
logic
and
register
sequential
always
posedge
clk
if
state
begin
if
a
state
end
else
begin
if
a
state
end
always
output
logic
combinational
if
state
out
else
out
endmodule
f
module
priority
input
a
output
reg
y
always
if
a
y
b
else
if
a
y
b
else
if
a
y
b
else
if
a
y
b
endmodule
g
module
divideby
FSM
input
clk
input
reset
output
out
reg
state
nextstate
parameter
S
b
parameter
S
b
parameter
S
b
State
Register
always
posedge
clk
posedge
reset
if
reset
state
S
else
state
nextstate
Next
State
Logic
always
state
case
state
S
nextstate
S
S
nextstate
S
nextstate
S
endcase
Output
Logic
assign
out
state
S
endmodule
Chapter
qxd
Exercises
h
module
mux
tri
input
d
d
input
s
output
y
tristate
t
d
s
y
tristate
t
d
s
y
endmodule
i
module
floprsen
input
clk
input
reset
input
set
input
d
output
reg
q
always
posedge
clk
posedge
reset
if
reset
q
else
q
d
always
set
if
set
q
endmodule
j
module
and
input
a
b
c
output
reg
y
reg
tmp
always
a
b
c
begin
tmp
a
b
y
tmp
c
end
endmodule
VHDL
Exercises
The
following
exercises
are
specific
to
VHDL
Exercise
In
VHDL
why
is
it
necessary
to
write
q
when
state
S
else
rather
than
simply
q
state
S
Chapter
CHAPTER
FOUR
Hardware
Description
Languages
Exercise
Each
of
the
following
VHDL
modules
contains
an
error
For
brevity
only
the
architecture
is
shown
assume
that
the
library
use
clause
and
entity
declaration
are
correct
Explain
the
error
and
show
how
to
fix
it
a
architecture
synth
of
latch
is
begin
process
clk
begin
if
clk
then
q
d
end
if
end
process
end
b
architecture
proc
of
gates
is
begin
process
a
begin
y
a
and
b
y
a
or
b
y
a
xor
b
y
a
nand
b
y
a
nor
b
end
process
end
c
architecture
synth
of
flop
is
begin
process
clk
if
clk
event
and
clk
then
q
d
end
d
architecture
synth
of
priority
is
begin
process
a
begin
if
a
then
y
elsif
a
then
y
elsif
a
then
y
elsif
a
then
y
end
if
end
process
end
e
architecture
synth
of
divideby
FSM
is
type
statetype
is
S
S
S
signal
state
nextstate
statetype
begin
process
clk
reset
begin
if
reset
then
state
S
elsif
clk
event
and
clk
then
state
nextstate
end
if
end
process
Chapter
qxd
Exercises
process
state
begin
case
state
is
when
S
nextstate
S
when
S
nextstate
S
when
S
nextstate
S
end
case
end
process
q
when
state
S
else
end
f
architecture
struct
of
mux
is
component
tristate
port
a
in
STD
LOGIC
VECTOR
downto
en
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
downto
end
component
begin
t
tristate
port
map
d
s
y
t
tristate
port
map
d
s
y
end
g
architecture
asynchronous
of
flopr
is
begin
process
clk
reset
begin
if
reset
then
q
elsif
clk
event
and
clk
then
q
d
end
if
end
process
process
set
begin
if
set
then
q
end
if
end
process
end
h
architecture
synth
of
mux
is
begin
y
d
when
s
else
d
when
s
else
d
end
Chapter
qxd
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Write
a
line
of
HDL
code
that
gates
a
bit
bus
called
data
with
another
signal
called
sel
to
produce
a
bit
result
If
sel
is
TRUE
result
data
Otherwise
result
should
be
all
s
Question
Explain
the
difference
between
blocking
and
nonblocking
assignments
in
Verilog
Give
examples
Question
What
does
the
following
Verilog
statement
do
result
data
hC
CHAPTER
FOUR
Hardware
Description
Languages
Ch
Introduction
Arithmetic
Circuits
Number
Systems
Sequential
Building
Blocks
Memory
Arrays
Logic
Arrays
Summary
Exercises
Interview
Questions
Digital
Building
Blocks
INTRODUCTION
Up
to
this
point
we
have
examined
the
design
of
combinational
and
sequential
circuits
using
Boolean
equations
schematics
and
HDLs
This
chapter
introduces
more
elaborate
combinational
and
sequential
building
blocks
used
in
digital
systems
These
blocks
include
arithmetic
circuits
counters
shift
registers
memory
arrays
and
logic
arrays
These
building
blocks
are
not
only
useful
in
their
own
right
but
they
also
demonstrate
the
principles
of
hierarchy
modularity
and
regularity
The
building
blocks
are
hierarchically
assembled
from
simpler
components
such
as
logic
gates
multiplexers
and
decoders
Each
building
block
has
a
well
defined
interface
and
can
be
treated
as
a
black
box
when
the
underlying
implementation
is
unimportant
The
regular
structure
of
each
building
block
is
easily
extended
to
different
sizes
In
Chapter
we
use
many
of
these
building
blocks
to
build
a
microprocessor
ARITHMETIC
CIRCUITS
Arithmetic
circuits
are
the
central
building
blocks
of
computers
Computers
and
digital
logic
perform
many
arithmetic
functions
addition
subtraction
comparisons
shifts
multiplication
and
division
This
section
describes
hardware
implementations
for
all
of
these
operations
Addition
Addition
is
one
of
the
most
common
operations
in
digital
systems
We
first
consider
how
to
add
two
bit
binary
numbers
We
then
extend
to
N
bit
binary
numbers
Adders
also
illustrate
trade
offs
between
speed
and
complexity
Half
Adder
We
begin
by
building
a
bit
half
adder
As
shown
in
Figure
the
half
adder
has
two
inputs
A
and
B
and
two
outputs
S
and
Cout
S
is
the
A
B
out
SC
S
A
B
Cout
AB
Half
Adder
A
B
S
Cout
FIGURE
bit
half
adder
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Carry
bit
A
B
out
SC
S
A
B
Cin
Cout
AB
ACin
BCin
Full
Adder
Cin
A
B
S
Cout
Cin
FIGURE
bit
full
adder
A
B
S
Cout
Cin
N
N
N
Figure
Carry
propagate
adder
sum
of
A
and
B
If
A
and
B
are
both
S
is
which
cannot
be
represented
with
a
single
binary
digit
Instead
it
is
indicated
with
a
carry
out
Cout
in
the
next
column
The
half
adder
can
be
built
from
an
XOR
gate
and
an
AND
gate
In
a
multi
bit
adder
Cout
is
added
or
carried
in
to
the
next
most
significant
bit
For
example
in
Figure
the
carry
bit
shown
in
blue
is
the
output
Cout
of
the
first
column
of
bit
addition
and
the
input
Cin
to
the
second
column
of
addition
However
the
half
adder
lacks
a
Cin
input
to
accept
Cout
of
the
previous
column
The
full
adder
described
in
the
next
section
solves
this
problem
Full
Adder
A
full
adder
introduced
in
Section
accepts
the
carry
in
Cin
as
shown
in
Figure
The
figure
also
shows
the
output
equations
for
S
and
Cout
Carry
Propagate
Adder
An
N
bit
adder
sums
two
N
bit
inputs
A
and
B
and
a
carry
in
Cin
to
produce
an
N
bit
result
S
and
a
carry
out
Cout
It
is
commonly
called
a
carry
propagate
adder
CPA
because
the
carry
out
of
one
bit
propagates
into
the
next
bit
The
symbol
for
a
CPA
is
shown
in
Figure
it
is
drawn
just
like
a
full
adder
except
that
A
B
and
S
are
busses
rather
than
single
bits
Three
common
CPA
implementations
are
called
ripplecarry
adders
carry
lookahead
adders
and
prefix
adders
Ripple
Carry
Adder
The
simplest
way
to
build
an
N
bit
carry
propagate
adder
is
to
chain
together
N
full
adders
The
Cout
of
one
stage
acts
as
the
Cin
of
the
next
stage
as
shown
in
Figure
for
bit
addition
This
is
called
a
ripplecarry
adder
It
is
a
good
application
of
modularity
and
regularity
the
full
adder
module
is
reused
many
times
to
form
a
larger
system
The
ripplecarry
adder
has
the
disadvantage
of
being
slow
when
N
is
large
S
depends
on
C
which
depends
on
C
which
depends
on
C
and
so
forth
all
the
way
back
to
Cin
as
shown
in
blue
in
Figure
We
say
that
the
carry
ripples
through
the
carry
chain
The
delay
of
the
adder
tripple
grows
directly
with
the
number
of
bits
as
given
in
Equation
where
tFA
is
the
delay
of
a
full
adder
t
ripple
NtFA
S
A
B
S
A
B
S
A
B
S
C
C
C
C
Cout
A
B
Cin
Figure
bit
ripple
carry
adder
Schematics
typically
show
signals
flowing
from
left
to
right
Arithmetic
circuits
break
this
rule
because
the
carries
flow
from
right
to
left
from
the
least
significant
column
to
the
most
significant
column
C
Carry
Lookahead
Adder
The
fundamental
reason
that
large
ripple
carry
adders
are
slow
is
that
the
carry
signals
must
propagate
through
every
bit
in
the
adder
A
carrylookahead
adder
is
another
type
of
carry
propagate
adder
that
solves
this
problem
by
dividing
the
adder
into
blocks
and
providing
circuitry
to
quickly
determine
the
carry
out
of
a
block
as
soon
as
the
carry
in
is
known
Thus
it
is
said
to
look
ahead
across
the
blocks
rather
than
waiting
to
ripple
through
all
the
full
adders
inside
a
block
For
example
a
bit
adder
may
be
divided
into
eight
bit
blocks
Carry
lookahead
adders
use
generate
G
and
propagate
P
signals
that
describe
how
a
column
or
block
determines
the
carry
out
The
ith
column
of
an
adder
is
said
to
generate
a
carry
if
it
produces
a
carry
out
independent
of
the
carry
in
The
ith
column
of
an
adder
is
guaranteed
to
generate
a
carry
Ci
if
Ai
and
Bi
are
both
Hence
Gi
the
generate
signal
for
column
i
is
calculated
as
Gi
AiBi
The
column
is
said
to
propagate
a
carry
if
it
produces
a
carry
out
whenever
there
is
a
carry
in
The
ith
column
will
propagate
a
carry
in
Ci
if
either
Ai
or
Bi
is
Thus
Pi
Ai
Bi
Using
these
definitions
we
can
rewrite
the
carry
logic
for
a
particular
column
of
the
adder
The
ith
column
of
an
adder
will
generate
a
carryout
Ci
if
it
either
generates
a
carry
Gi
or
propagates
a
carry
in
Pi
Ci
In
equation
form
The
generate
and
propagate
definitions
extend
to
multiple
bit
blocks
A
block
is
said
to
generate
a
carry
if
it
produces
a
carry
out
independent
of
the
carry
in
to
the
block
The
block
is
said
to
propagate
a
carry
if
it
produces
a
carry
out
whenever
there
is
a
carry
in
to
the
block
We
define
Gi
j
and
Pi
j
as
generate
and
propagate
signals
for
blocks
spanning
columns
i
through
j
A
block
generates
a
carry
if
the
most
significant
column
generates
a
carry
or
if
the
most
significant
column
propagates
a
carry
and
the
previous
column
generated
a
carry
and
so
forth
For
example
the
generate
logic
for
a
block
spanning
columns
through
is
A
block
propagates
a
carry
if
all
the
columns
in
the
block
propagate
the
carry
For
example
the
propagate
logic
for
a
block
spanning
columns
through
is
Using
the
block
generate
and
propagate
signals
we
can
quickly
compute
the
carry
out
of
the
block
Ci
using
the
carry
in
to
the
block
Cj
C
i
Gi
j
Pi
j
Cj
P
P
P
P
P
G
G
P
G
P
G
P
G
Ci
Ai
Bi
Ai
Bi
Ci
Gi
Pi
Ci
Arithmetic
Circuits
Throughout
the
ages
people
have
used
many
devices
to
perform
arithmetic
Toddlers
count
on
their
fingers
and
some
adults
stealthily
do
too
The
Chinese
and
Babylonians
invented
the
abacus
as
early
as
BC
Slide
rules
invented
in
were
in
use
until
the
s
when
scientific
hand
calculators
became
prevalent
Computers
and
digital
calculators
are
ubiquitous
today
What
will
be
next
Chapter
Figure
a
shows
a
bit
carry
lookahead
adder
composed
of
eight
bit
blocks
Each
block
contains
a
bit
ripple
carry
adder
and
some
lookahead
logic
to
compute
the
carry
out
of
the
block
given
the
carry
in
as
shown
in
Figure
b
The
AND
and
OR
gates
needed
to
compute
the
single
bit
generate
and
propagate
signals
Gi
and
Pi
from
Ai
and
Bi
are
left
out
for
brevity
Again
the
carry
lookahead
adder
demonstrates
modularity
and
regularity
All
of
the
CLA
blocks
compute
the
single
bit
and
block
generate
and
propagate
signals
simultaneously
The
critical
path
starts
with
computing
G
and
G
in
the
first
CLA
block
Cin
then
advances
directly
to
Cout
through
the
AND
OR
gate
in
each
block
until
the
last
For
a
large
adder
this
is
much
faster
than
waiting
for
the
carries
to
ripple
through
each
consecutive
bit
of
the
adder
Finally
the
critical
path
through
the
last
block
contains
a
short
ripple
carry
adder
Thus
an
N
bit
adder
divided
into
k
bit
blocks
has
a
delay
tCLA
tpg
tpg
block
N
k
tAND
OR
ktFA
CHAPTER
FIVE
Digital
Building
Blocks
B
P
G
P
G
P
G
P
G
P
P
P
P
G
Cin
Cout
A
S
C
B
A
S
C
B
A
S
C
B
A
S
Cin
b
a
B
A
S
Cin
B
A
S
C
C
B
A
S
C
B
A
S
bit
CLA
Block
bit
CLA
Block
bit
CLA
Block
bit
CLA
Block
C
Cout
Figure
a
bit
carrylookahead
adder
CLA
b
bit
CLA
block
Chapte
where
tpg
is
the
delay
of
the
individual
generate
propagate
gates
a
single
AND
or
OR
gate
to
generate
P
and
G
tpg
block
is
the
delay
to
find
the
generate
propagate
signals
Pi
j
and
Gi
j
for
a
k
bit
block
and
tAND
OR
is
the
delay
from
Cin
to
Cout
through
the
AND
OR
logic
of
the
k
bit
CLA
block
For
N
the
carry
lookahead
adder
is
generally
much
faster
than
the
ripple
carry
adder
However
the
adder
delay
still
increases
linearly
with
N
Example
RIPPLE
CARRY
ADDER
AND
CARRY
LOOKAHEAD
ADDER
DELAY
Compare
the
delays
of
a
bit
ripple
carry
adder
and
a
bit
carry
lookahead
adder
with
bit
blocks
Assume
that
each
two
input
gate
delay
is
ps
and
that
a
full
adder
delay
is
ps
Solution
According
to
Equation
the
propagation
delay
of
the
bit
ripplecarry
adder
is
ps
ns
The
CLA
has
tpg
ps
tpg
block
ps
ps
and
tAND
OR
ps
ps
According
to
Equation
the
propagation
delay
of
the
bit
carry
lookahead
adder
with
bit
blocks
is
thus
ps
ps
ps
ps
ns
almost
three
times
faster
than
the
ripple
carry
adder
Prefix
Adder
Prefix
adders
extend
the
generate
and
propagate
logic
of
the
carrylookahead
adder
to
perform
addition
even
faster
They
first
compute
G
and
P
for
pairs
of
columns
then
for
blocks
of
then
for
blocks
of
then
and
so
forth
until
the
generate
signal
for
every
column
is
known
The
sums
are
computed
from
these
generate
signals
In
other
words
the
strategy
of
a
prefix
adder
is
to
compute
the
carry
in
Ci
for
each
column
i
as
quickly
as
possible
then
to
compute
the
sum
using
Si
Ai
Bi
Ci
Define
column
i
to
hold
Cin
so
G
Cin
and
P
Then
Ci
Gi
because
there
will
be
a
carry
out
of
column
i
if
the
block
spanning
columns
i
through
generates
a
carry
The
generated
carry
is
either
generated
in
column
i
or
generated
in
a
previous
column
and
propagated
Thus
we
rewrite
Equation
as
Si
Ai
Bi
Gi
Hence
the
main
challenge
is
to
rapidly
compute
all
the
block
generate
signals
G
G
G
G
GN
These
signals
along
with
P
P
P
P
PN
are
called
prefixes
Arithmetic
Circuits
Early
computers
used
ripple
carry
adders
because
components
were
expensive
and
ripple
carry
adders
used
the
least
hardware
Virtually
all
modern
PCs
use
prefix
adders
on
critical
paths
because
transistors
are
now
cheap
and
speed
is
of
great
importance
Chapter
qxd
AM
Page
Figure
shows
an
N
bit
prefix
adder
The
adder
begins
with
a
precomputation
to
form
Pi
and
Gi
for
each
column
from
Ai
and
Bi
using
AND
and
OR
gates
It
then
uses
log
N
levels
of
black
cells
to
form
the
prefixes
of
Gi
j
and
Pi
j
A
black
cell
takes
inputs
from
the
upper
part
of
a
block
spanning
bits
i
k
and
from
the
lower
part
spanning
bits
k
j
It
combines
these
parts
to
form
generate
and
propagate
signals
for
the
entire
block
spanning
bits
i
j
using
the
equations
In
other
words
a
block
spanning
bits
i
j
will
generate
a
carry
if
the
upper
part
generates
a
carry
or
if
the
upper
part
propagates
a
carry
generated
in
the
lower
part
The
block
will
propagate
a
carry
if
both
the
Pi
j
Pi
k
Pk
j
Gi
j
Gi
k
Pi
k
Gk
j
CHAPTER
FIVE
Digital
Building
Blocks
Ai
Bi
Pi
i
Gi
i
Pi
kPk
j
Gi
k
Gk
j
Pi
j
Gi
j
Gi
Ai
Bi
Si
i
i
j
Legend
i
Figure
bit
prefix
adder
Chapter
upper
and
lower
parts
propagate
the
carry
Finally
the
prefix
adder
computes
the
sums
using
Equation
In
summary
the
prefix
adder
achieves
a
delay
that
grows
logarithmically
rather
than
linearly
with
the
number
of
columns
in
the
adder
This
speedup
is
significant
especially
for
adders
with
or
more
bits
but
it
comes
at
the
expense
of
more
hardware
than
a
simple
carrylookahead
adder
The
network
of
black
cells
is
called
a
prefix
tree
The
general
principle
of
using
prefix
trees
to
perform
computations
in
time
that
grows
logarithmically
with
the
number
of
inputs
is
a
powerful
technique
With
some
cleverness
it
can
be
applied
to
many
other
types
of
circuits
see
for
example
Exercise
The
critical
path
for
an
N
bit
prefix
adder
involves
the
precomputation
of
Pi
and
Gi
followed
by
log
N
stages
of
black
prefix
cells
to
obtain
all
the
prefixes
Gi
then
proceeds
through
the
final
XOR
gate
at
the
bottom
to
compute
Si
Mathematically
the
delay
of
an
N
bit
prefix
adder
is
where
tpg
prefix
is
the
delay
of
a
black
prefix
cell
Example
PREFIX
ADDER
DELAY
Compute
the
delay
of
a
bit
prefix
adder
Assume
that
each
two
input
gate
delay
is
ps
Solution
The
propagation
delay
of
each
black
prefix
cell
tpg
prefix
is
ps
i
e
two
gate
delays
Thus
using
Equation
the
propagation
delay
of
the
bit
prefix
adder
is
ps
log
ps
ps
ns
which
is
about
three
times
faster
than
the
carry
lookahead
adder
and
eight
times
faster
than
the
ripple
carry
adder
from
Example
In
practice
the
benefits
are
not
quite
this
great
but
prefix
adders
are
still
substantially
faster
than
the
alternatives
Putting
It
All
Together
This
section
introduced
the
half
adder
full
adder
and
three
types
of
carry
propagate
adders
ripple
carry
carry
lookahead
and
prefix
adders
Faster
adders
require
more
hardware
and
therefore
are
more
expensive
and
power
hungry
These
trade
offs
must
be
considered
when
choosing
an
appropriate
adder
for
a
design
Hardware
description
languages
provide
the
operation
to
specify
a
CPA
Modern
synthesis
tools
select
among
many
possible
implementations
choosing
the
cheapest
smallest
design
that
meets
the
speed
requirements
This
greatly
simplifies
the
designer
s
job
HDL
Example
describes
a
CPA
with
carries
in
and
out
tPA
tpg
log
N
tpgprefix
tXOR
Arithmetic
Circuits
Chapt
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
adder
parameter
N
input
N
a
b
input
cin
output
N
s
output
cout
assign
cout
s
a
b
cin
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
adder
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
cin
in
STD
LOGIC
s
out
STD
LOGIC
VECTOR
N
downto
cout
out
STD
LOGIC
end
architecture
synth
of
adder
is
signal
result
STD
LOGIC
VECTOR
N
downto
begin
result
a
b
cin
s
result
N
downto
cout
result
N
end
HDL
Example
ADDER
Subtraction
Recall
from
Section
that
adders
can
add
positive
and
negative
numbers
using
two
s
complement
number
representation
Subtraction
is
almost
as
easy
flip
the
sign
of
the
second
number
then
add
Flipping
the
sign
of
a
two
s
complement
number
is
done
by
inverting
the
bits
and
adding
To
compute
Y
A
B
first
create
the
two
s
complement
of
B
Invert
the
bits
of
B
to
obtain
and
add
to
get
Add
this
quantity
to
A
to
get
This
sum
can
be
performed
with
a
single
CPA
by
adding
with
Cin
Figure
shows
the
symbol
for
a
subtractor
and
the
underlying
hardware
for
performing
Y
A
B
HDL
Example
describes
a
subtractor
Comparators
A
comparator
determines
whether
two
binary
numbers
are
equal
or
if
one
is
greater
or
less
than
the
other
A
comparator
receives
two
N
bit
binary
numbers
A
and
B
There
are
two
common
types
of
comparators
A
B
Y
A
B
A
B
B
B
B
A
B
Y
a
N
N
N
Y
A
B
b
N
N
N
N
Figure
Subtractor
a
symbol
b
implementation
cout
s
cin
b
a
Figure
Synthesized
adder
Chapter
qxd
Arithmetic
Circuits
Verilog
module
subtractor
parameter
N
input
N
a
b
output
N
y
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
subtractor
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
y
out
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
subtractor
is
begin
y
a
b
end
HDL
Example
SUBTRACTOR
An
equality
comparator
produces
a
single
output
indicating
whether
A
is
equal
to
B
A
B
A
magnitude
comparator
produces
one
or
more
outputs
indicating
the
relative
values
of
A
and
B
The
equality
comparator
is
the
simpler
piece
of
hardware
Figure
shows
the
symbol
and
implementation
of
a
bit
equality
comparator
It
first
checks
to
determine
whether
the
corresponding
bits
in
each
column
of
A
and
B
are
equal
using
XNOR
gates
The
numbers
are
equal
if
all
of
the
columns
are
equal
Magnitude
comparison
is
usually
done
by
computing
A
B
and
looking
at
the
sign
most
significant
bit
of
the
result
as
shown
in
Figure
If
the
result
is
negative
i
e
the
sign
bit
is
then
A
is
less
than
B
Otherwise
A
is
greater
than
or
equal
to
B
HDL
Example
shows
how
to
use
various
comparison
operations
A
B
A
B
A
B
A
B
Equal
b
Figure
bit
equality
comparator
a
symbol
b
implementation
y
b
a
Figure
Synthesized
subtractor
a
A
B
Equal
A
B
BA
N
N
N
N
Figure
N
bit
magnitude
comparator
Chapter
qx
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
comparators
parameter
N
input
N
a
b
output
eq
neq
output
lt
lte
output
gt
gte
assign
eq
a
b
assign
neq
a
b
assign
lt
a
b
assign
lte
a
b
assign
gt
a
b
assign
gte
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
entity
comparators
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
eq
neq
lt
lte
gt
gte
out
STD
LOGIC
end
architecture
synth
of
comparators
is
begin
eq
when
a
b
else
neq
when
a
b
else
lt
when
a
b
else
lte
when
a
b
else
gt
when
a
b
else
gte
when
a
b
else
end
HDL
Example
COMPARATORS
ALU
An
Arithmetic
Logical
Unit
ALU
combines
a
variety
of
mathematical
and
logical
operations
into
a
single
unit
For
example
a
typical
ALU
might
perform
addition
subtraction
magnitude
comparison
AND
and
OR
operations
The
ALU
forms
the
heart
of
most
computer
systems
Figure
shows
the
symbol
for
an
N
bit
ALU
with
N
bit
inputs
and
outputs
The
ALU
receives
a
control
signal
F
that
specifies
which
gte
gt
lte
lt
neq
eq
b
a
Figure
Synthesized
comparators
ALU
N
N
N
A
B
Y
F
Figure
ALU
symbol
Chapter
qxd
function
to
perform
Control
signals
will
generally
be
shown
in
blue
to
distinguish
them
from
the
data
Table
lists
typical
functions
that
the
ALU
can
perform
The
SLT
function
is
used
for
magnitude
comparison
and
will
be
discussed
later
in
this
section
Figure
shows
an
implementation
of
the
ALU
The
ALU
contains
an
N
bit
adder
and
N
two
input
AND
and
OR
gates
It
also
contains
an
inverter
and
a
multiplexer
to
optionally
invert
input
B
when
the
F
control
signal
is
asserted
A
multiplexer
chooses
the
desired
function
based
on
the
F
control
signals
Arithmetic
Circuits
A
B
Cout
Y
F
F
N
S
N
N
N
N
N
N
N
N
N
extend
Zero
BB
Figure
N
bit
ALU
Table
ALU
operations
F
Function
A
AND
B
A
OR
B
A
B
not
used
A
AND
A
OR
A
B
SLT
B
B
C
More
specifically
the
arithmetic
and
logical
blocks
in
the
ALU
operate
on
A
and
BB
BB
is
either
B
or
depending
on
F
If
F
the
output
multiplexer
chooses
A
AND
BB
If
F
the
ALU
computes
A
OR
BB
If
F
the
ALU
performs
addition
or
subtraction
Note
that
F
is
also
the
carry
in
to
the
adder
Also
remember
that
in
two
s
complement
arithmetic
If
F
the
ALU
computes
A
B
If
F
the
ALU
computes
When
F
the
ALU
performs
the
set
if
less
than
SLT
operation
When
A
B
Y
Otherwise
Y
In
other
words
Y
is
set
to
if
A
is
less
than
B
SLT
is
performed
by
computing
S
A
B
If
S
is
negative
i
e
the
sign
bit
is
set
A
B
The
zero
extend
unit
produces
an
N
bit
output
by
concatenating
its
bit
input
with
s
in
the
most
significant
bits
The
sign
bit
the
N
th
bit
of
S
is
the
input
to
the
zero
extend
unit
Example
SET
LESS
THAN
Configure
a
bit
ALU
for
the
SLT
operation
Suppose
A
and
B
Show
the
control
signals
and
output
Y
Solution
Because
A
B
we
expect
Y
to
be
For
SLT
F
With
F
this
configures
the
adder
unit
as
a
subtractor
with
an
output
S
of
With
F
the
final
multiplexer
sets
Y
S
Some
ALUs
produce
extra
outputs
called
flags
that
indicate
information
about
the
ALU
output
For
example
an
overflow
flag
indicates
that
the
result
of
the
adder
overflowed
A
zero
flag
indicates
that
the
ALU
output
is
The
HDL
for
an
N
bit
ALU
is
left
to
Exercise
There
are
many
variations
on
this
basic
ALU
that
support
other
functions
such
as
XOR
or
equality
comparison
Shifters
and
Rotators
Shifters
and
rotators
move
bits
and
multiply
or
divide
by
powers
of
As
the
name
implies
a
shifter
shifts
a
binary
number
left
or
right
by
a
specified
number
of
positions
There
are
several
kinds
of
commonly
used
shifters
Logical
shifter
shifts
the
number
to
the
left
LSL
or
right
LSR
and
fills
empty
spots
with
s
Ex
LSR
LSL
Arithmetic
shifter
is
the
same
as
a
logical
shifter
but
on
right
shifts
fills
the
most
significant
bits
with
a
copy
of
the
old
most
significant
bit
msb
This
is
useful
for
multiplying
and
dividing
signed
numbers
A
B
A
B
B
B
B
CHAPTER
FIVE
Digital
Building
Blocks
Chapter
qxd
A
see
Sections
and
Arithmetic
shift
left
ASL
is
the
same
as
logical
shift
left
LSL
Ex
ASR
ASL
Rotator
rotates
number
in
circle
such
that
empty
spots
are
filled
with
bits
shifted
off
the
other
end
Ex
ROR
ROL
An
N
bit
shifter
can
be
built
from
N
N
multiplexers
The
input
is
shifted
by
to
N
bits
depending
on
the
value
of
the
log
N
bit
select
lines
Figure
shows
the
symbol
and
hardware
of
bit
shifters
The
operators
and
typically
indicate
shift
left
logical
shift
right
and
arithmetic
shift
right
respectively
Depending
on
the
value
of
the
bit
shift
amount
shamt
the
output
Y
receives
the
input
A
shifted
by
to
bits
For
all
shifters
when
shamt
Y
A
Exercise
covers
rotator
designs
A
left
shift
is
a
special
case
of
multiplication
A
left
shift
by
N
bits
multiplies
the
number
by
N
For
example
is
equivalent
to
Arithmetic
Circuits
Figure
bit
shifters
a
shift
left
b
logical
shift
right
c
arithmetic
shift
right
shamt
A
A
A
A
Y
Y
Y
Y
a
S
S
S
S
b
A
A
A
A
Y
Y
Y
Y
shamt
A
Y
shamt
S
S
S
S
c
A
A
A
A
Y
Y
Y
Y
shamt
S
S
S
S
A
Y
shamt
A
Y
shamt
Chapter
An
arithmetic
right
shift
is
a
special
case
of
division
An
arithmetic
right
shift
by
N
bits
divides
the
number
by
N
For
example
is
equivalent
to
Multiplication
Multiplication
of
unsigned
binary
numbers
is
similar
to
decimal
multiplication
but
involves
only
s
and
s
Figure
compares
multiplication
in
decimal
and
binary
In
both
cases
partial
products
are
formed
by
multiplying
a
single
digit
of
the
multiplier
with
the
entire
multiplicand
The
shifted
partial
products
are
summed
to
form
the
result
In
general
an
N
N
multiplier
multiplies
two
N
bit
numbers
and
produces
a
N
bit
result
The
partial
products
in
binary
multiplication
are
either
the
multiplicand
or
all
s
Multiplication
of
bit
binary
numbers
is
equivalent
to
the
AND
operation
so
AND
gates
are
used
to
form
the
partial
products
Figure
shows
the
symbol
function
and
implementation
of
a
multiplier
The
multiplier
receives
the
multiplicand
and
multiplier
A
and
B
and
produces
the
product
P
Figure
b
shows
how
partial
products
are
formed
Each
partial
product
is
a
single
multiplier
bit
B
B
B
or
B
AND
the
multiplicand
bits
A
A
A
A
With
N
bit
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Multiplication
a
decimal
b
binary
Figure
multiplier
a
symbol
b
function
c
implementation
a
multiplier
multiplicand
partial
products
result
b
a
x
A
B
P
b
B
B
B
B
A
B
A
B
A
B
A
B
A
A
A
A
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
A
B
P
P
P
P
P
P
P
P
P
c
P
P
P
P
P
P
P
A
A
A
A
B
B
B
B
Chap
operands
there
are
N
partial
products
and
N
stages
of
bit
adders
For
example
for
a
multiplier
the
partial
product
of
the
first
row
is
B
AND
A
A
A
A
This
partial
product
is
added
to
the
shifted
second
partial
product
B
AND
A
A
A
A
Subsequent
rows
of
AND
gates
and
adders
form
and
add
the
remaining
partial
products
The
HDL
for
a
multiplier
is
in
HDL
Example
As
with
adders
many
different
multiplier
designs
with
different
speed
cost
trade
offs
exist
Synthesis
tools
may
pick
the
most
appropriate
design
given
the
timing
constraints
Arithmetic
Circuits
Verilog
module
multiplier
parameter
N
input
N
a
b
output
N
y
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
multiplier
is
generic
N
integer
port
a
b
in
STD
LOGIC
VECTOR
N
downto
y
out
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
multiplier
is
begin
y
a
b
end
HDL
Example
MULTIPLIER
Division
Binary
division
can
be
performed
using
the
following
algorithm
for
normalized
unsigned
numbers
in
the
range
N
N
R
A
for
i
N
to
D
R
B
if
D
then
Qi
R
R
R
B
else
Qi
R
D
R
B
if
i
then
R
R
The
partial
remainder
R
is
initialized
to
the
dividend
A
The
divisor
B
is
repeatedly
subtracted
from
this
partial
remainder
to
determine
whether
it
fits
If
the
difference
D
is
negative
i
e
the
sign
bit
of
D
is
then
the
quotient
bit
Qi
is
and
the
difference
is
discarded
Otherwise
Qi
is
Figure
Synthesized
multiplier
y
b
a
Chapter
qxd
and
the
partial
remainder
is
updated
to
be
the
difference
In
any
event
the
partial
remainder
is
then
doubled
left
shifted
by
one
column
and
the
process
repeats
The
result
satisfies
Figure
shows
a
schematic
of
a
bit
array
divider
The
divider
computes
A
B
and
produces
a
quotient
Q
and
a
remainder
R
The
legend
shows
the
symbol
and
schematic
for
each
block
in
the
array
divider
The
signal
P
indicates
whether
R
B
is
negative
It
is
obtained
from
the
Cout
output
of
the
leftmost
block
in
the
row
which
is
the
sign
of
the
difference
The
delay
of
an
N
bit
array
divider
increases
proportionally
to
N
because
the
carry
must
ripple
through
all
N
stages
in
a
row
before
the
sign
is
determined
and
the
multiplexer
selects
R
or
D
This
repeats
for
all
N
rows
Division
is
a
slow
and
expensive
operation
in
hardware
and
therefore
should
be
used
as
infrequently
as
possible
Further
Reading
Computer
arithmetic
could
be
the
subject
of
an
entire
text
Digital
Arithmetic
by
Ercegovac
and
Lang
is
an
excellent
overview
of
the
entire
field
CMOS
VLSI
Design
by
Weste
and
Harris
covers
highperformance
circuit
designs
for
arithmetic
operations
A
B
Q
R
B
N
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Array
divider
R
B
D
R
P
Cout
Cin
R
B
P
R
Cout
Cin
A
A
A
A
Q
Q
Q
Q
B
B
B
B
R
R
R
R
Legend
Chapte
NUMBER
SYSTEMS
Computers
operate
on
both
integers
and
fractions
So
far
we
have
only
considered
representing
signed
or
unsigned
integers
as
introduced
in
Section
This
section
introduces
fixed
and
floating
point
number
systems
that
can
also
represent
rational
numbers
Fixed
point
numbers
are
analogous
to
decimals
some
of
the
bits
represent
the
integer
part
and
the
rest
represent
the
fraction
Floating
point
numbers
are
analogous
to
scientific
notation
with
a
mantissa
and
an
exponent
Fixed
Point
Number
Systems
Fixed
point
notation
has
an
implied
binary
point
between
the
integer
and
fraction
bits
analogous
to
the
decimal
point
between
the
integer
and
fraction
digits
of
an
ordinary
decimal
number
For
example
Figure
a
shows
a
fixed
point
number
with
four
integer
bits
and
four
fraction
bits
Figure
b
shows
the
implied
binary
point
in
blue
and
Figure
c
shows
the
equivalent
decimal
value
Signed
fixed
point
numbers
can
use
either
two
s
complement
or
sign
magnitude
notations
Figure
shows
the
fixed
point
representation
of
using
both
notations
with
four
integer
and
four
fraction
bits
The
implicit
binary
point
is
shown
in
blue
for
clarity
In
sign
magnitude
form
the
most
significant
bit
is
used
to
indicate
the
sign
The
two
s
complement
representation
is
formed
by
inverting
the
bits
of
the
absolute
value
and
adding
a
to
the
least
significant
rightmost
bit
In
this
case
the
least
significant
bit
position
is
in
the
column
Like
all
binary
number
representations
fixed
point
numbers
are
just
a
collection
of
bits
There
is
no
way
of
knowing
the
existence
of
the
binary
point
except
through
agreement
of
those
people
interpreting
the
number
Example
ARITHMETIC
WITH
FIXED
POINT
NUMBERS
Compute
using
fixed
point
numbers
Solution
First
convert
the
magnitude
of
the
second
number
to
fixedpoint
binary
notation
so
there
is
a
in
the
column
leaving
Because
there
is
a
in
the
column
Because
there
is
a
in
the
column
leaving
Thus
there
must
be
a
in
the
column
Putting
this
all
together
Use
two
s
complement
representation
for
signed
numbers
so
that
addition
works
correctly
Figure
shows
the
conversion
of
to
fixed
point
two
s
complement
notation
Figure
shows
the
fixed
point
binary
addition
and
the
decimal
equivalent
for
comparison
Note
that
the
leading
in
the
binary
fixed
point
addition
of
Figure
a
is
discarded
from
the
bit
result
Number
Systems
Figure
Fixed
point
notation
of
with
four
integer
bits
and
four
fraction
bits
a
b
c
Figure
Fixed
point
representation
of
a
absolute
value
b
sign
and
magnitude
c
two
s
complement
a
b
c
Fixed
point
number
systems
are
commonly
used
for
banking
and
financial
applications
that
require
precision
but
not
a
large
range
Chapter
qxd
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Fixed
point
two
s
complement
conversion
Add
Two
s
Complement
One
s
Complement
Binary
Magnitude
Figure
Addition
a
binary
fixed
point
b
decimal
equivalent
a
b
Figure
Floating
point
numbers
Figure
bit
floatingpoint
version
M
BE
Sign
Exponent
Mantissa
bit
bits
bits
Floating
Point
Number
Systems
Floating
point
numbers
are
analogous
to
scientific
notation
They
circumvent
the
limitation
of
having
a
constant
number
of
integer
and
fractional
bits
allowing
the
representation
of
very
large
and
very
small
numbers
Like
scientific
notation
floating
point
numbers
have
a
sign
mantissa
M
base
B
and
exponent
E
as
shown
in
Figure
For
example
the
number
is
the
decimal
scientific
notation
for
It
has
a
mantissa
of
a
base
of
and
an
exponent
of
The
decimal
point
floats
to
the
position
right
after
the
most
significant
digit
Floating
point
numbers
are
base
with
a
binary
mantissa
bits
are
used
to
represent
sign
bit
exponent
bits
and
mantissa
bits
Example
BIT
FLOATING
POINT
NUMBERS
Show
the
floating
point
representation
of
the
decimal
number
Solution
First
convert
the
decimal
number
into
binary
Figure
shows
the
bit
encoding
which
will
be
modified
later
for
efficiency
The
sign
bit
is
positive
the
exponent
bits
give
the
value
and
the
remaining
bits
are
the
mantissa
In
binary
floating
point
the
first
bit
of
the
mantissa
to
the
left
of
the
binary
point
is
always
and
therefore
need
not
be
stored
It
is
called
the
implicit
leading
one
Figure
shows
the
modified
floatingpoint
representation
of
The
implicit
leading
one
is
not
included
in
the
bit
mantissa
for
efficiency
Only
the
fraction
bits
are
stored
This
frees
up
an
extra
bit
for
useful
data
Chap
We
make
one
final
modification
to
the
exponent
field
The
exponent
needs
to
represent
both
positive
and
negative
exponents
To
do
so
floating
point
uses
a
biased
exponent
which
is
the
original
exponent
plus
a
constant
bias
bit
floating
point
uses
a
bias
of
For
example
for
the
exponent
the
biased
exponent
is
For
the
exponent
the
biased
exponent
is
Figure
shows
represented
in
floatingpoint
notation
with
an
implicit
leading
one
and
a
biased
exponent
of
This
notation
conforms
to
the
IEEE
floating
point
standard
Special
Cases
and
NaN
The
IEEE
floating
point
standard
has
special
cases
to
represent
numbers
such
as
zero
infinity
and
illegal
results
For
example
representing
the
number
zero
is
problematic
in
floating
point
notation
because
of
the
implicit
leading
one
Special
codes
with
exponents
of
all
s
or
all
s
are
reserved
for
these
special
cases
Table
shows
the
floating
point
representations
of
and
NaN
As
with
sign
magnitude
numbers
floating
point
has
both
positive
and
negative
NaN
is
used
for
numbers
that
don
t
exist
such
as
or
log
Single
and
Double
Precision
Formats
So
far
we
have
examined
bit
floating
point
numbers
This
format
is
also
called
single
precision
single
or
float
The
IEEE
standard
also
Number
Systems
Figure
Floating
point
version
Sign
Exponent
Fraction
bit
bits
bits
Figure
IEEE
floatingpoint
notation
Sign
Biased
Exponent
Fraction
bit
bits
bits
Table
IEEE
floating
point
notations
for
and
NaN
Number
Sign
Exponent
Fraction
X
NaN
X
non
zero
As
may
be
apparent
there
are
many
reasonable
ways
to
represent
floating
point
numbers
For
many
years
computer
manufacturers
used
incompatible
floating
point
formats
Results
from
one
computer
could
not
directly
be
interpreted
by
another
computer
The
Institute
of
Electrical
and
Electronics
Engineers
solved
this
problem
by
defining
the
IEEE
floatingpoint
standard
in
defining
floating
point
numbers
This
floating
point
format
is
now
almost
universally
used
and
is
the
one
discussed
in
this
section
Chapter
defines
bit
double
precision
also
called
double
numbers
that
provide
greater
precision
and
greater
range
Table
shows
the
number
of
bits
used
for
the
fields
in
each
format
Excluding
the
special
cases
mentioned
earlier
normal
single
precision
numbers
span
a
range
of
to
They
have
a
precision
of
about
seven
significant
decimal
digits
because
Similarly
normal
double
precision
numbers
span
a
range
of
to
and
have
a
precision
of
about
significant
decimal
digits
Rounding
Arithmetic
results
that
fall
outside
of
the
available
precision
must
round
to
a
neighboring
number
The
rounding
modes
are
round
down
round
up
round
toward
zero
and
round
to
nearest
The
default
rounding
mode
is
round
to
nearest
In
the
round
to
nearest
mode
if
two
numbers
are
equally
near
the
one
with
a
in
the
least
significant
position
of
the
fraction
is
chosen
Recall
that
a
number
overflows
when
its
magnitude
is
too
large
to
be
represented
Likewise
a
number
underflows
when
it
is
too
tiny
to
be
represented
In
round
to
nearest
mode
overflows
are
rounded
up
to
and
underflows
are
rounded
down
to
Floating
Point
Addition
Addition
with
floating
point
numbers
is
not
as
simple
as
addition
with
two
s
complement
numbers
The
steps
for
adding
floating
point
numbers
with
the
same
sign
are
as
follows
Extract
exponent
and
fraction
bits
Prepend
leading
to
form
the
mantissa
Compare
exponents
Shift
smaller
mantissa
if
necessary
Add
mantissas
Normalize
mantissa
and
adjust
exponent
if
necessary
Round
result
Assemble
exponent
and
fraction
back
into
floating
point
number
CHAPTER
FIVE
Digital
Building
Blocks
Floating
point
cannot
represent
some
numbers
exactly
like
However
when
you
type
into
your
calculator
you
see
exactly
not
To
handle
this
some
applications
such
as
calculators
and
financial
software
use
binary
coded
decimal
BCD
numbers
or
formats
with
a
base
exponent
BCD
numbers
encode
each
decimal
digit
using
four
bits
with
a
range
of
to
For
example
the
BCD
fixedpoint
notation
of
with
four
integer
bits
and
four
fraction
bits
would
be
Of
course
nothing
is
free
The
cost
is
increased
complexity
in
arithmetic
hardware
and
wasted
encodings
A
F
encodings
are
not
used
and
thus
decreased
performance
So
for
compute
intensive
applications
floating
point
is
much
faster
Table
Single
and
double
precision
floating
point
formats
Format
Total
Bits
Sign
Bits
Exponent
Bits
Fraction
Bits
single
double
Chap
Figure
shows
the
floating
point
addition
of
and
The
result
is
After
the
fraction
and
exponent
bits
are
extracted
and
the
implicit
leading
is
prepended
in
steps
and
the
exponents
are
compared
by
subtracting
the
smaller
exponent
from
the
larger
exponent
The
result
is
the
number
of
bits
by
which
the
smaller
number
is
shifted
to
the
right
to
align
the
implied
binary
point
i
e
to
make
the
exponents
equal
in
step
The
aligned
numbers
are
added
Because
the
sum
has
a
mantissa
that
is
greater
than
or
equal
to
the
result
is
normalized
by
shifting
it
to
the
right
one
bit
and
incrementing
the
exponent
In
this
example
the
result
is
exact
so
no
rounding
is
necessary
The
result
is
stored
in
floating
point
notation
by
removing
the
implicit
leading
one
of
the
mantissa
and
prepending
the
sign
bit
Number
Systems
Figure
Floating
point
addition
Step
Exponent
Step
Step
shift
amount
Step
Step
Step
Step
Floating
point
numbers
Step
No
rounding
necessary
Fraction
Floating
point
arithmetic
is
usually
done
in
hardware
to
make
it
fast
This
hardware
called
the
floating
point
unit
FPU
is
typically
distinct
from
the
central
processing
unit
CPU
The
infamous
floating
point
division
FDIV
bug
in
the
Pentium
FPU
cost
Intel
million
to
recall
and
replace
defective
chips
The
bug
occurred
simply
because
a
lookup
table
was
not
loaded
correctly
C
SEQUENTIAL
BUILDING
BLOCKS
This
section
examines
sequential
building
blocks
including
counters
and
shift
registers
Counters
An
N
bit
binary
counter
shown
in
Figure
is
a
sequential
arithmetic
circuit
with
clock
and
reset
inputs
and
an
N
bit
output
Q
Reset
initializes
the
output
to
The
counter
then
advances
through
all
N
possible
outputs
in
binary
order
incrementing
on
the
rising
edge
of
the
clock
Figure
shows
an
N
bit
counter
composed
of
an
adder
and
a
resettable
register
On
each
cycle
the
counter
adds
to
the
value
stored
in
the
register
HDL
Example
describes
a
binary
counter
with
asynchronous
reset
Other
types
of
counters
such
as
Up
Down
counters
are
explored
in
Exercises
through
CHAPTER
FIVE
Digital
Building
Blocks
Figure
N
bit
counter
Figure
Counter
symbol
Q
CLK
Reset
N
N
CLK
Reset
B
S
A
N
Q
N
r
Verilog
module
counter
parameter
N
input
clk
input
reset
output
reg
N
q
always
posedge
clk
or
posedge
reset
if
reset
q
else
q
q
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
use
IEEE
STD
LOGIC
ARITH
ALL
entity
counter
is
generic
N
integer
port
clk
reset
in
STD
LOGIC
q
buffer
STD
LOGIC
VECTOR
N
downto
end
architecture
synth
of
counter
is
begin
process
clk
reset
begin
if
reset
then
q
CONV
STD
LOGIC
VECTOR
N
elsif
clk
event
and
clk
then
q
q
end
if
end
process
end
HDL
Example
COUNTER
Figure
Synthesized
counter
R
q
reset
clk
Q
D
Chapter
Shift
Registers
A
shift
register
has
a
clock
a
serial
input
Sin
a
serial
output
Sout
and
N
parallel
outputs
QN
as
shown
in
Figure
On
each
rising
edge
of
the
clock
a
new
bit
is
shifted
in
from
Sin
and
all
the
subsequent
contents
are
shifted
forward
The
last
bit
in
the
shift
register
is
available
at
Sout
Shift
registers
can
be
viewed
as
serial
to
parallel
converters
The
input
is
provided
serially
one
bit
at
a
time
at
Sin
After
N
cycles
the
past
N
inputs
are
available
in
parallel
at
Q
A
shift
register
can
be
constructed
from
N
flip
flops
connected
in
series
as
shown
in
Figure
Some
shift
registers
also
have
a
reset
signal
to
initialize
all
of
the
flip
flops
A
related
circuit
is
a
parallel
to
serial
converter
that
loads
N
bits
in
parallel
then
shifts
them
out
one
at
a
time
A
shift
register
can
be
modified
to
perform
both
serial
to
parallel
and
parallel
to
serial
operations
by
adding
a
parallel
input
DN
and
a
control
signal
Load
as
shown
in
Figure
When
Load
is
asserted
the
flip
flops
are
loaded
in
parallel
from
the
D
inputs
Otherwise
the
shift
register
shifts
normally
HDL
Example
describes
such
a
shift
register
Scan
Chains
Shift
registers
are
often
used
to
test
sequential
circuits
using
a
technique
called
scan
chains
Testing
combinational
circuits
is
relatively
straightforward
Known
inputs
called
test
vectors
are
applied
and
the
outputs
are
checked
against
the
expected
result
Testing
sequential
circuits
is
more
difficult
because
the
circuits
have
state
Starting
from
a
known
initial
condition
a
large
number
of
cycles
of
test
vectors
may
be
needed
to
put
the
circuit
into
a
desired
state
For
example
testing
that
the
most
significant
bit
of
a
bit
counter
advances
from
to
requires
resetting
the
counter
then
applying
about
two
billion
clock
pulses
Sequential
Building
Blocks
Don
t
confuse
shift
registers
with
the
shifters
from
Section
Shift
registers
are
sequential
logic
blocks
that
shift
in
a
new
bit
on
each
clock
edge
Shifters
are
unclocked
combinational
logic
blocks
that
shift
an
input
by
a
specified
amount
Figure
Shift
register
symbol
N
Q
Sin
Sout
Figure
Shift
register
schematic
Figure
Shift
register
with
parallel
load
CLK
Sin
Sout
Q
Q
Q
QN
CLK
D
D
D
DN
Q
Q
Q
QN
Sin
Sout
Load
To
solve
this
problem
designers
like
to
be
able
to
directly
observe
and
control
all
the
state
of
the
machine
This
is
done
by
adding
a
test
mode
in
which
the
contents
of
all
flip
flops
can
be
read
out
or
loaded
with
desired
values
Most
systems
have
too
many
flip
flops
to
dedicate
individual
pins
to
read
and
write
each
flip
flop
Instead
all
the
flip
flops
in
the
system
are
connected
together
into
a
shift
register
called
a
scan
chain
In
normal
operation
the
flip
flops
load
data
from
their
D
input
and
ignore
the
scan
chain
In
test
mode
the
flip
flops
serially
shift
their
contents
out
and
shift
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
shiftreg
parameter
N
input
clk
input
reset
load
input
sin
input
N
d
output
reg
N
q
output
sout
always
posedge
clk
or
posedge
reset
if
reset
q
else
if
load
q
d
else
q
q
N
sin
assign
sout
q
N
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
ARITH
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
shiftreg
is
generic
N
integer
port
clk
reset
load
in
STD
LOGIC
sin
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
N
downto
q
buffer
STD
LOGIC
VECTOR
N
downto
sout
out
STD
LOGIC
end
architecture
synth
of
shiftreg
is
begin
process
clk
reset
begin
if
reset
then
q
CONV
STD
LOGIC
VECTOR
N
elsif
clk
event
and
clk
then
if
load
then
q
d
else
q
q
N
downto
sin
end
if
end
if
end
process
sout
q
N
end
HDL
Example
SHIFT
REGISTER
WITH
PARALLEL
LOAD
Figure
Synthesized
shiftreg
R
sout
q
d
sin
load
reset
clk
D
Q
Chapter
qxd
Memory
Arrays
Figure
Scannable
flip
flop
a
schematic
b
symbol
and
c
N
bit
scannable
register
Test
D
Sin
Q
Sout
a
D
Q
Sin
Sout
Test
b
D
Q
Sin
Sout
Test
D
Q
Sin
Sout
Test
D
Q
Sin
Sout
Test
D
Q
Sin
Sout
Test
c
Test
CLK
CLK
CLK
D
Q
D
Q
D
Q
DN
QN
Sin
Sout
in
new
contents
using
Sin
and
Sout
The
load
multiplexer
is
usually
integrated
into
the
flip
flop
to
produce
a
scannable
flip
flop
Figure
shows
the
schematic
and
symbol
for
a
scannable
flip
flop
and
illustrates
how
the
flops
are
cascaded
to
build
an
N
bit
scannable
register
For
example
the
bit
counter
could
be
tested
by
shifting
in
the
pattern
in
test
mode
counting
for
one
cycle
in
normal
mode
then
shifting
out
the
result
which
should
be
This
requires
only
cycles
MEMORY
ARRAYS
The
previous
sections
introduced
arithmetic
and
sequential
circuits
for
manipulating
data
Digital
systems
also
require
memories
to
store
the
data
used
and
generated
by
such
circuits
Registers
built
from
flipflops
are
a
kind
of
memory
that
stores
small
amounts
of
data
This
section
describes
memory
arrays
that
can
efficiently
store
large
amounts
of
data
The
section
begins
with
an
overview
describing
characteristics
shared
by
all
memory
arrays
It
then
introduces
three
types
of
memory
arrays
dynamic
random
access
memory
DRAM
static
random
access
memory
SRAM
and
read
only
memory
ROM
Each
memory
differs
in
the
way
it
stores
data
The
section
briefly
discusses
area
and
delay
trade
offs
and
shows
how
memory
arrays
are
used
not
only
to
store
data
but
also
to
perform
logic
functions
The
section
finishes
with
the
HDL
for
a
memory
array
Overview
Figure
shows
a
generic
symbol
for
a
memory
array
The
memory
is
organized
as
a
two
dimensional
array
of
memory
cells
The
memory
reads
or
writes
the
contents
of
one
of
the
rows
of
the
array
This
row
is
Address
Data
Array
N
M
Figure
Generic
memory
array
symbol
C
specified
by
an
Address
The
value
read
or
written
is
called
Data
An
array
with
N
bit
addresses
and
M
bit
data
has
N
rows
and
M
columns
Each
row
of
data
is
called
a
word
Thus
the
array
contains
N
M
bit
words
Figure
shows
a
memory
array
with
two
address
bits
and
three
data
bits
The
two
address
bits
specify
one
of
the
four
rows
data
words
in
the
array
Each
data
word
is
three
bits
wide
Figure
b
shows
some
possible
contents
of
the
memory
array
The
depth
of
an
array
is
the
number
of
rows
and
the
width
is
the
number
of
columns
also
called
the
word
size
The
size
of
an
array
is
given
as
depth
width
Figure
is
a
word
bit
array
or
simply
array
The
symbol
for
a
word
bit
array
is
shown
in
Figure
The
total
size
of
this
array
is
kilobits
Kb
Bit
Cells
Memory
arrays
are
built
as
an
array
of
bit
cells
each
of
which
stores
bit
of
data
Figure
shows
that
each
bit
cell
is
connected
to
a
wordline
and
a
bitline
For
each
combination
of
address
bits
the
memory
asserts
a
single
wordline
that
activates
the
bit
cells
in
that
row
When
the
wordline
is
HIGH
the
stored
bit
transfers
to
or
from
the
bitline
Otherwise
the
bitline
is
disconnected
from
the
bit
cell
The
circuitry
to
store
the
bit
varies
with
memory
type
To
read
a
bit
cell
the
bitline
is
initially
left
floating
Z
Then
the
wordline
is
turned
ON
allowing
the
stored
value
to
drive
the
bitline
to
or
To
write
a
bit
cell
the
bitline
is
strongly
driven
to
the
desired
value
Then
the
wordline
is
turned
ON
connecting
the
bitline
to
the
stored
bit
The
strongly
driven
bitline
overpowers
the
contents
of
the
bit
cell
writing
the
desired
value
into
the
stored
bit
Organization
Figure
shows
the
internal
organization
of
a
memory
array
Of
course
practical
memories
are
much
larger
but
the
behavior
of
larger
arrays
can
be
extrapolated
from
the
smaller
array
In
this
example
the
array
stores
the
data
from
Figure
b
During
a
memory
read
a
wordline
is
asserted
and
the
corresponding
row
of
bit
cells
drives
the
bitlines
HIGH
or
LOW
During
a
memory
write
the
bitlines
are
driven
HIGH
or
LOW
first
and
then
a
wordline
is
asserted
allowing
the
bitline
values
to
be
stored
in
that
row
of
bit
cells
For
example
to
read
Address
the
bitlines
are
left
floating
the
decoder
asserts
wordline
and
the
data
stored
in
that
row
of
bit
cells
reads
out
onto
the
Data
bitlines
To
write
the
value
to
Address
the
bitlines
are
driven
to
the
value
then
wordline
is
asserted
and
the
new
value
is
stored
in
the
bit
cells
CHAPTER
FIVE
Digital
Building
Blocks
a
Address
Data
Array
b
Address
depth
width
Data
Figure
memory
array
a
symbol
b
function
Address
Data
word
bit
Array
Figure
Kb
array
depth
words
width
bits
stored
bit
wordline
bitline
Figure
Bit
cell
Cha
Memory
Ports
All
memories
have
one
or
more
ports
Each
port
gives
read
and
or
write
access
to
one
memory
address
The
previous
examples
were
all
singleported
memories
Multiported
memories
can
access
several
addresses
simultaneously
Figure
shows
a
three
ported
memory
with
two
read
ports
and
one
write
port
Port
reads
the
data
from
address
A
onto
the
read
data
output
RD
Port
reads
the
data
from
address
A
onto
RD
Port
writes
the
data
from
the
write
data
input
WD
into
address
A
on
the
rising
edge
of
the
clock
if
the
write
enable
WE
is
asserted
Memory
Types
Memory
arrays
are
specified
by
their
size
depth
width
and
the
number
and
type
of
ports
All
memory
arrays
store
data
as
an
array
of
bit
cells
but
they
differ
in
how
they
store
bits
Memories
are
classified
based
on
how
they
store
bits
in
the
bit
cell
The
broadest
classification
is
random
access
memory
RAM
versus
read
only
memory
ROM
RAM
is
volatile
meaning
that
it
loses
its
data
when
the
power
is
turned
off
ROM
is
nonvolatile
meaning
that
it
retains
its
data
indefinitely
even
without
a
power
source
RAM
and
ROM
received
their
names
for
historical
reasons
that
are
no
longer
very
meaningful
RAM
is
called
random
access
memory
because
any
data
word
is
accessed
with
the
same
delay
as
any
other
A
sequential
access
memory
such
as
a
tape
recorder
accesses
nearby
data
more
quickly
than
faraway
data
e
g
at
the
other
end
of
the
tape
Memory
Arrays
wordline
Decoder
Address
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
stored
bit
wordline
wordline
wordline
bitline
bitline
bitline
Data
Data
Data
Figure
memory
array
A
A
WD
WE
A
CLK
Array
RD
RD
M
M
N
N
N
M
Figure
Three
ported
memory
ROM
is
called
read
only
memory
because
historically
it
could
only
be
read
but
not
written
These
names
are
confusing
because
ROMs
are
randomly
accessed
too
Worse
yet
most
modern
ROMs
can
be
written
as
well
as
read
The
important
distinction
to
remember
is
that
RAMs
are
volatile
and
ROMs
are
nonvolatile
The
two
major
types
of
RAMs
are
dynamic
RAM
DRAM
and
static
RAM
SRAM
Dynamic
RAM
stores
data
as
a
charge
on
a
capacitor
whereas
static
RAM
stores
data
using
a
pair
of
cross
coupled
inverters
There
are
many
flavors
of
ROMs
that
vary
by
how
they
are
written
and
erased
These
various
types
of
memories
are
discussed
in
the
subsequent
sections
Dynamic
Random
Access
Memory
Dynamic
RAM
DRAM
pronounced
dee
ram
stores
a
bit
as
the
presence
or
absence
of
charge
on
a
capacitor
Figure
shows
a
DRAM
bit
cell
The
bit
value
is
stored
on
a
capacitor
The
nMOS
transistor
behaves
as
a
switch
that
either
connects
or
disconnects
the
capacitor
from
the
bitline
When
the
wordline
is
asserted
the
nMOS
transistor
turns
ON
and
the
stored
bit
value
transfers
to
or
from
the
bitline
As
shown
in
Figure
a
when
the
capacitor
is
charged
to
VDD
the
stored
bit
is
when
it
is
discharged
to
GND
Figure
b
the
stored
bit
is
The
capacitor
node
is
dynamic
because
it
is
not
actively
driven
HIGH
or
LOW
by
a
transistor
tied
to
VDD
or
GND
Upon
a
read
data
values
are
transferred
from
the
capacitor
to
the
bitline
Upon
a
write
data
values
are
transferred
from
the
bitline
to
the
capacitor
Reading
destroys
the
bit
value
stored
on
the
capacitor
so
the
data
word
must
be
restored
rewritten
after
each
read
Even
when
DRAM
is
not
read
the
contents
must
be
refreshed
read
and
rewritten
every
few
milliseconds
because
the
charge
on
the
capacitor
gradually
leaks
away
Static
Random
Access
Memory
SRAM
Static
RAM
SRAM
pronounced
es
ram
is
static
because
stored
bits
do
not
need
to
be
refreshed
Figure
shows
an
SRAM
bit
cell
The
data
bit
is
stored
on
cross
coupled
inverters
like
those
described
in
Section
Each
cell
has
two
outputs
bitline
and
When
the
wordline
is
asserted
both
nMOS
transistors
turn
on
and
data
values
are
transferred
to
or
from
the
bitlines
Unlike
DRAM
if
noise
degrades
the
value
of
the
stored
bit
the
cross
coupled
inverters
restore
the
value
bitline
CHAPTER
FIVE
Digital
Building
Blocks
Robert
Dennard
Invented
DRAM
in
at
IBM
Although
many
were
skeptical
that
the
idea
would
work
by
the
mid
s
DRAM
was
in
virtually
all
computers
He
claims
to
have
done
little
creative
work
until
arriving
at
IBM
they
handed
him
a
patent
notebook
and
said
put
all
your
ideas
in
there
Since
he
has
received
patents
in
semiconductors
and
microelectronics
Photo
courtesy
of
IBM
wordline
bitline
stored
bit
Figure
DRAM
bit
cell
Area
and
Delay
Flip
flops
SRAMs
and
DRAMs
are
all
volatile
memories
but
each
has
different
area
and
delay
characteristics
Table
shows
a
comparison
of
these
three
types
of
volatile
memory
The
data
bit
stored
in
a
flip
flop
is
available
immediately
at
its
output
But
flip
flops
take
at
least
transistors
to
build
Generally
the
more
transistors
a
device
has
the
more
area
power
and
cost
it
requires
DRAM
latency
is
longer
than
that
of
SRAM
because
its
bitline
is
not
actively
driven
by
a
transistor
DRAM
must
wait
for
charge
to
move
relatively
slowly
from
the
capacitor
to
the
bitline
DRAM
also
has
lower
throughput
than
SRAM
because
it
must
refresh
data
periodically
and
after
a
read
Memory
latency
and
throughput
also
depend
on
memory
size
larger
memories
tend
to
be
slower
than
smaller
ones
if
all
else
is
the
same
The
best
memory
type
for
a
particular
design
depends
on
the
speed
cost
and
power
constraints
Register
Files
Digital
systems
often
use
a
number
of
registers
to
store
temporary
variables
This
group
of
registers
called
a
register
file
is
usually
built
as
a
small
multiported
SRAM
array
because
it
is
more
compact
than
an
array
of
flip
flops
Figure
shows
a
register
bit
three
ported
register
file
built
from
a
three
ported
memory
similar
to
that
of
Figure
The
Memory
Arrays
Figure
DRAM
stored
values
stored
bit
wordline
bitline
bitline
Figure
SRAM
bit
cell
Table
Memory
comparison
Memory
Transistors
per
Latency
Type
Bit
Cell
flip
flop
fast
SRAM
medium
DRAM
slow
CLK
A
A
WD
RD
RD
WE
A
Register
File
Figure
register
file
with
two
read
ports
and
one
write
port
wordline
bitline
a
stored
bit
wordline
bitline
b
stored
bit
register
file
has
two
read
ports
A
RD
and
A
RD
and
one
write
port
A
WD
The
bit
addresses
A
A
and
A
can
each
access
all
registers
So
two
registers
can
be
read
and
one
register
written
simultaneously
Read
Only
Memory
Read
only
memory
ROM
stores
a
bit
as
the
presence
or
absence
of
a
transistor
Figure
shows
a
simple
ROM
bit
cell
To
read
the
cell
the
bitline
is
weakly
pulled
HIGH
Then
the
wordline
is
turned
ON
If
the
transistor
is
present
it
pulls
the
bitline
LOW
If
it
is
absent
the
bitline
remains
HIGH
Note
that
the
ROM
bit
cell
is
a
combinational
circuit
and
has
no
state
to
forget
if
power
is
turned
off
The
contents
of
a
ROM
can
be
indicated
using
dot
notation
Figure
shows
the
dot
notation
for
a
word
bit
ROM
containing
the
data
from
Figure
A
dot
at
the
intersection
of
a
row
wordline
and
a
column
bitline
indicates
that
the
data
bit
is
For
example
the
top
wordline
has
a
single
dot
on
Data
so
the
data
word
stored
at
Address
is
Conceptually
ROMs
can
be
built
using
two
level
logic
with
a
group
of
AND
gates
followed
by
a
group
of
OR
gates
The
AND
gates
produce
all
possible
minterms
and
hence
form
a
decoder
Figure
shows
the
ROM
of
Figure
built
using
a
decoder
and
OR
gates
Each
dotted
row
in
Figure
is
an
input
to
an
OR
gate
in
Figure
For
data
bits
with
a
single
dot
in
this
case
Data
no
OR
gate
is
needed
This
representation
of
a
ROM
is
interesting
because
it
shows
how
the
ROM
can
perform
any
two
level
logic
function
In
practice
ROMs
are
built
from
transistors
instead
of
logic
gates
to
reduce
their
size
and
cost
Section
explores
the
transistor
level
implementation
further
CHAPTER
FIVE
Digital
Building
Blocks
wordline
bitline
wordline
bitline
bit
cell
containing
bit
cell
containing
Figure
ROM
bit
cells
containing
and
Decoder
Address
Data
Data
Data
Figure
ROM
dot
notation
C
The
contents
of
the
ROM
bit
cell
in
Figure
are
specified
during
manufacturing
by
the
presence
or
absence
of
a
transistor
in
each
bit
cell
A
programmable
ROM
PROM
pronounced
like
the
dance
places
a
transistor
in
every
bit
cell
but
provides
a
way
to
connect
or
disconnect
the
transistor
to
ground
Figure
shows
the
bit
cell
for
a
fuse
programmable
ROM
The
user
programs
the
ROM
by
applying
a
high
voltage
to
selectively
blow
fuses
If
the
fuse
is
present
the
transistor
is
connected
to
GND
and
the
cell
holds
a
If
the
fuse
is
destroyed
the
transistor
is
disconnected
from
ground
and
the
cell
holds
a
This
is
also
called
a
onetime
programmable
ROM
because
the
fuse
cannot
be
repaired
once
it
is
blown
Reprogrammable
ROMs
provide
a
reversible
mechanism
for
connecting
or
disconnecting
the
transistor
to
GND
Erasable
PROMs
EPROMs
pronounced
e
proms
replace
the
nMOS
transistor
and
fuse
with
a
floating
gate
transistor
The
floating
gate
is
not
physically
attached
to
any
other
wires
When
suitable
high
voltages
are
applied
electrons
tunnel
through
an
insulator
onto
the
floating
gate
turning
on
the
transistor
and
connecting
the
bitline
to
the
wordline
decoder
output
When
the
EPROM
is
exposed
to
intense
ultraviolet
UV
light
for
about
half
an
hour
the
electrons
are
knocked
off
the
floating
gate
turning
the
transistor
off
These
actions
are
called
programming
and
erasing
respectively
Electrically
erasable
PROMs
EEPROMs
pronounced
e
e
proms
or
double
e
proms
and
Flash
memory
use
similar
principles
but
include
circuitry
on
the
chip
for
erasing
as
well
as
programming
so
no
UV
light
is
necessary
EEPROM
bit
cells
are
individually
erasable
Flash
memory
erases
larger
blocks
of
bits
and
is
cheaper
because
fewer
erasing
circuits
are
needed
In
Flash
Memory
Arrays
Decoder
Data
Data
Data
Address
Figure
ROM
implementation
using
gates
wordline
bitline
bit
cell
containing
intact
fuse
wordline
bitline
bit
cell
containing
blown
fuse
Figure
Fuse
programmable
ROM
bit
cell
Fujio
Masuoka
Received
a
Ph
D
in
electrical
engineering
from
Tohoku
University
Japan
Developed
memories
and
highspeed
circuits
at
Toshiba
from
to
Invented
Flash
memory
as
an
unauthorized
project
pursued
during
nights
and
weekends
in
the
late
s
Flash
received
its
name
because
the
process
of
erasing
the
memory
reminds
one
of
the
flash
of
a
camera
Toshiba
was
slow
to
commercialize
the
idea
Intel
was
first
to
market
in
Flash
has
grown
into
a
billion
per
year
market
Dr
Masuoka
later
joined
the
faculty
at
Tohoku
University
memory
costs
less
than
per
GB
and
the
price
continues
to
drop
by
to
per
year
Flash
has
become
an
extremely
popular
way
to
store
large
amounts
of
data
in
portable
battery
powered
systems
such
as
cameras
and
music
players
In
summary
modern
ROMs
are
not
really
read
only
they
can
be
programmed
written
as
well
The
difference
between
RAM
and
ROM
is
that
ROMs
take
a
longer
time
to
write
but
are
nonvolatile
Logic
Using
Memory
Arrays
Although
they
are
used
primarily
for
data
storage
memory
arrays
can
also
perform
combinational
logic
functions
For
example
the
Data
output
of
the
ROM
in
Figure
is
the
XOR
of
the
two
Address
inputs
Likewise
Data
is
the
NAND
of
the
two
inputs
A
N
word
M
bit
memory
can
perform
any
combinational
function
of
N
inputs
and
M
outputs
For
example
the
ROM
in
Figure
performs
three
functions
of
two
inputs
Memory
arrays
used
to
perform
logic
are
called
lookup
tables
LUTs
Figure
shows
a
word
bit
memory
array
used
as
a
lookup
table
to
perform
the
function
Y
AB
Using
memory
to
perform
logic
the
user
can
look
up
the
output
value
for
a
given
input
combination
address
Each
address
corresponds
to
a
row
in
the
truth
table
and
each
data
bit
corresponds
to
an
output
value
Memory
HDL
HDL
Example
describes
a
N
word
M
bit
RAM
The
RAM
has
a
synchronous
enabled
write
In
other
words
writes
occur
on
the
rising
CHAPTER
FIVE
Digital
Building
Blocks
stored
bit
stored
bit
Decoder
A
stored
bit
bitline
stored
bit
Y
B
word
x
bit
Array
ABY
Truth
Table
A
A
Figure
word
bit
memory
array
used
as
a
lookup
table
Programmable
ROMs
can
be
configured
with
a
device
programmer
like
the
one
shown
below
The
device
programmer
is
attached
to
a
computer
which
specifies
the
type
of
ROM
and
the
data
values
to
program
The
device
programmer
blows
fuses
or
injects
charge
onto
a
floating
gate
on
the
ROM
Thus
the
programming
process
is
sometimes
called
burning
a
ROM
Flash
memory
drives
with
Universal
Serial
Bus
USB
connectors
have
replaced
floppy
disks
and
CDs
for
sharing
files
because
Flash
costs
have
dropped
so
dramatically
C
edge
of
the
clock
if
the
write
enable
we
is
asserted
Reads
occur
immediately
When
power
is
first
applied
the
contents
of
the
RAM
are
unpredictable
HDL
Example
describes
a
word
bit
ROM
The
contents
of
the
ROM
are
specified
in
the
HDL
case
statement
A
ROM
as
small
as
this
one
may
be
synthesized
into
logic
gates
rather
than
an
array
Note
that
the
seven
segment
decoder
from
HDL
Example
synthesizes
into
a
ROM
in
Figure
Memory
Arrays
Verilog
module
ram
parameter
N
M
input
clk
input
we
input
N
adr
input
M
din
output
M
dout
reg
M
mem
N
always
posedge
clk
if
we
mem
adr
din
assign
dout
mem
adr
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
ALL
use
IEEE
STD
LOGIC
ARITH
ALL
use
IEEE
STD
LOGIC
UNSIGNED
ALL
entity
ram
array
is
generic
N
integer
M
integer
port
clk
we
in
STD
LOGIC
adr
in
STD
LOGIC
VECTOR
N
downto
din
in
STD
LOGIC
VECTOR
M
downto
dout
out
STD
LOGIC
VECTOR
M
downto
end
architecture
synth
of
ram
array
is
type
mem
array
is
array
N
downto
of
STD
LOGIC
VECTOR
M
downto
signal
mem
mem
array
begin
process
clk
begin
if
clk
event
and
clk
then
if
we
then
mem
CONV
INTEGER
adr
din
end
if
end
if
end
process
dout
mem
CONV
INTEGER
adr
end
HDL
Example
RAM
ram
mem
dout
din
adr
we
clk
RADDR
DATA
DOUT
WADDR
WE
CLK
Figure
Synthesized
ram
Chapter
qxd
LOGIC
ARRAYS
Like
memory
gates
can
be
organized
into
regular
arrays
If
the
connections
are
made
programmable
these
logic
arrays
can
be
configured
to
perform
any
function
without
the
user
having
to
connect
wires
in
specific
ways
The
regular
structure
simplifies
design
Logic
arrays
are
mass
produced
in
large
quantities
so
they
are
inexpensive
Software
tools
allow
users
to
map
logic
designs
onto
these
arrays
Most
logic
arrays
are
also
reconfigurable
allowing
designs
to
be
modified
without
replacing
the
hardware
Reconfigurability
is
valuable
during
development
and
is
also
useful
in
the
field
because
a
system
can
be
upgraded
by
simply
downloading
the
new
configuration
This
section
introduces
two
types
of
logic
arrays
programmable
logic
arrays
PLAs
and
field
programmable
gate
arrays
FPGAs
PLAs
the
older
technology
perform
only
combinational
logic
functions
FPGAs
can
perform
both
combinational
and
sequential
logic
Programmable
Logic
Array
Programmable
logic
arrays
PLAs
implement
two
level
combinational
logic
in
sum
of
products
SOP
form
PLAs
are
built
from
an
AND
array
followed
by
an
OR
array
as
shown
in
Figure
The
inputs
in
true
and
complementary
form
drive
an
AND
array
which
produces
implicants
which
in
turn
are
ORed
together
to
form
the
outputs
An
M
N
P
bit
PLA
has
M
inputs
N
implicants
and
P
outputs
CHAPTER
FIVE
Digital
Building
Blocks
Verilog
module
rom
input
adr
output
reg
dout
always
adr
case
adr
b
dout
b
b
dout
b
b
dout
b
b
dout
b
endcase
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
rom
is
port
adr
in
STD
LOGIC
VECTOR
downto
dout
out
STD
LOGIC
VECTOR
downto
end
architecture
synth
of
rom
is
begin
process
adr
begin
case
adr
is
when
dout
when
dout
when
dout
when
dout
end
case
end
process
end
HDL
Example
ROM
Chapter
q
Figure
shows
the
dot
notation
for
a
bit
PLA
performing
the
functions
and
Each
row
in
the
AND
array
forms
an
implicant
Dots
in
each
row
of
the
AND
array
indicate
which
literals
comprise
the
implicant
The
AND
array
in
Figure
forms
three
implicants
and
Dots
in
the
OR
array
indicate
which
implicants
are
part
of
the
output
function
Figure
shows
how
PLAs
can
be
built
using
two
level
logic
An
alternative
implementation
is
given
in
Section
ROMs
can
be
viewed
as
a
special
case
of
PLAs
A
M
word
N
bit
ROM
is
simply
an
M
M
N
bit
PLA
The
decoder
behaves
as
an
AND
plane
that
produces
all
M
minterms
The
ROM
array
behaves
as
an
OR
plane
that
produces
the
outputs
If
the
function
does
not
depend
on
all
M
minterms
a
PLA
is
likely
to
be
smaller
than
a
ROM
For
example
an
word
bit
ROM
is
required
to
perform
the
same
functions
performed
by
the
bit
PLA
shown
in
Figures
and
ABC
ABC
AB
X
ABC
ABC
Y
AB
Logic
Arrays
AND
Array
OR
Array
Inputs
Outputs
Implicants
N
M
P
Figure
M
N
P
bit
PLA
X
Y
ABC
AB
ABC
ABC
AND
Array
OR
Array
Figure
bit
PLA
dot
notation
Ch
Programmable
logic
devices
PLDs
are
souped
up
PLAs
that
add
registers
and
various
other
features
to
the
basic
AND
OR
planes
However
PLDs
and
PLAs
have
largely
been
displaced
by
FPGAs
which
are
more
flexible
and
efficient
for
building
large
systems
Field
Programmable
Gate
Array
A
field
programmable
gate
array
FPGA
is
an
array
of
reconfigurable
gates
Using
software
programming
tools
a
user
can
implement
designs
on
the
FPGA
using
either
an
HDL
or
a
schematic
FPGAs
are
more
powerful
and
more
flexible
than
PLAs
for
several
reasons
They
can
implement
both
combinational
and
sequential
logic
They
can
also
implement
multilevel
logic
functions
whereas
PLAs
can
only
implement
two
level
logic
Modern
FPGAs
integrate
other
useful
functions
such
as
built
in
multipliers
and
large
RAM
arrays
FPGAs
are
built
as
an
array
of
configurable
logic
blocks
CLBs
Figure
shows
the
block
diagram
of
the
Spartan
FPGA
introduced
by
Xilinx
in
Each
CLB
can
be
configured
to
perform
combinational
or
sequential
functions
The
CLBs
are
surrounded
by
input
output
blocks
IOBs
for
interfacing
with
external
devices
The
IOBs
connect
CLB
inputs
and
outputs
to
pins
on
the
chip
package
CLBs
can
connect
to
other
CLBs
and
IOBs
through
programmable
routing
channels
The
remaining
blocks
shown
in
the
figure
aid
in
programming
the
device
Figure
shows
a
single
CLB
for
the
Spartan
FPGA
Other
brands
of
FPGAs
are
organized
somewhat
differently
but
the
same
general
principles
apply
The
CLB
contains
lookup
tables
LUTs
configurable
multiplexers
and
registers
The
FPGA
is
configured
by
specifying
the
contents
of
the
lookup
tables
and
the
select
signals
for
the
multiplexers
CHAPTER
FIVE
Digital
Building
Blocks
FPGAs
are
the
brains
of
many
consumer
products
including
automobiles
medical
equipment
and
media
devices
like
MP
players
The
Mercedes
Benz
S
Class
series
for
example
has
over
a
dozen
Xilinx
FPGAs
or
PLDs
for
uses
ranging
from
entertainment
to
navigation
to
cruise
control
systems
FPGAs
allow
for
quick
time
to
market
and
make
debugging
or
adding
features
late
in
the
design
process
easier
X
Y
ABC
AND
ARRAY
OR
ARRAY
ABC
AB
ABC
Figure
bit
PLA
using
two
level
logic
Each
Spartan
CLB
has
three
LUTs
the
four
input
F
and
G
LUTs
and
the
three
input
H
LUT
By
loading
the
appropriate
values
into
the
lookup
tables
the
F
and
G
LUTs
can
each
be
configured
to
perform
any
function
of
up
to
four
variables
and
the
H
LUT
can
perform
any
function
of
up
to
three
variables
Configuring
the
FPGA
also
involves
choosing
the
select
signals
that
determine
how
the
multiplexers
route
data
through
the
CLB
For
example
depending
on
the
multiplexer
configuration
the
H
LUT
may
receive
one
of
its
inputs
from
either
DIN
or
the
F
LUT
Similarly
it
receives
another
input
from
either
SR
or
the
G
LUT
The
third
input
always
comes
from
H
Logic
Arrays
Figure
Spartan
block
diagram
The
FPGA
produces
two
combinational
outputs
X
and
Y
Depending
on
the
multiplexer
configuration
X
comes
from
either
the
F
or
H
LUT
Y
comes
from
either
the
G
or
H
LUT
These
outputs
can
be
connected
to
other
CLBs
via
the
routing
channels
The
CLB
also
contains
two
flip
flops
Depending
on
the
configuration
the
flip
flop
inputs
may
come
from
DIN
or
from
the
F
G
or
H
LUT
The
flip
flop
outputs
XQ
and
YQ
also
can
be
connected
to
other
CLBs
via
the
routing
channels
In
summary
the
CLB
can
perform
up
to
two
combinational
and
or
two
registered
functions
All
of
the
functions
can
involve
at
least
four
variables
and
some
can
involve
up
to
nine
The
designer
configures
an
FPGA
by
first
creating
a
schematic
or
HDL
description
of
the
design
The
design
is
then
synthesized
onto
the
FPGA
The
synthesis
tool
determines
how
the
LUTs
multiplexers
and
routing
channels
should
be
configured
to
perform
the
specified
functions
This
configuration
information
is
then
downloaded
to
the
FPGA
Because
Xilinx
FPGAs
store
their
configuration
information
in
SRAM
they
can
be
easily
reprogrammed
They
may
download
the
SRAM
contents
from
a
computer
in
the
laboratory
or
from
an
EEPROM
chip
when
the
CHAPTER
FIVE
Digital
Building
Blocks
Figure
Spartan
CLB
system
is
turned
on
Some
manufacturers
include
EEPROM
directly
on
the
FPGA
or
use
one
time
programmable
fuses
to
configure
the
FPGA
Example
FUNCTIONS
BUILT
USING
CLBS
Explain
how
to
configure
a
CLB
to
perform
the
following
functions
a
and
b
Y
JKLMPQR
c
a
divide
by
counter
with
binary
state
encoding
see
Figure
a
Solution
a
Configure
the
F
LUT
to
compute
X
and
the
G
LUT
to
compute
Y
as
shown
in
Figure
Inputs
F
F
and
F
are
A
B
and
C
respectively
these
connections
are
set
by
the
routing
channels
Inputs
G
and
G
are
A
and
B
F
G
and
G
are
don
t
cares
and
may
be
connected
to
Configure
the
final
multiplexers
to
select
X
from
the
F
LUT
and
Y
from
the
G
LUT
In
general
a
CLB
can
compute
any
two
functions
of
up
to
four
variables
each
in
this
fashion
b
Configure
the
F
LUT
to
compute
F
JKLM
and
the
G
LUT
to
compute
G
PQR
Then
configure
the
H
LUT
to
compute
H
FG
Configure
the
final
multiplexer
to
select
Y
from
the
H
LUT
This
configuration
is
shown
in
Figure
In
general
a
CLB
can
compute
certain
functions
of
up
to
nine
variables
in
this
way
c
The
FSM
has
two
bits
of
state
S
and
one
output
Y
The
next
state
depends
on
the
two
bits
of
current
state
Use
the
F
LUT
and
G
LUT
to
compute
the
next
state
from
the
current
state
as
shown
in
Figure
Use
the
two
flipflops
to
hold
this
state
The
flip
flops
have
a
dedicated
reset
input
from
the
SR
signal
in
the
CLB
The
registered
outputs
are
fed
back
to
the
inputs
using
the
routing
channels
as
indicated
by
the
dashed
blue
lines
In
general
another
CLB
might
be
necessary
to
compute
the
output
Y
However
in
this
case
Y
so
Y
can
come
from
the
same
F
LUT
used
to
compute
Hence
the
entire
FSM
fits
in
a
single
CLB
In
general
an
FSM
requires
at
least
one
CLB
for
every
two
bits
of
state
and
it
may
require
more
CLBs
for
the
output
or
next
state
logic
if
they
are
too
complex
to
fit
in
a
single
LUT
S
S
X
ABC
ABC
Y
AB
Logic
Arrays
F
F
F
F
F
F
F
F
F
X
X
X
X
X
X
X
X
F
A
B
C
X
G
G
G
G
X
X
X
X
X
X
X
X
G
A
B
Y
G
G
G
G
G
A
B
A
B
C
Y
X
Figure
CLB
configuration
for
two
functions
of
up
to
four
inputs
each
Chapter
Example
CLB
DELAY
Alyssa
P
Hacker
is
building
a
finite
state
machine
that
must
run
at
MHz
She
uses
a
Spartan
FPGA
with
the
following
specifications
tCLB
ns
per
CLB
tsetup
ns
and
tpcq
ns
for
all
flip
flops
What
is
the
maximum
number
of
CLBs
her
design
can
use
You
can
ignore
interconnect
delay
Solution
Alyssa
uses
Equation
to
solve
for
the
maximum
propagation
delay
of
the
logic
tpd
Tc
tpcq
tsetup
Thus
tpd
ns
ns
ns
so
tpd
ns
The
delay
of
each
CLB
tCLB
is
ns
and
the
maximum
number
of
CLBs
N
is
NtCLB
ns
Thus
N
CHAPTER
FIVE
Digital
Building
Blocks
F
F
F
F
F
F
F
F
F
X
X
X
X
X
X
X
X
F
S
S
G
G
G
G
X
X
X
X
X
X
X
X
G
G
G
G
G
G
S
YQ
XQ
S
S
S
S
S
clk
clk
Reset
Reset
Y
S
S
Figure
CLB
configuration
for
FSM
with
two
bits
of
state
F
F
F
F
F
F
F
F
F
F
K
L
M
G
G
G
G
G
P
Q
G
G
G
G
G
P
Q
R
K
L
M
J
J
R
Y
X
X
X
X
X
X
X
X
H
FGH
X
X
X
X
G
H
F
H
Y
Figure
CLB
configuration
for
one
function
of
more
than
four
inputs
Chapte
Array
Implementations
To
minimize
their
size
and
cost
ROMs
and
PLAs
commonly
use
pseudonMOS
or
dynamic
circuits
see
Section
instead
of
conventional
logic
gates
Figure
a
shows
the
dot
notation
for
a
bit
ROM
that
performs
the
following
functions
X
A
B
and
Z
These
are
the
same
functions
as
those
of
Figure
with
the
address
inputs
renamed
A
and
B
and
the
data
outputs
renamed
X
Y
and
Z
The
pseudo
nMOS
implementation
is
given
in
Figure
b
Each
decoder
output
is
connected
to
the
gates
of
the
nMOS
transistors
in
its
row
Remember
that
in
pseudo
nMOS
circuits
the
weak
pMOS
transistor
pulls
the
output
HIGH
only
if
there
is
no
path
to
GND
through
the
pulldown
nMOS
network
Pull
down
transistors
are
placed
at
every
junction
without
a
dot
The
dots
from
the
dot
notation
diagram
of
Figure
a
are
left
faintly
visible
in
Figure
b
for
easy
comparison
The
weak
pull
up
transistors
pull
the
output
HIGH
for
each
wordline
without
a
pull
down
transistor
For
example
when
AB
the
wordline
is
HIGH
and
transistors
on
X
and
Z
turn
on
and
pull
those
outputs
LOW
The
Y
output
has
no
transistor
connecting
to
the
wordline
so
Y
is
pulled
HIGH
by
the
weak
pull
up
PLAs
can
also
be
built
using
pseudo
nMOS
circuits
as
shown
in
Figure
for
the
PLA
from
Figure
Pull
down
nMOS
transistors
are
placed
on
the
complement
of
dotted
literals
in
the
AND
array
and
on
dotted
rows
in
the
OR
array
The
columns
in
the
OR
array
are
sent
through
an
inverter
before
they
are
fed
to
the
output
bits
Again
the
blue
dots
from
the
dot
notation
diagram
of
Figure
are
left
faintly
visible
in
Figure
for
easy
comparison
Y
A
B
AB
Logic
Arrays
Decoder
A
A
X
a
A
B
Y
Z
Decoder
A
A
A
B
weak
b
XYZ
Figure
ROM
implementation
a
dot
notation
b
pseudo
nMOS
circuit
Many
ROMs
and
PLAs
use
dynamic
circuits
in
place
of
pseudo
nMOS
circuits
Dynamic
gates
turn
the
pMOS
transistor
ON
for
only
part
of
the
time
saving
power
when
the
pMOS
is
OFF
and
the
result
is
not
needed
Aside
from
this
dynamic
and
pseudo
nMOS
memory
arrays
are
similar
in
design
and
behavior
Chapt
SUMMARY
This
chapter
introduced
digital
building
blocks
used
in
many
digital
systems
These
blocks
include
arithmetic
circuits
such
as
adders
subtractors
comparators
shifters
multipliers
and
dividers
sequential
circuits
such
as
counters
and
shift
registers
and
arrays
for
memory
and
logic
The
chapter
also
explored
fixed
point
and
floating
point
representations
of
fractional
numbers
In
Chapter
we
use
these
building
blocks
to
build
a
microprocessor
Adders
form
the
basis
of
most
arithmetic
circuits
A
half
adder
adds
two
bit
inputs
A
and
B
and
produces
a
sum
and
a
carry
out
A
full
adder
extends
the
half
adder
to
also
accept
a
carry
in
N
full
adders
can
be
cascaded
to
form
a
carry
propagate
adder
CPA
that
adds
two
N
bit
numbers
This
type
of
CPA
is
called
a
ripple
carry
adder
because
the
carry
ripples
through
each
of
the
full
adders
Faster
CPAs
can
be
constructed
using
lookahead
or
prefix
techniques
A
subtractor
negates
the
second
input
and
adds
it
to
the
first
A
magnitude
comparator
subtracts
one
number
from
another
and
determines
the
relative
value
based
on
the
sign
of
the
result
A
multiplier
forms
partial
products
using
AND
gates
then
sums
these
bits
using
full
adders
A
divider
repeatedly
subtracts
the
divisor
from
the
partial
remainder
and
checks
the
sign
of
the
difference
to
determine
the
quotient
bits
A
counter
uses
an
adder
and
a
register
to
increment
a
running
count
Fractional
numbers
are
represented
using
fixed
point
or
floating
point
forms
Fixed
point
numbers
are
analogous
to
decimals
and
floating
point
numbers
are
analogous
to
scientific
notation
Fixed
point
numbers
use
ordinary
arithmetic
circuits
whereas
floating
point
numbers
require
more
elaborate
hardware
to
extract
and
process
the
sign
exponent
and
mantissa
CHAPTER
FIVE
Digital
Building
Blocks
X
Y
ABC
AB
ABC
ABC
AND
Array
OR
Array
weak
weak
Figure
bit
PLA
using
pseudo
nMOS
circuits
Large
memories
are
organized
into
arrays
of
words
The
memories
have
one
or
more
ports
to
read
and
or
write
the
words
Volatile
memories
such
as
SRAM
and
DRAM
lose
their
state
when
the
power
is
turned
off
SRAM
is
faster
than
DRAM
but
requires
more
transistors
A
register
file
is
a
small
multiported
SRAM
array
Nonvolatile
memories
called
ROMs
retain
their
state
indefinitely
Despite
their
names
most
modern
ROMs
can
be
written
Arrays
are
also
a
regular
way
to
build
logic
Memory
arrays
can
be
used
as
lookup
tables
to
perform
combinational
functions
PLAs
are
composed
of
dedicated
connections
between
configurable
AND
and
OR
arrays
they
only
implement
combinational
logic
FPGAs
are
composed
of
many
small
lookup
tables
and
registers
they
implement
combinational
and
sequential
logic
The
lookup
table
contents
and
their
interconnections
can
be
configured
to
perform
any
logic
function
Modern
FPGAs
are
easy
to
reprogram
and
are
large
and
cheap
enough
to
build
highly
sophisticated
digital
systems
so
they
are
widely
used
in
low
and
medium
volume
commercial
products
as
well
as
in
education
Summary
EXERCISES
Exercise
What
is
the
delay
for
the
following
types
of
bit
adders
Assume
that
each
two
input
gate
delay
is
ps
and
that
a
full
adder
delay
is
ps
a
a
ripple
carry
adder
b
a
carry
lookahead
adder
with
bit
blocks
c
a
prefix
adder
Exercise
Design
two
adders
a
bit
ripple
carry
adder
and
a
bit
carrylookahead
adder
with
bit
blocks
Use
only
two
input
gates
Each
two
input
gate
is
m
has
a
ps
delay
and
has
pF
of
total
gate
capacitance
You
may
assume
that
the
static
power
is
negligible
a
Compare
the
area
delay
and
power
of
the
adders
operating
at
MHz
b
Discuss
the
trade
offs
between
power
area
and
delay
Exercise
Explain
why
a
designer
might
choose
to
use
a
ripple
carry
adder
instead
of
a
carry
lookahead
adder
Exercise
Design
the
bit
prefix
adder
of
Figure
in
an
HDL
Simulate
and
test
your
module
to
prove
that
it
functions
correctly
Exercise
The
prefix
network
shown
in
Figure
uses
black
cells
to
compute
all
of
the
prefixes
Some
of
the
block
propagate
signals
are
not
actually
necessary
Design
a
gray
cell
that
receives
G
and
P
signals
for
bits
i
k
and
k
j
but
produces
only
Gi
j
not
Pi
j
Redraw
the
prefix
network
replacing
black
cells
with
gray
cells
wherever
possible
Exercise
The
prefix
network
shown
in
Figure
is
not
the
only
way
to
calculate
all
of
the
prefixes
in
logarithmic
time
The
Kogge
Stone
network
is
another
common
prefix
network
that
performs
the
same
function
using
a
different
connection
of
black
cells
Research
Kogge
Stone
adders
and
draw
a
schematic
similar
to
Figure
showing
the
connection
of
black
cells
in
a
Kogge
Stone
adder
Exercise
Recall
that
an
N
input
priority
encoder
has
log
N
outputs
that
encodes
which
of
the
N
inputs
gets
priority
see
Exercise
a
Design
an
N
input
priority
encoder
that
has
delay
that
increases
logarithmically
with
N
Sketch
your
design
and
give
the
delay
of
the
circuit
in
terms
of
the
delay
of
its
circuit
elements
b
Code
your
design
in
HDL
Simulate
and
test
your
module
to
prove
that
it
functions
correctly
CHAPTER
FIVE
Digital
Building
Blocks
C
Exercise
Design
the
following
comparators
for
bit
numbers
Sketch
the
schematics
a
not
equal
b
greater
than
c
less
than
or
equal
to
Exercise
Design
the
bit
ALU
shown
in
Figure
using
your
favorite
HDL
You
can
make
the
top
level
module
either
behavioral
or
structural
Exercise
Add
an
Overflow
output
to
the
bit
ALU
from
Exercise
The
output
is
TRUE
when
the
result
of
the
adder
overflows
Otherwise
it
is
FALSE
a
Write
a
Boolean
equation
for
the
Overflow
output
b
Sketch
the
Overflow
circuit
c
Design
the
modified
ALU
in
an
HDL
Exercise
Add
a
Zero
output
to
the
bit
ALU
from
Exercise
The
output
is
TRUE
when
Y
Exercise
Write
a
testbench
to
test
the
bit
ALU
from
Exercise
or
Then
use
it
to
test
the
ALU
Include
any
test
vector
files
necessary
Be
sure
to
test
enough
corner
cases
to
convince
a
reasonable
skeptic
that
the
ALU
functions
correctly
Exercise
Design
a
shifter
that
always
shifts
a
bit
input
left
by
bits
The
input
and
output
are
both
bits
Explain
the
design
in
words
and
sketch
a
schematic
Implement
your
design
in
your
favourite
HDL
Exercise
Design
bit
left
and
right
rotators
Sketch
a
schematic
of
your
design
Implement
your
design
in
your
favourite
HDL
Exercise
Design
an
bit
left
shifter
using
only
multiplexers
The
shifter
accepts
an
bit
input
A
and
a
bit
shift
amount
shamt
It
produces
an
bit
output
Y
Sketch
the
schematic
Exercise
Explain
how
to
build
any
N
bit
shifter
or
rotator
using
only
Nlog
N
multiplexers
Exercise
The
funnel
shifter
in
Figure
can
perform
any
N
bit
shift
or
rotate
operation
It
shifts
a
N
bit
input
right
by
k
bits
The
output
Y
is
the
N
least
significant
bits
of
the
result
The
most
significant
N
bits
of
the
input
are
Exercises
Ch
called
B
and
the
least
significant
N
bits
are
called
C
By
choosing
appropriate
values
of
B
C
and
k
the
funnel
shifter
can
perform
any
type
of
shift
or
rotate
Explain
what
these
values
should
be
in
terms
of
A
shamt
and
N
for
a
logical
right
shift
of
A
by
shamt
b
arithmetic
right
shift
of
A
by
shamt
c
left
shift
of
A
by
shamt
d
right
rotate
of
A
by
shamt
e
left
rotate
of
A
by
shamt
CHAPTER
FIVE
Digital
Building
Blocks
B
C
k
N
k
N
N
Y
N
Figure
Funnel
shifter
Exercise
Find
the
critical
path
for
the
multiplier
from
Figure
in
terms
of
an
AND
gate
delay
tAND
and
a
full
adder
delay
tFA
What
is
the
delay
of
an
N
N
multiplier
built
in
the
same
way
Exercise
Design
a
multiplier
that
handles
two
s
complement
numbers
Exercise
A
sign
extension
unit
extends
a
two
s
complement
number
from
M
to
N
N
M
bits
by
copying
the
most
significant
bit
of
the
input
into
the
upper
bits
of
the
output
see
Section
It
receives
an
M
bit
input
A
and
produces
an
N
bit
output
Y
Sketch
a
circuit
for
a
sign
extension
unit
with
a
bit
input
and
an
bit
output
Write
the
HDL
for
your
design
Exercise
A
zero
extension
unit
extends
an
unsigned
number
from
M
to
N
bits
N
M
by
putting
zeros
in
the
upper
bits
of
the
output
Sketch
a
circuit
for
a
zero
extension
unit
with
a
bit
input
and
an
bit
output
Write
the
HDL
for
your
design
Exercise
Compute
in
binary
using
the
standard
division
algorithm
from
elementary
school
Show
your
work
Exercise
What
is
the
range
of
numbers
that
can
be
represented
by
the
following
number
systems
a
bit
unsigned
fixed
point
numbers
with
integer
bits
and
fraction
bits
b
bit
sign
and
magnitude
fixed
point
numbers
with
integer
bits
and
fraction
bits
c
bit
two
s
complement
fixed
point
numbers
with
integer
bits
and
fraction
bits
Exercise
Express
the
following
base
numbers
in
bit
fixed
point
sign
magnitude
format
with
eight
integer
bits
and
eight
fraction
bits
Express
your
answer
in
hexadecimal
a
b
c
Exercise
Express
the
base
numbers
in
Exercise
in
bit
fixedpoint
two
s
complement
format
with
eight
integer
bits
and
eight
fraction
bits
Express
your
answer
in
hexadecimal
Exercise
Express
the
base
numbers
in
Exercise
in
IEEE
single
precision
floating
point
format
Express
your
answer
in
hexadecimal
Exercise
Convert
the
following
two
s
complement
binary
fixed
point
numbers
to
base
a
b
c
Exercise
When
adding
two
floating
point
numbers
the
number
with
the
smaller
exponent
is
shifted
Why
is
this
Explain
in
words
and
give
an
example
to
justify
your
explanation
Exercise
Add
the
following
IEEE
single
precision
floating
point
numbers
a
C
D
b
C
D
DC
c
FBE
FF
DFDE
Why
is
the
result
counterintuitive
Explain
Exercises
Ch
Exercise
Expand
the
steps
in
section
for
performing
floating
point
addition
to
work
for
negative
as
well
as
positive
floating
point
numbers
Exercise
Consider
IEEE
single
precision
floating
point
numbers
a
How
many
numbers
can
be
represented
by
IEEE
single
precision
floating
point
format
You
need
not
count
or
NaN
b
How
many
additional
numbers
could
be
represented
if
and
NaN
were
not
represented
c
Explain
why
and
NaN
are
given
special
representations
Exercise
Consider
the
following
decimal
numbers
and
a
Write
the
two
numbers
using
single
precision
floating
point
notation
Give
your
answers
in
hexadecimal
b
Perform
a
magnitude
comparison
of
the
two
bit
numbers
from
part
a
In
other
words
interpret
the
two
bit
numbers
as
two
s
complement
numbers
and
compare
them
Does
the
integer
comparison
give
the
correct
result
c
You
decide
to
come
up
with
a
new
single
precision
floating
point
notation
Everything
is
the
same
as
the
IEEE
single
precision
floating
point
standard
except
that
you
represent
the
exponent
using
two
s
complement
instead
of
a
bias
Write
the
two
numbers
using
your
new
standard
Give
your
answers
in
hexadecimal
e
Does
integer
comparison
work
with
your
new
floating
point
notation
from
part
d
f
Why
is
it
convenient
for
integer
comparison
to
work
with
floating
point
numbers
Exercise
Design
a
single
precision
floating
point
adder
using
your
favorite
HDL
Before
coding
the
design
in
an
HDL
sketch
a
schematic
of
your
design
Simulate
and
test
your
adder
to
prove
to
a
skeptic
that
it
functions
correctly
You
may
consider
positive
numbers
only
and
use
round
toward
zero
truncate
You
may
also
ignore
the
special
cases
given
in
Table
Exercise
In
this
problem
you
will
explore
the
design
of
a
bit
floatingpoint
multiplier
The
multiplier
has
two
bit
floating
point
inputs
and
produces
a
bit
floating
point
output
You
may
consider
positive
numbers
only
CHAPTER
FIVE
Digital
Building
Blocks
and
use
round
toward
zero
truncate
You
may
also
ignore
the
special
cases
given
in
Table
a
Write
the
steps
necessary
to
perform
bit
floating
point
multiplication
b
Sketch
the
schematic
of
a
bit
floating
point
multiplier
c
Design
a
bit
floating
point
multiplier
in
an
HDL
Simulate
and
test
your
multiplier
to
prove
to
a
skeptic
that
it
functions
correctly
Exercise
In
this
problem
you
will
explore
the
design
of
a
bit
prefix
adder
a
Sketch
a
schematic
of
your
design
b
Design
the
bit
prefix
adder
in
an
HDL
Simulate
and
test
your
adder
to
prove
that
it
functions
correctly
c
What
is
the
delay
of
your
bit
prefix
adder
from
part
a
Assume
that
each
two
input
gate
delay
is
ps
d
Design
a
pipelined
version
of
the
bit
prefix
adder
Sketch
the
schematic
of
your
design
How
fast
can
your
pipelined
prefix
adder
run
Make
the
design
run
as
fast
as
possible
e
Design
the
pipelined
bit
prefix
adder
in
an
HDL
Exercise
An
incrementer
adds
to
an
N
bit
number
Build
an
bit
incrementer
using
half
adders
Exercise
Build
a
bit
synchronous
Up
Down
counter
The
inputs
are
Reset
and
Up
When
Reset
is
the
outputs
are
all
Otherwise
when
Up
the
circuit
counts
up
and
when
Up
the
circuit
counts
down
Exercise
Design
a
bit
counter
that
adds
at
each
clock
edge
The
counter
has
reset
and
clock
inputs
Upon
reset
the
counter
output
is
all
Exercise
Modify
the
counter
from
Exercise
such
that
the
counter
will
either
increment
by
or
load
a
new
bit
value
D
on
each
clock
edge
depending
on
a
control
signal
PCSrc
When
PCSrc
the
counter
loads
the
new
value
D
Exercise
An
N
bit
Johnson
counter
consists
of
an
N
bit
shift
register
with
a
reset
signal
The
output
of
the
shift
register
Sout
is
inverted
and
fed
back
to
the
input
Sin
When
the
counter
is
reset
all
of
the
bits
are
cleared
to
a
Show
the
sequence
of
outputs
Q
produced
by
a
bit
Johnson
counter
starting
immediately
after
the
counter
is
reset
Exercises
Cha
b
How
many
cycles
elapse
until
an
N
bit
Johnson
counter
repeats
its
sequence
Explain
c
Design
a
decimal
counter
using
a
bit
Johnson
counter
ten
AND
gates
and
inverters
The
decimal
counter
has
a
clock
a
reset
and
ten
one
hot
outputs
Y
When
the
counter
is
reset
Y
is
asserted
On
each
subsequent
cycle
the
next
output
should
be
asserted
After
ten
cycles
the
counter
should
repeat
Sketch
a
schematic
of
the
decimal
counter
d
What
advantages
might
a
Johnson
counter
have
over
a
conventional
counter
Exercise
Write
the
HDL
for
a
bit
scannable
flip
flop
like
the
one
shown
in
Figure
Simulate
and
test
your
HDL
module
to
prove
that
it
functions
correctly
Exercise
The
English
language
has
a
good
deal
of
redundancy
that
allows
us
to
reconstruct
garbled
transmissions
Binary
data
can
also
be
transmitted
in
redundant
form
to
allow
error
correction
For
example
the
number
could
be
coded
as
and
the
number
could
be
coded
as
The
value
could
then
be
sent
over
a
noisy
channel
that
might
flip
up
to
two
of
the
bits
The
receiver
could
reconstruct
the
original
data
because
a
will
have
at
least
three
of
the
five
received
bits
as
s
similarly
a
will
have
at
least
three
s
a
Propose
an
encoding
to
send
or
encoded
using
five
bits
of
information
such
that
all
errors
that
corrupt
one
bit
of
the
encoded
data
can
be
corrected
Hint
the
encodings
and
for
and
respectively
will
not
work
b
Design
a
circuit
that
receives
your
five
bit
encoded
data
and
decodes
it
to
or
even
if
one
bit
of
the
transmitted
data
has
been
changed
c
Suppose
you
wanted
to
change
to
an
alternative
bit
encoding
How
might
you
implement
your
design
to
make
it
easy
to
change
the
encoding
without
having
to
use
different
hardware
Exercise
Flash
EEPROM
simply
called
Flash
memory
is
a
fairly
recent
invention
that
has
revolutionized
consumer
electronics
Research
and
explain
how
Flash
memory
works
Use
a
diagram
illustrating
the
floating
gate
Describe
how
a
bit
in
the
memory
is
programmed
Properly
cite
your
sources
Exercise
The
extraterrestrial
life
project
team
has
just
discovered
aliens
living
on
the
bottom
of
Mono
Lake
They
need
to
construct
a
circuit
to
classify
the
aliens
by
potential
planet
of
origin
based
on
measured
features
CHAPTER
FIVE
Digital
Building
Blocks
available
from
the
NASA
probe
greenness
brownness
sliminess
and
ugliness
Careful
consultation
with
xenobiologists
leads
to
the
following
conclusions
If
the
alien
is
green
and
slimy
or
ugly
brown
and
slimy
it
might
be
from
Mars
If
the
critter
is
ugly
brown
and
slimy
or
green
and
neither
ugly
nor
slimy
it
might
be
from
Venus
If
the
beastie
is
brown
and
neither
ugly
nor
slimy
or
is
green
and
slimy
it
might
be
from
Jupiter
Note
that
this
is
an
inexact
science
for
example
a
life
form
which
is
mottled
green
and
brown
and
is
slimy
but
not
ugly
might
be
from
either
Mars
or
Jupiter
a
Program
a
PLA
to
identify
the
alien
You
may
use
dot
notation
b
Program
a
ROM
to
identify
the
alien
You
may
use
dot
notation
c
Implement
your
design
in
an
HDL
Exercise
Implement
the
following
functions
using
a
single
ROM
Use
dot
notation
to
indicate
the
ROM
contents
a
b
c
Z
A
B
C
D
Exercise
Implement
the
functions
from
Exercise
using
an
PLA
You
may
use
dot
notation
Exercise
Specify
the
size
of
a
ROM
that
you
could
use
to
program
each
of
the
following
combinational
circuits
Is
using
a
ROM
to
implement
these
functions
a
good
design
choice
Explain
why
or
why
not
a
a
bit
adder
subtractor
with
Cin
and
Cout
b
an
multiplier
c
a
bit
priority
encoder
see
Exercise
Exercise
Consider
the
ROM
circuits
in
Figure
For
each
row
can
the
circuit
in
column
I
be
replaced
by
an
equivalent
circuit
in
column
II
by
proper
programming
of
the
latter
s
ROM
Y
AB
BD
X
AB
BCD
AB
Exercises
Chapte
CHAPTER
FIVE
Digital
Building
Blocks
K
I
A
RD
ROM
CLK
N
N
K
In
A
RD
ROM
N
N
A
RD
ROM
N
A
RD
ROM
K
CLK
K
A
RD
Out
ROM
K
K
CLK
K
In
A
RD
ROM
K
K
Out
CLK
K
In
A
RD
ROM
K
K
N
N
A
RD
ROM
N
Out
CLK
K
In
A
RD
ROM
K
K
N
N
Out
CLK
N
In
A
RD
ROM
N
N
N
A
RD
ROM
N
Out
CLK
N
In
A
RD
ROM
N
N
N
Out
II
a
b
c
d
Figure
ROM
circuits
Exercise
Give
an
example
of
a
nine
input
function
that
can
be
performed
using
only
one
Spartan
FPGA
CLB
Give
an
example
of
an
eight
input
function
that
cannot
be
performed
using
only
one
CLB
Exercise
How
many
Spartan
FPGA
CLBs
are
required
to
perform
each
of
the
following
functions
Show
how
to
configure
one
or
more
CLBs
to
perform
the
function
You
should
be
able
to
do
this
by
inspection
without
performing
logic
synthesis
a
The
combinational
function
from
Exercise
c
b
The
combinational
function
from
Exercise
c
c
The
two
output
function
from
Exercise
d
The
function
from
Exercise
e
A
four
input
priority
encoder
see
Exercise
f
An
eight
input
priority
encoder
see
Exercise
g
A
decoder
h
A
bit
carry
propagate
adder
with
no
carry
in
or
out
i
The
FSM
from
Exercise
j
The
Gray
code
counter
from
Exercise
Exercise
Consider
the
Spartan
CLB
shown
in
Figure
It
has
the
following
specifications
tpd
tcd
ns
per
CLB
tsetup
ns
thold
ns
and
tpcq
ns
for
all
flip
flops
a
What
is
the
minimum
number
of
Spartan
CLBs
required
to
implement
the
FSM
of
Figure
b
Without
clock
skew
what
is
the
fastest
clock
frequency
at
which
this
FSM
will
run
reliably
c
With
ns
of
clock
skew
what
is
the
fastest
frequency
at
which
the
FSM
will
run
reliably
Exercise
You
would
like
to
use
an
FPGA
to
implement
an
M
M
sorter
with
a
color
sensor
and
motors
to
put
red
candy
in
one
jar
and
green
candy
in
another
The
design
is
to
be
implemented
as
an
FSM
using
a
Spartan
XC
S
FPGA
a
chip
from
the
Spartan
series
family
It
is
considerably
faster
than
the
original
Spartan
FPGA
According
to
the
data
sheet
the
FPGA
has
timing
characteristics
shown
in
Table
Assume
that
the
design
is
small
enough
that
wire
delay
is
negligible
Exercises
Table
Spartan
XC
S
timing
Name
Value
ns
tpcq
tsetup
thold
tpd
per
CLB
tskew
You
would
like
your
FSM
to
run
at
MHz
What
is
the
maximum
number
of
CLBs
on
the
critical
path
What
is
the
fastest
speed
at
which
the
FSM
will
run
Chapt
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
What
is
the
largest
possible
result
of
multiplying
two
unsigned
N
bit
numbers
Question
Binary
coded
decimal
BCD
representation
uses
four
bits
to
encode
each
decimal
digit
For
example
is
represented
as
BCD
Explain
in
words
why
processors
might
use
BCD
representation
Question
Design
hardware
to
add
two
bit
unsigned
BCD
numbers
see
Question
Sketch
a
schematic
for
your
design
and
write
an
HDL
module
for
the
BCD
adder
The
inputs
are
A
B
and
Cin
and
the
outputs
are
S
and
Cout
Cin
and
Cout
are
bit
carries
and
A
B
and
S
are
bit
BCD
numbers
CHAPTER
FIVE
Digital
Building
Blocks
Introduction
Assembly
Language
Machine
Language
Programming
Addressing
Modes
Lights
Camera
Action
Compiling
Assembling
and
Loading
Odds
and
Ends
Real
World
Perspective
IA
Architecture
Summary
Exercises
Interview
Questions
Architecture
INTRODUCTION
The
previous
chapters
introduced
digital
design
principles
and
building
blocks
In
this
chapter
we
jump
up
a
few
levels
of
abstraction
to
define
the
architecture
of
a
computer
see
Figure
The
architecture
is
the
programmer
s
view
of
a
computer
It
is
defined
by
the
instruction
set
language
and
operand
locations
registers
and
memory
Many
different
architectures
exist
such
as
IA
MIPS
SPARC
and
PowerPC
The
first
step
in
understanding
any
computer
architecture
is
to
learn
its
language
The
words
in
a
computer
s
language
are
called
instructions
The
computer
s
vocabulary
is
called
the
instruction
set
All
programs
running
on
a
computer
use
the
same
instruction
set
Even
complex
software
applications
such
as
word
processing
and
spreadsheet
applications
are
eventually
compiled
into
a
series
of
simple
instructions
such
as
add
subtract
and
jump
Computer
instructions
indicate
both
the
operation
to
perform
and
the
operands
to
use
The
operands
may
come
from
memory
from
registers
or
from
the
instruction
itself
Computer
hardware
understands
only
s
and
s
so
instructions
are
encoded
as
binary
numbers
in
a
format
called
machine
language
Just
as
we
use
letters
to
encode
human
language
computers
use
binary
numbers
to
encode
machine
language
Microprocessors
are
digital
systems
that
read
and
execute
machine
language
instructions
However
humans
consider
reading
machine
language
to
be
tedious
so
we
prefer
to
represent
the
instructions
in
a
symbolic
format
called
assembly
language
The
instruction
sets
of
different
architectures
are
more
like
different
dialects
than
different
languages
Almost
all
architectures
define
basic
instructions
such
as
add
subtract
and
jump
that
operate
on
memory
or
registers
Once
you
have
learned
one
instruction
set
understanding
others
is
fairly
straightforward
A
computer
architecture
does
not
define
the
underlying
hardware
implementation
Often
many
different
hardware
implementations
of
a
single
architecture
exist
For
example
Intel
and
Advanced
Micro
Devices
AMD
sell
various
microprocessors
belonging
to
the
same
IA
architecture
They
all
can
run
the
same
programs
but
they
use
different
underlying
hardware
and
therefore
offer
trade
offs
in
performance
price
and
power
Some
microprocessors
are
optimized
for
high
performance
servers
whereas
others
are
optimized
for
long
battery
life
in
laptop
computers
The
specific
arrangement
of
registers
memories
ALUs
and
other
building
blocks
to
form
a
microprocessor
is
called
the
microarchitecture
and
will
be
the
subject
of
Chapter
Often
many
different
microarchitectures
exist
for
a
single
architecture
In
this
text
we
introduce
the
MIPS
architecture
that
was
first
developed
by
John
Hennessy
and
his
colleagues
at
Stanford
in
the
s
MIPS
processors
are
used
by
among
others
Silicon
Graphics
Nintendo
and
Cisco
We
start
by
introducing
the
basic
instructions
operand
locations
and
machine
language
formats
We
then
introduce
more
instructions
used
in
common
programming
constructs
such
as
branches
loops
array
manipulations
and
procedure
calls
Throughout
the
chapter
we
motivate
the
design
of
the
MIPS
architecture
using
four
principles
articulated
by
Patterson
and
Hennessy
simplicity
favors
regularity
make
the
common
case
fast
smaller
is
faster
and
good
design
demands
good
compromises
ASSEMBLY
LANGUAGE
Assembly
language
is
the
human
readable
representation
of
the
computer
s
native
language
Each
assembly
language
instruction
specifies
both
the
operation
to
perform
and
the
operands
on
which
to
operate
We
introduce
simple
arithmetic
instructions
and
show
how
these
operations
are
written
in
assembly
language
We
then
define
the
MIPS
instruction
operands
registers
memory
and
constants
We
assume
that
you
already
have
some
familiarity
with
a
high
level
programming
language
such
as
C
C
or
Java
These
languages
are
practically
identical
for
most
of
the
examples
in
this
chapter
but
where
they
differ
we
will
use
C
Instructions
The
most
common
operation
computers
perform
is
addition
Code
Example
shows
code
for
adding
variables
b
and
c
and
writing
the
result
to
a
The
program
is
shown
on
the
left
in
a
high
level
language
using
the
syntax
of
C
C
and
Java
and
then
rewritten
on
the
right
in
MIPS
assembly
language
Note
that
statements
in
a
C
program
end
with
a
semicolon
CHAPTER
SIX
Architecture
What
is
the
best
architecture
to
study
when
first
learning
the
subject
Commercially
successful
architectures
such
as
IA
are
satisfying
to
study
because
you
can
use
them
to
write
programs
on
real
computers
Unfortunately
many
of
these
architectures
are
full
of
warts
and
idiosyncrasies
accumulated
over
years
of
haphazard
development
by
different
engineering
teams
making
the
architectures
difficult
to
understand
and
implement
Many
textbooks
teach
imaginary
architectures
that
are
simplified
to
illustrate
the
key
concepts
We
follow
the
lead
of
David
Patterson
and
John
Hennessy
in
their
text
Computer
Organization
and
Design
by
focusing
on
the
MIPS
architecture
Hundreds
of
millions
of
MIPS
microprocessors
have
shipped
so
the
architecture
is
commercially
very
important
Yet
it
is
a
clean
architecture
with
little
odd
behavior
At
the
end
of
this
chapter
we
briefly
visit
the
IA
architecture
to
compare
and
contrast
it
with
MIPS
Chap
Assembly
Language
High
Level
Code
a
b
c
MIPS
Assembly
Code
add
a
b
c
Code
Example
ADDITION
High
Level
Code
a
b
c
MIPS
Assembly
Code
sub
a
b
c
Code
Example
SUBTRACTION
High
Level
Code
a
b
c
d
single
line
comment
multiple
line
comment
MIPS
Assembly
Code
sub
t
c
d
t
c
d
add
a
b
t
a
b
t
Code
Example
MORE
COMPLEX
CODE
The
first
part
of
the
assembly
instruction
add
is
called
the
mnemonic
and
indicates
what
operation
to
perform
The
operation
is
performed
on
b
and
c
the
source
operands
and
the
result
is
written
to
a
the
destination
operand
Code
Example
shows
that
subtraction
is
similar
to
addition
The
instruction
format
is
the
same
as
the
add
instruction
except
for
the
operation
specification
sub
This
consistent
instruction
format
is
an
example
of
the
first
design
principle
Design
Principle
Simplicity
favors
regularity
Instructions
with
a
consistent
number
of
operands
in
this
case
two
sources
and
one
destination
are
easier
to
encode
and
handle
in
hardware
More
complex
high
level
code
translates
into
multiple
MIPS
instructions
as
shown
in
Code
Example
In
the
high
level
language
examples
single
line
comments
begin
with
and
continue
until
the
end
of
the
line
Multiline
comments
begin
with
and
end
with
In
assembly
language
only
single
line
comments
are
used
They
begin
with
and
continue
until
the
end
of
the
line
The
assembly
language
program
in
Code
Example
requires
a
temporary
variable
t
to
store
the
intermediate
result
Using
multiple
assembly
mnemonic
pronounced
ni
mon
ik
comes
from
the
Greek
word
E
to
remember
The
assembly
language
mnemonic
is
easier
to
remember
than
a
machine
language
pattern
of
s
and
s
representing
the
same
operation
Chapter
language
instructions
to
perform
more
complex
operations
is
an
example
of
the
second
design
principle
of
computer
architecture
Design
Principle
Make
the
common
case
fast
The
MIPS
instruction
set
makes
the
common
case
fast
by
including
only
simple
commonly
used
instructions
The
number
of
instructions
is
kept
small
so
that
the
hardware
required
to
decode
the
instruction
and
its
operands
can
be
simple
small
and
fast
More
elaborate
operations
that
are
less
common
are
performed
using
sequences
of
multiple
simple
instructions
Thus
MIPS
is
a
reduced
instruction
set
computer
RISC
architecture
Architectures
with
many
complex
instructions
such
as
Intel
s
IA
architecture
are
complex
instruction
set
computers
CISC
For
example
IA
defines
a
string
move
instruction
that
copies
a
string
a
series
of
characters
from
one
part
of
memory
to
another
Such
an
operation
requires
many
possibly
even
hundreds
of
simple
instructions
in
a
RISC
machine
However
the
cost
of
implementing
complex
instructions
in
a
CISC
architecture
is
added
hardware
and
overhead
that
slows
down
the
simple
instructions
A
RISC
architecture
minimizes
the
hardware
complexity
and
the
necessary
instruction
encoding
by
keeping
the
set
of
distinct
instructions
small
For
example
an
instruction
set
with
simple
instructions
would
need
log
bits
to
encode
the
operation
An
instruction
set
with
complex
instructions
would
need
log
bits
of
encoding
per
instruction
In
a
CISC
machine
even
though
the
complex
instructions
may
be
used
only
rarely
they
add
overhead
to
all
instructions
even
the
simple
ones
Operands
Registers
Memory
and
Constants
An
instruction
operates
on
operands
In
Code
Example
the
variables
a
b
and
c
are
all
operands
But
computers
operate
on
s
and
s
not
variable
names
The
instructions
need
a
physical
location
from
which
to
retrieve
the
binary
data
Operands
can
be
stored
in
registers
or
memory
or
they
may
be
constants
stored
in
the
instruction
itself
Computers
use
various
locations
to
hold
operands
to
optimize
for
speed
and
data
capacity
Operands
stored
as
constants
or
in
registers
are
accessed
quickly
but
they
hold
only
a
small
amount
of
data
Additional
data
must
be
accessed
from
memory
which
is
large
but
slow
MIPS
is
called
a
bit
architecture
because
it
operates
on
bit
data
The
MIPS
architecture
has
been
extended
to
bits
in
commercial
products
but
we
will
consider
only
the
bit
form
in
this
book
Registers
Instructions
need
to
access
operands
quickly
so
that
they
can
run
fast
But
operands
stored
in
memory
take
a
long
time
to
retrieve
Therefore
CHAPTER
SIX
Architecture
Ch
most
architectures
specify
a
small
number
of
registers
that
hold
commonly
used
operands
The
MIPS
architecture
uses
registers
called
the
register
set
or
register
file
The
fewer
the
registers
the
faster
they
can
be
accessed
This
leads
to
the
third
design
principle
Design
Principle
Smaller
is
faster
Looking
up
information
from
a
small
number
of
relevant
books
on
your
desk
is
a
lot
faster
than
searching
for
the
information
in
the
stacks
at
a
library
Likewise
reading
data
from
a
small
set
of
registers
for
example
is
faster
than
reading
it
from
registers
or
a
large
memory
A
small
register
file
is
typically
built
from
a
small
SRAM
array
see
Section
The
SRAM
array
uses
a
small
decoder
and
bitlines
connected
to
relatively
few
memory
cells
so
it
has
a
shorter
critical
path
than
a
large
memory
does
Code
Example
shows
the
add
instruction
with
register
operands
MIPS
register
names
are
preceded
by
the
sign
The
variables
a
b
and
c
are
arbitrarily
placed
in
s
s
and
s
The
name
s
is
pronounced
register
s
or
dollar
s
The
instruction
adds
the
bit
values
contained
in
s
b
and
s
c
and
writes
the
bit
result
to
s
a
MIPS
generally
stores
variables
in
of
the
registers
s
s
and
t
t
Register
names
beginning
with
s
are
called
saved
registers
Following
MIPS
convention
these
registers
store
variables
such
as
a
b
and
c
Saved
registers
have
special
connotations
when
they
are
used
with
procedure
calls
see
Section
Register
names
beginning
with
t
are
called
temporary
registers
They
are
used
for
storing
temporary
variables
Code
Example
shows
MIPS
assembly
code
using
a
temporary
register
t
to
store
the
intermediate
calculation
of
c
d
Assembly
Language
High
Level
Code
a
b
c
MIPS
Assembly
Code
s
a
s
b
s
c
add
s
s
s
a
b
c
Code
Example
REGISTER
OPERANDS
High
Level
Code
a
b
c
d
MIPS
Assembly
Code
s
a
s
b
s
c
s
d
sub
t
s
s
t
c
d
add
s
s
t
a
b
t
Code
Example
TEMPORARY
REGISTERS
Chapter
qx
Example
TRANSLATING
HIGH
LEVEL
CODE
TO
ASSEMBLY
LANGUAGE
Translate
the
following
high
level
code
into
assembly
language
Assume
variables
a
c
are
held
in
registers
s
s
and
f
j
are
in
s
s
a
b
c
f
g
h
i
j
Solution
The
program
uses
four
assembly
language
instructions
MIPS
assembly
code
s
a
s
b
s
c
s
f
s
g
s
h
s
i
s
j
sub
s
s
s
a
b
c
add
t
s
s
t
g
h
add
t
s
s
t
i
j
sub
s
t
t
f
g
h
i
j
The
Register
Set
The
MIPS
architecture
defines
registers
Each
register
has
a
name
and
a
number
ranging
from
to
Table
lists
the
name
number
and
use
for
each
register
always
contains
the
value
because
this
constant
is
so
frequently
used
in
computer
programs
We
have
also
discussed
the
s
and
t
registers
The
remaining
registers
will
be
described
throughout
this
chapter
CHAPTER
SIX
Architecture
Table
MIPS
register
set
Name
Number
Use
the
constant
value
at
assembler
temporary
v
v
procedure
return
values
a
a
procedure
arguments
t
t
temporary
variables
s
s
saved
variables
t
t
temporary
variables
k
k
operating
system
OS
temporaries
gp
global
pointer
sp
stack
pointer
fp
frame
pointer
ra
procedure
return
address
Chapter
qxd
Memory
If
registers
were
the
only
storage
space
for
operands
we
would
be
confined
to
simple
programs
with
no
more
than
variables
However
data
can
also
be
stored
in
memory
When
compared
to
the
register
file
memory
has
many
data
locations
but
accessing
it
takes
a
longer
amount
of
time
Whereas
the
register
file
is
small
and
fast
memory
is
large
and
slow
For
this
reason
commonly
used
variables
are
kept
in
registers
By
using
a
combination
of
memory
and
registers
a
program
can
access
a
large
amount
of
data
fairly
quickly
As
described
in
Section
memories
are
organized
as
an
array
of
data
words
The
MIPS
architecture
uses
bit
memory
addresses
and
bit
data
words
MIPS
uses
a
byte
addressable
memory
That
is
each
byte
in
memory
has
a
unique
address
However
for
explanation
purposes
only
we
first
introduce
a
word
addressable
memory
and
afterward
describe
the
MIPS
byte
addressable
memory
Figure
shows
a
memory
array
that
is
word
addressable
That
is
each
bit
data
word
has
a
unique
bit
address
Both
the
bit
word
address
and
the
bit
data
value
are
written
in
hexadecimal
in
Figure
For
example
data
xF
F
AC
is
stored
at
memory
address
Hexadecimal
constants
are
written
with
the
prefix
x
By
convention
memory
is
drawn
with
low
memory
addresses
toward
the
bottom
and
high
memory
addresses
toward
the
top
MIPS
uses
the
load
word
instruction
lw
to
read
a
data
word
from
memory
into
a
register
Code
Example
loads
memory
word
into
s
The
lw
instruction
specifies
the
effective
address
in
memory
as
the
sum
of
a
base
address
and
an
offset
The
base
address
written
in
parentheses
in
the
instruction
is
a
register
The
offset
is
a
constant
written
before
the
parentheses
In
Code
Example
the
base
address
Assembly
Language
Data
F
E
E
F
F
AC
ABC
DEF
Word
Address
Word
Word
Word
Word
Figure
Word
addressable
memory
Assembly
Code
This
assembly
code
unlike
MIPS
assumes
word
addressable
memory
lw
s
read
memory
word
into
s
Code
Example
READING
WORD
ADDRESSABLE
MEMORY
is
which
holds
the
value
and
the
offset
is
so
the
lw
instruction
reads
from
memory
address
After
the
load
word
instruction
lw
is
executed
s
holds
the
value
xF
F
AC
which
is
the
data
value
stored
at
memory
address
in
Figure
Similarly
MIPS
uses
the
store
word
instruction
sw
to
write
a
data
word
from
a
register
into
memory
Code
Example
writes
the
contents
of
register
s
into
memory
word
These
examples
have
used
as
the
base
address
for
simplicity
but
remember
that
any
register
can
be
used
to
supply
the
base
address
The
previous
two
code
examples
have
shown
a
computer
architecture
with
a
word
addressable
memory
The
MIPS
memory
model
however
is
byte
addressable
not
word
addressable
Each
data
byte
has
a
unique
address
A
bit
word
consists
of
four
bit
bytes
So
each
word
address
is
a
multiple
of
as
shown
in
Figure
Again
both
the
bit
word
address
and
the
data
value
are
given
in
hexadecimal
Code
Example
shows
how
to
read
and
write
words
in
the
MIPS
byte
addressable
memory
The
word
address
is
four
times
the
word
number
The
MIPS
assembly
code
reads
words
and
and
writes
words
and
The
offset
can
be
written
in
decimal
or
hexadecimal
The
MIPS
architecture
also
provides
the
lb
and
sb
instructions
that
load
and
store
single
bytes
in
memory
rather
than
words
They
are
similar
to
lw
and
sw
and
will
be
discussed
further
in
Section
Byte
addressable
memories
are
organized
in
a
big
endian
or
littleendian
fashion
as
shown
in
Figure
In
both
formats
the
most
significant
byte
MSB
is
on
the
left
and
the
least
significant
byte
LSB
is
on
the
right
In
big
endian
machines
bytes
are
numbered
starting
with
CHAPTER
SIX
Architecture
Assembly
Code
This
assembly
code
unlike
MIPS
assumes
word
addressable
memory
sw
s
write
s
to
memory
word
Code
Example
WRITING
WORD
ADDRESSABLE
MEMORY
Word
Address
Data
C
width
bytes
F
EE
F
F
AC
ABCD
EF
Word
Word
Word
Word
Figure
Byte
addressable
memory
MSB
LSB
AB
C
DEF
Byte
Address
B
A
C
F
EDC
Byte
Address
Word
Address
Big
Endian
Little
Endian
MSB
LSB
Figure
Big
and
littleendian
memory
addressing
C
at
the
big
most
significant
end
In
little
endian
machines
bytes
are
numbered
starting
with
at
the
little
least
significant
end
Word
addresses
are
the
same
in
both
formats
and
refer
to
the
same
four
bytes
Only
the
addresses
of
bytes
within
a
word
differ
Example
BIG
AND
LITTLE
ENDIAN
MEMORY
Suppose
that
s
initially
contains
x
After
the
following
program
is
run
on
a
big
endian
system
what
value
does
s
contain
In
a
little
endian
system
lb
s
loads
the
data
at
byte
address
into
the
least
significant
byte
of
s
lb
is
discussed
in
detail
in
Section
sw
s
lb
s
Solution
Figure
shows
how
big
and
little
endian
machines
store
the
value
x
in
memory
word
After
the
load
byte
instruction
lb
s
s
would
contain
x
on
a
big
endian
system
and
x
on
a
little
endian
system
Assembly
Language
MIPS
Assembly
Code
w
s
read
data
word
xABCDEF
into
s
w
s
read
data
word
x
EE
into
s
w
s
xC
read
data
word
x
F
into
s
sw
s
write
s
to
data
word
sw
s
x
write
s
to
data
word
sw
s
write
s
to
data
word
Code
Example
ACCESSING
BYTE
ADDRESSABLE
MEMORY
The
terms
big
endian
and
littleendian
come
from
Jonathan
Swift
s
Gulliver
s
Travels
first
published
in
under
the
pseudonym
of
Isaac
Bickerstaff
In
his
stories
the
Lilliputian
king
required
his
citizens
the
LittleEndians
to
break
their
eggs
on
the
little
end
The
Big
Endians
were
rebels
who
broke
their
eggs
on
the
big
end
The
terms
were
first
applied
to
computer
architectures
by
Danny
Cohen
in
his
paper
On
Holy
Wars
and
a
Plea
for
Peace
published
on
April
Fools
Day
USC
ISI
IEN
Photo
courtesy
The
Brotherton
Collection
IEEDS
University
Library
SPIM
the
MIPS
simulator
that
comes
with
this
text
uses
the
endianness
of
the
machine
it
is
run
on
For
example
when
using
SPIM
on
an
Intel
IA
machine
the
memory
is
little
endian
With
an
older
Macintosh
or
Sun
SPARC
machine
memory
is
big
endian
Word
Address
Big
Endian
Little
Endian
Byte
Address
Data
Value
Byte
Address
Data
Value
MSB
LSB
MSB
LSB
Figure
Big
endian
and
little
endian
data
storage
IBM
s
PowerPC
formerly
found
in
Macintosh
computers
uses
big
endian
addressing
Intel
s
IA
architecture
found
in
PCs
uses
little
endian
addressing
Some
MIPS
processors
are
little
endian
and
some
are
big
endian
The
choice
of
endianness
is
completely
arbitrary
but
leads
to
hassles
when
sharing
data
between
big
endian
and
littleendian
computers
In
examples
in
this
text
we
will
use
little
endian
format
whenever
byte
ordering
matters
Ch
In
the
MIPS
architecture
word
addresses
for
lw
and
sw
must
be
word
aligned
That
is
the
address
must
be
divisible
by
Thus
the
instruction
lw
s
is
an
illegal
instruction
Some
architectures
such
as
IA
allow
non
word
aligned
data
reads
and
writes
but
MIPS
requires
strict
alignment
for
simplicity
Of
course
byte
addresses
for
load
byte
and
store
byte
lb
and
sb
need
not
be
word
aligned
Constants
Immediates
Load
word
and
store
word
lw
and
sw
also
illustrate
the
use
of
constants
in
MIPS
instructions
These
constants
are
called
immediates
because
their
values
are
immediately
available
from
the
instruction
and
do
not
require
a
register
or
memory
access
Add
immediate
addi
is
another
common
MIPS
instruction
that
uses
an
immediate
operand
addi
adds
the
immediate
specified
in
the
instruction
to
a
value
in
a
register
as
shown
in
Code
Example
The
immediate
specified
in
an
instruction
is
a
bit
two
s
complement
number
in
the
range
Subtraction
is
equivalent
to
adding
a
negative
number
so
in
the
interest
of
simplicity
there
is
no
subi
instruction
in
the
MIPS
architecture
Recall
that
the
add
and
sub
instructions
use
three
register
operands
But
the
lw
sw
and
addi
instructions
use
two
register
operands
and
a
constant
Because
the
instruction
formats
differ
lw
and
sw
instructions
violate
design
principle
simplicity
favors
regularity
However
this
issue
allows
us
to
introduce
the
last
design
principle
CHAPTER
SIX
Architecture
High
Level
Code
a
a
b
a
MIPS
Assembly
Code
s
a
s
b
addi
s
s
a
a
addi
s
s
b
a
Code
Example
IMMEDIATE
OPERANDS
Design
Principle
Good
design
demands
good
compromises
A
single
instruction
format
would
be
simple
but
not
flexible
The
MIPS
instruction
set
makes
the
compromise
of
supporting
three
instruction
formats
One
format
used
for
instructions
such
as
add
and
sub
has
three
register
operands
Another
used
for
instructions
such
as
lw
and
addi
has
two
register
operands
and
a
bit
immediate
A
third
to
be
discussed
later
has
a
bit
immediate
and
no
registers
The
next
section
discusses
the
three
MIPS
instruction
formats
and
shows
how
they
are
encoded
into
binary
Chapter
MACHINE
LANGUAGE
Assembly
language
is
convenient
for
humans
to
read
However
digital
circuits
understand
only
s
and
s
Therefore
a
program
written
in
assembly
language
is
translated
from
mnemonics
to
a
representation
using
only
s
and
s
called
machine
language
MIPS
uses
bit
instructions
Again
simplicity
favors
regularity
and
the
most
regular
choice
is
to
encode
all
instructions
as
words
that
can
be
stored
in
memory
Even
though
some
instructions
may
not
require
all
bits
of
encoding
variable
length
instructions
would
add
too
much
complexity
Simplicity
would
also
encourage
a
single
instruction
format
but
as
already
mentioned
that
is
too
restrictive
MIPS
makes
the
compromise
of
defining
three
instruction
formats
R
type
I
type
and
J
type
This
small
number
of
formats
allows
for
some
regularity
among
all
the
types
and
thus
simpler
hardware
while
also
accommodating
different
instruction
needs
such
as
the
need
to
encode
large
constants
in
the
instruction
R
type
instructions
operate
on
three
registers
I
type
instructions
operate
on
two
registers
and
a
bit
immediate
J
type
jump
instructions
operate
on
one
bit
immediate
We
introduce
all
three
formats
in
this
section
but
leave
the
discussion
of
J
type
instructions
for
Section
R
type
Instructions
The
name
R
type
is
short
for
register
type
R
type
instructions
use
three
registers
as
operands
two
as
sources
and
one
as
a
destination
Figure
shows
the
R
type
machine
instruction
format
The
bit
instruction
has
six
fields
op
rs
rt
rd
shamt
and
funct
Each
field
is
five
or
six
bits
as
indicated
The
operation
the
instruction
performs
is
encoded
in
the
two
fields
highlighted
in
blue
op
also
called
opcode
or
operation
code
and
funct
also
called
the
function
All
R
type
instructions
have
an
opcode
of
The
specific
R
type
operation
is
determined
by
the
funct
field
For
example
the
opcode
and
funct
fields
for
the
add
instruction
are
and
respectively
Similarly
the
sub
instruction
has
an
opcode
and
funct
field
of
and
The
operands
are
encoded
in
the
three
fields
rs
rt
and
rd
The
first
two
registers
rs
and
rt
are
the
source
registers
rd
is
the
destination
Machine
Language
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
R
type
Figure
R
type
machine
instruction
format
register
The
fields
contain
the
register
numbers
that
were
given
in
Table
For
example
s
is
register
The
fifth
field
shamt
is
used
only
in
shift
operations
In
those
instructions
the
binary
value
stored
in
the
bit
shamt
field
indicates
the
amount
to
shift
For
all
other
R
type
instructions
shamt
is
Figure
shows
the
machine
code
for
the
R
type
instructions
add
and
sub
Notice
that
the
destination
is
the
first
register
in
an
assembly
language
instruction
but
it
is
the
third
register
field
rd
in
the
machine
language
instruction
For
example
the
assembly
instruction
add
s
s
s
has
rs
s
rt
s
and
rd
s
Tables
B
and
B
in
Appendix
B
define
the
opcode
values
for
all
MIPS
instructions
and
the
funct
field
values
for
R
type
instructions
Example
TRANSLATING
ASSEMBLY
LANGUAGE
TO
MACHINE
LANGUAGE
Translate
the
following
assembly
language
statement
into
machine
language
add
t
s
s
Solution
According
to
Table
t
s
and
s
are
registers
and
According
to
Tables
B
and
B
add
has
an
opcode
of
and
a
funct
code
of
Thus
the
fields
and
machine
code
are
given
in
Figure
The
easiest
way
to
write
the
machine
language
in
hexadecimal
is
to
first
write
it
in
binary
then
look
at
consecutive
groups
of
four
bits
which
correspond
to
hexadecimal
digits
indicated
in
blue
Hence
the
machine
language
instruction
is
x
CHAPTER
SIX
Architecture
add
s
s
s
sub
t
t
t
Assembly
Code
Machine
Code
Field
Values
x
x
D
op
rs
rt
rd
shamt
funct
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
Figure
Machine
code
for
R
type
instructions
add
t
s
s
Assembly
Code
Machine
Code
Field
Values
x
op
rs
rt
rd
shamt
funct
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
Figure
Machine
code
for
the
R
type
instruction
of
Example
rs
is
short
for
register
source
rt
comes
after
rs
alphabetically
and
usually
indicates
the
second
register
source
Cha
I
Type
Instructions
The
name
I
type
is
short
for
immediate
type
I
type
instructions
use
two
register
operands
and
one
immediate
operand
Figure
shows
the
I
type
machine
instruction
format
The
bit
instruction
has
four
fields
op
rs
rt
and
imm
The
first
three
fields
op
rs
and
rt
are
like
those
of
R
type
instructions
The
imm
field
holds
the
bit
immediate
The
operation
is
determined
solely
by
the
opcode
highlighted
in
blue
The
operands
are
specified
in
the
three
fields
rs
rt
and
imm
rs
and
imm
are
always
used
as
source
operands
rt
is
used
as
a
destination
for
some
instructions
such
as
addi
and
lw
but
as
another
source
for
others
such
as
sw
Figure
shows
several
examples
of
encoding
I
type
instructions
Recall
that
negative
immediate
values
are
represented
using
bit
two
s
complement
notation
rt
is
listed
first
in
the
assembly
language
instruction
when
it
is
used
as
a
destination
but
it
is
the
second
register
field
in
the
machine
language
instruction
Machine
Language
op
rs
rt
imm
bits
bits
bits
bits
I
type
Figure
I
type
instruction
format
x
x
FFF
x
C
A
xAD
Assembly
Code
Machine
Code
Field
Values
op
rs
rt
imm
op
rs
rt
imm
addi
s
s
addi
t
s
lw
t
sw
s
t
bits
bits
bits
bits
bits
bits
bits
bits
Figure
Machine
code
for
I
type
instructions
Example
TRANSLATING
I
TYPE
ASSEMBLY
INSTRUCTIONS
INTO
MACHINE
CODE
Translate
the
following
I
type
instruction
into
machine
code
lw
s
s
Solution
According
to
Table
s
and
s
are
registers
and
respectively
Table
B
indicates
that
lw
has
an
opcode
of
rs
specifies
the
base
address
s
and
rt
specifies
the
destination
register
s
The
immediate
imm
encodes
the
bit
offset
Thus
the
fields
and
machine
code
are
given
in
Figure
I
type
instructions
have
a
bit
immediate
field
but
the
immediates
are
used
in
bit
operations
For
example
lw
adds
a
bit
offset
to
a
bit
base
register
What
should
go
in
the
upper
half
of
the
bits
For
positive
immediates
the
upper
half
should
be
all
s
but
for
negative
immediates
the
upper
half
should
be
all
s
Recall
from
Section
that
this
is
called
sign
extension
An
N
bit
two
s
complement
number
is
sign
extended
to
an
M
bit
number
M
N
by
copying
the
sign
bit
most
significant
bit
of
the
N
bit
number
into
all
of
the
upper
bits
of
the
M
bit
number
Sign
extending
a
two
s
complement
number
does
not
change
its
value
Most
MIPS
instructions
sign
extend
the
immediate
For
example
addi
lw
and
sw
do
sign
extension
to
support
both
positive
and
negative
immediates
An
exception
to
this
rule
is
that
logical
operations
andi
ori
xori
place
s
in
the
upper
half
this
is
called
zero
extension
rather
than
sign
extension
Logical
operations
are
discussed
further
in
Section
J
type
Instructions
The
name
J
type
is
short
for
jump
type
This
format
is
used
only
with
jump
instructions
see
Section
This
instruction
format
uses
a
single
bit
address
operand
addr
as
shown
in
Figure
Like
other
formats
J
type
instructions
begin
with
a
bit
opcode
The
remaining
bits
are
used
to
specify
an
address
addr
Further
discussion
and
machine
code
examples
of
J
type
instructions
are
given
in
Sections
and
Interpreting
Machine
Language
Code
To
interpret
machine
language
one
must
decipher
the
fields
of
each
bit
instruction
word
Different
instructions
use
different
formats
but
all
formats
start
with
a
bit
opcode
field
Thus
the
best
place
to
begin
is
to
look
at
the
opcode
If
it
is
the
instruction
is
R
type
otherwise
it
is
I
type
or
J
type
CHAPTER
SIX
Architecture
op
rs
rt
imm
op
rs
rt
imm
lw
s
s
Assembly
Code
Machine
Code
Field
Values
x
E
FFE
bits
bits
bits
bits
E
FF
E
Figure
Machine
code
for
the
I
type
instruction
op
addr
bits
bits
J
type
Figure
J
type
instruction
format
Example
TRANSLATING
MACHINE
LANGUAGE
TO
ASSEMBLY
LANGUAGE
Translate
the
following
machine
language
code
into
assembly
language
x
FFF
x
F
Solution
First
we
represent
each
instruction
in
binary
and
look
at
the
six
most
significant
bits
to
find
the
opcode
for
each
instruction
as
shown
in
Figure
The
opcode
determines
how
to
interpret
the
rest
of
the
bits
The
opcodes
are
and
indicating
an
addi
and
R
type
instruction
respectively
The
funct
field
of
the
R
type
instruction
is
indicating
that
it
is
a
sub
instruction
Figure
shows
the
assembly
code
equivalent
of
the
two
machine
instructions
Machine
Language
addi
s
s
Machine
Code
Assembly
Code
Field
Values
x
FFF
op
rs
rt
imm
op
rs
rt
imm
FF
F
x
F
sub
t
s
s
op
F
op
rs
rt
rd
shamt
funct
rs
rt
rd
shamt
funct
Figure
Machine
code
to
assembly
code
translation
The
Power
of
the
Stored
Program
A
program
written
in
machine
language
is
a
series
of
bit
numbers
representing
the
instructions
Like
other
binary
numbers
these
instructions
can
be
stored
in
memory
This
is
called
the
stored
program
concept
and
it
is
a
key
reason
why
computers
are
so
powerful
Running
a
different
program
does
not
require
large
amounts
of
time
and
effort
to
reconfigure
or
rewire
hardware
it
only
requires
writing
the
new
program
to
memory
Instead
of
dedicated
hardware
the
stored
program
offers
general
purpose
computing
In
this
way
a
computer
can
execute
applications
ranging
from
a
calculator
to
a
word
processor
to
a
video
player
simply
by
changing
the
stored
program
Instructions
in
a
stored
program
are
retrieved
or
fetched
from
memory
and
executed
by
the
processor
Even
large
complex
programs
are
simplified
to
a
series
of
memory
reads
and
instruction
executions
Figure
shows
how
machine
instructions
are
stored
in
memory
In
MIPS
programs
the
instructions
are
normally
stored
starting
at
address
x
Remember
that
MIPS
memory
is
byte
addressable
so
bit
byte
instruction
addresses
advance
by
bytes
not
To
run
or
execute
the
stored
program
the
processor
fetches
the
instructions
from
memory
sequentially
The
fetched
instructions
are
then
decoded
and
executed
by
the
digital
hardware
The
address
of
the
current
instruction
is
kept
in
a
bit
register
called
the
program
counter
PC
The
PC
is
separate
from
the
registers
shown
previously
in
Table
To
execute
the
code
in
Figure
the
operating
system
sets
the
PC
to
address
x
The
processor
reads
the
instruction
at
that
memory
address
and
executes
the
instruction
x
C
A
The
processor
then
increments
the
PC
by
to
x
fetches
and
executes
that
instruction
and
repeats
The
architectural
state
of
a
microprocessor
holds
the
state
of
a
program
For
MIPS
the
architectural
state
consists
of
the
register
file
and
PC
If
the
operating
system
saves
the
architectural
state
at
some
point
in
the
program
it
can
interrupt
the
program
do
something
else
then
restore
the
state
such
that
the
program
continues
properly
unaware
that
it
was
ever
interrupted
The
architectural
state
is
also
of
great
importance
when
we
build
a
microprocessor
in
Chapter
PROGRAMMING
Software
languages
such
as
C
or
Java
are
called
high
level
programming
languages
because
they
are
written
at
a
more
abstract
level
than
assembly
language
Many
high
level
languages
use
common
software
constructs
such
as
arithmetic
and
logical
operations
if
else
statements
for
and
while
loops
array
indexing
and
procedure
calls
In
this
section
we
explore
how
to
translate
these
high
level
constructs
into
MIPS
assembly
code
Arithmetic
Logical
Instructions
The
MIPS
architecture
defines
a
variety
of
arithmetic
and
logical
instructions
We
introduce
these
instructions
briefly
here
because
they
are
necessary
to
implement
higher
level
constructs
CHAPTER
SIX
Architecture
Figure
Stored
program
addi
t
s
Assembly
Code
Machine
Code
lw
t
add
s
s
s
sub
t
t
t
x
C
A
x
x
FFF
x
D
Address
Instructions
C
D
FFF
C
A
Stored
Program
Main
Memory
PC
Ada
Lovelace
Wrote
the
first
computer
program
It
calculated
the
Bernoulli
numbers
using
Charles
Babbage
s
Analytical
Engine
She
was
the
only
legitimate
child
of
the
poet
Lord
Byron
Logical
Instructions
MIPS
logical
operations
include
and
or
xor
and
nor
These
R
type
instructions
operate
bit
by
bit
on
two
source
registers
and
write
the
result
to
the
destination
register
Figure
shows
examples
of
these
operations
on
the
two
source
values
xFFFF
and
x
A
F
B
The
figure
shows
the
values
stored
in
the
destination
register
rd
after
the
instruction
executes
The
and
instruction
is
useful
for
masking
bits
i
e
forcing
unwanted
bits
to
For
example
in
Figure
xFFFF
AND
x
A
F
B
x
A
The
and
instruction
masks
off
the
bottom
two
bytes
and
places
the
unmasked
top
two
bytes
of
s
x
A
in
s
Any
subset
of
register
bits
can
be
masked
The
or
instruction
is
useful
for
combining
bits
from
two
registers
For
example
x
A
OR
x
FC
x
A
FC
a
combination
of
the
two
values
MIPS
does
not
provide
a
NOT
instruction
but
A
NOR
NOT
A
so
the
NOR
instruction
can
substitute
Logical
operations
can
also
operate
on
immediates
These
I
type
instructions
are
andi
ori
and
xori
nori
is
not
provided
because
the
same
functionality
can
be
easily
implemented
using
the
other
instructions
as
will
be
explored
in
Exercise
Figure
shows
examples
of
the
andi
ori
and
xori
instructions
The
figure
gives
the
values
of
Programming
s
s
s
s
s
s
Source
Registers
Assembly
Code
Result
and
s
s
s
or
s
s
s
xor
s
s
s
nor
s
s
s
Figure
Logical
operations
s
Assembly
Code
imm
s
s
s
andi
s
s
xFA
Source
Values
Result
ori
s
s
xFA
xori
s
s
xFA
zero
extended
Figure
Logical
operations
with
immediates
Cha
the
source
register
and
immediate
and
the
value
of
the
destination
register
rt
after
the
instruction
executes
Because
these
instructions
operate
on
a
bit
value
from
a
register
and
a
bit
immediate
they
first
zeroextend
the
immediate
to
bits
Shift
Instructions
Shift
instructions
shift
the
value
in
a
register
left
or
right
by
up
to
bits
Shift
operations
multiply
or
divide
by
powers
of
two
MIPS
shift
operations
are
sll
shift
left
logical
srl
shift
right
logical
and
sra
shift
right
arithmetic
As
discussed
in
Section
left
shifts
always
fill
the
least
significant
bits
with
s
However
right
shifts
can
be
either
logical
s
shift
into
the
most
significant
bits
or
arithmetic
the
sign
bit
shifts
into
the
most
significant
bits
Figure
shows
the
machine
code
for
the
R
type
instructions
sll
srl
and
sra
rt
i
e
s
holds
the
bit
value
to
be
shifted
and
shamt
gives
the
amount
by
which
to
shift
The
shifted
result
is
placed
in
rd
Figure
shows
the
register
values
for
the
shift
instructions
sll
srl
and
sra
Shifting
a
value
left
by
N
is
equivalent
to
multiplying
it
by
N
Likewise
arithmetically
shifting
a
value
right
by
N
is
equivalent
to
dividing
it
by
N
as
discussed
in
Section
MIPS
also
has
variable
shift
instructions
sllv
shift
left
logical
variable
srlv
shift
right
logical
variable
and
srav
shift
right
arithmetic
variable
Figure
shows
the
machine
code
for
these
instructions
CHAPTER
SIX
Architecture
sll
t
s
srl
s
s
sra
s
s
op
rs
rt
rd
shamt
funct
op
rs
rt
rd
shamt
funct
Assembly
Code
Machine
Code
Field
Values
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
x
x
x
Figure
Shift
instruction
machine
code
s
Assembly
Code
shamt
t
s
s
sll
t
s
Source
Values
Result
srl
s
s
sra
s
s
Figure
Shift
operations
rt
i
e
s
holds
the
value
to
be
shifted
and
the
five
least
significant
bits
of
rs
i
e
s
give
the
amount
to
shift
The
shifted
result
is
placed
in
rd
as
before
The
shamt
field
is
ignored
and
should
be
all
s
Figure
shows
register
values
for
each
type
of
variable
shift
instruction
Generating
Constants
The
addi
instruction
is
helpful
for
assigning
bit
constants
as
shown
in
Code
Example
Programming
sllv
s
s
s
srlv
s
s
s
srav
s
s
s
op
rs
rt
rd
shamt
funct
Machine
Code
Assembly
Code
Field
Values
op
rs
rt
rd
shamt
funct
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
bits
x
x
A
x
A
Figure
Variable
shift
instruction
machine
code
s
s
s
s
Assembly
Code
sllv
s
s
s
srlv
s
s
s
srav
s
s
s
Source
Values
Result
s
Figure
Variable
shift
operations
High
Level
Code
int
a
x
f
c
MIPS
Assembly
code
s
a
addi
s
x
f
c
a
x
f
c
Code
Example
BIT
CONSTANT
High
Level
Code
int
a
x
d
e
f
c
MIPS
Assembly
Code
s
a
lui
s
x
d
e
a
x
d
e
ori
s
s
x
f
c
a
x
d
e
f
c
Code
Example
BIT
CONSTANT
Chapter
To
assign
bit
constants
use
a
load
upper
immediate
instruction
lui
followed
by
an
or
immediate
ori
instruction
as
shown
in
Code
Example
lui
loads
a
bit
immediate
into
the
upper
half
of
a
register
and
sets
the
lower
half
to
As
mentioned
earlier
ori
merges
a
bit
immediate
into
the
lower
half
Multiplication
and
Division
Instructions
Multiplication
and
division
are
somewhat
different
from
other
arithmetic
operations
Multiplying
two
bit
numbers
produces
a
bit
product
Dividing
two
bit
numbers
produces
a
bit
quotient
and
a
bit
remainder
The
MIPS
architecture
has
two
special
purpose
registers
hi
and
lo
which
are
used
to
hold
the
results
of
multiplication
and
division
mult
s
s
multiplies
the
values
in
s
and
s
The
most
significant
bits
are
placed
in
hi
and
the
least
significant
bits
are
placed
in
lo
Similarly
div
s
s
computes
s
s
The
quotient
is
placed
in
lo
and
the
remainder
is
placed
in
hi
Branching
An
advantage
of
a
computer
over
a
calculator
is
its
ability
to
make
decisions
A
computer
performs
different
tasks
depending
on
the
input
For
example
if
else
statements
case
statements
while
loops
and
for
loops
all
conditionally
execute
code
depending
on
some
test
To
sequentially
execute
instructions
the
program
counter
increments
by
after
each
instruction
Branch
instructions
modify
the
program
counter
to
skip
over
sections
of
code
or
to
go
back
to
repeat
previous
code
Conditional
branch
instructions
perform
a
test
and
branch
only
if
the
test
is
TRUE
Unconditional
branch
instructions
called
jumps
always
branch
Conditional
Branches
The
MIPS
instruction
set
has
two
conditional
branch
instructions
branch
if
equal
beq
and
branch
if
not
equal
bne
beq
branches
when
the
values
in
two
registers
are
equal
and
bne
branches
when
they
are
not
equal
Code
Example
illustrates
the
use
of
beq
Note
that
branches
are
written
as
beq
rs
rt
imm
where
rs
is
the
first
source
register
This
order
is
reversed
from
most
I
type
instructions
When
the
program
in
Code
Example
reaches
the
branch
if
equal
instruction
beq
the
value
in
s
is
equal
to
the
value
in
s
so
the
branch
is
taken
That
is
the
next
instruction
executed
is
the
add
instruction
just
after
the
label
called
target
The
two
instructions
directly
after
the
branch
and
before
the
label
are
not
executed
Assembly
code
uses
labels
to
indicate
instruction
locations
in
the
program
When
the
assembly
code
is
translated
into
machine
code
these
CHAPTER
SIX
Architecture
The
int
data
type
in
C
refers
to
a
word
of
data
representing
a
two
s
complement
integer
MIPS
uses
bit
words
so
an
int
represents
a
number
in
the
range
hi
and
lo
are
not
among
the
usual
MIPS
registers
so
special
instructions
are
needed
to
access
them
mfhi
s
move
from
hi
copies
the
value
in
hi
to
s
mflo
s
move
from
lo
copies
the
value
in
lo
to
s
hi
and
lo
are
technically
part
of
the
architectural
state
however
we
generally
ignore
these
registers
in
this
book
Programming
MIPS
Assembly
Code
addi
s
s
addi
s
s
sll
s
s
s
beq
s
s
target
s
s
so
branch
is
taken
addi
s
s
not
executed
sub
s
s
s
not
executed
target
add
s
s
s
s
Code
Example
CONDITIONAL
BRANCHING
USING
beq
MIPS
Assembly
Code
addi
s
s
addi
s
s
s
s
s
s
bne
s
s
target
s
s
so
branch
is
not
taken
addi
s
s
s
sub
s
s
s
s
target
add
s
s
s
s
Code
Example
CONDITIONAL
BRANCHING
USING
bne
labels
are
translated
into
instruction
addresses
see
Section
MIPS
assembly
labels
are
followed
by
a
and
cannot
use
reserved
words
such
as
instruction
mnemonics
Most
programmers
indent
their
instructions
but
not
the
labels
to
help
make
labels
stand
out
Code
Example
shows
an
example
using
the
branch
if
not
equal
instruction
bne
In
this
case
the
branch
is
not
taken
because
s
is
equal
to
s
and
the
code
continues
to
execute
directly
after
the
bne
instruction
All
instructions
in
this
code
snippet
are
executed
Jump
A
program
can
unconditionally
branch
or
jump
using
the
three
types
of
jump
instructions
jump
j
jump
and
link
jal
and
jump
register
jr
Jump
j
jumps
directly
to
the
instruction
at
the
specified
label
Jump
and
link
jal
is
similar
to
j
but
is
used
by
procedures
to
save
a
return
address
as
will
be
discussed
in
Section
Jump
register
jr
jumps
to
the
address
held
in
a
register
Code
Example
shows
the
use
of
the
jump
instruction
j
After
the
j
target
instruction
the
program
in
Code
Example
unconditionally
continues
executing
the
add
instruction
at
the
label
target
All
of
the
instructions
between
the
jump
and
the
label
are
skipped
j
and
jal
are
J
type
instructions
jr
is
an
R
type
instruction
that
uses
only
the
rs
operand
Chapter
qxd
PM
CHAPTER
SIX
Architecture
MIPS
Assembly
Code
addi
s
s
addi
s
s
j
target
jump
to
target
addi
s
s
not
executed
sub
s
s
s
not
executed
target
add
s
s
s
s
Code
Example
UNCONDITIONAL
BRANCHING
USING
j
MIPS
Assembly
Code
x
addi
s
x
s
x
x
jr
s
jump
to
x
x
addi
s
not
executed
x
c
sra
s
s
not
executed
x
lw
s
s
executed
after
jr
instruction
Code
Example
UNCONDITIONAL
BRANCHING
USING
jr
Code
Example
shows
the
use
of
the
jump
register
instruction
jr
Instruction
addresses
are
given
to
the
left
of
each
instruction
jr
s
jumps
to
the
address
held
in
s
x
Conditional
Statements
if
statements
if
else
statements
and
case
statements
are
conditional
statements
commonly
used
by
high
level
languages
They
each
conditionally
execute
a
block
of
code
consisting
of
one
or
more
instructions
This
section
shows
how
to
translate
these
high
level
constructs
into
MIPS
assembly
language
If
Statements
An
if
statement
executes
a
block
of
code
the
if
block
only
when
a
condition
is
met
Code
Example
shows
how
to
translate
an
if
statement
into
MIPS
assembly
code
High
Level
Code
if
i
j
f
g
h
f
f
i
MIPS
Assembly
Code
s
f
s
g
s
h
s
i
s
j
bne
s
s
L
if
i
j
skip
if
block
add
s
s
s
if
block
f
g
h
L
sub
s
s
s
f
f
i
Code
Example
if
STATEMENT
Chapter
qxd
The
assembly
code
for
the
if
statement
tests
the
opposite
condition
of
the
one
in
the
high
level
code
In
Code
Example
the
high
level
code
tests
for
i
j
and
the
assembly
code
tests
for
i
j
The
bne
instruction
branches
skips
the
if
block
when
i
j
Otherwise
i
j
the
branch
is
not
taken
and
the
if
block
is
executed
as
desired
If
Else
Statements
if
else
statements
execute
one
of
two
blocks
of
code
depending
on
a
condition
When
the
condition
in
the
if
statement
is
met
the
if
block
is
executed
Otherwise
the
else
block
is
executed
Code
Example
shows
an
example
if
else
statement
Like
if
statements
if
else
assembly
code
tests
the
opposite
condition
of
the
one
in
the
high
level
code
For
example
in
Code
Example
the
high
level
code
tests
for
i
j
The
assembly
code
tests
for
the
opposite
condition
i
j
If
that
opposite
condition
is
TRUE
bne
skips
the
if
block
and
executes
the
else
block
Otherwise
the
if
block
executes
and
finishes
with
a
jump
instruction
j
to
jump
past
the
else
block
Programming
High
Level
Code
if
i
j
f
g
h
else
f
f
i
MIPS
Assembly
Code
s
f
s
g
s
h
s
i
s
j
bne
s
s
else
if
i
j
branch
to
else
add
s
s
s
if
block
f
g
h
j
L
skip
past
the
else
block
else
sub
s
s
s
else
block
f
f
i
L
Code
Example
if
else
STATEMENT
Switch
Case
Statements
switch
case
statements
execute
one
of
several
blocks
of
code
depending
on
the
conditions
If
no
conditions
are
met
the
default
block
is
executed
A
case
statement
is
equivalent
to
a
series
of
nested
if
else
statements
Code
Example
shows
two
high
level
code
snippets
with
the
same
functionality
they
calculate
the
fee
for
an
ATM
automatic
teller
machine
withdrawal
of
or
as
defined
by
amount
The
MIPS
assembly
implementation
is
the
same
for
both
high
level
code
snippets
Getting
Loopy
Loops
repeatedly
execute
a
block
of
code
depending
on
a
condition
for
loops
and
while
loops
are
common
loop
constructs
used
by
highlevel
languages
This
section
shows
how
to
translate
them
into
MIPS
assembly
language
Chapter
qxd
CHAPTER
SIX
Architecture
High
Level
Code
switch
amount
case
fee
break
case
fee
break
case
fee
break
default
fee
equivalent
function
using
if
else
statements
if
amount
fee
else
if
amount
fee
else
if
amount
fee
else
fee
MIPS
Assembly
Code
s
amount
s
fee
case
addi
t
t
bne
s
t
case
i
if
not
skip
to
case
addi
s
if
so
fee
j
done
and
break
out
of
case
case
addi
t
t
bne
s
t
case
i
if
not
skip
to
case
addi
s
if
so
fee
j
done
and
break
out
of
case
case
addi
t
t
bne
s
t
default
i
if
not
skip
to
default
addi
s
if
so
fee
j
done
and
break
out
of
case
default
add
s
charge
done
Code
Example
switch
case
STATEMENT
While
Loops
while
loops
repeatedly
execute
a
block
of
code
until
a
condition
is
not
met
The
while
loop
in
Code
Example
determines
the
value
of
x
such
that
x
It
executes
seven
times
until
pow
Like
if
else
statements
the
assembly
code
for
while
loops
tests
the
opposite
condition
of
the
one
given
in
the
high
level
code
If
that
opposite
condition
is
TRUE
the
while
loop
is
finished
High
Level
Code
int
pow
int
x
while
pow
pow
pow
x
x
MIPS
Assembly
Code
s
pow
s
x
addi
s
pow
addi
s
x
addi
t
t
for
comparison
while
beq
s
t
done
if
pow
exit
while
sll
s
s
pow
pow
addi
s
s
x
x
j
while
done
Code
Example
while
LOOP
Chapter
qxd
PM
Page
In
Code
Example
the
while
loop
compares
pow
to
and
exits
the
loop
if
it
is
equal
Otherwise
it
doubles
pow
using
a
left
shift
increments
x
and
jumps
back
to
the
start
of
the
while
loop
For
Loops
for
loops
like
while
loops
repeatedly
execute
a
block
of
code
until
a
condition
is
not
met
However
for
loops
add
support
for
a
loop
variable
which
typically
keeps
track
of
the
number
of
loop
executions
A
general
format
of
the
for
loop
is
for
initialization
condition
loop
operation
The
initialization
code
executes
before
the
for
loop
begins
The
condition
is
tested
at
the
beginning
of
each
loop
If
the
condition
is
not
met
the
loop
exits
The
loop
operation
executes
at
the
end
of
each
loop
Code
Example
adds
the
numbers
from
to
The
loop
variable
i
is
initialized
to
and
is
incremented
at
the
end
of
each
loop
iteration
At
the
beginning
of
each
iteration
the
for
loop
executes
only
when
i
is
not
equal
to
Otherwise
the
loop
is
finished
In
this
case
the
for
loop
executes
times
for
loops
can
be
implemented
using
a
while
loop
but
the
for
loop
is
often
convenient
Magnitude
Comparison
So
far
the
examples
have
used
beq
and
bne
to
perform
equality
or
inequality
comparisons
and
branches
MIPS
provides
the
set
less
than
instruction
slt
for
magnitude
comparison
slt
sets
rd
to
when
rs
rt
Otherwise
rd
is
Programming
High
Level
Code
int
sum
for
i
i
i
i
sum
sum
i
equivalent
to
the
following
while
loop
int
sum
int
i
while
i
sum
sum
i
i
i
MIPS
Assembly
Code
s
i
s
sum
add
s
sum
addi
s
i
addi
t
t
for
beq
s
t
done
if
i
branch
to
done
add
s
s
s
sum
sum
i
addi
s
s
increment
i
j
for
done
Code
Example
for
LOOP
Chapter
qxd
Example
LOOPS
USING
slt
The
following
high
level
code
adds
the
powers
of
from
to
Translate
it
into
assembly
language
high
level
code
int
sum
for
i
i
i
i
sum
sum
i
Solution
The
assembly
language
code
uses
the
set
less
than
slt
instruction
to
perform
the
less
than
comparison
in
the
for
loop
MIPS
assembly
code
s
i
s
sum
addi
s
sum
addi
s
i
addi
t
t
loop
slt
t
s
t
if
i
t
else
t
beq
t
done
if
t
i
branch
to
done
add
s
s
s
sum
sum
i
sll
s
s
i
i
j
loop
done
Exercise
explores
how
to
use
slt
for
other
magnitude
comparisons
including
greater
than
greater
than
or
equal
and
less
than
or
equal
Arrays
Arrays
are
useful
for
accessing
large
amounts
of
similar
data
An
array
is
organized
as
sequential
data
addresses
in
memory
Each
array
element
is
identified
by
a
number
called
its
index
The
number
of
elements
in
the
array
is
called
the
size
of
the
array
This
section
shows
how
to
access
array
elements
in
memory
Array
Indexing
Figure
shows
an
array
of
five
integers
stored
in
memory
The
index
ranges
from
to
In
this
case
the
array
is
stored
in
a
processor
s
main
memory
starting
at
base
address
x
The
base
address
gives
the
address
of
the
first
array
element
array
Code
Example
multiplies
the
first
two
elements
in
array
by
and
stores
them
back
in
the
array
The
first
step
in
accessing
an
array
element
is
to
load
the
base
address
of
the
array
into
a
register
Code
Example
loads
the
base
address
CHAPTER
SIX
Architecture
Chapter
qxd
into
s
Recall
that
the
load
upper
immediate
lui
and
or
immediate
ori
instructions
can
be
used
to
load
a
bit
constant
into
a
register
Code
Example
also
illustrates
why
lw
takes
a
base
address
and
an
offset
The
base
address
points
to
the
start
of
the
array
The
offset
can
be
used
to
access
subsequent
elements
of
the
array
For
example
array
is
stored
at
memory
address
x
one
word
or
four
bytes
after
array
so
it
is
accessed
at
an
offset
of
past
the
base
address
You
might
have
noticed
that
the
code
for
manipulating
each
of
the
two
array
elements
in
Code
Example
is
essentially
the
same
except
for
the
index
Duplicating
the
code
is
not
a
problem
when
accessing
two
array
elements
but
it
would
become
terribly
inefficient
for
accessing
all
of
the
elements
in
a
large
array
Code
Example
uses
a
for
loop
to
multiply
by
all
of
the
elements
of
a
element
array
stored
at
a
base
address
of
x
B
F
Figure
shows
the
element
array
in
memory
The
index
into
the
array
is
now
a
variable
i
rather
than
a
constant
so
we
cannot
take
advantage
of
the
immediate
offset
in
lw
Instead
we
compute
the
address
of
the
ith
element
and
store
it
in
t
Remember
that
each
array
element
is
a
word
but
that
memory
is
byte
addressed
so
the
offset
from
Programming
array
array
array
array
x
array
x
x
x
C
x
Main
Memory
Address
Data
Figure
Five
entry
array
with
base
address
of
x
High
Level
Code
int
array
array
array
array
array
MIPS
Assembly
Code
s
base
address
of
array
lui
s
x
s
x
ori
s
s
x
s
x
lw
t
s
t
array
sll
t
t
t
t
t
sw
t
s
array
t
lw
t
s
t
array
sll
t
t
t
t
t
sw
t
s
array
t
Code
Example
ACCESSING
ARRAYS
Chapter
qx
the
base
address
is
i
Shifting
left
by
is
a
convenient
way
to
multiply
by
in
MIPS
assembly
language
This
example
readily
extends
to
an
array
of
any
size
Bytes
and
Characters
Numbers
in
the
range
can
be
stored
in
a
single
byte
rather
than
an
entire
word
Because
there
are
much
fewer
than
characters
on
an
English
language
keyboard
English
characters
are
often
represented
by
bytes
The
C
language
uses
the
type
char
to
represent
a
byte
or
character
Early
computers
lacked
a
standard
mapping
between
bytes
and
English
characters
so
exchanging
text
between
computers
was
difficult
In
the
American
Standards
Association
published
the
American
Standard
Code
for
Information
Interchange
ASCII
which
assigns
each
text
character
a
unique
byte
value
Table
shows
these
character
encodings
for
printable
characters
The
ASCII
values
are
given
in
hexadecimal
Lower
case
and
upper
case
letters
differ
by
x
CHAPTER
SIX
Architecture
High
Level
Code
int
i
int
array
for
i
i
i
i
array
i
array
i
MIPS
Assembly
Code
s
array
base
address
s
i
initialization
code
lui
s
x
B
s
x
B
ori
s
s
xF
s
x
B
F
addi
s
i
addi
t
t
loop
slt
t
s
t
i
beq
t
done
if
not
then
done
sll
t
s
t
i
byte
offset
add
t
t
s
address
of
array
i
lw
t
t
t
array
i
sll
t
t
t
array
i
sw
t
t
array
i
array
i
addi
s
s
i
i
j
loop
repeat
done
Code
Example
ACCESSING
ARRAYS
USING
A
for
LOOP
Figure
Memory
holding
array
starting
at
base
address
x
B
F
Other
program
languages
such
as
Java
use
different
character
encodings
most
notably
Unicode
Unicode
uses
bits
to
represent
each
character
so
it
supports
accents
umlauts
and
Asian
languages
For
more
information
see
www
unicode
org
B
FF
C
array
B
FF
B
F
B
F
array
array
array
Main
Memory
Address
Data
Chapter
qxd
MIPS
provides
load
byte
and
store
byte
instructions
to
manipulate
bytes
or
characters
of
data
load
byte
unsigned
lbu
load
byte
lb
and
store
byte
sb
All
three
are
illustrated
in
Figure
Programming
ASCII
codes
developed
from
earlier
forms
of
character
encoding
Beginning
in
telegraph
machines
used
Morse
code
a
series
of
dots
and
dashes
to
represent
characters
For
example
the
letters
A
B
C
and
D
were
represented
as
and
respectively
The
number
of
dots
and
dashes
varied
with
each
letter
For
efficiency
common
letters
used
shorter
codes
In
Jean
MauriceEmile
Baudot
invented
a
bit
code
called
the
Baudot
code
For
example
A
B
C
and
D
were
represented
as
and
However
the
possible
encodings
of
this
bit
code
were
not
sufficient
for
all
the
English
characters
But
bit
encoding
was
Thus
as
electronic
communication
became
prevalent
bit
ASCII
encoding
emerged
as
the
standard
Table
ASCII
encodings
Char
Char
Char
Char
Char
Char
space
P
p
A
Q
a
q
B
R
b
r
C
S
c
s
D
T
d
t
E
U
e
u
F
V
f
v
G
W
g
w
H
X
h
x
I
Y
i
y
A
A
A
J
A
Z
A
j
A
z
B
B
B
K
B
B
k
B
C
C
C
L
C
C
l
C
D
D
D
M
D
D
m
D
E
E
E
N
E
E
n
E
F
F
F
O
F
F
o
Byte
Address
Data
CF
s
C
lbu
s
Little
Endian
Memory
Registers
s
FFFF
FF
C
lb
s
s
XX
XX
XX
B
sb
s
Figure
Instructions
for
loading
and
storing
bytes
Cha
Load
byte
unsigned
lbu
zero
extends
the
byte
and
load
byte
lb
sign
extends
the
byte
to
fill
the
entire
bit
register
Store
byte
sb
stores
the
least
significant
byte
of
the
bit
register
into
the
specified
byte
address
in
memory
In
Figure
lbu
loads
the
byte
at
memory
address
into
the
least
significant
byte
of
s
and
fills
the
remaining
register
bits
with
lb
loads
the
sign
extended
byte
at
memory
address
into
s
sb
stores
the
least
significant
byte
of
s
into
memory
byte
it
replaces
xF
with
x
B
The
more
significant
bytes
of
s
are
ignored
Example
USING
lb
AND
sb
TO
ACCESS
A
CHARACTER
ARRAY
The
following
high
level
code
converts
a
ten
entry
array
of
characters
from
lower
case
to
upper
case
by
subtracting
from
each
array
entry
Translate
it
into
MIPS
assembly
language
Remember
that
the
address
difference
between
array
elements
is
now
byte
not
bytes
Assume
that
s
already
holds
the
base
address
of
chararray
high
level
code
char
chararray
int
i
for
i
i
i
i
chararray
i
chararray
i
Solution
MIPS
assembly
code
s
base
address
of
chararray
s
i
addi
s
i
addi
t
t
loop
beq
t
s
done
if
i
exit
loop
add
t
s
s
t
address
of
chararray
i
lb
t
t
t
array
i
addi
t
t
convert
to
upper
case
t
t
sb
t
t
store
new
value
in
array
chararray
i
t
addi
s
s
i
i
j
loop
repeat
done
A
series
of
characters
is
called
a
string
Strings
have
a
variable
length
so
programming
languages
must
provide
a
way
to
determine
the
length
or
end
of
the
string
In
C
the
null
character
x
signifies
the
end
of
a
string
For
example
Figure
shows
the
string
Hello
x
C
C
F
stored
in
memory
The
string
is
seven
bytes
long
and
extends
from
address
x
FFF
to
x
FFF
The
first
character
of
the
string
H
x
is
stored
at
the
lowest
byte
address
x
FFF
CHAPTER
SIX
Architecture
Figure
The
string
Hello
stored
in
memory
Word
Address
FFF
FFF
Data
C
C
F
Little
Endian
Memory
Byte
Byte
Chapter
qxd
Procedure
Calls
High
level
languages
often
use
procedures
also
called
functions
to
reuse
frequently
accessed
code
and
to
make
a
program
more
readable
Procedures
have
inputs
called
arguments
and
an
output
called
the
return
value
Procedures
should
calculate
the
return
value
and
cause
no
other
unintended
side
effects
When
one
procedure
calls
another
the
calling
procedure
the
caller
and
the
called
procedure
the
callee
must
agree
on
where
to
put
the
arguments
and
the
return
value
In
MIPS
the
caller
conventionally
places
up
to
four
arguments
in
registers
a
a
before
making
the
procedure
call
and
the
callee
places
the
return
value
in
registers
v
v
before
finishing
By
following
this
convention
both
procedures
know
where
to
find
the
arguments
and
return
value
even
if
the
caller
and
callee
were
written
by
different
people
The
callee
must
not
interfere
with
the
function
of
the
caller
Briefly
this
means
that
the
callee
must
know
where
to
return
to
after
it
completes
and
it
must
not
trample
on
any
registers
or
memory
needed
by
the
caller
The
caller
stores
the
return
address
in
ra
at
the
same
time
it
jumps
to
the
callee
using
the
jump
and
link
instruction
jal
The
callee
must
not
overwrite
any
architectural
state
or
memory
that
the
caller
is
depending
on
Specifically
the
callee
must
leave
the
saved
registers
s
s
ra
and
the
stack
a
portion
of
memory
used
for
temporary
variables
unmodified
This
section
shows
how
to
call
and
return
from
a
procedure
It
shows
how
procedures
access
input
arguments
and
the
return
value
and
how
they
use
the
stack
to
store
temporary
variables
Procedure
Calls
and
Returns
MIPS
uses
the
jump
and
link
instruction
jal
to
call
a
procedure
and
the
jump
register
instruction
jr
to
return
from
a
procedure
Code
Example
shows
the
main
procedure
calling
the
simple
procedure
main
is
the
caller
and
simple
is
the
callee
The
simple
procedure
is
called
with
no
input
arguments
and
generates
no
return
value
it
simply
returns
to
the
caller
In
Code
Example
instruction
addresses
are
given
to
the
left
of
each
MIPS
instruction
in
hexadecimal
Programming
High
Level
Code
MIPS
Assembly
Code
int
main
simple
x
main
jal
simple
call
procedure
x
void
means
the
function
returns
no
value
void
simple
return
x
simple
jr
ra
return
Code
Example
simple
PROCEDURE
CALL
Jump
and
link
jal
and
jump
register
jr
ra
are
the
two
essential
instructions
needed
for
a
procedure
call
jal
performs
two
functions
it
stores
the
address
of
the
next
instruction
the
instruction
after
jal
in
the
return
address
register
ra
and
it
jumps
to
the
target
instruction
In
Code
Example
the
main
procedure
calls
the
simple
procedure
by
executing
the
jump
and
link
jal
instruction
jal
jumps
to
the
simple
label
and
stores
x
in
ra
The
simple
procedure
returns
immediately
by
executing
the
instruction
jr
ra
jumping
to
the
instruction
address
held
in
ra
The
main
procedure
then
continues
executing
at
this
address
x
Input
Arguments
and
Return
Values
The
simple
procedure
in
Code
Example
is
not
very
useful
because
it
receives
no
input
from
the
calling
procedure
main
and
returns
no
output
By
MIPS
convention
procedures
use
a
a
for
input
arguments
and
v
v
for
the
return
value
In
Code
Example
the
procedure
diffofsums
is
called
with
four
arguments
and
returns
one
result
According
to
MIPS
convention
the
calling
procedure
main
places
the
procedure
arguments
from
left
to
right
into
the
input
registers
a
a
The
called
procedure
diffofsums
stores
the
return
value
in
the
return
register
v
A
procedure
that
returns
a
bit
value
such
as
a
double
precision
floating
point
number
uses
both
return
registers
v
and
v
When
a
procedure
with
more
than
four
arguments
is
called
the
additional
input
arguments
are
placed
on
the
stack
which
we
discuss
next
CHAPTER
SIX
Architecture
High
Level
Code
MIPS
Assembly
Code
s
y
int
main
main
int
y
addi
a
argument
addi
a
argument
addi
a
argument
addi
a
argument
y
diffofsums
jal
diffofsums
call
procedure
add
s
v
y
returned
value
s
result
int
diffofsums
int
f
int
g
int
h
int
i
diffofsums
add
t
a
a
t
f
g
int
result
add
t
a
a
t
h
i
sub
s
t
t
result
f
g
h
i
result
f
g
h
i
add
v
s
put
return
value
in
v
return
result
jr
ra
return
to
caller
Code
Example
PROCEDURE
CALL
WITH
ARGUMENTS
AND
RETURN
VALUES
Code
Example
has
some
subtle
errors
Code
Examples
and
on
page
show
improved
versions
of
the
program
Chapter
qxd
Programming
The
Stack
The
stack
is
memory
that
is
used
to
save
local
variables
within
a
procedure
The
stack
expands
uses
more
memory
as
the
processor
needs
more
scratch
space
and
contracts
uses
less
memory
when
the
processor
no
longer
needs
the
variables
stored
there
Before
explaining
how
procedures
use
the
stack
to
store
temporary
variables
we
explain
how
the
stack
works
The
stack
is
a
last
in
first
out
LIFO
queue
Like
a
stack
of
dishes
the
last
item
pushed
onto
the
stack
the
top
dish
is
the
first
one
that
can
be
pulled
popped
off
Each
procedure
may
allocate
stack
space
to
store
local
variables
but
must
deallocate
it
before
returning
The
top
of
the
stack
is
the
most
recently
allocated
space
Whereas
a
stack
of
dishes
grows
up
in
space
the
MIPS
stack
grows
down
in
memory
The
stack
expands
to
lower
memory
addresses
when
a
program
needs
more
scratch
space
Figure
shows
a
picture
of
the
stack
The
stack
pointer
sp
is
a
special
MIPS
register
that
points
to
the
top
of
the
stack
A
pointer
is
a
fancy
name
for
a
memory
address
It
points
to
gives
the
address
of
data
For
example
in
Figure
a
the
stack
pointer
sp
holds
the
address
value
x
FFFFFFC
and
points
to
the
data
value
x
sp
points
to
the
top
of
the
stack
the
lowest
accessible
memory
address
on
the
stack
Thus
in
Figure
a
the
stack
cannot
access
memory
below
memory
word
x
FFFFFFC
The
stack
pointer
sp
starts
at
a
high
memory
address
and
decrements
to
expand
as
needed
Figure
b
shows
the
stack
expanding
to
allow
two
more
data
words
of
temporary
storage
To
do
so
sp
decrements
by
to
become
x
FFFFFF
Two
additional
data
words
xAABBCCDD
and
x
are
temporarily
stored
on
the
stack
One
of
the
important
uses
of
the
stack
is
to
save
and
restore
registers
that
are
used
by
a
procedure
Recall
that
a
procedure
should
calculate
a
return
value
but
have
no
other
unintended
side
effects
In
particular
it
should
not
modify
any
registers
besides
the
one
containing
the
return
value
v
The
diffofsums
procedure
in
Code
Example
violates
this
rule
because
it
modifies
t
t
and
s
If
main
had
been
using
t
t
or
s
before
the
call
to
diffofsums
the
contents
of
these
registers
would
have
been
corrupted
by
the
procedure
call
To
solve
this
problem
a
procedure
saves
registers
on
the
stack
before
it
modifies
them
then
restores
them
from
the
stack
before
it
returns
Specifically
it
performs
the
following
steps
Makes
space
on
the
stack
to
store
the
values
of
one
or
more
registers
Stores
the
values
of
the
registers
on
the
stack
Executes
the
procedure
using
the
registers
Restores
the
original
values
of
the
registers
from
the
stack
Deallocates
space
on
the
stack
Data
FFFFFFC
FFFFFF
FFFFFF
FFFFFF
Address
sp
a
FFFFFFC
FFFFFF
FFFFFF
FFFFFF
Address
b
Data
sp
AABBCCDD
Figure
The
stack
Code
Example
shows
an
improved
version
of
diffofsums
that
saves
and
restores
t
t
and
s
The
new
lines
are
indicated
in
blue
Figure
shows
the
stack
before
during
and
after
a
call
to
the
diffofsums
procedure
from
Code
Example
diffofsums
makes
room
for
three
words
on
the
stack
by
decrementing
the
stack
pointer
sp
by
It
then
stores
the
current
values
of
s
t
and
t
in
the
newly
allocated
space
It
executes
the
rest
of
the
procedure
changing
the
values
in
these
three
registers
At
the
end
of
the
procedure
diffofsums
restores
the
values
of
s
t
and
t
from
the
stack
deallocates
its
stack
space
and
returns
When
the
procedure
returns
v
holds
the
result
but
there
are
no
other
side
effects
s
t
t
and
sp
have
the
same
values
as
they
did
before
the
procedure
call
The
stack
space
that
a
procedure
allocates
for
itself
is
called
its
stack
frame
diffofsums
s
stack
frame
is
three
words
deep
The
principle
of
modularity
tells
us
that
each
procedure
should
access
only
its
own
stack
frame
not
the
frames
belonging
to
other
procedures
Preserved
Registers
Code
Example
assumes
that
temporary
registers
t
and
t
must
be
saved
and
restored
If
the
calling
procedure
does
not
use
those
registers
the
effort
to
save
and
restore
them
is
wasted
To
avoid
this
waste
MIPS
divides
registers
into
preserved
and
nonpreserved
categories
The
preserved
registers
include
s
s
hence
their
name
saved
The
nonpreserved
registers
include
t
t
hence
their
name
temporary
A
procedure
must
save
and
restore
any
of
the
preserved
registers
that
it
wishes
to
use
but
it
can
change
the
nonpreserved
registers
freely
Code
Example
shows
a
further
improved
version
of
diffofsums
that
saves
only
s
on
the
stack
t
and
t
are
nonpreserved
registers
so
they
need
not
be
saved
Remember
that
when
one
procedure
calls
another
the
former
is
the
caller
and
the
latter
is
the
callee
The
callee
must
save
and
restore
any
preserved
registers
that
it
wishes
to
use
The
callee
may
change
any
of
the
nonpreserved
registers
Hence
if
the
caller
is
holding
active
data
in
a
CHAPTER
SIX
Architecture
Data
FC
F
F
F
Address
sp
a
Data
sp
c
FC
F
F
F
Address
Data
FC
F
F
F
Address
sp
b
s
t
stack
frame
t
Figure
The
stack
a
before
b
during
and
c
after
diffofsums
procedure
call
nonpreserved
register
the
caller
needs
to
save
that
nonpreserved
register
before
making
the
procedure
call
and
then
needs
to
restore
it
afterward
For
these
reasons
preserved
registers
are
also
called
callee
save
and
nonpreserved
registers
are
called
caller
save
Table
summarizes
which
registers
are
preserved
s
s
are
generally
used
to
hold
local
variables
within
a
procedure
so
they
must
be
saved
ra
must
also
be
saved
so
that
the
procedure
knows
where
to
return
t
t
are
used
to
hold
temporary
results
before
they
are
assigned
to
local
variables
These
calculations
typically
complete
before
a
procedure
call
is
made
so
they
are
not
preserved
and
it
is
rare
that
the
caller
needs
to
save
them
a
a
are
often
overwritten
in
the
process
of
calling
a
procedure
Hence
they
must
be
saved
by
the
caller
if
the
caller
depends
on
any
of
its
own
arguments
after
a
called
procedure
returns
v
v
certainly
should
not
be
preserved
because
the
callee
returns
its
result
in
these
registers
Programming
MIPS
Assembly
Code
s
result
diffofsums
addi
sp
sp
make
space
on
stack
to
store
three
registers
sw
s
sp
save
s
on
stack
sw
t
sp
save
t
on
stack
sw
t
sp
save
t
on
stack
add
t
a
a
t
f
g
add
t
a
a
t
h
i
sub
s
t
t
result
f
g
h
i
add
v
s
put
return
value
in
v
lw
t
sp
restore
t
from
stack
lw
t
sp
restore
t
from
stack
lw
s
sp
restore
s
from
stack
addi
sp
sp
deallocate
stack
space
jr
ra
return
to
caller
Code
Example
PROCEDURE
SAVING
REGISTERS
ON
THE
STACK
MIPS
Assembly
Code
s
result
diffofsums
addi
sp
sp
make
space
on
stack
to
store
one
register
sw
s
sp
save
s
on
stack
add
t
a
a
t
f
g
add
t
a
a
t
h
i
sub
s
t
t
result
f
g
h
i
add
v
s
put
return
value
in
v
lw
s
sp
restore
s
from
stack
addi
sp
sp
deallocate
stack
space
jr
ra
return
to
caller
Code
Example
PROCEDURE
SAVING
PRESERVED
REGISTERS
ON
THE
STACK
Chapter
qxd
CHAPTER
SIX
Architecture
The
stack
above
the
stack
pointer
is
automatically
preserved
as
long
as
the
callee
does
not
write
to
memory
addresses
above
sp
In
this
way
it
does
not
modify
the
stack
frame
of
any
other
procedures
The
stack
pointer
itself
is
preserved
because
the
callee
deallocates
its
stack
frame
before
returning
by
adding
back
the
same
amount
that
it
subtracted
from
sp
at
the
beginning
of
the
procedure
Recursive
Procedure
Calls
A
procedure
that
does
not
call
others
is
called
a
leaf
procedure
an
example
is
diffofsums
A
procedure
that
does
call
others
is
called
a
nonleaf
procedure
As
mentioned
earlier
nonleaf
procedures
are
somewhat
more
complicated
because
they
may
need
to
save
nonpreserved
registers
on
the
stack
before
they
call
another
procedure
and
then
restore
those
registers
afterward
Specifically
the
caller
saves
any
nonpreserved
registers
t
t
and
a
a
that
are
needed
after
the
call
The
callee
saves
any
of
the
preserved
registers
s
s
and
ra
that
it
intends
to
modify
A
recursive
procedure
is
a
nonleaf
procedure
that
calls
itself
The
factorial
function
can
be
written
as
a
recursive
procedure
call
Recall
that
factorial
n
n
n
n
The
factorial
function
can
be
rewritten
recursively
as
factorial
n
n
factorial
n
The
factorial
of
is
simply
Code
Example
shows
the
factorial
function
written
as
a
recursive
procedure
To
conveniently
refer
to
program
addresses
we
assume
that
the
program
starts
at
address
x
The
factorial
procedure
might
modify
a
and
ra
so
it
saves
them
on
the
stack
It
then
checks
whether
n
If
so
it
puts
the
return
value
of
in
v
restores
the
stack
pointer
and
returns
to
the
caller
It
does
not
have
to
reload
ra
and
a
in
this
case
because
they
were
never
modified
If
n
the
procedure
recursively
calls
factorial
n
It
then
restores
the
value
of
n
a
and
the
return
address
ra
from
the
stack
performs
the
multiplication
and
returns
this
result
The
multiply
instruction
mul
v
a
v
multiplies
a
and
v
and
places
the
result
in
v
It
is
discussed
further
in
Section
Table
Preserved
and
nonpreserved
registers
Preserved
Nonpreserved
Saved
registers
s
s
Temporary
registers
t
t
Return
address
ra
Argument
registers
a
a
Stack
pointer
sp
Return
value
registers
v
v
Stack
above
the
stack
pointer
Stack
below
the
stack
pointer
Ch
Programming
High
Level
Code
int
factorial
int
n
if
n
return
else
return
n
factorial
n
MIPS
Assembly
Code
x
factorial
addi
sp
sp
make
room
on
stack
x
sw
a
sp
store
a
x
sw
ra
sp
store
ra
x
C
addi
t
t
xA
slt
t
a
t
a
xA
beq
t
else
no
goto
else
xA
addi
v
yes
return
xAC
addi
sp
sp
restore
sp
xB
jr
ra
return
xB
else
addi
a
a
n
n
xB
jal
factorial
recursive
call
xBC
lw
ra
sp
restore
ra
xC
lw
a
sp
restore
a
xC
addi
sp
sp
restore
sp
xC
mul
v
a
v
n
factorial
n
xCC
jr
ra
return
Code
Example
factorial
RECURSIVE
PROCEDURE
CALL
Figure
shows
the
stack
when
executing
factorial
We
assume
that
sp
initially
points
to
xFC
as
shown
in
Figure
a
The
procedure
creates
a
two
word
stack
frame
to
hold
a
and
ra
On
the
first
invocation
factorial
saves
a
holding
n
at
xF
and
ra
at
xF
as
shown
in
Figure
b
The
procedure
then
changes
a
to
n
and
recursively
calls
factorial
making
ra
hold
xBC
On
the
second
invocation
it
saves
a
holding
n
at
xF
and
ra
at
xEC
This
time
we
know
that
ra
contains
xBC
The
procedure
then
changes
a
to
n
and
recursively
calls
factorial
On
the
third
invocation
it
saves
a
holding
n
at
xE
and
ra
at
xE
This
time
ra
again
contains
xBC
The
third
invocation
of
Figure
Stack
during
factorial
procedure
call
when
n
a
before
call
b
after
last
recursive
call
c
after
return
sp
a
FC
F
F
F
EC
E
E
E
DC
Address
Data
FC
F
F
F
b
ra
EC
E
E
E
DC
sp
sp
sp
sp
Address
Data
a
x
ra
xBC
a
x
ra
xBC
a
x
FC
F
F
F
c
EC
E
E
E
DC
sp
sp
sp
sp
a
v
x
a
v
x
a
v
x
v
Address
Data
ra
a
x
ra
xBC
a
x
ra
xBC
a
x
Chapter
factorial
returns
the
value
in
v
and
deallocates
the
stack
frame
before
returning
to
the
second
invocation
The
second
invocation
restores
n
to
restores
ra
to
xBC
it
happened
to
already
have
this
value
deallocates
the
stack
frame
and
returns
v
to
the
first
invocation
The
first
invocation
restores
n
to
restores
ra
to
the
return
address
of
the
caller
deallocates
the
stack
frame
and
returns
v
Figure
c
shows
the
stack
as
the
recursively
called
procedures
return
When
factorial
returns
to
the
caller
the
stack
pointer
is
in
its
original
position
xFC
none
of
the
contents
of
the
stack
above
the
pointer
have
changed
and
all
of
the
preserved
registers
hold
their
original
values
v
holds
the
return
value
Additional
Arguments
and
Local
Variables
Procedures
may
have
more
than
four
input
arguments
and
local
variables
The
stack
is
used
to
store
these
temporary
values
By
MIPS
convention
if
a
procedure
has
more
than
four
arguments
the
first
four
are
passed
in
the
argument
registers
as
usual
Additional
arguments
are
passed
on
the
stack
just
above
sp
The
caller
must
expand
its
stack
to
make
room
for
the
additional
arguments
Figure
a
shows
the
caller
s
stack
for
calling
a
procedure
with
more
than
four
arguments
A
procedure
can
also
declare
local
variables
or
arrays
Local
variables
are
declared
within
a
procedure
and
can
be
accessed
only
within
that
procedure
Local
variables
are
stored
in
s
s
if
there
are
too
many
local
variables
they
can
also
be
stored
in
the
procedure
s
stack
frame
In
particular
local
arrays
are
stored
on
the
stack
Figure
b
shows
the
organization
of
a
callee
s
stack
frame
The
frame
holds
the
procedure
s
own
arguments
if
it
calls
other
procedures
the
return
address
and
any
of
the
saved
registers
that
the
procedure
will
CHAPTER
SIX
Architecture
sp
ra
if
needed
additional
arguments
a
a
if
needed
s
s
if
needed
local
variables
or
arrays
spstack
frame
additional
arguments
Figure
Stack
usage
left
before
call
right
after
call
Chap
modify
It
also
holds
local
arrays
and
any
excess
local
variables
If
the
callee
has
more
than
four
arguments
it
finds
them
in
the
caller
s
stack
frame
Accessing
additional
input
arguments
is
the
one
exception
in
which
a
procedure
can
access
stack
data
not
in
its
own
stack
frame
ADDRESSING
MODES
MIPS
uses
five
addressing
modes
register
only
immediate
base
PCrelative
and
pseudo
direct
The
first
three
modes
register
only
immediate
and
base
addressing
define
modes
of
reading
and
writing
operands
The
last
two
PC
relative
and
pseudo
direct
addressing
define
modes
of
writing
the
program
counter
PC
Register
Only
Addressing
Register
only
addressing
uses
registers
for
all
source
and
destination
operands
All
R
type
instructions
use
register
only
addressing
Immediate
Addressing
Immediate
addressing
uses
the
bit
immediate
along
with
registers
as
operands
Some
I
type
instructions
such
as
add
immediate
addi
and
load
upper
immediate
lui
use
immediate
addressing
Base
Addressing
Memory
access
instructions
such
as
load
word
lw
and
store
word
sw
use
base
addressing
The
effective
address
of
the
memory
operand
is
found
by
adding
the
base
address
in
register
rs
to
the
sign
extended
bit
offset
found
in
the
immediate
field
PC
relative
Addressing
Conditional
branch
instructions
use
PC
relative
addressing
to
specify
the
new
value
of
the
PC
if
the
branch
is
taken
The
signed
offset
in
the
immediate
field
is
added
to
the
PC
to
obtain
the
new
PC
hence
the
branch
destination
address
is
said
to
be
relative
to
the
current
PC
Code
Example
shows
part
of
the
factorial
procedure
from
Code
Example
Figure
shows
the
machine
code
for
the
beq
instruction
The
branch
target
address
BTA
is
the
address
of
the
next
instruction
to
execute
if
the
branch
is
taken
The
beq
instruction
in
Figure
has
a
BTA
of
xB
the
instruction
address
of
the
else
label
The
bit
immediate
field
gives
the
number
of
instructions
between
the
BTA
and
the
instruction
after
the
branch
instruction
the
instruction
at
PC
In
this
case
the
value
in
the
immediate
field
of
beq
is
because
the
BTA
xB
is
instructions
past
PC
xA
The
processor
calculates
the
BTA
from
the
instruction
by
signextending
the
bit
immediate
multiplying
it
by
to
convert
words
to
bytes
and
adding
it
to
PC
Addressing
Modes
Cha
Example
CALCULATING
THE
IMMEDIATE
FIELD
FOR
PC
RELATIVE
ADDRESSING
Calculate
the
immediate
field
and
show
the
machine
code
for
the
branch
not
equal
bne
instruction
in
the
following
program
MIPS
assembly
code
x
loop
add
t
a
s
x
lb
t
t
x
add
t
a
s
x
C
sb
t
t
x
addi
s
s
x
bne
t
loop
x
lw
s
sp
Solution
Figure
shows
the
machine
code
for
the
bne
instruction
Its
branch
target
address
x
is
instructions
behind
PC
x
so
the
immediate
field
is
CHAPTER
SIX
Architecture
op
rs
imm
beq
t
else
Machine
Code
Assembly
Code
bits
bits
bits
bits
x
bits
Field
Values
op
rs
rt
imm
bits
bits
bits
rt
Figure
Machine
code
for
beq
MIPS
Assembly
Code
xA
beq
t
else
xA
addi
v
xAC
addi
sp
sp
xB
jr
ra
xB
else
addi
a
a
xB
jal
factorial
Code
Example
CALCULATING
THE
BRANCH
TARGET
ADDRESS
bne
t
loop
Assembly
Code
Machine
Code
bits
bits
bits
bits
x
FFFA
bits
bits
bits
bits
op
rs
rt
imm
op
rs
rt
imm
Field
Values
Figure
bne
machine
code
Pseudo
Direct
Addressing
In
direct
addressing
an
address
is
specified
in
the
instruction
The
jump
instructions
j
and
jal
ideally
would
use
direct
addressing
to
specify
a
C
bit
jump
target
address
JTA
to
indicate
the
instruction
address
to
execute
next
Unfortunately
the
J
type
instruction
encoding
does
not
have
enough
bits
to
specify
a
full
bit
JTA
Six
bits
of
the
instruction
are
used
for
the
opcode
so
only
bits
are
left
to
encode
the
JTA
Fortunately
the
two
least
significant
bits
JTA
should
always
be
because
instructions
are
word
aligned
The
next
bits
JTA
are
taken
from
the
addr
field
of
the
instruction
The
four
most
significant
bits
JTA
are
obtained
from
the
four
most
significant
bits
of
PC
This
addressing
mode
is
called
pseudo
direct
Code
Example
illustrates
a
jal
instruction
using
pseudo
direct
addressing
The
JTA
of
the
jal
instruction
is
x
A
Figure
shows
the
machine
code
for
this
jal
instruction
The
top
four
bits
and
bottom
two
bits
of
the
JTA
are
discarded
The
remaining
bits
are
stored
in
the
bit
address
field
addr
The
processor
calculates
the
JTA
from
the
J
type
instruction
by
appending
two
s
and
prepending
the
four
most
significant
bits
of
PC
to
the
bit
address
field
addr
Because
the
four
most
significant
bits
of
the
JTA
are
taken
from
PC
the
jump
range
is
limited
The
range
limits
of
branch
and
jump
instructions
are
explored
in
Exercises
to
All
J
type
instructions
j
and
jal
use
pseudo
direct
addressing
Note
that
the
jump
register
instruction
jr
is
not
a
J
type
instruction
It
is
an
R
type
instruction
that
jumps
to
the
bit
value
held
in
register
rs
Addressing
Modes
MIPS
Assembly
Code
x
C
jal
sum
x
A
sum
add
v
a
a
Code
Example
CALCULATING
THE
JUMP
TARGET
ADDRESS
op
jal
sum
Assembly
Code
Machine
Code
x
C
op
bits
JTA
bit
addr
x
x
A
addr
x
bits
bits
bits
addr
Field
Values
Figure
jal
machine
code
Cha
CHAPTER
SIX
Architecture
LIGHTS
CAMERA
ACTION
COMPILING
ASSEMBLING
AND
LOADING
Up
until
now
we
have
shown
how
to
translate
short
high
level
code
snippets
into
assembly
and
machine
code
This
section
describes
how
to
compile
and
assemble
a
complete
high
level
program
and
how
to
load
the
program
into
memory
for
execution
We
begin
by
introducing
the
MIPS
memory
map
which
defines
where
code
data
and
stack
memory
are
located
We
then
show
the
steps
of
code
execution
for
a
sample
program
The
Memory
Map
With
bit
addresses
the
MIPS
address
space
spans
bytes
gigabytes
GB
Word
addresses
are
divisible
by
and
range
from
to
xFFFFFFFC
Figure
shows
the
MIPS
memory
map
The
MIPS
architecture
divides
the
address
space
into
four
parts
or
segments
the
text
segment
global
data
segment
dynamic
data
segment
and
reserved
segments
The
following
sections
describes
each
segment
The
Text
Segment
The
text
segment
stores
the
machine
language
program
It
is
large
enough
to
accommodate
almost
MB
of
code
Note
that
the
four
most
significant
bits
of
the
address
in
the
text
space
are
all
so
the
j
instruction
can
directly
jump
to
any
address
in
the
program
The
Global
Data
Segment
The
global
data
segment
stores
global
variables
that
in
contrast
to
local
variables
can
be
seen
by
all
procedures
in
a
program
Global
variables
Address
Segment
sp
x
FFFFFFC
xFFFFFFFC
x
x
FFFFFFC
x
x
FFFC
x
x
FFFFFFC
x
x
FFFFC
x
Reserved
Stack
Heap
Global
Data
Text
Reserved
gp
x
PC
x
Figure
MIPS
memory
map
Dynamic
Data
C
are
defined
at
start
up
before
the
program
begins
executing
These
variables
are
declared
outside
the
main
procedure
in
a
C
program
and
can
be
accessed
by
any
procedure
The
global
data
segment
is
large
enough
to
store
KB
of
global
variables
Global
variables
are
accessed
using
the
global
pointer
gp
which
is
initialized
to
x
Unlike
the
stack
pointer
sp
gp
does
not
change
during
program
execution
Any
global
variable
can
be
accessed
with
a
bit
positive
or
negative
offset
from
gp
The
offset
is
known
at
assembly
time
so
the
variables
can
be
efficiently
accessed
using
base
addressing
mode
with
constant
offsets
The
Dynamic
Data
Segment
The
dynamic
data
segment
holds
the
stack
and
the
heap
The
data
in
this
segment
are
not
known
at
start
up
but
are
dynamically
allocated
and
deallocated
throughout
the
execution
of
the
program
This
is
the
largest
segment
of
memory
used
by
a
program
spanning
almost
GB
of
the
address
space
As
discussed
in
Section
the
stack
is
used
to
save
and
restore
registers
used
by
procedures
and
to
hold
local
variables
such
as
arrays
The
stack
grows
downward
from
the
top
of
the
dynamic
data
segment
x
FFFFFFC
and
is
accessed
in
last
in
first
out
LIFO
order
The
heap
stores
data
that
is
allocated
by
the
program
during
runtime
In
C
memory
allocations
are
made
by
the
malloc
function
in
C
and
Java
new
is
used
to
allocate
memory
Like
a
heap
of
clothes
on
a
dorm
room
floor
heap
data
can
be
used
and
discarded
in
any
order
The
heap
grows
upward
from
the
bottom
of
the
dynamic
data
segment
If
the
stack
and
heap
ever
grow
into
each
other
the
program
s
data
can
become
corrupted
The
memory
allocator
tries
to
ensure
that
this
never
happens
by
returning
an
out
of
memory
error
if
there
is
insufficient
space
to
allocate
more
dynamic
data
The
Reserved
Segments
The
reserved
segments
are
used
by
the
operating
system
and
cannot
directly
be
used
by
the
program
Part
of
the
reserved
memory
is
used
for
interrupts
see
Section
and
for
memory
mapped
I
O
see
Section
Translating
and
Starting
a
Program
Figure
shows
the
steps
required
to
translate
a
program
from
a
highlevel
language
into
machine
language
and
to
start
executing
that
program
First
the
high
level
code
is
compiled
into
assembly
code
The
assembly
code
is
assembled
into
machine
code
in
an
object
file
The
linker
combines
the
machine
code
with
object
code
from
libraries
and
other
files
to
produce
an
entire
executable
program
In
practice
most
compilers
perform
all
three
steps
of
compiling
assembling
and
linking
Finally
the
loader
loads
Lights
Camera
Action
Compiling
Assembling
and
Loading
Grace
Hopper
Graduated
from
Yale
University
with
a
Ph
D
in
mathematics
Developed
the
first
compiler
while
working
for
the
Remington
Rand
Corporation
and
was
instrumental
in
developing
the
COBOL
programming
language
As
a
naval
officer
she
received
many
awards
including
a
World
War
II
Victory
Medal
and
the
National
Defence
Service
Medal
Ch
the
program
into
memory
and
starts
execution
The
remainder
of
this
section
walks
through
these
steps
for
a
simple
program
Step
Compilation
A
compiler
translates
high
level
code
into
assembly
language
Code
Example
shows
a
simple
high
level
program
with
three
global
CHAPTER
SIX
Architecture
Assembly
Code
High
Level
Code
Compiler
Object
File
Assembler
Executable
Linker
Memory
Loader
Object
Files
Library
Files
Figure
Steps
for
translating
and
starting
a
program
High
Level
Code
int
f
g
y
global
variables
int
main
void
f
g
y
sum
f
g
return
y
int
sum
int
a
int
b
return
a
b
MIPS
Assembly
Code
data
f
g
y
text
main
addi
sp
sp
make
stack
frame
sw
ra
sp
store
ra
on
stack
addi
a
a
sw
a
f
f
addi
a
a
sw
a
g
g
jal
sum
call
sum
procedure
sw
v
y
y
sum
f
g
lw
ra
sp
restore
ra
from
stack
addi
sp
sp
restore
stack
pointer
jr
ra
return
to
operating
system
sum
add
v
a
a
v
a
b
jr
ra
return
to
caller
Code
Example
COMPILING
A
HIGH
LEVEL
PROGRAM
Chapter
variables
and
two
procedures
along
with
the
assembly
code
produced
by
a
typical
compiler
The
data
and
text
keywords
are
assembler
directives
that
indicate
where
the
text
and
data
segments
begin
Labels
are
used
for
global
variables
f
g
and
y
Their
storage
location
will
be
determined
by
the
assembler
for
now
they
are
left
as
symbols
in
the
code
Step
Assembling
The
assembler
turns
the
assembly
language
code
into
an
object
file
containing
machine
language
code
The
assembler
makes
two
passes
through
the
assembly
code
On
the
first
pass
the
assembler
assigns
instruction
addresses
and
finds
all
the
symbols
such
as
labels
and
global
variable
names
The
code
after
the
first
assembler
pass
is
shown
here
x
main
addi
sp
sp
x
sw
ra
sp
x
addi
a
x
C
sw
a
f
x
addi
a
x
sw
a
g
x
jal
sum
x
C
sw
v
y
x
lw
ra
sp
x
addi
sp
sp
x
jr
ra
x
C
sum
add
v
a
a
x
jr
ra
The
names
and
addresses
of
the
symbols
are
kept
in
a
symbol
table
as
shown
in
Table
for
this
code
The
symbol
addresses
are
filled
in
after
the
first
pass
when
the
addresses
of
labels
are
known
Global
variables
are
assigned
storage
locations
in
the
global
data
segment
of
memory
starting
at
memory
address
x
On
the
second
pass
through
the
code
the
assembler
produces
the
machine
language
code
Addresses
for
the
global
variables
and
labels
are
taken
from
the
symbol
table
The
machine
language
code
and
symbol
table
are
stored
in
the
object
file
Lights
Camera
Action
Compiling
Assembling
and
Loading
Table
Symbol
table
Symbol
Address
f
x
g
x
y
x
main
x
sum
x
C
Executable
file
header
Text
Size
Data
Size
Text
segment
Data
segment
Address
Address
x
x
x
x
C
x
x
x
x
C
x
x
x
x
C
x
addi
sp
sp
sw
ra
sp
addi
a
sw
a
x
gp
addi
a
sw
a
x
gp
jal
x
C
sw
v
x
gp
lw
ra
sp
addi
sp
sp
jr
ra
add
v
a
a
jr
ra
x
x
x
f
g
y
x
bytes
xC
bytes
x
BDFFFC
xAFBF
x
xAF
x
xAF
x
C
B
xAF
x
FBF
x
BD
x
E
x
x
E
Instruction
Data
CHAPTER
SIX
Architecture
Step
Linking
Most
large
programs
contain
more
than
one
file
If
the
programmer
changes
only
one
of
the
files
it
would
be
wasteful
to
recompile
and
reassemble
the
other
files
In
particular
programs
often
call
procedures
in
library
files
these
library
files
almost
never
change
If
a
file
of
highlevel
code
is
not
changed
the
associated
object
file
need
not
be
updated
The
job
of
the
linker
is
to
combine
all
of
the
object
files
into
one
machine
language
file
called
the
executable
The
linker
relocates
the
data
and
instructions
in
the
object
files
so
that
they
are
not
all
on
top
of
each
other
It
uses
the
information
in
the
symbol
tables
to
adjust
the
addresses
of
global
variables
and
of
labels
that
are
relocated
In
our
example
there
is
only
one
object
file
so
no
relocation
is
necessary
Figure
shows
the
executable
file
It
has
three
sections
the
executable
file
header
the
text
segment
and
the
data
segment
The
executable
file
header
reports
the
text
size
code
size
and
data
size
amount
of
globally
declared
data
Both
are
given
in
units
of
bytes
The
text
segment
gives
the
instructions
and
the
addresses
where
they
are
to
be
stored
The
figure
shows
the
instructions
in
human
readable
format
next
to
the
machine
code
for
ease
of
interpretation
but
the
executable
file
includes
only
machine
instructions
The
data
segment
gives
the
address
of
each
global
variable
The
global
variables
are
addressed
with
respect
to
the
base
address
given
by
the
global
pointer
gp
For
example
the
Figure
Executable
y
g
f
x
E
x
x
E
x
BD
x
FBF
xAF
x
C
B
xAF
x
xAF
x
xAFBF
x
BDFFFC
Address
Memory
x
FFFFFFC
sp
x
FFFFFFC
x
x
Stack
Heap
gp
x
PC
x
x
Reserved
Reserved
first
store
instruction
sw
a
x
gp
stores
the
value
to
the
global
variable
f
which
is
located
at
memory
address
x
Remember
that
the
offset
x
is
a
bit
signed
number
that
is
sign
extended
and
added
to
the
base
address
gp
So
gp
x
x
xFFFF
x
the
memory
address
of
variable
f
Step
Loading
The
operating
system
loads
a
program
by
reading
the
text
segment
of
the
executable
file
from
a
storage
device
usually
the
hard
disk
into
the
text
segment
of
memory
The
operating
system
sets
gp
to
x
the
middle
of
the
global
data
segment
and
sp
to
x
FFFFFFC
the
top
of
the
dynamic
data
segment
then
performs
a
jal
x
to
jump
to
the
beginning
of
the
program
Figure
shows
the
memory
map
at
the
beginning
of
program
execution
Lights
Camera
Action
Compiling
Assembling
and
Loading
Figure
Executable
loaded
in
memory
Chap
ODDS
AND
ENDS
This
section
covers
a
few
optional
topics
that
do
not
fit
naturally
elsewhere
in
the
chapter
These
topics
include
pseudoinstructions
exceptions
signed
and
unsigned
arithmetic
instructions
and
floating
point
instructions
Pseudoinstructions
If
an
instruction
is
not
available
in
the
MIPS
instruction
set
it
is
probably
because
the
same
operation
can
be
performed
using
one
or
more
existing
MIPS
instructions
Remember
that
MIPS
is
a
reduced
instruction
set
computer
RISC
so
the
instruction
size
and
hardware
complexity
are
minimized
by
keeping
the
number
of
instructions
small
However
MIPS
defines
pseudoinstructions
that
are
not
actually
part
of
the
instruction
set
but
are
commonly
used
by
programmers
and
compilers
When
converted
to
machine
code
pseudoinstructions
are
translated
into
one
or
more
MIPS
instructions
Table
gives
examples
of
pseudoinstructions
and
the
MIPS
instructions
used
to
implement
them
For
example
the
load
immediate
pseudoinstruction
li
loads
a
bit
constant
using
a
combination
of
lui
and
ori
instructions
The
multiply
pseudoinstruction
mul
provides
a
three
operand
multiply
multiplying
two
registers
and
putting
the
least
significant
bits
of
the
result
into
a
third
register
The
no
operation
pseudoinstruction
nop
pronounced
no
op
performs
no
operation
The
PC
is
incremented
by
upon
its
execution
No
other
registers
or
memory
values
are
altered
The
machine
code
for
the
nop
instruction
is
x
Some
pseudoinstructions
require
a
temporary
register
for
intermediate
calculations
For
example
the
pseudoinstruction
beq
t
imm
Loop
compares
t
to
a
bit
immediate
imm
This
pseudoinstruction
CHAPTER
SIX
Architecture
Table
Pseudoinstructions
Corresponding
Pseudoinstruction
MIPS
Instructions
li
s
x
AA
lui
s
x
ori
s
xAA
mul
s
s
s
mult
s
s
mflo
s
clear
t
add
t
move
s
s
add
s
s
nop
sll
requires
a
temporary
register
in
which
to
store
the
bit
immediate
Assemblers
use
the
assembler
register
at
for
such
purposes
Table
shows
how
the
assembler
uses
at
in
converting
a
pseudoinstruction
to
real
MIPS
instructions
We
leave
it
as
Exercise
to
implement
other
pseudoinstructions
such
as
rotate
left
rol
and
rotate
right
ror
Exceptions
An
exception
is
like
an
unscheduled
procedure
call
that
jumps
to
a
new
address
Exceptions
may
be
caused
by
hardware
or
software
For
example
the
processor
may
receive
notification
that
the
user
pressed
a
key
on
a
keyboard
The
processor
may
stop
what
it
is
doing
determine
which
key
was
pressed
save
it
for
future
reference
then
resume
the
program
that
was
running
Such
a
hardware
exception
triggered
by
an
input
output
I
O
device
such
as
a
keyboard
is
often
called
an
interrupt
Alternatively
the
program
may
encounter
an
error
condition
such
as
an
undefined
instruction
The
program
then
jumps
to
code
in
the
operating
system
OS
which
may
choose
to
terminate
the
offending
program
Software
exceptions
are
sometimes
called
traps
Other
causes
of
exceptions
include
division
by
zero
attempts
to
read
nonexistent
memory
hardware
malfunctions
debugger
breakpoints
and
arithmetic
overflow
see
Section
The
processor
records
the
cause
of
an
exception
and
the
value
of
the
PC
at
the
time
the
exception
occurs
It
then
jumps
to
the
exception
handler
procedure
The
exception
handler
is
code
usually
in
the
OS
that
examines
the
cause
of
the
exception
and
responds
appropriately
by
reading
the
keyboard
on
a
hardware
interrupt
for
example
It
then
returns
to
the
program
that
was
executing
before
the
exception
took
place
In
MIPS
the
exception
handler
is
always
located
at
x
When
an
exception
occurs
the
processor
always
jumps
to
this
instruction
address
regardless
of
the
cause
The
MIPS
architecture
uses
a
special
purpose
register
called
the
Cause
register
to
record
the
cause
of
the
exception
Different
codes
are
used
to
record
different
exception
causes
as
given
in
Table
The
exception
handler
code
reads
the
Cause
register
to
determine
how
to
handle
the
exception
Some
other
architectures
jump
to
a
different
exception
handler
for
each
different
cause
instead
of
using
a
Cause
register
MIPS
uses
another
special
purpose
register
called
the
Exception
Program
Counter
EPC
to
store
the
value
of
the
PC
at
the
time
an
exception
Odds
and
Ends
Table
Pseudoinstruction
using
at
Corresponding
Pseudoinstruction
MIPS
Instructions
beq
t
imm
Loop
addi
at
imm
beq
t
at
Loop
takes
place
The
processor
returns
to
the
address
in
EPC
after
handling
the
exception
This
is
analogous
to
using
ra
to
store
the
old
value
of
the
PC
during
a
jal
instruction
The
EPC
and
Cause
registers
are
not
part
of
the
MIPS
register
file
The
mfc
move
from
coprocessor
instruction
copies
these
and
other
special
purpose
registers
into
one
of
the
general
purpose
registers
Coprocessor
is
called
the
MIPS
processor
control
it
handles
interrupts
and
processor
diagnostics
For
example
mfc
t
Cause
copies
the
Cause
register
into
t
The
syscall
and
break
instructions
cause
traps
to
perform
system
calls
or
debugger
breakpoints
The
exception
handler
uses
the
EPC
to
look
up
the
instruction
and
determine
the
nature
of
the
system
call
or
breakpoint
by
looking
at
the
fields
of
the
instruction
In
summary
an
exception
causes
the
processor
to
jump
to
the
exception
handler
The
exception
handler
saves
registers
on
the
stack
then
uses
mfc
to
look
at
the
cause
and
respond
accordingly
When
the
handler
is
finished
it
restores
the
registers
from
the
stack
copies
the
return
address
from
EPC
to
k
using
mfc
and
returns
using
jr
k
Signed
and
Unsigned
Instructions
Recall
that
a
binary
number
may
be
signed
or
unsigned
The
MIPS
architecture
uses
two
s
complement
representation
of
signed
numbers
MIPS
has
certain
instructions
that
come
in
signed
and
unsigned
flavors
including
addition
and
subtraction
multiplication
and
division
set
less
than
and
partial
word
loads
Addition
and
Subtraction
Addition
and
subtraction
are
performed
identically
whether
the
number
is
signed
or
unsigned
However
the
interpretation
of
the
results
is
different
As
mentioned
in
Section
if
two
large
signed
numbers
are
added
together
the
result
may
incorrectly
produce
the
opposite
sign
For
example
adding
the
following
two
huge
positive
numbers
gives
a
negative
CHAPTER
SIX
Architecture
Table
Exception
cause
codes
Exception
Cause
hardware
interrupt
x
system
call
x
breakpoint
divide
by
x
undefined
instruction
x
arithmetic
overflow
x
k
and
k
are
included
in
the
MIPS
register
set
They
are
reserved
by
the
OS
for
exception
handling
They
do
not
need
to
be
saved
and
restored
during
exceptions
result
x
FFFFFFF
x
FFFFFFF
xFFFFFFFE
Similarly
adding
two
huge
negative
numbers
gives
a
positive
result
x
x
x
This
is
called
arithmetic
overflow
The
C
language
ignores
arithmetic
overflows
but
other
languages
such
as
Fortran
require
that
the
program
be
notified
As
mentioned
in
Section
the
MIPS
processor
takes
an
exception
on
arithmetic
overflow
The
program
can
decide
what
to
do
about
the
overflow
for
example
it
might
repeat
the
calculation
with
greater
precision
to
avoid
the
overflow
then
return
to
where
it
left
off
MIPS
provides
signed
and
unsigned
versions
of
addition
and
subtraction
The
signed
versions
are
add
addi
and
sub
The
unsigned
versions
are
addu
addiu
and
subu
The
two
versions
are
identical
except
that
signed
versions
trigger
an
exception
on
overflow
whereas
unsigned
versions
do
not
Because
C
ignores
exceptions
C
programs
technically
use
the
unsigned
versions
of
these
instructions
Multiplication
and
Division
Multiplication
and
division
behave
differently
for
signed
and
unsigned
numbers
For
example
as
an
unsigned
number
xFFFFFFFF
represents
a
large
number
but
as
a
signed
number
it
represents
Hence
xFFFFFFFF
xFFFFFFFF
would
equal
xFFFFFFFE
if
the
numbers
were
unsigned
but
x
if
the
numbers
were
signed
Therefore
multiplication
and
division
come
in
both
signed
and
unsigned
flavors
mult
and
div
treat
the
operands
as
signed
numbers
multu
and
divu
treat
the
operands
as
unsigned
numbers
Set
Less
Than
Set
less
than
instructions
can
compare
either
two
registers
slt
or
a
register
and
an
immediate
slti
Set
less
than
also
comes
in
signed
slt
and
slti
and
unsigned
sltu
and
sltiu
versions
In
a
signed
comparison
x
is
less
than
any
other
number
because
it
is
the
most
negative
two
s
complement
number
In
an
unsigned
comparison
x
is
greater
than
x
FFFFFFF
but
less
than
x
because
all
numbers
are
positive
Beware
that
sltiu
sign
extends
the
immediate
before
treating
it
as
an
unsigned
number
For
example
sltiu
s
s
x
compares
s
to
xFFFF
treating
the
immediate
as
a
large
positive
number
Loads
As
described
in
Section
byte
loads
come
in
signed
lb
and
unsigned
lbu
versions
lb
sign
extends
the
byte
and
lbu
zero
extends
the
byte
to
fill
the
entire
bit
register
Similarly
MIPS
provides
signed
and
unsigned
half
word
loads
lh
and
lhu
which
load
two
bytes
into
the
lower
half
and
sign
or
zero
extend
the
upper
half
of
the
word
Odds
and
Ends
Chapt
cop
ft
fs
fd
funct
bits
bits
bits
bits
bits
bits
F
type
op
Floating
Point
Instructions
The
MIPS
architecture
defines
an
optional
floating
point
coprocessor
known
as
coprocessor
In
early
MIPS
implementations
the
floatingpoint
coprocessor
was
a
separate
chip
that
users
could
purchase
if
they
needed
fast
floating
point
math
In
most
recent
MIPS
implementations
the
floating
point
coprocessor
is
built
in
alongside
the
main
processor
MIPS
defines
bit
floating
point
registers
f
f
These
are
separate
from
the
ordinary
registers
used
so
far
MIPS
supports
both
single
and
double
precision
IEEE
floating
point
arithmetic
Doubleprecision
bit
numbers
are
stored
in
pairs
of
bit
registers
so
only
the
even
numbered
registers
f
f
f
f
are
used
to
specify
double
precision
operations
By
convention
certain
registers
are
reserved
for
certain
purposes
as
given
in
Table
Floating
point
instructions
all
have
an
opcode
of
They
require
both
a
funct
field
and
a
cop
coprocessor
field
to
indicate
the
type
of
instruction
Hence
MIPS
defines
the
F
type
instruction
format
for
floating
point
instructions
shown
in
Figure
Floating
point
instructions
come
in
both
single
and
double
precision
flavors
cop
for
single
precision
instructions
or
for
doubleprecision
instructions
Like
R
type
instructions
F
type
instructions
have
two
source
operands
fs
and
ft
and
one
destination
fd
Instruction
precision
is
indicated
by
s
and
d
in
the
mnemonic
Floating
point
arithmetic
instructions
include
addition
add
s
add
d
subtraction
sub
sz
sub
d
multiplication
mul
s
mul
d
and
division
div
s
div
d
as
well
as
negation
neg
s
neg
d
and
absolute
value
abs
s
abs
d
CHAPTER
SIX
Architecture
Table
MIPS
floating
point
register
set
Name
Number
Use
fv
fv
procedure
return
values
ft
ft
temporary
variables
fa
fa
procedure
arguments
ft
ft
temporary
variables
fs
fs
saved
variables
Figure
F
type
machine
instruction
format
C
Floating
point
branches
have
two
parts
First
a
compare
instruction
is
used
to
set
or
clear
the
floating
point
condition
flag
fpcond
Then
a
conditional
branch
checks
the
value
of
the
flag
The
compare
instructions
include
equality
c
seq
s
c
seq
d
less
than
c
lt
s
c
lt
d
and
less
than
or
equal
to
c
le
s
c
le
d
The
conditional
branch
instructions
are
bc
f
and
bc
t
that
branch
if
fpcond
is
FALSE
or
TRUE
respectively
Inequality
greater
than
or
equal
to
and
greater
than
comparisons
are
performed
with
seq
lt
and
le
followed
by
bc
f
Floating
point
registers
are
loaded
and
stored
from
memory
using
lwc
and
swc
These
instructions
move
bits
so
two
are
necessary
to
handle
a
double
precision
number
REAL
WORLD
PERSPECTIVE
IA
ARCHITECTURE
Almost
all
personal
computers
today
use
IA
architecture
microprocessors
IA
is
a
bit
architecture
originally
developed
by
Intel
AMD
also
sells
IA
compatible
microprocessors
The
IA
architecture
has
a
long
and
convoluted
history
dating
back
to
when
Intel
announced
the
bit
microprocessor
IBM
selected
the
and
its
cousin
the
for
IBM
s
first
personal
computers
In
Intel
introduced
the
bit
microprocessor
which
was
backward
compatible
with
the
so
it
could
run
software
developed
for
earlier
PCs
Processor
architectures
compatible
with
the
are
called
IA
or
x
processors
The
Pentium
Core
and
Athlon
processors
are
well
known
IA
processors
Section
describes
the
evolution
of
IA
microprocessors
in
more
detail
Various
groups
at
Intel
and
AMD
over
many
years
have
shoehorned
more
instructions
and
capabilities
into
the
antiquated
architecture
The
result
is
far
less
elegant
than
MIPS
As
Patterson
and
Hennessy
explain
this
checkered
ancestry
has
led
to
an
architecture
that
is
difficult
to
explain
and
impossible
to
love
However
software
compatibility
is
far
more
important
than
technical
elegance
so
IA
has
been
the
de
facto
PC
standard
for
more
than
two
decades
More
than
million
IA
processors
are
sold
every
year
This
huge
market
justifies
more
than
billion
of
research
and
development
annually
to
continue
improving
the
processors
IA
is
an
example
of
a
Complex
Instruction
Set
Computer
CISC
architecture
In
contrast
to
RISC
architectures
such
as
MIPS
each
CISC
instruction
can
do
more
work
Programs
for
CISC
architectures
usually
require
fewer
instructions
The
instruction
encodings
were
selected
to
be
more
compact
so
as
to
save
memory
when
RAM
was
far
more
expensive
than
it
is
today
instructions
are
of
variable
length
and
are
often
less
than
bits
The
trade
off
is
that
complicated
instructions
are
more
difficult
to
decode
and
tend
to
execute
more
slowly
Real
World
Perspective
IA
Architecture
This
section
introduces
the
IA
architecture
The
goal
is
not
to
make
you
into
an
IA
assembly
language
programmer
but
rather
to
illustrate
some
of
the
similarities
and
differences
between
IA
and
MIPS
We
think
it
is
interesting
to
see
how
IA
works
However
none
of
the
material
in
this
section
is
needed
to
understand
the
rest
of
the
book
Major
differences
between
IA
and
MIPS
are
summarized
in
Table
IA
Registers
The
microprocessor
provided
eight
bit
registers
It
could
separately
access
the
upper
and
lower
eight
bits
of
some
of
these
registers
When
the
bit
was
introduced
the
registers
were
extended
to
bits
These
registers
are
called
EAX
ECX
EDX
EBX
ESP
EBP
ESI
and
EDI
For
backward
compatibility
the
bottom
bits
and
some
of
the
bottom
bit
portions
are
also
usable
as
shown
in
Figure
The
eight
registers
are
almost
but
not
quite
general
purpose
Certain
instructions
cannot
use
certain
registers
Other
instructions
always
put
their
results
in
certain
registers
Like
sp
in
MIPS
ESP
is
normally
reserved
for
the
stack
pointer
The
IA
program
counter
is
called
EIP
the
extended
instruction
pointer
Like
the
MIPS
PC
it
advances
from
one
instruction
to
the
next
or
can
be
changed
with
branch
jump
and
subroutine
call
instructions
IA
Operands
MIPS
instructions
always
act
on
registers
or
immediates
Explicit
load
and
store
instructions
are
needed
to
move
data
between
memory
and
the
registers
In
contrast
IA
instructions
may
operate
on
registers
immediates
or
memory
This
partially
compensates
for
the
small
set
of
registers
CHAPTER
SIX
Architecture
Table
Major
differences
between
MIPS
and
IA
Feature
MIPS
IA
of
registers
general
purpose
some
restrictions
on
purpose
of
operands
source
destination
source
source
destination
operand
location
registers
or
immediates
registers
immediates
or
memory
operand
size
bits
or
bits
condition
codes
no
yes
instruction
types
simple
simple
and
complicated
instruction
encoding
fixed
bytes
variable
bytes
Figure
IA
registers
EAX
AH
AX
ECX
CH
CX
EDX
Byte
Byte
Byte
Byte
DH
DX
EBX
BH
BX
ESP
SP
EBP
BP
ESI
SI
EDI
DI
AL
CL
DL
BL
MIPS
instructions
generally
specify
three
operands
two
sources
and
one
destination
IA
instructions
specify
only
two
operands
The
first
is
a
source
The
second
is
both
a
source
and
the
destination
Hence
IA
instructions
always
overwrite
one
of
their
sources
with
the
result
Table
lists
the
combinations
of
operand
locations
in
IA
All
of
the
combinations
are
possible
except
memory
to
memory
Like
MIPS
IA
has
a
bit
memory
space
that
is
byte
addressable
However
IA
also
supports
a
much
wider
variety
of
memory
addressing
modes
The
memory
location
is
specified
with
any
combination
of
a
base
register
displacement
and
a
scaled
index
register
Table
illustrates
these
combinations
The
displacement
can
be
an
or
bit
value
The
scale
multiplying
the
index
register
can
be
or
The
base
displacement
mode
is
equivalent
to
the
MIPS
base
addressing
mode
for
loads
and
stores
The
scaled
index
provides
an
easy
way
to
access
arrays
or
structures
of
or
byte
elements
without
having
to
issue
a
sequence
of
instructions
to
generate
the
address
Real
World
Perspective
IA
Architecture
Table
Operand
locations
Source
Destination
Source
Example
Meaning
register
register
add
EAX
EBX
EAX
EAX
EBX
register
immediate
add
EAX
EAX
EAX
register
memory
add
EAX
EAX
EAX
Mem
memory
register
add
EAX
Mem
Mem
EAX
memory
immediate
add
Mem
Mem
Table
Memory
addressing
modes
Example
Meaning
Comment
add
EAX
EAX
EAX
Mem
displacement
add
EAX
ESP
EAX
EAX
Mem
ESP
base
addressing
add
EAX
EDX
EAX
EAX
Mem
EDX
base
displacement
add
EAX
EDI
EAX
EAX
Mem
EDI
displacement
scaled
index
add
EAX
EDX
EDI
EAX
EAX
Mem
EDX
EDI
base
displacement
scaled
index
Chapter
qxd
While
MIPS
always
acts
on
bit
words
IA
instructions
can
operate
on
or
bit
data
Table
illustrates
these
variations
Status
Flags
IA
like
many
CISC
architectures
uses
status
flags
also
called
condition
codes
to
make
decisions
about
branches
and
to
keep
track
of
carries
and
arithmetic
overflow
IA
uses
a
bit
register
called
EFLAGS
that
stores
the
status
flags
Some
of
the
bits
of
the
EFLAGS
register
are
given
in
Table
Other
bits
are
used
by
the
operating
system
The
architectural
state
of
an
IA
processor
includes
EFLAGS
as
well
as
the
eight
registers
and
the
EIP
IA
Instructions
IA
has
a
larger
set
of
instructions
than
MIPS
Table
describes
some
of
the
general
purpose
instructions
IA
also
has
instructions
for
floatingpoint
arithmetic
and
for
arithmetic
on
multiple
short
data
elements
packed
into
a
longer
word
D
indicates
the
destination
a
register
or
memory
location
and
S
indicates
the
source
a
register
memory
location
or
immediate
Note
that
some
instructions
always
act
on
specific
registers
For
example
bit
multiplication
always
takes
one
of
the
sources
from
EAX
and
always
puts
the
bit
result
in
EDX
and
EAX
LOOP
always
CHAPTER
SIX
Architecture
Table
Instructions
acting
on
or
bit
data
Example
Meaning
Data
Size
add
AH
BL
AH
AH
BL
bit
add
AX
AX
AX
xFFFF
bit
add
EAX
EDX
EAX
EAX
EDX
bit
Table
Selected
EFLAGS
Name
Meaning
CF
Carry
Flag
Carry
out
generated
by
last
arithmetic
operation
Indicates
overflow
in
unsigned
arithmetic
Also
used
for
propagating
the
carry
between
words
in
multiple
precision
arithmetic
ZF
Zero
Flag
Result
of
last
operation
was
zero
SF
Sign
Flag
Result
of
last
operation
was
negative
msb
OF
Overflow
Flag
Overflow
of
two
s
complement
arithmetic
Chap
Real
World
Perspective
IA
Architecture
Table
Selected
IA
instructions
Instruction
Meaning
Function
ADD
SUB
add
subtract
D
D
S
D
D
S
ADDC
add
with
carry
D
D
S
CF
INC
DEC
increment
decrement
D
D
D
D
CMP
compare
Set
flags
based
on
D
S
NEG
negate
D
D
AND
OR
XOR
logical
AND
OR
XOR
D
D
op
S
NOT
logical
NOT
D
D
IMUL
MUL
signed
unsigned
multiply
EDX
EAX
EAX
D
IDIV
DIV
signed
unsigned
divide
EDX
EAX
D
EAX
Quotient
EDX
Remainder
SAR
SHR
arithmetic
logical
shift
right
D
D
S
D
D
S
SAL
SHL
left
shift
D
D
S
ROR
ROL
rotate
right
left
Rotate
D
by
S
RCR
RCL
rotate
right
left
with
carry
Rotate
CF
and
D
by
S
BT
bit
test
CF
D
S
the
Sth
bit
of
D
BTR
BTS
bit
test
and
reset
set
CF
D
S
D
S
TEST
set
flags
based
on
masked
bits
Set
flags
based
on
D
AND
S
MOV
move
D
S
PUSH
push
onto
stack
ESP
ESP
Mem
ESP
S
POP
pop
off
stack
D
MEM
ESP
ESP
ESP
CLC
STC
clear
set
carry
flag
CF
JMP
unconditional
jump
relative
jump
EIP
EIP
S
absolute
jump
EIP
S
Jcc
conditional
jump
if
flag
EIP
EIP
S
LOOP
loop
ECX
ECX
if
ECX
EIP
EIP
imm
CALL
procedure
call
ESP
ESP
MEM
ESP
EIP
EIP
S
RET
procedure
return
EIP
MEM
ESP
ESP
ESP
Chapter
qxd
PM
Page
CHAPTER
SIX
Architecture
It
is
possible
to
construct
byte
instructions
if
all
the
optional
fields
are
used
However
IA
places
a
byte
limit
on
the
length
of
legal
instructions
Table
Selected
branch
conditions
Instruction
Meaning
Function
After
cmp
d
s
JZ
JE
jump
if
ZF
jump
if
D
S
JNZ
JNE
jump
if
ZF
jump
if
D
S
JGE
jump
if
SF
OF
jump
if
D
S
JG
jump
if
SF
OF
and
ZF
jump
if
D
S
JLE
jump
if
SF
OF
or
ZF
jump
if
D
S
JL
jump
if
SF
OF
jump
if
D
S
JC
JB
jump
if
CF
JNC
jump
if
CF
JO
jump
if
OF
JNO
jump
if
OF
JS
jump
if
SF
JNS
jump
if
SF
stores
the
loop
counter
in
ECX
PUSH
POP
CALL
and
RET
use
the
stack
pointer
ESP
Conditional
jumps
check
the
flags
and
branch
if
the
appropriate
condition
is
met
They
come
in
many
flavors
For
example
JZ
jumps
if
the
zero
flag
ZF
is
JNZ
jumps
if
the
zero
flag
is
The
jumps
usually
follow
an
instruction
such
as
the
compare
instruction
CMP
that
sets
the
flags
Table
lists
some
of
the
conditional
jumps
and
how
they
depend
on
the
flags
set
by
a
prior
compare
operation
IA
Instruction
Encoding
The
IA
instruction
encodings
are
truly
messy
a
legacy
of
decades
of
piecemeal
changes
Unlike
MIPS
whose
instructions
are
uniformly
bits
IA
instructions
vary
from
to
bytes
as
shown
in
Figure
The
opcode
may
be
or
bytes
It
is
followed
by
four
optional
fields
ModR
M
SIB
Displacement
and
Immediate
ModR
M
specifies
an
addressing
mode
SIB
specifies
the
scale
index
and
base
Chapter
qx
Real
World
Perspective
IA
Architecture
registers
in
certain
addressing
modes
Displacement
indicates
a
or
byte
displacement
in
certain
addressing
modes
And
Immediate
is
a
or
byte
constant
for
instructions
using
an
immediate
as
the
source
operand
Moreover
an
instruction
can
be
preceded
by
up
to
four
optional
byte
long
prefixes
that
modify
its
behavior
The
ModR
M
byte
uses
the
bit
Mod
and
bit
R
M
field
to
specify
the
addressing
mode
for
one
of
the
operands
The
operand
can
come
from
one
of
the
eight
registers
or
from
one
of
memory
addressing
modes
Due
to
artifacts
in
the
encodings
the
ESP
and
EBP
registers
are
not
available
for
use
as
the
base
or
index
register
in
certain
addressing
modes
The
Reg
field
specifies
the
register
used
as
the
other
operand
For
certain
instructions
that
do
not
require
a
second
operand
the
Reg
field
is
used
to
specify
three
more
bits
of
the
opcode
In
addressing
modes
using
a
scaled
index
register
the
SIB
byte
specifies
the
index
register
and
the
scale
or
If
both
a
base
and
index
are
used
the
SIB
byte
also
specifies
the
base
register
MIPS
fully
specifies
the
instruction
in
the
opcode
and
funct
fields
of
the
instruction
IA
uses
a
variable
number
of
bits
to
specify
different
instructions
It
uses
fewer
bits
to
specify
more
common
instructions
decreasing
the
average
length
of
the
instructions
Some
instructions
even
have
multiple
opcodes
For
example
add
AL
imm
performs
an
bit
add
of
an
immediate
to
AL
It
is
represented
with
the
byte
opcode
x
followed
by
a
byte
immediate
The
A
register
AL
AX
or
EAX
is
called
the
accumulator
On
the
other
hand
add
D
imm
performs
an
bit
add
of
an
immediate
to
an
arbitrary
destination
D
memory
or
a
register
It
is
represented
with
the
byte
opcode
x
followed
by
one
or
more
bytes
specifying
D
followed
by
a
byte
immediate
Many
instructions
have
shortened
encodings
when
the
destination
is
the
accumulator
In
the
original
the
opcode
specified
whether
the
instruction
acted
on
or
bit
operands
When
the
introduced
bit
operands
no
new
opcodes
were
available
to
specify
the
bit
form
Prefixes
ModR
M
SIB
Displacement
Immediate
Up
to
optional
prefixes
of
byte
each
or
byte
opcode
byte
for
certain
addressing
modes
byte
for
certain
addressing
modes
or
bytes
for
addressing
modes
with
displacement
or
bytes
for
addressing
modes
with
immediate
Mod
R
M
Scale
Index
Base
Reg
Opcode
Opcode
bits
bits
bits
bits
bits
bits
Figure
IA
instruction
encodings
Instead
the
same
opcode
was
used
for
both
and
bit
forms
An
additional
bit
in
the
code
segment
descriptor
used
by
the
OS
specifies
which
form
the
processor
should
choose
The
bit
is
set
to
for
backward
compatibility
with
programs
defaulting
the
opcode
to
bit
operands
It
is
set
to
for
programs
to
default
to
bit
operands
Moreover
the
programmer
can
specify
prefixes
to
change
the
form
for
a
particular
instruction
If
the
prefix
x
appears
before
the
opcode
the
alternative
size
operand
is
used
bits
in
bit
mode
or
bits
in
bit
mode
Other
IA
Peculiarities
The
introduced
segmentation
to
divide
memory
into
segments
of
up
to
KB
in
length
When
the
OS
enables
segmentation
addresses
are
computed
relative
to
the
beginning
of
the
segment
The
processor
checks
for
addresses
that
go
beyond
the
end
of
the
segment
and
indicates
an
error
thus
preventing
programs
from
accessing
memory
outside
their
own
segment
Segmentation
proved
to
be
a
hassle
for
programmers
and
is
not
used
in
modern
versions
of
the
Windows
operating
system
IA
contains
string
instructions
that
act
on
entire
strings
of
bytes
or
words
The
operations
include
moving
comparing
or
scanning
for
a
specific
value
In
modern
processors
these
instructions
are
usually
slower
than
performing
the
equivalent
operation
with
a
series
of
simpler
instructions
so
they
are
best
avoided
As
mentioned
earlier
the
x
prefix
is
used
to
choose
between
and
bit
operand
sizes
Other
prefixes
include
ones
used
to
lock
the
bus
to
control
access
to
shared
variables
in
a
multiprocessor
system
to
predict
whether
a
branch
will
be
taken
or
not
and
to
repeat
the
instruction
during
a
string
move
The
bane
of
any
architecture
is
to
run
out
of
memory
capacity
With
bit
addresses
IA
can
access
GB
of
memory
This
was
far
more
than
the
largest
computers
had
in
but
by
the
early
s
it
had
become
limiting
In
AMD
extended
the
address
space
and
register
sizes
to
bits
calling
the
enhanced
architecture
AMD
AMD
has
a
compatibility
mode
that
allows
it
to
run
bit
programs
unmodified
while
the
OS
takes
advantage
of
the
bigger
address
space
In
Intel
gave
in
and
adopted
the
bit
extensions
renaming
them
Extended
Memory
Technology
EM
T
With
bit
addresses
computers
can
access
exabytes
billion
GB
of
memory
For
those
curious
about
more
details
of
the
IA
architecture
the
IA
Intel
Architecture
Software
Developer
s
Manual
is
freely
available
on
Intel
s
Web
site
CHAPTER
SIX
Architecture
Intel
and
Hewlett
Packard
jointly
developed
a
new
bit
architecture
called
IA
in
the
mid
s
It
was
designed
from
a
clean
slate
bypassing
the
convoluted
history
of
IA
taking
advantage
of
years
of
new
research
in
computer
architecture
and
providing
a
bit
address
space
However
IA
has
yet
to
become
a
market
success
Most
computers
needing
the
large
address
space
now
use
the
bit
extensions
of
IA
The
Big
Picture
This
section
has
given
a
taste
of
some
of
the
differences
between
the
MIPS
RISC
architecture
and
the
IA
CISC
architecture
IA
tends
to
have
shorter
programs
because
a
complex
instruction
is
equivalent
to
a
series
of
simple
MIPS
instructions
and
because
the
instructions
are
encoded
to
minimize
memory
use
However
the
IA
architecture
is
a
hodgepodge
of
features
accumulated
over
the
years
some
of
which
are
no
longer
useful
but
must
be
kept
for
compatibility
with
old
programs
It
has
too
few
registers
and
the
instructions
are
difficult
to
decode
Merely
explaining
the
instruction
set
is
difficult
Despite
all
these
failings
IA
is
firmly
entrenched
as
the
dominant
computer
architecture
for
PCs
because
the
value
of
software
compatibility
is
so
great
and
because
the
huge
market
justifies
the
effort
required
to
build
fast
IA
microprocessors
SUMMARY
To
command
a
computer
you
must
speak
its
language
A
computer
architecture
defines
how
to
command
a
processor
Many
different
computer
architectures
are
in
widespread
commercial
use
today
but
once
you
understand
one
learning
others
is
much
easier
The
key
questions
to
ask
when
approaching
a
new
architecture
are
What
is
the
data
word
length
What
are
the
registers
How
is
memory
organized
What
are
the
instructions
MIPS
is
a
bit
architecture
because
it
operates
on
bit
data
The
MIPS
architecture
has
general
purpose
registers
In
principle
almost
any
register
can
be
used
for
any
purpose
However
by
convention
certain
registers
are
reserved
for
certain
purposes
for
ease
of
programming
and
so
that
procedures
written
by
different
programmers
can
communicate
easily
For
example
register
always
holds
the
constant
ra
holds
the
return
address
after
a
jal
instruction
and
a
a
and
v
v
hold
the
arguments
and
return
value
of
a
procedure
MIPS
has
a
byteaddressable
memory
system
with
bit
addresses
The
memory
map
was
described
in
Section
Instructions
are
bits
long
and
must
be
word
aligned
This
chapter
discussed
the
most
commonly
used
MIPS
instructions
The
power
of
defining
a
computer
architecture
is
that
a
program
written
for
any
given
architecture
can
run
on
many
different
implementations
of
that
architecture
For
example
programs
written
for
the
Intel
Summary
Chap
Pentium
processor
in
will
generally
still
run
and
run
much
faster
on
the
Intel
Core
Duo
or
AMD
Athlon
processors
in
In
the
first
part
of
this
book
we
learned
about
the
circuit
and
logic
levels
of
abstraction
In
this
chapter
we
jumped
up
to
the
architecture
level
In
the
next
chapter
we
study
microarchitecture
the
arrangement
of
digital
building
blocks
that
implement
a
processor
architecture
Microarchitecture
is
the
link
between
hardware
and
software
engineering
And
we
believe
it
is
one
of
the
most
exciting
topics
in
all
of
engineering
you
will
learn
to
build
your
own
microprocessor
CHAPTER
SIX
Architecture
Exercises
Exercise
Give
three
examples
from
the
MIPS
architecture
of
each
of
the
architecture
design
principles
simplicity
favors
regularity
make
the
common
case
fast
smaller
is
faster
and
good
design
demands
good
compromises
Explain
how
each
of
your
examples
exhibits
the
design
principle
Exercise
The
MIPS
architecture
has
a
register
set
that
consists
of
bit
registers
Is
it
possible
to
design
a
computer
architecture
without
a
register
set
If
so
briefly
describe
the
architecture
including
the
instruction
set
What
are
advantages
and
disadvantages
of
this
architecture
over
the
MIPS
architecture
Exercise
Consider
memory
storage
of
a
bit
word
stored
at
memory
word
in
a
byte
addressable
memory
a
What
is
the
byte
address
of
memory
word
b
What
are
the
byte
addresses
that
memory
word
spans
c
Draw
the
number
xFF
stored
at
word
in
both
big
endian
and
little
endian
machines
Your
drawing
should
be
similar
to
Figure
Clearly
label
the
byte
address
corresponding
to
each
data
byte
value
Exercise
Explain
how
the
following
program
can
be
used
to
determine
whether
a
computer
is
big
endian
or
little
endian
li
t
xABCD
sw
t
lb
s
Exercise
Write
the
following
strings
using
ASCII
encoding
Write
your
final
answers
in
hexadecimal
a
SOS
b
Cool
c
your
own
name
Exercise
Show
how
the
strings
in
Exercise
are
stored
in
a
byte
addressable
memory
on
a
a
big
endian
machine
and
b
a
little
endian
machine
starting
at
memory
address
x
C
Use
a
memory
diagram
similar
to
Figure
Clearly
indicate
the
memory
address
of
each
byte
on
each
machine
Exercises
Exercise
Convert
the
following
MIPS
assembly
code
into
machine
language
Write
the
instructions
in
hexadecimal
add
t
s
s
lw
t
x
t
addi
s
Exercise
Repeat
Exercise
for
the
following
MIPS
assembly
code
addi
s
sw
t
t
sub
t
s
s
Exercise
Consider
I
type
instructions
a
Which
instructions
from
Exercise
are
I
type
instructions
b
Sign
extend
the
bit
immediate
of
each
instruction
from
part
a
so
that
it
becomes
a
bit
number
Exercise
Convert
the
following
program
from
machine
language
into
MIPS
assembly
language
The
numbers
on
the
left
are
the
instruction
address
in
memory
and
the
numbers
on
the
right
give
the
instruction
at
that
address
Then
reverse
engineer
a
high
level
program
that
would
compile
into
this
assembly
language
routine
and
write
it
Explain
in
words
what
the
program
does
a
is
the
input
and
it
initially
contains
a
positive
number
n
v
is
the
output
x
x
x
x
x
x
a
x
c
x
x
x
x
x
x
x
x
c
x
Exercise
The
nori
instruction
is
not
part
of
the
MIPS
instruction
set
because
the
same
functionality
can
be
implemented
using
existing
instructions
Write
a
short
assembly
code
snippet
that
has
the
following
functionality
t
t
NOR
xF
Use
as
few
instructions
as
possible
CHAPTER
SIX
Architecture
C
Exercise
Implement
the
following
high
level
code
segments
using
the
slt
instruction
Assume
the
integer
variables
g
and
h
are
in
registers
s
and
s
respectively
a
if
g
h
g
g
h
else
g
g
h
b
if
g
h
g
g
else
h
h
c
if
g
h
g
else
h
Exercise
Write
a
procedure
in
a
high
level
language
for
int
find
int
array
int
size
size
specifies
the
number
of
elements
in
the
array
array
specifies
the
base
address
of
the
array
The
procedure
should
return
the
index
number
of
the
first
array
entry
that
holds
the
value
If
no
array
entry
is
it
should
return
the
value
Exercise
The
high
level
procedure
strcpy
copies
the
character
string
x
to
the
character
string
y
high
level
code
void
strcpy
char
x
char
y
int
i
while
x
i
y
i
x
i
i
i
a
Implement
the
strcpy
procedure
in
MIPS
assembly
code
Use
s
for
i
b
Draw
a
picture
of
the
stack
before
during
and
after
the
strcpy
procedure
call
Assume
sp
x
FFFFF
just
before
strcpy
is
called
Exercise
Convert
the
high
level
procedure
from
Exercise
into
MIPS
assembly
code
Exercises
This
simple
string
copy
program
has
a
serious
flaw
it
has
no
way
of
knowing
that
y
has
enough
space
to
receive
x
If
a
malicious
programmer
were
able
to
execute
strcpy
with
a
long
string
x
the
programmer
might
be
able
to
write
bytes
all
over
memory
possibly
even
modifying
code
stored
in
subsequent
memory
locations
With
some
cleverness
the
modified
code
might
take
over
the
machine
This
is
called
a
buffer
overflow
attack
it
is
employed
by
several
nasty
programs
including
the
infamous
Blaster
worm
which
caused
an
estimated
million
in
damages
in
Chapter
qxd
Exercise
Each
number
in
the
Fibonacci
series
is
the
sum
of
the
previous
two
numbers
Table
lists
the
first
few
numbers
in
the
series
fib
n
a
What
is
fib
n
for
n
and
n
b
Write
a
procedure
called
fib
in
a
high
level
language
that
returns
the
Fibonacci
number
for
any
nonnegative
value
of
n
Hint
You
probably
will
want
to
use
a
loop
Clearly
comment
your
code
c
Convert
the
high
level
procedure
of
part
b
into
MIPS
assembly
code
Add
comments
after
every
line
of
code
that
explain
clearly
what
it
does
Use
the
SPIM
simulator
to
test
your
code
on
fib
Exercise
Consider
the
MIPS
assembly
code
below
proc
proc
and
proc
are
non
leaf
procedures
proc
is
a
leaf
procedure
The
code
is
not
shown
for
each
procedure
but
the
comments
indicate
which
registers
are
used
within
each
procedure
x
proc
proc
uses
s
and
s
x
jal
proc
x
proc
proc
uses
s
s
x
C
jal
proc
x
proc
proc
uses
s
s
x
jal
proc
x
proc
proc
uses
no
preserved
registers
x
jr
ra
a
How
many
words
are
the
stack
frames
of
each
procedure
b
Sketch
the
stack
after
proc
is
called
Clearly
indicate
which
registers
are
stored
where
on
the
stack
Give
values
where
possible
CHAPTER
SIX
Architecture
Table
Fibonacci
series
n
fib
n
Ch
Exercise
Ben
Bitdiddle
is
trying
to
compute
the
function
f
a
b
a
b
for
nonnegative
b
He
goes
overboard
in
the
use
of
procedure
calls
and
recursion
and
produces
the
following
high
level
code
for
procedures
f
and
f
high
level
code
for
procedures
f
and
f
int
f
int
a
int
b
int
j
j
a
return
j
a
f
b
int
f
int
x
int
k
k
if
x
return
else
return
k
f
x
Ben
then
translates
the
two
procedures
into
assembly
language
as
follows
He
also
writes
a
procedure
test
that
calls
the
procedure
f
MIPS
assembly
code
f
a
a
a
b
s
j
f
a
x
s
k
x
test
addi
a
a
a
x
addi
a
a
b
x
jal
f
call
f
x
c
loop
j
loop
and
loop
forever
x
f
addi
sp
sp
make
room
on
the
stack
for
s
a
a
and
ra
x
sw
a
sp
save
a
b
x
sw
a
sp
save
a
a
x
c
sw
ra
sp
save
ra
x
sw
s
sp
save
s
x
add
s
a
s
a
j
a
x
add
a
a
place
b
as
argument
for
f
x
c
jal
f
call
f
b
x
lw
a
sp
restore
a
a
after
call
x
lw
a
sp
restore
a
b
after
call
x
add
v
v
s
v
f
b
j
x
c
add
v
v
a
v
f
b
j
a
x
lw
s
sp
restore
s
x
lw
ra
sp
restore
ra
x
addi
sp
sp
restore
sp
stack
pointer
x
c
jr
ra
return
to
point
of
call
x
f
addi
sp
sp
make
room
on
the
stack
for
s
a
and
ra
x
sw
a
sp
save
a
x
x
sw
ra
sp
save
return
address
x
c
sw
s
sp
save
s
x
addi
s
k
Exercises
Chapter
qxd
x
bne
a
else
x
x
addi
v
yes
return
value
should
be
x
c
j
done
and
clean
up
x
else
addi
a
a
no
a
a
x
x
x
jal
f
call
f
x
x
lw
a
sp
restore
a
x
x
c
add
v
v
s
v
f
x
k
x
done
lw
s
sp
restore
s
x
lw
ra
sp
restore
ra
x
addi
sp
sp
restore
sp
x
c
jr
ra
return
to
point
of
call
You
will
probably
find
it
useful
to
make
drawings
of
the
stack
similar
to
the
one
in
Figure
to
help
you
answer
the
following
questions
a
If
the
code
runs
starting
at
test
what
value
is
in
v
when
the
program
gets
to
loop
Does
his
program
correctly
compute
a
b
b
Suppose
Ben
deletes
the
instructions
at
addresses
x
C
and
x
that
save
and
restore
ra
Will
the
program
enter
an
infinite
loop
but
not
crash
crash
cause
the
stack
to
grow
beyond
the
dynamic
data
segment
or
the
PC
to
jump
to
a
location
outside
the
program
produce
an
incorrect
value
in
v
when
the
program
returns
to
loop
if
so
what
value
or
run
correctly
despite
the
deleted
lines
c
Repeat
part
b
when
the
instructions
at
the
following
instruction
addresses
are
deleted
i
x
and
x
instructions
that
save
and
restore
a
ii
x
and
x
instructions
that
save
and
restore
a
iii
x
and
x
instructions
that
save
and
restore
s
iv
x
and
x
instructions
that
save
and
restore
sp
v
x
C
and
x
instructions
that
save
and
restore
s
vi
x
and
x
instructions
that
save
and
restore
ra
vii
x
and
x
instructions
that
save
and
restore
a
Exercise
Convert
the
following
beq
j
and
jal
assembly
instructions
into
machine
code
Instruction
addresses
are
given
to
the
left
of
each
instruction
a
x
beq
t
s
Loop
x
x
x
C
Loop
CHAPTER
SIX
Architecture
Chapte
b
x
beq
t
s
done
x
done
c
x
C
back
x
beq
t
s
back
d
x
jal
proc
x
C
proc
e
x
back
x
C
j
back
Exercise
Consider
the
following
MIPS
assembly
language
snippet
The
numbers
to
the
left
of
each
instruction
indicate
the
instruction
address
x
add
a
a
x
c
jal
f
x
f
jr
ra
x
f
sw
s
s
x
bne
a
else
x
c
j
f
x
else
addi
a
a
x
j
f
a
Translate
the
instruction
sequence
into
machine
code
Write
the
machine
code
instructions
in
hexadecimal
b
List
the
addressing
mode
used
at
each
line
of
code
Exercise
Consider
the
following
C
code
snippet
C
code
void
set
array
int
num
int
i
int
array
for
i
i
i
i
array
i
compare
num
i
int
compare
int
a
int
b
if
sub
a
b
return
else
Exercises
Chapt
return
int
sub
int
a
int
b
return
a
b
a
Implement
the
C
code
snippet
in
MIPS
assembly
language
Use
s
to
hold
the
variable
i
Be
sure
to
handle
the
stack
pointer
appropriately
The
array
is
stored
on
the
stack
of
the
set
array
procedure
see
Section
b
Assume
set
array
is
the
first
procedure
called
Draw
the
status
of
the
stack
before
calling
set
array
and
during
each
procedure
call
Indicate
the
names
of
registers
and
variables
stored
on
the
stack
and
mark
the
location
of
sp
c
How
would
your
code
function
if
you
failed
to
store
ra
on
the
stack
Exercise
Consider
the
following
high
level
procedure
high
level
code
int
f
int
n
int
k
int
b
b
k
if
n
b
else
b
b
n
n
f
n
k
return
b
k
a
Translate
the
high
level
procedure
f
into
MIPS
assembly
language
Pay
particular
attention
to
properly
saving
and
restoring
registers
across
procedure
calls
and
using
the
MIPS
preserved
register
conventions
Clearly
comment
your
code
You
can
use
the
MIPS
mult
mfhi
and
mflo
instructions
The
procedure
starts
at
instruction
address
x
Keep
local
variable
b
in
s
b
Step
through
your
program
from
part
a
by
hand
for
the
case
of
f
Draw
a
picture
of
the
stack
similar
to
the
one
in
Figure
c
Write
the
register
name
and
data
value
stored
at
each
location
in
the
stack
and
keep
track
of
the
stack
pointer
value
sp
You
might
also
find
it
useful
to
keep
track
of
the
values
in
a
a
v
and
s
throughout
execution
Assume
that
when
f
is
called
s
xABCD
and
ra
x
What
is
the
final
value
of
v
Exercise
What
is
the
range
of
instruction
addresses
to
which
conditional
branches
such
as
beq
and
bne
can
branch
in
MIPS
Give
your
answer
in
number
of
instructions
relative
to
the
conditional
branch
instruction
CHAPTER
SIX
Architecture
Chapter
Exercise
The
following
questions
examine
the
limitations
of
the
jump
instruction
j
Give
your
answer
in
number
of
instructions
relative
to
the
jump
instruction
a
In
the
worst
case
how
far
can
the
jump
instruction
j
jump
forward
i
e
to
higher
addresses
The
worst
case
is
when
the
jump
instruction
cannot
jump
far
Explain
using
words
and
examples
as
needed
b
In
the
best
case
how
far
can
the
jump
instruction
j
jump
forward
The
best
case
is
when
the
jump
instruction
can
jump
the
farthest
Explain
c
In
the
worst
case
how
far
can
the
jump
instruction
j
jump
backward
to
lower
addresses
Explain
d
In
the
best
case
how
far
can
the
jump
instruction
j
jump
backward
Explain
Exercise
Explain
why
it
is
advantageous
to
have
a
large
address
field
addr
in
the
machine
format
for
the
jump
instructions
j
and
jal
Exercise
Write
assembly
code
that
jumps
to
the
instruction
Minstructions
from
the
first
instruction
Recall
that
Minstruction
instructions
instructions
Assume
that
your
code
begins
at
address
x
Use
a
minimum
number
of
instructions
Exercise
Write
a
procedure
in
high
level
code
that
takes
a
ten
entry
array
of
bit
integers
stored
in
little
endian
format
and
converts
it
to
big
endian
format
After
writing
the
high
level
code
convert
it
to
MIPS
assembly
code
Comment
all
your
code
and
use
a
minimum
number
of
instructions
Exercise
Consider
two
strings
string
and
string
a
Write
high
level
code
for
a
procedure
called
concat
that
concatenates
joins
together
the
two
strings
void
concat
char
string
char
string
char
stringconcat
The
procedure
does
not
return
a
value
It
concatenates
string
and
string
and
places
the
resulting
string
in
stringconcat
You
may
assume
that
the
character
array
stringconcat
is
large
enough
to
accommodate
the
concatenated
string
b
Convert
the
procedure
from
part
a
into
MIPS
assembly
language
Exercise
Write
a
MIPS
assembly
program
that
adds
two
positive
singleprecision
floating
point
numbers
held
in
s
and
s
Do
not
use
any
of
the
MIPS
floating
point
instructions
You
need
not
worry
about
any
of
the
encodings
that
are
reserved
for
special
purposes
e
g
NANs
INF
or
numbers
that
overflow
or
underflow
Use
the
SPIM
simulator
to
test
your
code
You
will
need
to
manually
set
the
values
of
s
and
s
to
test
your
code
Demonstrate
that
your
code
functions
reliably
Exercises
Ch
Exercise
Show
how
the
following
MIPS
program
would
be
loaded
into
memory
and
executed
MIPS
assembly
code
main
lw
a
x
lw
a
y
jal
diff
jr
ra
diff
sub
v
a
a
jr
ra
a
First
show
the
instruction
address
next
to
each
assembly
instruction
b
Draw
the
symbol
table
showing
the
labels
and
their
addresses
c
Convert
all
instructions
into
machine
code
d
How
big
how
many
bytes
are
the
data
and
text
segments
e
Sketch
a
memory
map
showing
where
data
and
instructions
are
stored
Exercise
Show
the
MIPS
instructions
that
implement
the
following
pseudoinstructions
You
may
use
the
assembler
register
at
but
you
may
not
corrupt
overwrite
any
other
registers
a
beq
t
imm
L
b
ble
t
t
L
c
bgt
t
t
L
d
bge
t
t
L
e
addi
t
imm
f
lw
t
imm
s
g
rol
t
t
rotate
t
left
by
and
put
the
result
in
t
h
ror
s
t
rotate
t
right
by
and
put
the
result
in
s
CHAPTER
SIX
Architecture
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
but
are
usually
open
to
any
assembly
language
Question
Write
MIPS
assembly
code
for
swapping
the
contents
of
two
registers
t
and
t
You
may
not
use
any
other
registers
Question
Suppose
you
are
given
an
array
of
both
positive
and
negative
integers
Write
MIPS
assembly
code
that
finds
the
subset
of
the
array
with
the
largest
sum
Assume
that
the
array
s
base
address
and
the
number
of
array
elements
are
in
a
and
a
respectively
Your
code
should
place
the
resulting
subset
of
the
array
starting
at
base
address
a
Write
code
that
runs
as
fast
as
possible
Question
You
are
given
an
array
that
holds
a
C
string
The
string
forms
a
sentence
Design
an
algorithm
for
reversing
the
words
in
the
sentence
and
storing
the
new
sentence
back
in
the
array
Implement
your
algorithm
using
MIPS
assembly
code
Question
Design
an
algorithm
for
counting
the
number
of
s
in
a
bit
number
Implement
your
algorithm
using
MIPS
assembly
code
Question
Write
MIPS
assembly
code
to
reverse
the
bits
in
a
register
Use
as
few
instructions
as
possible
Assume
the
register
of
interest
is
t
Question
Write
MIPS
assembly
code
to
test
whether
overflow
occurs
when
t
and
t
are
added
Use
a
minimum
number
of
instructions
Question
Design
an
algorithm
for
testing
whether
a
given
string
is
a
palindrome
Recall
that
a
palindrome
is
a
word
that
is
the
same
forward
and
backward
For
example
the
words
wow
and
racecar
are
palindromes
Implement
your
algorithm
using
MIPS
assembly
code
Interview
Questions
Introduction
Performance
Analysis
Single
Cycle
Processor
Multicycle
Processor
Pipelined
Processor
HDL
Representation
Exceptions
Advanced
Microarchitecture
Real
World
Perspective
IA
Microarchitecture
Summary
Exercises
Interview
Questions
Microarchitecture
INTRODUCTION
In
this
chapter
you
will
learn
how
to
piece
together
a
MIPS
microprocessor
Indeed
you
will
puzzle
out
three
different
versions
each
with
different
trade
offs
between
performance
cost
and
complexity
To
the
uninitiated
building
a
microprocessor
may
seem
like
black
magic
But
it
is
actually
relatively
straightforward
and
by
this
point
you
have
learned
everything
you
need
to
know
Specifically
you
have
learned
to
design
combinational
and
sequential
logic
given
functional
and
timing
specifications
You
are
familiar
with
circuits
for
arithmetic
and
memory
And
you
have
learned
about
the
MIPS
architecture
which
specifies
the
programmer
s
view
of
the
MIPS
processor
in
terms
of
registers
instructions
and
memory
This
chapter
covers
microarchitecture
which
is
the
connection
between
logic
and
architecture
Microarchitecture
is
the
specific
arrangement
of
registers
ALUs
finite
state
machines
FSMs
memories
and
other
logic
building
blocks
needed
to
implement
an
architecture
A
particular
architecture
such
as
MIPS
may
have
many
different
microarchitectures
each
with
different
trade
offs
of
performance
cost
and
complexity
They
all
run
the
same
programs
but
their
internal
designs
vary
widely
We
will
design
three
different
microarchitectures
in
this
chapter
to
illustrate
the
trade
offs
This
chapter
draws
heavily
on
David
Patterson
and
John
Hennessy
s
classic
MIPS
designs
in
their
text
Computer
Organization
and
Design
They
have
generously
shared
their
elegant
designs
which
have
the
virtue
of
illustrating
a
real
commercial
architecture
while
being
relatively
simple
and
easy
to
understand
Architectural
State
and
Instruction
Set
Recall
that
a
computer
architecture
is
defined
by
its
instruction
set
and
architectural
state
The
architectural
state
for
the
MIPS
processor
consists
of
the
program
counter
and
the
registers
Any
MIPS
microarchitecture
must
contain
all
of
this
state
Based
on
the
current
architectural
state
the
processor
executes
a
particular
instruction
with
a
particular
set
of
data
to
produce
a
new
architectural
state
Some
microarchitectures
contain
additional
nonarchitectural
state
to
either
simplify
the
logic
or
improve
performance
we
will
point
this
out
as
it
arises
To
keep
the
microarchitectures
easy
to
understand
we
consider
only
a
subset
of
the
MIPS
instruction
set
Specifically
we
handle
the
following
instructions
R
type
arithmetic
logic
instructions
add
sub
and
or
slt
Memory
instructions
lw
sw
Branches
beq
After
building
the
microarchitectures
with
these
instructions
we
extend
them
to
handle
addi
and
j
These
particular
instructions
were
chosen
because
they
are
sufficient
to
write
many
interesting
programs
Once
you
understand
how
to
implement
these
instructions
you
can
expand
the
hardware
to
handle
others
Design
Process
We
will
divide
our
microarchitectures
into
two
interacting
parts
the
datapath
and
the
control
The
datapath
operates
on
words
of
data
It
contains
structures
such
as
memories
registers
ALUs
and
multiplexers
MIPS
is
a
bit
architecture
so
we
will
use
a
bit
datapath
The
control
unit
receives
the
current
instruction
from
the
datapath
and
tells
the
datapath
how
to
execute
that
instruction
Specifically
the
control
unit
produces
multiplexer
select
register
enable
and
memory
write
signals
to
control
the
operation
of
the
datapath
A
good
way
to
design
a
complex
system
is
to
start
with
hardware
containing
the
state
elements
These
elements
include
the
memories
and
the
architectural
state
the
program
counter
and
registers
Then
add
blocks
of
combinational
logic
between
the
state
elements
to
compute
the
new
state
based
on
the
current
state
The
instruction
is
read
from
part
of
memory
load
and
store
instructions
then
read
or
write
data
from
another
part
of
memory
Hence
it
is
often
convenient
to
partition
the
overall
memory
into
two
smaller
memories
one
containing
instructions
and
the
other
containing
data
Figure
shows
a
block
diagram
with
the
four
state
elements
the
program
counter
register
file
and
instruction
and
data
memories
In
Figure
heavy
lines
are
used
to
indicate
bit
data
busses
Medium
lines
are
used
to
indicate
narrower
busses
such
as
the
bit
address
busses
on
the
register
file
Narrow
blue
lines
are
used
to
indicate
CHAPTER
SEVEN
Microarchitecture
David
Patterson
was
the
first
in
his
family
to
graduate
from
college
UCLA
He
has
been
a
professor
of
computer
science
at
UC
Berkeley
since
where
he
coinvented
RISC
the
Reduced
Instruction
Set
Computer
In
he
developed
the
SPARC
architecture
used
by
Sun
Microsystems
He
is
also
the
father
of
RAID
Redundant
Array
of
Inexpensive
Disks
and
NOW
Network
of
Workstations
John
Hennessy
is
president
of
Stanford
University
and
has
been
a
professor
of
electrical
engineering
and
computer
science
there
since
He
coinvented
RISC
He
developed
the
MIPS
architecture
at
Stanford
in
and
cofounded
MIPS
Computer
Systems
As
of
more
than
million
MIPS
microprocessors
have
been
sold
In
their
copious
free
time
these
two
modern
paragons
write
textbooks
for
recreation
and
relaxation
Cha
control
signals
such
as
the
register
file
write
enable
We
will
use
this
convention
throughout
the
chapter
to
avoid
cluttering
diagrams
with
bus
widths
Also
state
elements
usually
have
a
reset
input
to
put
them
into
a
known
state
at
start
up
Again
to
save
clutter
this
reset
is
not
shown
The
program
counter
is
an
ordinary
bit
register
Its
output
PC
points
to
the
current
instruction
Its
input
PC
indicates
the
address
of
the
next
instruction
The
instruction
memory
has
a
single
read
port
It
takes
a
bit
instruction
address
input
A
and
reads
the
bit
data
i
e
instruction
from
that
address
onto
the
read
data
output
RD
The
element
bit
register
file
has
two
read
ports
and
one
write
port
The
read
ports
take
bit
address
inputs
A
and
A
each
specifying
one
of
registers
as
source
operands
They
read
the
bit
register
values
onto
read
data
outputs
RD
and
RD
respectively
The
write
port
takes
a
bit
address
input
A
a
bit
write
data
input
WD
a
write
enable
input
WE
and
a
clock
If
the
write
enable
is
the
register
file
writes
the
data
into
the
specified
register
on
the
rising
edge
of
the
clock
The
data
memory
has
a
single
read
write
port
If
the
write
enable
WE
is
it
writes
data
WD
into
address
A
on
the
rising
edge
of
the
clock
If
the
write
enable
is
it
reads
address
A
onto
RD
The
instruction
memory
register
file
and
data
memory
are
all
read
combinationally
In
other
words
if
the
address
changes
the
new
data
appears
at
RD
after
some
propagation
delay
no
clock
is
involved
They
are
written
only
on
the
rising
edge
of
the
clock
In
this
fashion
the
state
of
the
system
is
changed
only
at
the
clock
edge
The
address
data
and
write
enable
must
setup
sometime
before
the
clock
edge
and
must
remain
stable
until
a
hold
time
after
the
clock
edge
Because
the
state
elements
change
their
state
only
on
the
rising
edge
of
the
clock
they
are
synchronous
sequential
circuits
The
microprocessor
is
Introduction
Resetting
the
PC
At
the
very
least
the
program
counter
must
have
a
reset
signal
to
initialize
its
value
when
the
processor
turns
on
MIPS
processors
initialize
the
PC
to
xBFC
on
reset
and
begin
executing
code
to
start
up
the
operating
system
OS
The
OS
then
loads
an
application
program
at
x
and
begins
executing
it
For
simplicity
in
this
chapter
we
will
reset
the
PC
to
x
and
place
our
programs
there
instead
This
is
an
oversimplification
used
to
treat
the
instruction
memory
as
a
ROM
in
most
real
processors
the
instruction
memory
must
be
writable
so
that
the
OS
can
load
a
new
program
into
memory
The
multicycle
microarchitecture
described
in
Section
is
more
realistic
in
that
it
uses
a
combined
memory
for
instructions
and
data
that
can
be
both
read
and
written
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
A
RD
Data
Memory
WD
PC
PC
WE
CLK
Figure
State
elements
of
MIPS
processor
Ch
built
of
clocked
state
elements
and
combinational
logic
so
it
too
is
a
synchronous
sequential
circuit
Indeed
the
processor
can
be
viewed
as
a
giant
finite
state
machine
or
as
a
collection
of
simpler
interacting
state
machines
MIPS
Microarchitectures
In
this
chapter
we
develop
three
microarchitectures
for
the
MIPS
processor
architecture
single
cycle
multicycle
and
pipelined
They
differ
in
the
way
that
the
state
elements
are
connected
together
and
in
the
amount
of
nonarchitectural
state
The
single
cycle
microarchitecture
executes
an
entire
instruction
in
one
cycle
It
is
easy
to
explain
and
has
a
simple
control
unit
Because
it
completes
the
operation
in
one
cycle
it
does
not
require
any
nonarchitectural
state
However
the
cycle
time
is
limited
by
the
slowest
instruction
The
multicycle
microarchitecture
executes
instructions
in
a
series
of
shorter
cycles
Simpler
instructions
execute
in
fewer
cycles
than
complicated
ones
Moreover
the
multicycle
microarchitecture
reduces
the
hardware
cost
by
reusing
expensive
hardware
blocks
such
as
adders
and
memories
For
example
the
adder
may
be
used
on
several
different
cycles
for
several
purposes
while
carrying
out
a
single
instruction
The
multicycle
microprocessor
accomplishes
this
by
adding
several
nonarchitectural
registers
to
hold
intermediate
results
The
multicycle
processor
executes
only
one
instruction
at
a
time
but
each
instruction
takes
multiple
clock
cycles
The
pipelined
microarchitecture
applies
pipelining
to
the
single
cycle
microarchitecture
It
therefore
can
execute
several
instructions
simultaneously
improving
the
throughput
significantly
Pipelining
must
add
logic
to
handle
dependencies
between
simultaneously
executing
instructions
It
also
requires
nonarchitectural
pipeline
registers
The
added
logic
and
registers
are
worthwhile
all
commercial
high
performance
processors
use
pipelining
today
We
explore
the
details
and
trade
offs
of
these
three
microarchitectures
in
the
subsequent
sections
At
the
end
of
the
chapter
we
briefly
mention
additional
techniques
that
are
used
to
get
even
more
speed
in
modern
high
performance
microprocessors
PERFORMANCE
ANALYSIS
As
we
mentioned
a
particular
processor
architecture
can
have
many
microarchitectures
with
different
cost
and
performance
trade
offs
The
cost
depends
on
the
amount
of
hardware
required
and
the
implementation
technology
Each
year
CMOS
processes
can
pack
more
transistors
on
a
chip
for
the
same
amount
of
money
and
processors
take
advantage
CHAPTER
SEVEN
Microarchitecture
of
these
additional
transistors
to
deliver
more
performance
Precise
cost
calculations
require
detailed
knowledge
of
the
implementation
technology
but
in
general
more
gates
and
more
memory
mean
more
dollars
This
section
lays
the
foundation
for
analyzing
performance
There
are
many
ways
to
measure
the
performance
of
a
computer
system
and
marketing
departments
are
infamous
for
choosing
the
method
that
makes
their
computer
look
fastest
regardless
of
whether
the
measurement
has
any
correlation
to
real
world
performance
For
example
Intel
and
Advanced
Micro
Devices
AMD
both
sell
compatible
microprocessors
conforming
to
the
IA
architecture
Intel
Pentium
III
and
Pentium
microprocessors
were
largely
advertised
according
to
clock
frequency
in
the
late
s
and
early
s
because
Intel
offered
higher
clock
frequencies
than
its
competitors
However
Intel
s
main
competitor
AMD
sold
Athlon
microprocessors
that
executed
programs
faster
than
Intel
s
chips
at
the
same
clock
frequency
What
is
a
consumer
to
do
The
only
gimmick
free
way
to
measure
performance
is
by
measuring
the
execution
time
of
a
program
of
interest
to
you
The
computer
that
executes
your
program
fastest
has
the
highest
performance
The
next
best
choice
is
to
measure
the
total
execution
time
of
a
collection
of
programs
that
are
similar
to
those
you
plan
to
run
this
may
be
necessary
if
you
haven
t
written
your
program
yet
or
if
somebody
else
who
doesn
t
have
your
program
is
making
the
measurements
Such
collections
of
programs
are
called
benchmarks
and
the
execution
times
of
these
programs
are
commonly
published
to
give
some
indication
of
how
a
processor
performs
The
execution
time
of
a
program
measured
in
seconds
is
given
by
Equation
The
number
of
instructions
in
a
program
depends
on
the
processor
architecture
Some
architectures
have
complicated
instructions
that
do
more
work
per
instruction
thus
reducing
the
number
of
instructions
in
a
program
However
these
complicated
instructions
are
often
slower
to
execute
in
hardware
The
number
of
instructions
also
depends
enormously
on
the
cleverness
of
the
programmer
For
the
purposes
of
this
chapter
we
will
assume
that
we
are
executing
known
programs
on
a
MIPS
processor
so
the
number
of
instructions
for
each
program
is
constant
independent
of
the
microarchitecture
The
number
of
cycles
per
instruction
often
called
CPI
is
the
number
of
clock
cycles
required
to
execute
an
average
instruction
It
is
the
reciprocal
of
the
throughput
instructions
per
cycle
or
IPC
Different
microarchitectures
have
different
CPIs
In
this
chapter
we
will
assume
Execution
Time
instructions
cycles
instructionseconds
cycle
Performance
Analysis
Chapte
we
have
an
ideal
memory
system
that
does
not
affect
the
CPI
In
Chapter
we
examine
how
the
processor
sometimes
has
to
wait
for
the
memory
which
increases
the
CPI
The
number
of
seconds
per
cycle
is
the
clock
period
Tc
The
clock
period
is
determined
by
the
critical
path
through
the
logic
on
the
processor
Different
microarchitectures
have
different
clock
periods
Logic
and
circuit
designs
also
significantly
affect
the
clock
period
For
example
a
carry
lookahead
adder
is
faster
than
a
ripple
carry
adder
Manufacturing
advances
have
historically
doubled
transistor
speeds
every
years
so
a
microprocessor
built
today
will
be
much
faster
than
one
from
last
decade
even
if
the
microarchitecture
and
logic
are
unchanged
The
challenge
of
the
microarchitect
is
to
choose
the
design
that
minimizes
the
execution
time
while
satisfying
constraints
on
cost
and
or
power
consumption
Because
microarchitectural
decisions
affect
both
CPI
and
Tc
and
are
influenced
by
logic
and
circuit
designs
determining
the
best
choice
requires
careful
analysis
There
are
many
other
factors
that
affect
overall
computer
performance
For
example
the
hard
disk
the
memory
the
graphics
system
and
the
network
connection
may
be
limiting
factors
that
make
processor
performance
irrelevant
The
fastest
microprocessor
in
the
world
doesn
t
help
surfing
the
Internet
on
a
dial
up
connection
But
these
other
factors
are
beyond
the
scope
of
this
book
SINGLE
CYCLE
PROCESSOR
We
first
design
a
MIPS
microarchitecture
that
executes
instructions
in
a
single
cycle
We
begin
constructing
the
datapath
by
connecting
the
state
elements
from
Figure
with
combinational
logic
that
can
execute
the
various
instructions
Control
signals
determine
which
specific
instruction
is
carried
out
by
the
datapath
at
any
given
time
The
controller
contains
combinational
logic
that
generates
the
appropriate
control
signals
based
on
the
current
instruction
We
conclude
by
analyzing
the
performance
of
the
single
cycle
processor
Single
Cycle
Datapath
This
section
gradually
develops
the
single
cycle
datapath
adding
one
piece
at
a
time
to
the
state
elements
from
Figure
The
new
connections
are
emphasized
in
black
or
blue
for
new
control
signals
while
the
hardware
that
has
already
been
studied
is
shown
in
gray
The
program
counter
PC
register
contains
the
address
of
the
instruction
to
execute
The
first
step
is
to
read
this
instruction
from
instruction
memory
Figure
shows
that
the
PC
is
simply
connected
to
the
address
input
of
the
instruction
memory
The
instruction
memory
reads
out
or
fetches
the
bit
instruction
labeled
Instr
CHAPTER
SEVEN
Microarchitecture
The
processor
s
actions
depend
on
the
specific
instruction
that
was
fetched
First
we
will
work
out
the
datapath
connections
for
the
lw
instruction
Then
we
will
consider
how
to
generalize
the
datapath
to
handle
the
other
instructions
For
a
lw
instruction
the
next
step
is
to
read
the
source
register
containing
the
base
address
This
register
is
specified
in
the
rs
field
of
the
instruction
Instr
These
bits
of
the
instruction
are
connected
to
the
address
input
of
one
of
the
register
file
read
ports
A
as
shown
in
Figure
The
register
file
reads
the
register
value
onto
RD
The
lw
instruction
also
requires
an
offset
The
offset
is
stored
in
the
immediate
field
of
the
instruction
Instr
Because
the
bit
immediate
might
be
either
positive
or
negative
it
must
be
sign
extended
to
bits
as
shown
in
Figure
The
bit
sign
extended
value
is
called
SignImm
Recall
from
Section
that
sign
extension
simply
copies
the
sign
bit
most
significant
bit
of
a
short
input
into
all
of
the
upper
bits
of
the
longer
output
Specifically
SignImm
Instr
and
SignImm
Instr
Single
Cycle
Processor
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
PC
Instr
A
RD
Data
Memory
WD
WE
CLK
Figure
Fetch
instruction
from
memory
Instr
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
PC
A
RD
Data
Memory
WD
WE
CLK
Figure
Read
source
operand
from
register
file
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
A
RD
Data
Memory
WD
WE
CLK
Figure
Sign
extend
the
immediate
The
processor
must
add
the
base
address
to
the
offset
to
find
the
address
to
read
from
memory
Figure
introduces
an
ALU
to
perform
this
addition
The
ALU
receives
two
operands
SrcA
and
SrcB
SrcA
comes
from
the
register
file
and
SrcB
comes
from
the
sign
extended
immediate
The
ALU
can
perform
many
operations
as
was
described
in
Section
The
bit
ALUControl
signal
specifies
the
operation
The
ALU
generates
a
bit
ALUResult
and
a
Zero
flag
that
indicates
whether
ALUResult
For
a
lw
instruction
the
ALUControl
signal
should
be
set
to
to
add
the
base
address
and
offset
ALUResult
is
sent
to
the
data
memory
as
the
address
for
the
load
instruction
as
shown
in
Figure
The
data
is
read
from
the
data
memory
onto
the
ReadData
bus
then
written
back
to
the
destination
register
in
the
register
file
at
the
end
of
the
cycle
as
shown
in
Figure
Port
of
the
register
file
is
the
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
SrcA
Zero
CLK
ALUControl
ALU
Figure
Compute
memory
address
A
A
WD
RD
RD
WE
A
SignImm
CLK
A
RD
Instruction
Memory
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
SrcA
RegWrite
Zero
CLK
ALUControl
ALU
Figure
Write
data
back
to
register
file
write
port
The
destination
register
for
the
lw
instruction
is
specified
in
the
rt
field
Instr
which
is
connected
to
the
port
address
input
A
of
the
register
file
The
ReadData
bus
is
connected
to
the
port
write
data
input
WD
of
the
register
file
A
control
signal
called
RegWrite
is
connected
to
the
port
write
enable
input
WE
and
is
asserted
during
a
lw
instruction
so
that
the
data
value
is
written
into
the
register
file
The
write
takes
place
on
the
rising
edge
of
the
clock
at
the
end
of
the
cycle
While
the
instruction
is
being
executed
the
processor
must
compute
the
address
of
the
next
instruction
PC
Because
instructions
are
bits
bytes
the
next
instruction
is
at
PC
Figure
uses
another
adder
to
increment
the
PC
by
The
new
address
is
written
into
the
program
counter
on
the
next
rising
edge
of
the
clock
This
completes
the
datapath
for
the
lw
instruction
Next
let
us
extend
the
datapath
to
also
handle
the
sw
instruction
Like
the
lw
instruction
the
sw
instruction
reads
a
base
address
from
port
of
the
register
and
sign
extends
an
immediate
The
ALU
adds
the
base
address
to
the
immediate
to
find
the
memory
address
All
of
these
functions
are
already
supported
by
the
datapath
The
sw
instruction
also
reads
a
second
register
from
the
register
file
and
writes
it
to
the
data
memory
Figure
shows
the
new
connections
for
this
function
The
register
is
specified
in
the
rt
field
Instr
These
bits
of
the
instruction
are
connected
to
the
second
register
file
read
port
A
The
register
value
is
read
onto
the
RD
port
It
is
connected
to
the
write
data
port
of
the
data
memory
The
write
enable
port
of
the
data
memory
WE
is
controlled
by
MemWrite
For
a
sw
instruction
MemWrite
to
write
the
data
to
memory
ALUControl
to
add
the
base
address
Single
Cycle
Processor
CLK
SignImm
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
SrcA
PCPlus
Result
RegWrite
Zero
CLK
ALUControl
ALU
Figure
Determine
address
of
next
instruction
for
PC
C
and
offset
and
RegWrite
because
nothing
should
be
written
to
the
register
file
Note
that
data
is
still
read
from
the
address
given
to
the
data
memory
but
that
this
ReadData
is
ignored
because
RegWrite
Next
consider
extending
the
datapath
to
handle
the
R
type
instructions
add
sub
and
or
and
slt
All
of
these
instructions
read
two
registers
from
the
register
file
perform
some
ALU
operation
on
them
and
write
the
result
back
to
a
third
register
file
They
differ
only
in
the
specific
ALU
operation
Hence
they
can
all
be
handled
with
the
same
hardware
using
different
ALUControl
signals
Figure
shows
the
enhanced
datapath
handling
R
type
instructions
The
register
file
reads
two
registers
The
ALU
performs
an
operation
on
these
two
registers
In
Figure
the
ALU
always
received
its
SrcB
operand
from
the
sign
extended
immediate
SignImm
Now
we
add
a
multiplexer
to
choose
SrcB
from
either
the
register
file
RD
port
or
SignImm
The
multiplexer
is
controlled
by
a
new
signal
ALUSrc
ALUSrc
is
for
R
type
instructions
to
choose
SrcB
from
the
register
file
it
is
for
lw
and
sw
to
choose
SignImm
This
principle
of
enhancing
the
datapath
s
capabilities
by
adding
a
multiplexer
to
choose
inputs
from
several
possibilities
is
extremely
useful
Indeed
we
will
apply
it
twice
more
to
complete
the
handling
of
R
type
instructions
In
Figure
the
register
file
always
got
its
write
data
from
the
data
memory
However
R
type
instructions
write
the
ALUResult
to
the
register
file
Therefore
we
add
another
multiplexer
to
choose
between
ReadData
and
ALUResult
We
call
its
output
Result
This
multiplexer
is
controlled
by
another
new
signal
MemtoReg
MemtoReg
is
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
Result
RegWrite
MemWrite
Zero
CLK
ALUControl
ALU
Figure
Write
data
to
memory
for
sw
instruction
for
R
type
instructions
to
choose
Result
from
the
ALUResult
it
is
for
lw
to
choose
ReadData
We
don
t
care
about
the
value
of
MemtoReg
for
sw
because
sw
does
not
write
to
the
register
file
Similarly
in
Figure
the
register
to
write
was
specified
by
the
rt
field
of
the
instruction
Instr
However
for
R
type
instructions
the
register
is
specified
by
the
rd
field
Instr
Thus
we
add
a
third
multiplexer
to
choose
WriteReg
from
the
appropriate
field
of
the
instruction
The
multiplexer
is
controlled
by
RegDst
RegDst
is
for
R
type
instructions
to
choose
WriteReg
from
the
rd
field
Instr
it
is
for
lw
to
choose
the
rt
field
Instr
We
don
t
care
about
the
value
of
RegDst
for
sw
because
sw
does
not
write
to
the
register
file
Finally
let
us
extend
the
datapath
to
handle
beq
beq
compares
two
registers
If
they
are
equal
it
takes
the
branch
by
adding
the
branch
offset
to
the
program
counter
Recall
that
the
offset
is
a
positive
or
negative
number
stored
in
the
imm
field
of
the
instruction
Instr
The
offset
indicates
the
number
of
instructions
to
branch
past
Hence
the
immediate
must
be
sign
extended
and
multiplied
by
to
get
the
new
program
counter
value
PC
PC
SignImm
Figure
shows
the
datapath
modifications
The
next
PC
value
for
a
taken
branch
PCBranch
is
computed
by
shifting
SignImm
left
by
bits
then
adding
it
to
PCPlus
The
left
shift
by
is
an
easy
way
to
multiply
by
because
a
shift
by
a
constant
amount
involves
just
wires
The
two
registers
are
compared
by
computing
SrcA
SrcB
using
the
ALU
If
ALUResult
is
as
indicated
by
the
Zero
flag
from
the
ALU
the
registers
are
equal
We
add
a
multiplexer
to
choose
PC
from
either
PCPlus
or
PCBranch
PCBranch
is
selected
if
the
instruction
is
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
WriteReg
Result
RegWrite
RegDst
ALUSrc
MemWrite
MemtoReg
Zero
CLK
ALUControl
varies
ALU
Figure
Datapath
enhancements
for
R
type
instruction
Cha
a
branch
and
the
Zero
flag
is
asserted
Hence
Branch
is
for
beq
and
for
other
instructions
For
beq
ALUControl
so
the
ALU
performs
a
subtraction
ALUSrc
to
choose
SrcB
from
the
register
file
RegWrite
and
MemWrite
are
because
a
branch
does
not
write
to
the
register
file
or
memory
We
don
t
care
about
the
values
of
RegDst
and
MemtoReg
because
the
register
file
is
not
written
This
completes
the
design
of
the
single
cycle
MIPS
processor
datapath
We
have
illustrated
not
only
the
design
itself
but
also
the
design
process
in
which
the
state
elements
are
identified
and
the
combinational
logic
connecting
the
state
elements
is
systematically
added
In
the
next
section
we
consider
how
to
compute
the
control
signals
that
direct
the
operation
of
our
datapath
Single
Cycle
Control
The
control
unit
computes
the
control
signals
based
on
the
opcode
and
funct
fields
of
the
instruction
Instr
and
Instr
Figure
shows
the
entire
single
cycle
MIPS
processor
with
the
control
unit
attached
to
the
datapath
Most
of
the
control
information
comes
from
the
opcode
but
R
type
instructions
also
use
the
funct
field
to
determine
the
ALU
operation
Thus
we
will
simplify
our
design
by
factoring
the
control
unit
into
two
blocks
of
combinational
logic
as
shown
in
Figure
The
main
decoder
computes
most
of
the
outputs
from
the
opcode
It
also
determines
a
bit
ALUOp
signal
The
ALU
decoder
uses
this
ALUOp
signal
in
conjunction
with
the
funct
field
to
compute
ALUControl
The
meaning
of
the
ALUOp
signal
is
given
in
Table
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
Zero
PCSrc
CLK
ALUControl
x
x
ALU
Figure
Datapath
enhancements
for
beq
instruction
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
Opcode
Control
Unit
Funct
ALUControl
Main
Decoder
ALUOp
ALU
Decoder
RegWrite
Figure
Control
unit
internal
structure
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
ALU
Figure
Complete
single
cycle
MIPS
processor
Table
ALUOp
encoding
ALUOp
Meaning
add
subtract
look
at
funct
field
n
a
Table
is
a
truth
table
for
the
ALU
decoder
Recall
that
the
meanings
of
the
three
ALUControl
signals
were
given
in
Table
Because
ALUOp
is
never
the
truth
table
can
use
don
t
care
s
X
and
X
instead
of
and
to
simplify
the
logic
When
ALUOp
is
or
the
ALU
should
add
or
subtract
respectively
When
ALUOp
is
the
decoder
examines
the
funct
field
to
determine
the
ALUControl
Note
that
for
the
R
type
instructions
we
implement
the
first
two
bits
of
the
funct
field
are
always
so
we
may
ignore
them
to
simplify
the
decoder
The
control
signals
for
each
instruction
were
described
as
we
built
the
datapath
Table
is
a
truth
table
for
the
main
decoder
that
summarizes
the
control
signals
as
a
function
of
the
opcode
All
R
type
instructions
use
the
same
main
decoder
values
they
differ
only
in
the
ALU
decoder
output
Recall
that
for
instructions
that
do
not
write
to
the
register
file
e
g
sw
and
beq
the
RegDst
and
MemtoReg
control
signals
are
don
t
cares
X
the
address
and
data
to
the
register
write
port
do
not
matter
because
RegWrite
is
not
asserted
The
logic
for
the
decoder
can
be
designed
using
your
favorite
techniques
for
combinational
logic
design
Example
SINGLE
CYCLE
PROCESSOR
OPERATION
Determine
the
values
of
the
control
signals
and
the
portions
of
the
datapath
that
are
used
when
executing
an
or
instruction
Solution
Figure
illustrates
the
control
signals
and
flow
of
data
during
execution
of
the
or
instruction
The
PC
points
to
the
memory
location
holding
the
instruction
and
the
instruction
memory
fetches
this
instruction
The
main
flow
of
data
through
the
register
file
and
ALU
is
represented
with
a
dashed
blue
line
The
register
file
reads
the
two
source
operands
specified
by
Instr
and
Instr
SrcB
should
come
from
the
second
port
of
the
register
CHAPTER
SEVEN
Microarchitecture
Table
ALU
decoder
truth
table
ALUOp
Funct
ALUControl
X
add
X
X
subtract
X
add
add
X
sub
subtract
X
and
and
X
or
or
X
slt
set
less
than
Table
Main
decoder
truth
table
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
R
type
lw
sw
X
X
beq
X
X
file
not
SignImm
so
ALUSrc
must
be
or
is
an
R
type
instruction
so
ALUOp
is
indicating
that
ALUControl
should
be
determined
from
the
funct
field
to
be
Result
is
taken
from
the
ALU
so
MemtoReg
is
The
result
is
written
to
the
register
file
so
RegWrite
is
The
instruction
does
not
write
memory
so
MemWrite
The
selection
of
the
destination
register
is
also
shown
with
a
dashed
blue
line
The
destination
register
is
specified
in
the
rd
field
Instr
so
RegDst
The
updating
of
the
PC
is
shown
with
the
dashed
gray
line
The
instruction
is
not
a
branch
so
Branch
and
hence
PCSrc
is
also
The
PC
gets
its
next
value
from
PCPlus
Note
that
data
certainly
does
flow
through
the
nonhighlighted
paths
but
that
the
value
of
that
data
is
unimportant
for
this
instruction
For
example
the
immediate
is
sign
extended
and
data
is
read
from
memory
but
these
values
do
not
influence
the
next
state
of
the
system
More
Instructions
We
have
considered
a
limited
subset
of
the
full
MIPS
instruction
set
Adding
support
for
the
addi
and
j
instructions
illustrates
the
principle
of
how
to
handle
new
instructions
and
also
gives
us
a
sufficiently
rich
instruction
set
to
write
many
interesting
programs
We
will
see
that
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
ALU
PC
Figure
Control
signals
and
data
flow
while
executing
or
instruction
supporting
some
instructions
simply
requires
enhancing
the
main
decoder
whereas
supporting
others
also
requires
more
hardware
in
the
datapath
Example
addi
INSTRUCTION
The
add
immediate
instruction
addi
adds
the
value
in
a
register
to
the
immediate
and
writes
the
result
to
another
register
The
datapath
already
is
capable
of
this
task
Determine
the
necessary
changes
to
the
controller
to
support
addi
Solution
All
we
need
to
do
is
add
a
new
row
to
the
main
decoder
truth
table
showing
the
control
signal
values
for
addi
as
given
in
Table
The
result
should
be
written
to
the
register
file
so
RegWrite
The
destination
register
is
specified
in
the
rt
field
of
the
instruction
so
RegDst
SrcB
comes
from
the
immediate
so
ALUSrc
The
instruction
is
not
a
branch
nor
does
it
write
memory
so
Branch
MemWrite
The
result
comes
from
the
ALU
not
memory
so
MemtoReg
Finally
the
ALU
should
add
so
ALUOp
CHAPTER
SEVEN
Microarchitecture
Table
Main
decoder
truth
table
enhanced
to
support
addi
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
R
type
lw
sw
X
X
beq
X
X
addi
Example
j
INSTRUCTION
The
jump
instruction
j
writes
a
new
value
into
the
PC
The
two
least
significant
bits
of
the
PC
are
always
because
the
PC
is
word
aligned
i
e
always
a
multiple
of
The
next
bits
are
taken
from
the
jump
address
field
in
Instr
The
upper
four
bits
are
taken
from
the
old
value
of
the
PC
The
existing
datapath
lacks
hardware
to
compute
PC
in
this
fashion
Determine
the
necessary
changes
to
both
the
datapath
and
controller
to
handle
j
Solution
First
we
must
add
hardware
to
compute
the
next
PC
value
PC
in
the
case
of
a
j
instruction
and
a
multiplexer
to
select
this
next
PC
as
shown
in
Figure
The
new
multiplexer
uses
the
new
Jump
control
signal
Ch
Now
we
must
add
a
row
to
the
main
decoder
truth
table
for
the
j
instruction
and
a
column
for
the
Jump
signal
as
shown
in
Table
The
Jump
control
signal
is
for
the
j
instruction
and
for
all
others
j
does
not
write
the
register
file
or
memory
so
RegWrite
MemWrite
Hence
we
don
t
care
about
the
computation
done
in
the
datapath
and
RegDst
ALUSrc
Branch
MemtoReg
ALUOp
X
Single
Cycle
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
PCJump
Jump
ALU
Figure
Single
cycle
MIPS
datapath
enhanced
to
support
the
j
instruction
Table
Main
decoder
truth
table
enhanced
to
support
j
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
Jump
R
type
lw
sw
X
X
beq
X
X
addi
j
X
X
X
X
XX
Performance
Analysis
Each
instruction
in
the
single
cycle
processor
takes
one
clock
cycle
so
the
CPI
is
The
critical
path
for
the
lw
instruction
is
shown
in
Figure
with
a
heavy
dashed
blue
line
It
starts
with
the
PC
loading
a
new
address
on
the
rising
edge
of
the
clock
The
instruction
memory
reads
the
next
instruction
The
register
file
reads
SrcA
While
the
register
file
is
reading
the
immediate
field
is
sign
extended
and
selected
at
the
ALUSrc
multiplexer
to
determine
SrcB
The
ALU
adds
SrcA
and
SrcB
to
find
the
effective
address
The
data
memory
reads
from
this
address
The
MemtoReg
multiplexer
selects
ReadData
Finally
Result
must
setup
at
the
register
file
before
the
next
rising
clock
edge
so
that
it
can
be
properly
written
Hence
the
cycle
time
is
In
most
implementation
technologies
the
ALU
memory
and
register
file
accesses
are
substantially
slower
than
other
operations
Therefore
the
cycle
time
simplifies
to
The
numerical
values
of
these
times
will
depend
on
the
specific
implementation
technology
Tc
tpcq
PC
tmem
tRFread
tmux
tALU
tRFsetup
tALU
tmem
tmux
tRFsetup
Tc
tpcq
PC
tmem
max
tRFread
tsext
tmux
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
WE
CLK
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl
A
RD
ALU
Sign
Extend
Figure
Critical
path
for
lw
instruction
Other
instructions
have
shorter
critical
paths
For
example
R
type
instructions
do
not
need
to
access
data
memory
However
we
are
disciplining
ourselves
to
synchronous
sequential
design
so
the
clock
period
is
constant
and
must
be
long
enough
to
accommodate
the
slowest
instruction
Example
SINGLE
CYCLE
PROCESSOR
PERFORMANCE
Ben
Bitdiddle
is
contemplating
building
the
single
cycle
MIPS
processor
in
a
nm
CMOS
manufacturing
process
He
has
determined
that
the
logic
elements
have
the
delays
given
in
Table
Help
him
compare
the
execution
time
for
a
program
with
billion
instructions
Solution
According
to
Equation
the
cycle
time
of
the
single
cycle
processor
is
Tc
ps
We
use
the
subscript
to
distinguish
it
from
subsequent
processor
designs
According
to
Equation
the
total
execution
time
is
T
instructions
cycle
instruction
s
cycle
seconds
Multicycle
Processor
Table
Delays
of
circuit
elements
Element
Parameter
Delay
ps
register
clk
to
Q
tpcq
register
setup
tsetup
multiplexer
tmux
ALU
tALU
memory
read
tmem
register
file
read
tRFread
register
file
setup
tRFsetup
MULTICYCLE
PROCESSOR
The
single
cycle
processor
has
three
primary
weaknesses
First
it
requires
a
clock
cycle
long
enough
to
support
the
slowest
instruction
lw
even
though
most
instructions
are
faster
Second
it
requires
three
adders
one
in
the
ALU
and
two
for
the
PC
logic
adders
are
relatively
expensive
circuits
especially
if
they
must
be
fast
And
third
it
has
separate
instruction
and
data
memories
which
may
not
be
realistic
Most
computers
have
a
single
large
memory
that
holds
both
instructions
and
data
and
that
can
be
read
and
written
Ch
The
multicycle
processor
addresses
these
weaknesses
by
breaking
an
instruction
into
multiple
shorter
steps
In
each
short
step
the
processor
can
read
or
write
the
memory
or
register
file
or
use
the
ALU
Different
instructions
use
different
numbers
of
steps
so
simpler
instructions
can
complete
faster
than
more
complex
ones
The
processor
needs
only
one
adder
this
adder
is
reused
for
different
purposes
on
various
steps
And
the
processor
uses
a
combined
memory
for
instructions
and
data
The
instruction
is
fetched
from
memory
on
the
first
step
and
data
may
be
read
or
written
on
later
steps
We
design
a
multicycle
processor
following
the
same
procedure
we
used
for
the
single
cycle
processor
First
we
construct
a
datapath
by
connecting
the
architectural
state
elements
and
memories
with
combinational
logic
But
this
time
we
also
add
nonarchitectural
state
elements
to
hold
intermediate
results
between
the
steps
Then
we
design
the
controller
The
controller
produces
different
signals
on
different
steps
during
execution
of
a
single
instruction
so
it
is
now
a
finite
state
machine
rather
than
combinational
logic
We
again
examine
how
to
add
new
instructions
to
the
processor
Finally
we
analyze
the
performance
of
the
multicycle
processor
and
compare
it
to
the
single
cycle
processor
Multicycle
Datapath
Again
we
begin
our
design
with
the
memory
and
architectural
state
of
the
MIPS
processor
shown
in
Figure
In
the
single
cycle
design
we
used
separate
instruction
and
data
memories
because
we
needed
to
read
the
instruction
memory
and
read
or
write
the
data
memory
all
in
one
cycle
Now
we
choose
to
use
a
combined
memory
for
both
instructions
and
data
This
is
more
realistic
and
it
is
feasible
because
we
can
read
the
instruction
in
one
cycle
then
read
or
write
the
data
in
a
separate
cycle
The
PC
and
register
file
remain
unchanged
We
gradually
build
the
datapath
by
adding
components
to
handle
each
step
of
each
instruction
The
new
connections
are
emphasized
in
black
or
blue
for
new
control
signals
whereas
the
hardware
that
has
already
been
studied
is
shown
in
gray
The
PC
contains
the
address
of
the
instruction
to
execute
The
first
step
is
to
read
this
instruction
from
instruction
memory
Figure
shows
that
the
PC
is
simply
connected
to
the
address
input
of
the
instruction
memory
The
instruction
is
read
and
stored
in
a
new
nonarchitectural
CHAPTER
SEVEN
Microarchitecture
CLK
A
RD
Instr
Data
Memory
PC
PC
WD
WE
CLK
EN
A
A
WD
RD
RD
WE
A
CLK
Register
File
Figure
State
elements
with
unified
instruction
data
memory
Instruction
Register
so
that
it
is
available
for
future
cycles
The
Instruction
Register
receives
an
enable
signal
called
IRWrite
that
is
asserted
when
it
should
be
updated
with
a
new
instruction
As
we
did
with
the
single
cycle
processor
we
will
work
out
the
datapath
connections
for
the
lw
instruction
Then
we
will
enhance
the
datapath
to
handle
the
other
instructions
For
a
lw
instruction
the
next
step
is
to
read
the
source
register
containing
the
base
address
This
register
is
specified
in
the
rs
field
of
the
instruction
Instr
These
bits
of
the
instruction
are
connected
to
one
of
the
address
inputs
A
of
the
register
file
as
shown
in
Figure
The
register
file
reads
the
register
onto
RD
This
value
is
stored
in
another
nonarchitectural
register
A
The
lw
instruction
also
requires
an
offset
The
offset
is
stored
in
the
immediate
field
of
the
instruction
Instr
and
must
be
signextended
to
bits
as
shown
in
Figure
The
bit
sign
extended
value
is
called
SignImm
To
be
consistent
we
might
store
SignImm
in
another
nonarchitectural
register
However
SignImm
is
a
combinational
function
of
Instr
and
will
not
change
while
the
current
instruction
is
being
processed
so
there
is
no
need
to
dedicate
a
register
to
hold
the
constant
value
The
address
of
the
load
is
the
sum
of
the
base
address
and
offset
We
use
an
ALU
to
compute
this
sum
as
shown
in
Figure
ALUControl
should
be
set
to
to
perform
an
addition
ALUResult
is
stored
in
a
nonarchitectural
register
called
ALUOut
The
next
step
is
to
load
the
data
from
the
calculated
address
in
the
memory
We
add
a
multiplexer
in
front
of
the
memory
to
choose
the
Multicycle
Processor
PC
Instr
CLK
EN
IRWrite
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
WD
WE
CLK
Figure
Fetch
instruction
from
memory
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Register
File
PC
PC
Instr
CLK
WD
WE
CLK
CLK
A
EN
IRWrite
Figure
Read
source
operand
from
register
file
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
CLK
WD
WE
CLK
CLK
A
EN
IRWrite
Figure
Sign
extend
the
immediate
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl
WD
WE
CLK
CLK
A
CLK
EN
IRWrite
ALU
Figure
Add
base
address
to
offset
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
IorD
IRWrite
ALU
Figure
Load
data
from
memory
memory
address
Adr
from
either
the
PC
or
ALUOut
as
shown
in
Figure
The
multiplexer
select
signal
is
called
IorD
to
indicate
either
an
instruction
or
data
address
The
data
read
from
the
memory
is
stored
in
another
nonarchitectural
register
called
Data
Notice
that
the
address
multiplexer
permits
us
to
reuse
the
memory
during
the
lw
instruction
On
the
first
step
the
address
is
taken
from
the
PC
to
fetch
the
instruction
On
a
later
step
the
address
is
taken
from
ALUOut
to
load
the
data
Hence
IorD
must
have
different
values
on
different
steps
In
Section
we
develop
the
FSM
controller
that
generates
these
sequences
of
control
signals
Finally
the
data
is
written
back
to
the
register
file
as
shown
in
Figure
The
destination
register
is
specified
by
the
rt
field
of
the
instruction
Instr
While
all
this
is
happening
the
processor
must
update
the
program
counter
by
adding
to
the
old
PC
In
the
single
cycle
processor
a
separate
adder
was
needed
In
the
multicycle
processor
we
can
use
the
existing
ALU
on
one
of
the
steps
when
it
is
not
busy
To
do
so
we
must
insert
source
multiplexers
to
choose
the
PC
and
the
constant
as
ALU
inputs
as
shown
in
Figure
A
two
input
multiplexer
controlled
by
ALUSrcA
chooses
either
the
PC
or
register
A
as
SrcA
A
four
input
multiplexer
controlled
by
ALUSrcB
chooses
either
or
SignImm
as
SrcB
We
use
the
other
two
multiplexer
inputs
later
when
we
extend
the
datapath
to
handle
other
instructions
The
numbering
of
inputs
to
the
multiplexer
is
arbitrary
To
update
the
PC
the
ALU
adds
SrcA
PC
to
SrcB
and
the
result
is
written
into
the
program
counter
register
The
PCWrite
control
signal
enables
the
PC
register
to
be
written
only
on
certain
cycles
Multicycle
Processor
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
RegWrite
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
IorD
IRWrite
ALU
Figure
Write
data
back
to
register
file
This
completes
the
datapath
for
the
lw
instruction
Next
let
us
extend
the
datapath
to
also
handle
the
sw
instruction
Like
the
lw
instruction
the
sw
instruction
reads
a
base
address
from
port
of
the
register
file
and
sign
extends
the
immediate
The
ALU
adds
the
base
address
to
the
immediate
to
find
the
memory
address
All
of
these
functions
are
already
supported
by
existing
hardware
in
the
datapath
The
only
new
feature
of
sw
is
that
we
must
read
a
second
register
from
the
register
file
and
write
it
into
the
memory
as
shown
in
Figure
The
register
is
specified
in
the
rt
field
of
the
instruction
Instr
which
is
connected
to
the
second
port
of
the
register
file
When
the
register
is
read
it
is
stored
in
a
nonarchitectural
register
B
On
the
next
step
it
is
sent
to
the
write
data
port
WD
of
the
data
memory
to
be
written
The
memory
receives
an
additional
MemWrite
control
signal
to
indicate
that
the
write
should
occur
CHAPTER
SEVEN
Microarchitecture
PCWrite
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
RegWrite
ALUSrcA
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
EN
ALUSrcB
IorD
IRWrite
ALU
Figure
Increment
PC
by
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegWrite
ALUSrcA
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
CLK
EN
EN
PCWrite
IorD
IRWrite
ALUSrcB
B
ALU
Figure
Enhanced
datapath
for
sw
instruction
Multicycle
Processor
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
PCWrite
IorD
IRWrite
ALUSrcB
ALU
Figure
Enhanced
datapath
for
R
type
instructions
For
R
type
instructions
the
instruction
is
again
fetched
and
the
two
source
registers
are
read
from
the
register
file
Another
input
of
the
SrcB
multiplexer
is
used
to
choose
register
B
as
the
second
source
register
for
the
ALU
as
shown
in
Figure
The
ALU
performs
the
appropriate
operation
and
stores
the
result
in
ALUOut
On
the
next
step
ALUOut
is
written
back
to
the
register
specified
by
the
rd
field
of
the
instruction
Instr
This
requires
two
new
multiplexers
The
MemtoReg
multiplexer
selects
whether
WD
comes
from
ALUOut
for
R
type
instructions
or
from
Data
for
lw
The
RegDst
instruction
selects
whether
the
destination
register
is
specified
in
the
rt
or
rd
field
of
the
instruction
For
the
beq
instruction
the
instruction
is
again
fetched
and
the
two
source
registers
are
read
from
the
register
file
To
determine
whether
the
registers
are
equal
the
ALU
subtracts
the
registers
and
examines
the
Zero
flag
Meanwhile
the
datapath
must
compute
the
next
value
of
the
PC
if
the
branch
is
taken
PC
PC
SignImm
In
the
single
cycle
processor
yet
another
adder
was
needed
to
compute
the
branch
address
In
the
multicycle
processor
the
ALU
can
be
reused
again
to
save
hardware
On
one
step
the
ALU
computes
PC
and
writes
it
back
to
the
program
counter
as
was
done
for
other
instructions
On
another
step
the
ALU
uses
this
updated
PC
value
to
compute
PC
SignImm
SignImm
is
left
shifted
by
to
multiply
it
by
as
shown
in
Figure
The
SrcB
multiplexer
chooses
this
value
and
adds
it
to
the
PC
This
sum
represents
the
destination
of
the
branch
and
is
stored
in
ALUOut
A
new
multiplexer
controlled
by
PCSrc
chooses
what
signal
should
be
sent
to
PC
The
program
counter
should
be
written
either
when
PCWrite
is
asserted
or
when
a
branch
is
taken
A
new
control
signal
Branch
indicates
that
the
beq
instruction
is
being
executed
The
branch
is
taken
if
Zero
is
also
asserted
Hence
the
datapath
computes
a
new
PC
write
Chap
enable
called
PCEn
which
is
TRUE
either
when
PCWrite
is
asserted
or
when
both
Branch
and
Zero
are
asserted
This
completes
the
design
of
the
multicycle
MIPS
processor
datapath
The
design
process
is
much
like
that
of
the
single
cycle
processor
in
that
hardware
is
systematically
connected
between
the
state
elements
to
handle
each
instruction
The
main
difference
is
that
the
instruction
is
executed
in
several
steps
Nonarchitectural
registers
are
inserted
to
hold
the
results
of
each
step
In
this
way
the
ALU
can
be
reused
several
times
saving
the
cost
of
extra
adders
Similarly
the
instructions
and
data
can
be
stored
in
one
shared
memory
In
the
next
section
we
develop
an
FSM
controller
to
deliver
the
appropriate
sequence
of
control
signals
to
the
datapath
on
each
step
of
each
instruction
Multicycle
Control
As
in
the
single
cycle
processor
the
control
unit
computes
the
control
signals
based
on
the
opcode
and
funct
fields
of
the
instruction
Instr
and
Instr
Figure
shows
the
entire
multicycle
MIPS
processor
with
the
control
unit
attached
to
the
datapath
The
datapath
is
shown
in
black
and
the
control
unit
is
shown
in
blue
As
in
the
single
cycle
processor
the
control
unit
is
partitioned
into
a
main
controller
and
an
ALU
decoder
as
shown
in
Figure
The
ALU
decoder
is
unchanged
and
follows
the
truth
table
of
Table
Now
however
the
main
controller
is
an
FSM
that
applies
the
proper
control
signals
on
the
proper
cycles
or
steps
The
sequence
of
control
signals
depends
on
the
instruction
being
executed
In
the
remainder
of
this
section
we
will
develop
the
FSM
state
transition
diagram
for
the
main
controller
CHAPTER
SEVEN
Microarchitecture
SignImm
b
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
IorD
IRWrite
ALUSrcB
PCWrite
PCEn
ALU
Figure
Enhanced
datapath
for
beq
instruction
The
main
controller
produces
multiplexer
select
and
register
enable
signals
for
the
datapath
The
select
signals
are
MemtoReg
RegDst
IorD
PCSrc
ALUSrcB
and
ALUSrcA
The
enable
signals
are
IRWrite
MemWrite
PCWrite
Branch
and
RegWrite
To
keep
the
following
state
transition
diagrams
readable
only
the
relevant
control
signals
are
listed
Select
signals
are
listed
only
when
their
value
matters
otherwise
they
are
don
t
cares
Enable
signals
are
listed
only
when
they
are
asserted
otherwise
they
are
Multicycle
Processor
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
tsDgeR
Branch
MemWrite
geRotmeM
ALUSrcA
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
ULA
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
IRWrite
ALUSrcB
IorD
PCWrite
PCEn
Figure
Complete
multicycle
MIPS
processor
ALUSrcA
PCSrc
Branch
ALUSrcB
Opcode
Control
Unit
ALUControl
Funct
Main
Controller
FSM
ALUOp
ALU
Decoder
RegWrite
PCWrite
IorD
MemWrite
IRWrite
RegDst
MemtoReg
Register
Enables
Multiplexer
Selects
Figure
Control
unit
internal
structure
The
first
step
for
any
instruction
is
to
fetch
the
instruction
from
memory
at
the
address
held
in
the
PC
The
FSM
enters
this
state
on
reset
To
read
memory
IorD
so
the
address
is
taken
from
the
PC
IRWrite
is
asserted
to
write
the
instruction
into
the
instruction
register
IR
Meanwhile
the
PC
should
be
incremented
by
to
point
to
the
next
instruction
Because
the
ALU
is
not
being
used
for
anything
else
the
processor
can
use
it
to
compute
PC
at
the
same
time
that
it
fetches
the
instruction
ALUSrcA
so
SrcA
comes
from
the
PC
ALUSrcB
so
SrcB
is
the
constant
ALUOp
so
the
ALU
decoder
produces
ALUControl
to
make
the
ALU
add
To
update
the
PC
with
this
new
value
PCSrc
and
PCWrite
is
asserted
These
control
signals
are
shown
in
Figure
The
data
flow
on
this
step
is
shown
in
Figure
with
the
instruction
fetch
shown
using
the
dashed
blue
line
and
the
PC
increment
shown
using
the
dashed
gray
line
The
next
step
is
to
read
the
register
file
and
decode
the
instruction
The
register
file
always
reads
the
two
sources
specified
by
the
rs
and
rt
fields
of
the
instruction
Meanwhile
the
immediate
is
sign
extended
Decoding
involves
examining
the
opcode
of
the
instruction
to
determine
what
to
do
next
No
control
signals
are
necessary
to
decode
the
instruction
but
the
FSM
must
wait
cycle
for
the
reading
and
decoding
to
complete
as
shown
in
Figure
The
new
state
is
highlighted
in
blue
The
data
flow
is
shown
in
Figure
CHAPTER
SEVEN
Microarchitecture
Reset
S
Fetch
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
Figure
Fetch
ALU
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
RegDst
MemtoReg
SrcB
ALUResult
SrcA
ALUOut
Branch
MemWrite
ALUSrcA
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IRWrite
IorD
PCWrite
PCEn
X
X
Figure
Data
flow
during
the
fetch
step
Now
the
FSM
proceeds
to
one
of
several
possible
states
depending
on
the
opcode
If
the
instruction
is
a
memory
load
or
store
lw
or
sw
the
multicycle
processor
computes
the
address
by
adding
the
base
address
to
the
sign
extended
immediate
This
requires
ALUSrcA
to
select
register
A
and
ALUSrcB
to
select
SignImm
ALUOp
so
the
ALU
adds
The
effective
address
is
stored
in
the
ALUOut
register
for
use
on
the
next
step
This
FSM
step
is
shown
in
Figure
and
the
data
flow
is
shown
in
Figure
If
the
instruction
is
lw
the
multicycle
processor
must
next
read
data
from
memory
and
write
it
to
the
register
file
These
two
steps
are
shown
in
Figure
To
read
from
memory
IorD
to
select
the
memory
address
that
was
just
computed
and
saved
in
ALUOut
This
address
in
memory
is
read
and
saved
in
the
Data
register
during
step
S
On
the
next
step
S
Data
is
written
to
the
register
file
MemtoReg
to
select
Multicycle
Processor
Reset
S
Fetch
S
Decode
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
Figure
Decode
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IRWrite
IorD
PCWrite
PCEn
X
X
X
X
XX
XXX
X
ALU
Figure
Data
flow
during
the
decode
step
Data
and
RegDst
to
pull
the
destination
register
from
the
rt
field
of
the
instruction
RegWrite
is
asserted
to
perform
the
write
completing
the
lw
instruction
Finally
the
FSM
returns
to
the
initial
state
S
to
fetch
the
next
instruction
For
these
and
subsequent
steps
try
to
visualize
the
data
flow
on
your
own
From
state
S
if
the
instruction
is
sw
the
data
read
from
the
second
port
of
the
register
file
is
simply
written
to
memory
IorD
to
select
the
address
computed
in
S
and
saved
in
ALUOut
MemWrite
is
asserted
to
write
the
memory
Again
the
FSM
returns
to
S
to
fetch
the
next
instruction
The
added
step
is
shown
in
Figure
CHAPTER
SEVEN
Microarchitecture
ALUSrcA
Reset
S
Fetch
S
MemAdr
S
Decode
Op
LW
or
Op
SW
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
ALUOp
ALUSrcB
Figure
Memory
address
computation
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IRWrite
IorD
PCWrite
PCEn
X
X
X
X
ALU
Figure
Data
flow
during
memory
address
computation
If
the
opcode
indicates
an
R
type
instruction
the
multicycle
processor
must
calculate
the
result
using
the
ALU
and
write
that
result
to
the
register
file
Figure
shows
these
two
steps
In
S
the
instruction
is
executed
by
selecting
the
A
and
B
registers
ALUSrcA
ALUSrcB
and
performing
the
ALU
operation
indicated
by
the
funct
field
of
the
instruction
ALUOp
for
all
R
type
instructions
The
ALUResult
is
stored
in
ALUOut
In
S
ALUOut
is
written
to
the
register
file
RegDst
because
the
destination
register
is
specified
in
the
rd
field
of
the
instruction
MemtoReg
because
the
write
data
WD
comes
from
ALUOut
RegWrite
is
asserted
to
write
the
register
file
For
a
beq
instruction
the
processor
must
calculate
the
destination
address
and
compare
the
two
source
registers
to
determine
whether
the
branch
should
be
taken
This
requires
two
uses
of
the
ALU
and
hence
might
seem
to
demand
two
new
states
Notice
however
that
the
ALU
was
not
used
during
S
when
the
registers
were
being
read
The
processor
might
as
well
use
the
ALU
at
that
time
to
compute
the
destination
address
by
adding
the
incremented
PC
PC
to
SignImm
as
shown
in
Multicycle
Processor
IorD
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
Op
LW
or
Op
SW
Op
LW
S
Mem
Writeback
PCWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
ALUSrcA
ALUOp
ALUSrcB
RegDst
MemtoReg
RegWrite
Figure
Memory
read
C
Figure
see
page
ALUSrcA
to
select
the
incremented
PC
ALUSrcB
to
select
SignImm
and
ALUOp
to
add
The
destination
address
is
stored
in
ALUOut
If
the
instruction
is
not
beq
the
computed
address
will
not
be
used
in
subsequent
cycles
but
its
computation
was
harmless
In
S
the
processor
compares
the
two
registers
by
subtracting
them
and
checking
to
determine
whether
the
result
is
If
it
is
the
processor
branches
to
the
address
that
was
just
computed
ALUSrcA
to
select
register
A
ALUSrcB
to
select
register
B
ALUOp
to
subtract
PCSrc
to
take
the
destination
address
from
ALUOut
and
Branch
to
update
the
PC
with
this
address
if
the
ALU
result
is
Putting
these
steps
together
Figure
shows
the
complete
main
controller
state
transition
diagram
for
the
multicycle
processor
see
page
Converting
it
to
hardware
is
a
straightforward
but
tedious
task
using
the
techniques
of
Chapter
Better
yet
the
FSM
can
be
coded
in
an
HDL
and
synthesized
using
the
techniques
of
Chapter
CHAPTER
SEVEN
Microarchitecture
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
Op
LW
or
Op
SW
Op
LW
Op
SW
S
Mem
Writeback
IRWrite
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
PCWrite
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
Figure
Memory
write
Now
we
see
why
the
PCSrc
multiplexer
is
necessary
to
choose
PC
from
either
ALUResult
in
S
or
ALUOut
in
S
Ch
More
Instructions
As
we
did
in
Section
for
the
single
cycle
processor
let
us
now
extend
the
multicycle
processor
to
support
the
addi
and
j
instructions
The
next
two
examples
illustrate
the
general
design
process
to
support
new
instructions
Example
addi
INSTRUCTION
Modify
the
multicycle
processor
to
support
addi
Solution
The
datapath
is
already
capable
of
adding
registers
to
immediates
so
all
we
need
to
do
is
add
new
states
to
the
main
controller
FSM
for
addi
as
shown
in
Figure
see
page
The
states
are
similar
to
those
for
R
type
instructions
In
S
register
A
is
added
to
SignImm
ALUSrcA
ALUSrcB
ALUOp
and
the
result
ALUResult
is
stored
in
ALUOut
In
S
ALUOut
is
written
Multicycle
Processor
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
Op
LW
or
Op
SW
Op
LW
S
Mem
Writeback
S
MemWrite
S
Execute
S
ALU
Writeback
Op
R
type
Op
SW
PCWrite
IRWrite
IorD
PCSrc
ALUOp
ALUSrcB
AluSrcA
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
ALUSrcA
ALUOp
ALUSrcB
Figure
Execute
R
type
operation
to
the
register
specified
by
the
rt
field
of
the
instruction
RegDst
MemtoReg
RegWrite
asserted
The
astute
reader
may
notice
that
S
and
S
are
identical
and
could
be
merged
into
a
single
state
Example
j
INSTRUCTION
Modify
the
multicycle
processor
to
support
j
Solution
First
we
must
modify
the
datapath
to
compute
the
next
PC
value
in
the
case
of
a
j
instruction
Then
we
add
a
state
to
the
main
controller
to
handle
the
instruction
Figure
shows
the
enhanced
datapath
see
page
The
jump
destination
address
is
formed
by
left
shifting
the
bit
addr
field
of
the
instruction
by
two
bits
then
prepending
the
four
most
significant
bits
of
the
already
incremented
PC
The
PCSrc
multiplexer
is
extended
to
take
this
address
as
a
third
input
CHAPTER
SEVEN
Microarchitecture
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
PCWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
IorD
RegDst
MemtoReg
RegWrite
IorD
MemWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
PCSrc
Branch
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
RegDst
MemtoReg
RegWrite
S
Mem
Writeback
Figure
Branch
Figure
shows
the
enhanced
main
controller
see
page
The
new
state
S
simply
selects
PC
as
the
PCJump
value
PCSrc
and
writes
the
PC
Note
that
the
PCSrc
select
signal
is
extended
to
two
bits
in
S
and
S
as
well
Performance
Analysis
The
execution
time
of
an
instruction
depends
on
both
the
number
of
cycles
it
uses
and
the
cycle
time
Whereas
the
single
cycle
processor
performed
all
instructions
in
one
cycle
the
multicycle
processor
uses
varying
numbers
of
cycles
for
the
various
instructions
However
the
multicycle
processor
does
less
work
in
a
single
cycle
and
thus
has
a
shorter
cycle
time
The
multicycle
processor
requires
three
cycles
for
beq
and
j
instructions
four
cycles
for
sw
addi
and
R
type
instructions
and
five
cycles
for
lw
instructions
The
CPI
depends
on
the
relative
likelihood
that
each
instruction
is
used
Multicycle
Processor
IorD
ALUSrcA
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
S
Mem
Writeback
PCWrite
IRWrite
PCSrc
ALUOp
ALUSrcB
AluSrcA
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
ALUOp
ALUSrcB
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
Branch
PCSrc
ALUOp
ALUSrcB
Figure
Complete
multicycle
control
FSM
C
Example
MULTICYCLE
PROCESSOR
CPI
The
SPECINT
benchmark
consists
of
approximately
loads
stores
branches
jumps
and
R
type
instructions
Determine
the
average
CPI
for
this
benchmark
Solution
The
average
CPI
is
the
sum
over
each
instruction
of
the
CPI
for
that
instruction
multiplied
by
the
fraction
of
the
time
that
instruction
is
used
For
this
benchmark
Average
CPI
This
is
better
than
the
worst
case
CPI
of
which
would
be
required
if
all
instructions
took
the
same
time
CHAPTER
SEVEN
Microarchitecture
IorD
IorD
MemWrite
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
S
Mem
Writeback
Op
ADDI
S
ADDI
Execute
S
ADDI
Writeback
IorD
PCWrite
IRWrite
PCSrc
ALUOp
ALUSrcB
AluSrcA
ALUSrcA
ALUOp
ALUSrcB
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
RegDst
RegWrite
MemtoReg
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
ALUOp
ALUSrcB
ALUSrcA
Branch
PCSrc
ALUOp
ALUSrcB
Figure
Main
controller
states
for
addi
Data
from
Patterson
and
Hennessy
Computer
Organization
and
Design
rd
Edition
Morgan
Kaufmann
Recall
that
we
designed
the
multicycle
processor
so
that
each
cycle
involved
one
ALU
operation
memory
access
or
register
file
access
Let
us
assume
that
the
register
file
is
faster
than
the
memory
and
that
writing
memory
is
faster
than
reading
memory
Examining
the
datapath
reveals
two
possible
critical
paths
that
would
limit
the
cycle
time
The
numerical
values
of
these
times
will
depend
on
the
specific
implementation
technology
Example
PROCESSOR
PERFORMANCE
COMPARISON
Ben
Bitdiddle
is
wondering
whether
he
would
be
better
off
building
the
multicycle
processor
instead
of
the
single
cycle
processor
For
both
designs
he
plans
on
using
a
nm
CMOS
manufacturing
process
with
the
delays
given
in
Table
Help
him
compare
each
processor
s
execution
time
for
billion
instructions
from
the
SPECINT
benchmark
see
Example
Solution
According
to
Equation
the
cycle
time
of
the
multicycle
processor
is
Tc
ps
Using
the
CPI
of
from
Example
the
total
execution
time
is
T
instructions
cycles
instruction
s
cycle
seconds
According
to
Example
the
singlecycle
processor
had
a
cycle
time
of
Tc
ps
a
CPI
of
and
a
total
execution
time
of
seconds
One
of
the
original
motivations
for
building
a
multicycle
processor
was
to
avoid
making
all
instructions
take
as
long
as
the
slowest
one
Unfortunately
this
example
shows
that
the
multicycle
processor
is
slower
than
the
single
cycle
Tc
tpcq
tmux
max
tALU
tmux
tmem
tsetup
Multicycle
Processor
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ULA
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
IorD
IRWrite
ALUSrcB
PCWrite
PCEn
addr
PCJump
Figure
Multicycle
MIPS
datapath
enhanced
to
support
the
j
instruction
Ch
processor
given
the
assumptions
of
CPI
and
circuit
element
delays
The
fundamental
problem
is
that
even
though
the
slowest
instruction
lw
was
broken
into
five
steps
the
multicycle
processor
cycle
time
was
not
nearly
improved
fivefold
This
is
partly
because
not
all
of
the
steps
are
exactly
the
same
length
and
partly
because
the
ps
sequencing
overhead
of
the
register
clk
to
Q
and
setup
time
must
now
be
paid
on
every
step
not
just
once
for
the
entire
instruction
In
general
engineers
have
learned
that
it
is
difficult
to
exploit
the
fact
that
some
computations
are
faster
than
others
unless
the
differences
are
large
Compared
with
the
single
cycle
processor
the
multicycle
processor
is
likely
to
be
less
expensive
because
it
eliminates
two
adders
and
combines
the
instruction
and
data
memories
into
a
single
unit
It
does
however
require
five
nonarchitectural
registers
and
additional
multiplexers
CHAPTER
SEVEN
Microarchitecture
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
PCWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
IorD
RegDst
MemtoReg
RegWrite
IorD
MemWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
PCSrc
Branch
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
RegDst
MemtoReg
RegWrite
S
Mem
Writeback
ALUSrcA
ALUSrcB
ALUOp
RegDst
MemtoReg
RegWrite
Op
ADDI
S
ADDI
Execute
S
ADDI
Writeback
PCSrc
PCWrite
Op
J
S
Jump
Figure
Main
controller
state
for
j
PIPELINED
PROCESSOR
Pipelining
introduced
in
Section
is
a
powerful
way
to
improve
the
throughput
of
a
digital
system
We
design
a
pipelined
processor
by
subdividing
the
single
cycle
processor
into
five
pipeline
stages
Thus
five
instructions
can
execute
simultaneously
one
in
each
stage
Because
each
stage
has
only
one
fifth
of
the
entire
logic
the
clock
frequency
is
almost
five
times
faster
Hence
the
latency
of
each
instruction
is
ideally
unchanged
but
the
throughput
is
ideally
five
times
better
Microprocessors
execute
millions
or
billions
of
instructions
per
second
so
throughput
is
more
important
than
latency
Pipelining
introduces
some
overhead
so
the
throughput
will
not
be
quite
as
high
as
we
might
ideally
desire
but
pipelining
nevertheless
gives
such
great
advantage
for
so
little
cost
that
all
modern
high
performance
microprocessors
are
pipelined
Reading
and
writing
the
memory
and
register
file
and
using
the
ALU
typically
constitute
the
biggest
delays
in
the
processor
We
choose
five
pipeline
stages
so
that
each
stage
involves
exactly
one
of
these
slow
steps
Specifically
we
call
the
five
stages
Fetch
Decode
Execute
Memory
and
Writeback
They
are
similar
to
the
five
steps
that
the
multicycle
processor
used
to
perform
lw
In
the
Fetch
stage
the
processor
reads
the
instruction
from
instruction
memory
In
the
Decode
stage
the
processor
reads
the
source
operands
from
the
register
file
and
decodes
the
instruction
to
produce
the
control
signals
In
the
Execute
stage
the
processor
performs
a
computation
with
the
ALU
In
the
Memory
stage
the
processor
reads
or
writes
data
memory
Finally
in
the
Writeback
stage
the
processor
writes
the
result
to
the
register
file
when
applicable
Figure
shows
a
timing
diagram
comparing
the
single
cycle
and
pipelined
processors
Time
is
on
the
horizontal
axis
and
instructions
are
on
the
vertical
axis
The
diagram
assumes
the
logic
element
delays
from
Table
but
ignores
the
delays
of
multiplexers
and
registers
In
the
single
cycle
processor
Figure
a
the
first
instruction
is
read
from
memory
at
time
next
the
operands
are
read
from
the
register
file
and
then
the
ALU
executes
the
necessary
computation
Finally
the
data
memory
may
be
accessed
and
the
result
is
written
back
to
the
register
file
by
ps
The
second
instruction
begins
when
the
first
completes
Hence
in
this
diagram
the
single
cycle
processor
has
an
instruction
latency
of
ps
and
a
throughput
of
instruction
per
ps
billion
instructions
per
second
In
the
pipelined
processor
Figure
b
the
length
of
a
pipeline
stage
is
set
at
ps
by
the
slowest
stage
the
memory
access
in
the
Fetch
or
Memory
stage
At
time
the
first
instruction
is
fetched
from
memory
At
ps
the
first
instruction
enters
the
Decode
stage
and
Pipelined
Processor
Time
ps
Instr
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
a
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Instr
b
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Fetch
Instruction
Decode
Read
Reg
Execute
ALU
Memory
Read
Write
Write
Reg
Fetch
Instruction
Figure
Timing
diagrams
a
single
cycle
processor
b
pipelined
processor
a
second
instruction
is
fetched
At
ps
the
first
instruction
executes
the
second
instruction
enters
the
Decode
stage
and
a
third
instruction
is
fetched
And
so
forth
until
all
the
instructions
complete
The
instruction
latency
is
ps
The
throughput
is
instruction
per
ps
billion
instructions
per
second
Because
the
stages
are
not
perfectly
balanced
with
equal
amounts
of
logic
the
latency
is
slightly
longer
for
the
pipelined
than
for
the
single
cycle
processor
Similarly
the
throughput
is
not
quite
five
times
as
great
for
a
five
stage
pipeline
as
for
the
single
cycle
processor
Nevertheless
the
throughput
advantage
is
substantial
Figure
shows
an
abstracted
view
of
the
pipeline
in
operation
in
which
each
stage
is
represented
pictorially
Each
pipeline
stage
is
represented
with
its
major
component
instruction
memory
IM
register
file
RF
read
ALU
execution
data
memory
DM
and
register
file
writeback
to
illustrate
the
flow
of
instructions
through
the
pipeline
Reading
across
a
row
shows
the
clock
cycles
in
which
a
particular
instruction
is
in
each
stage
For
example
the
sub
instruction
is
fetched
in
cycle
and
executed
in
cycle
Reading
down
a
column
shows
what
the
various
pipeline
stages
are
doing
on
a
particular
cycle
For
example
in
cycle
the
or
instruction
is
being
fetched
from
instruction
memory
while
s
is
being
read
from
the
register
file
the
ALU
is
computing
t
AND
t
the
data
memory
is
idle
and
the
register
file
is
writing
a
sum
to
s
Stages
are
shaded
to
indicate
when
they
are
used
For
example
the
data
memory
is
used
by
lw
in
cycle
and
by
sw
in
cycle
The
instruction
memory
and
ALU
are
used
in
every
cycle
The
register
file
is
written
by
Pipelined
Processor
Time
cycles
lw
s
RF
RF
s
DM
RF
t
t
RF
s
DM
RF
s
s
RF
s
DM
RF
t
t
RF
s
DM
RF
s
RF
s
DM
RF
t
t
RF
s
DM
add
s
t
t
sub
s
s
s
and
s
t
t
sw
s
s
or
s
t
t
add
IM
IM
IM
IM
IM
IM
lw
sub
and
sw
or
Figure
Abstract
view
of
pipeline
in
operation
C
every
instruction
except
sw
We
assume
that
in
the
pipelined
processor
the
register
file
is
written
in
the
first
part
of
a
cycle
and
read
in
the
second
part
as
suggested
by
the
shading
This
way
data
can
be
written
and
read
back
within
a
single
cycle
A
central
challenge
in
pipelined
systems
is
handling
hazards
that
occur
when
the
results
of
one
instruction
are
needed
by
a
subsequent
instruction
before
the
former
instruction
has
completed
For
example
if
the
add
in
Figure
used
s
rather
than
t
a
hazard
would
occur
because
the
s
register
has
not
been
written
by
the
lw
by
the
time
it
is
read
by
the
add
This
section
explores
forwarding
stalls
and
flushes
as
methods
to
resolve
hazards
Finally
this
section
revisits
performance
analysis
considering
sequencing
overhead
and
the
impact
of
hazards
Pipelined
Datapath
The
pipelined
datapath
is
formed
by
chopping
the
single
cycle
datapath
into
five
stages
separated
by
pipeline
registers
Figure
a
shows
the
single
cycle
datapath
stretched
out
to
leave
room
for
the
pipeline
registers
Figure
b
shows
the
pipelined
datapath
formed
by
inserting
four
pipeline
registers
to
separate
the
datapath
into
five
stages
The
stages
and
their
boundaries
are
indicated
in
blue
Signals
are
given
a
suffix
F
D
E
M
or
W
to
indicate
the
stage
in
which
they
reside
The
register
file
is
peculiar
because
it
is
read
in
the
Decode
stage
and
written
in
the
Writeback
stage
It
is
drawn
in
the
Decode
stage
but
the
write
address
and
data
come
from
the
Writeback
stage
This
feedback
will
lead
to
pipeline
hazards
which
are
discussed
in
Section
One
of
the
subtle
but
critical
issues
in
pipelining
is
that
all
signals
associated
with
a
particular
instruction
must
advance
through
the
pipeline
in
unison
Figure
b
has
an
error
related
to
this
issue
Can
you
find
it
The
error
is
in
the
register
file
write
logic
which
should
operate
in
the
Writeback
stage
The
data
value
comes
from
ResultW
a
Writeback
stage
signal
But
the
address
comes
from
WriteRegE
an
Execute
stage
signal
In
the
pipeline
diagram
of
Figure
during
cycle
the
result
of
the
lw
instruction
would
be
incorrectly
written
to
register
s
rather
than
s
Figure
shows
a
corrected
datapath
The
WriteReg
signal
is
now
pipelined
along
through
the
Memory
and
Writeback
stages
so
it
remains
in
sync
with
the
rest
of
the
instruction
WriteRegW
and
ResultW
are
fed
back
together
to
the
register
file
in
the
Writeback
stage
The
astute
reader
may
notice
that
the
PC
logic
is
also
problematic
because
it
might
be
updated
with
a
Fetch
or
a
Memory
stage
signal
PCPlus
F
or
PCBranchM
This
control
hazard
will
be
fixed
in
Section
CHAPTER
SEVEN
Microarchitecture
C
Pipelined
Control
The
pipelined
processor
takes
the
same
control
signals
as
the
single
cycle
processor
and
therefore
uses
the
same
control
unit
The
control
unit
examines
the
opcode
and
funct
fields
of
the
instruction
in
the
Decode
stage
to
produce
the
control
signals
as
was
described
in
Section
These
control
signals
must
be
pipelined
along
with
the
data
so
that
they
remain
synchronized
with
the
instruction
The
entire
pipelined
processor
with
control
is
shown
in
Figure
RegWrite
must
be
pipelined
into
the
Writeback
stage
before
it
feeds
back
to
the
register
file
just
as
WriteReg
was
pipelined
in
Figure
Pipelined
Processor
SignImm
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PC
PC
Instr
SrcB
ALUResult
ALU
ReadData
WriteData
SrcA
PCPlus
PCBranch
WriteReg
Result
Zero
CLK
a
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RtE
RdE
ALUOutM
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchM
ResultW
PCPlus
F
PCPlus
E
ZeroM
CLK
CLK
WriteRegE
CLK
CLK
CLK
ALU
b
Fetch
Decode
Execute
Memory
Writeback
Figure
Single
cycle
and
pipelined
datapaths
Hazards
In
a
pipelined
system
multiple
instructions
are
handled
concurrently
When
one
instruction
is
dependent
on
the
results
of
another
that
has
not
yet
completed
a
hazard
occurs
CHAPTER
SEVEN
Microarchitecture
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RtE
RdE
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchM
WriteRegM
ResultW
PCPlus
F
PCPlus
E
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
ZeroM
PCSrcM
CLK
CLK
CLK
CLK
CLK
WriteRegW
ALUControlE
RegWriteE
RegWriteM
RegWriteW
MemtoRegE
MemtoRegM
MemtoRegW
MemWriteE
MemWriteM
BranchE
BranchM
RegDstE
ALUSrcE
WriteRegE
Figure
Pipelined
processor
with
control
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RtE
RdE
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchM
WriteRegM
ResultW
PCPlus
F
PCPlus
E
ZeroM
CLK
CLK
WriteRegE
WriteRegW
CLK
CLK
CLK
Fetch
Decode
Execute
Memory
Writeback
Figure
Corrected
pipelined
datapath
The
register
file
can
be
read
and
written
in
the
same
cycle
Let
us
assume
that
the
write
takes
place
during
the
first
half
of
the
cycle
and
the
read
takes
place
during
the
second
half
of
the
cycle
so
that
a
register
can
be
written
and
read
back
in
the
same
cycle
without
introducing
a
hazard
Figure
illustrates
hazards
that
occur
when
one
instruction
writes
a
register
s
and
subsequent
instructions
read
this
register
This
is
called
a
read
after
write
RAW
hazard
The
add
instruction
writes
a
result
into
s
in
the
first
half
of
cycle
However
the
and
instruction
reads
s
on
cycle
obtaining
the
wrong
value
The
or
instruction
reads
s
on
cycle
again
obtaining
the
wrong
value
The
sub
instruction
reads
s
in
the
second
half
of
cycle
obtaining
the
correct
value
which
was
written
in
the
first
half
of
cycle
Subsequent
instructions
also
read
the
correct
value
of
s
The
diagram
shows
that
hazards
may
occur
in
this
pipeline
when
an
instruction
writes
a
register
and
either
of
the
two
subsequent
instructions
read
that
register
Without
special
treatment
the
pipeline
will
compute
the
wrong
result
On
closer
inspection
however
observe
that
the
sum
from
the
add
instruction
is
computed
by
the
ALU
in
cycle
and
is
not
strictly
needed
by
the
and
instruction
until
the
ALU
uses
it
in
cycle
In
principle
we
should
be
able
to
forward
the
result
from
one
instruction
to
the
next
to
resolve
the
RAW
hazard
without
slowing
down
the
pipeline
In
other
situations
explored
later
in
this
section
we
may
have
to
stall
the
pipeline
to
give
time
for
a
result
to
be
computed
before
the
subsequent
instruction
uses
the
result
In
any
event
something
must
be
done
to
solve
hazards
so
that
the
program
executes
correctly
despite
the
pipelining
Hazards
are
classified
as
data
hazards
or
control
hazards
A
data
hazard
occurs
when
an
instruction
tries
to
read
a
register
that
has
not
yet
been
written
back
by
a
previous
instruction
A
control
hazard
occurs
when
the
decision
of
what
instruction
to
fetch
next
has
not
been
made
by
the
time
the
fetch
takes
place
In
the
remainder
of
this
section
we
will
Pipelined
Processor
Time
cycles
add
s
s
s
RF
s
s
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
add
or
sub
Figure
Abstract
pipeline
diagram
illustrating
hazards
enhance
the
pipelined
processor
with
a
hazard
unit
that
detects
hazards
and
handles
them
appropriately
so
that
the
processor
executes
the
program
correctly
Solving
Data
Hazards
with
Forwarding
Some
data
hazards
can
be
solved
by
forwarding
also
called
bypassing
a
result
from
the
Memory
or
Writeback
stage
to
a
dependent
instruction
in
the
Execute
stage
This
requires
adding
multiplexers
in
front
of
the
ALU
to
select
the
operand
from
either
the
register
file
or
the
Memory
or
Writeback
stage
Figure
illustrates
this
principle
In
cycle
s
is
forwarded
from
the
Memory
stage
of
the
add
instruction
to
the
Execute
stage
of
the
dependent
and
instruction
In
cycle
s
is
forwarded
from
the
Writeback
stage
of
the
add
instruction
to
the
Execute
stage
of
the
dependent
or
instruction
Forwarding
is
necessary
when
an
instruction
in
the
Execute
stage
has
a
source
register
matching
the
destination
register
of
an
instruction
in
the
Memory
or
Writeback
stage
Figure
modifies
the
pipelined
processor
to
support
forwarding
It
adds
a
hazard
detection
unit
and
two
forwarding
multiplexers
The
hazard
detection
unit
receives
the
two
source
registers
from
the
instruction
in
the
Execute
stage
and
the
destination
registers
from
the
instructions
in
the
Memory
and
Writeback
stages
It
also
receives
the
RegWrite
signals
from
the
Memory
and
Writeback
stages
to
know
whether
the
destination
register
will
actually
be
written
for
example
the
sw
and
beq
instructions
do
not
write
results
to
the
register
file
and
hence
do
not
need
to
have
their
results
forwarded
Note
that
the
RegWrite
signals
are
connected
by
name
In
other
words
rather
than
cluttering
up
the
diagram
with
long
wires
running
from
the
control
signals
at
the
top
to
the
hazard
unit
at
the
bottom
the
connections
are
indicated
by
a
short
stub
of
wire
labeled
with
the
control
signal
name
to
which
it
is
connected
CHAPTER
SEVEN
Microarchitecture
Time
cycles
add
s
s
s
RF
s
s
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
add
or
sub
Figure
Abstract
pipeline
diagram
illustrating
forwarding
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
MhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
ForwardAE
ForwardBE
RegWriteM
RegWriteW
drazaH
E
sulPCP
EhcnarB
MhcnarB
MoreZ
ALU
Figure
Pipelined
processor
with
forwarding
to
solve
hazards
The
hazard
detection
unit
computes
control
signals
for
the
forwarding
multiplexers
to
choose
operands
from
the
register
file
or
from
the
results
in
the
Memory
or
Writeback
stage
It
should
forward
from
a
stage
if
that
stage
will
write
a
destination
register
and
the
destination
register
matches
the
source
register
However
is
hardwired
to
and
should
never
be
forwarded
If
both
the
Memory
and
Writeback
stages
contain
matching
destination
registers
the
Memory
stage
should
have
priority
because
it
contains
the
more
recently
executed
instruction
In
summary
the
function
of
the
forwarding
logic
for
SrcA
is
given
below
The
forwarding
logic
for
SrcB
ForwardBE
is
identical
except
that
it
checks
rt
rather
than
rs
if
rsE
AND
rsE
WriteRegM
AND
RegWriteM
then
ForwardAE
else
if
rsE
AND
rsE
WriteRegW
AND
RegWriteW
then
ForwardAE
else
ForwardAE
Solving
Data
Hazards
with
Stalls
Forwarding
is
sufficient
to
solve
RAW
data
hazards
when
the
result
is
computed
in
the
Execute
stage
of
an
instruction
because
its
result
can
then
be
forwarded
to
the
Execute
stage
of
the
next
instruction
Unfortunately
the
lw
instruction
does
not
finish
reading
data
until
the
end
of
the
Memory
stage
so
its
result
cannot
be
forwarded
to
the
Execute
stage
of
the
next
instruction
We
say
that
the
lw
instruction
has
a
two
cycle
latency
because
a
dependent
instruction
cannot
use
its
result
until
two
cycles
later
Figure
shows
this
problem
The
lw
instruction
receives
data
from
memory
at
the
end
of
cycle
But
the
and
instruction
needs
that
data
as
a
source
operand
at
the
beginning
of
cycle
There
is
no
way
to
solve
this
hazard
with
forwarding
CHAPTER
SEVEN
Microarchitecture
Time
cycles
lw
s
RF
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
lw
or
sub
Trouble
Figure
Abstract
pipeline
diagram
illustrating
trouble
forwarding
from
lw
The
alternative
solution
is
to
stall
the
pipeline
holding
up
operation
until
the
data
is
available
Figure
shows
stalling
the
dependent
instruction
and
in
the
Decode
stage
and
enters
the
Decode
stage
in
cycle
and
stalls
there
through
cycle
The
subsequent
instruction
or
must
remain
in
the
Fetch
stage
during
both
cycles
as
well
because
the
Decode
stage
is
full
In
cycle
the
result
can
be
forwarded
from
the
Writeback
stage
of
lw
to
the
Execute
stage
of
and
In
cycle
source
s
of
the
or
instruction
is
read
directly
from
the
register
file
with
no
need
for
forwarding
Notice
that
the
Execute
stage
is
unused
in
cycle
Likewise
Memory
is
unused
in
Cycle
and
Writeback
is
unused
in
cycle
This
unused
stage
propagating
through
the
pipeline
is
called
a
bubble
and
it
behaves
like
a
nop
instruction
The
bubble
is
introduced
by
zeroing
out
the
Execute
stage
control
signals
during
a
Decode
stall
so
that
the
bubble
performs
no
action
and
changes
no
architectural
state
In
summary
stalling
a
stage
is
performed
by
disabling
the
pipeline
register
so
that
the
contents
do
not
change
When
a
stage
is
stalled
all
previous
stages
must
also
be
stalled
so
that
no
subsequent
instructions
are
lost
The
pipeline
register
directly
after
the
stalled
stage
must
be
cleared
to
prevent
bogus
information
from
propagating
forward
Stalls
degrade
performance
so
they
should
only
be
used
when
necessary
Figure
modifies
the
pipelined
processor
to
add
stalls
for
lw
data
dependencies
The
hazard
unit
examines
the
instruction
in
the
Execute
stage
If
it
is
lw
and
its
destination
register
rtE
matches
either
source
operand
of
the
instruction
in
the
Decode
stage
rsD
or
rtD
that
instruction
must
be
stalled
in
the
Decode
stage
until
the
source
operand
is
ready
Stalls
are
supported
by
adding
enable
inputs
EN
to
the
Fetch
and
Decode
pipeline
registers
and
a
synchronous
reset
clear
CLR
input
to
the
Execute
pipeline
register
When
a
lw
stall
occurs
StallD
and
StallF
Pipelined
Processor
Time
cycles
lw
s
RF
RF
s
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
RF
s
s
RF
t
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
lw
or
sub
RF
s
s
IM
or
Stall
Figure
Abstract
pipeline
diagram
illustrating
stall
to
solve
hazards
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
MhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
drazaH
E
sulPCP
EhcnarB
MhcnarB
MoreZ
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallD
EN
EN
ALU
CLR
StallF
RegWriteW
Figure
Pipelined
processor
with
stalls
to
solve
lw
data
hazard
are
asserted
to
force
the
Decode
and
Fetch
stage
pipeline
registers
to
hold
their
old
values
FlushE
is
also
asserted
to
clear
the
contents
of
the
Execute
stage
pipeline
register
introducing
a
bubble
The
MemtoReg
signal
is
asserted
for
the
lw
instruction
Hence
the
logic
to
compute
the
stalls
and
flushes
is
lwstall
rsD
rtE
OR
rtD
rtE
AND
MemtoRegE
StallF
StallD
FlushE
lwstall
Solving
Control
Hazards
The
beq
instruction
presents
a
control
hazard
the
pipelined
processor
does
not
know
what
instruction
to
fetch
next
because
the
branch
decision
has
not
been
made
by
the
time
the
next
instruction
is
fetched
One
mechanism
for
dealing
with
the
control
hazard
is
to
stall
the
pipeline
until
the
branch
decision
is
made
i
e
PCSrc
is
computed
Because
the
decision
is
made
in
the
Memory
stage
the
pipeline
would
have
to
be
stalled
for
three
cycles
at
every
branch
This
would
severely
degrade
the
system
performance
An
alternative
is
to
predict
whether
the
branch
will
be
taken
and
begin
executing
instructions
based
on
the
prediction
Once
the
branch
decision
is
available
the
processor
can
throw
out
the
instructions
if
the
prediction
was
wrong
In
particular
suppose
that
we
predict
that
branches
are
not
taken
and
simply
continue
executing
the
program
in
order
If
the
branch
should
have
been
taken
the
three
instructions
following
the
branch
must
be
flushed
discarded
by
clearing
the
pipeline
registers
for
those
instructions
These
wasted
instruction
cycles
are
called
the
branch
misprediction
penalty
Figure
shows
such
a
scheme
in
which
a
branch
from
address
to
address
is
taken
The
branch
decision
is
not
made
until
cycle
by
which
point
the
and
or
and
sub
instructions
at
addresses
and
C
have
already
been
fetched
These
instructions
must
be
flushed
and
the
slt
instruction
is
fetched
from
address
in
cycle
This
is
somewhat
of
an
improvement
but
flushing
so
many
instructions
when
the
branch
is
taken
still
degrades
performance
We
could
reduce
the
branch
misprediction
penalty
if
the
branch
decision
could
be
made
earlier
Making
the
decision
simply
requires
comparing
the
values
of
two
registers
Using
a
dedicated
equality
comparator
is
much
faster
than
performing
a
subtraction
and
zero
detection
If
the
comparator
is
fast
enough
it
could
be
moved
back
into
the
Decode
stage
so
that
the
operands
are
read
from
the
register
file
and
compared
to
determine
the
next
PC
by
the
end
of
the
Decode
stage
Pipelined
Processor
Strictly
speaking
only
the
register
designations
RsE
RtE
and
RdE
and
the
control
signals
that
might
update
memory
or
architectural
state
RegWrite
MemWrite
and
Branch
need
to
be
cleared
as
long
as
these
signals
are
cleared
the
bubble
can
contain
random
data
that
has
no
effect
Figure
shows
the
pipeline
operation
with
the
early
branch
decision
being
made
in
cycle
In
cycle
the
and
instruction
is
flushed
and
the
slt
instruction
is
fetched
Now
the
branch
misprediction
penalty
is
reduced
to
only
one
instruction
rather
than
three
Figure
modifies
the
pipelined
processor
to
move
the
branch
decision
earlier
and
handle
control
hazards
An
equality
comparator
is
added
to
the
Decode
stage
and
the
PCSrc
AND
gate
is
moved
earlier
so
CHAPTER
SEVEN
Microarchitecture
Time
cycles
beq
t
t
RF
t
t
RF
DM
RF
s
s
RF
DM
RF
s
s
RF
DM
RF
s
s
RF
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
IM
IM
lw
or
sub
C
Flush
these
instructions
slt
t
s
s
RF
s
s
RF
DM
t
IM
slt
slt
Figure
Abstract
pipeline
diagram
illustrating
flushing
when
a
branch
is
taken
Time
cycles
beq
t
t
RF
t
t
RF
DM
RF
s
s
RF
DM
and
t
s
s
or
t
s
s
sub
t
s
s
and
IM
IM
lw
C
Flush
this
instruction
slt
t
s
s
RF
s
s
RF
DM
t
IM
slt
slt
Figure
Abstract
pipeline
diagram
illustrating
earlier
branch
decision
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
DhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
DcrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
drazaH
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallF
StallD
RegWriteW
CLR
EN
EN
ALU
CLR
Figure
Pipelined
processor
handling
branch
control
hazard
that
PCSrc
can
be
determined
in
the
Decoder
stage
rather
than
the
Memory
stage
The
PCBranch
adder
must
also
be
moved
into
the
Decode
stage
so
that
the
destination
address
can
be
computed
in
time
The
synchronous
clear
input
CLR
connected
to
PCSrcD
is
added
to
the
Decode
stage
pipeline
register
so
that
the
incorrectly
fetched
instruction
can
be
flushed
when
a
branch
is
taken
Unfortunately
the
early
branch
decision
hardware
introduces
a
new
RAW
data
hazard
Specifically
if
one
of
the
source
operands
for
the
branch
was
computed
by
a
previous
instruction
and
has
not
yet
been
written
into
the
register
file
the
branch
will
read
the
wrong
operand
value
from
the
register
file
As
before
we
can
solve
the
data
hazard
by
forwarding
the
correct
value
if
it
is
available
or
by
stalling
the
pipeline
until
the
data
is
ready
Figure
shows
the
modifications
to
the
pipelined
processor
needed
to
handle
the
Decode
stage
data
dependency
If
a
result
is
in
the
Writeback
stage
it
will
be
written
in
the
first
half
of
the
cycle
and
read
during
the
second
half
so
no
hazard
exists
If
the
result
of
an
ALU
instruction
is
in
the
Memory
stage
it
can
be
forwarded
to
the
equality
comparator
through
two
new
multiplexers
If
the
result
of
an
ALU
instruction
is
in
the
Execute
stage
or
the
result
of
a
lw
instruction
is
in
the
Memory
stage
the
pipeline
must
be
stalled
at
the
Decode
stage
until
the
result
is
ready
The
function
of
the
Decode
stage
forwarding
logic
is
given
below
ForwardAD
rsD
AND
rsD
WriteRegM
AND
RegWriteM
ForwardBD
rtD
AND
rtD
WriteRegM
AND
RegWriteM
The
function
of
the
stall
detection
logic
for
a
branch
is
given
below
The
processor
must
make
a
branch
decision
in
the
Decode
stage
If
either
of
the
sources
of
the
branch
depends
on
an
ALU
instruction
in
the
Execute
stage
or
on
a
lw
instruction
in
the
Memory
stage
the
processor
must
stall
until
the
sources
are
ready
branchstall
BranchD
AND
RegWriteE
AND
WriteRegE
rsD
OR
WriteRegE
rtD
OR
BranchD
AND
MemtoRegM
AND
WriteRegM
rsD
OR
WriteRegM
rtD
Now
the
processor
might
stall
due
to
either
a
load
or
a
branch
hazard
StallF
StallD
FlushE
lwstall
OR
branchstall
Hazard
Summary
In
summary
RAW
data
hazards
occur
when
an
instruction
depends
on
the
result
of
another
instruction
that
has
not
yet
been
written
into
the
register
file
The
data
hazards
can
be
resolved
by
forwarding
if
the
result
is
computed
soon
enough
otherwise
they
require
stalling
the
pipeline
until
the
result
is
available
Control
hazards
occur
when
the
decision
of
what
instruction
to
fetch
has
not
been
made
by
the
time
the
next
instruction
must
be
fetched
Control
hazards
are
solved
by
predicting
which
CHAPTER
SEVEN
Microarchitecture
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
omeM
yr
A
A
DW
DR
DR
EW
A
KLC
ngiS
dnetxE
retsigeR
eliF
DRA
ataD
yromeM
DW
EW
FCP
CP
DrtsnI
EBcrS
EsR
EdR
MtuOULA
WtuOULA
WataDdaeR
EataDetirW
MataDetirW
EAcrS
D
sulPCP
DhcnarBCP
geRetirW
M
WtluseR
F
sulPCP
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D
lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
Control
Unit
DcrSCP
KLC
KLC
KLC
KLC
KLC
geRetirW
W
E
lortnoCULA
EetirWgeR
MetirWgeR
WetirWgeR
EgeRotmeM
MgeRotmeM
WgeRotmeM
EetirWmeM
MetirWmeM
EtsDgeR
EcrSULA
geRetirW
E
DmmIngiS
EtR
DsR
DdR
DtR
tinU
drazaH
ForwardAD
ForwardAE
ForwardBE
BranchD
ForwardBD
RegWriteE
RegWriteM
MemtoRegE
MemtoRegM
FlushE
CLR
CLR
EN
EN
ALU
StallF
StallD
RegWriteW
Figure
Pipelined
processor
handling
data
dependencies
for
branch
instructions
instruction
should
be
fetched
and
flushing
the
pipeline
if
the
prediction
is
later
determined
to
be
wrong
Moving
the
decision
as
early
as
possible
minimizes
the
number
of
instructions
that
are
flushed
on
a
misprediction
You
may
have
observed
by
now
that
one
of
the
challenges
of
designing
a
pipelined
processor
is
to
understand
all
the
possible
interactions
between
instructions
and
to
discover
all
the
hazards
that
may
exist
Figure
shows
the
complete
pipelined
processor
handling
all
of
the
hazards
More
Instructions
Supporting
new
instructions
in
the
pipelined
processor
is
much
like
supporting
them
in
the
single
cycle
processor
However
new
instructions
may
introduce
hazards
that
must
be
detected
and
solved
In
particular
supporting
addi
and
j
instructions
on
the
pipelined
processor
requires
enhancing
the
controller
exactly
as
was
described
in
Section
and
adding
a
jump
multiplexer
to
the
datapath
after
the
branch
multiplexer
Like
a
branch
the
jump
takes
place
in
the
Decode
stage
so
the
subsequent
instruction
in
the
Fetch
stage
must
be
flushed
Designing
this
flush
logic
is
left
as
Exercise
Performance
Analysis
The
pipelined
processor
ideally
would
have
a
CPI
of
because
a
new
instruction
is
issued
every
cycle
However
a
stall
or
a
flush
wastes
a
cycle
so
the
CPI
is
slightly
higher
and
depends
on
the
specific
program
being
executed
Example
PIPELINED
PROCESSOR
CPI
The
SPECINT
benchmark
considered
in
Example
consists
of
approximately
loads
stores
branches
jumps
and
R
type
instructions
Assume
that
of
the
loads
are
immediately
followed
by
an
instruction
that
uses
the
result
requiring
a
stall
and
that
one
quarter
of
the
branches
are
mispredicted
requiring
a
flush
Assume
that
jumps
always
flush
the
subsequent
instruction
Ignore
other
hazards
Compute
the
average
CPI
of
the
pipelined
processor
Solution
The
average
CPI
is
the
sum
over
each
instruction
of
the
CPI
for
that
instruction
multiplied
by
the
fraction
of
time
that
instruction
is
used
Loads
take
one
clock
cycle
when
there
is
no
dependency
and
two
cycles
when
the
processor
must
stall
for
a
dependency
so
they
have
a
CPI
of
Branches
take
one
clock
cycle
when
they
are
predicted
properly
and
two
when
they
are
not
so
they
have
a
CPI
of
Jumps
always
have
a
CPI
of
All
other
instructions
have
a
CPI
of
Hence
for
this
benchmark
Average
CPI
CHAPTER
SEVEN
Microarchitecture
EqualD
SignImmE
CLK
A
RD
Instruction
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
A
RD
Data
Memory
WD
WE
PCF
PC
InstrD
SrcBE
RsE
RdE
ALUOutM
ALUOutW
ReadDataW
WriteDataE
WriteDataM
SrcAE
PCPlus
D
PCBranchD
WriteRegM
ResultW
PCPlus
F
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
PCSrcD
CLK
CLK
CLK
CLK
CLK
WriteRegW
ALUControlE
ALU
RegWriteE
RegWriteM
RegWriteW
MemtoRegE
MemtoRegM
MemtoRegW
MemWriteE
MemWriteM
RegDstE
ALUSrcE
WriteRegE
SignImmD
RtE
RsD
RdD
RtD
Hazard
Unit
StallF
StallD
ForwardAD
ForwardBD
ForwardAE
ForwardBE
MemtoRegE
RegWriteE
RegWriteM
RegWriteW
BranchD
FlushE
EN
EN
CLR
CLR
Figure
Pipelined
processor
with
full
hazard
handling
We
can
determine
the
cycle
time
by
considering
the
critical
path
in
each
of
the
five
pipeline
stages
shown
in
Figure
Recall
that
the
register
file
is
written
in
the
first
half
of
the
Writeback
cycle
and
read
in
the
second
half
of
the
Decode
cycle
Therefore
the
cycle
time
of
the
Decode
and
Writeback
stages
is
twice
the
time
necessary
to
do
the
half
cycle
of
work
Example
PROCESSOR
PERFORMANCE
COMPARISON
Ben
Bitdiddle
needs
to
compare
the
pipelined
processor
performance
to
that
of
the
single
cycle
and
multicycle
processors
considered
in
Example
Most
of
the
logic
delays
were
given
in
Table
The
other
element
delays
are
ps
for
an
equality
comparator
ps
for
an
AND
gate
ps
for
a
register
file
write
and
ps
for
a
memory
write
Help
Ben
compare
the
execution
time
of
billion
instructions
from
the
SPECINT
benchmark
for
each
processor
Solution
According
to
Equation
the
cycle
time
of
the
pipelined
processor
is
Tc
max
ps
According
to
Equation
the
total
execution
time
is
T
instructions
cycles
instruction
s
cycle
seconds
This
compares
to
seconds
for
the
single
cycle
processor
and
seconds
for
the
multicycle
processor
The
pipelined
processor
is
substantially
faster
than
the
others
However
its
advantage
over
the
single
cycle
processor
is
nowhere
near
the
five
fold
speedup
one
might
hope
to
get
from
a
five
stage
pipeline
The
pipeline
hazards
introduce
a
small
CPI
penalty
More
significantly
the
sequencing
overhead
clk
to
Q
and
setup
times
of
the
registers
applies
to
every
pipeline
stage
not
just
once
to
the
overall
datapath
Sequencing
overhead
limits
the
benefits
one
can
hope
to
achieve
from
pipelining
The
careful
reader
might
observe
that
the
Decode
stage
is
substantially
slower
than
the
others
because
the
register
file
write
read
and
branch
comparison
must
all
happen
in
half
a
cycle
Perhaps
moving
the
branch
comparison
to
the
Decode
stage
was
not
such
a
good
idea
If
branches
were
resolved
in
the
Execute
stage
instead
the
CPI
would
increase
slightly
because
a
mispredict
would
flush
two
instructions
but
the
cycle
time
would
decrease
substantially
giving
an
overall
speedup
The
pipelined
processor
is
similar
in
hardware
requirements
to
the
single
cycle
processor
but
it
adds
a
substantial
number
of
pipeline
registers
along
with
multiplexers
and
control
logic
to
resolve
hazards
Tc
max
tpcq
tmem
tsetup
tRFread
tmux
teq
tAND
tmux
tsetup
tpcq
tmux
tmux
tALU
tsetup
tpcq
tmemwrite
tsetup
tpcq
tmux
tRFwrite
CHAPTER
SEVEN
Microarchitecture
Fetch
Decode
Execute
Memory
Writeback
Chap
HDL
REPRESENTATION
This
section
presents
HDL
code
for
the
single
cycle
MIPS
processor
supporting
all
of
the
instructions
discussed
in
this
chapter
including
addi
and
j
The
code
illustrates
good
coding
practices
for
a
moderately
complex
system
HDL
code
for
the
multicycle
processor
and
pipelined
processor
are
left
to
Exercises
and
In
this
section
the
instruction
and
data
memories
are
separated
from
the
main
processor
and
connected
by
address
and
data
busses
This
is
more
realistic
because
most
real
processors
have
external
memory
It
also
illustrates
how
the
processor
can
communicate
with
the
outside
world
The
processor
is
composed
of
a
datapath
and
a
controller
The
controller
in
turn
is
composed
of
the
main
decoder
and
the
ALU
decoder
Figure
shows
a
block
diagram
of
the
single
cycle
MIPS
processor
interfaced
to
external
memories
The
HDL
code
is
partitioned
into
several
sections
Section
provides
HDL
for
the
single
cycle
processor
datapath
and
controller
Section
presents
the
generic
building
blocks
such
as
registers
and
multiplexers
that
are
used
by
any
microarchitecture
Section
HDL
Representation
Opcode
Controller
Funct
Datapath
A
RD
Instruction
Memory
PC
Instr
A
RD
Data
Memory
WD
WE
CLK
ALUOut
WriteData
ReadData
ALUControl
Branch
MemtoReg
ALUSrc
RegDst
MemWrite
Main
Decoder
ALUOp
ALU
Decoder
RegWrite
CLK
Reset
MIPS
Processor
External
Memory
PCSrc
Zero
Figure
MIPS
single
cycle
processor
interfaced
to
external
memory
introduces
the
testbench
and
external
memories
The
HDL
is
available
in
electronic
form
on
the
this
book
s
Web
site
see
the
preface
Single
Cycle
Processor
The
main
modules
of
the
single
cycle
MIPS
processor
module
are
given
in
the
following
HDL
examples
CHAPTER
SEVEN
Microarchitecture
Verilog
module
mips
input
clk
reset
output
pc
input
instr
output
memwrite
output
aluout
writedata
input
readdata
wire
memtoreg
branch
alusrc
regdst
regwrite
jump
wire
alucontrol
controller
c
instr
instr
zero
memtoreg
memwrite
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
datapath
dp
clk
reset
memtoreg
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
zero
pc
instr
aluout
writedata
readdata
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mips
is
single
cycle
MIPS
processor
port
clk
reset
in
STD
LOGIC
pc
out
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
memwrite
out
STD
LOGIC
aluout
writedata
out
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
mips
is
component
controller
port
op
funct
in
STD
LOGIC
VECTOR
downto
zero
in
STD
LOGIC
memtoreg
memwrite
out
STD
LOGIC
pcsrc
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
alucontrol
out
STD
LOGIC
VECTOR
downto
end
component
component
datapath
port
clk
reset
in
STD
LOGIC
memtoreg
pcsrc
in
STD
LOGIC
alusrc
regdst
in
STD
LOGIC
regwrite
jump
in
STD
LOGIC
alucontrol
in
STD
LOGIC
VECTOR
downto
zero
out
STD
LOGIC
pc
buffer
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
aluout
writedata
buffer
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
component
signal
memtoreg
alusrc
regdst
regwrite
jump
pcsrc
STD
LOGIC
signal
zero
STD
LOGIC
signal
alucontrol
STD
LOGIC
VECTOR
downto
begin
cont
controller
port
map
instr
downto
instr
downto
zero
memtoreg
memwrite
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
dp
datapath
port
map
clk
reset
memtoreg
pcsrc
alusrc
regdst
regwrite
jump
alucontrol
zero
pc
instr
aluout
writedata
readdata
end
HDL
Example
SINGLE
CYCLE
MIPS
PROCESSOR
HDL
Representation
Verilog
module
controller
input
op
funct
input
zero
output
memtoreg
memwrite
output
pcsrc
alusrc
output
regdst
regwrite
output
jump
output
alucontrol
wire
aluop
wire
branch
maindec
md
op
memtoreg
memwrite
branch
alusrc
regdst
regwrite
jump
aluop
aludec
ad
funct
aluop
alucontrol
assign
pcsrc
branch
zero
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
controller
is
single
cycle
control
decoder
port
op
funct
in
STD
LOGIC
VECTOR
downto
zero
in
STD
LOGIC
memtoreg
memwrite
out
STD
LOGIC
pcsrc
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
alucontrol
out
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
controller
is
component
maindec
port
op
in
STD
LOGIC
VECTOR
downto
memtoreg
memwrite
out
STD
LOGIC
branch
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
aluop
out
STD
LOGIC
VECTOR
downto
end
component
component
aludec
port
funct
in
STD
LOGIC
VECTOR
downto
aluop
in
STD
LOGIC
VECTOR
downto
alucontrol
out
STD
LOGIC
VECTOR
downto
end
component
signal
aluop
STD
LOGIC
VECTOR
downto
signal
branch
STD
LOGIC
begin
md
maindec
port
map
op
memtoreg
memwrite
branch
alusrc
regdst
regwrite
jump
aluop
ad
aludec
port
map
funct
aluop
alucontrol
pcsrc
branch
and
zero
end
HDL
Example
CONTROLLER
Verilog
module
maindec
input
op
output
memtoreg
memwrite
output
branch
alusrc
output
regdst
regwrite
output
jump
output
aluop
reg
controls
assign
regwrite
regdst
alusrc
branch
memwrite
memtoreg
jump
aluop
controls
always
case
op
b
controls
b
Rtyp
b
controls
b
LW
b
controls
b
SW
b
controls
b
BEQ
b
controls
b
ADDI
b
controls
b
J
default
controls
bxxxxxxxxx
endcase
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
maindec
is
main
control
decoder
port
op
in
STD
LOGIC
VECTOR
downto
memtoreg
memwrite
out
STD
LOGIC
branch
alusrc
out
STD
LOGIC
regdst
regwrite
out
STD
LOGIC
jump
out
STD
LOGIC
aluop
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
maindec
is
signal
controls
STD
LOGIC
VECTOR
downto
begin
process
op
begin
case
op
is
when
controls
Rtyp
when
controls
LW
when
controls
SW
when
controls
BEQ
when
controls
ADDI
when
controls
J
when
others
controls
illegal
op
end
case
end
process
regwrite
controls
regdst
controls
alusrc
controls
branch
controls
memwrite
controls
memtoreg
controls
jump
controls
aluop
controls
downto
end
HDL
Example
MAIN
DECODER
Verilog
module
aludec
input
funct
input
aluop
output
reg
alucontrol
always
case
aluop
b
alucontrol
b
add
b
alucontrol
b
sub
default
case
funct
RTYPE
b
alucontrol
b
ADD
b
alucontrol
b
SUB
b
alucontrol
b
AND
b
alucontrol
b
OR
b
alucontrol
b
SLT
default
alucontrol
bxxx
endcase
endcase
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
aludec
is
ALU
control
decoder
port
funct
in
STD
LOGIC
VECTOR
downto
aluop
in
STD
LOGIC
VECTOR
downto
alucontrol
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
aludec
is
begin
process
aluop
funct
begin
case
aluop
is
when
alucontrol
add
for
b
sb
addi
when
alucontrol
sub
for
beq
when
others
case
funct
is
R
type
instructions
when
alucontrol
add
when
alucontrol
sub
when
alucontrol
and
when
alucontrol
or
when
alucontrol
slt
when
others
alucontrol
end
case
end
case
end
process
end
HDL
Example
ALU
DECODER
Chapter
qxd
P
Verilog
module
datapath
input
clk
reset
input
memtoreg
pcsrc
input
alusrc
regdst
input
regwrite
jump
input
alucontrol
output
zero
output
pc
input
instr
output
aluout
writedata
input
readdata
wire
writereg
wire
pcnext
pcnextbr
pcplus
pcbranch
wire
signimm
signimmsh
wire
srca
srcb
wire
result
next
PC
logic
flopr
pcreg
clk
reset
pcnext
pc
adder
pcadd
pc
b
pcplus
sl
immsh
signimm
signimmsh
adder
pcadd
pcplus
signimmsh
pcbranch
mux
pcbrmux
pcplus
pcbranch
pcsrc
pcnextbr
mux
pcmux
pcnextbr
pcplus
instr
b
jump
pcnext
register
file
logic
regfile
rf
clk
regwrite
instr
instr
writereg
result
srca
writedata
mux
wrmux
instr
instr
regdst
writereg
mux
resmux
aluout
readdata
memtoreg
result
signext
se
instr
signimm
ALU
logic
mux
srcbmux
writedata
signimm
alusrc
srcb
alu
alu
srca
srcb
alucontrol
aluout
zero
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
ARITH
all
entity
datapath
is
MIPS
datapath
port
clk
reset
in
STD
LOGIC
memtoreg
pcsrc
in
STD
LOGIC
alusrc
regdst
in
STD
LOGIC
regwrite
jump
in
STD
LOGIC
alucontrol
in
STD
LOGIC
VECTOR
downto
zero
out
STD
LOGIC
pc
buffer
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
aluout
writedata
buffer
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
architecture
struct
of
datapath
is
component
alu
port
a
b
in
STD
LOGIC
VECTOR
downto
alucontrol
in
STD
LOGIC
VECTOR
downto
result
buffer
STD
LOGIC
VECTOR
downto
zero
out
STD
LOGIC
end
component
component
regfile
port
clk
in
STD
LOGIC
we
in
STD
LOGIC
ra
ra
wa
in
STD
LOGIC
VECTOR
downto
wd
in
STD
LOGIC
VECTOR
downto
rd
rd
out
STD
LOGIC
VECTOR
downto
end
component
component
adder
port
a
b
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
component
component
sl
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
component
component
signext
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
component
component
flopr
generic
width
integer
port
clk
reset
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
width
downto
q
out
STD
LOGIC
VECTOR
width
downto
end
component
component
mux
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
component
signal
writereg
STD
LOGIC
VECTOR
downto
signal
pcjump
pcnext
pcnextbr
pcplus
pcbranch
STD
LOGIC
VECTOR
downto
signal
signimm
signimmsh
STD
LOGIC
VECTOR
downto
signal
srca
srcb
result
STD
LOGIC
VECTOR
downto
begin
next
PC
logic
pcjump
pcplus
downto
instr
downto
pcreg
flopr
generic
map
port
map
clk
reset
pcnext
pc
pcadd
adder
port
map
pc
X
pcplus
immsh
sl
port
map
signimm
signimmsh
pcadd
adder
port
map
pcplus
signimmsh
pcbranch
pcbrmux
mux
generic
map
port
map
pcplus
pcbranch
pcsrc
pcnextbr
pcmux
mux
generic
map
port
map
pcnextbr
pcjump
jump
pcnext
register
file
logic
rf
regfile
port
map
clk
regwrite
instr
downto
instr
downto
writereg
result
srca
writedata
wrmux
mux
generic
map
port
map
instr
downto
instr
downto
regdst
writereg
resmux
mux
generic
map
port
map
aluout
readdata
memtoreg
result
se
signext
port
map
instr
downto
signimm
ALU
logic
srcbmux
mux
generic
map
port
map
writedata
signimm
alusrc
srcb
mainalu
alu
port
map
srca
srcb
alucontrol
aluout
zero
end
HDL
Example
DATAPATH
Ch
Verilog
module
regfile
input
clk
input
we
input
ra
ra
wa
input
wd
output
rd
rd
reg
rf
three
ported
register
file
read
two
ports
combinationally
write
third
port
on
rising
edge
of
clock
register
hardwired
to
always
posedge
clk
if
we
rf
wa
wd
assign
rd
ra
rf
ra
assign
rd
ra
rf
ra
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
regfile
is
three
port
register
file
port
clk
in
STD
LOGIC
we
in
STD
LOGIC
ra
ra
wa
in
STD
LOGIC
VECTOR
downto
wd
in
STD
LOGIC
VECTOR
downto
rd
rd
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
regfile
is
type
ramtype
is
array
downto
of
STD
LOGIC
VECTOR
downto
signal
mem
ramtype
begin
three
ported
register
file
read
two
ports
combinationally
write
third
port
on
rising
edge
of
clock
process
clk
begin
if
clk
event
and
clk
then
if
we
then
mem
CONV
INTEGER
wa
wd
end
if
end
if
end
process
process
ra
ra
begin
if
conv
integer
ra
then
rd
X
register
holds
else
rd
mem
CONV
INTEGER
ra
end
if
if
conv
integer
ra
then
rd
X
else
rd
mem
CONV
INTEGER
ra
end
if
end
process
end
HDL
Example
REGISTER
FILE
Verilog
module
adder
input
a
b
output
y
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
adder
is
adder
port
a
b
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
adder
is
begin
y
a
b
end
HDL
Example
ADDER
Generic
Building
Blocks
This
section
contains
generic
building
blocks
that
may
be
useful
in
any
MIPS
microarchitecture
including
a
register
file
adder
left
shift
unit
sign
extension
unit
resettable
flip
flop
and
multiplexer
The
HDL
for
the
ALU
is
left
to
Exercise
Verilog
module
sl
input
a
output
y
shift
left
by
assign
y
a
b
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
sl
is
shift
left
by
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
sl
is
begin
y
a
downto
end
HDL
Example
LEFT
SHIFT
MULTIPLY
BY
Verilog
module
signext
input
a
output
y
assign
y
a
a
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
signext
is
sign
extender
port
a
in
STD
LOGIC
VECTOR
downto
y
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
signext
is
begin
y
X
a
when
a
else
X
ffff
a
end
HDL
Example
SIGN
EXTENSION
Verilog
module
flopr
parameter
WIDTH
input
clk
reset
input
WIDTH
d
output
reg
WIDTH
q
always
posedge
clk
posedge
reset
if
reset
q
else
q
d
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
ARITH
all
entity
flopr
is
flip
flop
with
synchronous
reset
generic
width
integer
port
clk
reset
in
STD
LOGIC
d
in
STD
LOGIC
VECTOR
width
downto
q
out
STD
LOGIC
VECTOR
width
downto
end
architecture
asynchronous
of
flopr
is
begin
process
clk
reset
begin
if
reset
then
q
CONV
STD
LOGIC
VECTOR
width
elsif
clk
event
and
clk
then
q
d
end
if
end
process
end
HDL
Example
RESETTABLE
FLIP
FLOP
HDL
Representation
CHAPTER
SEVEN
Microarchitecture
Verilog
module
mux
parameter
WIDTH
input
WIDTH
d
d
input
s
output
WIDTH
y
assign
y
s
d
d
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
entity
mux
is
two
input
multiplexer
generic
width
integer
port
d
d
in
STD
LOGIC
VECTOR
width
downto
s
in
STD
LOGIC
y
out
STD
LOGIC
VECTOR
width
downto
end
architecture
behave
of
mux
is
begin
y
d
when
s
else
d
end
HDL
Example
MULTIPLEXER
mipstest
asm
David
Harris
hmc
edu
November
Test
the
MIPS
processor
add
sub
and
or
slt
addi
lw
sw
beq
j
If
successful
it
should
write
the
value
to
address
Assembly
Description
Address
Machine
main
addi
initialize
addi
initialize
c
addi
initialize
fff
or
or
c
e
and
and
add
a
beq
end
shouldn
t
be
taken
a
a
slt
c
a
beq
around
should
be
taken
addi
shouldn
t
happen
around
slt
e
a
add
c
sub
e
sw
ac
lw
c
j
end
should
be
taken
c
addi
shouldn
t
happen
end
sw
write
adr
ac
Figure
Assembly
and
machine
code
for
MIPS
test
program
c
fff
e
a
a
a
a
e
a
e
ac
c
ac
Figure
Contents
of
memfile
dat
Testbench
The
MIPS
testbench
loads
a
program
into
the
memories
The
program
in
Figure
exercises
all
of
the
instructions
by
performing
a
computation
that
should
produce
the
correct
answer
only
if
all
of
the
instructions
are
functioning
properly
Specifically
the
program
will
write
the
value
to
address
if
it
runs
correctly
and
is
unlikely
to
do
so
if
the
hardware
is
buggy
This
is
an
example
of
ad
hoc
testing
HDL
Representation
The
machine
code
is
stored
in
a
hexadecimal
file
called
memfile
dat
see
Figure
which
is
loaded
by
the
testbench
during
simulation
The
file
consists
of
the
machine
code
for
the
instructions
one
instruction
per
line
The
testbench
top
level
MIPS
module
and
external
memory
HDL
code
are
given
in
the
following
examples
The
memories
in
this
example
hold
words
each
Verilog
module
testbench
reg
clk
reg
reset
wire
writedata
dataadr
wire
memwrite
instantiate
device
to
be
tested
top
dut
clk
reset
writedata
dataadr
memwrite
initialize
test
initial
begin
reset
reset
end
generate
clock
to
sequence
tests
always
begin
clk
clk
end
check
results
always
negedge
clk
begin
if
memwrite
begin
if
dataadr
writedata
begin
display
Simulation
succeeded
stop
end
else
if
dataadr
begin
display
Simulation
failed
stop
end
end
end
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
testbench
is
end
architecture
test
of
testbench
is
component
top
port
clk
reset
in
STD
LOGIC
writedata
dataadr
out
STD
LOGIC
VECTOR
downto
memwrite
out
STD
LOGIC
end
component
signal
writedata
dataadr
STD
LOGIC
VECTOR
downto
signal
clk
reset
memwrite
STD
LOGIC
begin
instantiate
device
to
be
tested
dut
top
port
map
clk
reset
writedata
dataadr
memwrite
Generate
clock
with
ns
period
process
begin
clk
wait
for
ns
clk
wait
for
ns
end
process
Generate
reset
for
first
two
clock
cycles
process
begin
reset
wait
for
ns
reset
wait
end
process
check
that
gets
written
to
address
at
end
of
program
process
clk
begin
if
clk
event
and
clk
and
memwrite
then
if
conv
integer
dataadr
and
conv
integer
writedata
then
report
Simulation
succeeded
elsif
dataadr
then
report
Simulation
failed
end
if
end
if
end
process
end
HDL
Example
MIPS
TESTBENCH
Verilog
module
top
input
clk
reset
output
writedata
dataadr
output
memwrite
wire
pc
instr
readdata
instantiate
processor
and
memories
mips
mips
clk
reset
pc
instr
memwrite
dataadr
writedata
readdata
imem
imem
pc
instr
dmem
dmem
clk
memwrite
dataadr
writedata
readdata
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
IEEE
STD
LOGIC
UNSIGNED
all
entity
top
is
top
level
design
for
testing
port
clk
reset
in
STD
LOGIC
writedata
dataadr
buffer
STD
LOGIC
VECTOR
downto
memwrite
buffer
STD
LOGIC
end
architecture
test
of
top
is
component
mips
port
clk
reset
in
STD
LOGIC
pc
out
STD
LOGIC
VECTOR
downto
instr
in
STD
LOGIC
VECTOR
downto
memwrite
out
STD
LOGIC
aluout
writedata
out
STD
LOGIC
VECTOR
downto
readdata
in
STD
LOGIC
VECTOR
downto
end
component
component
imem
port
a
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
component
component
dmem
port
clk
we
in
STD
LOGIC
a
wd
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
component
signal
pc
instr
readdata
STD
LOGIC
VECTOR
downto
begin
instantiate
processor
and
memories
mips
mips
port
map
clk
reset
pc
instr
memwrite
dataadr
writedata
readdata
imem
imem
port
map
pc
downto
instr
dmem
dmem
port
map
clk
memwrite
dataadr
writedata
readdata
end
HDL
Example
MIPS
TOP
LEVEL
MODULE
module
dmem
input
clk
we
input
a
wd
output
rd
reg
RAM
assign
rd
RAM
a
word
aligned
always
posedge
clk
if
we
RAM
a
wd
endmodule
library
IEEE
use
IEEE
STD
LOGIC
all
use
STD
TEXTIO
all
use
IEEE
STD
LOGIC
UNSIGNED
all
use
IEEE
STD
LOGIC
ARITH
all
entity
dmem
is
data
memory
port
clk
we
in
STD
LOGIC
a
wd
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
dmem
is
begin
process
is
type
ramtype
is
array
downto
of
STD
LOGIC
VECTOR
downto
variable
mem
ramtype
begin
read
or
write
memory
loop
if
clk
event
and
clk
then
if
we
then
mem
CONV
INTEGER
a
downto
wd
end
if
end
if
rd
mem
CONV
INTEGER
a
downto
wait
on
clk
a
end
loop
end
process
end
HDL
Example
MIPS
DATA
MEMORY
EXCEPTIONS
Section
introduced
exceptions
which
cause
unplanned
changes
in
the
flow
of
a
program
In
this
section
we
enhance
the
multicycle
processor
to
support
two
types
of
exceptions
undefined
instructions
Verilog
module
imem
input
a
output
rd
reg
RAM
initial
begin
readmemh
memfile
dat
RAM
end
assign
rd
RAM
a
word
aligned
endmodule
VHDL
library
IEEE
use
IEEE
STD
LOGIC
all
use
STD
TEXTIO
all
use
IEEE
STD
LOGIC
UNSIGNED
all
use
IEEE
STD
LOGIC
ARITH
all
entity
imem
is
instruction
memory
port
a
in
STD
LOGIC
VECTOR
downto
rd
out
STD
LOGIC
VECTOR
downto
end
architecture
behave
of
imem
is
begin
process
is
file
mem
file
TEXT
variable
L
line
variable
ch
character
variable
index
result
integer
type
ramtype
is
array
downto
of
STD
LOGIC
VECTOR
downto
variable
mem
ramtype
begin
initialize
memory
from
file
for
i
in
to
loop
set
all
contents
low
mem
conv
integer
i
CONV
STD
LOGIC
VECTOR
end
loop
index
FILE
OPEN
mem
file
C
mips
memfile
dat
READ
MODE
while
not
endfile
mem
file
loop
readline
mem
file
L
result
for
i
in
to
loop
read
L
ch
if
ch
and
ch
then
result
result
character
pos
ch
character
pos
elsif
a
ch
and
ch
f
then
result
result
character
pos
ch
character
pos
a
else
report
Format
error
on
line
integer
image
index
severity
error
end
if
end
loop
mem
index
CONV
STD
LOGIC
VECTOR
result
index
index
end
loop
read
memory
loop
rd
mem
CONV
INTEGER
a
wait
on
a
end
loop
end
process
end
HDL
Example
MIPS
INSTRUCTION
MEMORY
and
arithmetic
overflow
Supporting
exceptions
in
other
microarchitectures
follows
similar
principles
As
described
in
Section
when
an
exception
takes
place
the
processor
copies
the
PC
to
the
EPC
register
and
stores
a
code
in
the
Cause
register
indicating
the
source
of
the
exception
Exception
causes
include
x
for
undefined
instructions
and
x
for
overflow
see
Table
The
processor
then
jumps
to
the
exception
handler
at
memory
address
x
The
exception
handler
is
code
that
responds
to
the
exception
It
is
part
of
the
operating
system
Also
as
discussed
in
Section
the
exception
registers
are
part
of
Coprocessor
a
portion
of
the
MIPS
processor
that
is
used
for
system
functions
Coprocessor
defines
up
to
special
purpose
registers
including
Cause
and
EPC
The
exception
handler
may
use
the
mfc
move
from
coprocessor
instruction
to
copy
these
special
purpose
registers
into
a
general
purpose
register
in
the
register
file
the
Cause
register
is
Coprocessor
register
and
EPC
is
register
To
handle
exceptions
we
must
add
EPC
and
Cause
registers
to
the
datapath
and
extend
the
PCSrc
multiplexer
to
accept
the
exception
handler
address
as
shown
in
Figure
The
two
new
registers
have
write
enables
EPCWrite
and
CauseWrite
to
store
the
PC
and
exception
cause
when
an
exception
takes
place
The
cause
is
generated
by
a
multiplexer
CHAPTER
SEVEN
Microarchitecture
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
ALU
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IorD
IRWrite
PCWrite
PCEn
jump
PCJump
x
Overflow
CLK
EN
EPCWrite
CLK
EN
CauseWrite
IntCause
x
x
EPC
Cause
Figure
Datapath
supporting
overflow
and
undefined
instruction
exceptions
Exceptions
that
selects
the
appropriate
code
for
the
exception
The
ALU
must
also
generate
an
overflow
signal
as
was
discussed
in
Section
To
support
the
mfc
instruction
we
also
add
a
way
to
select
the
Coprocessor
registers
and
write
them
to
the
register
file
as
shown
in
Figure
The
mfc
instruction
specifies
the
Coprocessor
register
by
Instr
in
this
diagram
only
the
Cause
and
EPC
registers
are
supported
We
add
another
input
to
the
MemtoReg
multiplexer
to
select
the
value
from
Coprocessor
The
modified
controller
is
shown
in
Figure
The
controller
receives
the
overflow
flag
from
the
ALU
It
generates
three
new
control
signals
one
to
write
the
EPC
a
second
to
write
the
Cause
register
and
a
third
to
select
the
Cause
It
also
includes
two
new
states
to
support
the
two
exceptions
and
another
state
to
handle
mfc
If
the
controller
receives
an
undefined
instruction
one
that
it
does
not
know
how
to
handle
it
proceeds
to
S
saves
the
PC
in
EPC
writes
x
to
the
Cause
register
and
jumps
to
the
exception
handler
Similarly
if
the
controller
detects
arithmetic
overflow
on
an
add
or
sub
instruction
it
proceeds
to
S
saves
the
PC
in
EPC
writes
x
Strictly
speaking
the
ALU
should
assert
overflow
only
for
add
and
sub
not
for
other
ALU
instructions
SignImm
CLK
A
RD
Instr
Data
Memory
A
A
WD
RD
RD
WE
A
CLK
Sign
Extend
Register
File
PC
PC
Instr
SrcB
ALUResult
SrcA
ALUOut
MemWrite
RegDst
MemtoReg
RegWrite
ALUSrcA
Branch
Zero
PCSrc
CLK
ALUControl
ALU
WD
WE
CLK
Adr
Data
CLK
CLK
A
B
CLK
EN
EN
ALUSrcB
IorD
IRWrite
PCWrite
PCEn
jump
PCJump
x
CLK
EN
EPCWrite
CLK
EN
CauseWrite
IntCause
x
x
EPC
Cause
Overflow
C
Figure
Datapath
supporting
mfc
CHAPTER
SEVEN
Microarchitecture
IorD
AluSrcA
ALUSrcB
ALUOp
PCSrc
IRWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
IorD
RegDst
MemtoReg
RegWrite
IorD
MemWrite
ALUSrcA
ALUSrcB
ALUOp
ALUSrcA
ALUSrcB
ALUOp
PCSrc
Branch
Reset
S
Fetch
S
MemAdr
S
Decode
S
MemRead
S
MemWrite
S
Execute
S
ALU
Writeback
S
Branch
Op
LW
or
Op
SW
Op
R
type
Op
BEQ
Op
LW
Op
SW
RegDst
MemtoReg
RegWrite
S
Mem
Writeback
ALUSrcA
ALUSrcB
ALUOp
RegDst
MemtoReg
RegWrite
Op
ADDI
S
ADDI
Execute
S
ADDI
Writeback
PCSrc
PCWrite
Op
J
S
Jump
Overflow
Overflow
S
Overflow
PCSrc
PCWrite
IntCause
CauseWrite
EPCWrite
Op
others
PCSrc
PCWrite
IntCause
CauseWrite
EPCWrite
S
Undefined
Memtoreg
RegWrite
Op
MFC
S
MFC
PCWrite
RegDst
Figure
Controller
supporting
exceptions
and
mfc
in
the
Cause
register
and
jumps
to
the
exception
handler
Note
that
when
an
exception
occurs
the
instruction
is
discarded
and
the
register
file
is
not
written
When
a
mfc
instruction
is
decoded
the
processor
goes
to
S
and
writes
the
appropriate
Coprocessor
register
to
the
main
register
file
ADVANCED
MICROARCHITECTURE
High
performance
microprocessors
use
a
wide
variety
of
techniques
to
run
programs
faster
Recall
that
the
time
required
to
run
a
program
is
proportional
to
the
period
of
the
clock
and
to
the
number
of
clock
cycles
per
instruction
CPI
Thus
to
increase
performance
we
would
like
to
speed
up
the
clock
and
or
reduce
the
CPI
This
section
surveys
some
existing
speedup
techniques
The
implementation
details
become
quite
complex
so
we
will
focus
on
the
concepts
Hennessy
Patterson
s
Computer
Architecture
text
is
a
definitive
reference
if
you
want
to
fully
understand
the
details
Every
to
years
advances
in
CMOS
manufacturing
reduce
transistor
dimensions
by
in
each
direction
doubling
the
number
of
transistors
that
can
fit
on
a
chip
A
manufacturing
process
is
characterized
by
its
feature
size
which
indicates
the
smallest
transistor
that
can
be
reliably
built
Smaller
transistors
are
faster
and
generally
consume
less
power
Thus
even
if
the
microarchitecture
does
not
change
the
clock
frequency
can
increase
because
all
the
gates
are
faster
Moreover
smaller
transistors
enable
placing
more
transistors
on
a
chip
Microarchitects
use
the
additional
transistors
to
build
more
complicated
processors
or
to
put
more
processors
on
a
chip
Unfortunately
power
consumption
increases
with
the
number
of
transistors
and
the
speed
at
which
they
operate
see
Section
Power
consumption
is
now
an
essential
concern
Microprocessor
designers
have
a
challenging
task
juggling
the
trade
offs
among
speed
power
and
cost
for
chips
with
billions
of
transistors
in
some
of
the
most
complex
systems
that
humans
have
ever
built
Deep
Pipelines
Aside
from
advances
in
manufacturing
the
easiest
way
to
speed
up
the
clock
is
to
chop
the
pipeline
into
more
stages
Each
stage
contains
less
logic
so
it
can
run
faster
This
chapter
has
considered
a
classic
five
stage
pipeline
but
to
stages
are
now
commonly
used
The
maximum
number
of
pipeline
stages
is
limited
by
pipeline
hazards
sequencing
overhead
and
cost
Longer
pipelines
introduce
more
dependencies
Some
of
the
dependencies
can
be
solved
by
forwarding
but
others
require
stalls
which
increase
the
CPI
The
pipeline
registers
between
each
stage
have
sequencing
overhead
from
their
setup
time
and
clk
to
Q
delay
as
well
as
clock
skew
This
sequencing
overhead
makes
adding
more
pipeline
stages
give
diminishing
returns
Finally
adding
more
stages
increases
the
cost
because
of
the
extra
pipeline
registers
and
hardware
required
to
handle
hazards
Advanced
Microarchitecture
Example
DEEP
PIPELINES
Consider
building
a
pipelined
processor
by
chopping
up
the
single
cycle
processor
into
N
stages
N
The
single
cycle
processor
has
a
propagation
delay
of
ps
through
the
combinational
logic
The
sequencing
overhead
of
a
register
is
ps
Assume
that
the
combinational
delay
can
be
arbitrarily
divided
into
any
number
of
stages
and
that
pipeline
hazard
logic
does
not
increase
the
delay
The
five
stage
pipeline
in
Example
has
a
CPI
of
Assume
that
each
additional
stage
increases
the
CPI
by
because
of
branch
mispredictions
and
other
pipeline
hazards
How
many
pipeline
stages
should
be
used
to
make
the
processor
execute
programs
as
fast
as
possible
Solution
If
the
ps
combinational
logic
delay
is
divided
into
N
stages
and
each
stage
also
pays
ps
of
sequencing
overhead
for
its
pipeline
register
the
cycle
time
is
Tc
N
The
CPI
is
N
The
time
per
instruction
or
instruction
time
is
the
product
of
the
cycle
time
and
the
CPI
Figure
plots
the
cycle
time
and
instruction
time
versus
the
number
of
stages
The
instruction
time
has
a
minimum
of
ps
at
N
stages
This
minimum
is
only
slightly
better
than
the
ps
per
instruction
achieved
with
a
six
stage
pipeline
In
the
late
s
and
early
s
microprocessors
were
marketed
largely
based
on
clock
frequency
Tc
This
pushed
microprocessors
to
use
very
deep
pipelines
to
stages
on
the
Pentium
to
maximize
the
clock
frequency
even
if
the
benefits
for
overall
performance
were
questionable
Power
is
proportional
to
clock
frequency
and
also
increases
with
the
number
of
pipeline
registers
so
now
that
power
consumption
is
so
important
pipeline
depths
are
decreasing
CHAPTER
SEVEN
Microarchitecture
Tc
Instruction
Time
Number
of
pipeline
stages
Time
ps
Figure
Cycle
time
and
instruction
time
versus
the
number
of
pipeline
stages
Branch
Prediction
An
ideal
pipelined
processor
would
have
a
CPI
of
The
branch
misprediction
penalty
is
a
major
reason
for
increased
CPI
As
pipelines
get
deeper
branches
are
resolved
later
in
the
pipeline
Thus
the
branch
misprediction
penalty
gets
larger
because
all
the
instructions
issued
after
the
mispredicted
branch
must
be
flushed
To
address
this
problem
most
pipelined
processors
use
a
branch
predictor
to
guess
whether
the
branch
should
be
taken
Recall
that
our
pipeline
from
Section
simply
predicted
that
branches
are
never
taken
Some
branches
occur
when
a
program
reaches
the
end
of
a
loop
e
g
a
for
or
while
statement
and
branches
back
to
repeat
the
loop
Loops
tend
to
be
executed
many
times
so
these
backward
branches
are
usually
taken
The
simplest
form
of
branch
prediction
checks
the
direction
of
the
branch
and
predicts
that
backward
branches
should
be
taken
This
is
called
static
branch
prediction
because
it
does
not
depend
on
the
history
of
the
program
Forward
branches
are
difficult
to
predict
without
knowing
more
about
the
specific
program
Therefore
most
processors
use
dynamic
branch
predictors
which
use
the
history
of
program
execution
to
guess
whether
a
branch
should
be
taken
Dynamic
branch
predictors
maintain
a
table
of
the
last
several
hundred
or
thousand
branch
instructions
that
the
processor
has
executed
The
table
sometimes
called
a
branch
target
buffer
includes
the
destination
of
the
branch
and
a
history
of
whether
the
branch
was
taken
To
see
the
operation
of
dynamic
branch
predictors
consider
the
following
loop
code
from
Code
Example
The
loop
repeats
times
and
the
beq
out
of
the
loop
is
taken
only
on
the
last
time
add
s
sum
add
s
i
addi
t
t
for
beq
s
t
done
if
i
branch
to
done
add
s
s
s
sum
sum
i
addi
s
s
increment
i
j
for
done
A
one
bit
dynamic
branch
predictor
remembers
whether
the
branch
was
taken
the
last
time
and
predicts
that
it
will
do
the
same
thing
the
next
time
While
the
loop
is
repeating
it
remembers
that
the
beq
was
not
taken
last
time
and
predicts
that
it
should
not
be
taken
next
time
This
is
a
correct
prediction
until
the
last
branch
of
the
loop
when
the
branch
does
get
taken
Unfortunately
if
the
loop
is
run
again
the
branch
predictor
remembers
that
the
last
branch
was
taken
Therefore
Advanced
Microarchitecture
it
incorrectly
predicts
that
the
branch
should
be
taken
when
the
loop
is
first
run
again
In
summary
a
bit
branch
predictor
mispredicts
the
first
and
last
branches
of
a
loop
A
bit
dynamic
branch
predictor
solves
this
problem
by
having
four
states
strongly
taken
weakly
taken
weakly
not
taken
and
strongly
not
taken
as
shown
in
Figure
When
the
loop
is
repeating
it
enters
the
strongly
not
taken
state
and
predicts
that
the
branch
should
not
be
taken
next
time
This
is
correct
until
the
last
branch
of
the
loop
which
is
taken
and
moves
the
predictor
to
the
weakly
not
taken
state
When
the
loop
is
first
run
again
the
branch
predictor
correctly
predicts
that
the
branch
should
not
be
taken
and
reenters
the
strongly
not
taken
state
In
summary
a
bit
branch
predictor
mispredicts
only
the
last
branch
of
a
loop
As
one
can
imagine
branch
predictors
may
be
used
to
track
even
more
history
of
the
program
to
increase
the
accuracy
of
predictions
Good
branch
predictors
achieve
better
than
accuracy
on
typical
programs
The
branch
predictor
operates
in
the
Fetch
stage
of
the
pipeline
so
that
it
can
determine
which
instruction
to
execute
on
the
next
cycle
When
it
predicts
that
the
branch
should
be
taken
the
processor
fetches
the
next
instruction
from
the
branch
destination
stored
in
the
branch
target
buffer
By
keeping
track
of
both
branch
and
jump
destinations
in
the
branch
target
buffer
the
processor
can
also
avoid
flushing
the
pipeline
during
jump
instructions
Superscalar
Processor
A
superscalar
processor
contains
multiple
copies
of
the
datapath
hardware
to
execute
multiple
instructions
simultaneously
Figure
shows
a
block
diagram
of
a
two
way
superscalar
processor
that
fetches
and
executes
two
instructions
per
cycle
The
datapath
fetches
two
instructions
at
a
time
from
the
instruction
memory
It
has
a
six
ported
register
file
to
read
four
source
operands
and
write
two
results
back
in
each
cycle
It
also
contains
two
ALUs
and
a
two
ported
data
memory
to
execute
the
two
instructions
at
the
same
time
CHAPTER
SEVEN
Microarchitecture
A
scalar
processor
acts
on
one
piece
of
data
at
a
time
A
vector
processor
acts
on
several
pieces
of
data
with
a
single
instruction
A
superscalar
processor
issues
several
instructions
at
a
time
each
of
which
operates
on
one
piece
of
data
Our
MIPS
pipelined
processor
is
a
scalar
processor
Vector
processors
were
popular
for
supercomputers
in
the
s
and
s
because
they
efficiently
handled
the
long
vectors
of
data
common
in
scientific
computations
Modern
highperformance
microprocessors
are
superscalar
because
issuing
several
independent
instructions
is
more
flexible
than
processing
vectors
However
modern
processors
also
include
hardware
to
handle
short
vectors
of
data
that
are
common
in
multimedia
and
graphics
applications
These
are
called
single
instruction
multiple
data
SIMD
units
strongly
taken
predict
taken
weakly
taken
predict
taken
weakly
not
taken
predict
not
taken
strongly
not
taken
predict
not
taken
taken
taken
taken
taken
taken
taken
taken
taken
Figure
bit
branch
predictor
state
transition
diagram
Figure
shows
a
pipeline
diagram
illustrating
the
two
way
superscalar
processor
executing
two
instructions
on
each
cycle
For
this
program
the
processor
has
a
CPI
of
Designers
commonly
refer
to
the
reciprocal
of
the
CPI
as
the
instructions
per
cycle
or
IPC
This
processor
has
an
IPC
of
on
this
program
Executing
many
instructions
simultaneously
is
difficult
because
of
dependencies
For
example
Figure
shows
a
pipeline
diagram
running
a
program
with
data
dependencies
The
dependencies
in
the
code
are
shown
in
blue
The
add
instruction
is
dependent
on
t
which
is
produced
by
the
lw
instruction
so
it
cannot
be
issued
at
the
same
time
as
lw
Indeed
the
add
instruction
stalls
for
yet
another
cycle
so
that
lw
can
forward
t
to
add
in
cycle
The
other
dependencies
between
Advanced
Microarchitecture
CLK
CLK
CLK
CLK
A
RD
A
A
A
RD
WD
WD
A
A
A
RD
RD
RD
Instruction
Memory
Register
File
Data
Memory
PC
CLK
A
A
WD
ALUs
WD
RD
RD
Figure
Superscalar
datapath
Time
cycles
RF
s
RF
t
DM
IM
lw
add
lw
t
s
add
t
s
s
sub
t
s
s
and
t
s
s
or
t
s
s
sw
s
s
t
s
s
RF
s
s
RF
t
DM
IM
sub
and
t
s
s
RF
s
s
RF
t
DM
IM
or
sw
s
s
Figure
Abstract
view
of
a
superscalar
pipeline
in
operation
CHAPTER
SEVEN
Microarchitecture
Stall
Time
cycles
RF
s
RF
t
DM
IM
lw
lw
t
s
add
t
t
s
sub
t
s
s
and
t
s
t
sw
s
t
s
t
add
RF
s
t
RF
t
DM
RF
t
s
RF
t
DM
IM
and
IM
or
and
sub
s
s
t
RF
t
RF
DM
sw
IM
s
s
s
s
s
t
or
t
s
s
IM
or
RF
sub
and
and
based
on
t
and
between
or
and
sw
based
on
t
are
handled
by
forwarding
results
produced
in
one
cycle
to
be
consumed
in
the
next
This
program
also
given
below
requires
five
cycles
to
issue
six
instructions
for
an
IPC
of
lw
t
s
add
t
t
s
sub
t
s
s
and
t
s
t
or
t
s
s
sw
s
t
Recall
that
parallelism
comes
in
temporal
and
spatial
forms
Pipelining
is
a
case
of
temporal
parallelism
Multiple
execution
units
is
a
case
of
spatial
parallelism
Superscalar
processors
exploit
both
forms
of
parallelism
to
squeeze
out
performance
far
exceeding
that
of
our
singlecycle
and
multicycle
processors
Commercial
processors
may
be
three
four
or
even
six
way
superscalar
They
must
handle
control
hazards
such
as
branches
as
well
as
data
hazards
Unfortunately
real
programs
have
many
dependencies
so
wide
superscalar
processors
rarely
fully
utilize
all
of
the
execution
units
Moreover
the
large
number
of
execution
units
and
complex
forwarding
networks
consume
vast
amounts
of
circuitry
and
power
Figure
Program
with
data
dependencies
Out
of
Order
Processor
To
cope
with
the
problem
of
dependencies
an
out
of
order
processor
looks
ahead
across
many
instructions
to
issue
or
begin
executing
independent
instructions
as
rapidly
as
possible
The
instructions
can
be
issued
in
a
different
order
than
that
written
by
the
programmer
as
long
as
dependencies
are
honored
so
that
the
program
produces
the
intended
result
Consider
running
the
same
program
from
Figure
on
a
two
way
superscalar
out
of
order
processor
The
processor
can
issue
up
to
two
instructions
per
cycle
from
anywhere
in
the
program
as
long
as
dependencies
are
observed
Figure
shows
the
data
dependencies
and
the
operation
of
the
processor
The
classifications
of
dependencies
as
RAW
and
WAR
will
be
discussed
shortly
The
constraints
on
issuing
instructions
are
described
below
Cycle
The
lw
instruction
issues
The
add
sub
and
and
instructions
are
dependent
on
lw
by
way
of
t
so
they
cannot
issue
yet
However
the
or
instruction
is
independent
so
it
also
issues
Advanced
Microarchitecture
Time
cycles
RF
s
RF
t
DM
IM
lw
lw
t
s
add
t
t
s
sub
t
s
s
and
t
s
t
sw
s
t
or
s
s
t
RF
t
RF
DM
sw
s
or
t
s
s
IM
RF
s
t
RF
t
DM
IM
add
sub
s
s
t
Two
cycle
latency
between
lW
and
use
of
t
RAW
WAR
RAW
RF
t
s
RF
DM
and
IM
t
RAW
Figure
Out
of
order
execution
of
a
program
with
dependencies
C
Cycle
Remember
that
there
is
a
two
cycle
latency
between
when
a
lw
instruction
issues
and
when
a
dependent
instruction
can
use
its
result
so
add
cannot
issue
yet
because
of
the
t
dependence
sub
writes
t
so
it
cannot
issue
before
add
lest
add
receive
the
wrong
value
of
t
and
is
dependent
on
sub
Only
the
sw
instruction
issues
Cycle
On
cycle
t
is
available
so
add
issues
sub
issues
simultaneously
because
it
will
not
write
t
until
after
add
consumes
t
Cycle
The
and
instruction
issues
t
is
forwarded
from
sub
to
and
The
out
of
order
processor
issues
the
six
instructions
in
four
cycles
for
an
IPC
of
The
dependence
of
add
on
lw
by
way
of
t
is
a
read
after
write
RAW
hazard
add
must
not
read
t
until
after
lw
has
written
it
This
is
the
type
of
dependency
we
are
accustomed
to
handling
in
the
pipelined
processor
It
inherently
limits
the
speed
at
which
the
program
can
run
even
if
infinitely
many
execution
units
are
available
Similarly
the
dependence
of
sw
on
or
by
way
of
t
and
of
and
on
sub
by
way
of
t
are
RAW
dependencies
The
dependence
between
sub
and
add
by
way
of
t
is
called
a
write
after
read
WAR
hazard
or
an
antidependence
sub
must
not
write
t
before
add
reads
t
so
that
add
receives
the
correct
value
according
to
the
original
order
of
the
program
WAR
hazards
could
not
occur
in
the
simple
MIPS
pipeline
but
they
may
happen
in
an
out
of
order
processor
if
the
dependent
instruction
in
this
case
sub
is
moved
too
early
A
WAR
hazard
is
not
essential
to
the
operation
of
the
program
It
is
merely
an
artifact
of
the
programmer
s
choice
to
use
the
same
register
for
two
unrelated
instructions
If
the
sub
instruction
had
written
t
instead
of
t
the
dependency
would
disappear
and
sub
could
be
issued
before
add
The
MIPS
architecture
only
has
registers
so
sometimes
the
programmer
is
forced
to
reuse
a
register
and
introduce
a
hazard
just
because
all
the
other
registers
are
in
use
A
third
type
of
hazard
not
shown
in
the
program
is
called
write
after
write
WAW
or
an
output
dependence
A
WAW
hazard
occurs
if
an
instruction
attempts
to
write
a
register
after
a
subsequent
instruction
has
already
written
it
The
hazard
would
result
in
the
wrong
value
being
CHAPTER
SEVEN
Microarchitecture
Ch
written
to
the
register
For
example
in
the
following
program
add
and
sub
both
write
t
The
final
value
in
t
should
come
from
sub
according
to
the
order
of
the
program
If
an
out
of
order
processor
attempted
to
execute
sub
first
the
WAW
hazard
would
occur
add
t
s
s
sub
t
s
s
WAW
hazards
are
not
essential
either
again
they
are
artifacts
caused
by
the
programmer
s
using
the
same
register
for
two
unrelated
instructions
If
the
sub
instruction
were
issued
first
the
program
could
eliminate
the
WAW
hazard
by
discarding
the
result
of
the
add
instead
of
writing
it
to
t
This
is
called
squashing
the
add
Out
of
order
processors
use
a
table
to
keep
track
of
instructions
waiting
to
issue
The
table
sometimes
called
a
scoreboard
contains
information
about
the
dependencies
The
size
of
the
table
determines
how
many
instructions
can
be
considered
for
issue
On
each
cycle
the
processor
examines
the
table
and
issues
as
many
instructions
as
it
can
limited
by
the
dependencies
and
by
the
number
of
execution
units
e
g
ALUs
memory
ports
that
are
available
The
instruction
level
parallelism
ILP
is
the
number
of
instructions
that
can
be
executed
simultaneously
for
a
particular
program
and
microarchitecture
Theoretical
studies
have
shown
that
the
ILP
can
be
quite
large
for
out
of
order
microarchitectures
with
perfect
branch
predictors
and
enormous
numbers
of
execution
units
However
practical
processors
seldom
achieve
an
ILP
greater
than
or
even
with
six
way
superscalar
datapaths
with
out
of
order
execution
Register
Renaming
Out
of
order
processors
use
a
technique
called
register
renaming
to
eliminate
WAR
hazards
Register
renaming
adds
some
nonarchitectural
renaming
registers
to
the
processor
For
example
a
MIPS
processor
might
add
renaming
registers
called
r
r
The
programmer
cannot
use
these
registers
directly
because
they
are
not
part
of
the
architecture
However
the
processor
is
free
to
use
them
to
eliminate
hazards
For
example
in
the
previous
section
a
WAR
hazard
occurred
between
the
sub
and
add
instructions
based
on
reusing
t
The
out
oforder
processor
could
rename
t
to
r
for
the
sub
instruction
Then
Advanced
Microarchitecture
You
might
wonder
why
the
add
needs
to
be
issued
at
all
The
reason
is
that
out
of
order
processors
must
guarantee
that
all
of
the
same
exceptions
occur
that
would
have
occurred
if
the
program
had
been
executed
in
its
original
order
The
add
potentially
may
produce
an
overflow
exception
so
it
must
be
issued
to
check
for
the
exception
even
though
the
result
can
be
discarded
sub
could
be
executed
sooner
because
r
has
no
dependency
on
the
add
instruction
The
processor
keeps
a
table
of
which
registers
were
renamed
so
that
it
can
consistently
rename
registers
in
subsequent
dependent
instructions
In
this
example
t
must
also
be
renamed
to
r
in
the
and
instruction
because
it
refers
to
the
result
of
sub
Figure
shows
the
same
program
from
Figure
executing
on
an
out
of
order
processor
with
register
renaming
t
is
renamed
to
r
in
sub
and
and
to
eliminate
the
WAR
hazard
The
constraints
on
issuing
instructions
are
described
below
Cycle
The
lw
instruction
issues
The
add
instruction
is
dependent
on
lw
by
way
of
t
so
it
cannot
issue
yet
However
the
sub
instruction
is
independent
now
that
its
destination
has
been
renamed
to
r
so
sub
also
issues
Cycle
Remember
that
there
is
a
two
cycle
latency
between
when
a
lw
issues
and
when
a
dependent
instruction
can
use
its
result
so
add
cannot
issue
yet
because
of
the
t
dependence
The
and
instruction
is
dependent
on
sub
so
it
can
issue
r
is
forwarded
from
sub
to
and
The
or
instruction
is
independent
so
it
also
issues
CHAPTER
SEVEN
Microarchitecture
Time
cycles
RF
s
RF
t
DM
IM
lw
lw
t
s
add
t
t
s
sub
r
s
s
and
t
s
r
sw
s
t
sub
s
s
r
RF
r
s
RF
DM
and
s
or
t
s
s
IM
RF
s
t
RF
t
DM
IM
add
sw
t
RAW
s
s
or
RAW
RAW
t
t
Figure
Out
of
order
execution
of
a
program
using
register
renaming
Ch
Cycle
On
cycle
t
is
available
so
add
issues
t
is
also
available
so
sw
issues
The
out
of
order
processor
with
register
renaming
issues
the
six
instructions
in
three
cycles
for
an
IPC
of
Single
Instruction
Multiple
Data
The
term
SIMD
pronounced
sim
dee
stands
for
single
instruction
multiple
data
in
which
a
single
instruction
acts
on
multiple
pieces
of
data
in
parallel
A
common
application
of
SIMD
is
to
perform
many
short
arithmetic
operations
at
once
especially
for
graphics
processing
This
is
also
called
packed
arithmetic
For
example
a
bit
microprocessor
might
pack
four
bit
data
elements
into
one
bit
word
Packed
add
and
subtract
instructions
operate
on
all
four
data
elements
within
the
word
in
parallel
Figure
shows
a
packed
bit
addition
summing
four
pairs
of
bit
numbers
to
produce
four
results
The
word
could
also
be
divided
into
two
bit
elements
Performing
packed
arithmetic
requires
modifying
the
ALU
to
eliminate
carries
between
the
smaller
data
elements
For
example
a
carry
out
of
a
b
should
not
affect
the
result
of
a
b
Short
data
elements
often
appear
in
graphics
processing
For
example
a
pixel
in
a
digital
photo
may
use
bits
to
store
each
of
the
red
green
and
blue
color
components
Using
an
entire
bit
word
to
process
one
of
these
components
wastes
the
upper
bits
When
the
components
from
four
adjacent
pixels
are
packed
into
a
bit
word
the
processing
can
be
performed
four
times
faster
SIMD
instructions
are
even
more
helpful
for
bit
architectures
which
can
pack
eight
bit
elements
four
bit
elements
or
two
bit
elements
into
a
single
bit
word
SIMD
instructions
are
also
used
for
floating
point
computations
for
example
four
bit
single
precision
floating
point
values
can
be
packed
into
a
single
bit
word
Advanced
Microarchitecture
padd
s
s
s
a
s
Bit
position
a
a
a
b
b
b
b
s
a
b
a
b
a
b
a
b
s
Figure
Packed
arithmetic
four
simultaneous
bit
additions
Multithreading
Because
the
ILP
of
real
programs
tends
to
be
fairly
low
adding
more
execution
units
to
a
superscalar
or
out
of
order
processor
gives
diminishing
returns
Another
problem
discussed
in
Chapter
is
that
memory
is
much
slower
than
the
processor
Most
loads
and
stores
access
a
smaller
and
faster
memory
called
a
cache
However
when
the
instructions
or
data
are
not
available
in
the
cache
the
processor
may
stall
for
or
more
cycles
while
retrieving
the
information
from
the
main
memory
Multithreading
is
a
technique
that
helps
keep
a
processor
with
many
execution
units
busy
even
if
the
ILP
of
a
program
is
low
or
the
program
is
stalled
waiting
for
memory
To
explain
multithreading
we
need
to
define
a
few
new
terms
A
program
running
on
a
computer
is
called
a
process
Computers
can
run
multiple
processes
simultaneously
for
example
you
can
play
music
on
a
PC
while
surfing
the
web
and
running
a
virus
checker
Each
process
consists
of
one
or
more
threads
that
also
run
simultaneously
For
example
a
word
processor
may
have
one
thread
handling
the
user
typing
a
second
thread
spell
checking
the
document
while
the
user
works
and
a
third
thread
printing
the
document
In
this
way
the
user
does
not
have
to
wait
for
example
for
a
document
to
finish
printing
before
being
able
to
type
again
In
a
conventional
processor
the
threads
only
give
the
illusion
of
running
simultaneously
The
threads
actually
take
turns
being
executed
on
the
processor
under
control
of
the
OS
When
one
thread
s
turn
ends
the
OS
saves
its
architectural
state
loads
the
architectural
state
of
the
next
thread
and
starts
executing
that
next
thread
This
procedure
is
called
context
switching
As
long
as
the
processor
switches
through
all
the
threads
fast
enough
the
user
perceives
all
of
the
threads
as
running
at
the
same
time
A
multithreaded
processor
contains
more
than
one
copy
of
its
architectural
state
so
that
more
than
one
thread
can
be
active
at
a
time
For
example
if
we
extended
a
MIPS
processor
to
have
four
program
counters
and
registers
four
threads
could
be
available
at
one
time
If
one
thread
stalls
while
waiting
for
data
from
main
memory
the
processor
could
context
switch
to
another
thread
without
any
delay
because
the
program
counter
and
registers
are
already
available
Moreover
if
one
thread
lacks
sufficient
parallelism
to
keep
all
the
execution
units
busy
another
thread
could
issue
instructions
to
the
idle
units
Multithreading
does
not
improve
the
performance
of
an
individual
thread
because
it
does
not
increase
the
ILP
However
it
does
improve
the
overall
throughput
of
the
processor
because
multiple
threads
can
use
processor
resources
that
would
have
been
idle
when
executing
a
single
thread
Multithreading
is
also
relatively
inexpensive
to
implement
because
it
replicates
only
the
PC
and
register
file
not
the
execution
units
and
memories
CHAPTER
SEVEN
Microarchitecture
Multiprocessors
A
multiprocessor
system
consists
of
multiple
processors
and
a
method
for
communication
between
the
processors
A
common
form
of
multiprocessing
in
computer
systems
is
symmetric
multiprocessing
SMP
in
which
two
or
more
identical
processors
share
a
single
main
memory
The
multiple
processors
may
be
separate
chips
or
multiple
cores
on
the
same
chip
Modern
processors
have
enormous
numbers
of
transistors
available
Using
them
to
increase
the
pipeline
depth
or
to
add
more
execution
units
to
a
superscalar
processor
gives
little
performance
benefit
and
is
wasteful
of
power
Around
the
year
computer
architects
made
a
major
shift
to
build
multiple
copies
of
the
processor
on
the
same
chip
these
copies
are
called
cores
Multiprocessors
can
be
used
to
run
more
threads
simultaneously
or
to
run
a
particular
thread
faster
Running
more
threads
simultaneously
is
easy
the
threads
are
simply
divided
up
among
the
processors
Unfortunately
typical
PC
users
need
to
run
only
a
small
number
of
threads
at
any
given
time
Running
a
particular
thread
faster
is
much
more
challenging
The
programmer
must
divide
the
thread
into
pieces
to
perform
on
each
processor
This
becomes
tricky
when
the
processors
need
to
communicate
with
each
other
One
of
the
major
challenges
for
computer
designers
and
programmers
is
to
effectively
use
large
numbers
of
processor
cores
Other
forms
of
multiprocessing
include
asymmetric
multiprocessing
and
clusters
Asymmetric
multiprocessors
use
separate
specialized
microprocessors
for
separate
tasks
For
example
a
cell
phone
contains
a
digital
signal
processor
DSP
with
specialized
instructions
to
decipher
the
wireless
data
in
real
time
and
a
separate
conventional
processor
to
interact
with
the
user
manage
the
phone
book
and
play
games
In
clustered
multiprocessing
each
processor
has
its
own
local
memory
system
Clustering
can
also
refer
to
a
group
of
PCs
connected
together
on
the
network
running
software
to
jointly
solve
a
large
problem
REAL
WORLD
PERSPECTIVE
IA
MICROARCHITECTURE
Section
introduced
the
IA
architecture
used
in
almost
all
PCs
This
section
tracks
the
evolution
of
IA
processors
through
progressively
faster
and
more
complicated
microarchitectures
The
same
principles
we
have
applied
to
the
MIPS
microarchitectures
are
used
in
IA
Intel
invented
the
first
single
chip
microprocessor
the
bit
in
as
a
flexible
controller
for
a
line
of
calculators
It
contained
transistors
manufactured
on
a
mm
sliver
of
silicon
in
a
process
with
a
m
feature
size
and
operated
at
KHz
A
photograph
of
the
chip
taken
under
a
microscope
is
shown
in
Figure
Real
World
Perspective
IA
Microarchitecture
Scientists
searching
for
signs
of
extraterrestrial
intelligence
use
the
world
s
largest
clustered
multiprocessors
to
analyze
radio
telescope
data
for
patterns
that
might
be
signs
of
life
in
other
solar
systems
The
cluster
consists
of
personal
computers
owned
by
more
than
million
volunteers
around
the
world
When
a
computer
in
the
cluster
is
idle
it
fetches
a
piece
of
the
data
from
a
centralized
server
analyzes
the
data
and
sends
the
results
back
to
the
server
You
can
volunteer
your
computer
s
idle
time
for
the
cluster
by
visiting
setiathome
berkeley
edu
In
places
columns
of
four
similar
looking
structures
are
visible
as
one
would
expect
in
a
bit
microprocessor
Around
the
periphery
are
bond
wires
which
are
used
to
connect
the
chip
to
its
package
and
the
circuit
board
The
inspired
the
bit
then
the
which
eventually
evolved
into
the
bit
in
and
the
in
In
Intel
introduced
the
which
extended
the
architecture
to
bits
and
defined
the
IA
architecture
Table
summarizes
major
Intel
IA
microprocessors
In
the
years
since
the
transistor
feature
size
has
shrunk
fold
the
number
of
transistors
CHAPTER
SEVEN
Microarchitecture
Figure
microprocessor
chip
Table
Evolution
of
Intel
IA
microprocessors
Processor
Year
Feature
Size
m
Transistors
Frequency
MHz
Microarchitecture
k
multicycle
M
pipelined
Pentium
M
superscalar
Pentium
II
M
out
of
order
Pentium
III
M
M
out
of
order
Pentium
M
out
of
order
Pentium
M
M
out
of
order
Core
Duo
M
dual
core
on
a
chip
has
increased
by
five
orders
of
magnitude
and
the
operating
frequency
has
increased
by
almost
four
orders
of
magnitude
No
other
field
of
engineering
has
made
such
astonishing
progress
in
such
a
short
time
The
is
a
multicycle
processor
The
major
components
are
labeled
on
the
chip
photograph
in
Figure
The
bit
datapath
is
clearly
visible
on
the
left
Each
of
the
columns
processes
one
bit
of
data
Some
of
the
control
signals
are
generated
using
a
microcode
PLA
that
steps
through
the
various
states
of
the
control
FSM
The
memory
management
unit
in
the
upper
right
controls
access
to
the
external
memory
The
shown
in
Figure
dramatically
improved
performance
using
pipelining
The
datapath
is
again
clearly
visible
along
with
the
control
logic
and
microcode
PLA
The
added
an
on
chip
floating
point
unit
previous
Intel
processors
either
sent
floating
point
instructions
to
a
separate
coprocessor
or
emulated
them
in
software
The
was
too
fast
for
external
memory
to
keep
up
so
it
incorporated
an
KB
cache
onto
the
chip
to
hold
the
most
commonly
used
instructions
and
data
Chapter
describes
caches
in
more
detail
and
revisits
the
cache
systems
on
Intel
IA
processors
Real
World
Perspective
IA
Microarchitecture
Microcode
PLA
bit
Datapath
Memory
Management
Unit
Controller
Figure
microprocessor
chip
The
Pentium
processor
shown
in
Figure
is
a
superscalar
processor
capable
of
executing
two
instructions
simultaneously
Intel
switched
to
the
name
Pentium
instead
of
because
AMD
was
becoming
a
serious
competitor
selling
interchangeable
chips
and
part
numbers
cannot
be
trademarked
The
Pentium
uses
separate
instruction
and
data
caches
It
also
uses
a
branch
predictor
to
reduce
the
performance
penalty
for
branches
The
Pentium
Pro
Pentium
II
and
Pentium
III
processors
all
share
a
common
out
of
order
microarchitecture
code
named
P
The
complex
IA
instructions
are
broken
down
into
one
or
more
micro
ops
similar
CHAPTER
SEVEN
Microarchitecture
bit
Data
path
Controller
KB
Cache
Floating
Point
Unit
Microcode
PLA
Figure
microprocessor
chip
Real
World
Perspective
IA
Microarchitecture
to
MIPS
instructions
The
micro
ops
are
then
executed
on
a
fast
out
oforder
execution
core
with
an
stage
pipeline
Figure
shows
the
Pentium
III
The
bit
datapath
is
called
the
Integer
Execution
Unit
IEU
The
floating
point
datapath
is
called
the
Floating
Point
Unit
KB
Data
Cache
Floating
Point
Unit
KB
Instruction
Cache
Multiprocessor
Logic
Instruction
Fetch
Decode
Complex
Instruction
Support
Bus
Interface
Unit
bit
Datapath
Superscalar
Controller
Branch
Prediction
Figure
Pentium
microprocessor
chip
Instruction
Fetch
KB
Cache
KB
Data
Cache
Data
TLB
Branch
Target
Buffer
FPU
IEU
SIMD
Register
Renaming
Microcode
PLA
KB
Level
Cache
Out
of
Order
Issue
Logic
Bus
Logic
Figure
Pentium
III
microprocessor
chip
CHAPTER
SEVEN
Microarchitecture
IEU
KB
Data
Cache
Trace
Cache
KB
Level
Cache
FPU
SIMD
Bus
Logic
Figure
Pentium
microprocessor
chip
FPU
The
processor
also
has
a
SIMD
unit
to
perform
packed
operations
on
short
integer
and
floating
point
data
A
larger
portion
of
the
chip
is
dedicated
to
issuing
instructions
out
of
order
than
to
actually
executing
the
instructions
The
instruction
and
data
caches
have
grown
to
KB
each
The
Pentium
III
also
has
a
larger
but
slower
KB
second
level
cache
on
the
same
chip
By
the
late
s
processors
were
marketed
largely
on
clock
speed
The
Pentium
is
another
out
of
order
processor
with
a
very
deep
pipeline
to
achieve
extremely
high
clock
frequencies
It
started
with
stages
and
later
versions
adopted
stages
to
achieve
frequencies
greater
than
GHz
The
chip
shown
in
Figure
packs
in
to
million
transistors
depending
on
the
cache
size
so
even
the
major
execution
units
are
difficult
to
see
on
the
photograph
Decoding
three
IA
instructions
per
cycle
is
impossible
at
such
high
clock
frequencies
because
the
instruction
encodings
are
so
complex
and
irregular
Instead
the
processor
predecodes
the
instructions
into
simpler
micro
ops
then
stores
the
micro
ops
in
a
memory
called
a
trace
cache
Later
versions
of
the
Pentium
also
perform
multithreading
to
increase
the
throughput
of
multiple
threads
The
Pentium
s
reliance
on
deep
pipelines
and
high
clock
speed
led
to
extremely
high
power
consumption
sometimes
more
than
W
This
is
unacceptable
in
laptops
and
makes
cooling
of
desktops
expensive
Intel
discovered
that
the
older
P
architecture
could
achieve
comparable
performance
at
much
lower
clock
speed
and
power
The
Pentium
M
uses
an
enhanced
version
of
the
P
out
of
order
microarchitecture
with
KB
instruction
and
data
caches
and
a
to
MB
second
level
cache
The
Core
Duo
is
a
multicore
processor
based
on
two
Pentium
M
cores
connected
to
a
shared
MB
second
level
cache
The
individual
functional
units
in
Figure
are
difficult
to
see
but
the
two
cores
and
the
large
cache
are
clearly
visible
SUMMARY
This
chapter
has
described
three
ways
to
build
MIPS
processors
each
with
different
performance
and
cost
trade
offs
We
find
this
topic
almost
magical
how
can
such
a
seemingly
complicated
device
as
a
microprocessor
actually
be
simple
enough
to
fit
in
a
half
page
schematic
Moreover
the
inner
workings
so
mysterious
to
the
uninitiated
are
actually
reasonably
straightforward
The
MIPS
microarchitectures
have
drawn
together
almost
every
topic
covered
in
the
text
so
far
Piecing
together
the
microarchitecture
puzzle
illustrates
the
principles
introduced
in
previous
chapters
including
the
design
of
combinational
and
sequential
circuits
covered
in
Chapters
and
the
application
of
many
of
the
building
blocks
described
in
Chapter
and
the
implementation
of
the
MIPS
architecture
introduced
in
Chapter
The
MIPS
microarchitectures
can
be
described
in
a
few
pages
of
HDL
using
the
techniques
from
Chapter
Building
the
microarchitectures
has
also
heavily
used
our
techniques
for
managing
complexity
The
microarchitectural
abstraction
forms
the
link
between
the
logic
and
architecture
abstractions
forming
Summary
Core
Core
MB
Shared
Level
Cache
Figure
Core
Duo
microprocessor
chip
the
crux
of
this
book
on
digital
design
and
computer
architecture
We
also
use
the
abstractions
of
block
diagrams
and
HDL
to
succinctly
describe
the
arrangement
of
components
The
microarchitectures
exploit
regularity
and
modularity
reusing
a
library
of
common
building
blocks
such
as
ALUs
memories
multiplexers
and
registers
Hierarchy
is
used
in
numerous
ways
The
microarchitectures
are
partitioned
into
the
datapath
and
control
units
Each
of
these
units
is
built
from
logic
blocks
which
can
be
built
from
gates
which
in
turn
can
be
built
from
transistors
using
the
techniques
developed
in
the
first
five
chapters
This
chapter
has
compared
single
cycle
multicycle
and
pipelined
microarchitectures
for
the
MIPS
processor
All
three
microarchitectures
implement
the
same
subset
of
the
MIPS
instruction
set
and
have
the
same
architectural
state
The
single
cycle
processor
is
the
most
straightforward
and
has
a
CPI
of
The
multicycle
processor
uses
a
variable
number
of
shorter
steps
to
execute
instructions
It
thus
can
reuse
the
ALU
rather
than
requiring
several
adders
However
it
does
require
several
nonarchitectural
registers
to
store
results
between
steps
The
multicycle
design
in
principle
could
be
faster
because
not
all
instructions
must
be
equally
long
In
practice
it
is
generally
slower
because
it
is
limited
by
the
slowest
steps
and
by
the
sequencing
overhead
in
each
step
The
pipelined
processor
divides
the
single
cycle
processor
into
five
relatively
fast
pipeline
stages
It
adds
pipeline
registers
between
the
stages
to
separate
the
five
instructions
that
are
simultaneously
executing
It
nominally
has
a
CPI
of
but
hazards
force
stalls
or
flushes
that
increase
the
CPI
slightly
Hazard
resolution
also
costs
some
extra
hardware
and
design
complexity
The
clock
period
ideally
could
be
five
times
shorter
than
that
of
the
single
cycle
processor
In
practice
it
is
not
that
short
because
it
is
limited
by
the
slowest
stage
and
by
the
sequencing
overhead
in
each
stage
Nevertheless
pipelining
provides
substantial
performance
benefits
All
modern
high
performance
microprocessors
use
pipelining
today
Although
the
microarchitectures
in
this
chapter
implement
only
a
subset
of
the
MIPS
architecture
we
have
seen
that
supporting
more
instructions
involves
straightforward
enhancements
of
the
datapath
and
controller
Supporting
exceptions
also
requires
simple
modifications
A
major
limitation
of
this
chapter
is
that
we
have
assumed
an
ideal
memory
system
that
is
fast
and
large
enough
to
store
the
entire
program
and
data
In
reality
large
fast
memories
are
prohibitively
expensive
The
next
chapter
shows
how
to
get
most
of
the
benefits
of
a
large
fast
memory
with
a
small
fast
memory
that
holds
the
most
commonly
used
information
and
one
or
more
larger
but
slower
memories
that
hold
the
rest
of
the
information
CHAPTER
SEVEN
Microarchitecture
Exercises
Exercises
Exercise
Suppose
that
one
of
the
following
control
signals
in
the
single
cycle
MIPS
processor
has
a
stuck
at
fault
meaning
that
the
signal
is
always
regardless
of
its
intended
value
What
instructions
would
malfunction
Why
a
RegWrite
b
ALUOp
c
MemWrite
Exercise
Repeat
Exercise
assuming
that
the
signal
has
a
stuck
at
fault
Exercise
Modify
the
single
cycle
MIPS
processor
to
implement
one
of
the
following
instructions
See
Appendix
B
for
a
definition
of
the
instructions
Mark
up
a
copy
of
Figure
to
indicate
the
changes
to
the
datapath
Name
any
new
control
signals
Mark
up
a
copy
of
Table
to
show
the
changes
to
the
main
decoder
Describe
any
other
changes
that
are
required
a
sll
b
lui
c
slti
d
blez
e
jal
f
lh
Exercise
Many
processor
architectures
have
a
load
with
postincrement
instruction
which
updates
the
index
register
to
point
to
the
next
memory
word
after
completing
the
load
lwinc
rt
imm
rs
is
equivalent
to
the
following
two
instructions
lw
rt
imm
rs
addi
rs
rs
Table
Main
decoder
truth
table
to
mark
up
with
changes
Instruction
Opcode
RegWrite
RegDst
ALUSrc
Branch
MemWrite
MemtoReg
ALUOp
R
type
lw
sw
X
X
beq
X
X
Repeat
Exercise
for
the
lwinc
instruction
Is
it
possible
to
add
the
instruction
without
modifying
the
register
file
Exercise
Add
a
single
precision
floating
point
unit
to
the
single
cycle
MIPS
processor
to
handle
add
s
sub
s
and
mul
s
Assume
that
you
have
single
precision
floating
point
adder
and
multiplier
units
available
Explain
what
changes
must
be
made
to
the
datapath
and
the
controller
Exercise
Your
friend
is
a
crack
circuit
designer
She
has
offered
to
redesign
one
of
the
units
in
the
single
cycle
MIPS
processor
to
have
half
the
delay
Using
the
delays
from
Table
which
unit
should
she
work
on
to
obtain
the
greatest
speedup
of
the
overall
processor
and
what
would
the
cycle
time
of
the
improved
machine
be
Exercise
Consider
the
delays
given
in
Table
Ben
Bitdiddle
builds
a
prefix
adder
that
reduces
the
ALU
delay
by
ps
If
the
other
element
delays
stay
the
same
find
the
new
cycle
time
of
the
single
cycle
MIPS
processor
and
determine
how
long
it
takes
to
execute
a
benchmark
with
billion
instructions
Exercise
Suppose
one
of
the
following
control
signals
in
the
multicycle
MIPS
processor
has
a
stuck
at
fault
meaning
that
the
signal
is
always
regardless
of
its
intended
value
What
instructions
would
malfunction
Why
a
MemtoReg
b
ALUOp
c
PCSrc
Exercise
Repeat
Exercise
assuming
that
the
signal
has
a
stuck
at
fault
Exercise
Modify
the
HDL
code
for
the
single
cycle
MIPS
processor
given
in
Section
to
handle
one
of
the
new
instructions
from
Exercise
Enhance
the
testbench
given
in
Section
to
test
the
new
instruction
Exercise
Modify
the
multicycle
MIPS
processor
to
implement
one
of
the
following
instructions
See
Appendix
B
for
a
definition
of
the
instructions
Mark
up
a
copy
of
Figure
to
indicate
the
changes
to
the
datapath
Name
any
new
control
signals
Mark
up
a
copy
of
Figure
to
show
the
changes
to
the
controller
FSM
Describe
any
other
changes
that
are
required
a
srlv
b
ori
c
xori
d
jr
e
bne
f
lbu
CHAPTER
SEVEN
Microarchitecture
Exercise
Repeat
Exercise
for
the
multicycle
MIPS
processor
Show
the
changes
to
the
multicycle
datapath
and
control
FSM
Is
it
possible
to
add
the
instruction
without
modifying
the
register
file
Exercise
Repeat
Exercise
for
the
multicycle
MIPS
processor
Exercise
Suppose
that
the
floating
point
adder
and
multiplier
from
Exercise
each
take
two
cycles
to
operate
In
other
words
the
inputs
are
applied
at
the
beginning
of
one
cycle
and
the
output
is
available
in
the
second
cycle
How
does
your
answer
to
Exercise
change
Exercise
Your
friend
the
crack
circuit
designer
has
offered
to
redesign
one
of
the
units
in
the
multicycle
MIPS
processor
to
be
much
faster
Using
the
delays
from
Table
which
unit
should
she
work
on
to
obtain
the
greatest
speedup
of
the
overall
processor
How
fast
should
it
be
Making
it
faster
than
necessary
is
a
waste
of
your
friend
s
effort
What
is
the
cycle
time
of
the
improved
processor
Exercise
Repeat
Exercise
for
the
multicycle
processor
Exercise
Suppose
the
multicycle
MIPS
processor
has
the
component
delays
given
in
Table
Alyssa
P
Hacker
designs
a
new
register
file
that
has
less
power
but
twice
as
much
delay
Should
she
switch
to
the
slower
but
lower
power
register
file
for
her
multicycle
processor
design
Exercise
Goliath
Corp
claims
to
have
a
patent
on
a
three
ported
register
file
Rather
than
fighting
Goliath
in
court
Ben
Bitdiddle
designs
a
new
register
file
that
has
only
a
single
read
write
port
like
the
combined
instruction
and
data
memory
Redesign
the
MIPS
multicycle
datapath
and
controller
to
use
his
new
register
file
Exercise
What
is
the
CPI
of
the
redesigned
multicycle
MIPS
processor
from
Exercise
Use
the
instruction
mix
from
Example
Exercise
How
many
cycles
are
required
to
run
the
following
program
on
the
multicycle
MIPS
processor
What
is
the
CPI
of
this
program
addi
s
sum
while
beq
s
done
if
result
execute
the
while
block
addi
s
s
while
block
result
result
j
while
done
Exercises
Exercise
Repeat
Exercise
for
the
following
program
add
s
i
add
s
sum
addi
t
t
loop
slt
t
s
t
if
i
t
else
t
beq
t
done
if
t
i
branch
to
done
add
s
s
s
sum
sum
i
addi
s
s
increment
i
j
loop
done
Exercise
Write
HDL
code
for
the
multicycle
MIPS
processor
The
processor
should
be
compatible
with
the
following
top
level
module
The
mem
module
is
used
to
hold
both
instructions
and
data
Test
your
processor
using
the
testbench
from
Section
module
top
input
clk
reset
output
writedata
adr
output
memwrite
wire
readdata
instantiate
processor
and
memories
mips
mips
clk
reset
adr
writedata
memwrite
readdata
mem
mem
clk
memwrite
adr
writedata
readdata
endmodule
module
mem
input
clk
we
input
a
wd
output
rd
reg
RAM
initial
begin
readmemh
memfile
dat
RAM
end
assign
rd
RAM
a
word
aligned
always
posedge
clk
if
we
RAM
a
wd
endmodule
Exercise
Extend
your
HDL
code
for
the
multicycle
MIPS
processor
from
Exercise
to
handle
one
of
the
new
instructions
from
Exercise
Enhance
the
testbench
to
test
the
new
instruction
CHAPTER
SEVEN
Microarchitecture
Exercise
The
pipelined
MIPS
processor
is
running
the
following
program
Which
registers
are
being
written
and
which
are
being
read
on
the
fifth
cycle
add
s
t
t
sub
s
t
t
and
s
s
s
or
s
t
t
slt
s
s
s
Exercise
Using
a
diagram
similar
to
Figure
show
the
forwarding
and
stalls
needed
to
execute
the
following
instructions
on
the
pipelined
MIPS
processor
add
t
s
s
sub
t
t
s
lw
t
t
and
t
t
t
Exercise
Repeat
Exercise
for
the
following
instructions
add
t
s
s
lw
t
s
sub
t
t
s
and
t
t
t
Exercise
How
many
cycles
are
required
for
the
pipelined
MIPS
processor
to
issue
all
of
the
instructions
for
the
program
in
Exercise
What
is
the
CPI
of
the
processor
on
this
program
Exercise
Explain
how
to
extend
the
pipelined
MIPS
processor
to
handle
the
addi
instruction
Exercise
Explain
how
to
extend
the
pipelined
processor
to
handle
the
j
instruction
Give
particular
attention
to
how
the
pipeline
is
flushed
when
a
jump
takes
place
Exercise
Examples
and
point
out
that
the
pipelined
MIPS
processor
performance
might
be
better
if
branches
take
place
during
the
Execute
stage
rather
than
the
Decode
stage
Show
how
to
modify
the
pipelined
processor
from
Figure
to
branch
in
the
Execute
stage
How
do
the
stall
and
flush
signals
change
Redo
Examples
and
to
find
the
new
CPI
cycle
time
and
overall
time
to
execute
the
program
Exercise
Your
friend
the
crack
circuit
designer
has
offered
to
redesign
one
of
the
units
in
the
pipelined
MIPS
processor
to
be
much
faster
Using
the
delays
from
Table
and
Example
which
unit
should
she
work
on
to
obtain
the
greatest
speedup
of
the
overall
processor
How
fast
should
it
be
Making
it
faster
than
necessary
is
a
waste
of
your
friend
s
effort
What
is
the
cycle
time
of
the
improved
processor
Exercises
Exercise
Consider
the
delays
from
Table
and
Example
Now
suppose
that
the
ALU
were
faster
Would
the
cycle
time
of
the
pipelined
MIPS
processor
change
What
if
the
ALU
were
slower
Exercise
Write
HDL
code
for
the
pipelined
MIPS
processor
The
processor
should
be
compatible
with
the
top
level
module
from
HDL
Example
It
should
support
all
of
the
instructions
described
in
this
chapter
including
addi
and
j
see
Exercises
and
Test
your
design
using
the
testbench
from
HDL
Example
Exercise
Design
the
hazard
unit
shown
in
Figure
for
the
pipelined
MIPS
processor
Use
an
HDL
to
implement
your
design
Sketch
the
hardware
that
a
synthesis
tool
might
generate
from
your
HDL
Exercise
A
nonmaskable
interrupt
NMI
is
triggered
by
an
input
pin
to
the
processor
When
the
pin
is
asserted
the
current
instruction
should
finish
then
the
processor
should
set
the
Cause
register
to
and
take
an
exception
Show
how
to
modify
the
multicycle
processor
in
Figures
and
to
handle
nonmaskable
interrupts
CHAPTER
SEVEN
Microarchitecture
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
at
interviews
for
digital
design
jobs
Question
Explain
the
advantages
of
pipelined
microprocessors
Question
If
additional
pipeline
stages
allow
a
processor
to
go
faster
why
don
t
processors
have
pipeline
stages
Question
Describe
what
a
hazard
is
in
a
microprocessor
and
explain
ways
in
which
it
can
be
resolved
What
are
the
pros
and
cons
of
each
way
Question
Describe
the
concept
of
a
superscalar
processor
and
its
pros
and
cons
Interview
Questions
Introduction
Memory
System
Performance
Analysis
Caches
Virtual
Memory
Memory
Mapped
I
O
Real
World
Perspective
IA
Memory
and
I
O
Systems
Summary
Exercises
Interview
Questions
Memory
Systems
INTRODUCTION
Computer
system
performance
depends
on
the
memory
system
as
well
as
the
processor
microarchitecture
Chapter
assumed
an
ideal
memory
system
that
could
be
accessed
in
a
single
clock
cycle
However
this
would
be
true
only
for
a
very
small
memory
or
a
very
slow
processor
Early
processors
were
relatively
slow
so
memory
was
able
to
keep
up
But
processor
speed
has
increased
at
a
faster
rate
than
memory
speeds
DRAM
memories
are
currently
to
times
slower
than
processors
The
increasing
gap
between
processor
and
DRAM
memory
speeds
demands
increasingly
ingenious
memory
systems
to
try
to
approximate
a
memory
that
is
as
fast
as
the
processor
This
chapter
investigates
practical
memory
systems
and
considers
trade
offs
of
speed
capacity
and
cost
The
processor
communicates
with
the
memory
system
over
a
memory
interface
Figure
shows
the
simple
memory
interface
used
in
our
multicycle
MIPS
processor
The
processor
sends
an
address
over
the
Address
bus
to
the
memory
system
For
a
read
MemWrite
is
and
the
memory
returns
the
data
on
the
ReadData
bus
For
a
write
MemWrite
is
and
the
processor
sends
data
to
memory
on
the
WriteData
bus
The
major
issues
in
memory
system
design
can
be
broadly
explained
using
a
metaphor
of
books
in
a
library
A
library
contains
many
books
on
the
shelves
If
you
were
writing
a
term
paper
on
the
meaning
of
dreams
you
might
go
to
the
library
and
pull
Freud
s
The
Interpretation
of
Dreams
off
the
shelf
and
bring
it
to
your
cubicle
After
skimming
it
you
might
put
it
back
and
pull
out
Jung
s
The
Psychology
of
the
We
realize
that
library
usage
is
plummeting
among
college
students
because
of
the
Internet
But
we
also
believe
that
libraries
contain
vast
troves
of
hard
won
human
knowledge
that
are
not
electronically
available
We
hope
that
Web
searching
does
not
completely
displace
the
art
of
library
research
Unconscious
You
might
then
go
back
for
another
quote
from
Interpretation
of
Dreams
followed
by
yet
another
trip
to
the
stacks
for
Freud
s
The
Ego
and
the
Id
Pretty
soon
you
would
get
tired
of
walking
from
your
cubicle
to
the
stacks
If
you
are
clever
you
would
save
time
by
keeping
the
books
in
your
cubicle
rather
than
schlepping
them
back
and
forth
Furthermore
when
you
pull
a
book
by
Freud
you
could
also
pull
several
of
his
other
books
from
the
same
shelf
This
metaphor
emphasizes
the
principle
introduced
in
Section
of
making
the
common
case
fast
By
keeping
books
that
you
have
recently
used
or
might
likely
use
in
the
future
at
your
cubicle
you
reduce
the
number
of
time
consuming
trips
to
the
stacks
In
particular
you
use
the
principles
of
temporal
and
spatial
locality
Temporal
locality
means
that
if
you
have
used
a
book
recently
you
are
likely
to
use
it
again
soon
Spatial
locality
means
that
when
you
use
one
particular
book
you
are
likely
to
be
interested
in
other
books
on
the
same
shelf
The
library
itself
makes
the
common
case
fast
by
using
these
principles
of
locality
The
library
has
neither
the
shelf
space
nor
the
budget
to
accommodate
all
of
the
books
in
the
world
Instead
it
keeps
some
of
the
lesser
used
books
in
deep
storage
in
the
basement
Also
it
may
have
an
interlibrary
loan
agreement
with
nearby
libraries
so
that
it
can
offer
more
books
than
it
physically
carries
In
summary
you
obtain
the
benefits
of
both
a
large
collection
and
quick
access
to
the
most
commonly
used
books
through
a
hierarchy
of
storage
The
most
commonly
used
books
are
in
your
cubicle
A
larger
collection
is
on
the
shelves
And
an
even
larger
collection
is
available
with
advanced
notice
from
the
basement
and
other
libraries
Similarly
memory
systems
use
a
hierarchy
of
storage
to
quickly
access
the
most
commonly
used
data
while
still
having
the
capacity
to
store
large
amounts
of
data
Memory
subsystems
used
to
build
this
hierarchy
were
introduced
in
Section
Computer
memories
are
primarily
built
from
dynamic
RAM
DRAM
and
static
RAM
SRAM
Ideally
the
computer
memory
system
is
fast
large
and
cheap
In
practice
a
single
memory
only
has
two
of
these
three
attributes
it
is
either
slow
small
or
expensive
But
computer
systems
can
approximate
the
ideal
by
combining
a
fast
small
cheap
memory
and
a
slow
large
cheap
memory
The
fast
memory
stores
the
most
commonly
used
data
and
instructions
so
on
average
the
memory
CHAPTER
EIGHT
Memory
Systems
Processor
Memory
Address
MemWrite
WriteData
ReadData
WE
CLK
Figure
The
memory
interface
system
appears
fast
The
large
memory
stores
the
remainder
of
the
data
and
instructions
so
the
overall
capacity
is
large
The
combination
of
two
cheap
memories
is
much
less
expensive
than
a
single
large
fast
memory
These
principles
extend
to
using
an
entire
hierarchy
of
memories
of
increasing
capacity
and
decreasing
speed
Computer
memory
is
generally
built
from
DRAM
chips
In
a
typical
PC
had
a
main
memory
consisting
of
MB
to
GB
of
DRAM
and
DRAM
cost
about
per
gigabyte
GB
DRAM
prices
have
declined
at
about
per
year
for
the
last
three
decades
and
memory
capacity
has
grown
at
the
same
rate
so
the
total
cost
of
the
memory
in
a
PC
has
remained
roughly
constant
Unfortunately
DRAM
speed
has
improved
by
only
about
per
year
whereas
processor
performance
has
improved
at
a
rate
of
to
per
year
as
shown
in
Figure
The
plot
shows
memory
and
processor
speeds
with
the
speeds
as
a
baseline
In
about
processor
and
memory
speeds
were
the
same
But
performance
has
diverged
since
then
with
memories
badly
lagging
DRAM
could
keep
up
with
processors
in
the
s
and
early
s
but
it
is
now
woefully
too
slow
The
DRAM
access
time
is
one
to
two
orders
of
magnitude
longer
than
the
processor
cycle
time
tens
of
nanoseconds
compared
to
less
than
one
nanosecond
To
counteract
this
trend
computers
store
the
most
commonly
used
instructions
and
data
in
a
faster
but
smaller
memory
called
a
cache
The
cache
is
usually
built
out
of
SRAM
on
the
same
chip
as
the
processor
The
cache
speed
is
comparable
to
the
processor
speed
because
SRAM
is
inherently
faster
than
DRAM
and
because
the
on
chip
memory
eliminates
lengthy
delays
caused
by
traveling
to
and
from
a
separate
chip
In
on
chip
SRAM
costs
were
on
the
order
of
GB
but
the
cache
is
relatively
small
kilobytes
to
a
few
megabytes
so
the
overall
Introduction
Year
CPU
Performance
Memory
Figure
Diverging
processor
and
memory
performance
Adapted
with
permission
from
Hennessy
and
Patterson
Computer
Architecture
A
Quantitative
Approach
rd
ed
Morgan
Kaufmann
cost
is
low
Caches
can
store
both
instructions
and
data
but
we
will
refer
to
their
contents
generically
as
data
If
the
processor
requests
data
that
is
available
in
the
cache
it
is
returned
quickly
This
is
called
a
cache
hit
Otherwise
the
processor
retrieves
the
data
from
main
memory
DRAM
This
is
called
a
cache
miss
If
the
cache
hits
most
of
the
time
then
the
processor
seldom
has
to
wait
for
the
slow
main
memory
and
the
average
access
time
is
low
The
third
level
in
the
memory
hierarchy
is
the
hard
disk
or
hard
drive
In
the
same
way
that
a
library
uses
the
basement
to
store
books
that
do
not
fit
in
the
stacks
computer
systems
use
the
hard
disk
to
store
data
that
does
not
fit
in
main
memory
In
a
hard
disk
cost
less
than
GB
and
had
an
access
time
of
about
ms
Hard
disk
costs
have
decreased
at
year
but
access
times
scarcely
improved
The
hard
disk
provides
an
illusion
of
more
capacity
than
actually
exists
in
the
main
memory
It
is
thus
called
virtual
memory
Like
books
in
the
basement
data
in
virtual
memory
takes
a
long
time
to
access
Main
memory
also
called
physical
memory
holds
a
subset
of
the
virtual
memory
Hence
the
main
memory
can
be
viewed
as
a
cache
for
the
most
commonly
used
data
from
the
hard
disk
Figure
summarizes
the
memory
hierarchy
of
the
computer
system
discussed
in
the
rest
of
this
chapter
The
processor
first
seeks
data
in
a
small
but
fast
cache
that
is
usually
located
on
the
same
chip
If
the
data
is
not
available
in
the
cache
the
processor
then
looks
in
main
memory
If
the
data
is
not
there
either
the
processor
fetches
the
data
from
virtual
memory
on
the
large
but
slow
hard
disk
Figure
illustrates
this
capacity
and
speed
trade
off
in
the
memory
hierarchy
and
lists
typical
costs
and
access
times
in
technology
As
access
time
decreases
speed
increases
Section
introduces
memory
system
performance
analysis
Section
explores
several
cache
organizations
and
Section
delves
into
virtual
memory
systems
To
conclude
this
chapter
explores
how
processors
can
access
input
and
output
devices
such
as
keyboards
and
monitors
in
much
the
same
way
as
they
access
memory
Section
investigates
such
memory
mapped
I
O
CHAPTER
EIGHT
Memory
Systems
CPU
Cache
Main
Memory
Processor
Chip
CLK
Hard
Disk
Figure
A
typical
memory
hierarchy
MEMORY
SYSTEM
PERFORMANCE
ANALYSIS
Designers
and
computer
buyers
need
quantitative
ways
to
measure
the
performance
of
memory
systems
to
evaluate
the
cost
benefit
trade
offs
of
various
alternatives
Memory
system
performance
metrics
are
miss
rate
or
hit
rate
and
average
memory
access
time
Miss
and
hit
rates
are
calculated
as
Number
of
misses
Miss
Rate
Hit
Rate
Number
of
total
memory
accesses
Number
of
hits
Hit
Rate
Miss
Rate
Number
of
total
memory
accesses
Example
CALCULATING
CACHE
PERFORMANCE
Suppose
a
program
has
data
access
instructions
loads
or
stores
and
of
these
requested
data
values
are
found
in
the
cache
The
other
data
values
are
supplied
to
the
processor
by
main
memory
or
disk
memory
What
are
the
miss
and
hit
rates
for
the
cache
Solution
The
miss
rate
is
The
hit
rate
is
Average
memory
access
time
AMAT
is
the
average
time
a
processor
must
wait
for
memory
per
load
or
store
instruction
In
the
typical
computer
system
from
Figure
the
processor
first
looks
for
the
data
in
the
cache
If
the
cache
misses
the
processor
then
looks
in
main
memory
If
the
main
memory
misses
the
processor
accesses
virtual
memory
on
the
hard
disk
Thus
AMAT
is
calculated
as
AMAT
tcache
MRcache
tMM
MRMMtVM
Memory
System
Performance
Analysis
Cache
Main
Memory
Speed
Virtual
Memory
Capacity
Technology
Cost
GB
Access
Time
SRAM
ns
DRAM
ns
Hard
Disk
ns
Figure
Memory
hierarchy
components
with
typical
characteristics
in
Chapter
qx
where
tcache
tMM
and
tVM
are
the
access
times
of
the
cache
main
memory
and
virtual
memory
and
MRcache
and
MRMM
are
the
cache
and
main
memory
miss
rates
respectively
Example
CALCULATING
AVERAGE
MEMORY
ACCESS
TIME
Suppose
a
computer
system
has
a
memory
organization
with
only
two
levels
of
hierarchy
a
cache
and
main
memory
What
is
the
average
memory
access
time
given
the
access
times
and
miss
rates
given
in
Table
Solution
The
average
memory
access
time
is
cycles
CHAPTER
EIGHT
Memory
Systems
Table
Access
times
and
miss
rates
Memory
Access
Time
Miss
Level
Cycles
Rate
Cache
Main
Memory
Gene
Amdahl
Most
famous
for
Amdahl
s
Law
an
observation
he
made
in
While
in
graduate
school
he
began
designing
computers
in
his
free
time
This
side
work
earned
him
his
Ph
D
in
theoretical
physics
in
He
joined
IBM
immediately
after
graduation
and
later
went
on
to
found
three
companies
including
one
called
Amdahl
Corporation
in
Example
IMPROVING
ACCESS
TIME
An
cycle
average
memory
access
time
means
that
the
processor
spends
ten
cycles
waiting
for
data
for
every
one
cycle
actually
using
that
data
What
cache
miss
rate
is
needed
to
reduce
the
average
memory
access
time
to
cycles
given
the
access
times
in
Table
Solution
If
the
miss
rate
is
m
the
average
access
time
is
m
Setting
this
time
to
and
solving
for
m
requires
a
cache
miss
rate
of
As
a
word
of
caution
performance
improvements
might
not
always
be
as
good
as
they
sound
For
example
making
the
memory
system
ten
times
faster
will
not
necessarily
make
a
computer
program
run
ten
times
as
fast
If
of
a
program
s
instructions
are
loads
and
stores
a
tenfold
memory
system
improvement
only
means
a
fold
improvement
in
program
performance
This
general
principle
is
called
Amdahl
s
Law
which
says
that
the
effort
spent
on
increasing
the
performance
of
a
subsystem
is
worthwhile
only
if
the
subsystem
affects
a
large
percentage
of
the
overall
performance
CACHES
A
cache
holds
commonly
used
memory
data
The
number
of
data
words
that
it
can
hold
is
called
the
capacity
C
Because
the
capacity
C
of
the
cache
is
smaller
than
that
of
main
memory
the
computer
system
designer
must
choose
what
subset
of
the
main
memory
is
kept
in
the
cache
When
the
processor
attempts
to
access
data
it
first
checks
the
cache
for
the
data
If
the
cache
hits
the
data
is
available
immediately
If
the
cache
misses
the
processor
fetches
the
data
from
main
memory
and
places
it
in
the
cache
for
future
use
To
accommodate
the
new
data
the
cache
must
replace
old
data
This
section
investigates
these
issues
in
cache
design
by
answering
the
following
questions
What
data
is
held
in
the
cache
How
is
the
data
found
and
What
data
is
replaced
to
make
room
for
new
data
when
the
cache
is
full
When
reading
the
next
sections
keep
in
mind
that
the
driving
force
in
answering
these
questions
is
the
inherent
spatial
and
temporal
locality
of
data
accesses
in
most
applications
Caches
use
spatial
and
temporal
locality
to
predict
what
data
will
be
needed
next
If
a
program
accesses
data
in
a
random
order
it
would
not
benefit
from
a
cache
As
we
explain
in
the
following
sections
caches
are
specified
by
their
capacity
C
number
of
sets
S
block
size
b
number
of
blocks
B
and
degree
of
associativity
N
Although
we
focus
on
data
cache
loads
the
same
principles
apply
for
fetches
from
an
instruction
cache
Data
cache
store
operations
are
similar
and
are
discussed
further
in
Section
What
Data
Is
Held
in
the
Cache
An
ideal
cache
would
anticipate
all
of
the
data
needed
by
the
processor
and
fetch
it
from
main
memory
ahead
of
time
so
that
the
cache
has
a
zero
miss
rate
Because
it
is
impossible
to
predict
the
future
with
perfect
accuracy
the
cache
must
guess
what
data
will
be
needed
based
on
the
past
pattern
of
memory
accesses
In
particular
the
cache
exploits
temporal
and
spatial
locality
to
achieve
a
low
miss
rate
Recall
that
temporal
locality
means
that
the
processor
is
likely
to
access
a
piece
of
data
again
soon
if
it
has
accessed
that
data
recently
Therefore
when
the
processor
loads
or
stores
data
that
is
not
in
the
cache
the
data
is
copied
from
main
memory
into
the
cache
Subsequent
requests
for
that
data
hit
in
the
cache
Recall
that
spatial
locality
means
that
when
the
processor
accesses
a
piece
of
data
it
is
also
likely
to
access
data
in
nearby
memory
locations
Therefore
when
the
cache
fetches
one
word
from
memory
it
may
also
fetch
several
adjacent
words
This
group
of
words
is
called
a
cache
block
The
number
of
words
in
the
cache
block
b
is
called
the
block
size
A
cache
of
capacity
C
contains
B
C
b
blocks
The
principles
of
temporal
and
spatial
locality
have
been
experimentally
verified
in
real
programs
If
a
variable
is
used
in
a
program
the
Caches
Cache
a
hiding
place
especially
for
concealing
and
preserving
provisions
or
implements
Merriam
Webster
Online
Dictionary
http
www
merriam
webster
com
C
same
variable
is
likely
to
be
used
again
creating
temporal
locality
If
an
element
in
an
array
is
used
other
elements
in
the
same
array
are
also
likely
to
be
used
creating
spatial
locality
How
Is
the
Data
Found
A
cache
is
organized
into
S
sets
each
of
which
holds
one
or
more
blocks
of
data
The
relationship
between
the
address
of
data
in
main
memory
and
the
location
of
that
data
in
the
cache
is
called
the
mapping
Each
memory
address
maps
to
exactly
one
set
in
the
cache
Some
of
the
address
bits
are
used
to
determine
which
cache
set
contains
the
data
If
the
set
contains
more
than
one
block
the
data
may
be
kept
in
any
of
the
blocks
in
the
set
Caches
are
categorized
based
on
the
number
of
blocks
in
a
set
In
a
direct
mapped
cache
each
set
contains
exactly
one
block
so
the
cache
has
S
B
sets
Thus
a
particular
main
memory
address
maps
to
a
unique
block
in
the
cache
In
an
N
way
set
associative
cache
each
set
contains
N
blocks
The
address
still
maps
to
a
unique
set
with
S
B
N
sets
But
the
data
from
that
address
can
go
in
any
of
the
N
blocks
in
that
set
A
fully
associative
cache
has
only
S
set
Data
can
go
in
any
of
the
B
blocks
in
the
set
Hence
a
fully
associative
cache
is
another
name
for
a
B
way
set
associative
cache
To
illustrate
these
cache
organizations
we
will
consider
a
MIPS
memory
system
with
bit
addresses
and
bit
words
The
memory
is
byte
addressable
and
each
word
is
four
bytes
so
the
memory
consists
of
words
aligned
on
word
boundaries
We
analyze
caches
with
an
eightword
capacity
C
for
the
sake
of
simplicity
We
begin
with
a
one
word
block
size
b
then
generalize
later
to
larger
blocks
Direct
Mapped
Cache
A
direct
mapped
cache
has
one
block
in
each
set
so
it
is
organized
into
S
B
sets
To
understand
the
mapping
of
memory
addresses
onto
cache
blocks
imagine
main
memory
as
being
mapped
into
b
word
blocks
just
as
the
cache
is
An
address
in
block
of
main
memory
maps
to
set
of
the
cache
An
address
in
block
of
main
memory
maps
to
set
of
the
cache
and
so
forth
until
an
address
in
block
B
of
main
memory
maps
to
block
B
of
the
cache
There
are
no
more
blocks
of
the
cache
so
the
mapping
wraps
around
such
that
block
B
of
main
memory
maps
to
block
of
the
cache
This
mapping
is
illustrated
in
Figure
for
a
direct
mapped
cache
with
a
capacity
of
eight
words
and
a
block
size
of
one
word
The
cache
has
eight
sets
each
of
which
contains
a
one
word
block
The
bottom
two
bits
of
the
address
are
always
because
they
are
word
aligned
The
next
log
bits
indicate
the
set
onto
which
the
memory
address
maps
Thus
the
data
at
addresses
x
x
CHAPTER
EIGHT
Memory
Systems
Chapter
xFFFFFFE
all
map
to
set
as
shown
in
blue
Likewise
data
at
addresses
x
xFFFFFFF
all
map
to
set
and
so
forth
Each
main
memory
address
maps
to
exactly
one
set
in
the
cache
Example
CACHE
FIELDS
To
what
cache
set
in
Figure
does
the
word
at
address
x
map
Name
another
address
that
maps
to
the
same
set
Solution
The
two
least
significant
bits
of
the
address
are
because
the
address
is
word
aligned
The
next
three
bits
are
so
the
word
maps
to
set
Words
at
addresses
x
x
x
xFFFFFFF
all
map
to
this
same
set
Because
many
addresses
map
to
a
single
set
the
cache
must
also
keep
track
of
the
address
of
the
data
actually
contained
in
each
set
The
least
significant
bits
of
the
address
specify
which
set
holds
the
data
The
remaining
most
significant
bits
are
called
the
tag
and
indicate
which
of
the
many
possible
addresses
is
held
in
that
set
In
our
previous
example
the
two
least
significant
bits
of
the
bit
address
are
called
the
byte
offset
because
they
indicate
the
byte
within
the
word
The
next
three
bits
are
called
the
set
bits
because
they
indicate
the
set
to
which
the
address
maps
In
general
the
number
of
set
bits
is
log
S
The
remaining
tag
bits
indicate
the
memory
address
of
the
data
stored
in
a
given
cache
set
Figure
shows
the
cache
fields
for
address
xFFFFFFE
It
maps
to
set
and
its
tag
is
all
s
Caches
Word
Main
Memory
mem
x
mem
x
mem
x
mem
x
C
mem
x
mem
x
mem
x
mem
x
C
mem
x
mem
x
mem
xFFFFFFE
mem
xFFFFFFE
mem
xFFFFFFE
mem
xFFFFFFEC
mem
xFFFFFFF
mem
xFFFFFFF
mem
xFFFFFFF
mem
xFFFFFFFC
Word
Cache
Address
Set
Set
Set
Set
Set
Set
Set
Set
Data
Figure
Mapping
of
main
memory
to
a
direct
mapped
cache
Example
CACHE
FIELDS
Find
the
number
of
set
and
tag
bits
for
a
direct
mapped
cache
with
sets
and
a
one
word
block
size
The
address
size
is
bits
Solution
A
cache
with
sets
requires
log
set
bits
The
two
least
significant
bits
of
the
address
are
the
byte
offset
and
the
remaining
bits
form
the
tag
Sometimes
such
as
when
the
computer
first
starts
up
the
cache
sets
contain
no
data
at
all
The
cache
uses
a
valid
bit
for
each
set
to
indicate
whether
the
set
holds
meaningful
data
If
the
valid
bit
is
the
contents
are
meaningless
Figure
shows
the
hardware
for
the
direct
mapped
cache
of
Figure
The
cache
is
constructed
as
an
eight
entry
SRAM
Each
entry
or
set
contains
one
line
consisting
of
bits
of
data
bits
of
tag
and
valid
bit
The
cache
is
accessed
using
the
bit
address
The
two
least
significant
bits
the
byte
offset
bits
are
ignored
for
word
accesses
The
next
three
bits
the
set
bits
specify
the
entry
or
set
in
the
cache
A
load
instruction
reads
the
specified
entry
from
the
cache
and
checks
the
tag
and
valid
bits
If
the
tag
matches
the
most
significant
CHAPTER
EIGHT
Memory
Systems
Tag
Set
Byte
Offset
Memory
Address
FFFFFF
E
Figure
Cache
fields
for
address
xFFFFFFE
when
mapping
to
the
cache
in
Figure
Tag
Data
Tag
Set
Byte
Offset
Memory
Address
Hit
Data
V
entry
x
bit
SRAM
Set
Set
Set
Set
Set
Set
Set
Set
Figure
Direct
mapped
cache
with
sets
Chap
bits
of
the
address
and
the
valid
bit
is
the
cache
hits
and
the
data
is
returned
to
the
processor
Otherwise
the
cache
misses
and
the
memory
system
must
fetch
the
data
from
main
memory
Example
TEMPORAL
LOCALITY
WITH
A
DIRECT
MAPPED
CACHE
Loops
are
a
common
source
of
temporal
and
spatial
locality
in
applications
Using
the
eight
entry
cache
of
Figure
show
the
contents
of
the
cache
after
executing
the
following
silly
loop
in
MIPS
assembly
code
Assume
that
the
cache
is
initially
empty
What
is
the
miss
rate
addi
t
loop
beq
t
done
lw
t
x
lw
t
xC
lw
t
x
addi
t
t
j
loop
done
Solution
The
program
contains
a
loop
that
repeats
for
five
iterations
Each
iteration
involves
three
memory
accesses
loads
resulting
in
total
memory
accesses
The
first
time
the
loop
executes
the
cache
is
empty
and
the
data
must
be
fetched
from
main
memory
locations
x
xC
and
x
into
cache
sets
and
respectively
However
the
next
four
times
the
loop
executes
the
data
is
found
in
the
cache
Figure
shows
the
contents
of
the
cache
during
the
last
request
to
memory
address
x
The
tags
are
all
because
the
upper
bits
of
the
addresses
are
The
miss
rate
is
Caches
V
Tag
Data
mem
x
Tag
Set
Byte
Offset
Memory
Address
V
mem
x
C
mem
x
Set
Set
Set
Set
Set
Set
Set
Set
Figure
Direct
mapped
cache
contents
When
two
recently
accessed
addresses
map
to
the
same
cache
block
a
conflict
occurs
and
the
most
recently
accessed
address
evicts
the
previous
one
from
the
block
Direct
mapped
caches
have
only
one
block
in
each
set
so
two
addresses
that
map
to
the
same
set
always
cause
a
conflict
The
example
on
the
next
page
illustrates
conflicts
Ch
Example
CACHE
BLOCK
CONFLICT
What
is
the
miss
rate
when
the
following
loop
is
executed
on
the
eight
word
direct
mapped
cache
from
Figure
Assume
that
the
cache
is
initially
empty
addi
t
loop
beq
t
done
lw
t
x
lw
t
x
addi
t
t
j
loop
done
Solution
Memory
addresses
x
and
x
both
map
to
set
During
the
initial
execution
of
the
loop
data
at
address
x
is
loaded
into
set
of
the
cache
Then
data
at
address
x
is
loaded
into
set
evicting
the
data
from
address
x
Upon
the
second
execution
of
the
loop
the
pattern
repeats
and
the
cache
must
refetch
data
at
address
x
evicting
data
from
address
x
The
two
addresses
conflict
and
the
miss
rate
is
Multi
way
Set
Associative
Cache
An
N
way
set
associative
cache
reduces
conflicts
by
providing
N
blocks
in
each
set
where
data
mapping
to
that
set
might
be
found
Each
memory
address
still
maps
to
a
specific
set
but
it
can
map
to
any
one
of
the
N
blocks
in
the
set
Hence
a
direct
mapped
cache
is
another
name
for
a
one
way
set
associative
cache
N
is
also
called
the
degree
of
associativity
of
the
cache
Figure
shows
the
hardware
for
a
C
word
N
way
set
associative
cache
The
cache
now
has
only
S
sets
rather
than
CHAPTER
EIGHT
Memory
Systems
Tag
Data
Tag
Set
Byte
Offset
Memory
Address
Data
Hit
V
V
Tag
Data
Hit
Hit
Hit
Way
Way
Set
Set
Set
Set
Figure
Two
way
set
associative
cache
Chap
Thus
only
log
set
bits
rather
than
are
used
to
select
the
set
The
tag
increases
from
to
bits
Each
set
contains
two
ways
or
degrees
of
associativity
Each
way
consists
of
a
data
block
and
the
valid
and
tag
bits
The
cache
reads
blocks
from
both
ways
in
the
selected
set
and
checks
the
tags
and
valid
bits
for
a
hit
If
a
hit
occurs
in
one
of
the
ways
a
multiplexer
selects
data
from
that
way
Set
associative
caches
generally
have
lower
miss
rates
than
direct
mapped
caches
of
the
same
capacity
because
they
have
fewer
conflicts
However
set
associative
caches
are
usually
slower
and
somewhat
more
expensive
to
build
because
of
the
output
multiplexer
and
additional
comparators
They
also
raise
the
question
of
which
way
to
replace
when
both
ways
are
full
this
is
addressed
further
in
Section
Most
commercial
systems
use
set
associative
caches
Example
SET
ASSOCIATIVE
CACHE
MISS
RATE
Repeat
Example
using
the
eight
word
two
way
set
associative
cache
from
Figure
Solution
Both
memory
accesses
to
addresses
x
and
x
map
to
set
However
the
cache
has
two
ways
so
it
can
accommodate
data
from
both
addresses
During
the
first
loop
iteration
the
empty
cache
misses
both
addresses
and
loads
both
words
of
data
into
the
two
ways
of
set
as
shown
in
Figure
On
the
next
four
iterations
the
cache
hits
Hence
the
miss
rate
is
Recall
that
the
direct
mapped
cache
of
the
same
size
from
Example
had
a
miss
rate
of
Caches
V
Tag
Data
V
Tag
Data
mem
x
mem
x
Way
Way
Set
Set
Set
Set
Figure
Two
way
set
associative
cache
contents
Fully
Associative
Cache
A
fully
associative
cache
contains
a
single
set
with
B
ways
where
B
is
the
number
of
blocks
A
memory
address
can
map
to
a
block
in
any
of
these
ways
A
fully
associative
cache
is
another
name
for
a
B
way
set
associative
cache
with
one
set
Figure
shows
the
SRAM
array
of
a
fully
associative
cache
with
eight
blocks
Upon
a
data
request
eight
tag
comparisons
not
shown
must
be
made
because
the
data
could
be
in
any
block
Similarly
an
multiplexer
chooses
the
proper
data
if
a
hit
occurs
Fully
associative
caches
tend
Ch
to
have
the
fewest
conflict
misses
for
a
given
cache
capacity
but
they
require
more
hardware
for
additional
tag
comparisons
They
are
best
suited
to
relatively
small
caches
because
of
the
large
number
of
comparators
Block
Size
The
previous
examples
were
able
to
take
advantage
only
of
temporal
locality
because
the
block
size
was
one
word
To
exploit
spatial
locality
a
cache
uses
larger
blocks
to
hold
several
consecutive
words
The
advantage
of
a
block
size
greater
than
one
is
that
when
a
miss
occurs
and
the
word
is
fetched
into
the
cache
the
adjacent
words
in
the
block
are
also
fetched
Therefore
subsequent
accesses
are
more
likely
to
hit
because
of
spatial
locality
However
a
large
block
size
means
that
a
fixed
size
cache
will
have
fewer
blocks
This
may
lead
to
more
conflicts
increasing
the
miss
rate
Moreover
it
takes
more
time
to
fetch
the
missing
cache
block
after
a
miss
because
more
than
one
data
word
is
fetched
from
main
memory
The
time
required
to
load
the
missing
block
into
the
cache
is
called
the
miss
penalty
If
the
adjacent
words
in
the
block
are
not
accessed
later
the
effort
of
fetching
them
is
wasted
Nevertheless
most
real
programs
benefit
from
larger
block
sizes
Figure
shows
the
hardware
for
a
C
word
direct
mapped
cache
with
a
b
word
block
size
The
cache
now
has
only
B
C
b
blocks
A
direct
mapped
cache
has
one
block
in
each
set
so
this
cache
is
CHAPTER
EIGHT
Memory
Systems
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
V
Tag
Data
Way
Way
Way
Way
Way
Way
Way
Way
Tag
Data
Tag
Byte
Offset
Memory
Address
Data
V
Block
Offset
Hit
Set
Set
Set
Figure
Eight
block
fully
associative
cache
Figure
Direct
mapped
cache
with
two
sets
and
a
four
word
block
size
Chap
organized
as
two
sets
Thus
only
log
bit
is
used
to
select
the
set
A
multiplexer
is
now
needed
to
select
the
word
within
the
block
The
multiplexer
is
controlled
by
the
log
block
offset
bits
of
the
address
The
most
significant
address
bits
form
the
tag
Only
one
tag
is
needed
for
the
entire
block
because
the
words
in
the
block
are
at
consecutive
addresses
Figure
shows
the
cache
fields
for
address
x
C
when
it
maps
to
the
direct
mapped
cache
of
Figure
The
byte
offset
bits
are
always
for
word
accesses
The
next
log
b
block
offset
bits
indicate
the
word
within
the
block
And
the
next
bit
indicates
the
set
The
remaining
bits
are
the
tag
Therefore
word
x
C
maps
to
set
word
in
the
cache
The
principle
of
using
larger
block
sizes
to
exploit
spatial
locality
also
applies
to
associative
caches
Example
SPATIAL
LOCALITY
WITH
A
DIRECT
MAPPED
CACHE
Repeat
Example
for
the
eight
word
direct
mapped
cache
with
a
four
word
block
size
Solution
Figure
shows
the
contents
of
the
cache
after
the
first
memory
access
On
the
first
loop
iteration
the
cache
misses
on
the
access
to
memory
address
x
This
access
loads
data
at
addresses
x
through
xC
into
the
cache
block
All
subsequent
accesses
as
shown
for
address
xC
hit
in
the
cache
Hence
the
miss
rate
is
Caches
Tag
Byte
Offset
Memory
Address
Block
Offset
C
Set
Figure
Cache
fields
for
address
x
C
when
mapping
to
the
cache
of
Figure
Set
V
Tag
Data
mem
x
C
Set
mem
x
mem
x
mem
x
Tag
Byte
Offset
Memory
Address
V
Block
Set
Offset
Figure
Cache
contents
with
a
block
size
b
of
four
words
Putting
It
All
Together
Caches
are
organized
as
two
dimensional
arrays
The
rows
are
called
sets
and
the
columns
are
called
ways
Each
entry
in
the
array
consists
of
Chap
a
data
block
and
its
associated
valid
and
tag
bits
Caches
are
characterized
by
capacity
C
block
size
b
and
number
of
blocks
B
C
b
number
of
blocks
in
a
set
N
Table
summarizes
the
various
cache
organizations
Each
address
in
memory
maps
to
only
one
set
but
can
be
stored
in
any
of
the
ways
Cache
capacity
associativity
set
size
and
block
size
are
typically
powers
of
This
makes
the
cache
fields
tag
set
and
block
offset
bits
subsets
of
the
address
bits
Increasing
the
associativity
N
usually
reduces
the
miss
rate
caused
by
conflicts
But
higher
associativity
requires
more
tag
comparators
Increasing
the
block
size
b
takes
advantage
of
spatial
locality
to
reduce
the
miss
rate
However
it
decreases
the
number
of
sets
in
a
fixed
sized
cache
and
therefore
could
lead
to
more
conflicts
It
also
increases
the
miss
penalty
What
Data
Is
Replaced
In
a
direct
mapped
cache
each
address
maps
to
a
unique
block
and
set
If
a
set
is
full
when
new
data
must
be
loaded
the
block
in
that
set
is
replaced
with
the
new
data
In
set
associative
and
fully
associative
caches
the
cache
must
choose
which
block
to
evict
when
a
cache
set
is
full
The
principle
of
temporal
locality
suggests
that
the
best
choice
is
to
evict
the
least
recently
used
block
because
it
is
least
likely
to
be
used
again
soon
Hence
most
associative
caches
have
a
least
recently
used
LRU
replacement
policy
In
a
two
way
set
associative
cache
a
use
bit
U
indicates
which
way
within
a
set
was
least
recently
used
Each
time
one
of
the
ways
is
used
U
is
adjusted
to
indicate
the
other
way
For
set
associative
caches
with
more
than
two
ways
tracking
the
least
recently
used
way
becomes
complicated
To
simplify
the
problem
the
ways
are
often
divided
into
two
groups
and
U
indicates
which
group
of
ways
was
least
recently
used
CHAPTER
EIGHT
Memory
Systems
Table
Cache
organizations
Number
of
Ways
Number
of
Sets
Organization
N
S
direct
mapped
B
set
associative
N
B
B
N
fully
associative
B
Chap
Upon
replacement
the
new
block
replaces
a
random
block
within
the
least
recently
used
group
Such
a
policy
is
called
pseudo
LRU
and
is
good
enough
in
practice
Example
LRU
REPLACEMENT
Show
the
contents
of
an
eight
word
two
way
set
associative
cache
after
executing
the
following
code
Assume
LRU
replacement
a
block
size
of
one
word
and
an
initially
empty
cache
lw
t
x
lw
t
x
lw
t
x
Solution
The
first
two
instructions
load
data
from
memory
addresses
x
and
x
into
set
of
the
cache
shown
in
Figure
a
U
indicates
that
data
in
way
was
the
least
recently
used
The
next
memory
access
to
address
x
also
maps
to
set
and
replaces
the
least
recently
used
data
in
way
as
shown
in
Figure
b
The
use
bit
U
is
set
to
to
indicate
that
data
in
way
was
the
least
recently
used
Caches
V
Tag
Data
V
Tag
Data
U
mem
x
mem
x
a
Way
Way
Set
Set
Set
Set
V
Tag
Data
V
Tag
Data
U
mem
x
mem
x
b
Way
Way
Set
Set
Set
Set
Figure
Two
way
associative
cache
with
LRU
replacement
Advanced
Cache
Design
Modern
systems
use
multiple
levels
of
caches
to
decrease
memory
access
time
This
section
explores
the
performance
of
a
two
level
caching
system
and
examines
how
block
size
associativity
and
cache
capacity
affect
miss
rate
The
section
also
describes
how
caches
handle
stores
or
writes
by
using
a
write
through
or
write
back
policy
C
Multiple
Level
Caches
Large
caches
are
beneficial
because
they
are
more
likely
to
hold
data
of
interest
and
therefore
have
lower
miss
rates
However
large
caches
tend
to
be
slower
than
small
ones
Modern
systems
often
use
two
levels
of
caches
as
shown
in
Figure
The
first
level
L
cache
is
small
enough
to
provide
a
one
or
two
cycle
access
time
The
second
level
L
cache
is
also
built
from
SRAM
but
is
larger
and
therefore
slower
than
the
L
cache
The
processor
first
looks
for
the
data
in
the
L
cache
If
the
L
cache
misses
the
processor
looks
in
the
L
cache
If
the
L
cache
misses
the
processor
fetches
the
data
from
main
memory
Some
modern
systems
add
even
more
levels
of
cache
to
the
memory
hierarchy
because
accessing
main
memory
is
so
slow
Example
SYSTEM
WITH
AN
L
CACHE
Use
the
system
of
Figure
with
access
times
of
and
cycles
for
the
L
cache
L
cache
and
main
memory
respectively
Assume
that
the
L
and
L
caches
have
miss
rates
of
and
respectively
Specifically
of
the
of
accesses
that
miss
the
L
cache
of
those
also
miss
the
L
cache
What
is
the
average
memory
access
time
AMAT
Solution
Each
memory
access
checks
the
L
cache
When
the
L
cache
misses
of
the
time
the
processor
checks
the
L
cache
When
the
L
cache
misses
of
the
time
the
processor
fetches
the
data
from
main
memory
Using
Equation
we
calculate
the
average
memory
access
time
as
follows
cycle
cycles
cycles
cycles
The
L
miss
rate
is
high
because
it
receives
only
the
hard
memory
accesses
those
that
miss
in
the
L
cache
If
all
accesses
went
directly
to
the
L
cache
the
L
miss
rate
would
be
about
CHAPTER
EIGHT
Memory
Systems
L
Cache
L
Cache
Main
Memory
Capacity
Virtual
Memory
Speed
Figure
Memory
hierarchy
with
two
levels
of
cache
C
Reducing
Miss
Rate
Cache
misses
can
be
reduced
by
changing
capacity
block
size
and
or
associativity
The
first
step
to
reducing
the
miss
rate
is
to
understand
the
causes
of
the
misses
The
misses
can
be
classified
as
compulsory
capacity
and
conflict
The
first
request
to
a
cache
block
is
called
a
compulsory
miss
because
the
block
must
be
read
from
memory
regardless
of
the
cache
design
Capacity
misses
occur
when
the
cache
is
too
small
to
hold
all
concurrently
used
data
Conflict
misses
are
caused
when
several
addresses
map
to
the
same
set
and
evict
blocks
that
are
still
needed
Changing
cache
parameters
can
affect
one
or
more
type
of
cache
miss
For
example
increasing
cache
capacity
can
reduce
conflict
and
capacity
misses
but
it
does
not
affect
compulsory
misses
On
the
other
hand
increasing
block
size
could
reduce
compulsory
misses
due
to
spatial
locality
but
might
actually
increase
conflict
misses
because
more
addresses
would
map
to
the
same
set
and
could
conflict
Memory
systems
are
complicated
enough
that
the
best
way
to
evaluate
their
performance
is
by
running
benchmarks
while
varying
cache
parameters
Figure
plots
miss
rate
versus
cache
size
and
degree
of
associativity
for
the
SPEC
benchmark
This
benchmark
has
a
small
number
of
compulsory
misses
shown
by
the
dark
region
near
the
x
axis
As
expected
when
cache
size
increases
capacity
misses
decrease
Increased
associativity
especially
for
small
caches
decreases
the
number
of
conflict
misses
shown
along
the
top
of
the
curve
Caches
way
way
way
way
Capacity
Compulsory
Cache
Size
KB
Miss
Rate
per
Type
Figure
Miss
rate
versus
cache
size
and
associativity
on
SPEC
benchmark
Adapted
with
permission
from
Hennessy
and
Patterson
Computer
Architecture
A
Quantitative
Approach
rd
ed
Morgan
Kaufmann
Increasing
associativity
beyond
four
or
eight
ways
provides
only
small
decreases
in
miss
rate
As
mentioned
miss
rate
can
also
be
decreased
by
using
larger
block
sizes
that
take
advantage
of
spatial
locality
But
as
block
size
increases
the
number
of
sets
in
a
fixed
size
cache
decreases
increasing
the
probability
of
conflicts
Figure
plots
miss
rate
versus
block
size
in
number
of
bytes
for
caches
of
varying
capacity
For
small
caches
such
as
the
KB
cache
increasing
the
block
size
beyond
bytes
increases
the
miss
rate
because
of
conflicts
For
larger
caches
increasing
the
block
size
does
not
change
the
miss
rate
However
large
block
sizes
might
still
increase
execution
time
because
of
the
larger
miss
penalty
the
time
required
to
fetch
the
missing
cache
block
from
main
memory
on
a
miss
Write
Policy
The
previous
sections
focused
on
memory
loads
Memory
stores
or
writes
follow
a
similar
procedure
as
loads
Upon
a
memory
store
the
processor
checks
the
cache
If
the
cache
misses
the
cache
block
is
fetched
from
main
memory
into
the
cache
and
then
the
appropriate
word
in
the
cache
block
is
written
If
the
cache
hits
the
word
is
simply
written
to
the
cache
block
Caches
are
classified
as
either
write
through
or
write
back
In
a
write
through
cache
the
data
written
to
a
cache
block
is
simultaneously
written
to
main
memory
In
a
write
back
cache
a
dirty
bit
D
is
associated
with
each
cache
block
D
is
when
the
cache
block
has
been
written
and
otherwise
Dirty
cache
blocks
are
written
back
to
main
memory
only
when
they
are
evicted
from
the
cache
A
write
through
CHAPTER
EIGHT
Memory
Systems
Miss
Rate
Block
Size
K
K
K
K
Figure
Miss
rate
versus
block
size
and
cache
size
on
SPEC
benchmark
Adapted
with
permission
from
Hennessy
and
Patterson
Computer
Architecture
A
Quantitative
Approach
rd
ed
Morgan
Kaufmann
cache
requires
no
dirty
bit
but
usually
requires
more
main
memory
writes
than
a
write
back
cache
Modern
caches
are
usually
write
back
because
main
memory
access
time
is
so
large
Example
WRITE
THROUGH
VERSUS
WRITE
BACK
Suppose
a
cache
has
a
block
size
of
four
words
How
many
main
memory
accesses
are
required
by
the
following
code
when
using
each
write
policy
writethrough
or
write
back
sw
t
x
sw
t
xC
sw
t
x
sw
t
x
Solution
All
four
store
instructions
write
to
the
same
cache
block
With
a
writethrough
cache
each
store
instruction
writes
a
word
to
main
memory
requiring
four
main
memory
writes
A
write
back
policy
requires
only
one
main
memory
access
when
the
dirty
cache
block
is
evicted
The
Evolution
of
MIPS
Caches
Table
traces
the
evolution
of
cache
organizations
used
by
the
MIPS
processor
from
to
The
major
trends
are
the
introduction
of
multiple
levels
of
cache
larger
cache
capacity
and
increased
associativity
These
trends
are
driven
by
the
growing
disparity
between
CPU
frequency
and
main
memory
speed
and
the
decreasing
cost
of
transistors
The
growing
difference
between
CPU
and
memory
speeds
necessitates
a
lower
miss
rate
to
avoid
the
main
memory
bottleneck
and
the
decreasing
cost
of
transistors
allows
larger
cache
sizes
Caches
Table
MIPS
cache
evolution
Year
CPU
MHz
L
Cache
L
Cache
R
none
none
R
KB
direct
mapped
none
R
KB
direct
mapped
MB
direct
mapped
R
KB
two
way
MB
two
way
R
KB
two
way
MB
two
way
R
A
KB
two
way
MB
two
way
Adapted
from
D
Sweetman
See
MIPS
Run
Morgan
Kaufmann
VIRTUAL
MEMORY
Most
modern
computer
systems
use
a
hard
disk
also
called
a
hard
drive
as
the
lowest
level
in
the
memory
hierarchy
see
Figure
Compared
with
the
ideal
large
fast
cheap
memory
a
hard
disk
is
large
and
cheap
but
terribly
slow
The
disk
provides
a
much
larger
capacity
than
is
possible
with
a
cost
effective
main
memory
DRAM
However
if
a
significant
fraction
of
memory
accesses
involve
the
disk
performance
is
dismal
You
may
have
encountered
this
on
a
PC
when
running
too
many
programs
at
once
Figure
shows
a
hard
disk
with
the
lid
of
its
case
removed
As
the
name
implies
the
hard
disk
contains
one
or
more
rigid
disks
or
platters
each
of
which
has
a
read
write
head
on
the
end
of
a
long
triangular
arm
The
head
moves
to
the
correct
location
on
the
disk
and
reads
or
writes
data
magnetically
as
the
disk
rotates
beneath
it
The
head
takes
several
milliseconds
to
seek
the
correct
location
on
the
disk
which
is
fast
from
a
human
perspective
but
millions
of
times
slower
than
the
processor
CHAPTER
EIGHT
Memory
Systems
Figure
Hard
disk
The
objective
of
adding
a
hard
disk
to
the
memory
hierarchy
is
to
inexpensively
give
the
illusion
of
a
very
large
memory
while
still
providing
the
speed
of
faster
memory
for
most
accesses
A
computer
with
only
MB
of
DRAM
for
example
could
effectively
provide
GB
of
memory
using
the
hard
disk
This
larger
GB
memory
is
called
virtual
memory
and
the
smaller
MB
main
memory
is
called
physical
memory
We
will
use
the
term
physical
memory
to
refer
to
main
memory
throughout
this
section
Programs
can
access
data
anywhere
in
virtual
memory
so
they
must
use
virtual
addresses
that
specify
the
location
in
virtual
memory
The
physical
memory
holds
a
subset
of
most
recently
accessed
virtual
memory
In
this
way
physical
memory
acts
as
a
cache
for
virtual
memory
Thus
most
accesses
hit
in
physical
memory
at
the
speed
of
DRAM
yet
the
program
enjoys
the
capacity
of
the
larger
virtual
memory
Virtual
memory
systems
use
different
terminologies
for
the
same
caching
principles
discussed
in
Section
Table
summarizes
the
analogous
terms
Virtual
memory
is
divided
into
virtual
pages
typically
KB
in
size
Physical
memory
is
likewise
divided
into
physical
pages
of
the
same
size
A
virtual
page
may
be
located
in
physical
memory
DRAM
or
on
the
disk
For
example
Figure
shows
a
virtual
memory
that
is
larger
than
physical
memory
The
rectangles
indicate
pages
Some
virtual
pages
are
present
in
physical
memory
and
some
are
located
on
the
disk
The
process
of
determining
the
physical
address
from
the
virtual
address
is
called
address
translation
If
the
processor
attempts
to
access
a
virtual
address
that
is
not
in
physical
memory
a
page
fault
occurs
and
the
operating
system
loads
the
page
from
the
hard
disk
into
physical
memory
To
avoid
page
faults
caused
by
conflicts
any
virtual
page
can
map
to
any
physical
page
In
other
words
physical
memory
behaves
as
a
fully
associative
cache
for
virtual
memory
In
a
conventional
fully
associative
cache
every
cache
block
has
a
comparator
that
checks
the
most
significant
address
bits
against
a
tag
to
determine
whether
the
request
hits
in
Virtual
Memory
A
computer
with
bit
addresses
can
access
a
maximum
of
bytes
GB
of
memory
This
is
one
of
the
motivations
for
moving
to
bit
computers
which
can
access
far
more
memory
Table
Analogous
cache
and
virtual
memory
terms
Cache
Virtual
Memory
Block
Page
Block
size
Page
size
Block
offset
Page
offset
Miss
Page
fault
Tag
Virtual
page
number
C
the
block
In
an
analogous
virtual
memory
system
each
physical
page
would
need
a
comparator
to
check
the
most
significant
virtual
address
bits
against
a
tag
to
determine
whether
the
virtual
page
maps
to
that
physical
page
A
realistic
virtual
memory
system
has
so
many
physical
pages
that
providing
a
comparator
for
each
page
would
be
excessively
expensive
Instead
the
virtual
memory
system
uses
a
page
table
to
perform
address
translation
A
page
table
contains
an
entry
for
each
virtual
page
indicating
its
location
in
physical
memory
or
that
it
is
on
the
disk
Each
load
or
store
instruction
requires
a
page
table
access
followed
by
a
physical
memory
access
The
page
table
access
translates
the
virtual
address
used
by
the
program
to
a
physical
address
The
physical
address
is
then
used
to
actually
read
or
write
the
data
The
page
table
is
usually
so
large
that
it
is
located
in
physical
memory
Hence
each
load
or
store
involves
two
physical
memory
accesses
a
page
table
access
and
a
data
access
To
speed
up
address
translation
a
translation
lookaside
buffer
TLB
caches
the
most
commonly
used
page
table
entries
The
remainder
of
this
section
elaborates
on
address
translation
page
tables
and
TLBs
Address
Translation
In
a
system
with
virtual
memory
programs
use
virtual
addresses
so
that
they
can
access
a
large
memory
The
computer
must
translate
these
virtual
addresses
to
either
find
the
address
in
physical
memory
or
take
a
page
fault
and
fetch
the
data
from
the
hard
disk
Recall
that
virtual
memory
and
physical
memory
are
divided
into
pages
The
most
significant
bits
of
the
virtual
or
physical
address
specify
the
virtual
or
physical
page
number
The
least
significant
bits
specify
the
word
within
the
page
and
are
called
the
page
offset
CHAPTER
EIGHT
Memory
Systems
Physical
Memory
Physical
Addresses
Virtual
Addresses
Hard
Disk
Address
Translation
Figure
Virtual
and
physical
pages
Figure
illustrates
the
page
organization
of
a
virtual
memory
system
with
GB
of
virtual
memory
and
MB
of
physical
memory
divided
into
KB
pages
MIPS
accommodates
bit
addresses
With
a
GB
byte
virtual
memory
only
the
least
significant
virtual
address
bits
are
used
the
nd
bit
is
always
Similarly
with
a
MB
byte
physical
memory
only
the
least
significant
physical
address
bits
are
used
the
upper
bits
are
always
Because
the
page
size
is
KB
bytes
there
are
virtual
pages
and
physical
pages
Thus
the
virtual
and
physical
page
numbers
are
and
bits
respectively
Physical
memory
can
only
hold
up
to
th
of
the
virtual
pages
at
any
given
time
The
rest
of
the
virtual
pages
are
kept
on
disk
Figure
shows
virtual
page
mapping
to
physical
page
virtual
page
x
FFFC
mapping
to
physical
page
x
FFE
and
so
forth
For
example
virtual
address
x
F
an
offset
of
x
F
within
virtual
page
maps
to
physical
address
x
F
an
offset
of
x
F
within
physical
page
The
least
significant
bits
of
the
virtual
and
physical
addresses
are
the
same
x
F
and
specify
the
page
offset
within
the
virtual
and
physical
pages
Only
the
page
number
needs
to
be
translated
to
obtain
the
physical
address
from
the
virtual
address
Figure
illustrates
the
translation
of
a
virtual
address
to
a
physical
address
The
least
significant
bits
indicate
the
page
offset
and
require
no
translation
The
upper
bits
of
the
virtual
address
specify
the
virtual
page
number
VPN
and
are
translated
to
a
bit
physical
page
number
PPN
The
next
two
sections
describe
how
page
tables
and
TLBs
are
used
to
perform
this
address
translation
Virtual
Memory
Physical
Memory
Physical
Page
Number
Physical
Addresses
Virtual
Memory
Virtual
Page
Number
Virtual
Addresses
FFF
x
FFF
x
FFFFFF
x
FFE
x
FFEFFF
x
x
FFF
x
x
FFF
FFE
FFFA
FFF
FFFC
FFFB
FFFE
FFFD
FFFF
x
FFFF
x
FFFFFFF
x
FFFE
x
FFFEFFF
x
FFFD
x
FFFDFFF
x
FFFC
x
FFFCFFF
x
FFFB
x
FFFBFFF
x
FFFA
x
FFFAFFF
x
x
FFF
x
x
FFF
x
x
FFF
x
FFF
x
FFF
FFF
x
x
FFF
x
x
FFF
x
x
FFF
x
x
FFF
Figure
Physical
and
virtual
pages
Chapt
Example
VIRTUAL
ADDRESS
TO
PHYSICAL
ADDRESS
TRANSLATION
Find
the
physical
address
of
virtual
address
x
C
using
the
virtual
memory
system
shown
in
Figure
Solution
The
bit
page
offset
x
C
requires
no
translation
The
remaining
bits
of
the
virtual
address
give
the
virtual
page
number
so
virtual
address
x
C
is
found
in
virtual
page
x
In
Figure
virtual
page
x
maps
to
physical
page
x
FFF
Thus
virtual
address
x
C
maps
to
physical
address
x
FFF
C
The
Page
Table
The
processor
uses
a
page
table
to
translate
virtual
addresses
to
physical
addresses
Recall
that
the
page
table
contains
an
entry
for
each
virtual
page
This
entry
contains
a
physical
page
number
and
a
valid
bit
If
the
valid
bit
is
the
virtual
page
maps
to
the
physical
page
specified
in
the
entry
Otherwise
the
virtual
page
is
found
on
disk
Because
the
page
table
is
so
large
it
is
stored
in
physical
memory
Let
us
assume
for
now
that
it
is
stored
as
a
contiguous
array
as
shown
in
Figure
This
page
table
contains
the
mapping
of
the
memory
system
of
Figure
The
page
table
is
indexed
with
the
virtual
page
number
VPN
For
example
entry
specifies
that
virtual
page
maps
to
physical
page
Entry
is
invalid
V
so
virtual
page
is
located
on
disk
Example
USING
THE
PAGE
TABLE
TO
PERFORM
ADDRESS
TRANSLATION
Find
the
physical
address
of
virtual
address
x
C
using
the
page
table
shown
in
Figure
Solution
Figure
shows
the
virtual
address
to
physical
address
translation
for
virtual
address
x
C
The
bit
page
offset
requires
no
translation
The
remaining
bits
of
the
virtual
address
are
the
virtual
page
number
x
and
CHAPTER
EIGHT
Memory
Systems
PPN
Page
Offset
VPN
Page
Offset
Virtual
Address
Physical
Address
Translation
Figure
Translation
from
virtual
address
to
physical
address
Page
Table
Virtual
Page
Number
FFFA
FFFC
FFFB
FFFE
FFFD
FFFF
V
Physical
Page
Number
x
x
FFE
x
x
FFF
Figure
The
page
table
for
Figure
C
The
page
table
can
be
stored
anywhere
in
physical
memory
at
the
discretion
of
the
OS
The
processor
typically
uses
a
dedicated
register
called
the
page
table
register
to
store
the
base
address
of
the
page
table
in
physical
memory
To
perform
a
load
or
store
the
processor
must
first
translate
the
virtual
address
to
a
physical
address
and
then
access
the
data
at
that
physical
address
The
processor
extracts
the
virtual
page
number
from
the
virtual
address
and
adds
it
to
the
page
table
register
to
find
the
physical
address
of
the
page
table
entry
The
processor
then
reads
this
page
table
entry
from
physical
memory
to
obtain
the
physical
page
number
If
the
entry
is
valid
it
merges
this
physical
page
number
with
the
page
offset
to
create
the
physical
address
Finally
it
reads
or
writes
data
at
this
physical
address
Because
the
page
table
is
stored
in
physical
memory
each
load
or
store
involves
two
physical
memory
accesses
Virtual
Memory
x
x
FFE
x
Page
Table
x
FFF
V
Virtual
Address
x
C
Hit
Physical
Page
Number
Virtual
Page
Number
Page
Offset
Physical
Address
x
FFF
C
Figure
Address
translation
using
the
page
table
give
the
index
into
the
page
table
The
page
table
maps
virtual
page
x
to
physical
page
x
FFF
So
virtual
address
x
C
maps
to
physical
address
x
FFF
C
The
least
significant
bits
are
the
same
in
both
the
physical
and
the
virtual
address
The
Translation
Lookaside
Buffer
Virtual
memory
would
have
a
severe
performance
impact
if
it
required
a
page
table
read
on
every
load
or
store
doubling
the
delay
of
loads
and
stores
Fortunately
page
table
accesses
have
great
temporal
locality
The
temporal
and
spatial
locality
of
data
accesses
and
the
large
page
size
mean
that
many
consecutive
loads
or
stores
are
likely
to
reference
the
same
page
Therefore
if
the
processor
remembers
the
last
page
table
entry
that
it
read
it
can
probably
reuse
this
translation
without
rereading
the
page
table
In
general
the
processor
can
keep
the
last
several
page
table
entries
in
a
small
cache
called
a
translation
lookaside
buffer
TLB
The
processor
looks
aside
to
find
the
translation
in
the
TLB
before
having
to
access
the
page
table
in
physical
memory
In
real
programs
the
vast
majority
of
accesses
hit
in
the
TLB
avoiding
the
time
consuming
page
table
reads
from
physical
memory
A
TLB
is
organized
as
a
fully
associative
cache
and
typically
holds
to
entries
Each
TLB
entry
holds
a
virtual
page
number
and
its
corresponding
physical
page
number
The
TLB
is
accessed
using
the
virtual
page
number
If
the
TLB
hits
it
returns
the
corresponding
physical
page
number
Otherwise
the
processor
must
read
the
page
table
in
physical
memory
The
TLB
is
designed
to
be
small
enough
that
it
can
be
accessed
in
less
than
one
cycle
Even
so
TLBs
typically
have
a
hit
rate
of
greater
than
The
TLB
decreases
the
number
of
memory
accesses
required
for
most
load
or
store
instructions
from
two
to
one
Example
USING
THE
TLB
TO
PERFORM
ADDRESS
TRANSLATION
Consider
the
virtual
memory
system
of
Figure
Use
a
two
entry
TLB
or
explain
why
a
page
table
access
is
necessary
to
translate
virtual
addresses
x
C
and
x
FB
to
physical
addresses
Suppose
the
TLB
currently
holds
valid
translations
of
virtual
pages
x
and
x
FFFD
Solution
Figure
shows
the
two
entry
TLB
with
the
request
for
virtual
address
x
C
The
TLB
receives
the
virtual
page
number
of
the
incoming
address
x
and
compares
it
to
the
virtual
page
number
of
each
entry
Entry
matches
and
is
valid
so
the
request
hits
The
translated
physical
address
is
the
physical
page
number
of
the
matching
entry
x
FFF
concatenated
with
the
page
offset
of
the
virtual
address
As
always
the
page
offset
requires
no
translation
The
request
for
virtual
address
x
FB
misses
in
the
TLB
So
the
request
is
forwarded
to
the
page
table
for
translation
CHAPTER
EIGHT
Memory
Systems
Memory
Protection
So
far
this
section
has
focused
on
using
virtual
memory
to
provide
a
fast
inexpensive
large
memory
An
equally
important
reason
to
use
virtual
memory
is
to
provide
protection
between
concurrently
running
programs
As
you
probably
know
modern
computers
typically
run
several
programs
or
processes
at
the
same
time
All
of
the
programs
are
simultaneously
present
in
physical
memory
In
a
well
designed
computer
system
the
programs
should
be
protected
from
each
other
so
that
no
program
can
crash
or
hijack
another
program
Specifically
no
program
should
be
able
to
access
another
program
s
memory
without
permission
This
is
called
memory
protection
Virtual
memory
systems
provide
memory
protection
by
giving
each
program
its
own
virtual
address
space
Each
program
can
use
as
much
memory
as
it
wants
in
that
virtual
address
space
but
only
a
portion
of
the
virtual
address
space
is
in
physical
memory
at
any
given
time
Each
program
can
use
its
entire
virtual
address
space
without
having
to
worry
about
where
other
programs
are
physically
located
However
a
program
can
access
only
those
physical
pages
that
are
mapped
in
its
page
table
In
this
way
a
program
cannot
accidentally
or
maliciously
access
another
program
s
physical
pages
because
they
are
not
mapped
in
its
page
table
In
some
cases
multiple
programs
access
common
instructions
or
data
The
operating
system
adds
control
bits
to
each
page
table
entry
to
determine
which
programs
if
any
can
write
to
the
shared
physical
pages
Virtual
Memory
Hit
V
Hit
Hit
Hit
Virtual
Page
Number
Physical
Page
Number
Entry
x
FFFD
x
x
x
FFF
Virtual
Address
x
C
Virtual
Page
Number
Page
Offset
V
Virtual
Page
Number
Physical
Page
Number
Entry
Physical
Address
x
FFF
C
TLB
Figure
Address
translation
using
a
two
entry
TLB
Replacement
Policies
Virtual
memory
systems
use
write
back
and
an
approximate
least
recently
used
LRU
replacement
policy
A
write
through
policy
where
each
write
to
physical
memory
initiates
a
write
to
disk
would
be
impractical
Store
instructions
would
operate
at
the
speed
of
the
disk
instead
of
the
speed
of
the
processor
milliseconds
instead
of
nanoseconds
Under
the
writeback
policy
the
physical
page
is
written
back
to
disk
only
when
it
is
evicted
from
physical
memory
Writing
the
physical
page
back
to
disk
and
reloading
it
with
a
different
virtual
page
is
called
swapping
so
the
disk
in
a
virtual
memory
system
is
sometimes
called
swap
space
The
processor
swaps
out
one
of
the
least
recently
used
physical
pages
when
a
page
fault
occurs
then
replaces
that
page
with
the
missing
virtual
page
To
support
these
replacement
policies
each
page
table
entry
contains
two
additional
status
bits
a
dirty
bit
D
and
a
use
bit
U
The
dirty
bit
is
if
any
store
instructions
have
changed
the
physical
page
since
it
was
read
from
disk
When
a
physical
page
is
swapped
out
it
needs
to
be
written
back
to
disk
only
if
its
dirty
bit
is
otherwise
the
disk
already
holds
an
exact
copy
of
the
page
The
use
bit
is
if
the
physical
page
has
been
accessed
recently
As
in
a
cache
system
exact
LRU
replacement
would
be
impractically
complicated
Instead
the
OS
approximates
LRU
replacement
by
periodically
resetting
all
the
use
bits
in
the
page
table
When
a
page
is
accessed
its
use
bit
is
set
to
Upon
a
page
fault
the
OS
finds
a
page
with
U
to
swap
out
of
physical
memory
Thus
it
does
not
necessarily
replace
the
least
recently
used
page
just
one
of
the
least
recently
used
pages
Multilevel
Page
Tables
Page
tables
can
occupy
a
large
amount
of
physical
memory
For
example
the
page
table
from
the
previous
sections
for
a
GB
virtual
memory
with
KB
pages
would
need
entries
If
each
entry
is
bytes
the
page
table
is
bytes
bytes
MB
To
conserve
physical
memory
page
tables
can
be
broken
up
into
multiple
usually
two
levels
The
first
level
page
table
is
always
kept
in
physical
memory
It
indicates
where
small
second
level
page
tables
are
stored
in
virtual
memory
The
second
level
page
tables
each
contain
the
actual
translations
for
a
range
of
virtual
pages
If
a
particular
range
of
translations
is
not
actively
used
the
corresponding
second
level
page
table
can
be
swapped
out
to
the
hard
disk
so
it
does
not
waste
physical
memory
In
a
two
level
page
table
the
virtual
page
number
is
split
into
two
parts
the
page
table
number
and
the
page
table
offset
as
shown
in
Figure
The
page
table
number
indexes
the
first
level
page
table
which
must
reside
in
physical
memory
The
first
level
page
table
entry
gives
the
base
address
of
the
second
level
page
table
or
indicates
that
CHAPTER
EIGHT
Memory
Systems
Cha
it
must
be
fetched
from
disk
when
V
is
The
page
table
offset
indexes
the
second
level
page
table
The
remaining
bits
of
the
virtual
address
are
the
page
offset
as
before
for
a
page
size
of
KB
In
Figure
the
bit
virtual
page
number
is
broken
into
and
bits
to
indicate
the
page
table
number
and
the
page
table
offset
respectively
Thus
the
first
level
page
table
has
entries
Each
of
these
second
level
page
tables
has
K
entries
If
each
of
the
first
and
second
level
page
table
entries
is
bits
bytes
and
only
two
secondlevel
page
tables
are
present
in
physical
memory
at
once
the
hierarchical
page
table
uses
only
bytes
K
bytes
KB
of
physical
memory
The
two
level
page
table
requires
a
fraction
of
the
physical
memory
needed
to
store
the
entire
page
table
MB
The
drawback
of
a
two
level
page
table
is
that
it
adds
yet
another
memory
access
for
translation
when
the
TLB
misses
Example
USING
A
MULTILEVEL
PAGE
TABLE
FOR
ADDRESS
TRANSLATION
Figure
shows
the
possible
contents
of
the
two
level
page
table
from
Figure
The
contents
of
only
one
second
level
page
table
are
shown
Using
this
two
level
page
table
describe
what
happens
on
an
access
to
virtual
address
x
FEFB
Virtual
Memory
First
Level
Page
Table
Page
Table
Address
K
entries
entries
Page
Table
Number
Page
Table
Offset
Virtual
Address
V
Physical
Page
V
Number
Second
Level
Page
Tables
Page
Offset
Figure
Hierarchical
page
tables
Chap
MEMORY
MAPPED
I
O
Processors
also
use
the
memory
interface
to
communicate
with
input
output
I
O
devices
such
as
keyboards
monitors
and
printers
A
processor
accesses
an
I
O
device
using
the
address
and
data
busses
in
the
same
way
that
it
accesses
memory
A
portion
of
the
address
space
is
dedicated
to
I
O
devices
rather
than
memory
For
example
suppose
that
addresses
in
the
range
CHAPTER
EIGHT
Memory
Systems
Page
Table
Address
Page
Table
Number
Page
Table
Offset
Virtual
Address
V
Physical
Page
V
Number
Page
Offset
x
Valid
x
First
Level
Page
Table
Second
Level
Page
Tables
x
FE
FB
Valid
Physical
Address
x
F
FB
x
FFE
x
x
FC
x
C
x
x
F
Figure
Address
translation
using
a
two
level
page
table
Solution
As
always
only
the
virtual
page
number
requires
translation
The
most
significant
nine
bits
of
the
virtual
address
x
give
the
page
table
number
the
index
into
the
first
level
page
table
The
first
level
page
table
at
entry
x
indicates
that
the
second
level
page
table
is
resident
in
memory
V
and
its
physical
address
is
x
The
next
ten
bits
of
the
virtual
address
x
FE
are
the
page
table
offset
which
gives
the
index
into
the
second
level
page
table
Entry
is
at
the
bottom
of
the
second
level
page
table
and
entry
x
FF
is
at
the
top
Entry
x
FE
in
the
secondlevel
page
table
indicates
that
the
virtual
page
is
resident
in
physical
memory
V
and
that
the
physical
page
number
is
x
F
The
physical
page
number
is
concatenated
with
the
page
offset
to
form
the
physical
address
x
F
FB
Ch
xFFFF
to
xFFFFFFFF
are
used
for
I
O
Recall
from
Section
that
these
addresses
are
in
a
reserved
portion
of
the
memory
map
Each
I
O
device
is
assigned
one
or
more
memory
addresses
in
this
range
A
store
to
the
specified
address
sends
data
to
the
device
A
load
receives
data
from
the
device
This
method
of
communicating
with
I
O
devices
is
called
memory
mapped
I
O
In
a
system
with
memory
mapped
I
O
a
load
or
store
may
access
either
memory
or
an
I
O
device
Figure
shows
the
hardware
needed
to
support
two
memory
mapped
I
O
devices
An
address
decoder
determines
which
device
communicates
with
the
processor
It
uses
the
Address
and
MemWrite
signals
to
generate
control
signals
for
the
rest
of
the
hardware
The
ReadData
multiplexer
selects
between
memory
and
the
various
I
O
devices
Write
enabled
registers
hold
the
values
written
to
the
I
O
devices
Example
COMMUNICATING
WITH
I
O
DEVICES
Suppose
I
O
Device
in
Figure
is
assigned
the
memory
address
xFFFFFFF
Show
the
MIPS
assembly
code
for
writing
the
value
to
I
O
Device
and
for
reading
the
output
value
from
I
O
Device
Solution
The
following
MIPS
assembly
code
writes
the
value
to
I
O
Device
addi
t
sw
t
xFFF
The
address
decoder
asserts
WE
because
the
address
is
xFFFFFFF
and
MemWrite
is
TRUE
The
value
on
the
WriteData
bus
is
written
into
the
register
connected
to
the
input
pins
of
I
O
Device
Memory
Mapped
I
O
Processor
Memory
Address
MemWrite
WriteData
I
O
ReadData
Device
I
O
Device
CLK
EN
EN
Address
Decoder
WE
WEM
RDsel
WE
WE
CLK
CLK
Figure
Support
hardware
for
memory
mapped
I
O
Some
architectures
notably
IA
use
specialized
instructions
instead
of
memorymapped
I
O
to
communicate
with
I
O
devices
These
instructions
are
of
the
following
form
where
devicel
and
device
are
the
unique
ID
of
the
peripheral
device
lwio
t
device
swio
t
device
This
type
of
communication
with
I
O
devices
is
called
programmed
I
O
Recall
that
the
bit
immediate
xFFF
is
sign
extended
to
the
bit
value
xFFFFFFF
To
read
from
I
O
Device
the
processor
performs
the
following
MIPS
assembly
code
lw
t
xFFF
The
address
decoder
sets
RDsel
to
because
it
detects
the
address
xFFFFFFF
and
MemWrite
is
FALSE
The
output
of
I
O
Device
passes
through
the
multiplexer
onto
the
ReadData
bus
and
is
loaded
into
t
in
the
processor
Software
that
communicates
with
an
I
O
device
is
called
a
device
driver
You
have
probably
downloaded
or
installed
device
drivers
for
your
printer
or
other
I
O
device
Writing
a
device
driver
requires
detailed
knowledge
about
the
I
O
device
hardware
Other
programs
call
functions
in
the
device
driver
to
access
the
device
without
having
to
understand
the
low
level
device
hardware
To
illustrate
memory
mapped
I
O
hardware
and
software
the
rest
of
this
section
describes
interfacing
a
commercial
speech
synthesizer
chip
to
a
MIPS
processor
Speech
Synthesizer
Hardware
The
Radio
Shack
SP
speech
synthesizer
chip
generates
robot
like
speech
Words
are
composed
of
one
or
more
allophones
the
fundamental
units
of
sound
For
example
the
word
hello
uses
five
allophones
represented
by
the
following
symbols
in
the
SP
speech
chip
HH
EH
LL
AX
OW
The
speech
synthesizer
uses
bit
codes
to
represent
different
allophones
that
appear
in
the
English
language
For
example
the
five
allophones
for
the
word
hello
correspond
to
the
hexadecimal
values
x
B
x
x
D
x
F
x
respectively
The
processor
sends
a
series
of
allophones
to
the
speech
synthesizer
which
drives
a
speaker
to
blabber
the
sounds
Figure
shows
the
pinout
of
the
SP
speech
chip
The
I
O
pins
highlighted
in
blue
are
used
to
interface
with
the
MIPS
processor
to
produce
speech
Pins
A
receive
the
bit
allophone
encoding
from
the
processor
The
allophone
sound
is
produced
on
the
Digital
Out
pin
The
Digital
Out
signal
is
first
amplified
and
then
sent
to
a
speaker
The
other
two
highlighted
pins
SBY
and
ALD
are
status
and
control
pins
When
the
SBY
output
is
the
speech
chip
is
standing
by
and
is
ready
to
receive
a
new
allophone
On
the
falling
edge
of
the
address
load
input
ALD
the
speech
chip
reads
the
allophone
specified
by
A
Other
pins
such
as
power
and
ground
VDD
and
VSS
and
the
clock
OSC
must
be
connected
as
shown
but
are
not
driven
by
the
processor
Figure
shows
the
speech
synthesizer
interfaced
to
the
MIPS
processor
The
processor
uses
three
memory
mapped
I
O
addresses
to
communicate
with
the
speech
synthesizer
We
arbitrarily
have
chosen
CHAPTER
EIGHT
Memory
Systems
See
www
speechchips
com
for
more
information
about
the
SP
and
the
allophone
encodings
that
the
A
port
is
mapped
to
address
xFFFFFF
ALD
to
xFFFFFF
and
SBY
to
xFFFFFF
Although
the
WriteData
bus
is
bits
only
the
least
significant
bits
are
used
for
A
and
the
least
significant
bit
is
used
for
ALD
the
other
bits
are
ignored
Similarly
SBY
is
read
on
the
least
significant
bit
of
the
ReadData
bus
the
other
bits
are
Memory
Mapped
I
O
OSC
OSC
ROM
Clock
SBY
Reset
Digital
Out
VD
Test
Ser
In
ALD
SE
A
A
A
A
VSS
Reset
ROM
Disable
C
C
C
VDD
SBY
To
Processor
From
Processor
From
Processor
From
Processor
LRQ
A
A
Ser
Out
A
A
SPO
MHz
A
Amplifier
Speaker
Processor
Memory
Address
MemWrite
WriteData
ReadData
SP
CLK
EN
EN
Address
Decoder
WE
CLK
CLK
A
ALD
SBY
WE
WE
WEM
RDsel
Figure
SPO
speech
synthesizer
chip
pinout
Figure
Hardware
for
driving
the
SPO
speech
synthesizer
Speech
Synthesizer
Device
Driver
The
device
driver
controls
the
speech
synthesizer
by
sending
an
appropriate
series
of
allophones
over
the
memory
mapped
I
O
interface
It
follows
the
protocol
expected
by
the
SPO
chip
given
below
Set
ALD
to
Wait
until
the
chip
asserts
SBY
to
indicate
that
it
is
finished
speaking
the
previous
allophone
and
is
ready
for
the
next
Write
a
bit
allophone
to
A
Reset
ALD
to
to
initiate
speech
This
sequence
can
be
repeated
for
any
number
of
allophones
The
MIPS
assembly
in
Code
Example
writes
five
allophones
to
the
speech
chip
The
allophone
encodings
are
stored
as
bit
values
in
a
five
entry
array
starting
at
memory
address
x
The
assembly
code
in
Code
Example
polls
or
repeatedly
checks
the
SBY
signal
to
determine
when
the
speech
chip
is
ready
to
receive
a
new
allophone
The
code
functions
correctly
but
wastes
valuable
processor
cycles
that
could
be
used
to
perform
useful
work
Instead
of
polling
the
processor
could
use
an
interrupt
connected
to
SBY
When
SBY
rises
the
processor
stops
what
it
is
doing
and
jumps
to
code
that
handles
the
interrupt
In
the
case
of
the
speech
synthesizer
the
interrupt
handler
CHAPTER
EIGHT
Memory
Systems
init
addi
t
t
value
to
write
to
ALD
addi
t
t
array
size
lui
t
x
t
array
base
address
addi
t
t
array
index
start
sw
t
xFF
ALD
loop
lw
t
xFF
t
SBY
beq
t
loop
loop
until
SBY
add
t
t
t
t
address
of
allophone
lw
t
t
t
allophone
sw
t
xFF
A
allophone
sw
xFF
ALD
to
initiate
speech
addi
t
t
increment
array
index
beq
t
t
done
last
allophone
in
array
j
start
repeat
done
Code
Example
SPEECH
CHIP
DEVICE
DRIVER
Chapter
qxd
would
send
the
next
allophone
then
let
the
processor
resume
what
it
was
doing
before
the
interrupt
As
described
in
Section
the
processor
handles
interrupts
like
any
other
exception
REAL
WORLD
PERSPECTIVE
IA
MEMORY
AND
I
O
SYSTEMS
As
processors
get
faster
they
need
ever
more
elaborate
memory
hierarchies
to
keep
a
steady
supply
of
data
and
instructions
flowing
This
section
describes
the
memory
systems
of
IA
processors
to
illustrate
the
progression
Section
contained
photographs
of
the
processors
highlighting
the
on
chip
caches
IA
also
has
an
unusual
programmed
I
O
system
that
differs
from
the
more
common
memory
mapped
I
O
IA
Cache
Systems
The
initially
produced
in
operated
at
MHz
It
lacked
a
cache
so
it
directly
accessed
main
memory
for
all
instructions
and
data
Depending
on
the
speed
of
the
memory
the
processor
might
get
an
immediate
response
or
it
might
have
to
pause
for
one
or
more
cycles
for
the
memory
to
react
These
cycles
are
called
wait
states
and
they
increase
the
CPI
of
the
processor
Microprocessor
clock
frequencies
have
increased
by
at
least
per
year
since
then
whereas
memory
latency
has
scarcely
diminished
The
delay
from
when
the
processor
sends
an
address
to
main
memory
until
the
memory
returns
the
data
can
now
exceed
processor
clock
cycles
Therefore
caches
with
a
low
miss
rate
are
essential
to
good
performance
Table
summarizes
the
evolution
of
cache
systems
on
Intel
IA
processors
The
introduced
a
unified
write
through
cache
to
hold
both
instructions
and
data
Most
high
performance
computer
systems
also
provided
a
larger
second
level
cache
on
the
motherboard
using
commercially
available
SRAM
chips
that
were
substantially
faster
than
main
memory
The
Pentium
processor
introduced
separate
instruction
and
data
caches
to
avoid
contention
during
simultaneous
requests
for
data
and
instructions
The
caches
used
a
write
back
policy
reducing
the
communication
with
main
memory
Again
a
larger
second
level
cache
typically
KB
was
usually
offered
on
the
motherboard
The
P
series
of
processors
Pentium
Pro
Pentium
II
and
Pentium
III
were
designed
for
much
higher
clock
frequencies
The
second
level
cache
on
the
motherboard
could
not
keep
up
so
it
was
moved
closer
to
the
processor
to
improve
its
latency
and
throughput
The
Pentium
Pro
was
packaged
in
a
multichip
module
MCM
containing
both
the
processor
chip
and
a
second
level
cache
chip
as
shown
in
Figure
Like
the
Pentium
the
processor
had
separate
KB
level
instruction
and
data
Real
World
Perspective
IA
Memory
and
I
O
Systems
caches
However
these
caches
were
nonblocking
so
that
the
out
oforder
processor
could
continue
executing
subsequent
cache
accesses
even
if
the
cache
missed
a
particular
access
and
had
to
fetch
data
from
main
memory
The
second
level
cache
was
KB
KB
or
MB
in
size
and
could
operate
at
the
same
speed
as
the
processor
Unfortunately
the
MCM
packaging
proved
too
expensive
for
high
volume
manufacturing
Therefore
the
Pentium
II
was
sold
in
a
lower
cost
cartridge
containing
the
processor
and
the
second
level
cache
The
level
caches
were
doubled
in
size
to
compensate
for
the
fact
that
the
second
level
cache
operated
at
half
the
processor
s
speed
The
Pentium
III
integrated
a
full
speed
second
level
cache
directly
onto
the
same
chip
as
the
processor
A
cache
on
the
same
chip
can
operate
at
better
latency
and
throughput
so
it
is
substantially
more
effective
than
an
off
chip
cache
of
the
same
size
The
Pentium
offered
a
nonblocking
level
data
cache
It
switched
to
a
trace
cache
to
store
instructions
after
they
had
been
decoded
into
micro
ops
avoiding
the
delay
of
redecoding
each
time
instructions
were
fetched
from
the
cache
The
Pentium
M
design
was
adapted
from
the
Pentium
III
It
further
increased
the
level
caches
to
KB
each
and
featured
a
to
MB
level
cache
The
Core
Duo
contains
two
modified
Pentium
M
processors
and
CHAPTER
EIGHT
Memory
Systems
Table
Evolution
of
Intel
IA
microprocessor
memory
systems
Frequency
Level
Level
Processor
Year
MHz
Data
Cache
Instruction
Cache
Level
Cache
none
none
none
KB
unified
none
on
chip
Pentium
KB
KB
none
on
chip
Pentium
Pro
KB
KB
KB
MB
on
MCM
Pentium
II
KB
KB
KB
on
cartridge
Pentium
III
KB
KB
KB
on
chip
Pentium
KB
K
op
KB
MB
trace
cache
on
chip
Pentium
M
KB
KB
MB
on
chip
Core
Duo
KB
core
KB
core
MB
shared
on
chip
a
shared
MB
cache
on
one
chip
The
shared
cache
is
used
for
communication
between
the
processors
one
can
write
data
to
the
cache
and
the
other
can
read
it
IA
Virtual
Memory
IA
processors
operate
in
either
real
mode
or
protected
mode
Real
mode
is
backward
compatible
with
the
original
It
only
uses
bits
of
addresses
limiting
memory
to
MB
and
it
does
not
allow
virtual
memory
Protected
mode
was
introduced
with
the
and
extended
to
bit
addresses
with
the
It
supports
virtual
memory
with
KB
pages
It
also
provides
memory
protection
so
that
one
program
cannot
access
the
pages
belonging
to
other
programs
Hence
a
buggy
or
malicious
program
cannot
crash
or
corrupt
other
programs
All
modern
operating
systems
now
use
protected
mode
A
bit
address
permits
up
to
GB
of
memory
Processors
since
the
Pentium
Pro
have
bumped
the
memory
capacity
to
GB
using
a
Real
World
Perspective
IA
Memory
and
I
O
Systems
Figure
Pentium
Pro
multichip
module
with
processor
left
and
KB
cache
right
in
a
pin
grid
array
PGA
package
Courtesy
Intel
Although
memory
protection
became
available
in
the
hardware
in
the
early
s
Microsoft
Windows
took
almost
years
to
take
advantage
of
the
feature
and
prevent
bad
programs
from
crashing
the
entire
computer
Until
the
release
of
Windows
consumer
versions
of
Windows
were
notoriously
unstable
The
lag
between
hardware
features
and
software
support
can
be
extremely
long
technique
called
physical
address
extension
Each
process
uses
bit
addresses
The
virtual
memory
system
maps
these
addresses
onto
a
larger
bit
virtual
memory
space
It
uses
different
page
tables
for
each
process
so
that
each
process
can
have
its
own
address
space
of
up
to
GB
IA
Programmed
I
O
Most
architectures
use
memory
mapped
I
O
described
in
Section
in
which
programs
access
I
O
devices
by
reading
and
writing
memory
locations
IA
uses
programmed
I
O
in
which
special
IN
and
OUT
instructions
are
used
to
read
and
write
I
O
devices
IA
defines
I
O
ports
The
IN
instruction
reads
one
two
or
four
bytes
from
the
port
specified
by
DX
into
AL
AX
or
EAX
OUT
is
similar
but
writes
the
port
Connecting
a
peripheral
device
to
a
programmed
I
O
system
is
similar
to
connecting
it
to
a
memory
mapped
system
When
accessing
an
I
O
port
the
processor
sends
the
port
number
rather
than
the
memory
address
on
the
least
significant
bits
of
the
address
bus
The
device
reads
or
writes
data
from
the
data
bus
The
major
difference
is
that
the
processor
also
produces
an
M
IO
signal
When
M
IO
the
processor
is
accessing
memory
When
it
is
the
process
is
accessing
one
of
the
I
O
devices
The
address
decoder
must
also
look
at
M
IO
to
generate
the
appropriate
enables
for
main
memory
and
for
the
I
O
devices
I
O
devices
can
also
send
interrupts
to
the
processor
to
indicate
that
they
are
ready
to
communicate
SUMMARY
Memory
system
organization
is
a
major
factor
in
determining
computer
performance
Different
memory
technologies
such
as
DRAM
SRAM
and
hard
disks
offer
trade
offs
in
capacity
speed
and
cost
This
chapter
introduced
cache
and
virtual
memory
organizations
that
use
a
hierarchy
of
memories
to
approximate
an
ideal
large
fast
inexpensive
memory
Main
memory
is
typically
built
from
DRAM
which
is
significantly
slower
than
the
processor
A
cache
reduces
access
time
by
keeping
commonly
used
data
in
fast
SRAM
Virtual
memory
increases
the
memory
capacity
by
using
a
hard
disk
to
store
data
that
does
not
fit
in
the
main
memory
Caches
and
virtual
memory
add
complexity
and
hardware
to
a
computer
system
but
the
benefits
usually
outweigh
the
costs
All
modern
personal
computers
use
caches
and
virtual
memory
Most
processors
also
use
the
memory
interface
to
communicate
with
I
O
devices
This
is
called
memory
mapped
I
O
Programs
use
load
and
store
operations
to
access
the
I
O
devices
CHAPTER
EIGHT
Memory
Systems
C
EPILOGUE
This
chapter
brings
us
to
the
end
of
our
journey
together
into
the
realm
of
digital
systems
We
hope
this
book
has
conveyed
the
beauty
and
thrill
of
the
art
as
well
as
the
engineering
knowledge
You
have
learned
to
design
combinational
and
sequential
logic
using
schematics
and
hardware
description
languages
You
are
familiar
with
larger
building
blocks
such
as
multiplexers
ALUs
and
memories
Computers
are
one
of
the
most
fascinating
applications
of
digital
systems
You
have
learned
how
to
program
a
MIPS
processor
in
its
native
assembly
language
and
how
to
build
the
processor
and
memory
system
using
digital
building
blocks
Throughout
you
have
seen
the
application
of
abstraction
discipline
hierarchy
modularity
and
regularity
With
these
techniques
we
have
pieced
together
the
puzzle
of
a
microprocessor
s
inner
workings
From
cell
phones
to
digital
television
to
Mars
rovers
to
medical
imaging
systems
our
world
is
an
increasingly
digital
place
Imagine
what
Faustian
bargain
Charles
Babbage
would
have
made
to
take
a
similar
journey
a
century
and
a
half
ago
He
merely
aspired
to
calculate
mathematical
tables
with
mechanical
precision
Today
s
digital
systems
are
yesterday
s
science
fiction
Might
Dick
Tracy
have
listened
to
iTunes
on
his
cell
phone
Would
Jules
Verne
have
launched
a
constellation
of
global
positioning
satellites
into
space
Could
Hippocrates
have
cured
illness
using
high
resolution
digital
images
of
the
brain
But
at
the
same
time
George
Orwell
s
nightmare
of
ubiquitous
government
surveillance
becomes
closer
to
reality
each
day
And
rogue
states
develop
nuclear
weapons
using
laptop
computers
more
powerful
than
the
room
sized
supercomputers
that
simulated
Cold
War
bombs
The
microprocessor
revolution
continues
to
accelerate
The
changes
in
the
coming
decades
will
surpass
those
of
the
past
You
now
have
the
tools
to
design
and
build
these
new
systems
that
will
shape
our
future
With
your
newfound
power
comes
profound
responsibility
We
hope
that
you
will
use
it
not
just
for
fun
and
riches
but
also
for
the
benefit
of
humanity
Epilogue
Exercises
Exercise
In
less
than
one
page
describe
four
everyday
activities
that
exhibit
temporal
or
spatial
locality
List
two
activities
for
each
type
of
locality
and
be
specific
Exercise
In
one
paragraph
describe
two
short
computer
applications
that
exhibit
temporal
and
or
spatial
locality
Describe
how
Be
specific
Exercise
Come
up
with
a
sequence
of
addresses
for
which
a
direct
mapped
cache
with
a
size
capacity
of
words
and
block
size
of
words
outperforms
a
fully
associative
cache
with
least
recently
used
LRU
replacement
that
has
the
same
capacity
and
block
size
Exercise
Repeat
Exercise
for
the
case
when
the
fully
associative
cache
outperforms
the
direct
mapped
cache
Exercise
Describe
the
trade
offs
of
increasing
each
of
the
following
cache
parameters
while
keeping
the
others
the
same
a
block
size
b
associativity
c
cache
size
Exercise
Is
the
miss
rate
of
a
two
way
set
associative
cache
always
usually
occasionally
or
never
better
than
that
of
a
direct
mapped
cache
of
the
same
capacity
and
block
size
Explain
Exercise
Each
of
the
following
statements
pertains
to
the
miss
rate
of
caches
Mark
each
statement
as
true
or
false
Briefly
explain
your
reasoning
present
a
counterexample
if
the
statement
is
false
a
A
two
way
set
associative
cache
always
has
a
lower
miss
rate
than
a
direct
mapped
cache
with
the
same
block
size
and
total
capacity
b
A
KB
direct
mapped
cache
always
has
a
lower
miss
rate
than
an
KB
direct
mapped
cache
with
the
same
block
size
c
An
instruction
cache
with
a
byte
block
size
usually
has
a
lower
miss
rate
than
an
instruction
cache
with
an
byte
block
size
given
the
same
degree
of
associativity
and
total
capacity
CHAPTER
EIGHT
Memory
Systems
Exercise
A
cache
has
the
following
parameters
b
block
size
given
in
numbers
of
words
S
number
of
sets
N
number
of
ways
and
A
number
of
address
bits
a
In
terms
of
the
parameters
described
what
is
the
cache
capacity
C
b
In
terms
of
the
parameters
described
what
is
the
total
number
of
bits
required
to
store
the
tags
c
What
are
S
and
N
for
a
fully
associative
cache
of
capacity
C
words
with
block
size
b
d
What
is
S
for
a
direct
mapped
cache
of
size
C
words
and
block
size
b
Exercise
A
word
cache
has
the
parameters
given
in
Exercise
Consider
the
following
repeating
sequence
of
lw
addresses
given
in
hexadecimal
C
C
C
C
C
C
Assuming
least
recently
used
LRU
replacement
for
associative
caches
determine
the
effective
miss
rate
if
the
sequence
is
input
to
the
following
caches
ignoring
startup
effects
i
e
compulsory
misses
a
direct
mapped
cache
S
b
word
b
fully
associative
cache
N
b
word
c
two
way
set
associative
cache
S
b
word
d
direct
mapped
cache
S
b
words
Exercise
Suppose
you
are
running
a
program
with
the
following
data
access
pattern
The
pattern
is
executed
only
once
x
x
x
x
x
x
a
If
you
use
a
direct
mapped
cache
with
a
cache
size
of
KB
and
a
block
size
of
bytes
words
how
many
sets
are
in
the
cache
b
With
the
same
cache
and
block
size
as
in
part
a
what
is
the
miss
rate
of
the
direct
mapped
cache
for
the
given
memory
access
pattern
c
For
the
given
memory
access
pattern
which
of
the
following
would
decrease
the
miss
rate
the
most
Cache
capacity
is
kept
constant
Circle
one
i
Increasing
the
degree
of
associativity
to
ii
Increasing
the
block
size
to
bytes
Exercises
Chapter
iii
Either
i
or
ii
iv
Neither
i
nor
ii
Exercise
You
are
building
an
instruction
cache
for
a
MIPS
processor
It
has
a
total
capacity
of
C
c
bytes
It
is
N
n
way
set
associative
N
with
a
block
size
of
b
b
bytes
b
Give
your
answers
to
the
following
questions
in
terms
of
these
parameters
a
Which
bits
of
the
address
are
used
to
select
a
word
within
a
block
b
Which
bits
of
the
address
are
used
to
select
the
set
within
the
cache
c
How
many
bits
are
in
each
tag
d
How
many
tag
bits
are
in
the
entire
cache
Exercise
Consider
a
cache
with
the
following
parameters
N
associativity
b
block
size
words
W
word
size
bits
C
cache
size
K
words
A
address
size
bits
You
need
consider
only
word
addresses
a
Show
the
tag
set
block
offset
and
byte
offset
bits
of
the
address
State
how
many
bits
are
needed
for
each
field
b
What
is
the
size
of
all
the
cache
tags
in
bits
c
Suppose
each
cache
block
also
has
a
valid
bit
V
and
a
dirty
bit
D
What
is
the
size
of
each
cache
set
including
data
tag
and
status
bits
d
Design
the
cache
using
the
building
blocks
in
Figure
and
a
small
number
of
two
input
logic
gates
The
cache
design
must
include
tag
storage
data
storage
address
comparison
data
output
selection
and
any
other
parts
you
feel
are
relevant
Note
that
the
multiplexer
and
comparator
blocks
may
be
any
size
n
or
p
bits
wide
respectively
but
the
SRAM
blocks
must
be
K
bits
Be
sure
to
include
a
neatly
labeled
block
diagram
CHAPTER
EIGHT
Memory
Systems
K
SRAM
p
p
n
n
n
Figure
Building
blocks
Chapter
Exercise
You
ve
joined
a
hot
new
Internet
startup
to
build
wrist
watches
with
a
built
in
pager
and
Web
browser
It
uses
an
embedded
processor
with
a
multilevel
cache
scheme
depicted
in
Figure
The
processor
includes
a
small
on
chip
cache
in
addition
to
a
large
off
chip
second
level
cache
Yes
the
watch
weighs
pounds
but
you
should
see
it
surf
Assume
that
the
processor
uses
bit
physical
addresses
but
accesses
data
only
on
word
boundaries
The
caches
have
the
characteristics
given
in
Table
The
DRAM
has
an
access
time
of
tm
and
a
size
of
MB
a
For
a
given
word
in
memory
what
is
the
total
number
of
locations
in
which
it
might
be
found
in
the
on
chip
cache
and
in
the
second
level
cache
b
What
is
the
size
in
bits
of
each
tag
for
the
on
chip
cache
and
the
secondlevel
cache
c
Give
an
expression
for
the
average
memory
read
access
time
The
caches
are
accessed
in
sequence
d
Measurements
show
that
for
a
particular
problem
of
interest
the
on
chip
cache
hit
rate
is
and
the
second
level
cache
hit
rate
is
However
when
the
on
chip
cache
is
disabled
the
second
level
cache
hit
rate
shoots
up
to
Give
a
brief
explanation
of
this
behavior
Exercises
Figure
Computer
system
CPU
Level
Cache
Level
Cache
Main
Memory
Processor
Chip
Table
Memory
characteristics
Characteristic
On
chip
Cache
Off
chip
Cache
organization
four
way
set
associative
direct
mapped
hit
rate
A
B
access
time
ta
tb
block
size
bytes
bytes
number
of
blocks
K
Exercise
This
chapter
described
the
least
recently
used
LRU
replacement
policy
for
multiway
associative
caches
Other
less
common
replacement
policies
include
first
in
first
out
FIFO
and
random
policies
FIFO
replacement
evicts
the
block
that
has
been
there
the
longest
regardless
of
how
recently
it
was
accessed
Random
replacement
randomly
picks
a
block
to
evict
a
Discuss
the
advantages
and
disadvantages
of
each
of
these
replacement
policies
b
Describe
a
data
access
pattern
for
which
FIFO
would
perform
better
than
LRU
Exercise
You
are
building
a
computer
with
a
hierarchical
memory
system
that
consists
of
separate
instruction
and
data
caches
followed
by
main
memory
You
are
using
the
MIPS
multicycle
processor
from
Figure
running
at
GHz
a
Suppose
the
instruction
cache
is
perfect
i
e
always
hits
but
the
data
cache
has
a
miss
rate
On
a
cache
miss
the
processor
stalls
for
ns
to
access
main
memory
then
resumes
normal
operation
Taking
cache
misses
into
account
what
is
the
average
memory
access
time
b
How
many
clock
cycles
per
instruction
CPI
on
average
are
required
for
load
and
store
word
instructions
considering
the
non
ideal
memory
system
c
Consider
the
benchmark
application
of
Example
that
has
loads
stores
branches
jumps
and
R
type
instructions
Taking
the
non
ideal
memory
system
into
account
what
is
the
average
CPI
for
this
benchmark
d
Now
suppose
that
the
instruction
cache
is
also
non
ideal
and
has
a
miss
rate
What
is
the
average
CPI
for
the
benchmark
in
part
c
Take
into
account
both
instruction
and
data
cache
misses
Exercise
If
a
computer
uses
bit
virtual
addresses
how
much
virtual
memory
can
it
access
Note
that
bytes
terabyte
bytes
petabyte
and
bytes
exabyte
Exercise
A
supercomputer
designer
chooses
to
spend
million
on
DRAM
and
the
same
amount
on
hard
disks
for
virtual
memory
Using
the
prices
from
Figure
how
much
physical
and
virtual
memory
will
the
computer
have
How
many
bits
of
physical
and
virtual
addresses
are
necessary
to
access
this
memory
CHAPTER
EIGHT
Memory
Systems
Data
from
Patterson
and
Hennessy
Computer
Organization
and
Design
rd
Edition
Morgan
Kaufmann
Used
with
permission
Cha
Exercise
Consider
a
virtual
memory
system
that
can
address
a
total
of
bytes
You
have
unlimited
hard
disk
space
but
are
limited
to
only
MB
of
semiconductor
physical
memory
Assume
that
virtual
and
physical
pages
are
each
KB
in
size
a
How
many
bits
is
the
physical
address
b
What
is
the
maximum
number
of
virtual
pages
in
the
system
c
How
many
physical
pages
are
in
the
system
d
How
many
bits
are
the
virtual
and
physical
page
numbers
e
Suppose
that
you
come
up
with
a
direct
mapped
scheme
that
maps
virtual
pages
to
physical
pages
The
mapping
uses
the
least
significant
bits
of
the
virtual
page
number
to
determine
the
physical
page
number
How
many
virtual
pages
are
mapped
to
each
physical
page
Why
is
this
direct
mapping
a
bad
plan
f
Clearly
a
more
flexible
and
dynamic
scheme
for
translating
virtual
addresses
into
physical
addresses
is
required
than
the
one
described
in
part
d
Suppose
you
use
a
page
table
to
store
mappings
translations
from
virtual
page
number
to
physical
page
number
How
many
page
table
entries
will
the
page
table
contain
g
Assume
that
in
addition
to
the
physical
page
number
each
page
table
entry
also
contains
some
status
information
in
the
form
of
a
valid
bit
V
and
a
dirty
bit
D
How
many
bytes
long
is
each
page
table
entry
Round
up
to
an
integer
number
of
bytes
h
Sketch
the
layout
of
the
page
table
What
is
the
total
size
of
the
page
table
in
bytes
Exercise
You
decide
to
speed
up
the
virtual
memory
system
of
Exercise
by
using
a
translation
lookaside
buffer
TLB
Suppose
your
memory
system
has
the
characteristics
shown
in
Table
The
TLB
and
cache
miss
rates
indicate
how
often
the
requested
entry
is
not
found
The
main
memory
miss
rate
indicates
how
often
page
faults
occur
Exercises
Table
Memory
characteristics
Memory
Unit
Access
Time
Cycles
Miss
Rate
TLB
cache
main
memory
disk
a
What
is
the
average
memory
access
time
of
the
virtual
memory
system
before
and
after
adding
the
TLB
Assume
that
the
page
table
is
always
resident
in
physical
memory
and
is
never
held
in
the
data
cache
b
If
the
TLB
has
entries
how
big
in
bits
is
the
TLB
Give
numbers
for
data
physical
page
number
tag
virtual
page
number
and
valid
bits
of
each
entry
Show
your
work
clearly
c
Sketch
the
TLB
Clearly
label
all
fields
and
dimensions
d
What
size
SRAM
would
you
need
to
build
the
TLB
described
in
part
c
Give
your
answer
in
terms
of
depth
width
Exercise
Suppose
the
MIPS
multicycle
processor
described
in
Section
uses
a
virtual
memory
system
a
Sketch
the
location
of
the
TLB
in
the
multicycle
processor
schematic
b
Describe
how
adding
a
TLB
affects
processor
performance
Exercise
The
virtual
memory
system
you
are
designing
uses
a
single
level
page
table
built
from
dedicated
hardware
SRAM
and
associated
logic
It
supports
bit
virtual
addresses
bit
physical
addresses
and
byte
KB
pages
Each
page
table
entry
contains
a
physical
page
number
a
valid
bit
V
and
a
dirty
bit
D
a
What
is
the
total
size
of
the
page
table
in
bits
b
The
operating
system
team
proposes
reducing
the
page
size
from
to
KB
but
the
hardware
engineers
on
your
team
object
on
the
grounds
of
added
hardware
cost
Explain
their
objection
c
The
page
table
is
to
be
integrated
on
the
processor
chip
along
with
the
on
chip
cache
The
on
chip
cache
deals
only
with
physical
not
virtual
addresses
Is
it
possible
to
access
the
appropriate
set
of
the
on
chip
cache
concurrently
with
the
page
table
access
for
a
given
memory
access
Explain
briefly
the
relationship
that
is
necessary
for
concurrent
access
to
the
cache
set
and
page
table
entry
d
Is
it
possible
to
perform
the
tag
comparison
in
the
on
chip
cache
concurrently
with
the
page
table
access
for
a
given
memory
access
Explain
briefly
Exercise
Describe
a
scenario
in
which
the
virtual
memory
system
might
affect
how
an
application
is
written
Be
sure
to
include
a
discussion
of
how
the
page
size
and
physical
memory
size
affect
the
performance
of
the
application
CHAPTER
EIGHT
Memory
Systems
Exercise
Suppose
you
own
a
personal
computer
PC
that
uses
bit
virtual
addresses
a
What
is
the
maximum
amount
of
virtual
memory
space
each
program
can
use
b
How
does
the
size
of
your
PC
s
hard
disk
affect
performance
c
How
does
the
size
of
your
PC
s
physical
memory
affect
performance
Exercise
Use
MIPS
memory
mapped
I
O
to
interact
with
a
user
Each
time
the
user
presses
a
button
a
pattern
of
your
choice
displays
on
five
light
emitting
diodes
LEDs
Suppose
the
input
button
is
mapped
to
address
xFFFFFF
and
the
LEDs
are
mapped
to
address
xFFFFFF
When
the
button
is
pushed
its
output
is
otherwise
it
is
a
Write
MIPS
code
to
implement
this
functionality
b
Draw
a
schematic
similar
to
Figure
for
this
memory
mapped
I
O
system
c
Write
HDL
code
to
implement
the
address
decoder
for
your
memorymapped
I
O
system
Exercise
Finite
state
machines
FSMs
like
the
ones
you
built
in
Chapter
can
also
be
implemented
in
software
a
Implement
the
traffic
light
FSM
from
Figure
using
MIPS
assembly
code
The
inputs
TA
and
TB
are
memory
mapped
to
bit
and
bit
respectively
of
address
xFFFFF
The
two
bit
outputs
LA
and
LB
are
mapped
to
bits
and
bits
respectively
of
address
xFFFFF
Assume
one
hot
output
encodings
for
each
light
LA
and
LB
red
is
yellow
is
and
green
is
b
Draw
a
schematic
similar
to
Figure
for
this
memory
mapped
I
O
system
c
Write
HDL
code
to
implement
the
address
decoder
for
your
memorymapped
I
O
system
Exercises
Interview
Questions
The
following
exercises
present
questions
that
have
been
asked
on
interviews
Question
Explain
the
difference
between
direct
mapped
set
associative
and
fully
associative
caches
For
each
cache
type
describe
an
application
for
which
that
cache
type
will
perform
better
than
the
other
two
Question
Explain
how
virtual
memory
systems
work
Question
Explain
the
advantages
and
disadvantages
of
using
a
virtual
memory
system
Question
Explain
how
cache
performance
might
be
affected
by
the
virtual
page
size
of
a
memory
system
Question
Can
addresses
used
for
memory
mapped
I
O
be
cached
Explain
why
or
why
not
CHAPTER
EIGHT
Memory
Systems
A
A
Introduction
A
xx
Logic
A
Programmable
Logic
A
Application
Specific
Integrated
Circuits
A
Data
Sheets
A
Logic
Families
A
Packaging
and
Assembly
A
Transmission
Lines
A
Economics
Digital
System
Implementation
A
INTRODUCTION
This
appendix
introduces
practical
issues
in
the
design
of
digital
systems
The
material
in
this
appendix
is
not
necessary
for
understanding
the
rest
of
the
book
However
it
seeks
to
demystify
the
process
of
building
real
digital
systems
Moreover
we
believe
that
the
best
way
to
understand
digital
systems
is
to
build
and
debug
them
yourself
in
the
laboratory
Digital
systems
are
usually
built
using
one
or
more
chips
One
strategy
is
to
connect
together
chips
containing
individual
logic
gates
or
larger
elements
such
as
arithmetic
logical
units
ALUs
or
memories
Another
is
to
use
programmable
logic
which
contains
generic
arrays
of
circuitry
that
can
be
programmed
to
perform
specific
logic
functions
Yet
a
third
is
to
design
a
custom
integrated
circuit
containing
the
specific
logic
necessary
for
the
system
These
three
strategies
offer
trade
offs
in
cost
speed
power
consumption
and
design
time
that
are
explored
in
the
following
sections
This
appendix
also
examines
the
physical
packaging
and
assembly
of
circuits
the
transmission
lines
that
connect
the
chips
and
the
economics
of
digital
systems
A
XX
LOGIC
In
the
s
and
s
many
digital
systems
were
built
from
simple
chips
each
containing
a
handful
of
logic
gates
For
example
the
chip
contains
six
NOT
gates
the
contains
four
AND
gates
and
the
contains
two
flip
flops
These
chips
are
collectively
referred
to
as
xx
series
logic
They
were
sold
by
many
manufacturers
typically
for
to
cents
per
chip
These
chips
are
now
largely
obsolete
but
they
are
still
handy
for
simple
digital
systems
or
class
projects
because
they
are
so
inexpensive
and
easy
to
use
xx
series
chips
are
commonly
sold
in
pin
dual
inline
packages
DIPs
LS
inverter
chip
in
a
pin
dual
inline
package
The
part
number
is
on
the
first
line
LS
indicates
the
logic
family
see
Section
A
The
N
suffix
indicates
a
DIP
package
The
large
S
is
the
logo
of
the
manufacturer
Signetics
The
bottom
two
lines
of
gibberish
are
codes
indicating
the
batch
in
which
the
chip
was
manufactured
A
Logic
Gates
Figure
A
shows
the
pinout
diagrams
for
a
variety
of
popular
xx
series
chips
containing
basic
logic
gates
These
are
sometimes
called
small
scale
integration
SSI
chips
because
they
are
built
from
a
few
transistors
The
pin
packages
typically
have
a
notch
at
the
top
or
a
dot
on
the
top
left
to
indicate
orientation
Pins
are
numbered
starting
with
in
the
upper
left
and
going
counterclockwise
around
the
package
The
chips
need
to
receive
power
VDD
V
and
ground
GND
V
at
pins
and
respectively
The
number
of
logic
gates
on
the
chip
is
determined
by
the
number
of
pins
Note
that
pins
and
of
the
chip
are
not
connected
NC
to
anything
The
flip
flop
has
the
usual
D
CLK
and
Q
terminals
It
also
has
a
complementary
output
Moreover
it
receives
asynchronous
set
also
called
preset
or
PRE
and
reset
also
called
clear
or
CLR
signals
These
are
active
low
in
other
words
the
flop
sets
when
resets
when
and
operates
normally
when
A
Other
Functions
The
xx
series
also
includes
somewhat
more
complex
logic
functions
including
those
shown
in
Figures
A
and
A
These
are
called
mediumscale
integration
MSI
chips
Most
use
larger
packages
to
accommodate
more
inputs
and
outputs
Power
and
ground
are
still
provided
at
the
upper
right
and
lower
left
respectively
of
each
chip
A
general
functional
description
is
provided
for
each
chip
See
the
manufacturer
s
data
sheets
for
complete
descriptions
A
PROGRAMMABLE
LOGIC
Programmable
logic
consists
of
arrays
of
circuitry
that
can
be
configured
to
perform
specific
logic
functions
We
have
already
introduced
three
forms
of
programmable
logic
programmable
read
only
memories
PROMs
programmable
logic
arrays
PLAs
and
field
programmable
gate
arrays
FPGAs
This
section
shows
chip
implementations
for
each
of
these
Configuration
of
these
chips
may
be
performed
by
blowing
on
chip
fuses
to
connect
or
disconnect
circuit
elements
This
is
called
one
time
programmable
OTP
logic
because
once
a
fuse
is
blown
it
cannot
be
restored
Alternatively
the
configuration
may
be
stored
in
a
memory
that
can
be
reprogrammed
at
will
Reprogrammable
logic
is
convenient
in
the
laboratory
because
the
same
chip
can
be
reused
during
development
A
PROMs
As
discussed
in
Section
PROMs
can
be
used
as
lookup
tables
A
N
word
M
bit
PROM
can
be
programmed
to
perform
any
combinational
function
of
N
inputs
and
M
outputs
Design
changes
simply
CLR
PRE
CLR
PRE
Q
APPENDIX
A
Digital
System
Implementation
Appendi
A
Programmable
Logic
Q
D
Q
GND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
NAND
GND
Y
VDD
A
B
Y
A
B
Y
B
A
Y
B
A
NOR
GND
A
VDD
Y
A
Y
A
Y
A
Y
A
Y
A
Y
NOT
GND
A
VDD
B
A
B
C
Y
C
Y
C
B
A
Y
AND
GND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
AND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
XOR
GND
A
VDD
B
Y
A
B
Y
B
A
Y
B
A
Y
OR
GND
GND
A
VDD
B
C
D
Y
D
C
NC
B
A
Y
AND
NC
GND
VDD
D
Q
FLOP
CLK
CLR
PRE
Q
CLR
D
PRE
Q
Q
CLK
reset
set
D
Q
Q
reset
set
Figure
A
Common
xx
series
logic
gates
APPENDIX
A
Digital
System
Implementation
CLR
CLK
D
D
D
D
ENP
GND
Counter
VDD
Q
RCO
LOAD
ENT
Q
Q
Q
always
posedge
CLK
if
CLRb
Q
b
else
if
LOADb
Q
D
else
if
ENP
ENT
Q
Q
assign
RCO
Q
b
ENT
bit
Counter
CLK
clock
Q
counter
output
D
parallel
input
CLRb
async
reset
sync
reset
LOADb
load
Q
from
D
ENP
ENT
enables
RCO
ripple
carry
out
G
S
D
D
D
D
Y
GND
Mux
VDD
G
S
D
D
D
D
Y
always
Gb
S
D
if
Gb
Y
else
Y
D
S
always
Gb
S
D
if
Gb
Y
else
Y
D
S
Two
Multiplexers
D
data
S
select
Y
output
Gb
enable
A
A
A
G
A
G
B
G
Y
GND
Decoder
VDD
Y
Y
Y
Y
Y
Y
Y
Decoder
A
address
output
G
active
high
enable
G
active
low
enables
G
G
A
G
B
A
Y
x
x
xxx
x
xxx
xxx
Y
S
D
D
D
D
Y
GND
Mux
VDD
G
D
D
Y
D
D
Y
always
if
Gb
Y
else
Y
S
D
D
if
Gb
Y
else
Y
S
D
D
if
Gb
Y
else
Y
S
D
D
if
Gb
Y
else
Y
S
D
D
Four
Multiplexers
D
data
S
select
Y
output
Gb
enable
EN
A
Y
A
Y
GND
Tristate
Buffer
VDD
EN
Y
A
Y
A
Y
A
A
Y
A
Y
Y
A
assign
Y
ENb
bzzzz
A
assign
Y
ENB
bzzzz
A
bit
Tristate
Buffer
A
input
Y
output
ENb
enable
always
posedge
clk
if
ENb
Q
D
EN
Q
D
D
Q
GND
Register
VDD
Q
D
D
Q
Q
D
D
Q
D
D
Q
Q
CLK
bit
Enableable
Register
CLK
clock
D
data
Q
output
ENb
enable
Yb
Figure
A
Medium
scale
integration
chips
Note
Verilog
variable
names
cannot
start
with
numbers
but
the
names
in
the
example
code
in
Figure
A
are
chosen
to
match
the
manufacturer
s
data
sheet
A
Programmable
Logic
bit
ALU
A
B
Y
output
F
function
select
M
mode
select
Cbn
carry
in
Cbnplus
carry
out
AeqB
equality
in
some
modes
X
Y
carry
lookahead
adder
outputs
B
A
S
S
S
S
Cn
M
F
F
F
GND
ALU
VDD
A
B
A
B
A
B
Y
Cn
X
A
B
F
D
D
LT
RBO
RBI
D
D
GND
Segment
Decoder
VDD
f
g
a
b
c
d
e
segment
Display
Decoder
D
data
a
f
segments
low
ON
LTb
light
test
RBIb
ripple
blanking
in
RBOb
ripple
blanking
out
RBO
LT
RBI
D
a
b
c
d
e
f
g
x
x
x
x
x
x
a
b
c
d
e
f
g
B
AltBin
AeqBin
AgtBin
AgtBout
AeqBout
AltBout
GND
Comparator
VDD
A
B
A
A
B
A
B
always
if
A
B
A
B
AgtBin
begin
AgtBout
AeqBout
AltBout
end
else
if
A
B
A
B
AltBin
begin
AgtBout
AeqBout
AltBout
end
else
begin
AgtBout
AeqBout
AltBout
end
bit
Comparator
A
B
data
relin
input
relation
relout
output
relation
always
case
F
Y
M
A
A
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
b
b
Cbn
Y
M
A
B
A
A
B
Cbn
Y
M
B
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
B
A
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
B
A
B
A
B
Cbn
Y
M
A
B
A
B
Cbn
Y
M
A
A
Cbn
Y
M
A
B
A
B
A
Cbn
Y
M
A
B
A
B
A
Cbn
Y
M
A
A
Cbn
endcase
inputs
Figure
A
More
medium
scale
integration
MSI
chips
involve
replacing
the
contents
of
the
PROM
rather
than
rewiring
connections
between
chips
Lookup
tables
are
useful
for
small
functions
but
become
prohibitively
expensive
as
the
number
of
inputs
grows
For
example
the
classic
KB
Kb
erasable
PROM
EPROM
is
shown
in
Figure
A
The
EPROM
has
address
lines
to
specify
one
of
the
K
words
and
data
lines
to
read
the
byte
of
data
at
that
word
The
chip
enable
and
output
enable
must
both
be
asserted
for
data
to
be
read
The
maximum
propagation
delay
is
ps
In
normal
operation
and
VPP
is
not
used
The
EPROM
is
usually
programmed
on
a
special
programmer
that
sets
applies
V
to
VPP
and
uses
a
special
sequence
of
inputs
to
configure
the
memory
Modern
PROMs
are
similar
in
concept
but
have
much
larger
capacities
and
more
pins
Flash
memory
is
the
cheapest
type
of
PROM
selling
for
about
per
gigabyte
in
Prices
have
historically
declined
by
to
per
year
A
PLAs
As
discussed
in
Section
PLAs
contain
AND
and
OR
planes
to
compute
any
combinational
function
written
in
sum
of
products
form
The
AND
and
OR
planes
can
be
programmed
using
the
same
techniques
for
PROMs
A
PLA
has
two
columns
for
each
input
and
one
column
for
each
output
It
has
one
row
for
each
minterm
This
organization
is
more
efficient
than
a
PROM
for
many
functions
but
the
array
still
grows
excessively
large
for
functions
with
numerous
I
Os
and
minterms
Many
different
manufacturers
have
extended
the
basic
PLA
concept
to
build
programmable
logic
devices
PLDs
that
include
registers
PGM
PGM
APPENDIX
A
Digital
System
Implementation
A
VPP
A
A
A
A
A
A
VDD
PGM
NC
A
A
A
OE
A
A
A
D
D
D
GND
CE
D
D
D
D
D
assign
D
CEb
OEb
ROM
A
bZ
KB
EPROM
A
address
input
D
data
output
CEb
chip
enable
OEb
output
enable
PGMb
program
VPP
program
voltage
NC
no
connection
Figure
A
KB
EPROM
Ap
The
V
is
one
of
the
most
popular
classic
PLDs
It
has
dedicated
input
pins
and
outputs
The
outputs
can
come
directly
from
the
PLA
or
from
clocked
registers
on
the
chip
The
outputs
can
also
be
fed
back
into
the
PLA
Thus
the
V
can
directly
implement
FSMs
with
up
to
inputs
outputs
and
bits
of
state
The
V
costs
about
in
quantities
of
PLDs
have
been
rendered
mostly
obsolete
by
the
rapid
improvements
in
capacity
and
cost
of
FPGAs
A
FPGAs
As
discussed
in
Section
FPGAs
consist
of
arrays
of
configurable
logic
blocks
CLBs
connected
together
with
programmable
wires
The
CLBs
contain
small
lookup
tables
and
flip
flops
FPGAs
scale
gracefully
to
extremely
large
capacities
with
thousands
of
lookup
tables
Xilinx
and
Altera
are
two
of
the
leading
FPGA
manufacturers
Lookup
tables
and
programmable
wires
are
flexible
enough
to
implement
any
logic
function
However
they
are
an
order
of
magnitude
less
efficient
in
speed
and
cost
chip
area
than
hard
wired
versions
of
the
same
functions
Thus
FPGAs
often
include
specialized
blocks
such
as
memories
multipliers
and
even
entire
microprocessors
Figure
A
shows
the
design
process
for
a
digital
system
on
an
FPGA
The
design
is
usually
specified
with
a
hardware
description
language
HDL
although
some
FPGA
tools
also
support
schematics
The
design
is
then
simulated
Inputs
are
applied
and
compared
against
expected
outputs
to
verify
that
the
logic
is
correct
Usually
some
debugging
is
required
Next
logic
synthesis
converts
the
HDL
into
Boolean
functions
Good
synthesis
tools
produce
a
schematic
of
the
functions
and
the
prudent
designer
examines
these
schematics
as
well
as
any
warnings
produced
during
synthesis
to
ensure
that
the
desired
logic
was
produced
Sometimes
sloppy
coding
leads
to
circuits
that
are
much
larger
than
intended
or
to
circuits
with
asynchronous
logic
When
the
synthesis
results
are
good
the
FPGA
tool
maps
the
functions
onto
the
CLBs
of
a
specific
chip
The
place
and
route
tool
determines
which
functions
go
in
which
lookup
tables
and
how
they
are
wired
together
Wire
delay
increases
with
length
so
critical
circuits
should
be
placed
close
together
If
the
design
is
too
big
to
fit
on
the
chip
it
must
be
reengineered
Timing
analysis
compares
the
timing
constraints
e
g
an
intended
clock
speed
of
MHz
against
the
actual
circuit
delays
and
reports
any
errors
If
the
logic
is
too
slow
it
may
have
to
be
redesigned
or
pipelined
differently
When
the
design
is
correct
a
file
is
generated
specifying
the
contents
of
all
the
CLBs
and
the
programming
of
all
the
wires
on
the
FPGA
Many
FPGAs
store
this
configuration
information
in
static
RAM
that
must
be
reloaded
each
time
the
FPGA
is
turned
on
The
FPGA
can
download
this
information
from
a
computer
in
the
A
Programmable
Logic
Design
Entry
Synthesis
Logic
Verification
Mapping
Timing
Analysis
Configure
FPGA
Place
and
Route
Debug
Debug
Too
big
Too
slow
Figure
A
FPGA
design
flow
laboratory
or
can
read
it
from
a
nonvolatile
ROM
when
power
is
first
applied
Example
A
FPGA
TIMING
ANALYSIS
Alyssa
P
Hacker
is
using
an
FPGA
to
implement
an
M
M
sorter
with
a
color
sensor
and
motors
to
put
red
candy
in
one
jar
and
green
candy
in
another
Her
design
is
implemented
as
an
FSM
and
she
is
using
a
Spartan
XC
S
FPGA
a
chip
from
the
Spartan
series
family
According
to
the
data
sheet
the
FPGA
has
the
timing
characteristics
shown
in
Table
A
Assume
that
the
design
is
small
enough
that
wire
delay
is
negligible
Alyssa
would
like
her
FSM
to
run
at
MHz
What
is
the
maximum
number
of
CLBs
on
the
critical
path
What
is
the
fastest
speed
at
which
her
FSM
could
possibly
run
SOLUTION
At
MHz
the
cycle
time
Tc
is
ns
Alyssa
uses
Equation
figure
to
out
the
minimum
combinational
propagation
delay
tpd
at
this
cycle
time
tpd
ns
ns
ns
ns
A
Alyssa
s
FSM
can
use
at
most
consecutive
CLBs
to
implement
the
next
state
logic
The
fastest
speed
at
which
an
FSM
will
run
on
a
Spartan
FPGA
is
when
it
is
using
a
single
CLB
for
the
next
state
logic
The
minimum
cycle
time
is
Tc
ns
ns
ns
ns
A
Therefore
the
maximum
frequency
is
MHz
APPENDIX
A
Digital
System
Implementation
Table
A
Spartan
XC
S
timing
name
value
ns
tpcq
tsetup
thold
tpd
per
CLB
tskew
Xilinx
advertises
the
XC
S
E
FPGA
with
lookup
tables
and
flip
flops
for
in
quantities
of
in
In
more
modest
quantities
medium
sized
FPGAs
typically
cost
about
and
the
largest
Ap
FPGAs
cost
hundreds
or
even
thousands
of
dollars
The
cost
has
declined
at
approximately
per
year
so
FPGAs
are
becoming
extremely
popular
A
APPLICATION
SPECIFIC
INTEGRATED
CIRCUITS
Application
specific
integrated
circuits
ASICs
are
chips
designed
for
a
particular
purpose
Graphics
accelerators
network
interface
chips
and
cell
phone
chips
are
common
examples
of
ASICS
The
ASIC
designer
places
transistors
to
form
logic
gates
and
wires
the
gates
together
Because
the
ASIC
is
hardwired
for
a
specific
function
it
is
typically
several
times
faster
than
an
FPGA
and
occupies
an
order
of
magnitude
less
chip
area
and
hence
cost
than
an
FPGA
with
the
same
function
However
the
masks
specifying
where
transistors
and
wires
are
located
on
the
chip
cost
hundreds
of
thousands
of
dollars
to
produce
The
fabrication
process
usually
requires
to
weeks
to
manufacture
package
and
test
the
ASICs
If
errors
are
discovered
after
the
ASIC
is
manufactured
the
designer
must
correct
the
problem
generate
new
masks
and
wait
for
another
batch
of
chips
to
be
fabricated
Hence
ASICs
are
suitable
only
for
products
that
will
be
produced
in
large
quantities
and
whose
function
is
well
defined
in
advance
Figure
A
shows
the
ASIC
design
process
which
is
similar
to
the
FPGA
design
process
of
Figure
A
Logic
verification
is
especially
important
because
correction
of
errors
after
the
masks
are
produced
is
expensive
Synthesis
produces
a
netlist
consisting
of
logic
gates
and
connections
between
the
gates
the
gates
in
this
netlist
are
placed
and
the
wires
are
routed
between
gates
When
the
design
is
satisfactory
masks
are
generated
and
used
to
fabricate
the
ASIC
A
single
speck
of
dust
can
ruin
an
ASIC
so
the
chips
must
be
tested
after
fabrication
The
fraction
of
manufactured
chips
that
work
is
called
the
yield
it
is
typically
to
depending
on
the
size
of
the
chip
and
the
maturity
of
the
manufacturing
process
Finally
the
working
chips
are
placed
in
packages
as
will
be
discussed
in
Section
A
A
DATA
SHEETS
Integrated
circuit
manufacturers
publish
data
sheets
that
describe
the
functions
and
performance
of
their
chips
It
is
essential
to
read
and
understand
the
data
sheets
One
of
the
leading
sources
of
errors
in
digital
systems
comes
from
misunderstanding
the
operation
of
a
chip
Data
sheets
are
usually
available
from
the
manufacturer
s
Web
site
If
you
cannot
locate
the
data
sheet
for
a
part
and
do
not
have
clear
documentation
from
another
source
don
t
use
the
part
Some
of
the
entries
in
the
data
sheet
may
be
cryptic
Often
the
manufacturer
publishes
data
books
containing
data
sheets
for
many
related
parts
The
A
Data
Sheets
Design
Entry
Synthesis
Logic
Verification
Timing
Analysis
Generate
Masks
Place
and
Route
Debug
Debug
Too
big
Too
slow
Fabricate
ASIC
Test
ASIC
Package
ASIC
Defective
Figure
A
ASIC
design
flow
beginning
of
the
data
book
has
additional
explanatory
information
This
information
can
usually
be
found
on
the
Web
with
a
careful
search
This
section
dissects
the
Texas
Instruments
TI
data
sheet
for
a
HC
inverter
chip
The
data
sheet
is
relatively
simple
but
illustrates
many
of
the
major
elements
TI
still
manufacturers
a
wide
variety
of
xx
series
chips
In
the
past
many
other
companies
built
these
chips
too
but
the
market
is
consolidating
as
the
sales
decline
Figure
A
shows
the
first
page
of
the
data
sheet
Some
of
the
key
sections
are
highlighted
in
blue
The
title
is
SN
HC
SN
HC
HEX
INVERTERS
HEX
INVERTERS
means
that
the
chip
contains
six
inverters
SN
indicates
that
TI
is
the
manufacturer
Other
manufacture
codes
include
MC
for
Motorola
and
DM
for
National
Semiconductor
You
can
generally
ignore
these
codes
because
all
of
the
manufacturers
build
compatible
xxseries
logic
HC
is
the
logic
family
high
speed
CMOS
The
logic
family
determines
the
speed
and
power
consumption
of
the
chip
but
not
the
function
For
example
the
HC
and
LS
chips
all
contain
six
inverters
but
they
differ
in
performance
and
cost
Other
logic
families
are
discussed
in
Section
A
The
xx
chips
operate
across
the
commercial
or
industrial
temperature
range
to
C
or
to
C
respectively
whereas
the
xx
chips
operate
across
the
military
temperature
range
to
C
and
sell
for
a
higher
price
but
are
otherwise
compatible
The
is
available
in
many
different
packages
and
it
is
important
to
order
the
one
you
intended
when
you
make
a
purchase
The
packages
are
distinguished
by
a
suffix
on
the
part
number
N
indicates
a
plastic
dual
inline
package
PDIP
which
fits
in
a
breadboard
or
can
be
soldered
in
through
holes
in
a
printed
circuit
board
Other
packages
are
discussed
in
Section
A
The
function
table
shows
that
each
gate
inverts
its
input
If
A
is
HIGH
H
Y
is
LOW
L
and
vice
versa
The
table
is
trivial
in
this
case
but
is
more
interesting
for
more
complex
chips
Figure
A
shows
the
second
page
of
the
data
sheet
The
logic
diagram
indicates
that
the
chip
contains
inverters
The
absolute
maximum
section
indicates
conditions
beyond
which
the
chip
could
be
destroyed
In
particular
the
power
supply
voltage
VCC
also
called
VDD
in
this
book
should
not
exceed
V
The
continuous
output
current
should
not
exceed
mA
The
thermal
resistance
or
impedance
JA
is
used
to
calculate
the
temperature
rise
caused
by
the
chip
s
dissipating
power
If
the
ambient
temperature
in
the
vicinity
of
the
chip
is
TA
and
the
chip
dissipates
Pchip
then
the
temperature
on
the
chip
itself
at
its
junction
with
the
package
is
TJ
TA
Pchip
JA
A
For
example
if
a
chip
in
a
plastic
DIP
package
is
operating
in
a
hot
box
at
C
and
consumes
mW
the
junction
temperature
will
APPENDIX
A
Digital
System
Implementation
A
A
Data
Sheets
SCLS
D
DECEMBER
REVISED
JULY
POST
OFFICE
BOX
DALLAS
TEXAS
Wide
Operating
Voltage
Range
of
V
to
V
Outputs
Can
Drive
Up
To
LSTTL
Loads
Low
Power
Consumption
A
Max
ICC
Typical
tpd
ns
mA
Output
Drive
at
V
Low
Input
Current
of
A
Max
A
Y
A
Y
A
Y
GND
VCC
A
Y
A
Y
A
Y
SN
HC
J
OR
W
PACKAGE
SN
HC
D
N
NS
OR
PW
PACKAGE
TOPVIEW
Y
NC
A
NC
Y
A
NC
Y
NC
A
Y
A
CN
Y
A
VCC
A
Y
DNG
CN
SN
HC
FK
PACKAGE
TOPVIEW
NC
No
internal
connection
description
ordering
information
ORDERING
INFORMATION
TA
PACKAGE
ORDERABLE
PARTNUMBER
TOP
SIDE
MARKING
PDIP
N
Tube
of
SN
HC
N
SN
HC
N
Tube
of
SN
HC
D
SOIC
D
Reel
of
SN
HC
DR
HC
Reel
of
SN
HC
DT
C
to
C
SOP
NS
Reel
of
SN
HC
NSR
HC
Tube
of
SN
HC
PW
TSSOP
PW
Reel
of
SN
HC
PWR
HC
Reel
of
SN
HC
PWT
CDIP
J
Tube
of
SNJ
HC
J
SNJ
HC
J
C
to
C
CFP
W
Tube
of
SNJ
HC
W
SNJ
HC
W
LCCC
FK
Tube
of
SNJ
HC
FK
SNJ
HC
FK
Package
drawings
standard
packing
quantities
thermal
data
symbolization
and
PCB
design
guidelines
are
available
at
www
ti
com
sc
package
FUNCTION
TABLE
each
inverter
INPUT
A
OUTPUT
Y
H
L
L
H
Please
be
aware
that
an
important
notice
concerning
availability
standard
warranty
and
use
in
critical
applications
of
Texas
Instruments
semiconductor
products
and
disclaimers
there
to
appears
at
the
end
of
this
data
sheet
PRODUCTION
DATA
information
is
current
as
of
publication
date
Copyright
c
Texas
Instruments
Incorporated
Products
conform
to
specifications
per
the
terms
of
Texas
Instruments
standard
warranty
Production
processing
does
not
necessarily
include
testing
of
all
parameters
On
products
compliant
to
MIL
PRF
all
parameters
are
tested
unless
otherwise
noted
On
all
other
products
production
processing
does
not
necessarily
include
testing
of
all
parameters
SN
HC
SN
HC
HEX
INVERTERS
The
HC
devices
contain
six
independent
inverters
They
perform
the
Boolean
function
Y
A
in
positive
logic
Figure
A
data
sheet
page
APPENDIX
A
Digital
System
Implementation
SN
HC
SN
HC
HEX
INVERTERS
SCLS
D
DECEMBER
REVISED
JULY
POST
OFFICE
BOX
DALLAS
TEXAS
logic
diagram
positive
logic
A
Y
absolute
maximum
ratings
over
operating
free
air
temperature
range
unless
otherwise
noted
Supply
voltage
range
VCC
V
to
V
Input
clamp
current
IIK
VI
or
VI
VCC
see
Note
mA
Output
clamp
current
IOK
VO
or
VO
VCC
see
Note
mA
Continuous
output
current
IO
VO
to
VCC
mA
Continuous
current
through
VCC
or
GND
mA
Package
thermal
impedance
JA
egakcapD
etoNees
C
W
N
egakcap
C
W
N
egakcapS
C
W
P
egakcapW
C
W
Storage
temperature
range
Tstg
C
to
C
Stresses
beyond
those
listed
under
absolute
maximum
ratings
may
cause
permanent
damage
to
the
device
These
are
stress
ratings
only
and
functional
operation
of
the
device
at
these
or
any
other
conditions
beyond
those
indicated
under
recommended
operating
conditions
is
not
implied
Exposure
to
absolute
maximum
rated
conditions
for
extended
periods
may
affect
device
reliability
NOTES
The
input
and
output
voltage
ratings
may
be
exceeded
if
the
input
and
output
current
ratings
are
observed
The
package
thermal
impedance
is
calculated
in
accordance
with
JESD
recommended
operating
conditions
see
Note
SN
HC
SN
HC
UNIT
MIN
NOM
MAX
MIN
NOM
MAX
UNIT
VCC
Supply
voltage
V
VCC
V
VIH
High
level
input
voltage
VCC
V
V
VCC
V
VCC
V
VIL
Low
level
input
voltage
VCC
V
V
VCC
V
VI
Input
voltage
VCC
VCC
V
VO
Output
voltage
VCC
VCC
V
VCC
V
t
v
Input
transition
rise
fall
time
VCC
V
ns
VCC
V
TA
Operating
free
air
temperature
C
NOTE
All
unused
inputs
of
the
device
must
be
held
at
VCC
or
GND
to
ensure
proper
device
operation
Refer
to
the
TI
application
report
Implications
of
Slow
or
Floating
CMOS
Inputs
literature
number
SCBA
TEXAS
INSTRUMENTS
Figure
A
datasheet
page
climb
to
C
W
C
W
C
Internal
power
dissipation
is
seldom
important
for
xx
series
chips
but
it
becomes
important
for
modern
chips
that
dissipate
tens
of
watts
or
more
The
recommended
operating
conditions
define
the
environment
in
which
the
chip
should
be
used
Within
these
conditions
the
chip
should
meet
specifications
These
conditions
are
more
stringent
than
the
absolute
maximums
For
example
the
power
supply
voltage
should
be
between
and
V
The
input
logic
levels
for
the
HC
logic
family
depend
on
VDD
Use
the
V
entries
when
VDD
V
to
allow
for
a
droop
in
the
power
supply
caused
by
noise
in
the
system
Figure
A
shows
the
third
page
of
the
data
sheet
The
electrical
characteristics
describe
how
the
device
performs
when
used
within
the
recommended
operating
conditions
if
the
inputs
are
held
constant
For
example
if
VCC
V
and
droops
to
V
and
the
output
current
IOH
IOL
does
not
exceed
A
VOH
V
and
VOL
V
in
the
worst
case
If
the
output
current
increases
the
output
voltages
become
less
ideal
because
the
transistors
on
the
chip
struggle
to
provide
the
current
The
HC
logic
family
uses
CMOS
transistors
that
draw
very
little
current
The
current
into
each
input
is
guaranteed
to
be
less
than
nA
and
is
typically
only
nA
at
room
temperature
The
quiescent
power
supply
current
IDD
drawn
while
the
chip
is
idle
is
less
than
A
Each
input
has
less
than
pF
of
capacitance
The
switching
characteristics
define
how
the
device
performs
when
used
within
the
recommended
operating
conditions
if
the
inputs
change
The
propagation
delay
tpd
is
measured
from
when
the
input
passes
through
VCC
to
when
the
output
passes
through
VCC
If
VCC
is
nominally
V
and
the
chip
drives
a
capacitance
of
less
than
pF
the
propagation
delay
will
not
exceed
ns
and
typically
will
be
much
faster
Recall
that
each
input
may
present
pF
so
the
chip
cannot
drive
more
than
five
identical
chips
at
full
speed
Indeed
stray
capacitance
from
the
wires
connecting
chips
cuts
further
into
the
useful
load
The
transition
time
also
called
the
rise
fall
time
is
measured
as
the
output
transitions
between
VCC
and
VCC
Recall
from
Section
that
chips
consume
both
static
and
dynamic
power
Static
power
is
low
for
HC
circuits
At
C
the
maximum
quiescent
supply
current
is
A
At
V
this
gives
a
static
power
consumption
of
mW
The
dynamic
power
depends
on
the
capacitance
being
driven
and
the
switching
frequency
The
has
an
internal
power
dissipation
capacitance
of
pF
per
inverter
If
all
six
inverters
on
the
switch
at
MHz
and
drive
external
loads
of
pF
then
the
dynamic
power
given
by
Equation
is
pF
pF
MHz
mW
and
the
maximum
total
power
is
mW
A
Data
Sheets
Appendi
APPENDIX
A
Digital
System
Implementation
SN
HC
SN
HC
HEX
INVERTERS
SCLS
D
DECEMBER
REVISED
JULY
POST
OFFICE
BOX
DALLAS
TEXAS
TEXAS
INSTRUMENTS
electrical
characteristics
over
recommended
operating
free
air
temperature
range
unless
otherwise
noted
PARAMETER
TEST
CONDITIONS
VCC
TA
C
SN
HC
SN
HC
UNIT
MIN
TYP
MAX
MIN
MAX
MIN
MAX
UNIT
V
I
OH
A
I
OL
A
V
VOH
VOL
VI
VIH
or
VIL
VI
VIH
or
VIL
VI
VCC
or
VI
VCC
or
V
V
I
OH
mA
I
OL
mA
V
I
OH
mA
I
OL
mA
V
V
V
V
V
V
V
I
I
V
nA
I
CC
I
O
V
A
Ci
Vto
V
pF
switching
characteristics
over
recommended
operating
free
air
temperature
range
CL
pF
unless
otherwise
noted
see
Figure
FROM
TO
VCC
TA
C
SN
HC
SN
HC
PARAMETER
UNIT
INPUT
OUTPUT
MIN
TYP
MAX
MIN
MAX
MIN
MAX
UNIT
V
t
pd
A
Y
V
ns
V
V
t
t
Y
V
ns
V
operating
characteristics
TA
C
PARAMETER
TEST
CONDITIONS
TYP
UNIT
Cpd
Power
dissipation
capacitance
per
inverter
No
load
pF
Figure
A
datasheet
page
A
LOGIC
FAMILIES
The
xx
series
logic
chips
have
been
manufactured
using
many
different
technologies
called
logic
families
that
offer
different
speed
power
and
logic
level
trade
offs
Other
chips
are
usually
designed
to
be
compatible
with
some
of
these
logic
families
The
original
chips
such
as
the
were
built
using
bipolar
transistors
in
a
technology
called
TransistorTransistor
Logic
TTL
Newer
technologies
add
one
or
more
letters
after
the
to
indicate
the
logic
family
such
as
LS
HC
or
AHCT
Table
A
summarizes
the
most
common
V
logic
families
Advances
in
bipolar
circuits
and
process
technology
led
to
the
Schottky
S
and
Low
Power
Schottky
LS
families
Both
are
faster
than
TTL
Schottky
draws
more
power
whereas
Low
power
Schottky
draws
less
Advanced
Schottky
AS
and
Advanced
Low
Power
Schottky
ALS
have
improved
speed
and
power
compared
to
S
and
LS
Fast
F
logic
is
faster
and
draws
less
power
than
AS
All
of
these
families
provide
more
current
for
LOW
outputs
than
for
HIGH
outputs
and
hence
have
asymmetric
logic
levels
They
conform
to
the
TTL
logic
levels
VIH
V
VIL
V
VOH
V
and
VOL
V
A
Logic
Families
Table
A
Typical
specifications
for
V
logic
families
CMOS
TTL
Bipolar
TTL
CMOS
Compatible
Characteristic
TTL
S
LS
AS
ALS
F
HC
AHC
HCT
AHCT
tpd
ns
VIH
V
VIL
V
VOH
V
VOL
V
IOH
mA
IOL
mA
IIL
mA
IIH
mA
IDD
mA
Cpd
pF
n
a
cost
US
obsolete
Per
unit
in
quantities
of
for
the
from
Texas
Instruments
in
Ap
As
CMOS
circuits
matured
in
the
s
and
s
they
became
popular
because
they
draw
very
little
power
supply
or
input
current
The
High
Speed
CMOS
HC
and
Advanced
High
Speed
CMOS
AHC
families
draw
almost
no
static
power
They
also
deliver
the
same
current
for
HIGH
and
LOW
outputs
They
conform
to
the
CMOS
logic
levels
VIH
V
VIL
V
VOH
V
and
VOL
V
Unfortunately
these
levels
are
incompatible
with
TTL
circuits
because
a
TTL
HIGH
output
of
V
may
not
be
recognized
as
a
legal
CMOS
HIGH
input
This
motivates
the
use
of
High
Speed
TTL
compatible
CMOS
HCT
and
Advanced
High
Speed
TTLcompatible
CMOS
AHCT
which
accept
TTL
input
logic
levels
and
generate
valid
CMOS
output
logic
levels
These
families
are
slightly
slower
than
their
pure
CMOS
counterparts
All
CMOS
chips
are
sensitive
to
electrostatic
discharge
ESD
caused
by
static
electricity
Ground
yourself
by
touching
a
large
metal
object
before
handling
CMOS
chips
lest
you
zap
them
The
xx
series
logic
is
inexpensive
The
newer
logic
families
are
often
cheaper
than
the
obsolete
ones
The
LS
family
is
widely
available
and
robust
and
is
a
popular
choice
for
laboratory
or
hobby
projects
that
have
no
special
performance
requirements
The
V
standard
collapsed
in
the
mid
s
when
transistors
became
too
small
to
withstand
the
voltage
Moreover
lower
voltage
offers
lower
power
consumption
Now
and
even
lower
voltages
are
commonly
used
The
plethora
of
voltages
raises
challenges
in
communicating
between
chips
with
different
power
supplies
Table
A
lists
some
of
the
low
voltage
logic
families
Not
all
xx
parts
are
available
in
all
of
these
logic
families
All
of
the
low
voltage
logic
families
use
CMOS
transistors
the
workhorse
of
modern
integrated
circuits
They
operate
over
a
wide
range
of
VDD
but
the
speed
degrades
at
lower
voltage
Low
Voltage
CMOS
LVC
logic
and
Advanced
Low
Voltage
CMOS
ALVC
logic
are
commonly
used
at
or
V
LVC
withstands
inputs
up
to
V
so
it
can
receive
inputs
from
V
CMOS
or
TTL
circuits
Advanced
Ultra
Low
Voltage
CMOS
AUC
is
commonly
used
at
or
V
and
is
exceptionally
fast
Both
ALVC
and
AUC
withstand
inputs
up
to
V
so
they
can
receive
inputs
from
V
circuits
FPGAs
often
offer
separate
voltage
supplies
for
the
internal
logic
called
the
core
and
for
the
input
output
I
O
pins
As
FPGAs
have
advanced
the
core
voltage
has
dropped
from
to
and
V
to
save
power
and
avoid
damaging
the
very
small
transistors
FPGAs
have
configurable
I
Os
that
can
operate
at
many
different
voltages
so
as
to
be
compatible
with
the
rest
of
the
system
APPENDIX
A
Digital
System
Implementation
Ap
A
PACKAGING
AND
ASSEMBLY
Integrated
circuits
are
typically
placed
in
packages
made
of
plastic
or
ceramic
The
packages
serve
a
number
of
functions
including
connecting
the
tiny
metal
I
O
pads
of
the
chip
to
larger
pins
in
the
package
for
ease
of
connection
protecting
the
chip
from
physical
damage
and
spreading
the
heat
generated
by
the
chip
over
a
larger
area
to
help
with
cooling
The
packages
are
placed
on
a
breadboard
or
printed
circuit
board
and
wired
together
to
assemble
the
system
Packages
Figure
A
shows
a
variety
of
integrated
circuit
packages
Packages
can
be
generally
categorized
as
through
hole
or
surface
mount
SMT
Through
hole
packages
as
their
name
implies
have
pins
that
can
be
inserted
through
holes
in
a
printed
circuit
board
or
into
a
socket
Dual
inline
packages
DIPs
have
two
rows
of
pins
with
inch
spacing
between
pins
Pin
grid
arrays
PGAs
support
more
pins
in
a
smaller
package
by
placing
the
pins
under
the
package
SMT
packages
are
soldered
directly
to
the
surface
of
a
printed
circuit
board
without
using
holes
Pins
on
SMT
parts
are
called
leads
The
thin
small
outline
package
TSOP
has
two
rows
of
closely
spaced
leads
typically
inch
spacing
Plastic
leaded
chip
carriers
PLCCs
have
J
shaped
leads
on
all
four
A
Packaging
and
Assembly
Table
A
Typical
specifications
for
low
voltage
logic
families
LVC
ALVC
AUC
Vdd
V
tpd
ns
VIH
V
VIL
V
VOH
V
VOL
V
IO
mA
II
mA
IDD
mA
Cpd
pF
cost
US
not
available
Delay
and
capacitance
not
available
at
the
time
of
writing
sides
with
inch
spacing
They
can
be
soldered
directly
to
a
board
or
placed
in
special
sockets
Quad
flat
packs
QFPs
accommodate
a
large
number
of
pins
using
closely
spaced
legs
on
all
four
sides
Ball
grid
arrays
BGAs
eliminate
the
legs
altogether
Instead
they
have
hundreds
of
tiny
solder
balls
on
the
underside
of
the
package
They
are
carefully
placed
over
matching
pads
on
a
printed
circuit
board
then
heated
so
that
the
solder
melts
and
joins
the
package
to
the
underlying
board
Breadboards
DIPs
are
easy
to
use
for
prototyping
because
they
can
be
placed
in
a
breadboard
A
breadboard
is
a
plastic
board
containing
rows
of
sockets
as
shown
in
Figure
A
All
five
holes
in
a
row
are
connected
together
Each
pin
of
the
package
is
placed
in
a
hole
in
a
separate
row
Wires
can
be
placed
in
adjacent
holes
in
the
same
row
to
make
connections
to
the
pin
Breadboards
often
provide
separate
columns
of
connected
holes
running
the
height
of
the
board
to
distribute
power
and
ground
Figure
A
shows
a
breadboard
containing
a
majority
gate
built
with
a
LS
AND
chip
and
a
LS
OR
chip
The
schematic
of
the
circuit
is
shown
in
Figure
A
Each
gate
in
the
schematic
is
labeled
with
the
chip
or
and
the
pin
numbers
of
the
inputs
and
outputs
see
Figure
A
Observe
that
the
same
connections
are
made
on
the
breadboard
The
inputs
are
connected
to
pins
and
of
the
chip
and
the
output
is
measured
at
pin
of
the
chip
Power
and
ground
are
connected
to
pins
and
respectively
of
each
chip
from
the
vertical
power
and
ground
columns
that
are
attached
to
the
banana
plug
receptacles
Vb
and
Va
Labeling
the
schematic
in
this
way
and
checking
off
connections
as
they
are
made
is
a
good
way
to
reduce
the
number
of
mistakes
made
during
breadboarding
Unfortunately
it
is
easy
to
accidentally
plug
a
wire
in
the
wrong
hole
or
have
a
wire
fall
out
so
breadboarding
requires
a
great
deal
of
care
and
usually
some
debugging
in
the
laboratory
Breadboards
are
suited
only
to
prototyping
not
production
APPENDIX
A
Digital
System
Implementation
Figure
A
Integrated
circuit
packages
Printed
Circuit
Boards
Instead
of
breadboarding
chip
packages
may
be
soldered
to
a
printed
circuit
board
PCB
The
PCB
is
formed
of
alternating
layers
of
conducting
copper
and
insulating
epoxy
The
copper
is
etched
to
form
wires
called
traces
Holes
called
vias
are
drilled
through
the
board
and
plated
with
metal
to
connect
between
layers
PCBs
are
usually
designed
with
computer
aided
design
CAD
tools
You
can
etch
and
drill
your
own
simple
boards
in
the
laboratory
or
you
can
send
the
board
design
to
a
specialized
factory
for
inexpensive
mass
production
Factories
have
turnaround
times
of
days
or
weeks
for
cheap
mass
production
runs
and
typically
charge
a
few
hundred
dollars
in
setup
fees
and
a
few
dollars
per
board
for
moderately
complex
boards
built
in
large
quantities
A
Packaging
and
Assembly
BC
Y
A
Figure
A
Majority
gate
schematic
with
chips
and
pins
identified
GND
Rows
of
pins
are
internally
connected
VDD
Figure
A
Majority
circuit
on
breadboard
APPENDIX
A
Digital
System
Implementation
Signal
Layer
Signal
Layer
Power
Plane
Ground
Plane
Copper
Trace
Insulator
Figure
A
Printed
circuit
board
cross
section
PCB
traces
are
normally
made
of
copper
because
of
its
low
resistance
The
traces
are
embedded
in
an
insulating
material
usually
a
green
fireresistant
plastic
called
FR
A
PCB
also
typically
has
copper
power
and
ground
layers
called
planes
between
signal
layers
Figure
A
shows
a
cross
section
of
a
PCB
The
signal
layers
are
on
the
top
and
bottom
and
the
power
and
ground
planes
are
embedded
in
the
center
of
the
board
The
power
and
ground
planes
have
low
resistance
so
they
distribute
stable
power
to
components
on
the
board
They
also
make
the
capacitance
and
inductance
of
the
traces
uniform
and
predictable
Figure
A
shows
a
PCB
for
a
s
vintage
Apple
II
computer
At
the
top
is
a
Motorola
microprocessor
Beneath
are
six
Kb
ROM
chips
forming
KB
of
ROM
containing
the
operating
system
Three
rows
of
eight
Kb
DRAM
chips
provide
KB
of
RAM
On
the
right
are
several
rows
of
xx
series
logic
for
memory
address
decoding
and
other
functions
The
lines
between
chips
are
traces
that
wire
the
chips
together
The
dots
at
the
ends
of
some
of
the
traces
are
vias
filled
with
metal
Putting
It
All
Together
Most
modern
chips
with
large
numbers
of
inputs
and
outputs
use
SMT
packages
especially
QFPs
and
BGAs
These
packages
require
a
printed
circuit
board
rather
than
a
breadboard
Working
with
BGAs
is
especially
challenging
because
they
require
specialized
assembly
equipment
Moreover
the
balls
cannot
be
probed
with
a
voltmeter
or
oscilloscope
during
debugging
in
the
laboratory
because
they
are
hidden
under
the
package
In
summary
the
designer
needs
to
consider
packaging
early
on
to
determine
whether
a
breadboard
can
be
used
during
prototyping
and
whether
BGA
parts
will
be
required
Professional
engineers
rarely
use
breadboards
when
they
are
confident
of
connecting
chips
together
correctly
without
experimentation
A
TRANSMISSION
LINES
We
have
assumed
so
far
that
wires
are
equipotential
connections
that
have
a
single
voltage
along
their
entire
length
Signals
actually
propagate
along
wires
at
the
speed
of
light
in
the
form
of
electromagnetic
waves
If
the
wires
are
short
enough
or
the
signals
change
slowly
the
equipotential
assumption
is
good
enough
When
the
wire
is
long
or
the
signal
is
very
fast
the
transmission
time
along
the
wire
becomes
important
to
accurately
determine
the
circuit
delay
We
must
model
such
wires
as
transmission
lines
in
which
a
wave
of
voltage
and
current
propagates
at
the
speed
of
light
When
the
wave
reaches
the
end
of
the
line
it
may
reflect
back
along
the
line
The
reflection
may
cause
noise
and
odd
behaviors
unless
steps
are
taken
to
limit
it
Hence
the
digital
designer
must
consider
transmission
line
behavior
to
accurately
account
for
the
delay
and
noise
effects
in
long
wires
Electromagnetic
waves
travel
at
the
speed
of
light
in
a
given
medium
which
is
fast
but
not
instantaneous
The
speed
of
light
depends
on
the
permittivity
and
permeability
of
the
medium
v
s
sLC
A
Transmission
Lines
Figure
A
Apple
II
circuit
board
The
capacitance
C
and
inductance
L
of
a
wire
are
related
to
the
permittivity
and
permeability
of
the
physical
medium
in
which
the
wire
is
located
Ap
The
speed
of
light
in
free
space
is
c
m
s
Signals
in
a
PCB
travel
at
about
half
this
speed
because
the
FR
insulator
has
four
times
the
permittivity
of
air
Thus
PCB
signals
travel
at
about
m
s
or
cm
ns
The
time
delay
for
a
signal
to
travel
along
a
transmission
line
of
length
l
is
td
l
v
A
The
characteristic
impedance
of
a
transmission
line
Z
pronounced
Z
naught
is
the
ratio
of
voltage
to
current
in
a
wave
traveling
along
the
line
Z
V
I
It
is
not
the
resistance
of
the
wire
a
good
transmission
line
in
a
digital
system
typically
has
negligible
resistance
Z
depends
on
the
inductance
and
capacitance
of
the
line
see
the
derivation
in
Section
A
and
typically
has
a
value
of
to
A
Figure
A
shows
the
symbol
for
a
transmission
line
The
symbol
resembles
a
coaxial
cable
with
an
inner
signal
conductor
and
an
outer
grounded
conductor
like
that
used
in
television
cable
wiring
The
key
to
understanding
the
behavior
of
transmission
lines
is
to
visualize
the
wave
of
voltage
propagating
along
the
line
at
the
speed
of
light
When
the
wave
reaches
the
end
of
the
line
it
may
be
absorbed
or
reflected
depending
on
the
termination
or
load
at
the
end
Reflections
travel
back
along
the
line
adding
to
the
voltage
already
on
the
line
Terminations
are
classified
as
matched
open
short
or
mismatched
The
following
subsections
explore
how
a
wave
propagates
along
the
line
and
what
happens
to
the
wave
when
it
reaches
the
termination
A
Matched
Termination
Figure
A
shows
a
transmission
line
of
length
l
with
a
matched
termination
which
means
that
the
load
impedance
ZL
is
equal
to
the
characteristic
impedance
Z
The
transmission
line
has
a
characteristic
impedance
of
One
end
of
the
line
is
connected
to
a
voltage
Z
v
L
C
APPENDIX
A
Digital
System
Implementation
I
V
Z
C
L
Figure
A
Transmission
line
symbol
Appendi
A
Transmission
Lines
VS
t
Z
length
l
td
l
v
A
C
B
l
ZL
Figure
A
Transmission
line
with
matched
termination
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
ZL
A
B
C
Figure
A
Equivalent
circuit
of
Figure
A
at
steady
state
source
through
a
switch
that
closes
at
time
t
The
other
end
is
connected
to
the
matched
load
This
section
analyzes
the
voltages
and
currents
at
points
A
B
and
C
at
the
beginning
of
the
line
one
third
of
the
length
along
the
line
and
at
the
end
of
the
line
respectively
Figure
A
shows
the
voltages
at
points
A
B
and
C
over
time
Initially
there
is
no
voltage
or
current
flowing
in
the
transmission
line
because
the
switch
is
open
At
time
t
the
switch
closes
and
the
voltage
source
launches
a
wave
with
voltage
V
VS
along
the
line
Because
the
characteristic
impedance
is
Z
the
wave
has
current
I
VS
Z
The
voltage
reaches
the
beginning
of
the
line
point
A
immediately
as
shown
in
Figure
A
a
The
wave
propagates
along
the
line
at
the
speed
of
light
At
time
td
the
wave
reaches
point
B
The
voltage
at
this
point
abruptly
rises
from
to
VS
as
shown
in
Figure
A
b
At
time
td
the
incident
wave
reaches
point
C
at
the
end
of
the
line
and
the
voltage
rises
there
too
All
of
the
current
I
flows
into
the
resistor
ZL
producing
a
voltage
across
the
resistor
of
ZLI
ZL
VS
Z
VS
because
ZL
Z
This
voltage
is
consistent
with
the
wave
flowing
along
the
transmission
line
Thus
the
wave
is
absorbed
by
the
load
impedance
and
the
transmission
line
reaches
its
steady
state
In
steady
state
the
transmission
line
behaves
like
an
ideal
equipotential
wire
because
it
is
after
all
just
a
wire
The
voltage
at
all
points
along
the
line
must
be
identical
Figure
A
shows
the
steady
state
equivalent
model
of
the
circuit
in
Figure
A
The
voltage
is
VS
everywhere
along
the
wire
Example
A
TRANSMISSION
LINE
WITH
MATCHED
SOURCE
AND
LOAD
TERMINATIONS
Figure
A
shows
a
transmission
line
with
matched
source
and
load
impedances
ZS
and
ZL
Plot
the
voltage
at
nodes
A
B
and
C
versus
time
When
does
the
system
reach
steady
state
and
what
is
the
equivalent
circuit
at
steady
state
SOLUTION
When
the
voltage
source
has
a
source
impedance
ZS
in
series
with
the
transmission
line
part
of
the
voltage
drops
across
ZS
and
the
remainder
a
t
t
d
VA
VS
c
t
VS
t
d
VC
b
t
VS
t
d
t
d
VB
Appendi
propagates
down
the
transmission
line
At
first
the
transmission
line
behaves
as
an
impedance
Z
because
the
load
at
the
end
of
the
line
cannot
possibly
influence
the
behavior
of
the
line
until
a
speed
of
light
delay
has
elapsed
Hence
by
the
voltage
divider
equation
the
incident
voltage
flowing
down
the
line
is
A
Thus
at
t
a
wave
of
voltage
is
sent
down
the
line
from
point
A
Again
the
signal
reaches
point
B
at
time
td
and
point
C
at
td
as
shown
in
Figure
A
All
of
the
current
is
absorbed
by
the
load
impedance
ZL
so
the
circuit
enters
steady
state
at
t
td
In
steady
state
the
entire
line
is
at
VS
just
as
the
steady
state
equivalent
circuit
in
Figure
A
would
predict
A
Open
Termination
When
the
load
impedance
is
not
equal
to
Z
the
termination
cannot
absorb
all
of
the
current
and
some
of
the
wave
must
be
reflected
Figure
A
shows
a
transmission
line
with
an
open
load
termination
No
current
can
flow
through
an
open
termination
so
the
current
at
point
C
must
always
be
The
voltage
on
the
line
is
initially
zero
At
t
the
switch
closes
and
a
wave
of
voltage
begins
propagating
down
the
line
Notice
that
this
initial
wave
is
the
same
as
that
of
Example
A
and
is
independent
of
the
termination
because
the
load
at
the
end
of
the
line
cannot
influence
the
behavior
at
the
beginning
until
at
least
td
has
elapsed
This
wave
reaches
point
B
at
td
and
point
C
at
td
as
shown
in
Figure
A
When
the
incident
wave
reaches
point
C
it
cannot
continue
forward
because
the
wire
is
open
It
must
instead
reflect
back
toward
the
source
The
reflected
wave
also
has
voltage
because
the
open
termination
reflects
the
entire
wave
The
voltage
at
any
point
is
the
sum
of
the
incident
and
reflected
waves
At
time
t
td
the
voltage
at
point
C
is
The
reflected
wave
reaches
point
B
at
td
and
point
A
at
td
When
it
reaches
point
A
the
wave
is
absorbed
by
the
source
termination
V
VS
VS
VS
V
VS
VVS
Z
Z
ZS
VS
V
VS
V
VS
Z
Z
ZS
VS
APPENDIX
A
Digital
System
Implementation
t
VA
a
VS
t
d
t
b
t
d
VS
VB
t
c
t
d
VC
VS
t
d
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
ZL
ZS
A
B
C
VS
Figure
A
Equivalent
circuit
of
Figure
A
at
steady
state
VS
t
Z
ZL
ZS
length
l
td
l
v
A
B
C
l
Figure
A
Transmission
line
with
matched
source
and
load
impedances
Appendix
A
qxd
A
Transmission
Lines
impedance
that
matches
the
characteristic
impedance
of
the
line
Thus
the
system
reaches
steady
state
at
time
t
td
and
the
transmission
line
becomes
equivalent
to
an
equipotential
wire
with
voltage
VS
and
current
I
A
Short
Termination
Figure
A
shows
a
transmission
line
terminated
with
a
short
circuit
to
ground
Thus
the
voltage
at
point
C
must
always
be
As
in
the
previous
examples
the
voltages
on
the
line
are
initially
When
the
switch
closes
a
wave
of
voltage
begins
propagating
down
the
line
Figure
A
When
it
reaches
the
end
of
the
line
it
must
reflect
with
opposite
polarity
The
reflected
wave
with
voltage
adds
to
the
incident
wave
ensuring
that
the
voltage
at
point
C
remains
The
reflected
wave
reaches
the
source
at
time
t
td
and
is
absorbed
by
the
source
impedance
At
this
point
the
system
reaches
steady
state
and
the
transmission
line
is
equivalent
to
an
equipotential
wire
with
voltage
V
V
VS
V
VS
t
VS
VS
t
d
t
d
t
d
VB
t
VS
t
d
VC
c
b
a
t
t
d
VA
VS
VS
t
d
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
t
Z
length
l
td
l
v
A
C
B
l
ZS
Figure
A
Transmission
line
with
open
load
termination
VS
t
Z
length
l
td
l
v
A
C
B
l
ZS
Figure
A
Transmission
line
with
short
termination
A
Mismatched
Termination
The
termination
impedance
is
said
to
be
mismatched
when
it
does
not
equal
the
characteristic
impedance
of
the
line
In
general
when
an
incident
wave
reaches
a
mismatched
termination
part
of
the
wave
is
absorbed
and
part
is
reflected
The
reflection
coefficient
kr
indicates
the
fraction
of
the
incident
wave
Vi
that
is
reflected
Vr
krVi
Section
A
derives
the
reflection
coefficient
using
conservation
of
current
arguments
It
shows
that
when
an
incident
wave
flowing
along
a
Appendi
transmission
line
of
characteristic
impedance
Z
reaches
a
termination
impedance
ZT
at
the
end
of
the
line
the
reflection
coefficient
is
A
Note
a
few
special
cases
If
the
termination
is
an
open
circuit
ZT
kr
because
the
incident
wave
is
entirely
reflected
so
the
current
out
the
end
of
the
line
remains
zero
If
the
termination
is
a
short
circuit
ZT
kr
because
the
incident
wave
is
reflected
with
negative
polarity
so
the
voltage
at
the
end
of
the
line
remains
zero
If
the
termination
is
a
matched
load
ZT
Z
kr
because
the
incident
wave
is
absorbed
Figure
A
illustrates
reflections
in
a
transmission
line
with
a
mismatched
load
termination
of
ZT
ZL
and
Z
so
kr
As
in
previous
examples
the
voltage
on
the
line
is
initially
When
the
switch
closes
a
wave
of
voltage
propagates
down
the
line
reaching
the
end
at
t
td
When
the
incident
wave
reaches
the
termination
at
the
end
of
the
line
one
fifth
of
the
wave
is
reflected
and
the
remaining
four
fifths
flows
into
the
load
impedance
Thus
the
reflected
wave
has
a
voltage
The
total
voltage
at
point
C
is
the
sum
of
the
incoming
and
reflected
voltages
At
t
td
the
reflected
wave
reaches
point
A
where
it
is
absorbed
by
the
matched
termination
ZS
Figure
A
plots
the
voltages
and
currents
along
the
line
Again
note
that
in
steady
state
in
this
case
at
time
t
td
the
transmission
line
is
equivalent
to
an
equipotential
wire
as
shown
in
Figure
A
At
steady
state
the
system
acts
like
a
voltage
divider
so
Reflections
can
occur
at
both
ends
of
the
transmission
line
Figure
A
shows
a
transmission
line
with
a
source
impedance
ZS
of
and
an
open
termination
at
the
load
The
reflection
coefficients
at
the
load
and
source
krL
and
krS
are
and
respectively
In
this
case
waves
reflect
off
both
ends
of
the
transmission
line
until
a
steady
state
is
reached
The
bounce
diagram
shown
in
Figure
A
helps
visualize
reflections
off
both
ends
of
the
transmission
line
The
horizontal
axis
represents
VA
VB
VC
VS
ZL
ZL
ZS
VS
VS
VC
VS
VS
VS
V
VS
VS
V
VS
kr
ZT
Z
ZT
Z
APPENDIX
A
Digital
System
Implementation
t
t
d
VA
VS
t
d
a
t
VS
t
d
t
d
t
d
VB
b
c
t
VS
t
d
VC
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
t
Z
length
l
td
l
v
A
C
B
l
ZL
ZS
Figure
A
Transmission
line
with
mismatched
termination
Appendix
A
qxd
distance
along
the
transmission
line
and
the
vertical
axis
represents
time
increasing
downward
The
two
sides
of
the
bounce
diagram
represent
the
source
and
load
ends
of
the
transmission
line
points
A
and
C
The
incoming
and
reflected
signal
waves
are
drawn
as
diagonal
lines
between
points
A
and
C
At
time
t
the
source
impedance
and
transmission
line
behave
as
a
voltage
divider
launching
a
voltage
wave
of
from
point
A
toward
point
C
At
time
t
td
the
signal
reaches
point
C
and
is
completely
reflected
krL
At
time
t
td
the
reflected
wave
of
reaches
point
A
and
is
reflected
with
a
reflection
coefficient
krS
to
produce
a
wave
of
traveling
toward
point
C
and
so
forth
The
voltage
at
a
given
time
at
any
point
on
the
transmission
line
is
the
sum
of
all
the
incident
and
reflected
waves
Thus
at
time
t
td
the
voltage
at
point
C
is
At
time
t
td
the
voltage
at
point
C
is
and
so
forth
Figure
A
plots
the
voltages
against
time
As
t
approaches
infinity
the
voltages
approach
steady
state
with
VA
VB
VC
VS
VS
VS
VS
VS
VS
VS
VS
VS
VS
VS
VS
A
Transmission
Lines
Figure
A
Voltage
waveforms
for
Figure
A
at
points
A
B
and
C
VS
t
length
l
td
l
v
A
C
B
l
ZS
Z
Figure
A
Transmission
line
with
mismatched
source
and
load
terminations
A
B
C
VS
ZS
ZL
Figure
A
Equivalent
circuit
of
Figure
A
at
steady
state
Voltage
A
C
VS
t
td
krS
B
VS
VS
VS
VS
VS
t
td
t
td
t
td
t
td
t
td
t
td
krL
Figure
A
Bounce
diagram
for
Figure
A
t
t
d
VA
VS
VS
t
d
a
t
VB
VS
VS
t
d
t
d
t
d
b
t
t
d
VC
c
VS
Appendix
A
q
A
When
to
Use
Transmission
Line
Models
Transmission
line
models
for
wires
are
needed
whenever
the
wire
delay
td
is
longer
than
a
fraction
e
g
of
the
edge
rates
rise
or
fall
times
of
a
signal
If
the
wire
delay
is
shorter
it
has
an
insignificant
effect
on
the
propagation
delay
of
the
signal
and
the
reflections
dissipate
while
the
signal
is
transitioning
If
the
wire
delay
is
longer
it
must
be
considered
in
order
to
accurately
predict
the
propagation
delay
and
waveform
of
the
signal
In
particular
reflections
may
distort
the
digital
characteristic
of
a
waveform
resulting
in
incorrect
logic
operations
Recall
that
signals
travel
on
a
PCB
at
about
cm
ns
For
TTL
logic
with
edge
rates
of
ns
wires
must
be
modeled
as
transmission
lines
only
if
they
are
longer
than
cm
ns
cm
ns
PCB
traces
are
usually
less
than
cm
so
most
traces
can
be
modeled
as
ideal
equipotential
wires
In
contrast
many
modern
chips
have
edge
rates
of
ns
or
less
so
traces
longer
than
about
cm
about
inches
must
be
modeled
as
transmission
lines
Clearly
use
of
edge
rates
that
are
crisper
than
necessary
just
causes
difficulties
for
the
designer
Breadboards
lack
a
ground
plane
so
the
electromagnetic
fields
of
each
signal
are
nonuniform
and
difficult
to
model
Moreover
the
fields
interact
with
other
signals
This
can
cause
strange
reflections
and
crosstalk
between
signals
Thus
breadboards
are
unreliable
above
a
few
megahertz
In
contrast
PCBs
have
good
transmission
lines
with
consistent
characteristic
impedance
and
velocity
along
the
entire
line
As
long
as
they
are
terminated
with
a
source
or
load
impedance
that
is
matched
to
the
impedance
of
the
line
PCB
traces
do
not
suffer
from
reflections
A
Proper
Transmission
Line
Terminations
There
are
two
common
ways
to
properly
terminate
a
transmission
line
shown
in
Figure
A
In
parallel
termination
Figure
A
a
the
driver
has
a
low
impedance
ZS
A
load
resistor
ZL
with
impedance
Z
is
placed
in
parallel
with
the
load
between
the
input
of
the
receiver
gate
and
APPENDIX
A
Digital
System
Implementation
Figure
A
Voltage
and
current
waveforms
for
Figure
A
t
VS
VA
td
VS
VS
td
td
a
t
VB
VS
VS
VS
VS
VS
td
td
td
td
td
td
b
t
VC
c
td
td
td
VS
VS
VS
Ap
ground
When
the
driver
switches
from
to
VDD
it
sends
a
wave
with
voltage
VDD
down
the
line
The
wave
is
absorbed
by
the
matched
load
termination
and
no
reflections
take
place
In
series
termination
Figure
A
b
a
source
resistor
Z
is
placed
in
series
with
the
driver
to
raise
the
source
impedance
to
Z
The
load
has
a
high
impedance
ZL
When
the
driver
switches
it
sends
a
wave
with
voltage
VDD
down
the
line
The
wave
reflects
at
the
open
circuit
load
and
returns
bringing
the
voltage
on
the
line
up
to
VDD
The
wave
is
absorbed
at
the
source
termination
Both
schemes
are
similar
in
that
the
voltage
at
the
receiver
transitions
from
to
VDD
at
t
td
just
as
one
would
desire
They
differ
in
power
consumption
and
in
the
waveforms
that
appear
elsewhere
along
the
line
Parallel
termination
dissipates
power
continuously
through
the
load
resistor
when
the
line
is
at
a
high
voltage
Series
termination
dissipates
no
DC
power
because
the
load
is
an
open
circuit
However
in
series
terminated
lines
points
near
the
middle
of
the
transmission
line
initially
see
a
voltage
of
VDD
until
the
reflection
returns
If
other
gates
are
attached
to
the
A
Transmission
Lines
driver
A
C
B
gate
b
receiver
gate
series
termination
resistor
ZL
t
t
td
t
VDD
VA
VB
VC
t
t
t
V
DD
VDD
VB
VC
A
C
B
a
driver
gate
receiver
gate
parallel
termination
resistor
ZS
Z
ZL
Z
ZS
Z
Z
VA
VDD
VDD
td
td
VDD
td
td
td
td
td
td
td
VDD
VDD
Figure
A
Termination
schemes
a
parallel
b
series
A
middle
of
the
line
they
will
momentarily
see
an
illegal
logic
level
Therefore
series
termination
works
best
for
point
to
point
communication
with
a
single
driver
and
a
single
receiver
Parallel
termination
is
better
for
a
bus
with
multiple
receivers
because
receivers
at
the
middle
of
the
line
never
see
an
illegal
logic
level
A
Derivation
of
Z
Z
is
the
ratio
of
voltage
to
current
in
a
wave
propagating
along
a
transmission
line
This
section
derives
Z
it
assumes
some
previous
knowledge
of
resistor
inductor
capacitor
RLC
circuit
analysis
Imagine
applying
a
step
voltage
to
the
input
of
a
semi
infinite
transmission
line
so
that
there
are
no
reflections
Figure
A
shows
the
semi
infinite
line
and
a
model
of
a
segment
of
the
line
of
length
dx
R
L
and
C
are
the
values
of
resistance
inductance
and
capacitance
per
unit
length
Figure
A
b
shows
the
transmission
line
model
with
a
resistive
component
R
This
is
called
a
lossy
transmission
line
model
because
energy
is
dissipated
or
lost
in
the
resistance
of
the
wire
However
this
loss
is
often
negligible
and
we
can
simplify
analysis
by
ignoring
the
resistive
component
and
treating
the
transmission
line
as
an
ideal
transmission
line
as
shown
in
Figure
A
c
Voltage
and
current
are
functions
of
time
and
space
throughout
the
transmission
line
as
given
by
Equations
A
and
A
A
A
Taking
the
space
derivative
of
Equation
A
and
the
time
derivative
of
Equation
A
and
substituting
gives
Equation
A
the
wave
equation
A
Z
is
the
ratio
of
voltage
to
current
in
the
transmission
line
as
illustrated
in
Figure
A
a
Z
must
be
independent
of
the
length
of
the
line
because
the
behavior
of
the
wave
cannot
depend
on
things
at
a
distance
Because
it
is
independent
of
length
the
impedance
must
still
equal
Z
after
the
addition
of
a
small
amount
of
transmission
line
dx
as
shown
in
Figure
A
b
x
V
x
t
LC
t
I
x
t
x
I
x
t
C
t
V
x
t
xV
x
t
L
t
I
x
t
APPENDIX
A
Digital
System
Implementation
dx
x
a
Cdx
Rdx
dx
Ldx
b
Ldx
Cdx
dx
c
Figure
A
Transmission
line
models
a
semi
infinite
cable
b
lossy
c
ideal
App
Using
the
impedances
of
an
inductor
and
a
capacitor
we
rewrite
the
relationship
of
Figure
A
in
equation
form
Z
j
Ldx
Z
j
Cdx
A
Rearranging
we
get
Z
j
C
j
L
Z
LCdx
A
Taking
the
limit
as
dx
approaches
the
last
term
vanishes
and
we
find
that
A
A
Derivation
of
the
Reflection
Coefficient
The
reflection
coefficient
kr
is
derived
using
conservation
of
current
Figure
A
shows
a
transmission
line
with
characteristic
impedance
Z
and
load
impedance
ZL
Imagine
an
incident
wave
of
voltage
Vi
and
current
Ii
When
the
wave
reaches
the
termination
some
current
IL
flows
through
the
load
impedance
causing
a
voltage
drop
VL
The
remainder
of
the
current
reflects
back
down
the
line
in
a
wave
of
voltage
Vr
and
current
Ir
Z
is
the
ratio
of
voltage
to
current
in
waves
propagating
along
the
line
so
The
voltage
on
the
line
is
the
sum
of
the
voltages
of
the
incident
and
reflected
waves
The
current
flowing
in
the
positive
direction
on
the
line
is
the
difference
between
the
currents
of
the
incident
and
reflected
waves
VL
Vi
Vr
A
IL
Ii
Ir
A
Vi
Ii
Vr
Ir
Z
Z
v
L
C
A
Transmission
Lines
j
Ldx
dx
j
Cdx
V
Z
I
V
I
a
Z
b
Figure
A
Transmission
line
model
a
for
entire
line
and
b
with
additional
length
dx
Figure
A
Transmission
line
showing
incoming
reflected
and
load
voltages
and
currents
Z
ZL
Ii
Vi
IL
Ir
Vr
VL
Appendi
APPENDIX
A
Digital
System
Implementation
Using
Ohm
s
law
and
substituting
for
IL
Ii
and
Ir
in
Equation
A
we
get
A
Rearranging
we
solve
for
the
reflection
coefficient
kr
A
A
Putting
It
All
Together
Transmission
lines
model
the
fact
that
signals
take
time
to
propagate
down
long
wires
because
the
speed
of
light
is
finite
An
ideal
transmission
line
has
uniform
inductance
L
and
capacitance
C
per
unit
length
and
zero
resistance
The
transmission
line
is
characterized
by
its
characteristic
impedance
Z
and
delay
td
which
can
be
derived
from
the
inductance
capacitance
and
wire
length
The
transmission
line
has
significant
delay
and
noise
effects
on
signals
whose
rise
fall
times
are
less
than
about
td
This
means
that
for
systems
with
ns
rise
fall
times
PCB
traces
longer
than
about
cm
must
be
analyzed
as
transmission
lines
to
accurately
understand
their
behavior
A
digital
system
consisting
of
a
gate
driving
a
long
wire
attached
to
the
input
of
a
second
gate
can
be
modeled
with
a
transmission
line
as
shown
in
Figure
A
The
voltage
source
source
impedance
ZS
and
switch
model
the
first
gate
switching
from
to
at
time
The
driver
gate
cannot
supply
infinite
current
this
is
modeled
by
ZS
ZS
is
usually
small
for
a
logic
gate
but
a
designer
may
choose
to
add
a
resistor
in
series
with
the
gate
to
raise
ZS
and
match
the
impedance
of
the
line
The
input
to
the
second
gate
is
modeled
as
ZL
CMOS
circuits
usually
have
little
input
current
so
ZL
may
be
close
to
infinity
The
designer
may
also
choose
to
add
a
resistor
in
parallel
with
the
second
gate
between
the
gate
input
and
ground
so
that
ZL
matches
the
impedance
of
the
line
Vr
Vi
ZL
Z
ZL
Z
kr
Vi
Vr
ZL
Vi
Z
Vr
Z
a
long
wire
driver
gate
receiver
gate
VS
td
Z
Z
S
b
receiver
gate
long
wire
driver
gate
ZL
t
Figure
A
Digital
system
modeled
with
transmission
line
App
When
the
first
gate
switches
a
wave
of
voltage
is
driven
onto
the
transmission
line
The
source
impedance
and
transmission
line
form
a
voltage
divider
so
the
voltage
of
the
incident
wave
is
A
At
time
td
the
wave
reaches
the
end
of
the
line
Part
is
absorbed
by
the
load
impedance
and
part
is
reflected
The
reflection
coefficient
kr
indicates
the
portion
that
is
reflected
kr
Vr
Vi
where
Vr
is
the
voltage
of
the
reflected
wave
and
Vi
is
the
voltage
of
the
incident
wave
A
The
reflected
wave
adds
to
the
voltage
already
on
the
line
It
reaches
the
source
at
time
td
where
part
is
absorbed
and
part
is
again
reflected
The
reflections
continue
back
and
forth
and
the
voltage
on
the
line
eventually
approaches
the
value
that
would
be
expected
if
the
line
were
a
simple
equipotential
wire
A
ECONOMICS
Although
digital
design
is
so
much
fun
that
some
of
us
would
do
it
for
free
most
designers
and
companies
intend
to
make
money
Therefore
economic
considerations
are
a
major
factor
in
design
decisions
The
cost
of
a
digital
system
can
be
divided
into
nonrecurring
engineering
costs
NRE
and
recurring
costs
NRE
accounts
for
the
cost
of
designing
the
system
It
includes
the
salaries
of
the
design
team
computer
and
software
costs
and
the
costs
of
producing
the
first
working
unit
The
fully
loaded
cost
of
a
designer
in
the
United
States
in
including
salary
health
insurance
retirement
plan
and
a
computer
with
design
tools
is
roughly
per
year
so
design
costs
can
be
significant
Recurring
costs
are
the
cost
of
each
additional
unit
this
includes
components
manufacturing
marketing
technical
support
and
shipping
The
sales
price
must
cover
not
only
the
cost
of
the
system
but
also
other
costs
such
as
office
rental
taxes
and
salaries
of
staff
who
do
not
directly
contribute
to
the
design
such
as
the
janitor
and
the
CEO
After
all
of
these
expenses
the
company
should
still
make
a
profit
Example
A
BEN
TRIES
TO
MAKE
SOME
MONEY
Ben
Bitdiddle
has
designed
a
crafty
circuit
for
counting
raindrops
He
decides
to
sell
the
device
and
try
to
make
some
money
but
he
needs
help
deciding
what
implementation
to
use
He
decides
to
use
either
an
FPGA
or
an
ASIC
The
kr
ZL
Z
ZL
Z
Vi
VS
Z
Z
ZS
A
Economics
App
development
kit
to
design
and
test
the
FPGA
costs
Each
FPGA
costs
The
ASIC
costs
for
a
mask
set
and
per
chip
Regardless
of
what
chip
implementation
he
chooses
Ben
needs
to
mount
the
packaged
chip
on
a
printed
circuit
board
PCB
which
will
cost
him
per
board
He
thinks
he
can
sell
devices
per
month
Ben
has
coerced
a
team
of
bright
undergraduates
into
designing
the
chip
for
their
senior
project
so
it
doesn
t
cost
him
anything
to
design
If
the
sales
price
has
to
be
twice
the
cost
profit
margin
and
the
product
life
is
years
which
implementation
is
the
better
choice
SOLUTION
Ben
figures
out
the
total
cost
for
each
implementation
over
years
as
shown
in
Table
A
Over
years
Ben
plans
on
selling
devices
and
the
total
cost
is
given
in
Table
A
for
each
option
If
the
product
life
is
only
two
years
the
FPGA
option
is
clearly
superior
The
per
unit
cost
is
and
the
sales
price
is
per
unit
to
give
a
profit
margin
The
ASIC
option
would
have
cost
and
would
have
sold
for
per
unit
Example
A
BEN
GETS
GREEDY
After
seeing
the
marketing
ads
for
his
product
Ben
thinks
he
can
sell
even
more
chips
per
month
than
originally
expected
If
he
were
to
choose
the
ASIC
option
how
many
devices
per
month
would
he
have
to
sell
to
make
the
ASIC
option
more
profitable
than
the
FPGA
option
SOLUTION
Ben
solves
for
the
minimum
number
of
units
N
that
he
would
need
to
sell
in
years
N
N
APPENDIX
A
Digital
System
Implementation
Table
A
ASIC
vs
FPGA
costs
Cost
ASIC
FPGA
NRE
chip
PCB
TOTAL
per
unit
Appendix
A
Economics
Solving
the
equation
gives
N
units
or
units
per
month
He
would
need
to
almost
double
his
monthly
sales
to
benefit
from
the
ASIC
solution
Example
A
BEN
GETS
LESS
GREEDY
Ben
realizes
that
his
eyes
have
gotten
too
big
for
his
stomach
and
he
doesn
t
think
he
can
sell
more
than
devices
per
month
But
he
does
think
the
product
life
can
be
longer
than
years
At
a
sales
volume
of
devices
per
month
how
long
would
the
product
life
have
to
be
to
make
the
ASIC
option
worthwhile
SOLUTION
If
Ben
sells
more
than
units
in
total
the
ASIC
option
is
the
best
choice
So
Ben
would
need
to
sell
at
a
volume
of
per
month
for
at
least
months
rounding
up
which
is
almost
years
By
then
his
product
is
likely
to
be
obsolete
Chips
are
usually
purchased
from
a
distributor
rather
than
directly
from
the
manufacturer
unless
you
are
ordering
tens
of
thousands
of
units
Digikey
www
digikey
com
is
a
leading
distributor
that
sells
a
wide
variety
of
electronics
Jameco
www
jameco
com
and
All
Electronics
www
allelectronics
com
have
eclectic
catalogs
that
are
competitively
priced
and
well
suited
to
hobbyists
A
MIPS
Instructions
B
This
appendix
summarizes
MIPS
instructions
used
in
this
book
Tables
B
B
define
the
opcode
and
funct
fields
for
each
instruction
along
with
a
short
description
of
what
the
instruction
does
The
following
notations
are
used
reg
contents
of
the
register
imm
bit
immediate
field
of
the
I
type
instruction
addr
bit
address
field
of
the
J
type
instruction
SignImm
sign
extended
immediate
imm
imm
ZeroImm
zero
extended
immediate
b
imm
Address
rs
SignImm
Address
contents
of
memory
location
Address
BTA
branch
target
address
PC
SignImm
JTA
jump
target
address
PC
addr
b
The
SPIM
simulator
has
no
branch
delay
slot
so
BTA
is
PC
SignImm
Thus
if
you
use
the
SPIM
assembler
to
create
machine
code
for
a
real
MIPS
processor
you
must
decrement
the
immediate
field
by
to
compensate
Appendix
B
qxd
Table
B
Instructions
sorted
by
opcode
APPENDIX
B
MIPS
Instructions
Opcode
Name
Description
Operation
R
type
all
R
type
instructions
see
Table
B
bltz
bgez
branch
less
than
zero
if
rs
PC
BTA
rt
branch
greater
than
or
if
rs
PC
BTA
equal
to
zero
j
jump
PC
JTA
jal
jump
and
link
ra
PC
PC
JTA
beq
branch
if
equal
if
rs
rt
PC
BTA
bne
branch
if
not
equal
if
rs
rt
PC
BTA
blez
branch
if
less
than
or
equal
to
zero
if
rs
PC
BTA
bgtz
branch
if
greater
than
zero
if
rs
PC
BTA
addi
add
immediate
rt
rs
SignImm
addiu
add
immediate
unsigned
rt
rs
SignImm
slti
set
less
than
immediate
rs
SignImm
rt
rt
sltiu
set
less
than
immediate
unsigned
rs
SignImm
rt
rt
andi
and
immediate
rt
rs
ZeroImm
ori
or
immediate
rt
rs
ZeroImm
xori
xor
immediate
rt
rs
ZeroImm
lui
load
upper
immediate
rt
Imm
b
mfc
move
from
to
coprocessor
rt
rd
rd
rt
rs
mtc
rd
is
in
coprocessor
F
type
fop
F
type
instructions
see
Table
B
bc
f
bc
t
fop
branch
if
fpcond
is
if
fpcond
PC
BTA
rt
FALSE
TRUE
if
fpcond
PC
BTA
lb
load
byte
rt
SignExt
Address
lh
load
halfword
rt
SignExt
Address
lw
load
word
rt
Address
lbu
load
byte
unsigned
rt
ZeroExt
Address
lhu
load
halfword
unsigned
rt
ZeroExt
Address
sb
store
byte
Address
rt
continued
Appendix
B
qxd
PM
Page
Table
B
Instructions
sorted
by
opcode
Cont
d
Table
B
R
type
instructions
sorted
by
funct
field
Appendix
B
Funct
Name
Description
Operation
sll
shift
left
logical
rd
rt
shamt
srl
shift
right
logical
rd
rt
shamt
sra
shift
right
arithmetic
rd
rt
shamt
sllv
shift
left
logical
rd
rt
rs
variable
assembly
sllv
rd
rt
rs
srlv
shift
right
logical
rd
rt
rs
variable
assembly
srlv
rd
rt
rs
srav
shift
right
arithmetic
rd
rt
rs
variable
assembly
srav
rd
rt
rs
jr
jump
register
PC
rs
jalr
jump
and
link
register
ra
PC
PC
rs
syscall
system
call
system
call
exception
break
break
break
exception
mfhi
move
from
hi
rd
hi
mthi
move
to
hi
hi
rs
mflo
move
from
lo
rd
lo
mtlo
move
to
lo
lo
rs
mult
multiply
hi
lo
rs
rt
multu
multiply
unsigned
hi
lo
rs
rt
div
divide
lo
rs
rt
hi
rs
rt
sh
store
halfword
Address
rt
sw
store
word
Address
rt
lwc
load
word
to
FP
coprocessor
ft
Address
swc
store
word
to
FP
coprocessor
Address
ft
continued
Opcode
Name
Description
Operation
Appendix
B
qxd
APPENDIX
B
MIPS
Instructions
Table
B
R
type
instructions
sorted
by
funct
field
Cont
d
Table
B
F
type
instructions
fop
Funct
Name
Description
Operation
divu
divide
unsigned
lo
rs
rt
hi
rs
rt
add
add
rd
rs
rt
addu
add
unsigned
rd
rs
rt
sub
subtract
rd
rs
rt
subu
subtract
unsigned
rd
rs
rt
and
and
rd
rs
rt
or
or
rd
rs
rt
xor
xor
rd
rs
rt
nor
nor
rd
rs
rt
slt
set
less
than
rs
rt
rd
rd
sltu
set
less
than
unsigned
rs
rt
rd
rd
Funct
Name
Description
Operation
add
s
add
d
FP
add
fd
fs
ft
sub
s
sub
d
FP
subtract
fd
fs
ft
mul
s
mul
d
FP
multiply
fd
fs
ft
div
s
div
d
FP
divide
fd
fs
ft
abs
s
abs
d
FP
absolute
value
fd
fs
fs
fs
neg
s
neg
d
FP
negation
fd
fs
c
seq
s
c
seq
d
FP
equality
comparison
fpcond
fs
ft
c
lt
s
c
lt
d
FP
less
than
comparison
fpcond
fs
ft
c
le
s
c
le
d
FP
less
than
or
equal
comparison
fpcond
fs
ft
Appendix
B
qxd
Further
Reading
Berlin
L
The
Man
Behind
the
Microchip
Robert
Noyce
and
the
Invention
of
Silicon
Valley
Oxford
University
Press
The
fascinating
biography
of
Robert
Noyce
an
inventor
of
the
microchip
and
founder
of
Fairchild
and
Intel
For
anyone
thinking
of
working
in
Silicon
Valley
this
book
gives
insights
into
the
culture
of
the
region
a
culture
influenced
more
heavily
by
Noyce
than
any
other
individual
Colwell
R
The
Pentium
Chronicles
The
People
Passion
and
Politics
Behind
Intel
s
Landmark
Chips
Wiley
An
insider
s
tale
of
the
development
of
several
generations
of
Intel
s
Pentium
chips
told
by
one
of
the
leaders
of
the
project
For
those
considering
a
career
in
the
field
this
book
offers
views
into
the
managment
of
huge
design
projects
and
a
behind
the
scenes
look
at
one
of
the
most
significant
commercial
microprocessor
lines
Ercegovac
M
and
Lang
T
Digital
Arithmetic
Morgan
Kaufmann
The
most
complete
text
on
computer
arithmetic
systems
An
excellent
resource
for
building
high
quality
arithmetic
units
for
computers
Hennessy
J
and
Patterson
D
Computer
Architecture
A
Quantitative
Approach
th
ed
Morgan
Kaufmann
The
authoritative
text
on
advanced
computer
architecture
If
you
are
intrigued
about
the
inner
workings
of
cutting
edge
microprocessors
this
is
the
book
for
you
Kidder
T
The
Soul
of
a
New
Machine
Back
Bay
Books
A
classic
story
of
the
design
of
a
computer
system
Three
decades
later
the
story
is
still
a
page
turner
and
the
insights
on
project
managment
and
technology
still
ring
true
Pedroni
V
Circuit
Design
with
VHDL
MIT
Press
A
reference
showing
how
to
design
circuits
with
VHDL
Thomas
D
and
Moorby
P
The
Verilog
Hardware
Description
Language
th
ed
Kluwer
Academic
Publishers
Ciletti
M
Advanced
Digital
Design
with
the
Verilog
HDL
Prentice
Hall
Both
excellent
references
covering
Verilog
in
more
detail
Further
Reading
Verilog
IEEE
Standard
IEEE
STD
The
IEEE
standard
for
the
Verilog
Hardware
Description
Language
last
updated
in
Available
at
ieeexplore
ieee
org
VHDL
IEEE
Standard
IEEE
STD
The
IEEE
standard
for
VHDL
last
updated
in
Available
from
IEEE
Available
at
ieeexplore
ieee
org
Wakerly
J
Digital
Design
Principles
and
Practices
th
ed
Prentice
Hall
A
comprehensive
and
readable
text
on
digital
design
and
an
excellent
reference
book
Weste
N
and
Harris
D
CMOS
VLSI
Design
rd
ed
Addison
Wesley
Very
Large
Scale
Integration
VLSI
Design
is
the
art
and
science
of
building
chips
containing
oodles
of
transistors
This
book
coauthored
by
one
of
our
favorite
writers
spans
the
field
from
the
beginning
through
the
most
advanced
techniques
used
in
commercial
products
Index
See
also
LOW
OFF
See
also
HIGH
ON
xx
series
logic
parts
Mux
Decoder
Mux
AND
AND
AND
Counter
FLOP
NAND
NOR
NOT
OR
XOR
Register
Tristate
buffer
schematics
of
A
add
Adders
carry
lookahead
See
Carrylookahead
adder
carry
propagate
CPA
See
Carrypropagate
adder
prefix
See
Prefix
adder
ripple
carry
See
Ripple
carry
adder
add
immediate
addi
add
immediate
unsigned
addiu
add
unsigned
addu
Addition
floating
point
overflow
See
Overflow
two
s
complement
underflow
See
Underflow
unsigned
binary
Address
physical
translation
virtual
word
alignment
Addressing
modes
base
immediate
MIPS
PC
relative
pseudo
direct
register
only
Advanced
microarchitecture
branch
prediction
See
Branch
prediction
deep
pipelines
See
Deep
pipelines
multiprocessors
See
Multiprocessors
multithreading
See
Multithreading
out
of
order
processor
See
Out
of
order
processor
register
renaming
See
Register
renaming
single
instruction
multiple
data
See
Single
instruction
multiple
data
SIMD
units
superscalar
processor
See
Superscalar
processor
vector
processor
See
Single
instruction
multiple
data
SIMD
units
Alignment
See
Word
alignment
ALU
See
Arithmetic
logical
unit
ALU
decoder
ALU
decoder
truth
table
ALUControl
ALUOp
ALUOut
ALUResult
AMAT
See
Average
memory
access
time
Amdahl
Gene
Amdahl
s
Law
Anodes
and
immediate
andi
AND
gate
Application
specific
integrated
circuits
ASICs
Architectural
state
Architecture
See
Instruction
Set
Architecture
Arithmetic
See
also
Adders
Addition
Comparator
Divider
Multiplier
adders
See
Adders
addition
See
Addition
ALU
See
Arithmetic
logical
unit
circuits
comparators
See
Comparators
divider
See
Divider
division
See
Division
fixed
point
floating
point
logical
instructions
multiplier
See
Multiplier
packed
rotators
See
Rotators
shifters
See
Shifters
signed
and
unsigned
subtraction
See
Subtraction
Arithmetic
Continued
subtractor
See
Subtractor
underflow
See
Addition
Underflow
Arithmetic
logical
unit
ALU
bit
adder
See
Adders
ALUOp
ALUResult
ALUOut
comparator
See
Comparators
control
subtractor
See
Subtractor
Arrays
accessing
bytes
and
characters
FPGA
See
Field
programmable
gate
array
logic
See
Logic
arrays
memory
See
Memory
RAM
ROM
ASCII
American
Standard
Code
for
Information
Interchange
codes
table
of
Assembler
directives
ASICs
See
Application
specific
integrated
circuits
Assembly
language
MIPS
See
MIPS
assembly
language
Associativity
Average
memory
access
time
AMAT
Asynchronous
circuits
Asynchronous
inputs
Asynchronous
resettable
registers
HDL
for
Axioms
See
Boolean
axioms
B
Babbage
Charles
Base
address
Base
register
Base
number
representations
See
Binary
numbers
Base
number
representations
See
Octal
numbers
Base
number
representations
See
Hexadecimal
numbers
Block
digital
building
See
Digital
building
blocks
in
code
else
block
if
block
Base
addressing
Baudot
Jean
Maurice
Emile
Behavioral
modeling
Benchmarks
SPEC
Biased
numbers
See
also
Floating
point
Big
Endian
Binary
numbers
See
also
Arithmetic
ASCII
binary
coded
decimal
conversion
See
Number
conversion
fixed
point
floating
point
See
Floating
point
numbers
signed
sign
magnitude
See
Sign
magnitude
numbers
two
s
complement
See
Two
s
complement
numbers
unsigned
Binary
to
decimal
conversion
Binary
to
hexadecimal
conversion
Bit
dirty
least
significant
most
significant
sign
use
valid
Bit
cells
Bit
swizzling
Bitwise
operators
Boole
George
Boolean
algebra
axioms
equation
simplification
theorems
Boolean
axioms
Boolean
equations
product
of
sums
POS
canonical
form
sum
of
products
SOP
canonical
form
terminology
Boolean
theorems
DeMorgan
s
complement
consensus
covering
combining
Branching
calculating
address
conditional
prediction
target
address
BTA
unconditional
jump
Branch
equal
beq
Branch
not
equal
bne
Branch
control
hazards
See
also
Hazards
Branch
prediction
Breadboards
Bubble
pushing
Buffer
tristate
Bugs
Bus
tristate
Bypassing
Bytes
Byte
addressable
memory
Byte
order
Big
Endian
Little
Endian
C
C
programming
language
overflows
strings
Caches
See
also
Memory
accessing
advanced
design
associativity
block
size
blocks
capacity
data
placement
data
replacement
Index
definition
direct
mapped
dirty
bit
entry
fields
evolution
of
MIPS
fully
associative
hit
hit
rate
IA
systems
level
L
mapping
miss
capacity
compulsory
conflict
miss
rate
miss
rate
versus
cache
parameters
miss
penalty
multiway
set
associative
nonblocking
organizations
performance
set
associative
tag
use
bit
valid
bit
write
policy
CAD
See
Computer
aided
design
Canonical
form
Capacitors
Capacity
miss
Carry
lookahead
adder
Carry
propagate
adder
CPA
Cathodes
Cause
register
Chips
xx
series
logic
See
xx
series
logic
Circuits
xx
series
logic
See
xx
series
logic
application
specific
integrated
ASIC
arithmetic
See
Arithmetic
asynchronous
bistable
device
combinational
definition
delays
calculating
dynamic
multiple
output
pipelining
priority
synchronous
sequential
synthesized
timing
timing
analysis
types
without
glitch
CISC
See
Complex
instruction
set
computers
CLBs
See
Configurable
logic
blocks
Clock
cycle
See
Clock
period
Clock
period
Clock
rate
See
Clock
period
Clock
cycles
per
instruction
CPI
Clustered
computers
Clock
skew
CMOS
See
Complementary
MetalOxide
Semiconductor
Logic
Code
Code
size
Combinational
composition
Combinational
logic
design
Boolean
algebra
Boolean
equations
building
blocks
delays
don
t
cares
HDLs
and
See
Hardware
description
languages
Karnaugh
maps
logic
multilevel
overview
precedence
timing
two
level
X
s
contention
See
Contention
X
s
don
t
cares
See
Don
t
cares
Z
s
floating
See
Floating
Comparators
equality
magnitude
Compiler
Complementary
Metal
OxideSemiconductor
Logic
CMOS
bubble
pushing
logic
gates
NAND
gate
NOR
gate
NOT
gate
transistors
Complex
instruction
set
computers
CISC
Compulsory
miss
Computer
aided
design
CAD
Computer
Organization
and
Design
Patterson
and
Hennessy
Complexity
management
abstraction
discipline
hierarchy
modularity
regularity
Conditional
assignment
Conditional
branches
Condition
codes
Conflict
misses
Conditional
statements
Constants
See
also
Immediates
Contamination
delay
Contention
X
Context
switch
Control
signals
Control
unit
multicycle
MIPS
processor
FSM
pipelined
MIPS
processor
single
cycle
MIPS
processor
Configurable
logic
blocks
CLBs
Control
hazards
See
Branch
control
hazards
Pipelining
Coprocessor
Counters
Covalent
bond
CPA
See
Carry
propagate
adder
CPI
See
Clock
cycles
per
instruction
Critical
path
Cyclic
paths
Cycle
time
See
Clock
Period
D
Data
hazards
See
Hazards
Data
memory
Data
sheets
Datapath
See
also
MIPS
microprocessors
Index
Datapath
Continued
elements
multicycle
MIPS
processor
pipelined
MIPS
processor
single
cycle
MIPS
processor
Data
segments
See
Memory
map
Data
types
See
Hardware
description
languages
DC
See
Direct
current
Decimal
numbers
conversion
to
binary
and
hexadecimal
See
Number
conversion
scientific
notation
Decimal
to
Binary
conversion
Decimal
to
hexadecimal
conversion
Decoders
implementation
logic
parameterized
Deep
pipelines
Delay
DeMorgan
Augustus
DeMorgan
s
theorem
Dennard
Robert
Destination
register
Device
under
test
DUT
See
also
Unit
under
test
Device
driver
Dice
Digital
abstraction
DC
transfer
characteristics
logic
levels
noise
margins
supply
voltage
Digital
design
abstraction
discipline
hierarchy
modularity
regularity
Digital
system
implementation
xx
series
logic
See
xx
series
logic
application
specific
integrated
circuits
ASICs
data
sheets
economics
logic
families
overview
packaging
and
assembly
breadboards
packages
printed
circuit
boards
programmable
logic
transmission
lines
See
Transmission
lines
Diodes
DIP
See
Dual
inline
package
Direct
current
DC
transfer
characteristics
Direct
mapped
cache
Dirty
bit
Discipline
Disk
See
Hard
disk
divide
div
divide
unsigned
divu
Divider
Division
floating
point
instructions
Divisor
Don
t
care
X
Dopant
atoms
Double
See
Double
precision
floatingpoint
numbers
Double
precision
floating
point
numbers
DRAM
See
Dynamic
random
access
memory
Driver
See
Device
driver
Dual
inline
package
DIP
DUT
See
Device
under
test
Dynamic
discipline
Dynamic
data
segment
Dynamic
random
access
memory
DRAM
E
Edison
Thomas
Edge
triggered
digital
systems
Equations
simplification
Electrically
erasable
programmable
read
only
memory
EEPROM
Enabled
registers
HDL
for
EPC
See
Exception
Program
Counter
register
Erasable
programmable
read
only
memory
EPROM
Exceptions
Exception
handler
Exception
program
counter
EPC
Exclusive
or
See
XOR
Executable
file
Execution
time
Exponent
F
Failures
FDIV
bug
FET
See
Field
effect
transistors
Field
programmable
gate
array
FPGA
Field
effect
transistors
FIFO
See
First
in
first
out
queue
See
also
Queue
Finite
state
machines
FSMs
design
example
divide
by
factoring
HDLs
and
Mealy
machines
See
Mealy
machines
Moore
machines
See
Moore
machines
Moore
versus
Mealy
machines
state
encodings
state
transition
diagram
First
in
first
out
FIFO
queue
Fixed
point
numbers
Flash
memory
Flip
flops
See
also
Registers
comparison
with
latches
D
enabled
register
resettable
asynchronous
synchronous
transistor
count
transistor
level
Floating
Z
Floating
point
numbers
Index
addition
See
also
Addition
converting
binary
or
decimal
to
See
Number
conversions
division
See
also
Division
double
precision
FDIV
bug
See
FDIV
bug
floating
point
unit
FPU
instructions
rounding
single
precision
special
cases
infinity
not
a
number
NaN
Forwarding
Fractional
numbers
FPGA
See
Field
programmable
gate
array
Frequency
See
also
Clock
period
FSM
See
Finite
state
machines
Full
adder
See
also
Adder
Addition
HDL
using
always
process
using
nonblocking
assignments
Fully
associative
cache
Funct
field
Functional
specification
Functions
See
Procedure
calls
Fuse
G
Gates
AND
NAND
NOR
OR
transistor
level
implementation
XOR
XNOR
Gedanken
Experiments
on
Sequential
Machines
Moore
Generate
signal
Glitches
Global
pointer
gp
See
also
Static
data
segment
Gray
Frank
Gray
codes
Gulliver
s
Travels
Swift
H
Half
word
Half
adder
Hard
disk
Hard
drive
See
Hard
disk
Hardware
reduction
Hardware
description
languages
HDLs
assignment
statements
behavioral
modeling
combinational
logic
bit
swizzling
bitwise
operators
blocking
and
nonblocking
assignments
case
statements
conditional
assignment
delays
if
statements
internal
variables
numbers
precedence
reduction
operators
synthesis
tools
See
Synthesis
Tools
Verilog
VHDL
libraries
and
types
Z
s
and
X
s
finite
state
machines
generate
statement
generic
building
blocks
invalid
logic
level
language
origins
modules
origins
of
overview
parameterized
modules
representation
sequential
logic
enabled
registers
latches
multiple
registers
registers
See
also
Registers
resettable
registers
simulation
and
synthesis
single
cycle
processor
structural
modeling
testbenches
Hazards
See
also
Glitches
control
hazards
data
hazards
solving
forwarding
stalls
WAR
See
Write
after
read
WAW
See
Write
after
write
Hazard
unit
Heap
HDLs
See
Hardware
description
languages
Hennessy
John
Hexadecimal
numbers
to
binary
conversion
table
Hierarchy
HIGH
See
also
ON
High
level
programming
languages
translating
into
assembly
compiling
linking
and
launching
Hit
High
impedance
See
Floating
Z
High
Z
See
Floating
High
impedance
Z
Hold
time
I
I
type
instruction
IA
microprocessor
branch
conditions
cache
systems
encoding
evolution
instructions
memory
and
input
output
I
O
systems
operands
programmed
I
O
registers
status
flags
virtual
memory
IEEE
floating
point
standard
Idempotency
Idioms
IEEE
Index
If
statements
If
else
statements
ILP
See
Instruction
level
parallelism
Immediates
Immediate
addressing
Information
amount
of
IorD
I
O
input
output
communicating
with
device
driver
devices
See
also
Peripheral
devices
memory
interface
memory
mapped
I
O
Inputs
asynchronous
Instruction
encoding
See
Machine
Language
Instruction
register
IR
Instruction
set
architecture
ISA
See
also
MIPS
instruction
set
architecture
Input
output
blocks
IOBs
Input
terminals
Institute
of
Electrical
and
Electronics
Engineers
Instruction
decode
Instruction
encoding
See
Instruction
format
Instruction
format
F
type
I
type
J
type
R
type
Instruction
level
parallelism
ILP
Instruction
memory
Instruction
set
See
Instruction
set
architecture
Instructions
See
also
Language
arithmetic
logical
floating
point
IA
I
type
J
type
loads
See
Loads
multiplication
and
division
pseudoinstructions
R
type
set
less
than
shift
signed
and
unsigned
Intel
Inverter
See
NOT
gate
Integrated
circuits
ICs
costs
manufacturing
process
Intel
See
IA
microprocessors
Interrupts
See
Exceptions
An
Investigation
of
the
Laws
of
Thought
Boole
Involution
IOBs
See
Input
output
blocks
I
type
instructions
J
Java
See
also
Language
JTA
See
Jump
target
address
J
type
instructions
Jump
See
also
Branch
unconditional
Programming
Jump
target
address
JTA
K
K
maps
See
Karnaugh
maps
Karnaugh
Maurice
Karnaugh
maps
K
maps
logic
minimization
using
prime
implicants
seven
segment
display
decoder
with
don
t
cares
without
glitches
Kilby
Jack
Kilobyte
K
maps
See
Karnaugh
maps
L
Labels
Language
See
also
Instructions
assembly
high
level
machine
mnemonic
translating
assembly
to
machine
Last
in
first
out
LIFO
queue
See
also
Stack
Queue
Latches
comparison
with
flip
flops
D
SR
transistor
level
Latency
Lattice
Leaf
procedures
Least
recently
used
LRU
replacement
Least
significant
bit
lsb
Least
significant
byte
LSB
LIFO
See
Last
in
first
out
queue
Literal
Little
Endian
Load
byte
lb
byte
unsigned
lbu
half
lh
immediate
li
upper
immediate
lui
word
lw
Loading
Locality
Local
variables
Logic
See
also
Multilevel
combinational
logic
Sequential
logic
design
bubble
pushing
See
Bubble
pushing
combinational
See
Combinational
logic
families
gates
See
Logic
gates
hardware
reduction
See
Hardware
reduction
Equation
simplification
multilevel
See
Multilevel
combinational
logic
programmable
sequential
See
Sequential
logic
synthesis
two
level
using
memory
arrays
See
also
Logic
arrays
Logic
arrays
field
programmable
gate
array
programmable
logic
array
transistor
level
implementations
Logic
families
Index
compatibility
specifications
Logic
gates
buffer
delays
multiple
input
gates
two
input
gates
types
AND
See
AND
gate
AOI
and
or
invert
See
And
or
invert
gate
NAND
See
NAND
gate
NOR
See
NOR
gate
NOT
See
NOT
gate
OAI
or
and
invert
See
Or
and
invert
gate
OR
See
OR
gate
XOR
See
XOR
gate
XNOR
See
XNOR
gate
Logic
levels
Logical
operations
Lookup
tables
LUTs
Loops
for
while
LOW
See
also
OFF
Low
Voltage
CMOS
Logic
LVCMOS
Low
Voltage
TTL
Logic
LVTTL
LRU
See
Least
recently
used
replacement
LSB
See
Least
significant
byte
LUTs
See
Lookup
tables
LVCMOS
See
Low
Voltage
CMOS
Logic
LVTTL
See
Low
Voltage
TTL
Logic
M
Machine
language
function
fields
interpreting
code
I
type
instructions
J
type
instructions
opcodes
R
type
instructions
stored
programs
translating
from
assembly
language
translating
into
assembly
language
Main
decoder
Main
memory
Mapping
Mantissa
See
also
Floating
point
numbers
Masuoka
Fujio
MCM
See
Multichip
module
Mealy
George
H
Mealy
machines
combined
state
transition
and
output
table
state
transition
diagram
timing
diagram
Mean
time
between
failures
MTBF
Memory
access
average
memory
access
time
AMAT
cache
See
Caches
DRAM
See
Dynamic
random
access
memory
hierarchy
interface
main
map
dynamic
data
segment
global
data
segment
reserved
segment
text
segment
nonvolatile
performance
physical
protection
See
also
Virtual
memory
RAM
ROM
separate
data
and
instruction
shared
stack
See
Stack
types
flip
flops
latches
DRAM
registers
register
file
SRAM
virtual
See
also
Virtual
memory
volatile
word
addressable
See
Wordaddressable
memory
Memory
arrays
area
bit
cells
delay
DRAM
See
Dynamic
random
access
memory
HDL
code
for
logic
implementation
using
See
also
Logic
arrays
organization
overview
ports
register
files
built
using
types
DRAM
See
Dynamic
random
access
memory
ROM
See
Read
only
memory
SRAM
See
Static
random
access
memory
Memory
mapped
I
O
input
output
address
decoder
communicating
with
I
O
devices
hardware
speech
synthesizer
device
driver
speech
synthesizer
hardware
SP
Memory
protection
Memory
systems
caches
See
Caches
IA
MIPS
overview
performance
analysis
virtual
memory
See
Virtual
memory
Mercedes
Benz
Metal
oxide
semiconductor
field
effect
transistors
MOSFETs
See
also
CMOS
nMOS
pMOS
transistors
Metastability
MTBF
See
Mean
time
between
failures
metastable
state
probability
of
failure
resolution
time
synchronizers
A
Method
of
Synthesizing
Sequential
Circuits
Mealy
Index
Microarchitecture
advanced
See
Advanced
microarchitecture
architectural
state
See
Architectural
State
See
also
Architecture
design
process
exception
handling
HDL
representation
IA
See
IA
microprocessor
instruction
set
See
Instruction
set
MIPS
See
MIPS
microprocessor
overview
performance
analysis
types
advanced
See
Advanced
microarchitecture
multicycle
See
Multicycle
MIPS
processor
pipelined
See
Pipelined
MIPS
processor
single
cycle
See
Single
cycle
MIPS
processor
Microprocessors
advanced
See
Advanced
microarchitecture
chips
See
Chips
clock
frequencies
IA
See
IA
microprocessor
instructions
See
MIPS
instructions
IA
instructions
MIPS
See
MIPS
microprocessor
Microsoft
Windows
Minterms
Maxterms
MIPS
Millions
of
instructions
per
second
See
Millions
of
instructions
per
second
MIPS
architecture
See
MIPS
instruction
set
architecture
ISA
MIPS
assembly
language
See
also
MIPS
instruction
set
architecture
addressing
modes
assembler
directives
instructions
logical
instructions
mnemonic
operands
procedure
calls
table
of
instructions
translating
machine
language
to
translating
to
machine
language
MIPS
instruction
set
architecture
ISA
addressing
modes
assembly
language
compiling
assembling
and
loading
exceptions
floating
point
instructions
IA
instructions
machine
language
MIPS
instructions
overview
programming
pseudoinstructions
signed
and
unsigned
instructions
SPARC
translating
and
starting
a
program
See
Translating
and
starting
a
program
MIPS
instructions
formats
F
type
I
type
J
type
R
type
tables
of
opcodes
R
type
funct
fields
types
arithmetic
branching
See
Branching
division
floating
point
logical
multiplication
pseudoinstructions
MIPS
microprocessor
ALU
multicycle
See
Multicycle
MIPS
processor
pipelined
See
Pipelined
MIPS
processor
single
cycle
See
Single
cycle
MIPS
processor
MIPS
processor
See
MIPS
microprocessor
MIPS
registers
nonpreserved
preserved
table
of
MIPS
single
cycle
HDL
implementation
building
blocks
controller
datapath
testbench
top
level
module
Misses
AMAT
See
Average
memory
access
time
cache
capacity
compulsory
conflict
page
fault
Miss
penalty
Miss
rate
Mnemonic
Modeling
structural
See
Structural
modeling
Modeling
behavioral
See
Behavioral
modeling
Modularity
Modules
in
HDL
See
also
Hardware
description
languages
behavioral
parameterized
structural
Moore
Edward
F
Moore
Gordon
Moore
machine
output
table
state
transition
diagram
state
transition
table
timing
diagram
Moore
s
law
MOSFETs
See
Metal
oxide
semiconductor
field
effect
transistors
Most
significant
bit
msb
Most
significant
byte
MSB
Move
from
hi
mfhi
Move
from
lo
mflo
Move
from
coprocessor
mfc
msb
See
Most
significant
bit
MSB
See
Most
significant
byte
MTBF
See
Mean
time
before
failure
Multichip
module
MCM
Multicycle
MIPS
processor
control
control
FSM
datapath
performance
analysis
Multilevel
combinational
logic
See
also
Logic
Multilevel
page
table
Multiplexers
Index
instance
logic
parameterized
See
also
Hardware
description
languages
symbol
and
truth
table
timing
with
type
conversion
wide
Multiplicand
Multiplication
See
also
Arithmetic
Multiplier
architecture
instructions
Multiplier
multiply
mult
multiply
unsigned
multu
Multiprocessors
chip
Multithreading
Mux
See
Multiplexers
N
NaN
See
Not
a
number
Negation
See
also
Taking
the
two
s
complement
Not
a
number
NaN
NAND
gate
nor
NOR
gate
NOT
gate
See
also
Inverter
in
HDL
using
always
process
Nested
procedure
calls
Netlist
Next
state
Nibbles
nMOS
nMOS
transistors
No
operation
See
nop
Noise
margins
nop
Noyce
Robert
Number
conversion
See
also
Number
systems
Binary
numbers
binary
to
decimal
binary
to
hexadecimal
decimal
to
binary
decimal
to
hexadecimal
taking
the
two
s
complement
Number
systems
addition
See
Addition
binary
numbers
See
Binary
numbers
comparison
of
conversion
of
See
Number
conversions
decimal
numbers
See
Decimal
numbers
estimating
powers
of
two
fixed
point
See
Fixedpoint
numbers
floating
point
See
Floating
point
numbers
in
hardware
description
languages
hexadecimal
numbers
See
Hexadecimal
numbers
negative
and
positive
rounding
See
also
Floatingpoint
numbers
sign
bit
signed
See
also
Signed
binary
numbers
sign
magnitude
numbers
See
Sign
magnitude
numbers
two
s
complement
numbers
See
Two
s
complement
numbers
unsigned
O
OAI
gate
See
Or
and
invert
gate
Object
files
Octal
numbers
OFF
See
also
LOW
Offset
ON
See
also
HIGH
Asserted
One
cold
One
hot
Opcode
Operands
Operators
bitwise
or
immediate
ori
OR
gate
Or
and
invert
OAI
gate
precedence
table
of
reduction
Out
of
order
execution
Out
of
order
processor
Output
devices
Output
terminals
Overflow
detecting
with
addition
P
Page
size
Page
fault
Page
offset
Page
table
Parity
Parallelism
pipelining
See
Pipelining
Pipelined
MIPS
processor
SIMD
See
Single
instruction
multiple
data
unit
spatial
and
temporal
vector
processor
See
Vector
processor
Patterson
David
PCBs
See
Printed
circuit
boards
PC
relative
addressing
See
also
Addressing
modes
PCSrc
PCWrite
Pentium
processors
See
also
Intel
IA
Pentium
Pentium
II
Pentium
III
Pentium
M
Pentium
Pro
Perfect
induction
Performance
Peripheral
devices
See
I
O
devices
Perl
programming
language
Physical
memory
Physical
pages
Physical
page
number
Pipelined
MIPS
processor
See
also
MIPS
Architecture
Microarchitecture
control
datapath
forwarding
hazards
See
Hazards
performance
analysis
processor
performance
comparison
stalls
See
also
Hazards
timing
Index
Pipelining
hazards
See
Hazards
PLAs
See
Programmable
logic
arrays
Plastic
leaded
chip
carriers
PLCCs
PLCCs
See
Plastic
leaded
chip
carriers
PLDs
See
Programmable
logic
devices
pMOS
pMOS
transistors
Pointer
global
stack
Pop
See
also
Stack
Ports
POS
See
Product
of
sums
form
Power
consumption
Prediction
See
Branch
prediction
Prefix
adder
Preserved
registers
Prime
implicants
Printed
circuit
boards
PCBs
Procedure
calls
arguments
and
variables
nested
preserved
versus
nonpreserved
registers
returns
return
values
stack
frame
stack
usage
Processors
See
Microprocessors
Product
of
sums
POS
canonical
form
Program
counter
PC
Programmable
logic
arrays
PLAs
Programmable
logic
devices
PLDs
Programmable
read
only
memories
PROMs
See
also
Read
only
memories
Programming
arithmetic
logical
instructions
See
Arithmetic
arrays
See
Arrays
branching
See
Branching
conditional
statements
See
Conditional
statements
constants
immediates
loops
See
Loops
procedure
calls
See
Procedure
calls
shift
instructions
translating
and
starting
a
program
Programming
languages
Propagation
delay
Protection
memory
See
Memory
protection
Proving
Boolean
theorems
See
Perfect
induction
PROMs
See
Programmable
read
only
memories
Pseudo
direct
addressing
See
also
Addressing
modes
Pseudo
nMOS
logic
See
also
Transistors
Pseudoinstructions
Push
See
also
Stack
Q
Queue
FIFO
See
First
in
first
out
queue
LIFO
See
Last
in
first
out
queue
Q
output
See
Sequential
logic
design
R
R
type
instructions
RAM
See
Random
access
memory
Random
access
memory
RAM
See
also
Memory
arrays
synthesized
Read
only
memory
See
also
Memory
arrays
EPROM
See
Erasable
programmable
read
only
memory
EEPROM
See
Electrically
erasable
programmable
read
only
memory
flash
memory
See
Flash
memory
PROM
See
Programmable
read
only
memory
transistor
level
implementation
Read
write
head
Recursive
procedure
calls
See
also
Procedure
calls
Reduced
instruction
set
computer
RISC
Reduction
operators
See
also
Hardware
description
languages
Verilog
RegDst
Register
only
addressing
See
also
Addressing
modes
Register
renaming
See
also
Advanced
microarchitecture
Register
s
See
also
Flip
flops
Register
file
arguments
a
a
assembler
temporary
at
enabled
See
Enabled
registers
file
global
pointer
gp
multiple
program
counter
PC
preserved
and
nonpreserved
renaming
resettable
See
Resettable
registers
asynchronous
synchronous
return
address
ra
Register
set
See
also
Register
file
MIPS
registers
IA
registers
Regularity
RegWrite
Replacement
policies
See
also
Caches
Virtual
memory
Resettable
registers
asynchronous
See
Asynchronous
resettable
registers
synchronous
See
Synchronous
resettable
registers
Return
address
ra
Ripple
carry
adder
RISC
See
Reduced
instruction
set
computer
ROM
See
Read
Only
Memory
Rotators
Rounding
S
Scalar
processor
Scan
chains
See
also
Shift
registers
Schematic
Scientific
notation
Index
Seek
time
Segments
memory
Semiconductor
industry
sales
Semiconductors
See
also
Transistors
CMOS
See
Complementary
metal
oxide
silicon
diodes
See
Diodes
transistors
See
Transistors
MOSFET
See
Metal
oxide
silicon
field
effect
transistors
nMOS
See
nMOS
transistors
pMOS
See
pMOS
transistors
pseudo
nMOS
See
Pseudo
nMOS
Sensitivity
list
Sequential
building
blocks
See
Sequential
logic
Sequential
logic
enabled
registers
latches
multiple
registers
overview
registers
shift
registers
synchronous
timing
of
See
also
Timing
set
if
less
than
slt
set
if
less
than
immediate
slti
set
if
less
than
immediate
unsigned
sltiu
set
if
less
than
unsigned
sltu
Setup
time
Seven
segment
display
decoder
with
don
t
cares
Shared
memory
Shift
amount
shamt
shift
left
logical
sll
shift
left
logical
variable
sllv
shift
right
arithmetic
sra
shift
right
arithmetic
variable
srav
shift
right
logical
srl
shift
right
logical
variable
srlv
Shifters
arithmetic
logical
Shift
instructions
Shift
registers
Short
path
Sign
magnitude
numbers
Sign
bit
Sign
extension
Significand
See
Mantissa
Silicon
Si
Silicon
dioxide
SO
Silicon
Valley
Simplicity
SIMD
See
Single
instruction
multiple
data
units
Single
cycle
MIPS
processor
See
also
MIPS
microprocessor
MIPS
architecture
MIPS
microarchitecture
control
ALU
decoder
truth
table
See
ALU
decoder
Main
decoder
truth
table
See
Main
decoder
datapath
HDL
representation
operation
performance
analysis
timing
Single
instruction
multiple
data
SIMD
units
Slash
notation
SPARC
architecture
SRAM
See
Static
random
access
memory
SP
Spatial
locality
Speech
synthesis
device
driver
SP
Stack
See
also
Memory
map
Procedure
calls
Queue
dynamic
data
segment
frame
LIFO
See
Last
in
first
out
queue
pointer
Stalls
See
also
Hazards
Static
discipline
Static
random
access
memory
SRAM
Status
flags
Stored
program
concept
Stores
store
byte
sb
store
half
sh
store
word
sw
Strings
Structural
modeling
subtract
sub
subtract
unsigned
subu
Subtraction
Subtractor
Sum
of
products
SOP
canonical
form
Sun
Microsystems
Superscalar
processor
Supply
voltage
Swap
space
Swift
Jonathan
Switch
case
statements
Symbol
table
Synchronizers
asynchronous
inputs
MTBF
See
Mean
time
before
failure
probability
of
failure
See
Probability
of
failure
Synchronous
resettable
registers
HDL
for
Synchronous
sequential
logic
problematic
circuits
Synthesis
Synthesis
Tools
T
Taking
the
two
s
complement
Temporal
locality
Testbenches
self
checking
with
test
vector
file
Text
segment
Theorems
See
Boolean
Theorems
Thin
small
outline
package
TSOP
Threshold
voltage
Throughput
Timing
analysis
delay
glitches
of
sequential
logic
clock
skew
dynamic
discipline
hold
time
See
Hold
time
hold
time
constraint
See
Hold
time
constraint
Hold
time
violation
hold
time
violation
See
Hold
time
violation
Hold
time
constraint
Index
Timing
Continued
metastability
setup
time
See
Setup
time
setup
time
constraint
setup
time
violations
resolution
time
See
also
Metastability
synchronizers
system
timing
specification
TLB
See
Translation
lookaside
buffer
Token
Transistors
CMOS
See
Complement
metal
oxide
silicon
nMOS
See
nMOS
pMOS
See
pMOS
Transistor
Transistor
Logic
TTL
Translation
lookaside
buffer
TLB
Translating
and
starting
a
program
Transmission
gates
See
also
Transistors
Transmission
lines
reflection
coefficient
Z
matched
termination
mismatched
termination
open
termination
proper
terminations
short
termination
when
to
use
Tristate
buffer
Truth
tables
ALU
decoder
don
t
care
main
decoder
multiplexer
seven
segment
display
decoder
SR
latch
with
undefined
and
floating
inputs
TSOP
See
Thin
small
outline
package
TTL
See
Transistor
Transistor
Logic
Two
level
logic
Two
s
complement
numbers
See
also
Binary
numbers
U
Unconditional
branches
See
also
Jumps
Unicode
See
also
ASCII
Unit
under
test
UUT
Unity
gain
points
Unsigned
numbers
Use
bit
UUT
See
Unit
under
test
V
Valid
bit
See
also
Caches
Virtual
memory
Vanity
Fair
Carroll
VCC
VDD
Vector
processors
See
also
Advanced
microarchitecture
Verilog
decoder
accessing
parts
of
busses
adder
ALU
decoder
AND
architecture
body
assign
statement
asynchronous
reset
bad
synchronizer
with
blocking
assignments
bit
swizzling
blocking
assignment
case
sensitivity
casez
combinational
logic
comments
comparators
continuous
assignment
statement
controller
counter
datapath
default
divide
by
finite
state
machine
D
latch
eight
input
AND
entity
declaration
full
adder
using
always
process
using
nonblocking
assignments
IEEE
STD
LOGIC
IEEE
STD
LOGIC
SIGNED
IEEE
STD
LOGIC
UNSIGNED
inverters
using
always
process
left
shift
library
use
clause
logic
gates
with
delays
main
decoder
MIPS
testbench
MIPS
top
level
module
multiplexers
multiplier
nonblocking
assignment
NOT
numbers
operator
precedence
OR
parameterized
N
N
decoder
N
bit
multiplexer
N
input
AND
gate
pattern
recognizer
Mealy
FSM
Moore
FSM
priority
circuit
RAM
reg
register
register
file
resettable
flip
flop
resettable
enabled
register
resettable
register
ROM
self
checking
testbench
seven
segment
display
decoder
shift
register
sign
extension
single
cycle
MIPS
processor
STD
LOGIC
statement
structural
models
Index
multiplexer
multiplexer
subtractor
synchronizer
synchronous
reset
testbench
with
test
vector
file
tristate
buffer
truth
tables
with
undefined
and
floating
inputs
type
declaration
wires
VHDL
libraries
and
types
decoder
accessing
parts
of
busses
adder
architecture
asynchronous
reset
bad
synchronizer
with
blocking
assignments
bit
swizzling
boolean
case
sensitivity
clk
event
combinational
logic
comments
comparators
concurrent
signal
assignment
controller
CONV
STD
LOGIC
VECTOR
counter
datapath
decoder
divide
by
finite
state
machine
D
latch
eight
input
AND
expression
full
adder
using
always
process
using
nonblocking
assignments
generic
statement
inverters
using
always
process
left
shift
logic
gates
with
delays
main
decoder
MIPS
testbench
MIPS
top
level
module
multiplexers
multiplier
numbers
operand
operator
precedence
others
parameterized
N
bit
multiplexer
N
input
AND
gate
N
N
decoder
pattern
recognizer
Mealy
FSM
Moore
FSM
priority
circuit
process
RAM
register
register
file
resettable
enabled
register
resettable
flip
flop
resettable
register
RISING
EDGE
ROM
selected
signal
assignment
statements
self
checking
testbench
seven
segment
display
decoder
shift
register
sign
extension
signals
simulation
waveforms
with
delays
single
cycle
MIPS
processor
STD
LOGIC
ARITH
structural
models
multiplexer
multiplexer
subtractor
synchronizer
synchronous
reset
testbench
with
test
vector
file
tristate
buffer
truth
tables
with
undefined
and
floating
inputs
VHSIC
Virtual
address
Virtual
memory
address
translation
IA
memory
protection
pages
page
faults
page
offset
page
table
multilevel
page
tables
replacement
policies
translation
lookaside
buffer
TLB
See
Translation
lookaside
buffer
write
policies
Virtual
pages
Virtual
page
number
Volatile
memory
See
also
DRAM
SRAM
Flip
flops
Voltage
threshold
VSS
W
Wafers
Wall
Larry
WAR
See
Write
after
read
WAW
See
Write
after
write
While
loop
White
space
Whitmore
Georgiana
Wire
Word
addressable
memory
Write
policies
Write
after
read
WAR
hazard
See
also
Hazards
Write
after
write
WAW
hazard
See
also
Hazards
X
XOR
gate
XNOR
gate
Xilinx
FPGA
X
See
Contention
Don
t
care
Z
Z
See
Floating
Zero
extension
Index
Preface
Preface
to
the
first
edition
Chapter
A
Tutorial
Introduction
Getting
Started
Variables
and
Arithmetic
Expressions
The
for
statement
Symbolic
Constants
Character
Input
and
Output
File
Copying
Character
Counting
Line
Counting
Word
Counting
Arrays
Functions
Arguments
Call
by
Value
Character
Arrays
External
Variables
and
Scope
Chapter
Types
Operators
and
Expressions
Variable
Names
Data
Types
and
Sizes
Constants
Declarations
Arithmetic
Operators
Relational
and
Logical
Operators
Type
Conversions
Increment
and
Decrement
Operators
Bitwise
Operators
Assignment
Operators
and
Expressions
Conditional
Expressions
Precedence
and
Order
of
Evaluation
Chapter
Control
Flow
Statements
and
Blocks
If
Else
Else
If
Switch
Loops
While
and
For
Loops
Do
While
Break
and
Continue
Goto
and
labels
Chapter
Functions
and
Program
Structure
Basics
of
Functions
Functions
Returning
Non
integers
External
Variables
Scope
Rules
Header
Files
Static
Variables
Register
Variables
Block
Structure
Initialization
Recursion
The
C
Preprocessor
File
Inclusion
Macro
Substitution
Conditional
Inclusion
Chapter
Pointers
and
Arrays
Pointers
and
Addresses
Pointers
and
Function
Arguments
Pointers
and
Arrays
Address
Arithmetic
Character
Pointers
and
Functions
Pointer
Arrays
Pointers
to
Pointers
Multi
dimensional
Arrays
Initialization
of
Pointer
Arrays
Pointers
vs
Multi
dimensional
Arrays
Command
line
Arguments
Pointers
to
Functions
Complicated
Declarations
Chapter
Structures
Basics
of
Structures
Structures
and
Functions
Arrays
of
Structures
Pointers
to
Structures
Self
referential
Structures
Table
Lookup
Typedef
Unions
Bit
fields
Chapter
Input
and
Output
Standard
Input
and
Output
Formatted
Output
printf
Variable
length
Argument
Lists
Formatted
Input
Scanf
File
Access
Error
Handling
Stderr
and
Exit
Line
Input
and
Output
Miscellaneous
Functions
String
Operations
Character
Class
Testing
and
Conversion
Ungetc
Command
Execution
Storage
Management
Mathematical
Functions
Random
Number
generation
Chapter
The
UNIX
System
Interface
File
Descriptors
Low
Level
I
O
Read
and
Write
Open
Creat
Close
Unlink
Random
Access
Lseek
Example
An
implementation
of
Fopen
and
Getc
Example
Listing
Directories
Example
A
Storage
Allocator
Appendix
A
Reference
Manual
A
Introduction
A
Lexical
Conventions
A
Tokens
A
Comments
A
Identifiers
A
Keywords
A
Constants
A
String
Literals
A
Syntax
Notation
A
Meaning
of
Identifiers
A
Storage
Class
A
Basic
Types
A
Derived
types
A
Type
Qualifiers
A
Objects
and
Lvalues
A
Conversions
A
Integral
Promotion
A
Integral
Conversions
A
Integer
and
Floating
A
Floating
Types
A
Arithmetic
Conversions
A
Pointers
and
Integers
A
Void
A
Pointers
to
Void
A
Expressions
A
Pointer
Conversion
A
Primary
Expressions
A
Postfix
Expressions
A
Unary
Operators
A
Casts
A
Multiplicative
Operators
A
Additive
Operators
A
Shift
Operators
A
Relational
Operators
A
Equality
Operators
A
Bitwise
AND
Operator
A
Bitwise
Exclusive
OR
Operator
A
Bitwise
Inclusive
OR
Operator
A
Logical
AND
Operator
A
Logical
OR
Operator
A
Conditional
Operator
A
Assignment
Expressions
A
Comma
Operator
A
Constant
Expressions
A
Declarations
A
Storage
Class
Specifiers
A
Type
Specifiers
A
Structure
and
Union
Declarations
A
Enumerations
A
Declarators
A
Meaning
of
Declarators
A
Initialization
A
Type
names
A
Typedef
A
Type
Equivalence
A
Statements
A
Labeled
Statements
A
Expression
Statement
A
Compound
Statement
A
Selection
Statements
A
Iteration
Statements
A
Jump
statements
A
External
Declarations
A
Function
Definitions
A
External
Declarations
A
Scope
and
Linkage
A
Lexical
Scope
A
Linkage
A
Preprocessing
A
Trigraph
Sequences
A
Line
Splicing
A
Macro
Definition
and
Expansion
A
File
Inclusion
A
Conditional
Compilation
A
Line
Control
A
Error
Generation
A
Pragmas
A
Null
directive
A
Predefined
names
A
Grammar
Appendix
B
Standard
Library
B
Input
and
Output
stdio
h
B
File
Operations
B
Formatted
Output
B
Formatted
Input
B
Character
Input
and
Output
Functions
B
Direct
Input
and
Output
Functions
B
File
Positioning
Functions
B
Error
Functions
B
Character
Class
Tests
ctype
h
B
String
Functions
string
h
B
Mathematical
Functions
math
h
B
Utility
Functions
stdlib
h
B
Diagnostics
assert
h
B
Variable
Argument
Lists
stdarg
h
B
Non
local
Jumps
setjmp
h
B
Signals
signal
h
B
Date
and
Time
Functions
time
h
B
Implementation
defined
Limits
limits
h
and
float
h
Appendix
C
Summary
of
Changes
Preface
The
computing
world
has
undergone
a
revolution
since
the
publication
of
The
C
Programming
Language
in
Big
computers
are
much
bigger
and
personal
computers
have
capabilities
that
rival
mainframes
of
a
decade
ago
During
this
time
C
has
changed
too
although
only
modestly
and
it
has
spread
far
beyond
its
origins
as
the
language
of
the
UNIX
operating
system
The
growing
popularity
of
C
the
changes
in
the
language
over
the
years
and
the
creation
of
compilers
by
groups
not
involved
in
its
design
combined
to
demonstrate
a
need
for
a
more
precise
and
more
contemporary
definition
of
the
language
than
the
first
edition
of
this
book
provided
In
the
American
National
Standards
Institute
ANSI
established
a
committee
whose
goal
was
to
produce
an
unambiguous
and
machine
independent
definition
of
the
language
C
while
still
retaining
its
spirit
The
result
is
the
ANSI
standard
for
C
The
standard
formalizes
constructions
that
were
hinted
but
not
described
in
the
first
edition
particularly
structure
assignment
and
enumerations
It
provides
a
new
form
of
function
declaration
that
permits
cross
checking
of
definition
with
use
It
specifies
a
standard
library
with
an
extensive
set
of
functions
for
performing
input
and
output
memory
management
string
manipulation
and
similar
tasks
It
makes
precise
the
behavior
of
features
that
were
not
spelled
out
in
the
original
definition
and
at
the
same
time
states
explicitly
which
aspects
of
the
language
remain
machine
dependent
This
Second
Edition
of
The
C
Programming
Language
describes
C
as
defined
by
the
ANSI
standard
Although
we
have
noted
the
places
where
the
language
has
evolved
we
have
chosen
to
write
exclusively
in
the
new
form
For
the
most
part
this
makes
no
significant
difference
the
most
visible
change
is
the
new
form
of
function
declaration
and
definition
Modern
compilers
already
support
most
features
of
the
standard
We
have
tried
to
retain
the
brevity
of
the
first
edition
C
is
not
a
big
language
and
it
is
not
well
served
by
a
big
book
We
have
improved
the
exposition
of
critical
features
such
as
pointers
that
are
central
to
C
programming
We
have
refined
the
original
examples
and
have
added
new
examples
in
several
chapters
For
instance
the
treatment
of
complicated
declarations
is
augmented
by
programs
that
convert
declarations
into
words
and
vice
versa
As
before
all
examples
have
been
tested
directly
from
the
text
which
is
in
machine
readable
form
Appendix
A
the
reference
manual
is
not
the
standard
but
our
attempt
to
convey
the
essentials
of
the
standard
in
a
smaller
space
It
is
meant
for
easy
comprehension
by
programmers
but
not
as
a
definition
for
compiler
writers
that
role
properly
belongs
to
the
standard
itself
Appendix
B
is
a
summary
of
the
facilities
of
the
standard
library
It
too
is
meant
for
reference
by
programmers
not
implementers
Appendix
C
is
a
concise
summary
of
the
changes
from
the
original
version
As
we
said
in
the
preface
to
the
first
edition
C
wears
well
as
one
s
experience
with
it
grows
With
a
decade
more
experience
we
still
feel
that
way
We
hope
that
this
book
will
help
you
learn
C
and
use
it
well
We
are
deeply
indebted
to
friends
who
helped
us
to
produce
this
second
edition
Jon
Bently
Doug
Gwyn
Doug
McIlroy
Peter
Nelson
and
Rob
Pike
gave
us
perceptive
comments
on
almost
every
page
of
draft
manuscripts
We
are
grateful
for
careful
reading
by
Al
Aho
Dennis
Allison
Joe
Campbell
G
R
Emlin
Karen
Fortgang
Allen
Holub
Andrew
Hume
Dave
Kristol
John
Linderman
Dave
Prosser
Gene
Spafford
and
Chris
van
Wyk
We
also
received
helpful
suggestions
from
Bill
Cheswick
Mark
Kernighan
Andy
Koenig
Robin
Lake
Tom
London
Jim
Reeds
Clovis
Tondo
and
Peter
Weinberger
Dave
Prosser
answered
many
detailed
questions
about
the
ANSI
standard
We
used
Bjarne
Stroustrup
s
C
translator
extensively
for
local
testing
of
our
programs
and
Dave
Kristol
provided
us
with
an
ANSI
C
compiler
for
final
testing
Rich
Drechsler
helped
greatly
with
typesetting
Our
sincere
thanks
to
all
Brian
W
Kernighan
Dennis
M
Ritchie
Preface
to
the
first
edition
C
is
a
general
purpose
programming
language
with
features
economy
of
expression
modern
flow
control
and
data
structures
and
a
rich
set
of
operators
C
is
not
a
very
high
level
language
nor
a
big
one
and
is
not
specialized
to
any
particular
area
of
application
But
its
absence
of
restrictions
and
its
generality
make
it
more
convenient
and
effective
for
many
tasks
than
supposedly
more
powerful
languages
C
was
originally
designed
for
and
implemented
on
the
UNIX
operating
system
on
the
DEC
PDP
by
Dennis
Ritchie
The
operating
system
the
C
compiler
and
essentially
all
UNIX
applications
programs
including
all
of
the
software
used
to
prepare
this
book
are
written
in
C
Production
compilers
also
exist
for
several
other
machines
including
the
IBM
System
the
Honeywell
and
the
Interdata
C
is
not
tied
to
any
particular
hardware
or
system
however
and
it
is
easy
to
write
programs
that
will
run
without
change
on
any
machine
that
supports
C
This
book
is
meant
to
help
the
reader
learn
how
to
program
in
C
It
contains
a
tutorial
introduction
to
get
new
users
started
as
soon
as
possible
separate
chapters
on
each
major
feature
and
a
reference
manual
Most
of
the
treatment
is
based
on
reading
writing
and
revising
examples
rather
than
on
mere
statements
of
rules
For
the
most
part
the
examples
are
complete
real
programs
rather
than
isolated
fragments
All
examples
have
been
tested
directly
from
the
text
which
is
in
machine
readable
form
Besides
showing
how
to
make
effective
use
of
the
language
we
have
also
tried
where
possible
to
illustrate
useful
algorithms
and
principles
of
good
style
and
sound
design
The
book
is
not
an
introductory
programming
manual
it
assumes
some
familiarity
with
basic
programming
concepts
like
variables
assignment
statements
loops
and
functions
Nonetheless
a
novice
programmer
should
be
able
to
read
along
and
pick
up
the
language
although
access
to
more
knowledgeable
colleague
will
help
In
our
experience
C
has
proven
to
be
a
pleasant
expressive
and
versatile
language
for
a
wide
variety
of
programs
It
is
easy
to
learn
and
it
wears
well
as
on
s
experience
with
it
grows
We
hope
that
this
book
will
help
you
to
use
it
well
The
thoughtful
criticisms
and
suggestions
of
many
friends
and
colleagues
have
added
greatly
to
this
book
and
to
our
pleasure
in
writing
it
In
particular
Mike
Bianchi
Jim
Blue
Stu
Feldman
Doug
McIlroy
Bill
Roome
Bob
Rosin
and
Larry
Rosler
all
read
multiple
volumes
with
care
We
are
also
indebted
to
Al
Aho
Steve
Bourne
Dan
Dvorak
Chuck
Haley
Debbie
Haley
Marion
Harris
Rick
Holt
Steve
Johnson
John
Mashey
Bob
Mitze
Ralph
Muha
Peter
Nelson
Elliot
Pinson
Bill
Plauger
Jerry
Spivack
Ken
Thompson
and
Peter
Weinberger
for
helpful
comments
at
various
stages
and
to
Mile
Lesk
and
Joe
Ossanna
for
invaluable
assistance
with
typesetting
Brian
W
Kernighan
Dennis
M
Ritchie
Chapter
A
Tutorial
Introduction
Let
us
begin
with
a
quick
introduction
in
C
Our
aim
is
to
show
the
essential
elements
of
the
language
in
real
programs
but
without
getting
bogged
down
in
details
rules
and
exceptions
At
this
point
we
are
not
trying
to
be
complete
or
even
precise
save
that
the
examples
are
meant
to
be
correct
We
want
to
get
you
as
quickly
as
possible
to
the
point
where
you
can
write
useful
programs
and
to
do
that
we
have
to
concentrate
on
the
basics
variables
and
constants
arithmetic
control
flow
functions
and
the
rudiments
of
input
and
output
We
are
intentionally
leaving
out
of
this
chapter
features
of
C
that
are
important
for
writing
bigger
programs
These
include
pointers
structures
most
of
C
s
rich
set
of
operators
several
controlflow
statements
and
the
standard
library
This
approach
and
its
drawbacks
Most
notable
is
that
the
complete
story
on
any
particular
feature
is
not
found
here
and
the
tutorial
by
being
brief
may
also
be
misleading
And
because
the
examples
do
not
use
the
full
power
of
C
they
are
not
as
concise
and
elegant
as
they
might
be
We
have
tried
to
minimize
these
effects
but
be
warned
Another
drawback
is
that
later
chapters
will
necessarily
repeat
some
of
this
chapter
We
hope
that
the
repetition
will
help
you
more
than
it
annoys
In
any
case
experienced
programmers
should
be
able
to
extrapolate
from
the
material
in
this
chapter
to
their
own
programming
needs
Beginners
should
supplement
it
by
writing
small
similar
programs
of
their
own
Both
groups
can
use
it
as
a
framework
on
which
to
hang
the
more
detailed
descriptions
that
begin
in
Chapter
Getting
Started
The
only
way
to
learn
a
new
programming
language
is
by
writing
programs
in
it
The
first
program
to
write
is
the
same
for
all
languages
Print
the
words
hello
world
This
is
a
big
hurdle
to
leap
over
it
you
have
to
be
able
to
create
the
program
text
somewhere
compile
it
successfully
load
it
run
it
and
find
out
where
your
output
went
With
these
mechanical
details
mastered
everything
else
is
comparatively
easy
In
C
the
program
to
print
hello
world
is
include
stdio
h
main
printf
hello
world
n
Just
how
to
run
this
program
depends
on
the
system
you
are
using
As
a
specific
example
on
the
UNIX
operating
system
you
must
create
the
program
in
a
file
whose
name
ends
in
c
such
as
hello
c
then
compile
it
with
the
command
cc
hello
c
If
you
haven
t
botched
anything
such
as
omitting
a
character
or
misspelling
something
the
compilation
will
proceed
silently
and
make
an
executable
file
called
a
out
If
you
run
a
out
by
typing
the
command
a
out
it
will
print
hello
world
On
other
systems
the
rules
will
be
different
check
with
a
local
expert
Now
for
some
explanations
about
the
program
itself
A
C
program
whatever
its
size
consists
of
functions
and
variables
A
function
contains
statements
that
specify
the
computing
operations
to
be
done
and
variables
store
values
used
during
the
computation
C
functions
are
like
the
subroutines
and
functions
in
Fortran
or
the
procedures
and
functions
of
Pascal
Our
example
is
a
function
named
main
Normally
you
are
at
liberty
to
give
functions
whatever
names
you
like
but
main
is
special
your
program
begins
executing
at
the
beginning
of
main
This
means
that
every
program
must
have
a
main
somewhere
main
will
usually
call
other
functions
to
help
perform
its
job
some
that
you
wrote
and
others
from
libraries
that
are
provided
for
you
The
first
line
of
the
program
include
stdio
h
tells
the
compiler
to
include
information
about
the
standard
input
output
library
the
line
appears
at
the
beginning
of
many
C
source
files
The
standard
library
is
described
in
Chapter
and
Appendix
B
One
method
of
communicating
data
between
functions
is
for
the
calling
function
to
provide
a
list
of
values
called
arguments
to
the
function
it
calls
The
parentheses
after
the
function
name
surround
the
argument
list
In
this
example
main
is
defined
to
be
a
function
that
expects
no
arguments
which
is
indicated
by
the
empty
list
include
stdio
h
include
information
about
standard
library
main
define
a
function
called
main
that
received
no
argument
values
statements
of
main
are
enclosed
in
braces
printf
hello
world
n
main
calls
library
function
printf
to
print
this
sequence
of
characters
n
represents
the
newline
character
The
first
C
program
The
statements
of
a
function
are
enclosed
in
braces
The
function
main
contains
only
one
statement
printf
hello
world
n
A
function
is
called
by
naming
it
followed
by
a
parenthesized
list
of
arguments
so
this
calls
the
function
printf
with
the
argument
hello
world
n
printf
is
a
library
function
that
prints
output
in
this
case
the
string
of
characters
between
the
quotes
A
sequence
of
characters
in
double
quotes
like
hello
world
n
is
called
a
character
string
or
string
constant
For
the
moment
our
only
use
of
character
strings
will
be
as
arguments
for
printf
and
other
functions
The
sequence
n
in
the
string
is
C
notation
for
the
newline
character
which
when
printed
advances
the
output
to
the
left
margin
on
the
next
line
If
you
leave
out
the
n
a
worthwhile
experiment
you
will
find
that
there
is
no
line
advance
after
the
output
is
printed
You
must
use
n
to
include
a
newline
character
in
the
printf
argument
if
you
try
something
like
printf
hello
world
the
C
compiler
will
produce
an
error
message
printf
never
supplies
a
newline
character
automatically
so
several
calls
may
be
used
to
build
up
an
output
line
in
stages
Our
first
program
could
just
as
well
have
been
written
include
stdio
h
main
printf
hello
printf
world
printf
n
to
produce
identical
output
Notice
that
n
represents
only
a
single
character
An
escape
sequence
like
n
provides
a
general
and
extensible
mechanism
for
representing
hard
to
type
or
invisible
characters
Among
the
others
that
C
provides
are
t
for
tab
b
for
backspace
for
the
double
quote
and
for
the
backslash
itself
There
is
a
complete
list
in
Section
Exercise
Run
the
hello
world
program
on
your
system
Experiment
with
leaving
out
parts
of
the
program
to
see
what
error
messages
you
get
Exercise
Experiment
to
find
out
what
happens
when
prints
s
argument
string
contains
c
where
c
is
some
character
not
listed
above
Variables
and
Arithmetic
Expressions
The
next
program
uses
the
formula
oC
oF
to
print
the
following
table
of
Fahrenheit
temperatures
and
their
centigrade
or
Celsius
equivalents
The
program
itself
still
consists
of
the
definition
of
a
single
function
named
main
It
is
longer
than
the
one
that
printed
hello
world
but
not
complicated
It
introduces
several
new
ideas
including
comments
declarations
variables
arithmetic
expressions
loops
and
formatted
output
include
stdio
h
print
Fahrenheit
Celsius
table
for
fahr
main
int
fahr
celsius
int
lower
upper
step
lower
lower
limit
of
temperature
scale
upper
upper
limit
step
step
size
fahr
lower
while
fahr
upper
celsius
fahr
printf
d
t
d
n
fahr
celsius
fahr
fahr
step
The
two
lines
print
Fahrenheit
Celsius
table
for
fahr
are
a
comment
which
in
this
case
explains
briefly
what
the
program
does
Any
characters
between
and
are
ignored
by
the
compiler
they
may
be
used
freely
to
make
a
program
easier
to
understand
Comments
may
appear
anywhere
where
a
blank
tab
or
newline
can
In
C
all
variables
must
be
declared
before
they
are
used
usually
at
the
beginning
of
the
function
before
any
executable
statements
A
declaration
announces
the
properties
of
variables
it
consists
of
a
name
and
a
list
of
variables
such
as
int
fahr
celsius
int
lower
upper
step
The
type
int
means
that
the
variables
listed
are
integers
by
contrast
with
float
which
means
floating
point
i
e
numbers
that
may
have
a
fractional
part
The
range
of
both
int
and
float
depends
on
the
machine
you
are
using
bits
ints
which
lie
between
and
are
common
as
are
bit
ints
A
float
number
is
typically
a
bit
quantity
with
at
least
six
significant
digits
and
magnitude
generally
between
about
and
C
provides
several
other
data
types
besides
int
and
float
including
char
character
a
single
byte
short
short
integer
long
long
integer
double
double
precision
floating
point
The
size
of
these
objects
is
also
machine
dependent
There
are
also
arrays
structures
and
unions
of
these
basic
types
pointers
to
them
and
functions
that
return
them
all
of
which
we
will
meet
in
due
course
Computation
in
the
temperature
conversion
program
begins
with
the
assignment
statements
lower
upper
step
which
set
the
variables
to
their
initial
values
Individual
statements
are
terminated
by
semicolons
Each
line
of
the
table
is
computed
the
same
way
so
we
use
a
loop
that
repeats
once
per
output
line
this
is
the
purpose
of
the
while
loop
while
fahr
upper
The
while
loop
operates
as
follows
The
condition
in
parentheses
is
tested
If
it
is
true
fahr
is
less
than
or
equal
to
upper
the
body
of
the
loop
the
three
statements
enclosed
in
braces
is
executed
Then
the
condition
is
re
tested
and
if
true
the
body
is
executed
again
When
the
test
becomes
false
fahr
exceeds
upper
the
loop
ends
and
execution
continues
at
the
statement
that
follows
the
loop
There
are
no
further
statements
in
this
program
so
it
terminates
The
body
of
a
while
can
be
one
or
more
statements
enclosed
in
braces
as
in
the
temperature
converter
or
a
single
statement
without
braces
as
in
while
i
j
i
i
In
either
case
we
will
always
indent
the
statements
controlled
by
the
while
by
one
tab
stop
which
we
have
shown
as
four
spaces
so
you
can
see
at
a
glance
which
statements
are
inside
the
loop
The
indentation
emphasizes
the
logical
structure
of
the
program
Although
C
compilers
do
not
care
about
how
a
program
looks
proper
indentation
and
spacing
are
critical
in
making
programs
easy
for
people
to
read
We
recommend
writing
only
one
statement
per
line
and
using
blanks
around
operators
to
clarify
grouping
The
position
of
braces
is
less
important
although
people
hold
passionate
beliefs
We
have
chosen
one
of
several
popular
styles
Pick
a
style
that
suits
you
then
use
it
consistently
Most
of
the
work
gets
done
in
the
body
of
the
loop
The
Celsius
temperature
is
computed
and
assigned
to
the
variable
celsius
by
the
statement
celsius
fahr
The
reason
for
multiplying
by
and
dividing
by
instead
of
just
multiplying
by
is
that
in
C
as
in
many
other
languages
integer
division
truncates
any
fractional
part
is
discarded
Since
and
are
integers
would
be
truncated
to
zero
and
so
all
the
Celsius
temperatures
would
be
reported
as
zero
This
example
also
shows
a
bit
more
of
how
printf
works
printf
is
a
general
purpose
output
formatting
function
which
we
will
describe
in
detail
in
Chapter
Its
first
argument
is
a
string
of
characters
to
be
printed
with
each
indicating
where
one
of
the
other
second
third
arguments
is
to
be
substituted
and
in
what
form
it
is
to
be
printed
For
instance
d
specifies
an
integer
argument
so
the
statement
printf
d
t
d
n
fahr
celsius
causes
the
values
of
the
two
integers
fahr
and
celsius
to
be
printed
with
a
tab
t
between
them
Each
construction
in
the
first
argument
of
printf
is
paired
with
the
corresponding
second
argument
third
argument
etc
they
must
match
up
properly
by
number
and
type
or
you
will
get
wrong
answers
By
the
way
printf
is
not
part
of
the
C
language
there
is
no
input
or
output
defined
in
C
itself
printf
is
just
a
useful
function
from
the
standard
library
of
functions
that
are
normally
accessible
to
C
programs
The
behaviour
of
printf
is
defined
in
the
ANSI
standard
however
so
its
properties
should
be
the
same
with
any
compiler
and
library
that
conforms
to
the
standard
In
order
to
concentrate
on
C
itself
we
don
t
talk
much
about
input
and
output
until
chapter
In
particular
we
will
defer
formatted
input
until
then
If
you
have
to
input
numbers
read
the
discussion
of
the
function
scanf
in
Section
scanf
is
like
printf
except
that
it
reads
input
instead
of
writing
output
There
are
a
couple
of
problems
with
the
temperature
conversion
program
The
simpler
one
is
that
the
output
isn
t
very
pretty
because
the
numbers
are
not
right
justified
That
s
easy
to
fix
if
we
augment
each
d
in
the
printf
statement
with
a
width
the
numbers
printed
will
be
rightjustified
in
their
fields
For
instance
we
might
say
printf
d
d
n
fahr
celsius
to
print
the
first
number
of
each
line
in
a
field
three
digits
wide
and
the
second
in
a
field
six
digits
wide
like
this
The
more
serious
problem
is
that
because
we
have
used
integer
arithmetic
the
Celsius
temperatures
are
not
very
accurate
for
instance
oF
is
actually
about
oC
not
To
get
more
accurate
answers
we
should
use
floating
point
arithmetic
instead
of
integer
This
requires
some
changes
in
the
program
Here
is
the
second
version
include
stdio
h
print
Fahrenheit
Celsius
table
for
fahr
floating
point
version
main
float
fahr
celsius
float
lower
upper
step
lower
lower
limit
of
temperatuire
scale
upper
upper
limit
step
step
size
fahr
lower
while
fahr
upper
celsius
fahr
printf
f
f
n
fahr
celsius
fahr
fahr
step
This
is
much
the
same
as
before
except
that
fahr
and
celsius
are
declared
to
be
float
and
the
formula
for
conversion
is
written
in
a
more
natural
way
We
were
unable
to
use
in
the
previous
version
because
integer
division
would
truncate
it
to
zero
A
decimal
point
in
a
constant
indicates
that
it
is
floating
point
however
so
is
not
truncated
because
it
is
the
ratio
of
two
floating
point
values
If
an
arithmetic
operator
has
integer
operands
an
integer
operation
is
performed
If
an
arithmetic
operator
has
one
floating
point
operand
and
one
integer
operand
however
the
integer
will
be
converted
to
floating
point
before
the
operation
is
done
If
we
had
written
fahr
the
would
be
automatically
converted
to
floating
point
Nevertheless
writing
floating
point
constants
with
explicit
decimal
points
even
when
they
have
integral
values
emphasizes
their
floating
point
nature
for
human
readers
The
detailed
rules
for
when
integers
are
converted
to
floating
point
are
in
Chapter
For
now
notice
that
the
assignment
fahr
lower
and
the
test
while
fahr
upper
also
work
in
the
natural
way
the
int
is
converted
to
float
before
the
operation
is
done
The
printf
conversion
specification
f
says
that
a
floating
point
number
here
fahr
is
to
be
printed
at
least
three
characters
wide
with
no
decimal
point
and
no
fraction
digits
f
describes
another
number
celsius
that
is
to
be
printed
at
least
six
characters
wide
with
digit
after
the
decimal
point
The
output
looks
like
this
Width
and
precision
may
be
omitted
from
a
specification
f
says
that
the
number
is
to
be
at
least
six
characters
wide
f
specifies
two
characters
after
the
decimal
point
but
the
width
is
not
constrained
and
f
merely
says
to
print
the
number
as
floating
point
d
print
as
decimal
integer
d
print
as
decimal
integer
at
least
characters
wide
f
print
as
floating
point
f
print
as
floating
point
at
least
characters
wide
f
print
as
floating
point
characters
after
decimal
point
f
print
as
floating
point
at
least
wide
and
after
decimal
point
Among
others
printf
also
recognizes
o
for
octal
x
for
hexadecimal
c
for
character
s
for
character
string
and
for
itself
Exercise
Modify
the
temperature
conversion
program
to
print
a
heading
above
the
table
Exercise
Write
a
program
to
print
the
corresponding
Celsius
to
Fahrenheit
table
The
for
statement
There
are
plenty
of
different
ways
to
write
a
program
for
a
particular
task
Let
s
try
a
variation
on
the
temperature
converter
include
stdio
h
print
Fahrenheit
Celsius
table
main
int
fahr
for
fahr
fahr
fahr
fahr
printf
d
f
n
fahr
fahr
This
produces
the
same
answers
but
it
certainly
looks
different
One
major
change
is
the
elimination
of
most
of
the
variables
only
fahr
remains
and
we
have
made
it
an
int
The
lower
and
upper
limits
and
the
step
size
appear
only
as
constants
in
the
for
statement
itself
a
new
construction
and
the
expression
that
computes
the
Celsius
temperature
now
appears
as
the
third
argument
of
printf
instead
of
a
separate
assignment
statement
This
last
change
is
an
instance
of
a
general
rule
in
any
context
where
it
is
permissible
to
use
the
value
of
some
type
you
can
use
a
more
complicated
expression
of
that
type
Since
the
third
argument
of
printf
must
be
a
floating
point
value
to
match
the
f
any
floating
point
expression
can
occur
here
The
for
statement
is
a
loop
a
generalization
of
the
while
If
you
compare
it
to
the
earlier
while
its
operation
should
be
clear
Within
the
parentheses
there
are
three
parts
separated
by
semicolons
The
first
part
the
initialization
fahr
is
done
once
before
the
loop
proper
is
entered
The
second
part
is
the
test
or
condition
that
controls
the
loop
fahr
This
condition
is
evaluated
if
it
is
true
the
body
of
the
loop
here
a
single
ptintf
is
executed
Then
the
increment
step
fahr
fahr
is
executed
and
the
condition
re
evaluated
The
loop
terminates
if
the
condition
has
become
false
As
with
the
while
the
body
of
the
loop
can
be
a
single
statement
or
a
group
of
statements
enclosed
in
braces
The
initialization
condition
and
increment
can
be
any
expressions
The
choice
between
while
and
for
is
arbitrary
based
on
which
seems
clearer
The
for
is
usually
appropriate
for
loops
in
which
the
initialization
and
increment
are
single
statements
and
logically
related
since
it
is
more
compact
than
while
and
it
keeps
the
loop
control
statements
together
in
one
place
Exercise
Modify
the
temperature
conversion
program
to
print
the
table
in
reverse
order
that
is
from
degrees
to
Symbolic
Constants
A
final
observation
before
we
leave
temperature
conversion
forever
It
s
bad
practice
to
bury
magic
numbers
like
and
in
a
program
they
convey
little
information
to
someone
who
might
have
to
read
the
program
later
and
they
are
hard
to
change
in
a
systematic
way
One
way
to
deal
with
magic
numbers
is
to
give
them
meaningful
names
A
define
line
defines
a
symbolic
name
or
symbolic
constant
to
be
a
particular
string
of
characters
define
name
replacement
list
Thereafter
any
occurrence
of
name
not
in
quotes
and
not
part
of
another
name
will
be
replaced
by
the
corresponding
replacement
text
The
name
has
the
same
form
as
a
variable
name
a
sequence
of
letters
and
digits
that
begins
with
a
letter
The
replacement
text
can
be
any
sequence
of
characters
it
is
not
limited
to
numbers
include
stdio
h
define
LOWER
lower
limit
of
table
define
UPPER
upper
limit
define
STEP
step
size
print
Fahrenheit
Celsius
table
main
int
fahr
for
fahr
LOWER
fahr
UPPER
fahr
fahr
STEP
printf
d
f
n
fahr
fahr
The
quantities
LOWER
UPPER
and
STEP
are
symbolic
constants
not
variables
so
they
do
not
appear
in
declarations
Symbolic
constant
names
are
conventionally
written
in
upper
case
so
they
can
ber
readily
distinguished
from
lower
case
variable
names
Notice
that
there
is
no
semicolon
at
the
end
of
a
define
line
Character
Input
and
Output
We
are
going
to
consider
a
family
of
related
programs
for
processing
character
data
You
will
find
that
many
programs
are
just
expanded
versions
of
the
prototypes
that
we
discuss
here
The
model
of
input
and
output
supported
by
the
standard
library
is
very
simple
Text
input
or
output
regardless
of
where
it
originates
or
where
it
goes
to
is
dealt
with
as
streams
of
characters
A
text
stream
is
a
sequence
of
characters
divided
into
lines
each
line
consists
of
zero
or
more
characters
followed
by
a
newline
character
It
is
the
responsibility
of
the
library
to
make
each
input
or
output
stream
confirm
this
model
the
C
programmer
using
the
library
need
not
worry
about
how
lines
are
represented
outside
the
program
The
standard
library
provides
several
functions
for
reading
or
writing
one
character
at
a
time
of
which
getchar
and
putchar
are
the
simplest
Each
time
it
is
called
getchar
reads
the
next
input
character
from
a
text
stream
and
returns
that
as
its
value
That
is
after
c
getchar
the
variable
c
contains
the
next
character
of
input
The
characters
normally
come
from
the
keyboard
input
from
files
is
discussed
in
Chapter
The
function
putchar
prints
a
character
each
time
it
is
called
putchar
c
prints
the
contents
of
the
integer
variable
c
as
a
character
usually
on
the
screen
Calls
to
putchar
and
printf
may
be
interleaved
the
output
will
appear
in
the
order
in
which
the
calls
are
made
File
Copying
Given
getchar
and
putchar
you
can
write
a
surprising
amount
of
useful
code
without
knowing
anything
more
about
input
and
output
The
simplest
example
is
a
program
that
copies
its
input
to
its
output
one
character
at
a
time
read
a
character
while
charater
is
not
end
of
file
indicator
output
the
character
just
read
read
a
character
Converting
this
into
C
gives
include
stdio
h
copy
input
to
output
st
version
main
int
c
c
getchar
while
c
EOF
putchar
c
c
getchar
The
relational
operator
means
not
equal
to
What
appears
to
be
a
character
on
the
keyboard
or
screen
is
of
course
like
everything
else
stored
internally
just
as
a
bit
pattern
The
type
char
is
specifically
meant
for
storing
such
character
data
but
any
integer
type
can
be
used
We
used
int
for
a
subtle
but
important
reason
The
problem
is
distinguishing
the
end
of
input
from
valid
data
The
solution
is
that
getchar
returns
a
distinctive
value
when
there
is
no
more
input
a
value
that
cannot
be
confused
with
any
real
character
This
value
is
called
EOF
for
end
of
file
We
must
declare
c
to
be
a
type
big
enough
to
hold
any
value
that
getchar
returns
We
can
t
use
char
since
c
must
be
big
enough
to
hold
EOF
in
addition
to
any
possible
char
Therefore
we
use
int
EOF
is
an
integer
defined
in
stdio
h
but
the
specific
numeric
value
doesn
t
matter
as
long
as
it
is
not
the
same
as
any
char
value
By
using
the
symbolic
constant
we
are
assured
that
nothing
in
the
program
depends
on
the
specific
numeric
value
The
program
for
copying
would
be
written
more
concisely
by
experienced
C
programmers
In
C
any
assignment
such
as
c
getchar
is
an
expression
and
has
a
value
which
is
the
value
of
the
left
hand
side
after
the
assignment
This
means
that
a
assignment
can
appear
as
part
of
a
larger
expression
If
the
assignment
of
a
character
to
c
is
put
inside
the
test
part
of
a
while
loop
the
copy
program
can
be
written
this
way
include
stdio
h
copy
input
to
output
nd
version
main
int
c
while
c
getchar
EOF
putchar
c
The
while
gets
a
character
assigns
it
to
c
and
then
tests
whether
the
character
was
the
endof
file
signal
If
it
was
not
the
body
of
the
while
is
executed
printing
the
character
The
while
then
repeats
When
the
end
of
the
input
is
finally
reached
the
while
terminates
and
so
does
main
This
version
centralizes
the
input
there
is
now
only
one
reference
to
getchar
and
shrinks
the
program
The
resulting
program
is
more
compact
and
once
the
idiom
is
mastered
easier
to
read
You
ll
see
this
style
often
It
s
possible
to
get
carried
away
and
create
impenetrable
code
however
a
tendency
that
we
will
try
to
curb
The
parentheses
around
the
assignment
within
the
condition
are
necessary
The
precedence
of
is
higher
than
that
of
which
means
that
in
the
absence
of
parentheses
the
relational
test
would
be
done
before
the
assignment
So
the
statement
c
getchar
EOF
is
equivalent
to
c
getchar
EOF
This
has
the
undesired
effect
of
setting
c
to
or
depending
on
whether
or
not
the
call
of
getchar
returned
end
of
file
More
on
this
in
Chapter
Exercsise
Verify
that
the
expression
getchar
EOF
is
or
Exercise
Write
a
program
to
print
the
value
of
EOF
Character
Counting
The
next
program
counts
characters
it
is
similar
to
the
copy
program
include
stdio
h
count
characters
in
input
st
version
main
long
nc
nc
while
getchar
EOF
nc
printf
ld
n
nc
The
statement
nc
presents
a
new
operator
which
means
increment
by
one
You
could
instead
write
nc
nc
but
nc
is
more
concise
and
often
more
efficient
There
is
a
corresponding
operator
to
decrement
by
The
operators
and
can
be
either
prefix
operators
nc
or
postfix
operators
nc
these
two
forms
have
different
values
in
expressions
as
will
be
shown
in
Chapter
but
nc
and
nc
both
increment
nc
For
the
moment
we
will
will
stick
to
the
prefix
form
The
character
counting
program
accumulates
its
count
in
a
long
variable
instead
of
an
int
long
integers
are
at
least
bits
Although
on
some
machines
int
and
long
are
the
same
size
on
others
an
int
is
bits
with
a
maximum
value
of
and
it
would
take
relatively
little
input
to
overflow
an
int
counter
The
conversion
specification
ld
tells
printf
that
the
corresponding
argument
is
a
long
integer
It
may
be
possible
to
cope
with
even
bigger
numbers
by
using
a
double
double
precision
float
We
will
also
use
a
for
statement
instead
of
a
while
to
illustrate
another
way
to
write
the
loop
include
stdio
h
count
characters
in
input
nd
version
main
double
nc
for
nc
gechar
EOF
nc
printf
f
n
nc
printf
uses
f
for
both
float
and
double
f
suppresses
the
printing
of
the
decimal
point
and
the
fraction
part
which
is
zero
The
body
of
this
for
loop
is
empty
because
all
the
work
is
done
in
the
test
and
increment
parts
But
the
grammatical
rules
of
C
require
that
a
for
statement
have
a
body
The
isolated
semicolon
called
a
null
statement
is
there
to
satisfy
that
requirement
We
put
it
on
a
separate
line
to
make
it
visible
Before
we
leave
the
character
counting
program
observe
that
if
the
input
contains
no
characters
the
while
or
for
test
fails
on
the
very
first
call
to
getchar
and
the
program
produces
zero
the
right
answer
This
is
important
One
of
the
nice
things
about
while
and
for
is
that
they
test
at
the
top
of
the
loop
before
proceeding
with
the
body
If
there
is
nothing
to
do
nothing
is
done
even
if
that
means
never
going
through
the
loop
body
Programs
should
act
intelligently
when
given
zero
length
input
The
while
and
for
statements
help
ensure
that
programs
do
reasonable
things
with
boundary
conditions
Line
Counting
The
next
program
counts
input
lines
As
we
mentioned
above
the
standard
library
ensures
that
an
input
text
stream
appears
as
a
sequence
of
lines
each
terminated
by
a
newline
Hence
counting
lines
is
just
counting
newlines
include
stdio
h
count
lines
in
input
main
int
c
nl
nl
while
c
getchar
EOF
if
c
n
nl
printf
d
n
nl
The
body
of
the
while
now
consists
of
an
if
which
in
turn
controls
the
increment
nl
The
if
statement
tests
the
parenthesized
condition
and
if
the
condition
is
true
executes
the
statement
or
group
of
statements
in
braces
that
follows
We
have
again
indented
to
show
what
is
controlled
by
what
The
double
equals
sign
is
the
C
notation
for
is
equal
to
like
Pascal
s
single
or
Fortran
s
EQ
This
symbol
is
used
to
distinguish
the
equality
test
from
the
single
that
C
uses
for
assignment
A
word
of
caution
newcomers
to
C
occasionally
write
when
they
mean
As
we
will
see
in
Chapter
the
result
is
usually
a
legal
expression
so
you
will
get
no
warning
A
character
written
between
single
quotes
represents
an
integer
value
equal
to
the
numerical
value
of
the
character
in
the
machine
s
character
set
This
is
called
a
character
constant
although
it
is
just
another
way
to
write
a
small
integer
So
for
example
A
is
a
character
constant
in
the
ASCII
character
set
its
value
is
the
internal
representation
of
the
character
A
Of
course
A
is
to
be
preferred
over
its
meaning
is
obvious
and
it
is
independent
of
a
particular
character
set
The
escape
sequences
used
in
string
constants
are
also
legal
in
character
constants
so
n
stands
for
the
value
of
the
newline
character
which
is
in
ASCII
You
should
note
carefully
that
n
is
a
single
character
and
in
expressions
is
just
an
integer
on
the
other
hand
n
is
a
string
constant
that
happens
to
contain
only
one
character
The
topic
of
strings
versus
characters
is
discussed
further
in
Chapter
Exercise
Write
a
program
to
count
blanks
tabs
and
newlines
Exercise
Write
a
program
to
copy
its
input
to
its
output
replacing
each
string
of
one
or
more
blanks
by
a
single
blank
Exercise
Write
a
program
to
copy
its
input
to
its
output
replacing
each
tab
by
t
each
backspace
by
b
and
each
backslash
by
This
makes
tabs
and
backspaces
visible
in
an
unambiguous
way
Word
Counting
The
fourth
in
our
series
of
useful
programs
counts
lines
words
and
characters
with
the
loose
definition
that
a
word
is
any
sequence
of
characters
that
does
not
contain
a
blank
tab
or
newline
This
is
a
bare
bones
version
of
the
UNIX
program
wc
include
stdio
h
define
IN
inside
a
word
define
OUT
outside
a
word
count
lines
words
and
characters
in
input
main
int
c
nl
nw
nc
state
state
OUT
nl
nw
nc
while
c
getchar
EOF
nc
if
c
n
nl
if
c
c
n
c
t
state
OUT
else
if
state
OUT
state
IN
nw
printf
d
d
d
n
nl
nw
nc
Every
time
the
program
encounters
the
first
character
of
a
word
it
counts
one
more
word
The
variable
state
records
whether
the
program
is
currently
in
a
word
or
not
initially
it
is
not
in
a
word
which
is
assigned
the
value
OUT
We
prefer
the
symbolic
constants
IN
and
OUT
to
the
literal
values
and
because
they
make
the
program
more
readable
In
a
program
as
tiny
as
this
it
makes
little
difference
but
in
larger
programs
the
increase
in
clarity
is
well
worth
the
modest
extra
effort
to
write
it
this
way
from
the
beginning
You
ll
also
find
that
it
s
easier
to
make
extensive
changes
in
programs
where
magic
numbers
appear
only
as
symbolic
constants
The
line
nl
nw
nc
sets
all
three
variables
to
zero
This
is
not
a
special
case
but
a
consequence
of
the
fact
that
an
assignment
is
an
expression
with
the
value
and
assignments
associated
from
right
to
left
It
s
as
if
we
had
written
nl
nw
nc
The
operator
means
OR
so
the
line
if
c
c
n
c
t
says
if
c
is
a
blank
or
c
is
a
newline
or
c
is
a
tab
Recall
that
the
escape
sequence
t
is
a
visible
representation
of
the
tab
character
There
is
a
corresponding
operator
for
AND
its
precedence
is
just
higher
than
Expressions
connected
by
or
are
evaluated
left
to
right
and
it
is
guaranteed
that
evaluation
will
stop
as
soon
as
the
truth
or
falsehood
is
known
If
c
is
a
blank
there
is
no
need
to
test
whether
it
is
a
newline
or
tab
so
these
tests
are
not
made
This
isn
t
particularly
important
here
but
is
significant
in
more
complicated
situations
as
we
will
soon
see
The
example
also
shows
an
else
which
specifies
an
alternative
action
if
the
condition
part
of
an
if
statement
is
false
The
general
form
is
if
expression
statement
else
statement
One
and
only
one
of
the
two
statements
associated
with
an
if
else
is
performed
If
the
expression
is
true
statement
is
executed
if
not
statement
is
executed
Each
statement
can
be
a
single
statement
or
several
in
braces
In
the
word
count
program
the
one
after
the
else
is
an
if
that
controls
two
statements
in
braces
Exercise
How
would
you
test
the
word
count
program
What
kinds
of
input
are
most
likely
to
uncover
bugs
if
there
are
any
Exercise
Write
a
program
that
prints
its
input
one
word
per
line
Arrays
Let
is
write
a
program
to
count
the
number
of
occurrences
of
each
digit
of
white
space
characters
blank
tab
newline
and
of
all
other
characters
This
is
artificial
but
it
permits
us
to
illustrate
several
aspects
of
C
in
one
program
There
are
twelve
categories
of
input
so
it
is
convenient
to
use
an
array
to
hold
the
number
of
occurrences
of
each
digit
rather
than
ten
individual
variables
Here
is
one
version
of
the
program
include
stdio
h
count
digits
white
space
others
main
int
c
i
nwhite
nother
int
ndigit
nwhite
nother
for
i
i
i
ndigit
i
while
c
getchar
EOF
if
c
c
ndigit
c
else
if
c
c
n
c
t
nwhite
else
nother
printf
digits
for
i
i
i
printf
d
ndigit
i
printf
white
space
d
other
d
n
nwhite
nother
The
output
of
this
program
on
itself
is
digits
white
space
other
The
declaration
int
ndigit
declares
ndigit
to
be
an
array
of
integers
Array
subscripts
always
start
at
zero
in
C
so
the
elements
are
ndigit
ndigit
ndigit
This
is
reflected
in
the
for
loops
that
initialize
and
print
the
array
A
subscript
can
be
any
integer
expression
which
includes
integer
variables
like
i
and
integer
constants
This
particular
program
relies
on
the
properties
of
the
character
representation
of
the
digits
For
example
the
test
if
c
c
determines
whether
the
character
in
c
is
a
digit
If
it
is
the
numeric
value
of
that
digit
is
c
This
works
only
if
have
consecutive
increasing
values
Fortunately
this
is
true
for
all
character
sets
By
definition
chars
are
just
small
integers
so
char
variables
and
constants
are
identical
to
ints
in
arithmetic
expressions
This
is
natural
and
convenient
for
example
c
is
an
integer
expression
with
a
value
between
and
corresponding
to
the
character
to
stored
in
c
and
thus
a
valid
subscript
for
the
array
ndigit
The
decision
as
to
whether
a
character
is
a
digit
white
space
or
something
else
is
made
with
the
sequence
if
c
c
ndigit
c
else
if
c
c
n
c
t
nwhite
else
nother
The
pattern
if
condition
statement
else
if
condition
statement
else
statementn
occurs
frequently
in
programs
as
a
way
to
express
a
multi
way
decision
The
conditions
are
evaluated
in
order
from
the
top
until
some
condition
is
satisfied
at
that
point
the
corresponding
statement
part
is
executed
and
the
entire
construction
is
finished
Any
statement
can
be
several
statements
enclosed
in
braces
If
none
of
the
conditions
is
satisfied
the
statement
after
the
final
else
is
executed
if
it
is
present
If
the
final
else
and
statement
are
omitted
as
in
the
word
count
program
no
action
takes
place
There
can
be
any
number
of
else
if
condition
statement
groups
between
the
initial
if
and
the
final
else
As
a
matter
of
style
it
is
advisable
to
format
this
construction
as
we
have
shown
if
each
if
were
indented
past
the
previous
else
a
long
sequence
of
decisions
would
march
off
the
right
side
of
the
page
The
switch
statement
to
be
discussed
in
Chapter
provides
another
way
to
write
a
multiway
branch
that
is
particulary
suitable
when
the
condition
is
whether
some
integer
or
character
expression
matches
one
of
a
set
of
constants
For
contrast
we
will
present
a
switch
version
of
this
program
in
Section
Exercise
Write
a
program
to
print
a
histogram
of
the
lengths
of
words
in
its
input
It
is
easy
to
draw
the
histogram
with
the
bars
horizontal
a
vertical
orientation
is
more
challenging
Exercise
Write
a
program
to
print
a
histogram
of
the
frequencies
of
different
characters
in
its
input
Functions
In
C
a
function
is
equivalent
to
a
subroutine
or
function
in
Fortran
or
a
procedure
or
function
in
Pascal
A
function
provides
a
convenient
way
to
encapsulate
some
computation
which
can
then
be
used
without
worrying
about
its
implementation
With
properly
designed
functions
it
is
possible
to
ignore
how
a
job
is
done
knowing
what
is
done
is
sufficient
C
makes
the
sue
of
functions
easy
convinient
and
efficient
you
will
often
see
a
short
function
defined
and
called
only
once
just
because
it
clarifies
some
piece
of
code
So
far
we
have
used
only
functions
like
printf
getchar
and
putchar
that
have
been
provided
for
us
now
it
s
time
to
write
a
few
of
our
own
Since
C
has
no
exponentiation
operator
like
the
of
Fortran
let
us
illustrate
the
mechanics
of
function
definition
by
writing
a
function
power
m
n
to
raise
an
integer
m
to
a
positive
integer
power
n
That
is
the
value
of
power
is
This
function
is
not
a
practical
exponentiation
routine
since
it
handles
only
positive
powers
of
small
integers
but
it
s
good
enough
for
illustration
The
standard
library
contains
a
function
pow
x
y
that
computes
x
y
Here
is
the
function
power
and
a
main
program
to
exercise
it
so
you
can
see
the
whole
structure
at
once
include
stdio
h
int
power
int
m
int
n
test
power
function
main
int
i
for
i
i
i
printf
d
d
d
n
i
power
i
power
i
return
power
raise
base
to
n
th
power
n
int
power
int
base
int
n
int
i
p
p
for
i
i
n
i
p
p
base
return
p
A
function
definition
has
this
form
return
type
function
name
parameter
declarations
if
any
declarations
statements
Function
definitions
can
appear
in
any
order
and
in
one
source
file
or
several
although
no
function
can
be
split
between
files
If
the
source
program
appears
in
several
files
you
may
have
to
say
more
to
compile
and
load
it
than
if
it
all
appears
in
one
but
that
is
an
operating
system
matter
not
a
language
attribute
For
the
moment
we
will
assume
that
both
functions
are
in
the
same
file
so
whatever
you
have
learned
about
running
C
programs
will
still
work
The
function
power
is
called
twice
by
main
in
the
line
printf
d
d
d
n
i
power
i
power
i
Each
call
passes
two
arguments
to
power
which
each
time
returns
an
integer
to
be
formatted
and
printed
In
an
expression
power
i
is
an
integer
just
as
and
i
are
Not
all
functions
produce
an
integer
value
we
will
take
this
up
in
Chapter
The
first
line
of
power
itself
int
power
int
base
int
n
declares
the
parameter
types
and
names
and
the
type
of
the
result
that
the
function
returns
The
names
used
by
power
for
its
parameters
are
local
to
power
and
are
not
visible
to
any
other
function
other
routines
can
use
the
same
names
without
conflict
This
is
also
true
of
the
variables
i
and
p
the
i
in
power
is
unrelated
to
the
i
in
main
We
will
generally
use
parameter
for
a
variable
named
in
the
parenthesized
list
in
a
function
The
terms
formal
argument
and
actual
argument
are
sometimes
used
for
the
same
distinction
The
value
that
power
computes
is
returned
to
main
by
the
return
statement
Any
expression
may
follow
return
return
expression
A
function
need
not
return
a
value
a
return
statement
with
no
expression
causes
control
but
no
useful
value
to
be
returned
to
the
caller
as
does
falling
off
the
end
of
a
function
by
reaching
the
terminating
right
brace
And
the
calling
function
can
ignore
a
value
returned
by
a
function
You
may
have
noticed
that
there
is
a
return
statement
at
the
end
of
main
Since
main
is
a
function
like
any
other
it
may
return
a
value
to
its
caller
which
is
in
effect
the
environment
in
which
the
program
was
executed
Typically
a
return
value
of
zero
implies
normal
termination
non
zero
values
signal
unusual
or
erroneous
termination
conditions
In
the
interests
of
simplicity
we
have
omitted
return
statements
from
our
main
functions
up
to
this
point
but
we
will
include
them
hereafter
as
a
reminder
that
programs
should
return
status
to
their
environment
The
declaration
int
power
int
base
int
n
just
before
main
says
that
power
is
a
function
that
expects
two
int
arguments
and
returns
an
int
This
declaration
which
is
called
a
function
prototype
has
to
agree
with
the
definition
and
uses
of
power
It
is
an
error
if
the
definition
of
a
function
or
any
uses
of
it
do
not
agree
with
its
prototype
parameter
names
need
not
agree
Indeed
parameter
names
are
optional
in
a
function
prototype
so
for
the
prototype
we
could
have
written
int
power
int
int
Well
chosen
names
are
good
documentation
however
so
we
will
often
use
them
A
note
of
history
the
biggest
change
between
ANSI
C
and
earlier
versions
is
how
functions
are
declared
and
defined
In
the
original
definition
of
C
the
power
function
would
have
been
written
like
this
power
raise
base
to
n
th
power
n
old
style
version
power
base
n
int
base
n
int
i
p
p
for
i
i
n
i
p
p
base
return
p
The
parameters
are
named
between
the
parentheses
and
their
types
are
declared
before
opening
the
left
brace
undeclared
parameters
are
taken
as
int
The
body
of
the
function
is
the
same
as
before
The
declaration
of
power
at
the
beginning
of
the
program
would
have
looked
like
this
int
power
No
parameter
list
was
permitted
so
the
compiler
could
not
readily
check
that
power
was
being
called
correctly
Indeed
since
by
default
power
would
have
been
assumed
to
return
an
int
the
entire
declaration
might
well
have
been
omitted
The
new
syntax
of
function
prototypes
makes
it
much
easier
for
a
compiler
to
detect
errors
in
the
number
of
arguments
or
their
types
The
old
style
of
declaration
and
definition
still
works
in
ANSI
C
at
least
for
a
transition
period
but
we
strongly
recommend
that
you
use
the
new
form
when
you
have
a
compiler
that
supports
it
Exercise
Rewrite
the
temperature
conversion
program
of
Section
to
use
a
function
for
conversion
Arguments
Call
by
Value
One
aspect
of
C
functions
may
be
unfamiliar
to
programmers
who
are
used
to
some
other
languages
particulary
Fortran
In
C
all
function
arguments
are
passed
by
value
This
means
that
the
called
function
is
given
the
values
of
its
arguments
in
temporary
variables
rather
than
the
originals
This
leads
to
some
different
properties
than
are
seen
with
call
by
reference
languages
like
Fortran
or
with
var
parameters
in
Pascal
in
which
the
called
routine
has
access
to
the
original
argument
not
a
local
copy
Call
by
value
is
an
asset
however
not
a
liability
It
usually
leads
to
more
compact
programs
with
fewer
extraneous
variables
because
parameters
can
be
treated
as
conveniently
initialized
local
variables
in
the
called
routine
For
example
here
is
a
version
of
power
that
makes
use
of
this
property
power
raise
base
to
n
th
power
n
version
int
power
int
base
int
n
int
p
for
p
n
n
p
p
base
return
p
The
parameter
n
is
used
as
a
temporary
variable
and
is
counted
down
a
for
loop
that
runs
backwards
until
it
becomes
zero
there
is
no
longer
a
need
for
the
variable
i
Whatever
is
done
to
n
inside
power
has
no
effect
on
the
argument
that
power
was
originally
called
with
When
necessary
it
is
possible
to
arrange
for
a
function
to
modify
a
variable
in
a
calling
routine
The
caller
must
provide
the
address
of
the
variable
to
be
set
technically
a
pointer
to
the
variable
and
the
called
function
must
declare
the
parameter
to
be
a
pointer
and
access
the
variable
indirectly
through
it
We
will
cover
pointers
in
Chapter
The
story
is
different
for
arrays
When
the
name
of
an
array
is
used
as
an
argument
the
value
passed
to
the
function
is
the
location
or
address
of
the
beginning
of
the
array
there
is
no
copying
of
array
elements
By
subscripting
this
value
the
function
can
access
and
alter
any
argument
of
the
array
This
is
the
topic
of
the
next
section
Character
Arrays
The
most
common
type
of
array
in
C
is
the
array
of
characters
To
illustrate
the
use
of
character
arrays
and
functions
to
manipulate
them
let
s
write
a
program
that
reads
a
set
of
text
lines
and
prints
the
longest
The
outline
is
simple
enough
while
there
s
another
line
if
it
s
longer
than
the
previous
longest
save
it
save
its
length
print
longest
line
This
outline
makes
it
clear
that
the
program
divides
naturally
into
pieces
One
piece
gets
a
new
line
another
saves
it
and
the
rest
controls
the
process
Since
things
divide
so
nicely
it
would
be
well
to
write
them
that
way
too
Accordingly
let
us
first
write
a
separate
function
getline
to
fetch
the
next
line
of
input
We
will
try
to
make
the
function
useful
in
other
contexts
At
the
minimum
getline
has
to
return
a
signal
about
possible
end
of
file
a
more
useful
design
would
be
to
return
the
length
of
the
line
or
zero
if
end
of
file
is
encountered
Zero
is
an
acceptable
end
of
file
return
because
it
is
never
a
valid
line
length
Every
text
line
has
at
least
one
character
even
a
line
containing
only
a
newline
has
length
When
we
find
a
line
that
is
longer
than
the
previous
longest
line
it
must
be
saved
somewhere
This
suggests
a
second
function
copy
to
copy
the
new
line
to
a
safe
place
Finally
we
need
a
main
program
to
control
getline
and
copy
Here
is
the
result
include
stdio
h
define
MAXLINE
maximum
input
line
length
int
getline
char
line
int
maxline
void
copy
char
to
char
from
print
the
longest
input
line
main
int
len
current
line
length
int
max
maximum
length
seen
so
far
char
line
MAXLINE
current
input
line
char
longest
MAXLINE
longest
line
saved
here
max
while
len
getline
line
MAXLINE
if
len
max
max
len
copy
longest
line
if
max
there
was
a
line
printf
s
longest
return
getline
read
a
line
into
s
return
length
int
getline
char
s
int
lim
int
c
i
for
i
i
lim
c
getchar
EOF
c
n
i
s
i
c
if
c
n
s
i
c
i
s
i
return
i
copy
copy
from
into
to
assume
to
is
big
enough
void
copy
char
to
char
from
int
i
i
while
to
i
from
i
i
The
functions
getline
and
copy
are
declared
at
the
beginning
of
the
program
which
we
assume
is
contained
in
one
file
main
and
getline
communicate
through
a
pair
of
arguments
and
a
returned
value
In
getline
the
arguments
are
declared
by
the
line
int
getline
char
s
int
lim
which
specifies
that
the
first
argument
s
is
an
array
and
the
second
lim
is
an
integer
The
purpose
of
supplying
the
size
of
an
array
in
a
declaration
is
to
set
aside
storage
The
length
of
an
array
s
is
not
necessary
in
getline
since
its
size
is
set
in
main
getline
uses
return
to
send
a
value
back
to
the
caller
just
as
the
function
power
did
This
line
also
declares
that
getline
returns
an
int
since
int
is
the
default
return
type
it
could
be
omitted
Some
functions
return
a
useful
value
others
like
copy
are
used
only
for
their
effect
and
return
no
value
The
return
type
of
copy
is
void
which
states
explicitly
that
no
value
is
returned
getline
puts
the
character
the
null
character
whose
value
is
zero
at
the
end
of
the
array
it
is
creating
to
mark
the
end
of
the
string
of
characters
This
conversion
is
also
used
by
the
C
language
when
a
string
constant
like
hello
n
appears
in
a
C
program
it
is
stored
as
an
array
of
characters
containing
the
characters
in
the
string
and
terminated
with
a
to
mark
the
end
The
s
format
specification
in
printf
expects
the
corresponding
argument
to
be
a
string
represented
in
this
form
copy
also
relies
on
the
fact
that
its
input
argument
is
terminated
with
a
and
copies
this
character
into
the
output
It
is
worth
mentioning
in
passing
that
even
a
program
as
small
as
this
one
presents
some
sticky
design
problems
For
example
what
should
main
do
if
it
encounters
a
line
which
is
bigger
than
its
limit
getline
works
safely
in
that
it
stops
collecting
when
the
array
is
full
even
if
no
newline
has
been
seen
By
testing
the
length
and
the
last
character
returned
main
can
determine
whether
the
line
was
too
long
and
then
cope
as
it
wishes
In
the
interests
of
brevity
we
have
ignored
this
issue
There
is
no
way
for
a
user
of
getline
to
know
in
advance
how
long
an
input
line
might
be
so
getline
checks
for
overflow
On
the
other
hand
the
user
of
copy
already
knows
or
can
find
out
how
big
the
strings
are
so
we
have
chosen
not
to
add
error
checking
to
it
Exercise
Revise
the
main
routine
of
the
longest
line
program
so
it
will
correctly
print
the
length
of
arbitrary
long
input
lines
and
as
much
as
possible
of
the
text
Exercise
Write
a
program
to
print
all
input
lines
that
are
longer
than
characters
Exercise
Write
a
program
to
remove
trailing
blanks
and
tabs
from
each
line
of
input
and
to
delete
entirely
blank
lines
Exercise
Write
a
function
reverse
s
that
reverses
the
character
string
s
Use
it
to
write
a
program
that
reverses
its
input
a
line
at
a
time
External
Variables
and
Scope
The
variables
in
main
such
as
line
longest
etc
are
private
or
local
to
main
Because
they
are
declared
within
main
no
other
function
can
have
direct
access
to
them
The
same
is
true
of
the
variables
in
other
functions
for
example
the
variable
i
in
getline
is
unrelated
to
the
i
in
copy
Each
local
variable
in
a
function
comes
into
existence
only
when
the
function
is
called
and
disappears
when
the
function
is
exited
This
is
why
such
variables
are
usually
known
as
automatic
variables
following
terminology
in
other
languages
We
will
use
the
term
automatic
henceforth
to
refer
to
these
local
variables
Chapter
discusses
the
static
storage
class
in
which
local
variables
do
retain
their
values
between
calls
Because
automatic
variables
come
and
go
with
function
invocation
they
do
not
retain
their
values
from
one
call
to
the
next
and
must
be
explicitly
set
upon
each
entry
If
they
are
not
set
they
will
contain
garbage
As
an
alternative
to
automatic
variables
it
is
possible
to
define
variables
that
are
external
to
all
functions
that
is
variables
that
can
be
accessed
by
name
by
any
function
This
mechanism
is
rather
like
Fortran
COMMON
or
Pascal
variables
declared
in
the
outermost
block
Because
external
variables
are
globally
accessible
they
can
be
used
instead
of
argument
lists
to
communicate
data
between
functions
Furthermore
because
external
variables
remain
in
existence
permanently
rather
than
appearing
and
disappearing
as
functions
are
called
and
exited
they
retain
their
values
even
after
the
functions
that
set
them
have
returned
An
external
variable
must
be
defined
exactly
once
outside
of
any
function
this
sets
aside
storage
for
it
The
variable
must
also
be
declared
in
each
function
that
wants
to
access
it
this
states
the
type
of
the
variable
The
declaration
may
be
an
explicit
extern
statement
or
may
be
implicit
from
context
To
make
the
discussion
concrete
let
us
rewrite
the
longest
line
program
with
line
longest
and
max
as
external
variables
This
requires
changing
the
calls
declarations
and
bodies
of
all
three
functions
include
stdio
h
define
MAXLINE
maximum
input
line
size
int
max
maximum
length
seen
so
far
char
line
MAXLINE
current
input
line
char
longest
MAXLINE
longest
line
saved
here
int
getline
void
void
copy
void
print
longest
input
line
specialized
version
main
int
len
extern
int
max
extern
char
longest
max
while
len
getline
if
len
max
max
len
copy
if
max
there
was
a
line
printf
s
longest
return
getline
specialized
version
int
getline
void
int
c
i
extern
char
line
for
i
i
MAXLINE
c
getchar
EOF
c
n
i
line
i
c
if
c
n
line
i
c
i
line
i
return
i
copy
specialized
version
void
copy
void
int
i
extern
char
line
longest
i
while
longest
i
line
i
i
The
external
variables
in
main
getline
and
copy
are
defined
by
the
first
lines
of
the
example
above
which
state
their
type
and
cause
storage
to
be
allocated
for
them
Syntactically
external
definitions
are
just
like
definitions
of
local
variables
but
since
they
occur
outside
of
functions
the
variables
are
external
Before
a
function
can
use
an
external
variable
the
name
of
the
variable
must
be
made
known
to
the
function
the
declaration
is
the
same
as
before
except
for
the
added
keyword
extern
In
certain
circumstances
the
extern
declaration
can
be
omitted
If
the
definition
of
the
external
variable
occurs
in
the
source
file
before
its
use
in
a
particular
function
then
there
is
no
need
for
an
extern
declaration
in
the
function
The
extern
declarations
in
main
getline
and
copy
are
thus
redundant
In
fact
common
practice
is
to
place
definitions
of
all
external
variables
at
the
beginning
of
the
source
file
and
then
omit
all
extern
declarations
If
the
program
is
in
several
source
files
and
a
variable
is
defined
in
file
and
used
in
file
and
file
then
extern
declarations
are
needed
in
file
and
file
to
connect
the
occurrences
of
the
variable
The
usual
practice
is
to
collect
extern
declarations
of
variables
and
functions
in
a
separate
file
historically
called
a
header
that
is
included
by
include
at
the
front
of
each
source
file
The
suffix
h
is
conventional
for
header
names
The
functions
of
the
standard
library
for
example
are
declared
in
headers
like
stdio
h
This
topic
is
discussed
at
length
in
Chapter
and
the
library
itself
in
Chapter
and
Appendix
B
Since
the
specialized
versions
of
getline
and
copy
have
no
arguments
logic
would
suggest
that
their
prototypes
at
the
beginning
of
the
file
should
be
getline
and
copy
But
for
compatibility
with
older
C
programs
the
standard
takes
an
empty
list
as
an
old
style
declaration
and
turns
off
all
argument
list
checking
the
word
void
must
be
used
for
an
explicitly
empty
list
We
will
discuss
this
further
in
Chapter
You
should
note
that
we
are
using
the
words
definition
and
declaration
carefully
when
we
refer
to
external
variables
in
this
section
Definition
refers
to
the
place
where
the
variable
is
created
or
assigned
storage
declaration
refers
to
places
where
the
nature
of
the
variable
is
stated
but
no
storage
is
allocated
By
the
way
there
is
a
tendency
to
make
everything
in
sight
an
extern
variable
because
it
appears
to
simplify
communications
argument
lists
are
short
and
variables
are
always
there
when
you
want
them
But
external
variables
are
always
there
even
when
you
don
t
want
them
Relying
too
heavily
on
external
variables
is
fraught
with
peril
since
it
leads
to
programs
whose
data
connections
are
not
all
obvious
variables
can
be
changed
in
unexpected
and
even
inadvertent
ways
and
the
program
is
hard
to
modify
The
second
version
of
the
longest
line
program
is
inferior
to
the
first
partly
for
these
reasons
and
partly
because
it
destroys
the
generality
of
two
useful
functions
by
writing
into
them
the
names
of
the
variables
they
manipulate
At
this
point
we
have
covered
what
might
be
called
the
conventional
core
of
C
With
this
handful
of
building
blocks
it
s
possible
to
write
useful
programs
of
considerable
size
and
it
would
probably
be
a
good
idea
if
you
paused
long
enough
to
do
so
These
exercises
suggest
programs
of
somewhat
greater
complexity
than
the
ones
earlier
in
this
chapter
Exercise
Write
a
program
detab
that
replaces
tabs
in
the
input
with
the
proper
number
of
blanks
to
space
to
the
next
tab
stop
Assume
a
fixed
set
of
tab
stops
say
every
n
columns
Should
n
be
a
variable
or
a
symbolic
parameter
Exercise
Write
a
program
entab
that
replaces
strings
of
blanks
by
the
minimum
number
of
tabs
and
blanks
to
achieve
the
same
spacing
Use
the
same
tab
stops
as
for
detab
When
either
a
tab
or
a
single
blank
would
suffice
to
reach
a
tab
stop
which
should
be
given
preference
Exercise
Write
a
program
to
fold
long
input
lines
into
two
or
more
shorter
lines
after
the
last
non
blank
character
that
occurs
before
the
n
th
column
of
input
Make
sure
your
program
does
something
intelligent
with
very
long
lines
and
if
there
are
no
blanks
or
tabs
before
the
specified
column
Exercise
Write
a
program
to
remove
all
comments
from
a
C
program
Don
t
forget
to
handle
quoted
strings
and
character
constants
properly
C
comments
don
t
nest
Exercise
Write
a
program
to
check
a
C
program
for
rudimentary
syntax
errors
like
unmatched
parentheses
brackets
and
braces
Don
t
forget
about
quotes
both
single
and
double
escape
sequences
and
comments
This
program
is
hard
if
you
do
it
in
full
generality
Chapter
Types
Operators
and
Expressions
Variables
and
constants
are
the
basic
data
objects
manipulated
in
a
program
Declarations
list
the
variables
to
be
used
and
state
what
type
they
have
and
perhaps
what
their
initial
values
are
Operators
specify
what
is
to
be
done
to
them
Expressions
combine
variables
and
constants
to
produce
new
values
The
type
of
an
object
determines
the
set
of
values
it
can
have
and
what
operations
can
be
performed
on
it
These
building
blocks
are
the
topics
of
this
chapter
The
ANSI
standard
has
made
many
small
changes
and
additions
to
basic
types
and
expressions
There
are
now
signed
and
unsigned
forms
of
all
integer
types
and
notations
for
unsigned
constants
and
hexadecimal
character
constants
Floating
point
operations
may
be
done
in
single
precision
there
is
also
a
long
double
type
for
extended
precision
String
constants
may
be
concatenated
at
compile
time
Enumerations
have
become
part
of
the
language
formalizing
a
feature
of
long
standing
Objects
may
be
declared
const
which
prevents
them
from
being
changed
The
rules
for
automatic
coercions
among
arithmetic
types
have
been
augmented
to
handle
the
richer
set
of
types
Variable
Names
Although
we
didn
t
say
so
in
Chapter
there
are
some
restrictions
on
the
names
of
variables
and
symbolic
constants
Names
are
made
up
of
letters
and
digits
the
first
character
must
be
a
letter
The
underscore
counts
as
a
letter
it
is
sometimes
useful
for
improving
the
readability
of
long
variable
names
Don
t
begin
variable
names
with
underscore
however
since
library
routines
often
use
such
names
Upper
and
lower
case
letters
are
distinct
so
x
and
X
are
two
different
names
Traditional
C
practice
is
to
use
lower
case
for
variable
names
and
all
upper
case
for
symbolic
constants
At
least
the
first
characters
of
an
internal
name
are
significant
For
function
names
and
external
variables
the
number
may
be
less
than
because
external
names
may
be
used
by
assemblers
and
loaders
over
which
the
language
has
no
control
For
external
names
the
standard
guarantees
uniqueness
only
for
characters
and
a
single
case
Keywords
like
if
else
int
float
etc
are
reserved
you
can
t
use
them
as
variable
names
They
must
be
in
lower
case
It
s
wise
to
choose
variable
names
that
are
related
to
the
purpose
of
the
variable
and
that
are
unlikely
to
get
mixed
up
typographically
We
tend
to
use
short
names
for
local
variables
especially
loop
indices
and
longer
names
for
external
variables
Data
Types
and
Sizes
There
are
only
a
few
basic
data
types
in
C
char
a
single
byte
capable
of
holding
one
character
in
the
local
character
set
int
an
integer
typically
reflecting
the
natural
size
of
integers
on
the
host
machine
float
single
precision
floating
point
double
double
precision
floating
point
In
addition
there
are
a
number
of
qualifiers
that
can
be
applied
to
these
basic
types
short
and
long
apply
to
integers
short
int
sh
long
int
counter
The
word
int
can
be
omitted
in
such
declarations
and
typically
it
is
The
intent
is
that
short
and
long
should
provide
different
lengths
of
integers
where
practical
int
will
normally
be
the
natural
size
for
a
particular
machine
short
is
often
bits
long
and
int
either
or
bits
Each
compiler
is
free
to
choose
appropriate
sizes
for
its
own
hardware
subject
only
to
the
the
restriction
that
shorts
and
ints
are
at
least
bits
longs
are
at
least
bits
and
short
is
no
longer
than
int
which
is
no
longer
than
long
The
qualifier
signed
or
unsigned
may
be
applied
to
char
or
any
integer
unsigned
numbers
are
always
positive
or
zero
and
obey
the
laws
of
arithmetic
modulo
n
where
n
is
the
number
of
bits
in
the
type
So
for
instance
if
chars
are
bits
unsigned
char
variables
have
values
between
and
while
signed
chars
have
values
between
and
in
a
two
s
complement
machine
Whether
plain
chars
are
signed
or
unsigned
is
machine
dependent
but
printable
characters
are
always
positive
The
type
long
double
specifies
extended
precision
floating
point
As
with
integers
the
sizes
of
floating
point
objects
are
implementation
defined
float
double
and
long
double
could
represent
one
two
or
three
distinct
sizes
The
standard
headers
limits
h
and
float
h
contain
symbolic
constants
for
all
of
these
sizes
along
with
other
properties
of
the
machine
and
compiler
These
are
discussed
in
Appendix
B
Exercise
Write
a
program
to
determine
the
ranges
of
char
short
int
and
long
variables
both
signed
and
unsigned
by
printing
appropriate
values
from
standard
headers
and
by
direct
computation
Harder
if
you
compute
them
determine
the
ranges
of
the
various
floating
point
types
Constants
An
integer
constant
like
is
an
int
A
long
constant
is
written
with
a
terminal
l
ell
or
L
as
in
L
an
integer
constant
too
big
to
fit
into
an
int
will
also
be
taken
as
a
long
Unsigned
constants
are
written
with
a
terminal
u
or
U
and
the
suffix
ul
or
UL
indicates
unsigned
long
Floating
point
constants
contain
a
decimal
point
or
an
exponent
e
or
both
their
type
is
double
unless
suffixed
The
suffixes
f
or
F
indicate
a
float
constant
l
or
L
indicate
a
long
double
The
value
of
an
integer
can
be
specified
in
octal
or
hexadecimal
instead
of
decimal
A
leading
zero
on
an
integer
constant
means
octal
a
leading
x
or
X
means
hexadecimal
For
example
decimal
can
be
written
as
in
octal
and
x
f
or
x
F
in
hex
Octal
and
hexadecimal
constants
may
also
be
followed
by
L
to
make
them
long
and
U
to
make
them
unsigned
XFUL
is
an
unsigned
long
constant
with
value
decimal
A
character
constant
is
an
integer
written
as
one
character
within
single
quotes
such
as
x
The
value
of
a
character
constant
is
the
numeric
value
of
the
character
in
the
machine
s
character
set
For
example
in
the
ASCII
character
set
the
character
constant
has
the
value
which
is
unrelated
to
the
numeric
value
If
we
write
instead
of
a
numeric
value
like
that
depends
on
the
character
set
the
program
is
independent
of
the
particular
value
and
easier
to
read
Character
constants
participate
in
numeric
operations
just
as
any
other
integers
although
they
are
most
often
used
in
comparisons
with
other
characters
Certain
characters
can
be
represented
in
character
and
string
constants
by
escape
sequences
like
n
newline
these
sequences
look
like
two
characters
but
represent
only
one
In
addition
an
arbitrary
byte
sized
bit
pattern
can
be
specified
by
ooo
where
ooo
is
one
to
three
octal
digits
or
by
xhh
where
hh
is
one
or
more
hexadecimal
digits
a
f
A
F
So
we
might
write
define
VTAB
ASCII
vertical
tab
define
BELL
ASCII
bell
character
or
in
hexadecimal
define
VTAB
xb
ASCII
vertical
tab
define
BELL
x
ASCII
bell
character
The
complete
set
of
escape
sequences
is
a
alert
bell
character
backslash
b
backspace
question
mark
f
formfeed
single
quote
n
newline
double
quote
r
carriage
return
ooo
octal
number
t
horizontal
tab
xhh
hexadecimal
number
v
vertical
tab
The
character
constant
represents
the
character
with
value
zero
the
null
character
is
often
written
instead
of
to
emphasize
the
character
nature
of
some
expression
but
the
numeric
value
is
just
A
constant
expression
is
an
expression
that
involves
only
constants
Such
expressions
may
be
evaluated
at
during
compilation
rather
than
run
time
and
accordingly
may
be
used
in
any
place
that
a
constant
can
occur
as
in
define
MAXLINE
char
line
MAXLINE
or
define
LEAP
in
leap
years
int
days
LEAP
A
string
constant
or
string
literal
is
a
sequence
of
zero
or
more
characters
surrounded
by
double
quotes
as
in
I
am
a
string
or
the
empty
string
The
quotes
are
not
part
of
the
string
but
serve
only
to
delimit
it
The
same
escape
sequences
used
in
character
constants
apply
in
strings
represents
the
double
quote
character
String
constants
can
be
concatenated
at
compile
time
hello
world
is
equivalent
to
hello
world
This
is
useful
for
splitting
up
long
strings
across
several
source
lines
Technically
a
string
constant
is
an
array
of
characters
The
internal
representation
of
a
string
has
a
null
character
at
the
end
so
the
physical
storage
required
is
one
more
than
the
number
of
characters
written
between
the
quotes
This
representation
means
that
there
is
no
limit
to
how
long
a
string
can
be
but
programs
must
scan
a
string
completely
to
determine
its
length
The
standard
library
function
strlen
s
returns
the
length
of
its
character
string
argument
s
excluding
the
terminal
Here
is
our
version
strlen
return
length
of
s
int
strlen
char
s
int
i
while
s
i
i
return
i
strlen
and
other
string
functions
are
declared
in
the
standard
header
string
h
Be
careful
to
distinguish
between
a
character
constant
and
a
string
that
contains
a
single
character
x
is
not
the
same
as
x
The
former
is
an
integer
used
to
produce
the
numeric
value
of
the
letter
x
in
the
machine
s
character
set
The
latter
is
an
array
of
characters
that
contains
one
character
the
letter
x
and
a
There
is
one
other
kind
of
constant
the
enumeration
constant
An
enumeration
is
a
list
of
constant
integer
values
as
in
enum
boolean
NO
YES
The
first
name
in
an
enum
has
value
the
next
and
so
on
unless
explicit
values
are
specified
If
not
all
values
are
specified
unspecified
values
continue
the
progression
from
the
last
specified
value
as
the
second
of
these
examples
enum
escapes
BELL
a
BACKSPACE
b
TAB
t
NEWLINE
n
VTAB
v
RETURN
r
enum
months
JAN
FEB
MAR
APR
MAY
JUN
JUL
AUG
SEP
OCT
NOV
DEC
FEB
MAR
etc
Names
in
different
enumerations
must
be
distinct
Values
need
not
be
distinct
in
the
same
enumeration
Enumerations
provide
a
convenient
way
to
associate
constant
values
with
names
an
alternative
to
define
with
the
advantage
that
the
values
can
be
generated
for
you
Although
variables
of
enum
types
may
be
declared
compilers
need
not
check
that
what
you
store
in
such
a
variable
is
a
valid
value
for
the
enumeration
Nevertheless
enumeration
variables
offer
the
chance
of
checking
and
so
are
often
better
than
defines
In
addition
a
debugger
may
be
able
to
print
values
of
enumeration
variables
in
their
symbolic
form
Declarations
All
variables
must
be
declared
before
use
although
certain
declarations
can
be
made
implicitly
by
content
A
declaration
specifies
a
type
and
contains
a
list
of
one
or
more
variables
of
that
type
as
in
int
lower
upper
step
char
c
line
Variables
can
be
distributed
among
declarations
in
any
fashion
the
lists
above
could
well
be
written
as
int
lower
int
upper
int
step
char
c
char
line
The
latter
form
takes
more
space
but
is
convenient
for
adding
a
comment
to
each
declaration
for
subsequent
modifications
A
variable
may
also
be
initialized
in
its
declaration
If
the
name
is
followed
by
an
equals
sign
and
an
expression
the
expression
serves
as
an
initializer
as
in
char
esc
int
i
int
limit
MAXLINE
float
eps
e
If
the
variable
in
question
is
not
automatic
the
initialization
is
done
once
only
conceptionally
before
the
program
starts
executing
and
the
initializer
must
be
a
constant
expression
An
explicitly
initialized
automatic
variable
is
initialized
each
time
the
function
or
block
it
is
in
is
entered
the
initializer
may
be
any
expression
External
and
static
variables
are
initialized
to
zero
by
default
Automatic
variables
for
which
is
no
explicit
initializer
have
undefined
i
e
garbage
values
The
qualifier
const
can
be
applied
to
the
declaration
of
any
variable
to
specify
that
its
value
will
not
be
changed
For
an
array
the
const
qualifier
says
that
the
elements
will
not
be
altered
const
double
e
const
char
msg
warning
The
const
declaration
can
also
be
used
with
array
arguments
to
indicate
that
the
function
does
not
change
that
array
int
strlen
const
char
The
result
is
implementation
defined
if
an
attempt
is
made
to
change
a
const
Arithmetic
Operators
The
binary
arithmetic
operators
are
and
the
modulus
operator
Integer
division
truncates
any
fractional
part
The
expression
x
y
produces
the
remainder
when
x
is
divided
by
y
and
thus
is
zero
when
y
divides
x
exactly
For
example
a
year
is
a
leap
year
if
it
is
divisible
by
but
not
by
except
that
years
divisible
by
are
leap
years
Therefore
if
year
year
year
printf
d
is
a
leap
year
n
year
else
printf
d
is
not
a
leap
year
n
year
The
operator
cannot
be
applied
to
a
float
or
double
The
direction
of
truncation
for
and
the
sign
of
the
result
for
are
machine
dependent
for
negative
operands
as
is
the
action
taken
on
overflow
or
underflow
The
binary
and
operators
have
the
same
precedence
which
is
lower
than
the
precedence
of
and
which
is
in
turn
lower
than
unary
and
Arithmetic
operators
associate
left
to
right
Table
at
the
end
of
this
chapter
summarizes
precedence
and
associativity
for
all
operators
Relational
and
Logical
Operators
The
relational
operators
are
They
all
have
the
same
precedence
Just
below
them
in
precedence
are
the
equality
operators
Relational
operators
have
lower
precedence
than
arithmetic
operators
so
an
expression
like
i
lim
is
taken
as
i
lim
as
would
be
expected
More
interesting
are
the
logical
operators
and
Expressions
connected
by
or
are
evaluated
left
to
right
and
evaluation
stops
as
soon
as
the
truth
or
falsehood
of
the
result
is
known
Most
C
programs
rely
on
these
properties
For
example
here
is
a
loop
from
the
input
function
getline
that
we
wrote
in
Chapter
for
i
i
lim
c
getchar
n
c
EOF
i
s
i
c
Before
reading
a
new
character
it
is
necessary
to
check
that
there
is
room
to
store
it
in
the
array
s
so
the
test
i
lim
must
be
made
first
Moreover
if
this
test
fails
we
must
not
go
on
and
read
another
character
Similarly
it
would
be
unfortunate
if
c
were
tested
against
EOF
before
getchar
is
called
therefore
the
call
and
assignment
must
occur
before
the
character
in
c
is
tested
The
precedence
of
is
higher
than
that
of
and
both
are
lower
than
relational
and
equality
operators
so
expressions
like
i
lim
c
getchar
n
c
EOF
need
no
extra
parentheses
But
since
the
precedence
of
is
higher
than
assignment
parentheses
are
needed
in
c
getchar
n
to
achieve
the
desired
result
of
assignment
to
c
and
then
comparison
with
n
By
definition
the
numeric
value
of
a
relational
or
logical
expression
is
if
the
relation
is
true
and
if
the
relation
is
false
The
unary
negation
operator
converts
a
non
zero
operand
into
and
a
zero
operand
in
A
common
use
of
is
in
constructions
like
if
valid
rather
than
if
valid
It
s
hard
to
generalize
about
which
form
is
better
Constructions
like
valid
read
nicely
if
not
valid
but
more
complicated
ones
can
be
hard
to
understand
Exercise
Write
a
loop
equivalent
to
the
for
loop
above
without
using
or
Type
Conversions
When
an
operator
has
operands
of
different
types
they
are
converted
to
a
common
type
according
to
a
small
number
of
rules
In
general
the
only
automatic
conversions
are
those
that
convert
a
narrower
operand
into
a
wider
one
without
losing
information
such
as
converting
an
integer
into
floating
point
in
an
expression
like
f
i
Expressions
that
don
t
make
sense
like
using
a
float
as
a
subscript
are
disallowed
Expressions
that
might
lose
information
like
assigning
a
longer
integer
type
to
a
shorter
or
a
floating
point
type
to
an
integer
may
draw
a
warning
but
they
are
not
illegal
A
char
is
just
a
small
integer
so
chars
may
be
freely
used
in
arithmetic
expressions
This
permits
considerable
flexibility
in
certain
kinds
of
character
transformations
One
is
exemplified
by
this
naive
implementation
of
the
function
atoi
which
converts
a
string
of
digits
into
its
numeric
equivalent
atoi
convert
s
to
integer
int
atoi
char
s
int
i
n
n
for
i
s
i
s
i
i
n
n
s
i
return
n
As
we
discussed
in
Chapter
the
expression
s
i
gives
the
numeric
value
of
the
character
stored
in
s
i
because
the
values
of
etc
form
a
contiguous
increasing
sequence
Another
example
of
char
to
int
conversion
is
the
function
lower
which
maps
a
single
character
to
lower
case
for
the
ASCII
character
set
If
the
character
is
not
an
upper
case
letter
lower
returns
it
unchanged
lower
convert
c
to
lower
case
ASCII
only
int
lower
int
c
if
c
A
c
Z
return
c
a
A
else
return
c
This
works
for
ASCII
because
corresponding
upper
case
and
lower
case
letters
are
a
fixed
distance
apart
as
numeric
values
and
each
alphabet
is
contiguous
there
is
nothing
but
letters
between
A
and
Z
This
latter
observation
is
not
true
of
the
EBCDIC
character
set
however
so
this
code
would
convert
more
than
just
letters
in
EBCDIC
The
standard
header
ctype
h
described
in
Appendix
B
defines
a
family
of
functions
that
provide
tests
and
conversions
that
are
independent
of
character
set
For
example
the
function
tolower
is
a
portable
replacement
for
the
function
lower
shown
above
Similarly
the
test
c
c
can
be
replaced
by
isdigit
c
We
will
use
the
ctype
h
functions
from
now
on
There
is
one
subtle
point
about
the
conversion
of
characters
to
integers
The
language
does
not
specify
whether
variables
of
type
char
are
signed
or
unsigned
quantities
When
a
char
is
converted
to
an
int
can
it
ever
produce
a
negative
integer
The
answer
varies
from
machine
to
machine
reflecting
differences
in
architecture
On
some
machines
a
char
whose
leftmost
bit
is
will
be
converted
to
a
negative
integer
sign
extension
On
others
a
char
is
promoted
to
an
int
by
adding
zeros
at
the
left
end
and
thus
is
always
positive
The
definition
of
C
guarantees
that
any
character
in
the
machine
s
standard
printing
character
set
will
never
be
negative
so
these
characters
will
always
be
positive
quantities
in
expressions
But
arbitrary
bit
patterns
stored
in
character
variables
may
appear
to
be
negative
on
some
machines
yet
positive
on
others
For
portability
specify
signed
or
unsigned
if
non
character
data
is
to
be
stored
in
char
variables
Relational
expressions
like
i
j
and
logical
expressions
connected
by
and
are
defined
to
have
value
if
true
and
if
false
Thus
the
assignment
d
c
c
sets
d
to
if
c
is
a
digit
and
if
not
However
functions
like
isdigit
may
return
any
nonzero
value
for
true
In
the
test
part
of
if
while
for
etc
true
just
means
non
zero
so
this
makes
no
difference
Implicit
arithmetic
conversions
work
much
as
expected
In
general
if
an
operator
like
or
that
takes
two
operands
a
binary
operator
has
operands
of
different
types
the
lower
type
is
promoted
to
the
higher
type
before
the
operation
proceeds
The
result
is
of
the
integer
type
Section
of
Appendix
A
states
the
conversion
rules
precisely
If
there
are
no
unsigned
operands
however
the
following
informal
set
of
rules
will
suffice
If
either
operand
is
long
double
convert
the
other
to
long
double
Otherwise
if
either
operand
is
double
convert
the
other
to
double
Otherwise
if
either
operand
is
float
convert
the
other
to
float
Otherwise
convert
char
and
short
to
int
Then
if
either
operand
is
long
convert
the
other
to
long
Notice
that
floats
in
an
expression
are
not
automatically
converted
to
double
this
is
a
change
from
the
original
definition
In
general
mathematical
functions
like
those
in
math
h
will
use
double
precision
The
main
reason
for
using
float
is
to
save
storage
in
large
arrays
or
less
often
to
save
time
on
machines
where
double
precision
arithmetic
is
particularly
expensive
Conversion
rules
are
more
complicated
when
unsigned
operands
are
involved
The
problem
is
that
comparisons
between
signed
and
unsigned
values
are
machine
dependent
because
they
depend
on
the
sizes
of
the
various
integer
types
For
example
suppose
that
int
is
bits
and
long
is
bits
Then
L
U
because
U
which
is
an
unsigned
int
is
promoted
to
a
signed
long
But
L
UL
because
L
is
promoted
to
unsigned
long
and
thus
appears
to
be
a
large
positive
number
Conversions
take
place
across
assignments
the
value
of
the
right
side
is
converted
to
the
type
of
the
left
which
is
the
type
of
the
result
A
character
is
converted
to
an
integer
either
by
sign
extension
or
not
as
described
above
Longer
integers
are
converted
to
shorter
ones
or
to
chars
by
dropping
the
excess
high
order
bits
Thus
in
int
i
char
c
i
c
c
i
the
value
of
c
is
unchanged
This
is
true
whether
or
not
sign
extension
is
involved
Reversing
the
order
of
assignments
might
lose
information
however
If
x
is
float
and
i
is
int
then
x
i
and
i
x
both
cause
conversions
float
to
int
causes
truncation
of
any
fractional
part
When
a
double
is
converted
to
float
whether
the
value
is
rounded
or
truncated
is
implementation
dependent
Since
an
argument
of
a
function
call
is
an
expression
type
conversion
also
takes
place
when
arguments
are
passed
to
functions
In
the
absence
of
a
function
prototype
char
and
short
become
int
and
float
becomes
double
This
is
why
we
have
declared
function
arguments
to
be
int
and
double
even
when
the
function
is
called
with
char
and
float
Finally
explicit
type
conversions
can
be
forced
coerced
in
any
expression
with
a
unary
operator
called
a
cast
In
the
construction
type
name
expression
the
expression
is
converted
to
the
named
type
by
the
conversion
rules
above
The
precise
meaning
of
a
cast
is
as
if
the
expression
were
assigned
to
a
variable
of
the
specified
type
which
is
then
used
in
place
of
the
whole
construction
For
example
the
library
routine
sqrt
expects
a
double
argument
and
will
produce
nonsense
if
inadvertently
handled
something
else
sqrt
is
declared
in
math
h
So
if
n
is
an
integer
we
can
use
sqrt
double
n
to
convert
the
value
of
n
to
double
before
passing
it
to
sqrt
Note
that
the
cast
produces
the
value
of
n
in
the
proper
type
n
itself
is
not
altered
The
cast
operator
has
the
same
high
precedence
as
other
unary
operators
as
summarized
in
the
table
at
the
end
of
this
chapter
If
arguments
are
declared
by
a
function
prototype
as
the
normally
should
be
the
declaration
causes
automatic
coercion
of
any
arguments
when
the
function
is
called
Thus
given
a
function
prototype
for
sqrt
double
sqrt
double
the
call
root
sqrt
coerces
the
integer
into
the
double
value
without
any
need
for
a
cast
The
standard
library
includes
a
portable
implementation
of
a
pseudo
random
number
generator
and
a
function
for
initializing
the
seed
the
former
illustrates
a
cast
unsigned
long
int
next
rand
return
pseudo
random
integer
on
int
rand
void
next
next
return
unsigned
int
next
srand
set
seed
for
rand
void
srand
unsigned
int
seed
next
seed
Exercise
Write
a
function
htoi
s
which
converts
a
string
of
hexadecimal
digits
including
an
optional
x
or
X
into
its
equivalent
integer
value
The
allowable
digits
are
through
a
through
f
and
A
through
F
Increment
and
Decrement
Operators
C
provides
two
unusual
operators
for
incrementing
and
decrementing
variables
The
increment
operator
adds
to
its
operand
while
the
decrement
operator
subtracts
We
have
frequently
used
to
increment
variables
as
in
if
c
n
nl
The
unusual
aspect
is
that
and
may
be
used
either
as
prefix
operators
before
the
variable
as
in
n
or
postfix
operators
after
the
variable
n
In
both
cases
the
effect
is
to
increment
n
But
the
expression
n
increments
n
before
its
value
is
used
while
n
increments
n
after
its
value
has
been
used
This
means
that
in
a
context
where
the
value
is
being
used
not
just
the
effect
n
and
n
are
different
If
n
is
then
x
n
sets
x
to
but
x
n
sets
x
to
In
both
cases
n
becomes
The
increment
and
decrement
operators
can
only
be
applied
to
variables
an
expression
like
i
j
is
illegal
In
a
context
where
no
value
is
wanted
just
the
incrementing
effect
as
in
if
c
n
nl
prefix
and
postfix
are
the
same
But
there
are
situations
where
one
or
the
other
is
specifically
called
for
For
instance
consider
the
function
squeeze
s
c
which
removes
all
occurrences
of
the
character
c
from
the
string
s
squeeze
delete
all
c
from
s
void
squeeze
char
s
int
c
int
i
j
for
i
j
s
i
i
if
s
i
c
s
j
s
i
s
j
Each
time
a
non
c
occurs
it
is
copied
into
the
current
j
position
and
only
then
is
j
incremented
to
be
ready
for
the
next
character
This
is
exactly
equivalent
to
if
s
i
c
s
j
s
i
j
Another
example
of
a
similar
construction
comes
from
the
getline
function
that
we
wrote
in
Chapter
where
we
can
replace
if
c
n
s
i
c
i
by
the
more
compact
if
c
n
s
i
c
As
a
third
example
consider
the
standard
function
strcat
s
t
which
concatenates
the
string
t
to
the
end
of
string
s
strcat
assumes
that
there
is
enough
space
in
s
to
hold
the
combination
As
we
have
written
it
strcat
returns
no
value
the
standard
library
version
returns
a
pointer
to
the
resulting
string
strcat
concatenate
t
to
end
of
s
s
must
be
big
enough
void
strcat
char
s
char
t
int
i
j
i
j
while
s
i
find
end
of
s
i
while
s
i
t
j
copy
t
As
each
member
is
copied
from
t
to
s
the
postfix
is
applied
to
both
i
and
j
to
make
sure
that
they
are
in
position
for
the
next
pass
through
the
loop
Exercise
Write
an
alternative
version
of
squeeze
s
s
that
deletes
each
character
in
s
that
matches
any
character
in
the
string
s
Exercise
Write
the
function
any
s
s
which
returns
the
first
location
in
a
string
s
where
any
character
from
the
string
s
occurs
or
if
s
contains
no
characters
from
s
The
standard
library
function
strpbrk
does
the
same
job
but
returns
a
pointer
to
the
location
Bitwise
Operators
C
provides
six
operators
for
bit
manipulation
these
may
only
be
applied
to
integral
operands
that
is
char
short
int
and
long
whether
signed
or
unsigned
bitwise
AND
bitwise
inclusive
OR
bitwise
exclusive
OR
left
shift
right
shift
one
s
complement
unary
The
bitwise
AND
operator
is
often
used
to
mask
off
some
set
of
bits
for
example
n
n
sets
to
zero
all
but
the
low
order
bits
of
n
The
bitwise
OR
operator
is
used
to
turn
bits
on
x
x
SET
ON
sets
to
one
in
x
the
bits
that
are
set
to
one
in
SET
ON
The
bitwise
exclusive
OR
operator
sets
a
one
in
each
bit
position
where
its
operands
have
different
bits
and
zero
where
they
are
the
same
One
must
distinguish
the
bitwise
operators
and
from
the
logical
operators
and
which
imply
left
to
right
evaluation
of
a
truth
value
For
example
if
x
is
and
y
is
then
x
y
is
zero
while
x
y
is
one
The
shift
operators
and
perform
left
and
right
shifts
of
their
left
operand
by
the
number
of
bit
positions
given
by
the
right
operand
which
must
be
non
negative
Thus
x
shifts
the
value
of
x
by
two
positions
filling
vacated
bits
with
zero
this
is
equivalent
to
multiplication
by
Right
shifting
an
unsigned
quantity
always
fits
the
vacated
bits
with
zero
Right
shifting
a
signed
quantity
will
fill
with
bit
signs
arithmetic
shift
on
some
machines
and
with
bits
logical
shift
on
others
The
unary
operator
yields
the
one
s
complement
of
an
integer
that
is
it
converts
each
bit
into
a
bit
and
vice
versa
For
example
x
x
sets
the
last
six
bits
of
x
to
zero
Note
that
x
is
independent
of
word
length
and
is
thus
preferable
to
for
example
x
which
assumes
that
x
is
a
bit
quantity
The
portable
form
involves
no
extra
cost
since
is
a
constant
expression
that
can
be
evaluated
at
compile
time
As
an
illustration
of
some
of
the
bit
operators
consider
the
function
getbits
x
p
n
that
returns
the
right
adjusted
n
bit
field
of
x
that
begins
at
position
p
We
assume
that
bit
position
is
at
the
right
end
and
that
n
and
p
are
sensible
positive
values
For
example
getbits
x
returns
the
three
bits
in
positions
and
right
adjusted
getbits
get
n
bits
from
position
p
unsigned
getbits
unsigned
x
int
p
int
n
return
x
p
n
n
The
expression
x
p
n
moves
the
desired
field
to
the
right
end
of
the
word
is
all
bits
shifting
it
left
n
positions
with
n
places
zeros
in
the
rightmost
n
bits
complementing
that
with
makes
a
mask
with
ones
in
the
rightmost
n
bits
Exercise
Write
a
function
setbits
x
p
n
y
that
returns
x
with
the
n
bits
that
begin
at
position
p
set
to
the
rightmost
n
bits
of
y
leaving
the
other
bits
unchanged
Exercise
Write
a
function
invert
x
p
n
that
returns
x
with
the
n
bits
that
begin
at
position
p
inverted
i
e
changed
into
and
vice
versa
leaving
the
others
unchanged
Exercise
Write
a
function
rightrot
x
n
that
returns
the
value
of
the
integer
x
rotated
to
the
right
by
n
positions
Assignment
Operators
and
Expressions
An
expression
such
as
i
i
in
which
the
variable
on
the
left
side
is
repeated
immediately
on
the
right
can
be
written
in
the
compressed
form
i
The
operator
is
called
an
assignment
operator
Most
binary
operators
operators
like
that
have
a
left
and
right
operand
have
a
corresponding
assignment
operator
op
where
op
is
one
of
If
expr
and
expr
are
expressions
then
expr
op
expr
is
equivalent
to
expr
expr
op
expr
except
that
expr
is
computed
only
once
Notice
the
parentheses
around
expr
x
y
means
x
x
y
rather
than
x
x
y
As
an
example
the
function
bitcount
counts
the
number
of
bits
in
its
integer
argument
bitcount
count
bits
in
x
int
bitcount
unsigned
x
int
b
for
b
x
x
if
x
b
return
b
Declaring
the
argument
x
to
be
an
unsigned
ensures
that
when
it
is
right
shifted
vacated
bits
will
be
filled
with
zeros
not
sign
bits
regardless
of
the
machine
the
program
is
run
on
Quite
apart
from
conciseness
assignment
operators
have
the
advantage
that
they
correspond
better
to
the
way
people
think
We
say
add
to
i
or
increment
i
by
not
take
i
add
then
put
the
result
back
in
i
Thus
the
expression
i
is
preferable
to
i
i
In
addition
for
a
complicated
expression
like
yyval
yypv
p
p
yypv
p
the
assignment
operator
makes
the
code
easier
to
understand
since
the
reader
doesn
t
have
to
check
painstakingly
that
two
long
expressions
are
indeed
the
same
or
to
wonder
why
they
re
not
And
an
assignment
operator
may
even
help
a
compiler
to
produce
efficient
code
We
have
already
seen
that
the
assignment
statement
has
a
value
and
can
occur
in
expressions
the
most
common
example
is
while
c
getchar
EOF
The
other
assignment
operators
etc
can
also
occur
in
expressions
although
this
is
less
frequent
In
all
such
expressions
the
type
of
an
assignment
expression
is
the
type
of
its
left
operand
and
the
value
is
the
value
after
the
assignment
Exercise
In
a
two
s
complement
number
system
x
x
deletes
the
rightmost
bit
in
x
Explain
why
Use
this
observation
to
write
a
faster
version
of
bitcount
Conditional
Expressions
The
statements
if
a
b
z
a
else
z
b
compute
in
z
the
maximum
of
a
and
b
The
conditional
expression
written
with
the
ternary
operator
provides
an
alternate
way
to
write
this
and
similar
constructions
In
the
expression
expr
expr
expr
the
expression
expr
is
evaluated
first
If
it
is
non
zero
true
then
the
expression
expr
is
evaluated
and
that
is
the
value
of
the
conditional
expression
Otherwise
expr
is
evaluated
and
that
is
the
value
Only
one
of
expr
and
expr
is
evaluated
Thus
to
set
z
to
the
maximum
of
a
and
b
z
a
b
a
b
z
max
a
b
It
should
be
noted
that
the
conditional
expression
is
indeed
an
expression
and
it
can
be
used
wherever
any
other
expression
can
be
If
expr
and
expr
are
of
different
types
the
type
of
the
result
is
determined
by
the
conversion
rules
discussed
earlier
in
this
chapter
For
example
if
f
is
a
float
and
n
an
int
then
the
expression
n
f
n
is
of
type
float
regardless
of
whether
n
is
positive
Parentheses
are
not
necessary
around
the
first
expression
of
a
conditional
expression
since
the
precedence
of
is
very
low
just
above
assignment
They
are
advisable
anyway
however
since
they
make
the
condition
part
of
the
expression
easier
to
see
The
conditional
expression
often
leads
to
succinct
code
For
example
this
loop
prints
n
elements
of
an
array
per
line
with
each
column
separated
by
one
blank
and
with
each
line
including
the
last
terminated
by
a
newline
for
i
i
n
i
printf
d
c
a
i
i
i
n
n
A
newline
is
printed
after
every
tenth
element
and
after
the
n
th
All
other
elements
are
followed
by
one
blank
This
might
look
tricky
but
it
s
more
compact
than
the
equivalent
ifelse
Another
good
example
is
printf
You
have
d
items
s
n
n
n
s
Exercise
Rewrite
the
function
lower
which
converts
upper
case
letters
to
lower
case
with
a
conditional
expression
instead
of
if
else
Precedence
and
Order
of
Evaluation
Table
summarizes
the
rules
for
precedence
and
associativity
of
all
operators
including
those
that
we
have
not
yet
discussed
Operators
on
the
same
line
have
the
same
precedence
rows
are
in
order
of
decreasing
precedence
so
for
example
and
all
have
the
same
precedence
which
is
higher
than
that
of
binary
and
The
operator
refers
to
function
call
The
operators
and
are
used
to
access
members
of
structures
they
will
be
covered
in
Chapter
along
with
sizeof
size
of
an
object
Chapter
discusses
indirection
through
a
pointer
and
address
of
an
object
and
Chapter
discusses
the
comma
operator
Operators
Associativity
left
to
right
type
sizeof
right
to
left
left
to
right
left
to
right
left
to
right
left
to
right
left
to
right
left
to
right
left
to
right
left
to
right
left
to
right
left
to
right
right
to
left
right
to
left
left
to
right
Unary
and
have
higher
precedence
than
the
binary
forms
Table
Precedence
and
Associativity
of
Operators
Note
that
the
precedence
of
the
bitwise
operators
and
falls
below
and
This
implies
that
bit
testing
expressions
like
if
x
MASK
must
be
fully
parenthesized
to
give
proper
results
C
like
most
languages
does
not
specify
the
order
in
which
the
operands
of
an
operator
are
evaluated
The
exceptions
are
and
For
example
in
a
statement
like
x
f
g
f
may
be
evaluated
before
g
or
vice
versa
thus
if
either
f
or
g
alters
a
variable
on
which
the
other
depends
x
can
depend
on
the
order
of
evaluation
Intermediate
results
can
be
stored
in
temporary
variables
to
ensure
a
particular
sequence
Similarly
the
order
in
which
function
arguments
are
evaluated
is
not
specified
so
the
statement
printf
d
d
n
n
power
n
WRONG
can
produce
different
results
with
different
compilers
depending
on
whether
n
is
incremented
before
power
is
called
The
solution
of
course
is
to
write
n
printf
d
d
n
n
power
n
Function
calls
nested
assignment
statements
and
increment
and
decrement
operators
cause
side
effects
some
variable
is
changed
as
a
by
product
of
the
evaluation
of
an
expression
In
any
expression
involving
side
effects
there
can
be
subtle
dependencies
on
the
order
in
which
variables
taking
part
in
the
expression
are
updated
One
unhappy
situation
is
typified
by
the
statement
a
i
i
The
question
is
whether
the
subscript
is
the
old
value
of
i
or
the
new
Compilers
can
interpret
this
in
different
ways
and
generate
different
answers
depending
on
their
interpretation
The
standard
intentionally
leaves
most
such
matters
unspecified
When
side
effects
assignment
to
variables
take
place
within
an
expression
is
left
to
the
discretion
of
the
compiler
since
the
best
order
depends
strongly
on
machine
architecture
The
standard
does
specify
that
all
side
effects
on
arguments
take
effect
before
a
function
is
called
but
that
would
not
help
in
the
call
to
printf
above
The
moral
is
that
writing
code
that
depends
on
order
of
evaluation
is
a
bad
programming
practice
in
any
language
Naturally
it
is
necessary
to
know
what
things
to
avoid
but
if
you
don
t
know
how
they
are
done
on
various
machines
you
won
t
be
tempted
to
take
advantage
of
a
particular
implementation
Chapter
Control
Flow
The
control
flow
of
a
language
specify
the
order
in
which
computations
are
performed
We
have
already
met
the
most
common
control
flow
constructions
in
earlier
examples
here
we
will
complete
the
set
and
be
more
precise
about
the
ones
discussed
before
Statements
and
Blocks
An
expression
such
as
x
or
i
or
printf
becomes
a
statement
when
it
is
followed
by
a
semicolon
as
in
x
i
printf
In
C
the
semicolon
is
a
statement
terminator
rather
than
a
separator
as
it
is
in
languages
like
Pascal
Braces
and
are
used
to
group
declarations
and
statements
together
into
a
compound
statement
or
block
so
that
they
are
syntactically
equivalent
to
a
single
statement
The
braces
that
surround
the
statements
of
a
function
are
one
obvious
example
braces
around
multiple
statements
after
an
if
else
while
or
for
are
another
Variables
can
be
declared
inside
any
block
we
will
talk
about
this
in
Chapter
There
is
no
semicolon
after
the
right
brace
that
ends
a
block
If
Else
The
if
else
statement
is
used
to
express
decisions
Formally
the
syntax
is
if
expression
statement
else
statement
where
the
else
part
is
optional
The
expression
is
evaluated
if
it
is
true
that
is
if
expression
has
a
non
zero
value
statement
is
executed
If
it
is
false
expression
is
zero
and
if
there
is
an
else
part
statement
is
executed
instead
Since
an
if
tests
the
numeric
value
of
an
expression
certain
coding
shortcuts
are
possible
The
most
obvious
is
writing
if
expression
instead
of
if
expression
Sometimes
this
is
natural
and
clear
at
other
times
it
can
be
cryptic
Because
the
else
part
of
an
if
else
is
optional
there
is
an
ambiguity
when
an
else
if
omitted
from
a
nested
if
sequence
This
is
resolved
by
associating
the
else
with
the
closest
previous
else
less
if
For
example
in
if
n
if
a
b
z
a
else
z
b
the
else
goes
to
the
inner
if
as
we
have
shown
by
indentation
If
that
isn
t
what
you
want
braces
must
be
used
to
force
the
proper
association
if
n
if
a
b
z
a
else
z
b
The
ambiguity
is
especially
pernicious
in
situations
like
this
if
n
for
i
i
n
i
if
s
i
printf
return
i
else
WRONG
printf
error
n
is
negative
n
The
indentation
shows
unequivocally
what
you
want
but
the
compiler
doesn
t
get
the
message
and
associates
the
else
with
the
inner
if
This
kind
of
bug
can
be
hard
to
find
it
s
a
good
idea
to
use
braces
when
there
are
nested
ifs
By
the
way
notice
that
there
is
a
semicolon
after
z
a
in
if
a
b
z
a
else
z
b
This
is
because
grammatically
a
statement
follows
the
if
and
an
expression
statement
like
z
a
is
always
terminated
by
a
semicolon
Else
If
The
construction
if
expression
statement
else
if
expression
statement
else
if
expression
statement
else
if
expression
statement
else
statement
occurs
so
often
that
it
is
worth
a
brief
separate
discussion
This
sequence
of
if
statements
is
the
most
general
way
of
writing
a
multi
way
decision
The
expressions
are
evaluated
in
order
if
an
expression
is
true
the
statement
associated
with
it
is
executed
and
this
terminates
the
whole
chain
As
always
the
code
for
each
statement
is
either
a
single
statement
or
a
group
of
them
in
braces
The
last
else
part
handles
the
none
of
the
above
or
default
case
where
none
of
the
other
conditions
is
satisfied
Sometimes
there
is
no
explicit
action
for
the
default
in
that
case
the
trailing
else
statement
can
be
omitted
or
it
may
be
used
for
error
checking
to
catch
an
impossible
condition
To
illustrate
a
three
way
decision
here
is
a
binary
search
function
that
decides
if
a
particular
value
x
occurs
in
the
sorted
array
v
The
elements
of
v
must
be
in
increasing
order
The
function
returns
the
position
a
number
between
and
n
if
x
occurs
in
v
and
if
not
Binary
search
first
compares
the
input
value
x
to
the
middle
element
of
the
array
v
If
x
is
less
than
the
middle
value
searching
focuses
on
the
lower
half
of
the
table
otherwise
on
the
upper
half
In
either
case
the
next
step
is
to
compare
x
to
the
middle
element
of
the
selected
half
This
process
of
dividing
the
range
in
two
continues
until
the
value
is
found
or
the
range
is
empty
binsearch
find
x
in
v
v
v
n
int
binsearch
int
x
int
v
int
n
int
low
high
mid
low
high
n
while
low
high
mid
low
high
if
x
v
mid
high
mid
else
if
x
v
mid
low
mid
else
found
match
return
mid
return
no
match
The
fundamental
decision
is
whether
x
is
less
than
greater
than
or
equal
to
the
middle
element
v
mid
at
each
step
this
is
a
natural
for
else
if
Exercise
Our
binary
search
makes
two
tests
inside
the
loop
when
one
would
suffice
at
the
price
of
more
tests
outside
Write
a
version
with
only
one
test
inside
the
loop
and
measure
the
difference
in
run
time
Switch
The
switch
statement
is
a
multi
way
decision
that
tests
whether
an
expression
matches
one
of
a
number
of
constant
integer
values
and
branches
accordingly
switch
expression
case
const
expr
statements
case
const
expr
statements
default
statements
Each
case
is
labeled
by
one
or
more
integer
valued
constants
or
constant
expressions
If
a
case
matches
the
expression
value
execution
starts
at
that
case
All
case
expressions
must
be
different
The
case
labeled
default
is
executed
if
none
of
the
other
cases
are
satisfied
A
default
is
optional
if
it
isn
t
there
and
if
none
of
the
cases
match
no
action
at
all
takes
place
Cases
and
the
default
clause
can
occur
in
any
order
In
Chapter
we
wrote
a
program
to
count
the
occurrences
of
each
digit
white
space
and
all
other
characters
using
a
sequence
of
if
else
if
else
Here
is
the
same
program
with
a
switch
include
stdio
h
main
count
digits
white
space
others
int
c
i
nwhite
nother
ndigit
nwhite
nother
for
i
i
i
ndigit
i
while
c
getchar
EOF
switch
c
case
case
case
case
case
case
case
case
case
case
ndigit
c
break
case
case
n
case
t
nwhite
break
default
nother
break
printf
digits
for
i
i
i
printf
d
ndigit
i
printf
white
space
d
other
d
n
nwhite
nother
return
The
break
statement
causes
an
immediate
exit
from
the
switch
Because
cases
serve
just
as
labels
after
the
code
for
one
case
is
done
execution
falls
through
to
the
next
unless
you
take
explicit
action
to
escape
break
and
return
are
the
most
common
ways
to
leave
a
switch
A
break
statement
can
also
be
used
to
force
an
immediate
exit
from
while
for
and
do
loops
as
will
be
discussed
later
in
this
chapter
Falling
through
cases
is
a
mixed
blessing
On
the
positive
side
it
allows
several
cases
to
be
attached
to
a
single
action
as
with
the
digits
in
this
example
But
it
also
implies
that
normally
each
case
must
end
with
a
break
to
prevent
falling
through
to
the
next
Falling
through
from
one
case
to
another
is
not
robust
being
prone
to
disintegration
when
the
program
is
modified
With
the
exception
of
multiple
labels
for
a
single
computation
fall
throughs
should
be
used
sparingly
and
commented
As
a
matter
of
good
form
put
a
break
after
the
last
case
the
default
here
even
though
it
s
logically
unnecessary
Some
day
when
another
case
gets
added
at
the
end
this
bit
of
defensive
programming
will
save
you
Exercise
Write
a
function
escape
s
t
that
converts
characters
like
newline
and
tab
into
visible
escape
sequences
like
n
and
t
as
it
copies
the
string
t
to
s
Use
a
switch
Write
a
function
for
the
other
direction
as
well
converting
escape
sequences
into
the
real
characters
Loops
While
and
For
We
have
already
encountered
the
while
and
for
loops
In
while
expression
statement
the
expression
is
evaluated
If
it
is
non
zero
statement
is
executed
and
expression
is
reevaluated
This
cycle
continues
until
expression
becomes
zero
at
which
point
execution
resumes
after
statement
The
for
statement
for
expr
expr
expr
statement
is
equivalent
to
expr
while
expr
statement
expr
except
for
the
behaviour
of
continue
which
is
described
in
Section
Grammatically
the
three
components
of
a
for
loop
are
expressions
Most
commonly
expr
and
expr
are
assignments
or
function
calls
and
expr
is
a
relational
expression
Any
of
the
three
parts
can
be
omitted
although
the
semicolons
must
remain
If
expr
or
expr
is
omitted
it
is
simply
dropped
from
the
expansion
If
the
test
expr
is
not
present
it
is
taken
as
permanently
true
so
for
is
an
infinite
loop
presumably
to
be
broken
by
other
means
such
as
a
break
or
return
Whether
to
use
while
or
for
is
largely
a
matter
of
personal
preference
For
example
in
while
c
getchar
c
n
c
t
skip
white
space
characters
there
is
no
initialization
or
re
initialization
so
the
while
is
most
natural
The
for
is
preferable
when
there
is
a
simple
initialization
and
increment
since
it
keeps
the
loop
control
statements
close
together
and
visible
at
the
top
of
the
loop
This
is
most
obvious
in
for
i
i
n
i
which
is
the
C
idiom
for
processing
the
first
n
elements
of
an
array
the
analog
of
the
Fortran
DO
loop
or
the
Pascal
for
The
analogy
is
not
perfect
however
since
the
index
variable
i
retains
its
value
when
the
loop
terminates
for
any
reason
Because
the
components
of
the
for
are
arbitrary
expressions
for
loops
are
not
restricted
to
arithmetic
progressions
Nonetheless
it
is
bad
style
to
force
unrelated
computations
into
the
initialization
and
increment
of
a
for
which
are
better
reserved
for
loop
control
operations
As
a
larger
example
here
is
another
version
of
atoi
for
converting
a
string
to
its
numeric
equivalent
This
one
is
slightly
more
general
than
the
one
in
Chapter
it
copes
with
optional
leading
white
space
and
an
optional
or
sign
Chapter
shows
atof
which
does
the
same
conversion
for
floating
point
numbers
The
structure
of
the
program
reflects
the
form
of
the
input
skip
white
space
if
any
get
sign
if
any
get
integer
part
and
convert
it
Each
step
does
its
part
and
leaves
things
in
a
clean
state
for
the
next
The
whole
process
terminates
on
the
first
character
that
could
not
be
part
of
a
number
include
ctype
h
atoi
convert
s
to
integer
version
int
atoi
char
s
int
i
n
sign
for
i
isspace
s
i
i
skip
white
space
sign
s
i
if
s
i
s
i
skip
sign
i
for
n
isdigit
s
i
i
n
n
s
i
return
sign
n
The
standard
library
provides
a
more
elaborate
function
strtol
for
conversion
of
strings
to
long
integers
see
Section
of
Appendix
B
The
advantages
of
keeping
loop
control
centralized
are
even
more
obvious
when
there
are
several
nested
loops
The
following
function
is
a
Shell
sort
for
sorting
an
array
of
integers
The
basic
idea
of
this
sorting
algorithm
which
was
invented
in
by
D
L
Shell
is
that
in
early
stages
far
apart
elements
are
compared
rather
than
adjacent
ones
as
in
simpler
interchange
sorts
This
tends
to
eliminate
large
amounts
of
disorder
quickly
so
later
stages
have
less
work
to
do
The
interval
between
compared
elements
is
gradually
decreased
to
one
at
which
point
the
sort
effectively
becomes
an
adjacent
interchange
method
shellsort
sort
v
v
n
into
increasing
order
void
shellsort
int
v
int
n
int
gap
i
j
temp
for
gap
n
gap
gap
for
i
gap
i
n
i
for
j
i
gap
j
v
j
v
j
gap
j
gap
temp
v
j
v
j
v
j
gap
v
j
gap
temp
There
are
three
nested
loops
The
outermost
controls
the
gap
between
compared
elements
shrinking
it
from
n
by
a
factor
of
two
each
pass
until
it
becomes
zero
The
middle
loop
steps
along
the
elements
The
innermost
loop
compares
each
pair
of
elements
that
is
separated
by
gap
and
reverses
any
that
are
out
of
order
Since
gap
is
eventually
reduced
to
one
all
elements
are
eventually
ordered
correctly
Notice
how
the
generality
of
the
for
makes
the
outer
loop
fit
in
the
same
form
as
the
others
even
though
it
is
not
an
arithmetic
progression
One
final
C
operator
is
the
comma
which
most
often
finds
use
in
the
for
statement
A
pair
of
expressions
separated
by
a
comma
is
evaluated
left
to
right
and
the
type
and
value
of
the
result
are
the
type
and
value
of
the
right
operand
Thus
in
a
for
statement
it
is
possible
to
place
multiple
expressions
in
the
various
parts
for
example
to
process
two
indices
in
parallel
This
is
illustrated
in
the
function
reverse
s
which
reverses
the
string
s
in
place
include
string
h
reverse
reverse
string
s
in
place
void
reverse
char
s
int
c
i
j
for
i
j
strlen
s
i
j
i
j
c
s
i
s
i
s
j
s
j
c
The
commas
that
separate
function
arguments
variables
in
declarations
etc
are
not
comma
operators
and
do
not
guarantee
left
to
right
evaluation
Comma
operators
should
be
used
sparingly
The
most
suitable
uses
are
for
constructs
strongly
related
to
each
other
as
in
the
for
loop
in
reverse
and
in
macros
where
a
multistep
computation
has
to
be
a
single
expression
A
comma
expression
might
also
be
appropriate
for
the
exchange
of
elements
in
reverse
where
the
exchange
can
be
thought
of
a
single
operation
for
i
j
strlen
s
i
j
i
j
c
s
i
s
i
s
j
s
j
c
Exercise
Write
a
function
expand
s
s
that
expands
shorthand
notations
like
a
z
in
the
string
s
into
the
equivalent
complete
list
abc
xyz
in
s
Allow
for
letters
of
either
case
and
digits
and
be
prepared
to
handle
cases
like
a
b
c
and
a
z
and
a
z
Arrange
that
a
leading
or
trailing
is
taken
literally
Loops
Do
While
As
we
discussed
in
Chapter
the
while
and
for
loops
test
the
termination
condition
at
the
top
By
contrast
the
third
loop
in
C
the
do
while
tests
at
the
bottom
after
making
each
pass
through
the
loop
body
the
body
is
always
executed
at
least
once
The
syntax
of
the
do
is
do
statement
while
expression
The
statement
is
executed
then
expression
is
evaluated
If
it
is
true
statement
is
evaluated
again
and
so
on
When
the
expression
becomes
false
the
loop
terminates
Except
for
the
sense
of
the
test
do
while
is
equivalent
to
the
Pascal
repeat
until
statement
Experience
shows
that
do
while
is
much
less
used
than
while
and
for
Nonetheless
from
time
to
time
it
is
valuable
as
in
the
following
function
itoa
which
converts
a
number
to
a
character
string
the
inverse
of
atoi
The
job
is
slightly
more
complicated
than
might
be
thought
at
first
because
the
easy
methods
of
generating
the
digits
generate
them
in
the
wrong
order
We
have
chosen
to
generate
the
string
backwards
then
reverse
it
itoa
convert
n
to
characters
in
s
void
itoa
int
n
char
s
int
i
sign
if
sign
n
record
sign
n
n
make
n
positive
i
do
generate
digits
in
reverse
order
s
i
n
get
next
digit
while
n
delete
it
if
sign
s
i
s
i
reverse
s
The
do
while
is
necessary
or
at
least
convenient
since
at
least
one
character
must
be
installed
in
the
array
s
even
if
n
is
zero
We
also
used
braces
around
the
single
statement
that
makes
up
the
body
of
the
do
while
even
though
they
are
unnecessary
so
the
hasty
reader
will
not
mistake
the
while
part
for
the
beginning
of
a
while
loop
Exercise
In
a
two
s
complement
number
representation
our
version
of
itoa
does
not
handle
the
largest
negative
number
that
is
the
value
of
n
equal
to
wordsize
Explain
why
not
Modify
it
to
print
that
value
correctly
regardless
of
the
machine
on
which
it
runs
Exercise
Write
the
function
itob
n
s
b
that
converts
the
integer
n
into
a
base
b
character
representation
in
the
string
s
In
particular
itob
n
s
formats
s
as
a
hexadecimal
integer
in
s
Exercise
Write
a
version
of
itoa
that
accepts
three
arguments
instead
of
two
The
third
argument
is
a
minimum
field
width
the
converted
number
must
be
padded
with
blanks
on
the
left
if
necessary
to
make
it
wide
enough
Break
and
Continue
It
is
sometimes
convenient
to
be
able
to
exit
from
a
loop
other
than
by
testing
at
the
top
or
bottom
The
break
statement
provides
an
early
exit
from
for
while
and
do
just
as
from
switch
A
break
causes
the
innermost
enclosing
loop
or
switch
to
be
exited
immediately
The
following
function
trim
removes
trailing
blanks
tabs
and
newlines
from
the
end
of
a
string
using
a
break
to
exit
from
a
loop
when
the
rightmost
non
blank
non
tab
non
newline
is
found
trim
remove
trailing
blanks
tabs
newlines
int
trim
char
s
int
n
for
n
strlen
s
n
n
if
s
n
s
n
t
s
n
n
break
s
n
return
n
strlen
returns
the
length
of
the
string
The
for
loop
starts
at
the
end
and
scans
backwards
looking
for
the
first
character
that
is
not
a
blank
or
tab
or
newline
The
loop
is
broken
when
one
is
found
or
when
n
becomes
negative
that
is
when
the
entire
string
has
been
scanned
You
should
verify
that
this
is
correct
behavior
even
when
the
string
is
empty
or
contains
only
white
space
characters
The
continue
statement
is
related
to
break
but
less
often
used
it
causes
the
next
iteration
of
the
enclosing
for
while
or
do
loop
to
begin
In
the
while
and
do
this
means
that
the
test
part
is
executed
immediately
in
the
for
control
passes
to
the
increment
step
The
continue
statement
applies
only
to
loops
not
to
switch
A
continue
inside
a
switch
inside
a
loop
causes
the
next
loop
iteration
As
an
example
this
fragment
processes
only
the
non
negative
elements
in
the
array
a
negative
values
are
skipped
for
i
i
n
i
if
a
i
skip
negative
elements
continue
do
positive
elements
The
continue
statement
is
often
used
when
the
part
of
the
loop
that
follows
is
complicated
so
that
reversing
a
test
and
indenting
another
level
would
nest
the
program
too
deeply
Goto
and
labels
C
provides
the
infinitely
abusable
goto
statement
and
labels
to
branch
to
Formally
the
goto
statement
is
never
necessary
and
in
practice
it
is
almost
always
easy
to
write
code
without
it
We
have
not
used
goto
in
this
book
Nevertheless
there
are
a
few
situations
where
gotos
may
find
a
place
The
most
common
is
to
abandon
processing
in
some
deeply
nested
structure
such
as
breaking
out
of
two
or
more
loops
at
once
The
break
statement
cannot
be
used
directly
since
it
only
exits
from
the
innermost
loop
Thus
for
for
if
disaster
goto
error
error
clean
up
the
mess
This
organization
is
handy
if
the
error
handling
code
is
non
trivial
and
if
errors
can
occur
in
several
places
A
label
has
the
same
form
as
a
variable
name
and
is
followed
by
a
colon
It
can
be
attached
to
any
statement
in
the
same
function
as
the
goto
The
scope
of
a
label
is
the
entire
function
As
another
example
consider
the
problem
of
determining
whether
two
arrays
a
and
b
have
an
element
in
common
One
possibility
is
for
i
i
n
i
for
j
j
m
j
if
a
i
b
j
goto
found
didn
t
find
any
common
element
found
got
one
a
i
b
j
Code
involving
a
goto
can
always
be
written
without
one
though
perhaps
at
the
price
of
some
repeated
tests
or
an
extra
variable
For
example
the
array
search
becomes
found
for
i
i
n
found
i
for
j
j
m
found
j
if
a
i
b
j
found
if
found
got
one
a
i
b
j
else
didn
t
find
any
common
element
With
a
few
exceptions
like
those
cited
here
code
that
relies
on
goto
statements
is
generally
harder
to
understand
and
to
maintain
than
code
without
gotos
Although
we
are
not
dogmatic
about
the
matter
it
does
seem
that
goto
statements
should
be
used
rarely
if
at
all
Chapter
Functions
and
Program
Structure
Functions
break
large
computing
tasks
into
smaller
ones
and
enable
people
to
build
on
what
others
have
done
instead
of
starting
over
from
scratch
Appropriate
functions
hide
details
of
operation
from
parts
of
the
program
that
don
t
need
to
know
about
them
thus
clarifying
the
whole
and
easing
the
pain
of
making
changes
C
has
been
designed
to
make
functions
efficient
and
easy
to
use
C
programs
generally
consist
of
many
small
functions
rather
than
a
few
big
ones
A
program
may
reside
in
one
or
more
source
files
Source
files
may
be
compiled
separately
and
loaded
together
along
with
previously
compiled
functions
from
libraries
We
will
not
go
into
that
process
here
however
since
the
details
vary
from
system
to
system
Function
declaration
and
definition
is
the
area
where
the
ANSI
standard
has
made
the
most
changes
to
C
As
we
saw
first
in
Chapter
it
is
now
possible
to
declare
the
type
of
arguments
when
a
function
is
declared
The
syntax
of
function
declaration
also
changes
so
that
declarations
and
definitions
match
This
makes
it
possible
for
a
compiler
to
detect
many
more
errors
than
it
could
before
Furthermore
when
arguments
are
properly
declared
appropriate
type
coercions
are
performed
automatically
The
standard
clarifies
the
rules
on
the
scope
of
names
in
particular
it
requires
that
there
be
only
one
definition
of
each
external
object
Initialization
is
more
general
automatic
arrays
and
structures
may
now
be
initialized
The
C
preprocessor
has
also
been
enhanced
New
preprocessor
facilities
include
a
more
complete
set
of
conditional
compilation
directives
a
way
to
create
quoted
strings
from
macro
arguments
and
better
control
over
the
macro
expansion
process
Basics
of
Functions
To
begin
with
let
us
design
and
write
a
program
to
print
each
line
of
its
input
that
contains
a
particular
pattern
or
string
of
characters
This
is
a
special
case
of
the
UNIX
program
grep
For
example
searching
for
the
pattern
of
letters
ould
in
the
set
of
lines
Ah
Love
could
you
and
I
with
Fate
conspire
To
grasp
this
sorry
Scheme
of
Things
entire
Would
not
we
shatter
it
to
bits
and
then
Re
mould
it
nearer
to
the
Heart
s
Desire
will
produce
the
output
Ah
Love
could
you
and
I
with
Fate
conspire
Would
not
we
shatter
it
to
bits
and
then
Re
mould
it
nearer
to
the
Heart
s
Desire
The
job
falls
neatly
into
three
pieces
while
there
s
another
line
if
the
line
contains
the
pattern
print
it
Although
it
s
certainly
possible
to
put
the
code
for
all
of
this
in
main
a
better
way
is
to
use
the
structure
to
advantage
by
making
each
part
a
separate
function
Three
small
pieces
are
better
to
deal
with
than
one
big
one
because
irrelevant
details
can
be
buried
in
the
functions
and
the
chance
of
unwanted
interactions
is
minimized
And
the
pieces
may
even
be
useful
in
other
programs
While
there
s
another
line
is
getline
a
function
that
we
wrote
in
Chapter
and
print
it
is
printf
which
someone
has
already
provided
for
us
This
means
we
need
only
write
a
routine
to
decide
whether
the
line
contains
an
occurrence
of
the
pattern
We
can
solve
that
problem
by
writing
a
function
strindex
s
t
that
returns
the
position
or
index
in
the
string
s
where
the
string
t
begins
or
if
s
does
not
contain
t
Because
C
arrays
begin
at
position
zero
indexes
will
be
zero
or
positive
and
so
a
negative
value
like
is
convenient
for
signaling
failure
When
we
later
need
more
sophisticated
pattern
matching
we
only
have
to
replace
strindex
the
rest
of
the
code
can
remain
the
same
The
standard
library
provides
a
function
strstr
that
is
similar
to
strindex
except
that
it
returns
a
pointer
instead
of
an
index
Given
this
much
design
filling
in
the
details
of
the
program
is
straightforward
Here
is
the
whole
thing
so
you
can
see
how
the
pieces
fit
together
For
now
the
pattern
to
be
searched
for
is
a
literal
string
which
is
not
the
most
general
of
mechanisms
We
will
return
shortly
to
a
discussion
of
how
to
initialize
character
arrays
and
in
Chapter
will
show
how
to
make
the
pattern
a
parameter
that
is
set
when
the
program
is
run
There
is
also
a
slightly
different
version
of
getline
you
might
find
it
instructive
to
compare
it
to
the
one
in
Chapter
include
stdio
h
define
MAXLINE
maximum
input
line
length
int
getline
char
line
int
max
int
strindex
char
source
char
searchfor
char
pattern
ould
pattern
to
search
for
find
all
lines
matching
pattern
main
char
line
MAXLINE
int
found
while
getline
line
MAXLINE
if
strindex
line
pattern
printf
s
line
found
return
found
getline
get
line
into
s
return
length
int
getline
char
s
int
lim
int
c
i
i
while
lim
c
getchar
EOF
c
n
s
i
c
if
c
n
s
i
c
s
i
return
i
strindex
return
index
of
t
in
s
if
none
int
strindex
char
s
char
t
int
i
j
k
for
i
s
i
i
for
j
i
k
t
k
s
j
t
k
j
k
if
k
t
k
return
i
return
Each
function
definition
has
the
form
return
type
function
name
argument
declarations
declarations
and
statements
Various
parts
may
be
absent
a
minimal
function
is
dummy
which
does
nothing
and
returns
nothing
A
do
nothing
function
like
this
is
sometimes
useful
as
a
place
holder
during
program
development
If
the
return
type
is
omitted
int
is
assumed
A
program
is
just
a
set
of
definitions
of
variables
and
functions
Communication
between
the
functions
is
by
arguments
and
values
returned
by
the
functions
and
through
external
variables
The
functions
can
occur
in
any
order
in
the
source
file
and
the
source
program
can
be
split
into
multiple
files
so
long
as
no
function
is
split
The
return
statement
is
the
mechanism
for
returning
a
value
from
the
called
function
to
its
caller
Any
expression
can
follow
return
return
expression
The
expression
will
be
converted
to
the
return
type
of
the
function
if
necessary
Parentheses
are
often
used
around
the
expression
but
they
are
optional
The
calling
function
is
free
to
ignore
the
returned
value
Furthermore
there
need
to
be
no
expression
after
return
in
that
case
no
value
is
returned
to
the
caller
Control
also
returns
to
the
caller
with
no
value
when
execution
falls
off
the
end
of
the
function
by
reaching
the
closing
right
brace
It
is
not
illegal
but
probably
a
sign
of
trouble
if
a
function
returns
a
value
from
one
place
and
no
value
from
another
In
any
case
if
a
function
fails
to
return
a
value
its
value
is
certain
to
be
garbage
The
pattern
searching
program
returns
a
status
from
main
the
number
of
matches
found
This
value
is
available
for
use
by
the
environment
that
called
the
program
The
mechanics
of
how
to
compile
and
load
a
C
program
that
resides
on
multiple
source
files
vary
from
one
system
to
the
next
On
the
UNIX
system
for
example
the
cc
command
mentioned
in
Chapter
does
the
job
Suppose
that
the
three
functions
are
stored
in
three
files
called
main
c
getline
c
and
strindex
c
Then
the
command
cc
main
c
getline
c
strindex
c
compiles
the
three
files
placing
the
resulting
object
code
in
files
main
o
getline
o
and
strindex
o
then
loads
them
all
into
an
executable
file
called
a
out
If
there
is
an
error
say
in
main
c
the
file
can
be
recompiled
by
itself
and
the
result
loaded
with
the
previous
object
files
with
the
command
cc
main
c
getline
o
strindex
o
The
cc
command
uses
the
c
versus
o
naming
convention
to
distinguish
source
files
from
object
files
Exercise
Write
the
function
strindex
s
t
which
returns
the
position
of
the
rightmost
occurrence
of
t
in
s
or
if
there
is
none
Functions
Returning
Non
integers
So
far
our
examples
of
functions
have
returned
either
no
value
void
or
an
int
What
if
a
function
must
return
some
other
type
many
numerical
functions
like
sqrt
sin
and
cos
return
double
other
specialized
functions
return
other
types
To
illustrate
how
to
deal
with
this
let
us
write
and
use
the
function
atof
s
which
converts
the
string
s
to
its
doubleprecision
floating
point
equivalent
atof
if
an
extension
of
atoi
which
we
showed
versions
of
in
Chapters
and
It
handles
an
optional
sign
and
decimal
point
and
the
presence
or
absence
of
either
part
or
fractional
part
Our
version
is
not
a
high
quality
input
conversion
routine
that
would
take
more
space
than
we
care
to
use
The
standard
library
includes
an
atof
the
header
stdlib
h
declares
it
First
atof
itself
must
declare
the
type
of
value
it
returns
since
it
is
not
int
The
type
name
precedes
the
function
name
include
ctype
h
atof
convert
string
s
to
double
double
atof
char
s
double
val
power
int
i
sign
for
i
isspace
s
i
i
skip
white
space
sign
s
i
if
s
i
s
i
i
for
val
isdigit
s
i
i
val
val
s
i
if
s
i
i
for
power
isdigit
s
i
i
val
val
s
i
power
return
sign
val
power
Second
and
just
as
important
the
calling
routine
must
know
that
atof
returns
a
non
int
value
One
way
to
ensure
this
is
to
declare
atof
explicitly
in
the
calling
routine
The
declaration
is
shown
in
this
primitive
calculator
barely
adequate
for
check
book
balancing
which
reads
one
number
per
line
optionally
preceded
with
a
sign
and
adds
them
up
printing
the
running
sum
after
each
input
include
stdio
h
define
MAXLINE
rudimentary
calculator
main
double
sum
atof
char
char
line
MAXLINE
int
getline
char
line
int
max
sum
while
getline
line
MAXLINE
printf
t
g
n
sum
atof
line
return
The
declaration
double
sum
atof
char
says
that
sum
is
a
double
variable
and
that
atof
is
a
function
that
takes
one
char
argument
and
returns
a
double
The
function
atof
must
be
declared
and
defined
consistently
If
atof
itself
and
the
call
to
it
in
main
have
inconsistent
types
in
the
same
source
file
the
error
will
be
detected
by
the
compiler
But
if
as
is
more
likely
atof
were
compiled
separately
the
mismatch
would
not
be
detected
atof
would
return
a
double
that
main
would
treat
as
an
int
and
meaningless
answers
would
result
In
the
light
of
what
we
have
said
about
how
declarations
must
match
definitions
this
might
seem
surprising
The
reason
a
mismatch
can
happen
is
that
if
there
is
no
function
prototype
a
function
is
implicitly
declared
by
its
first
appearance
in
an
expression
such
as
sum
atof
line
If
a
name
that
has
not
been
previously
declared
occurs
in
an
expression
and
is
followed
by
a
left
parentheses
it
is
declared
by
context
to
be
a
function
name
the
function
is
assumed
to
return
an
int
and
nothing
is
assumed
about
its
arguments
Furthermore
if
a
function
declaration
does
not
include
arguments
as
in
double
atof
that
too
is
taken
to
mean
that
nothing
is
to
be
assumed
about
the
arguments
of
atof
all
parameter
checking
is
turned
off
This
special
meaning
of
the
empty
argument
list
is
intended
to
permit
older
C
programs
to
compile
with
new
compilers
But
it
s
a
bad
idea
to
use
it
with
new
C
programs
If
the
function
takes
arguments
declare
them
if
it
takes
no
arguments
use
void
Given
atof
properly
declared
we
could
write
atoi
convert
a
string
to
int
in
terms
of
it
atoi
convert
string
s
to
integer
using
atof
int
atoi
char
s
double
atof
char
s
return
int
atof
s
Notice
the
structure
of
the
declarations
and
the
return
statement
The
value
of
the
expression
in
return
expression
is
converted
to
the
type
of
the
function
before
the
return
is
taken
Therefore
the
value
of
atof
a
double
is
converted
automatically
to
int
when
it
appears
in
this
return
since
the
function
atoi
returns
an
int
This
operation
does
potentionally
discard
information
however
so
some
compilers
warn
of
it
The
cast
states
explicitly
that
the
operation
is
intended
and
suppresses
any
warning
Exercise
Extend
atof
to
handle
scientific
notation
of
the
form
e
where
a
floating
point
number
may
be
followed
by
e
or
E
and
an
optionally
signed
exponent
External
Variables
A
C
program
consists
of
a
set
of
external
objects
which
are
either
variables
or
functions
The
adjective
external
is
used
in
contrast
to
internal
which
describes
the
arguments
and
variables
defined
inside
functions
External
variables
are
defined
outside
of
any
function
and
are
thus
potentionally
available
to
many
functions
Functions
themselves
are
always
external
because
C
does
not
allow
functions
to
be
defined
inside
other
functions
By
default
external
variables
and
functions
have
the
property
that
all
references
to
them
by
the
same
name
even
from
functions
compiled
separately
are
references
to
the
same
thing
The
standard
calls
this
property
external
linkage
In
this
sense
external
variables
are
analogous
to
Fortran
COMMON
blocks
or
variables
in
the
outermost
block
in
Pascal
We
will
see
later
how
to
define
external
variables
and
functions
that
are
visible
only
within
a
single
source
file
Because
external
variables
are
globally
accessible
they
provide
an
alternative
to
function
arguments
and
return
values
for
communicating
data
between
functions
Any
function
may
access
an
external
variable
by
referring
to
it
by
name
if
the
name
has
been
declared
somehow
If
a
large
number
of
variables
must
be
shared
among
functions
external
variables
are
more
convenient
and
efficient
than
long
argument
lists
As
pointed
out
in
Chapter
however
this
reasoning
should
be
applied
with
some
caution
for
it
can
have
a
bad
effect
on
program
structure
and
lead
to
programs
with
too
many
data
connections
between
functions
External
variables
are
also
useful
because
of
their
greater
scope
and
lifetime
Automatic
variables
are
internal
to
a
function
they
come
into
existence
when
the
function
is
entered
and
disappear
when
it
is
left
External
variables
on
the
other
hand
are
permanent
so
they
can
retain
values
from
one
function
invocation
to
the
next
Thus
if
two
functions
must
share
some
data
yet
neither
calls
the
other
it
is
often
most
convenient
if
the
shared
data
is
kept
in
external
variables
rather
than
being
passed
in
and
out
via
arguments
Let
us
examine
this
issue
with
a
larger
example
The
problem
is
to
write
a
calculator
program
that
provides
the
operators
and
Because
it
is
easier
to
implement
the
calculator
will
use
reverse
Polish
notation
instead
of
infix
Reverse
Polish
notation
is
used
by
some
pocket
calculators
and
in
languages
like
Forth
and
Postscript
In
reverse
Polish
notation
each
operator
follows
its
operands
an
infix
expression
like
is
entered
as
Parentheses
are
not
needed
the
notation
is
unambiguous
as
long
as
we
know
how
many
operands
each
operator
expects
The
implementation
is
simple
Each
operand
is
pushed
onto
a
stack
when
an
operator
arrives
the
proper
number
of
operands
two
for
binary
operators
is
popped
the
operator
is
applied
to
them
and
the
result
is
pushed
back
onto
the
stack
In
the
example
above
for
instance
and
are
pushed
then
replaced
by
their
difference
Next
and
are
pushed
and
then
replaced
by
their
sum
The
product
of
and
which
is
replaces
them
on
the
stack
The
value
on
the
top
of
the
stack
is
popped
and
printed
when
the
end
of
the
input
line
is
encountered
The
structure
of
the
program
is
thus
a
loop
that
performs
the
proper
operation
on
each
operator
and
operand
as
it
appears
while
next
operator
or
operand
is
not
end
of
file
indicator
if
number
push
it
else
if
operator
pop
operands
do
operation
push
result
else
if
newline
pop
and
print
top
of
stack
else
error
The
operation
of
pushing
and
popping
a
stack
are
trivial
but
by
the
time
error
detection
and
recovery
are
added
they
are
long
enough
that
it
is
better
to
put
each
in
a
separate
function
than
to
repeat
the
code
throughout
the
whole
program
And
there
should
be
a
separate
function
for
fetching
the
next
input
operator
or
operand
The
main
design
decision
that
has
not
yet
been
discussed
is
where
the
stack
is
that
is
which
routines
access
it
directly
On
possibility
is
to
keep
it
in
main
and
pass
the
stack
and
the
current
stack
position
to
the
routines
that
push
and
pop
it
But
main
doesn
t
need
to
know
about
the
variables
that
control
the
stack
it
only
does
push
and
pop
operations
So
we
have
decided
to
store
the
stack
and
its
associated
information
in
external
variables
accessible
to
the
push
and
pop
functions
but
not
to
main
Translating
this
outline
into
code
is
easy
enough
If
for
now
we
think
of
the
program
as
existing
in
one
source
file
it
will
look
like
this
includes
defines
function
declarations
for
main
main
external
variables
for
push
and
pop
void
push
double
f
double
pop
void
int
getop
char
s
routines
called
by
getop
Later
we
will
discuss
how
this
might
be
split
into
two
or
more
source
files
The
function
main
is
a
loop
containing
a
big
switch
on
the
type
of
operator
or
operand
this
is
a
more
typical
use
of
switch
than
the
one
shown
in
Section
include
stdio
h
include
stdlib
h
for
atof
define
MAXOP
max
size
of
operand
or
operator
define
NUMBER
signal
that
a
number
was
found
int
getop
char
void
push
double
double
pop
void
reverse
Polish
calculator
main
int
type
double
op
char
s
MAXOP
while
type
getop
s
EOF
switch
type
case
NUMBER
push
atof
s
break
case
push
pop
pop
break
case
push
pop
pop
break
case
op
pop
push
pop
op
break
case
op
pop
if
op
push
pop
op
else
printf
error
zero
divisor
n
break
case
n
printf
t
g
n
pop
break
default
printf
error
unknown
command
s
n
s
break
return
Because
and
are
commutative
operators
the
order
in
which
the
popped
operands
are
combined
is
irrelevant
but
for
and
the
left
and
right
operand
must
be
distinguished
In
push
pop
pop
WRONG
the
order
in
which
the
two
calls
of
pop
are
evaluated
is
not
defined
To
guarantee
the
right
order
it
is
necessary
to
pop
the
first
value
into
a
temporary
variable
as
we
did
in
main
define
MAXVAL
maximum
depth
of
val
stack
int
sp
next
free
stack
position
double
val
MAXVAL
value
stack
push
push
f
onto
value
stack
void
push
double
f
if
sp
MAXVAL
val
sp
f
else
printf
error
stack
full
can
t
push
g
n
f
pop
pop
and
return
top
value
from
stack
double
pop
void
if
sp
return
val
sp
else
printf
error
stack
empty
n
return
A
variable
is
external
if
it
is
defined
outside
of
any
function
Thus
the
stack
and
stack
index
that
must
be
shared
by
push
and
pop
are
defined
outside
these
functions
But
main
itself
does
not
refer
to
the
stack
or
stack
position
the
representation
can
be
hidden
Let
us
now
turn
to
the
implementation
of
getop
the
function
that
fetches
the
next
operator
or
operand
The
task
is
easy
Skip
blanks
and
tabs
If
the
next
character
is
not
a
digit
or
a
hexadecimal
point
return
it
Otherwise
collect
a
string
of
digits
which
might
include
a
decimal
point
and
return
NUMBER
the
signal
that
a
number
has
been
collected
include
ctype
h
int
getch
void
void
ungetch
int
getop
get
next
character
or
numeric
operand
int
getop
char
s
int
i
c
while
s
c
getch
c
t
s
if
isdigit
c
c
return
c
not
a
number
i
if
isdigit
c
collect
integer
part
while
isdigit
s
i
c
getch
if
c
collect
fraction
part
while
isdigit
s
i
c
getch
s
i
if
c
EOF
ungetch
c
return
NUMBER
What
are
getch
and
ungetch
It
is
often
the
case
that
a
program
cannot
determine
that
it
has
read
enough
input
until
it
has
read
too
much
One
instance
is
collecting
characters
that
make
up
a
number
until
the
first
non
digit
is
seen
the
number
is
not
complete
But
then
the
program
has
read
one
character
too
far
a
character
that
it
is
not
prepared
for
The
problem
would
be
solved
if
it
were
possible
to
un
read
the
unwanted
character
Then
every
time
the
program
reads
one
character
too
many
it
could
push
it
back
on
the
input
so
the
rest
of
the
code
could
behave
as
if
it
had
never
been
read
Fortunately
it
s
easy
to
simulate
ungetting
a
character
by
writing
a
pair
of
cooperating
functions
getch
delivers
the
next
input
character
to
be
considered
ungetch
will
return
them
before
reading
new
input
How
they
work
together
is
simple
ungetch
puts
the
pushed
back
characters
into
a
shared
buffer
a
character
array
getch
reads
from
the
buffer
if
there
is
anything
else
and
calls
getchar
if
the
buffer
is
empty
There
must
also
be
an
index
variable
that
records
the
position
of
the
current
character
in
the
buffer
Since
the
buffer
and
the
index
are
shared
by
getch
and
ungetch
and
must
retain
their
values
between
calls
they
must
be
external
to
both
routines
Thus
we
can
write
getch
ungetch
and
their
shared
variables
as
define
BUFSIZE
char
buf
BUFSIZE
buffer
for
ungetch
int
bufp
next
free
position
in
buf
int
getch
void
get
a
possibly
pushed
back
character
return
bufp
buf
bufp
getchar
void
ungetch
int
c
push
character
back
on
input
if
bufp
BUFSIZE
printf
ungetch
too
many
characters
n
else
buf
bufp
c
The
standard
library
includes
a
function
ungetch
that
provides
one
character
of
pushback
we
will
discuss
it
in
Chapter
We
have
used
an
array
for
the
pushback
rather
than
a
single
character
to
illustrate
a
more
general
approach
Exercise
Given
the
basic
framework
it
s
straightforward
to
extend
the
calculator
Add
the
modulus
operator
and
provisions
for
negative
numbers
Exercise
Add
the
commands
to
print
the
top
elements
of
the
stack
without
popping
to
duplicate
it
and
to
swap
the
top
two
elements
Add
a
command
to
clear
the
stack
Exercise
Add
access
to
library
functions
like
sin
exp
and
pow
See
math
h
in
Appendix
B
Section
Exercise
Add
commands
for
handling
variables
It
s
easy
to
provide
twenty
six
variables
with
single
letter
names
Add
a
variable
for
the
most
recently
printed
value
Exercise
Write
a
routine
ungets
s
that
will
push
back
an
entire
string
onto
the
input
Should
ungets
know
about
buf
and
bufp
or
should
it
just
use
ungetch
Exercise
Suppose
that
there
will
never
be
more
than
one
character
of
pushback
Modify
getch
and
ungetch
accordingly
Exercise
Our
getch
and
ungetch
do
not
handle
a
pushed
back
EOF
correctly
Decide
what
their
properties
ought
to
be
if
an
EOF
is
pushed
back
then
implement
your
design
Exercise
An
alternate
organization
uses
getline
to
read
an
entire
input
line
this
makes
getch
and
ungetch
unnecessary
Revise
the
calculator
to
use
this
approach
Scope
Rules
The
functions
and
external
variables
that
make
up
a
C
program
need
not
all
be
compiled
at
the
same
time
the
source
text
of
the
program
may
be
kept
in
several
files
and
previously
compiled
routines
may
be
loaded
from
libraries
Among
the
questions
of
interest
are
How
are
declarations
written
so
that
variables
are
properly
declared
during
compilation
How
are
declarations
arranged
so
that
all
the
pieces
will
be
properly
connected
when
the
program
is
loaded
How
are
declarations
organized
so
there
is
only
one
copy
How
are
external
variables
initialized
Let
us
discuss
these
topics
by
reorganizing
the
calculator
program
into
several
files
As
a
practical
matter
the
calculator
is
too
small
to
be
worth
splitting
but
it
is
a
fine
illustration
of
the
issues
that
arise
in
larger
programs
The
scope
of
a
name
is
the
part
of
the
program
within
which
the
name
can
be
used
For
an
automatic
variable
declared
at
the
beginning
of
a
function
the
scope
is
the
function
in
which
the
name
is
declared
Local
variables
of
the
same
name
in
different
functions
are
unrelated
The
same
is
true
of
the
parameters
of
the
function
which
are
in
effect
local
variables
The
scope
of
an
external
variable
or
a
function
lasts
from
the
point
at
which
it
is
declared
to
the
end
of
the
file
being
compiled
For
example
if
main
sp
val
push
and
pop
are
defined
in
one
file
in
the
order
shown
above
that
is
main
int
sp
double
val
MAXVAL
void
push
double
f
double
pop
void
then
the
variables
sp
and
val
may
be
used
in
push
and
pop
simply
by
naming
them
no
further
declarations
are
needed
But
these
names
are
not
visible
in
main
nor
are
push
and
pop
themselves
On
the
other
hand
if
an
external
variable
is
to
be
referred
to
before
it
is
defined
or
if
it
is
defined
in
a
different
source
file
from
the
one
where
it
is
being
used
then
an
extern
declaration
is
mandatory
It
is
important
to
distinguish
between
the
declaration
of
an
external
variable
and
its
definition
A
declaration
announces
the
properties
of
a
variable
primarily
its
type
a
definition
also
causes
storage
to
be
set
aside
If
the
lines
int
sp
double
val
MAXVAL
appear
outside
of
any
function
they
define
the
external
variables
sp
and
val
cause
storage
to
be
set
aside
and
also
serve
as
the
declarations
for
the
rest
of
that
source
file
On
the
other
hand
the
lines
extern
int
sp
extern
double
val
declare
for
the
rest
of
the
source
file
that
sp
is
an
int
and
that
val
is
a
double
array
whose
size
is
determined
elsewhere
but
they
do
not
create
the
variables
or
reserve
storage
for
them
There
must
be
only
one
definition
of
an
external
variable
among
all
the
files
that
make
up
the
source
program
other
files
may
contain
extern
declarations
to
access
it
There
may
also
be
extern
declarations
in
the
file
containing
the
definition
Array
sizes
must
be
specified
with
the
definition
but
are
optional
with
an
extern
declaration
Initialization
of
an
external
variable
goes
only
with
the
definition
Although
it
is
not
a
likely
organization
for
this
program
the
functions
push
and
pop
could
be
defined
in
one
file
and
the
variables
val
and
sp
defined
and
initialized
in
another
Then
these
definitions
and
declarations
would
be
necessary
to
tie
them
together
in
file
extern
int
sp
extern
double
val
void
push
double
f
double
pop
void
in
file
int
sp
double
val
MAXVAL
Because
the
extern
declarations
in
file
lie
ahead
of
and
outside
the
function
definitions
they
apply
to
all
functions
one
set
of
declarations
suffices
for
all
of
file
This
same
organization
would
also
bee
needed
if
the
definition
of
sp
and
val
followed
their
use
in
one
file
Header
Files
Let
is
now
consider
dividing
the
calculator
program
into
several
source
files
as
it
might
be
is
each
of
the
components
were
substantially
bigger
The
main
function
would
go
in
one
file
which
we
will
call
main
c
push
pop
and
their
variables
go
into
a
second
file
stack
c
getop
goes
into
a
third
getop
c
Finally
getch
and
ungetch
go
into
a
fourth
file
getch
c
we
separate
them
from
the
others
because
they
would
come
from
a
separately
compiled
library
in
a
realistic
program
There
is
one
more
thing
to
worry
about
the
definitions
and
declarations
shared
among
files
As
much
as
possible
we
want
to
centralize
this
so
that
there
is
only
one
copy
to
get
and
keep
right
as
the
program
evolves
Accordingly
we
will
place
this
common
material
in
a
header
file
calc
h
which
will
be
included
as
necessary
The
include
line
is
described
in
Section
The
resulting
program
then
looks
like
this
There
is
a
tradeoff
between
the
desire
that
each
file
have
access
only
to
the
information
it
needs
for
its
job
and
the
practical
reality
that
it
is
harder
to
maintain
more
header
files
Up
to
some
moderate
program
size
it
is
probably
best
to
have
one
header
file
that
contains
everything
that
is
to
be
shared
between
any
two
parts
of
the
program
that
is
the
decision
we
made
here
For
a
much
larger
program
more
organization
and
more
headers
would
be
needed
Static
Variables
The
variables
sp
and
val
in
stack
c
and
buf
and
bufp
in
getch
c
are
for
the
private
use
of
the
functions
in
their
respective
source
files
and
are
not
meant
to
be
accessed
by
anything
else
The
static
declaration
applied
to
an
external
variable
or
function
limits
the
scope
of
that
object
to
the
rest
of
the
source
file
being
compiled
External
static
thus
provides
a
way
to
hide
names
like
buf
and
bufp
in
the
getch
ungetch
combination
which
must
be
external
so
they
can
be
shared
yet
which
should
not
be
visible
to
users
of
getch
and
ungetch
Static
storage
is
specified
by
prefixing
the
normal
declaration
with
the
word
static
If
the
two
routines
and
the
two
variables
are
compiled
in
one
file
as
in
static
char
buf
BUFSIZE
buffer
for
ungetch
static
int
bufp
next
free
position
in
buf
int
getch
void
void
ungetch
int
c
then
no
other
routine
will
be
able
to
access
buf
and
bufp
and
those
names
will
not
conflict
with
the
same
names
in
other
files
of
the
same
program
In
the
same
way
the
variables
that
push
and
pop
use
for
stack
manipulation
can
be
hidden
by
declaring
sp
and
val
to
be
static
The
external
static
declaration
is
most
often
used
for
variables
but
it
can
be
applied
to
functions
as
well
Normally
function
names
are
global
visible
to
any
part
of
the
entire
program
If
a
function
is
declared
static
however
its
name
is
invisible
outside
of
the
file
in
which
it
is
declared
The
static
declaration
can
also
be
applied
to
internal
variables
Internal
static
variables
are
local
to
a
particular
function
just
as
automatic
variables
are
but
unlike
automatics
they
remain
in
existence
rather
than
coming
and
going
each
time
the
function
is
activated
This
means
that
internal
static
variables
provide
private
permanent
storage
within
a
single
function
Exercise
Modify
getop
so
that
it
doesn
t
need
to
use
ungetch
Hint
use
an
internal
static
variable
Register
Variables
A
register
declaration
advises
the
compiler
that
the
variable
in
question
will
be
heavily
used
The
idea
is
that
register
variables
are
to
be
placed
in
machine
registers
which
may
result
in
smaller
and
faster
programs
But
compilers
are
free
to
ignore
the
advice
The
register
declaration
looks
like
register
int
x
register
char
c
and
so
on
The
register
declaration
can
only
be
applied
to
automatic
variables
and
to
the
formal
parameters
of
a
function
In
this
later
case
it
looks
like
f
register
unsigned
m
register
long
n
register
int
i
In
practice
there
are
restrictions
on
register
variables
reflecting
the
realities
of
underlying
hardware
Only
a
few
variables
in
each
function
may
be
kept
in
registers
and
only
certain
types
are
allowed
Excess
register
declarations
are
harmless
however
since
the
word
register
is
ignored
for
excess
or
disallowed
declarations
And
it
is
not
possible
to
take
the
address
of
a
register
variable
a
topic
covered
in
Chapter
regardless
of
whether
the
variable
is
actually
placed
in
a
register
The
specific
restrictions
on
number
and
types
of
register
variables
vary
from
machine
to
machine
Block
Structure
C
is
not
a
block
structured
language
in
the
sense
of
Pascal
or
similar
languages
because
functions
may
not
be
defined
within
other
functions
On
the
other
hand
variables
can
be
defined
in
a
block
structured
fashion
within
a
function
Declarations
of
variables
including
initializations
may
follow
the
left
brace
that
introduces
any
compound
statement
not
just
the
one
that
begins
a
function
Variables
declared
in
this
way
hide
any
identically
named
variables
in
outer
blocks
and
remain
in
existence
until
the
matching
right
brace
For
example
in
if
n
int
i
declare
a
new
i
for
i
i
n
i
the
scope
of
the
variable
i
is
the
true
branch
of
the
if
this
i
is
unrelated
to
any
i
outside
the
block
An
automatic
variable
declared
and
initialized
in
a
block
is
initialized
each
time
the
block
is
entered
Automatic
variables
including
formal
parameters
also
hide
external
variables
and
functions
of
the
same
name
Given
the
declarations
int
x
int
y
f
double
x
double
y
then
within
the
function
f
occurrences
of
x
refer
to
the
parameter
which
is
a
double
outside
f
they
refer
to
the
external
int
The
same
is
true
of
the
variable
y
As
a
matter
of
style
it
s
best
to
avoid
variable
names
that
conceal
names
in
an
outer
scope
the
potential
for
confusion
and
error
is
too
great
Initialization
Initialization
has
been
mentioned
in
passing
many
times
so
far
but
always
peripherally
to
some
other
topic
This
section
summarizes
some
of
the
rules
now
that
we
have
discussed
the
various
storage
classes
In
the
absence
of
explicit
initialization
external
and
static
variables
are
guaranteed
to
be
initialized
to
zero
automatic
and
register
variables
have
undefined
i
e
garbage
initial
values
Scalar
variables
may
be
initialized
when
they
are
defined
by
following
the
name
with
an
equals
sign
and
an
expression
int
x
char
squota
long
day
L
L
L
L
milliseconds
day
For
external
and
static
variables
the
initializer
must
be
a
constant
expression
the
initialization
is
done
once
conceptionally
before
the
program
begins
execution
For
automatic
and
register
variables
the
initializer
is
not
restricted
to
being
a
constant
it
may
be
any
expression
involving
previously
defined
values
even
function
calls
For
example
the
initialization
of
the
binary
search
program
in
Section
could
be
written
as
int
binsearch
int
x
int
v
int
n
int
low
int
high
n
int
mid
instead
of
int
low
high
mid
low
high
n
In
effect
initialization
of
automatic
variables
are
just
shorthand
for
assignment
statements
Which
form
to
prefer
is
largely
a
matter
of
taste
We
have
generally
used
explicit
assignments
because
initializers
in
declarations
are
harder
to
see
and
further
away
from
the
point
of
use
An
array
may
be
initialized
by
following
its
declaration
with
a
list
of
initializers
enclosed
in
braces
and
separated
by
commas
For
example
to
initialize
an
array
days
with
the
number
of
days
in
each
month
int
days
When
the
size
of
the
array
is
omitted
the
compiler
will
compute
the
length
by
counting
the
initializers
of
which
there
are
in
this
case
If
there
are
fewer
initializers
for
an
array
than
the
specified
size
the
others
will
be
zero
for
external
static
and
automatic
variables
It
is
an
error
to
have
too
many
initializers
There
is
no
way
to
specify
repetition
of
an
initializer
nor
to
initialize
an
element
in
the
middle
of
an
array
without
supplying
all
the
preceding
values
as
well
Character
arrays
are
a
special
case
of
initialization
a
string
may
be
used
instead
of
the
braces
and
commas
notation
char
pattern
ould
is
a
shorthand
for
the
longer
but
equivalent
char
pattern
o
u
l
d
In
this
case
the
array
size
is
five
four
characters
plus
the
terminating
Recursion
C
functions
may
be
used
recursively
that
is
a
function
may
call
itself
either
directly
or
indirectly
Consider
printing
a
number
as
a
character
string
As
we
mentioned
before
the
digits
are
generated
in
the
wrong
order
low
order
digits
are
available
before
high
order
digits
but
they
have
to
be
printed
the
other
way
around
There
are
two
solutions
to
this
problem
On
is
to
store
the
digits
in
an
array
as
they
are
generated
then
print
them
in
the
reverse
order
as
we
did
with
itoa
in
section
The
alternative
is
a
recursive
solution
in
which
printd
first
calls
itself
to
cope
with
any
leading
digits
then
prints
the
trailing
digit
Again
this
version
can
fail
on
the
largest
negative
number
include
stdio
h
printd
print
n
in
decimal
void
printd
int
n
if
n
putchar
n
n
if
n
printd
n
putchar
n
When
a
function
calls
itself
recursively
each
invocation
gets
a
fresh
set
of
all
the
automatic
variables
independent
of
the
previous
set
This
in
printd
the
first
printd
receives
the
argument
n
It
passes
to
a
second
printd
which
in
turn
passes
to
a
third
The
third
level
printd
prints
then
returns
to
the
second
level
That
printd
prints
then
returns
to
the
first
level
That
one
prints
and
terminates
Another
good
example
of
recursion
is
quicksort
a
sorting
algorithm
developed
by
C
A
R
Hoare
in
Given
an
array
one
element
is
chosen
and
the
others
partitioned
in
two
subsets
those
less
than
the
partition
element
and
those
greater
than
or
equal
to
it
The
same
process
is
then
applied
recursively
to
the
two
subsets
When
a
subset
has
fewer
than
two
elements
it
doesn
t
need
any
sorting
this
stops
the
recursion
Our
version
of
quicksort
is
not
the
fastest
possible
but
it
s
one
of
the
simplest
We
use
the
middle
element
of
each
subarray
for
partitioning
qsort
sort
v
left
v
right
into
increasing
order
void
qsort
int
v
int
left
int
right
int
i
last
void
swap
int
v
int
i
int
j
if
left
right
do
nothing
if
array
contains
return
fewer
than
two
elements
swap
v
left
left
right
move
partition
elem
last
left
to
v
for
i
left
i
right
i
partition
if
v
i
v
left
swap
v
last
i
swap
v
left
last
restore
partition
elem
qsort
v
left
last
qsort
v
last
right
We
moved
the
swapping
operation
into
a
separate
function
swap
because
it
occurs
three
times
in
qsort
swap
interchange
v
i
and
v
j
void
swap
int
v
int
i
int
j
int
temp
temp
v
i
v
i
v
j
v
j
temp
The
standard
library
includes
a
version
of
qsort
that
can
sort
objects
of
any
type
Recursion
may
provide
no
saving
in
storage
since
somewhere
a
stack
of
the
values
being
processed
must
be
maintained
Nor
will
it
be
faster
But
recursive
code
is
more
compact
and
often
much
easier
to
write
and
understand
than
the
non
recursive
equivalent
Recursion
is
especially
convenient
for
recursively
defined
data
structures
like
trees
we
will
see
a
nice
example
in
Section
Exercise
Adapt
the
ideas
of
printd
to
write
a
recursive
version
of
itoa
that
is
convert
an
integer
into
a
string
by
calling
a
recursive
routine
Exercise
Write
a
recursive
version
of
the
function
reverse
s
which
reverses
the
string
s
in
place
The
C
Preprocessor
C
provides
certain
language
facilities
by
means
of
a
preprocessor
which
is
conceptionally
a
separate
first
step
in
compilation
The
two
most
frequently
used
features
are
include
to
include
the
contents
of
a
file
during
compilation
and
define
to
replace
a
token
by
an
arbitrary
sequence
of
characters
Other
features
described
in
this
section
include
conditional
compilation
and
macros
with
arguments
File
Inclusion
File
inclusion
makes
it
easy
to
handle
collections
of
defines
and
declarations
among
other
things
Any
source
line
of
the
form
include
filename
or
include
filename
is
replaced
by
the
contents
of
the
file
filename
If
the
filename
is
quoted
searching
for
the
file
typically
begins
where
the
source
program
was
found
if
it
is
not
found
there
or
if
the
name
is
enclosed
in
and
searching
follows
an
implementation
defined
rule
to
find
the
file
An
included
file
may
itself
contain
include
lines
There
are
often
several
include
lines
at
the
beginning
of
a
source
file
to
include
common
define
statements
and
extern
declarations
or
to
access
the
function
prototype
declarations
for
library
functions
from
headers
like
stdio
h
Strictly
speaking
these
need
not
be
files
the
details
of
how
headers
are
accessed
are
implementation
dependent
include
is
the
preferred
way
to
tie
the
declarations
together
for
a
large
program
It
guarantees
that
all
the
source
files
will
be
supplied
with
the
same
definitions
and
variable
declarations
and
thus
eliminates
a
particularly
nasty
kind
of
bug
Naturally
when
an
included
file
is
changed
all
files
that
depend
on
it
must
be
recompiled
Macro
Substitution
A
definition
has
the
form
define
name
replacement
text
It
calls
for
a
macro
substitution
of
the
simplest
kind
subsequent
occurrences
of
the
token
name
will
be
replaced
by
the
replacement
text
The
name
in
a
define
has
the
same
form
as
a
variable
name
the
replacement
text
is
arbitrary
Normally
the
replacement
text
is
the
rest
of
the
line
but
a
long
definition
may
be
continued
onto
several
lines
by
placing
a
at
the
end
of
each
line
to
be
continued
The
scope
of
a
name
defined
with
define
is
from
its
point
of
definition
to
the
end
of
the
source
file
being
compiled
A
definition
may
use
previous
definitions
Substitutions
are
made
only
for
tokens
and
do
not
take
place
within
quoted
strings
For
example
if
YES
is
a
defined
name
there
would
be
no
substitution
in
printf
YES
or
in
YESMAN
Any
name
may
be
defined
with
any
replacement
text
For
example
define
forever
for
infinite
loop
defines
a
new
word
forever
for
an
infinite
loop
It
is
also
possible
to
define
macros
with
arguments
so
the
replacement
text
can
be
different
for
different
calls
of
the
macro
As
an
example
define
a
macro
called
max
define
max
A
B
A
B
A
B
Although
it
looks
like
a
function
call
a
use
of
max
expands
into
in
line
code
Each
occurrence
of
a
formal
parameter
here
A
or
B
will
be
replaced
by
the
corresponding
actual
argument
Thus
the
line
x
max
p
q
r
s
will
be
replaced
by
the
line
x
p
q
r
s
p
q
r
s
So
long
as
the
arguments
are
treated
consistently
this
macro
will
serve
for
any
data
type
there
is
no
need
for
different
kinds
of
max
for
different
data
types
as
there
would
be
with
functions
If
you
examine
the
expansion
of
max
you
will
notice
some
pitfalls
The
expressions
are
evaluated
twice
this
is
bad
if
they
involve
side
effects
like
increment
operators
or
input
and
output
For
instance
max
i
j
WRONG
will
increment
the
larger
twice
Some
care
also
has
to
be
taken
with
parentheses
to
make
sure
the
order
of
evaluation
is
preserved
consider
what
happens
when
the
macro
define
square
x
x
x
WRONG
is
invoked
as
square
z
Nonetheless
macros
are
valuable
One
practical
example
comes
from
stdio
h
in
which
getchar
and
putchar
are
often
defined
as
macros
to
avoid
the
run
time
overhead
of
a
function
call
per
character
processed
The
functions
in
ctype
h
are
also
usually
implemented
as
macros
Names
may
be
undefined
with
undef
usually
to
ensure
that
a
routine
is
really
a
function
not
a
macro
undef
getchar
int
getchar
void
Formal
parameters
are
not
replaced
within
quoted
strings
If
however
a
parameter
name
is
preceded
by
a
in
the
replacement
text
the
combination
will
be
expanded
into
a
quoted
string
with
the
parameter
replaced
by
the
actual
argument
This
can
be
combined
with
string
concatenation
to
make
for
example
a
debugging
print
macro
define
dprint
expr
printf
expr
g
n
expr
When
this
is
invoked
as
in
dprint
x
y
the
macro
is
expanded
into
printf
x
y
g
n
x
y
and
the
strings
are
concatenated
so
the
effect
is
printf
x
y
g
n
x
y
Within
the
actual
argument
each
is
replaced
by
and
each
by
so
the
result
is
a
legal
string
constant
The
preprocessor
operator
provides
a
way
to
concatenate
actual
arguments
during
macro
expansion
If
a
parameter
in
the
replacement
text
is
adjacent
to
a
the
parameter
is
replaced
by
the
actual
argument
the
and
surrounding
white
space
are
removed
and
the
result
is
rescanned
For
example
the
macro
paste
concatenates
its
two
arguments
define
paste
front
back
front
back
so
paste
name
creates
the
token
name
The
rules
for
nested
uses
of
are
arcane
further
details
may
be
found
in
Appendix
A
Exercise
Define
a
macro
swap
t
x
y
that
interchanges
two
arguments
of
type
t
Block
structure
will
help
Conditional
Inclusion
It
is
possible
to
control
preprocessing
itself
with
conditional
statements
that
are
evaluated
during
preprocessing
This
provides
a
way
to
include
code
selectively
depending
on
the
value
of
conditions
evaluated
during
compilation
The
if
line
evaluates
a
constant
integer
expression
which
may
not
include
sizeof
casts
or
enum
constants
If
the
expression
is
non
zero
subsequent
lines
until
an
endif
or
elif
or
else
are
included
The
preprocessor
statement
elif
is
like
else
if
The
expression
defined
name
in
a
if
is
if
the
name
has
been
defined
and
otherwise
For
example
to
make
sure
that
the
contents
of
a
file
hdr
h
are
included
only
once
the
contents
of
the
file
are
surrounded
with
a
conditional
like
this
if
defined
HDR
define
HDR
contents
of
hdr
h
go
here
endif
The
first
inclusion
of
hdr
h
defines
the
name
HDR
subsequent
inclusions
will
find
the
name
defined
and
skip
down
to
the
endif
A
similar
style
can
be
used
to
avoid
including
files
multiple
times
If
this
style
is
used
consistently
then
each
header
can
itself
include
any
other
headers
on
which
it
depends
without
the
user
of
the
header
having
to
deal
with
the
interdependence
This
sequence
tests
the
name
SYSTEM
to
decide
which
version
of
a
header
to
include
if
SYSTEM
SYSV
define
HDR
sysv
h
elif
SYSTEM
BSD
define
HDR
bsd
h
elif
SYSTEM
MSDOS
define
HDR
msdos
h
else
define
HDR
default
h
endif
include
HDR
The
ifdef
and
ifndef
lines
are
specialized
forms
that
test
whether
a
name
is
defined
The
first
example
of
if
above
could
have
been
written
ifndef
HDR
define
HDR
contents
of
hdr
h
go
here
endif
Chapter
Pointers
and
Arrays
A
pointer
is
a
variable
that
contains
the
address
of
a
variable
Pointers
are
much
used
in
C
partly
because
they
are
sometimes
the
only
way
to
express
a
computation
and
partly
because
they
usually
lead
to
more
compact
and
efficient
code
than
can
be
obtained
in
other
ways
Pointers
and
arrays
are
closely
related
this
chapter
also
explores
this
relationship
and
shows
how
to
exploit
it
Pointers
have
been
lumped
with
the
goto
statement
as
a
marvelous
way
to
create
impossibleto
understand
programs
This
is
certainly
true
when
they
are
used
carelessly
and
it
is
easy
to
create
pointers
that
point
somewhere
unexpected
With
discipline
however
pointers
can
also
be
used
to
achieve
clarity
and
simplicity
This
is
the
aspect
that
we
will
try
to
illustrate
The
main
change
in
ANSI
C
is
to
make
explicit
the
rules
about
how
pointers
can
be
manipulated
in
effect
mandating
what
good
programmers
already
practice
and
good
compilers
already
enforce
In
addition
the
type
void
pointer
to
void
replaces
char
as
the
proper
type
for
a
generic
pointer
Pointers
and
Addresses
Let
us
begin
with
a
simplified
picture
of
how
memory
is
organized
A
typical
machine
has
an
array
of
consecutively
numbered
or
addressed
memory
cells
that
may
be
manipulated
individually
or
in
contiguous
groups
One
common
situation
is
that
any
byte
can
be
a
char
a
pair
of
one
byte
cells
can
be
treated
as
a
short
integer
and
four
adjacent
bytes
form
a
long
A
pointer
is
a
group
of
cells
often
two
or
four
that
can
hold
an
address
So
if
c
is
a
char
and
p
is
a
pointer
that
points
to
it
we
could
represent
the
situation
this
way
The
unary
operator
gives
the
address
of
an
object
so
the
statement
p
c
assigns
the
address
of
c
to
the
variable
p
and
p
is
said
to
point
to
c
The
operator
only
applies
to
objects
in
memory
variables
and
array
elements
It
cannot
be
applied
to
expressions
constants
or
register
variables
The
unary
operator
is
the
indirection
or
dereferencing
operator
when
applied
to
a
pointer
it
accesses
the
object
the
pointer
points
to
Suppose
that
x
and
y
are
integers
and
ip
is
a
pointer
to
int
This
artificial
sequence
shows
how
to
declare
a
pointer
and
how
to
use
and
int
x
y
z
int
ip
ip
is
a
pointer
to
int
ip
x
ip
now
points
to
x
y
ip
y
is
now
ip
x
is
now
ip
z
ip
now
points
to
z
The
declaration
of
x
y
and
z
are
what
we
ve
seen
all
along
The
declaration
of
the
pointer
ip
int
ip
is
intended
as
a
mnemonic
it
says
that
the
expression
ip
is
an
int
The
syntax
of
the
declaration
for
a
variable
mimics
the
syntax
of
expressions
in
which
the
variable
might
appear
This
reasoning
applies
to
function
declarations
as
well
For
example
double
dp
atof
char
says
that
in
an
expression
dp
and
atof
s
have
values
of
double
and
that
the
argument
of
atof
is
a
pointer
to
char
You
should
also
note
the
implication
that
a
pointer
is
constrained
to
point
to
a
particular
kind
of
object
every
pointer
points
to
a
specific
data
type
There
is
one
exception
a
pointer
to
void
is
used
to
hold
any
type
of
pointer
but
cannot
be
dereferenced
itself
We
ll
come
back
to
it
in
Section
If
ip
points
to
the
integer
x
then
ip
can
occur
in
any
context
where
x
could
so
ip
ip
increments
ip
by
The
unary
operators
and
bind
more
tightly
than
arithmetic
operators
so
the
assignment
y
ip
takes
whatever
ip
points
at
adds
and
assigns
the
result
to
y
while
ip
increments
what
ip
points
to
as
do
ip
and
ip
The
parentheses
are
necessary
in
this
last
example
without
them
the
expression
would
increment
ip
instead
of
what
it
points
to
because
unary
operators
like
and
associate
right
to
left
Finally
since
pointers
are
variables
they
can
be
used
without
dereferencing
For
example
if
iq
is
another
pointer
to
int
iq
ip
copies
the
contents
of
ip
into
iq
thus
making
iq
point
to
whatever
ip
pointed
to
Pointers
and
Function
Arguments
Since
C
passes
arguments
to
functions
by
value
there
is
no
direct
way
for
the
called
function
to
alter
a
variable
in
the
calling
function
For
instance
a
sorting
routine
might
exchange
two
out
of
order
arguments
with
a
function
called
swap
It
is
not
enough
to
write
swap
a
b
where
the
swap
function
is
defined
as
void
swap
int
x
int
y
WRONG
int
temp
temp
x
x
y
y
temp
Because
of
call
by
value
swap
can
t
affect
the
arguments
a
and
b
in
the
routine
that
called
it
The
function
above
swaps
copies
of
a
and
b
The
way
to
obtain
the
desired
effect
is
for
the
calling
program
to
pass
pointers
to
the
values
to
be
changed
swap
a
b
Since
the
operator
produces
the
address
of
a
variable
a
is
a
pointer
to
a
In
swap
itself
the
parameters
are
declared
as
pointers
and
the
operands
are
accessed
indirectly
through
them
void
swap
int
px
int
py
interchange
px
and
py
int
temp
temp
px
px
py
py
temp
Pictorially
Pointer
arguments
enable
a
function
to
access
and
change
objects
in
the
function
that
called
it
As
an
example
consider
a
function
getint
that
performs
free
format
input
conversion
by
breaking
a
stream
of
characters
into
integer
values
one
integer
per
call
getint
has
to
return
the
value
it
found
and
also
signal
end
of
file
when
there
is
no
more
input
These
values
have
to
be
passed
back
by
separate
paths
for
no
matter
what
value
is
used
for
EOF
that
could
also
be
the
value
of
an
input
integer
One
solution
is
to
have
getint
return
the
end
of
file
status
as
its
function
value
while
using
a
pointer
argument
to
store
the
converted
integer
back
in
the
calling
function
This
is
the
scheme
used
by
scanf
as
well
see
Section
The
following
loop
fills
an
array
with
integers
by
calls
to
getint
int
n
array
SIZE
getint
int
for
n
n
SIZE
getint
array
n
EOF
n
Each
call
sets
array
n
to
the
next
integer
found
in
the
input
and
increments
n
Notice
that
it
is
essential
to
pass
the
address
of
array
n
to
getint
Otherwise
there
is
no
way
for
getint
to
communicate
the
converted
integer
back
to
the
caller
Our
version
of
getint
returns
EOF
for
end
of
file
zero
if
the
next
input
is
not
a
number
and
a
positive
value
if
the
input
contains
a
valid
number
include
ctype
h
int
getch
void
void
ungetch
int
getint
get
next
integer
from
input
into
pn
int
getint
int
pn
int
c
sign
while
isspace
c
getch
skip
white
space
if
isdigit
c
c
EOF
c
c
ungetch
c
it
is
not
a
number
return
sign
c
if
c
c
c
getch
for
pn
isdigit
c
c
getch
pn
pn
c
pn
sign
if
c
EOF
ungetch
c
return
c
Throughout
getint
pn
is
used
as
an
ordinary
int
variable
We
have
also
used
getch
and
ungetch
described
in
Section
so
the
one
extra
character
that
must
be
read
can
be
pushed
back
onto
the
input
Exercise
As
written
getint
treats
a
or
not
followed
by
a
digit
as
a
valid
representation
of
zero
Fix
it
to
push
such
a
character
back
on
the
input
Exercise
Write
getfloat
the
floating
point
analog
of
getint
What
type
does
getfloat
return
as
its
function
value
Pointers
and
Arrays
In
C
there
is
a
strong
relationship
between
pointers
and
arrays
strong
enough
that
pointers
and
arrays
should
be
discussed
simultaneously
Any
operation
that
can
be
achieved
by
array
subscripting
can
also
be
done
with
pointers
The
pointer
version
will
in
general
be
faster
but
at
least
to
the
uninitiated
somewhat
harder
to
understand
The
declaration
int
a
defines
an
array
of
size
that
is
a
block
of
consecutive
objects
named
a
a
a
The
notation
a
i
refers
to
the
i
th
element
of
the
array
If
pa
is
a
pointer
to
an
integer
declared
as
int
pa
then
the
assignment
pa
a
sets
pa
to
point
to
element
zero
of
a
that
is
pa
contains
the
address
of
a
Now
the
assignment
x
pa
will
copy
the
contents
of
a
into
x
If
pa
points
to
a
particular
element
of
an
array
then
by
definition
pa
points
to
the
next
element
pa
i
points
i
elements
after
pa
and
pa
i
points
i
elements
before
Thus
if
pa
points
to
a
pa
refers
to
the
contents
of
a
pa
i
is
the
address
of
a
i
and
pa
i
is
the
contents
of
a
i
These
remarks
are
true
regardless
of
the
type
or
size
of
the
variables
in
the
array
a
The
meaning
of
adding
to
a
pointer
and
by
extension
all
pointer
arithmetic
is
that
pa
points
to
the
next
object
and
pa
i
points
to
the
i
th
object
beyond
pa
The
correspondence
between
indexing
and
pointer
arithmetic
is
very
close
By
definition
the
value
of
a
variable
or
expression
of
type
array
is
the
address
of
element
zero
of
the
array
Thus
after
the
assignment
pa
a
pa
and
a
have
identical
values
Since
the
name
of
an
array
is
a
synonym
for
the
location
of
the
initial
element
the
assignment
pa
a
can
also
be
written
as
pa
a
Rather
more
surprising
at
first
sight
is
the
fact
that
a
reference
to
a
i
can
also
be
written
as
a
i
In
evaluating
a
i
C
converts
it
to
a
i
immediately
the
two
forms
are
equivalent
Applying
the
operator
to
both
parts
of
this
equivalence
it
follows
that
a
i
and
a
i
are
also
identical
a
i
is
the
address
of
the
i
th
element
beyond
a
As
the
other
side
of
this
coin
if
pa
is
a
pointer
expressions
might
use
it
with
a
subscript
pa
i
is
identical
to
pa
i
In
short
an
array
and
index
expression
is
equivalent
to
one
written
as
a
pointer
and
offset
There
is
one
difference
between
an
array
name
and
a
pointer
that
must
be
kept
in
mind
A
pointer
is
a
variable
so
pa
a
and
pa
are
legal
But
an
array
name
is
not
a
variable
constructions
like
a
pa
and
a
are
illegal
When
an
array
name
is
passed
to
a
function
what
is
passed
is
the
location
of
the
initial
element
Within
the
called
function
this
argument
is
a
local
variable
and
so
an
array
name
parameter
is
a
pointer
that
is
a
variable
containing
an
address
We
can
use
this
fact
to
write
another
version
of
strlen
which
computes
the
length
of
a
string
strlen
return
length
of
string
s
int
strlen
char
s
int
n
for
n
s
s
n
return
n
Since
s
is
a
pointer
incrementing
it
is
perfectly
legal
s
has
no
effect
on
the
character
string
in
the
function
that
called
strlen
but
merely
increments
strlen
s
private
copy
of
the
pointer
That
means
that
calls
like
strlen
hello
world
string
constant
strlen
array
char
array
strlen
ptr
char
ptr
all
work
As
formal
parameters
in
a
function
definition
char
s
and
char
s
are
equivalent
we
prefer
the
latter
because
it
says
more
explicitly
that
the
variable
is
a
pointer
When
an
array
name
is
passed
to
a
function
the
function
can
at
its
convenience
believe
that
it
has
been
handed
either
an
array
or
a
pointer
and
manipulate
it
accordingly
It
can
even
use
both
notations
if
it
seems
appropriate
and
clear
It
is
possible
to
pass
part
of
an
array
to
a
function
by
passing
a
pointer
to
the
beginning
of
the
subarray
For
example
if
a
is
an
array
f
a
and
f
a
both
pass
to
the
function
f
the
address
of
the
subarray
that
starts
at
a
Within
f
the
parameter
declaration
can
read
f
int
arr
or
f
int
arr
So
as
far
as
f
is
concerned
the
fact
that
the
parameter
refers
to
part
of
a
larger
array
is
of
no
consequence
If
one
is
sure
that
the
elements
exist
it
is
also
possible
to
index
backwards
in
an
array
p
p
and
so
on
are
syntactically
legal
and
refer
to
the
elements
that
immediately
precede
p
Of
course
it
is
illegal
to
refer
to
objects
that
are
not
within
the
array
bounds
Address
Arithmetic
If
p
is
a
pointer
to
some
element
of
an
array
then
p
increments
p
to
point
to
the
next
element
and
p
i
increments
it
to
point
i
elements
beyond
where
it
currently
does
These
and
similar
constructions
are
the
simples
forms
of
pointer
or
address
arithmetic
C
is
consistent
and
regular
in
its
approach
to
address
arithmetic
its
integration
of
pointers
arrays
and
address
arithmetic
is
one
of
the
strengths
of
the
language
Let
us
illustrate
by
writing
a
rudimentary
storage
allocator
There
are
two
routines
The
first
alloc
n
returns
a
pointer
to
n
consecutive
character
positions
which
can
be
used
by
the
caller
of
alloc
for
storing
characters
The
second
afree
p
releases
the
storage
thus
acquired
so
it
can
be
reused
later
The
routines
are
rudimentary
because
the
calls
to
afree
must
be
made
in
the
opposite
order
to
the
calls
made
on
alloc
That
is
the
storage
managed
by
alloc
and
afree
is
a
stack
or
last
in
first
out
The
standard
library
provides
analogous
functions
called
malloc
and
free
that
have
no
such
restrictions
in
Section
we
will
show
how
they
can
be
implemented
The
easiest
implementation
is
to
have
alloc
hand
out
pieces
of
a
large
character
array
that
we
will
call
allocbuf
This
array
is
private
to
alloc
and
afree
Since
they
deal
in
pointers
not
array
indices
no
other
routine
need
know
the
name
of
the
array
which
can
be
declared
static
in
the
source
file
containing
alloc
and
afree
and
thus
be
invisible
outside
it
In
practical
implementations
the
array
may
well
not
even
have
a
name
it
might
instead
be
obtained
by
calling
malloc
or
by
asking
the
operating
system
for
a
pointer
to
some
unnamed
block
of
storage
The
other
information
needed
is
how
much
of
allocbuf
has
been
used
We
use
a
pointer
called
allocp
that
points
to
the
next
free
element
When
alloc
is
asked
for
n
characters
it
checks
to
see
if
there
is
enough
room
left
in
allocbuf
If
so
alloc
returns
the
current
value
of
allocp
i
e
the
beginning
of
the
free
block
then
increments
it
by
n
to
point
to
the
next
free
area
If
there
is
no
room
alloc
returns
zero
afree
p
merely
sets
allocp
to
p
if
p
is
inside
allocbuf
define
ALLOCSIZE
size
of
available
space
static
char
allocbuf
ALLOCSIZE
storage
for
alloc
static
char
allocp
allocbuf
next
free
position
char
alloc
int
n
return
pointer
to
n
characters
if
allocbuf
ALLOCSIZE
allocp
n
it
fits
allocp
n
return
allocp
n
old
p
else
not
enough
room
return
void
afree
char
p
free
storage
pointed
to
by
p
if
p
allocbuf
p
allocbuf
ALLOCSIZE
allocp
p
In
general
a
pointer
can
be
initialized
just
as
any
other
variable
can
though
normally
the
only
meaningful
values
are
zero
or
an
expression
involving
the
address
of
previously
defined
data
of
appropriate
type
The
declaration
static
char
allocp
allocbuf
defines
allocp
to
be
a
character
pointer
and
initializes
it
to
point
to
the
beginning
of
allocbuf
which
is
the
next
free
position
when
the
program
starts
This
could
also
have
been
written
static
char
allocp
allocbuf
since
the
array
name
is
the
address
of
the
zeroth
element
The
test
if
allocbuf
ALLOCSIZE
allocp
n
it
fits
checks
if
there
s
enough
room
to
satisfy
a
request
for
n
characters
If
there
is
the
new
value
of
allocp
would
be
at
most
one
beyond
the
end
of
allocbuf
If
the
request
can
be
satisfied
alloc
returns
a
pointer
to
the
beginning
of
a
block
of
characters
notice
the
declaration
of
the
function
itself
If
not
alloc
must
return
some
signal
that
there
is
no
space
left
C
guarantees
that
zero
is
never
a
valid
address
for
data
so
a
return
value
of
zero
can
be
used
to
signal
an
abnormal
event
in
this
case
no
space
Pointers
and
integers
are
not
interchangeable
Zero
is
the
sole
exception
the
constant
zero
may
be
assigned
to
a
pointer
and
a
pointer
may
be
compared
with
the
constant
zero
The
symbolic
constant
NULL
is
often
used
in
place
of
zero
as
a
mnemonic
to
indicate
more
clearly
that
this
is
a
special
value
for
a
pointer
NULL
is
defined
in
stdio
h
We
will
use
NULL
henceforth
Tests
like
if
allocbuf
ALLOCSIZE
allocp
n
it
fits
and
if
p
allocbuf
p
allocbuf
ALLOCSIZE
show
several
important
facets
of
pointer
arithmetic
First
pointers
may
be
compared
under
certain
circumstances
If
p
and
q
point
to
members
of
the
same
array
then
relations
like
etc
work
properly
For
example
p
q
is
true
if
p
points
to
an
earlier
element
of
the
array
than
q
does
Any
pointer
can
be
meaningfully
compared
for
equality
or
inequality
with
zero
But
the
behavior
is
undefined
for
arithmetic
or
comparisons
with
pointers
that
do
not
point
to
members
of
the
same
array
There
is
one
exception
the
address
of
the
first
element
past
the
end
of
an
array
can
be
used
in
pointer
arithmetic
Second
we
have
already
observed
that
a
pointer
and
an
integer
may
be
added
or
subtracted
The
construction
p
n
means
the
address
of
the
n
th
object
beyond
the
one
p
currently
points
to
This
is
true
regardless
of
the
kind
of
object
p
points
to
n
is
scaled
according
to
the
size
of
the
objects
p
points
to
which
is
determined
by
the
declaration
of
p
If
an
int
is
four
bytes
for
example
the
int
will
be
scaled
by
four
Pointer
subtraction
is
also
valid
if
p
and
q
point
to
elements
of
the
same
array
and
p
q
then
q
p
is
the
number
of
elements
from
p
to
q
inclusive
This
fact
can
be
used
to
write
yet
another
version
of
strlen
strlen
return
length
of
string
s
int
strlen
char
s
char
p
s
while
p
p
return
p
s
In
its
declaration
p
is
initialized
to
s
that
is
to
point
to
the
first
character
of
the
string
In
the
while
loop
each
character
in
turn
is
examined
until
the
at
the
end
is
seen
Because
p
points
to
characters
p
advances
p
to
the
next
character
each
time
and
p
s
gives
the
number
of
characters
advanced
over
that
is
the
string
length
The
number
of
characters
in
the
string
could
be
too
large
to
store
in
an
int
The
header
stddef
h
defines
a
type
ptrdiff
t
that
is
large
enough
to
hold
the
signed
difference
of
two
pointer
values
If
we
were
being
cautious
however
we
would
use
size
t
for
the
return
value
of
strlen
to
match
the
standard
library
version
size
t
is
the
unsigned
integer
type
returned
by
the
sizeof
operator
Pointer
arithmetic
is
consistent
if
we
had
been
dealing
with
floats
which
occupy
more
storage
that
chars
and
if
p
were
a
pointer
to
float
p
would
advance
to
the
next
float
Thus
we
could
write
another
version
of
alloc
that
maintains
floats
instead
of
chars
merely
by
changing
char
to
float
throughout
alloc
and
afree
All
the
pointer
manipulations
automatically
take
into
account
the
size
of
the
objects
pointed
to
The
valid
pointer
operations
are
assignment
of
pointers
of
the
same
type
adding
or
subtracting
a
pointer
and
an
integer
subtracting
or
comparing
two
pointers
to
members
of
the
same
array
and
assigning
or
comparing
to
zero
All
other
pointer
arithmetic
is
illegal
It
is
not
legal
to
add
two
pointers
or
to
multiply
or
divide
or
shift
or
mask
them
or
to
add
float
or
double
to
them
or
even
except
for
void
to
assign
a
pointer
of
one
type
to
a
pointer
of
another
type
without
a
cast
Character
Pointers
and
Functions
A
string
constant
written
as
I
am
a
string
is
an
array
of
characters
In
the
internal
representation
the
array
is
terminated
with
the
null
character
so
that
programs
can
find
the
end
The
length
in
storage
is
thus
one
more
than
the
number
of
characters
between
the
double
quotes
Perhaps
the
most
common
occurrence
of
string
constants
is
as
arguments
to
functions
as
in
printf
hello
world
n
When
a
character
string
like
this
appears
in
a
program
access
to
it
is
through
a
character
pointer
printf
receives
a
pointer
to
the
beginning
of
the
character
array
That
is
a
string
constant
is
accessed
by
a
pointer
to
its
first
element
String
constants
need
not
be
function
arguments
If
pmessage
is
declared
as
char
pmessage
then
the
statement
pmessage
now
is
the
time
assigns
to
pmessage
a
pointer
to
the
character
array
This
is
not
a
string
copy
only
pointers
are
involved
C
does
not
provide
any
operators
for
processing
an
entire
string
of
characters
as
a
unit
There
is
an
important
difference
between
these
definitions
char
amessage
now
is
the
time
an
array
char
pmessage
now
is
the
time
a
pointer
amessage
is
an
array
just
big
enough
to
hold
the
sequence
of
characters
and
that
initializes
it
Individual
characters
within
the
array
may
be
changed
but
amessage
will
always
refer
to
the
same
storage
On
the
other
hand
pmessage
is
a
pointer
initialized
to
point
to
a
string
constant
the
pointer
may
subsequently
be
modified
to
point
elsewhere
but
the
result
is
undefined
if
you
try
to
modify
the
string
contents
We
will
illustrate
more
aspects
of
pointers
and
arrays
by
studying
versions
of
two
useful
functions
adapted
from
the
standard
library
The
first
function
is
strcpy
s
t
which
copies
the
string
t
to
the
string
s
It
would
be
nice
just
to
say
s
t
but
this
copies
the
pointer
not
the
characters
To
copy
the
characters
we
need
a
loop
The
array
version
first
strcpy
copy
t
to
s
array
subscript
version
void
strcpy
char
s
char
t
int
i
i
while
s
i
t
i
i
For
contrast
here
is
a
version
of
strcpy
with
pointers
strcpy
copy
t
to
s
pointer
version
void
strcpy
char
s
char
t
int
i
i
while
s
t
s
t
Because
arguments
are
passed
by
value
strcpy
can
use
the
parameters
s
and
t
in
any
way
it
pleases
Here
they
are
conveniently
initialized
pointers
which
are
marched
along
the
arrays
a
character
at
a
time
until
the
that
terminates
t
has
been
copied
into
s
In
practice
strcpy
would
not
be
written
as
we
showed
it
above
Experienced
C
programmers
would
prefer
strcpy
copy
t
to
s
pointer
version
void
strcpy
char
s
char
t
while
s
t
This
moves
the
increment
of
s
and
t
into
the
test
part
of
the
loop
The
value
of
t
is
the
character
that
t
pointed
to
before
t
was
incremented
the
postfix
doesn
t
change
t
until
after
this
character
has
been
fetched
In
the
same
way
the
character
is
stored
into
the
old
s
position
before
s
is
incremented
This
character
is
also
the
value
that
is
compared
against
to
control
the
loop
The
net
effect
is
that
characters
are
copied
from
t
to
s
up
and
including
the
terminating
As
the
final
abbreviation
observe
that
a
comparison
against
is
redundant
since
the
question
is
merely
whether
the
expression
is
zero
So
the
function
would
likely
be
written
as
strcpy
copy
t
to
s
pointer
version
void
strcpy
char
s
char
t
while
s
t
Although
this
may
seem
cryptic
at
first
sight
the
notational
convenience
is
considerable
and
the
idiom
should
be
mastered
because
you
will
see
it
frequently
in
C
programs
The
strcpy
in
the
standard
library
string
h
returns
the
target
string
as
its
function
value
The
second
routine
that
we
will
examine
is
strcmp
s
t
which
compares
the
character
strings
s
and
t
and
returns
negative
zero
or
positive
if
s
is
lexicographically
less
than
equal
to
or
greater
than
t
The
value
is
obtained
by
subtracting
the
characters
at
the
first
position
where
s
and
t
disagree
strcmp
return
if
s
t
if
s
t
if
s
t
int
strcmp
char
s
char
t
int
i
for
i
s
i
t
i
i
if
s
i
return
return
s
i
t
i
The
pointer
version
of
strcmp
strcmp
return
if
s
t
if
s
t
if
s
t
int
strcmp
char
s
char
t
for
s
t
s
t
if
s
return
return
s
t
Since
and
are
either
prefix
or
postfix
operators
other
combinations
of
and
and
occur
although
less
frequently
For
example
p
decrements
p
before
fetching
the
character
that
p
points
to
In
fact
the
pair
of
expressions
p
val
push
val
onto
stack
val
p
pop
top
of
stack
into
val
are
the
standard
idiom
for
pushing
and
popping
a
stack
see
Section
The
header
string
h
contains
declarations
for
the
functions
mentioned
in
this
section
plus
a
variety
of
other
string
handling
functions
from
the
standard
library
Exercise
Write
a
pointer
version
of
the
function
strcat
that
we
showed
in
Chapter
strcat
s
t
copies
the
string
t
to
the
end
of
s
Exercise
Write
the
function
strend
s
t
which
returns
if
the
string
t
occurs
at
the
end
of
the
string
s
and
zero
otherwise
Exercise
Write
versions
of
the
library
functions
strncpy
strncat
and
strncmp
which
operate
on
at
most
the
first
n
characters
of
their
argument
strings
For
example
strncpy
s
t
n
copies
at
most
n
characters
of
t
to
s
Full
descriptions
are
in
Appendix
B
Exercise
Rewrite
appropriate
programs
from
earlier
chapters
and
exercises
with
pointers
instead
of
array
indexing
Good
possibilities
include
getline
Chapters
and
atoi
itoa
and
their
variants
Chapters
and
reverse
Chapter
and
strindex
and
getop
Chapter
Pointer
Arrays
Pointers
to
Pointers
Since
pointers
are
variables
themselves
they
can
be
stored
in
arrays
just
as
other
variables
can
Let
us
illustrate
by
writing
a
program
that
will
sort
a
set
of
text
lines
into
alphabetic
order
a
stripped
down
version
of
the
UNIX
program
sort
In
Chapter
we
presented
a
Shell
sort
function
that
would
sort
an
array
of
integers
and
in
Chapter
we
improved
on
it
with
a
quicksort
The
same
algorithms
will
work
except
that
now
we
have
to
deal
with
lines
of
text
which
are
of
different
lengths
and
which
unlike
integers
can
t
be
compared
or
moved
in
a
single
operation
We
need
a
data
representation
that
will
cope
efficiently
and
conveniently
with
variable
length
text
lines
This
is
where
the
array
of
pointers
enters
If
the
lines
to
be
sorted
are
stored
end
to
end
in
one
long
character
array
then
each
line
can
be
accessed
by
a
pointer
to
its
first
character
The
pointers
themselves
can
bee
stored
in
an
array
Two
lines
can
be
compared
by
passing
their
pointers
to
strcmp
When
two
out
of
order
lines
have
to
be
exchanged
the
pointers
in
the
pointer
array
are
exchanged
not
the
text
lines
themselves
This
eliminates
the
twin
problems
of
complicated
storage
management
and
high
overhead
that
would
go
with
moving
the
lines
themselves
The
sorting
process
has
three
steps
read
all
the
lines
of
input
sort
them
print
them
in
order
As
usual
it
s
best
to
divide
the
program
into
functions
that
match
this
natural
division
with
the
main
routine
controlling
the
other
functions
Let
us
defer
the
sorting
step
for
a
moment
and
concentrate
on
the
data
structure
and
the
input
and
output
The
input
routine
has
to
collect
and
save
the
characters
of
each
line
and
build
an
array
of
pointers
to
the
lines
It
will
also
have
to
count
the
number
of
input
lines
since
that
information
is
needed
for
sorting
and
printing
Since
the
input
function
can
only
cope
with
a
finite
number
of
input
lines
it
can
return
some
illegal
count
like
if
too
much
input
is
presented
The
output
routine
only
has
to
print
the
lines
in
the
order
in
which
they
appear
in
the
array
of
pointers
include
stdio
h
include
string
h
define
MAXLINES
max
lines
to
be
sorted
char
lineptr
MAXLINES
pointers
to
text
lines
int
readlines
char
lineptr
int
nlines
void
writelines
char
lineptr
int
nlines
void
qsort
char
lineptr
int
left
int
right
sort
input
lines
main
int
nlines
number
of
input
lines
read
if
nlines
readlines
lineptr
MAXLINES
qsort
lineptr
nlines
writelines
lineptr
nlines
return
else
printf
error
input
too
big
to
sort
n
return
define
MAXLEN
max
length
of
any
input
line
int
getline
char
int
char
alloc
int
readlines
read
input
lines
int
readlines
char
lineptr
int
maxlines
int
len
nlines
char
p
line
MAXLEN
nlines
while
len
getline
line
MAXLEN
if
nlines
maxlines
p
alloc
len
NULL
return
else
line
len
delete
newline
strcpy
p
line
lineptr
nlines
p
return
nlines
writelines
write
output
lines
void
writelines
char
lineptr
int
nlines
int
i
for
i
i
nlines
i
printf
s
n
lineptr
i
The
function
getline
is
from
Section
The
main
new
thing
is
the
declaration
for
lineptr
char
lineptr
MAXLINES
says
that
lineptr
is
an
array
of
MAXLINES
elements
each
element
of
which
is
a
pointer
to
a
char
That
is
lineptr
i
is
a
character
pointer
and
lineptr
i
is
the
character
it
points
to
the
first
character
of
the
i
th
saved
text
line
Since
lineptr
is
itself
the
name
of
an
array
it
can
be
treated
as
a
pointer
in
the
same
manner
as
in
our
earlier
examples
and
writelines
can
be
written
instead
as
writelines
write
output
lines
void
writelines
char
lineptr
int
nlines
while
nlines
printf
s
n
lineptr
Initially
lineptr
points
to
the
first
line
each
element
advances
it
to
the
next
line
pointer
while
nlines
is
counted
down
With
input
and
output
under
control
we
can
proceed
to
sorting
The
quicksort
from
Chapter
needs
minor
changes
the
declarations
have
to
be
modified
and
the
comparison
operation
must
be
done
by
calling
strcmp
The
algorithm
remains
the
same
which
gives
us
some
confidence
that
it
will
still
work
qsort
sort
v
left
v
right
into
increasing
order
void
qsort
char
v
int
left
int
right
int
i
last
void
swap
char
v
int
i
int
j
if
left
right
do
nothing
if
array
contains
return
fewer
than
two
elements
swap
v
left
left
right
last
left
for
i
left
i
right
i
if
strcmp
v
i
v
left
swap
v
last
i
swap
v
left
last
qsort
v
left
last
qsort
v
last
right
Similarly
the
swap
routine
needs
only
trivial
changes
swap
interchange
v
i
and
v
j
void
swap
char
v
int
i
int
j
char
temp
temp
v
i
v
i
v
j
v
j
temp
Since
any
individual
element
of
v
alias
lineptr
is
a
character
pointer
temp
must
be
also
so
one
can
be
copied
to
the
other
Exercise
Rewrite
readlines
to
store
lines
in
an
array
supplied
by
main
rather
than
calling
alloc
to
maintain
storage
How
much
faster
is
the
program
Multi
dimensional
Arrays
C
provides
rectangular
multi
dimensional
arrays
although
in
practice
they
are
much
less
used
than
arrays
of
pointers
In
this
section
we
will
show
some
of
their
properties
Consider
the
problem
of
date
conversion
from
day
of
the
month
to
day
of
the
year
and
vice
versa
For
example
March
is
the
th
day
of
a
non
leap
year
and
the
st
day
of
a
leap
year
Let
us
define
two
functions
to
do
the
conversions
day
of
year
converts
the
month
and
day
into
the
day
of
the
year
and
month
day
converts
the
day
of
the
year
into
the
month
and
day
Since
this
latter
function
computes
two
values
the
month
and
day
arguments
will
be
pointers
month
day
m
d
sets
m
to
and
d
to
February
th
These
functions
both
need
the
same
information
a
table
of
the
number
of
days
in
each
month
thirty
days
hath
September
Since
the
number
of
days
per
month
differs
for
leap
years
and
non
leap
years
it
s
easier
to
separate
them
into
two
rows
of
a
two
dimensional
array
than
to
keep
track
of
what
happens
to
February
during
computation
The
array
and
the
functions
for
performing
the
transformations
are
as
follows
static
char
daytab
day
of
year
set
day
of
year
from
month
day
int
day
of
year
int
year
int
month
int
day
int
i
leap
leap
year
year
year
for
i
i
month
i
day
daytab
leap
i
return
day
month
day
set
month
day
from
day
of
year
void
month
day
int
year
int
yearday
int
pmonth
int
pday
int
i
leap
leap
year
year
year
for
i
yearday
daytab
leap
i
i
yearday
daytab
leap
i
pmonth
i
pday
yearday
Recall
that
the
arithmetic
value
of
a
logical
expression
such
as
the
one
for
leap
is
either
zero
false
or
one
true
so
it
can
be
used
as
a
subscript
of
the
array
daytab
The
array
daytab
has
to
be
external
to
both
day
of
year
and
month
day
so
they
can
both
use
it
We
made
it
char
to
illustrate
a
legitimate
use
of
char
for
storing
small
non
character
integers
daytab
is
the
first
two
dimensional
array
we
have
dealt
with
In
C
a
two
dimensional
array
is
really
a
one
dimensional
array
each
of
whose
elements
is
an
array
Hence
subscripts
are
written
as
daytab
i
j
row
col
rather
than
daytab
i
j
WRONG
Other
than
this
notational
distinction
a
two
dimensional
array
can
be
treated
in
much
the
same
way
as
in
other
languages
Elements
are
stored
by
rows
so
the
rightmost
subscript
or
column
varies
fastest
as
elements
are
accessed
in
storage
order
An
array
is
initialized
by
a
list
of
initializers
in
braces
each
row
of
a
two
dimensional
array
is
initialized
by
a
corresponding
sub
list
We
started
the
array
daytab
with
a
column
of
zero
so
that
month
numbers
can
run
from
the
natural
to
instead
of
to
Since
space
is
not
at
a
premium
here
this
is
clearer
than
adjusting
the
indices
If
a
two
dimensional
array
is
to
be
passed
to
a
function
the
parameter
declaration
in
the
function
must
include
the
number
of
columns
the
number
of
rows
is
irrelevant
since
what
is
passed
is
as
before
a
pointer
to
an
array
of
rows
where
each
row
is
an
array
of
ints
In
this
particular
case
it
is
a
pointer
to
objects
that
are
arrays
of
ints
Thus
if
the
array
daytab
is
to
be
passed
to
a
function
f
the
declaration
of
f
would
be
f
int
daytab
It
could
also
be
f
int
daytab
since
the
number
of
rows
is
irrelevant
or
it
could
be
f
int
daytab
which
says
that
the
parameter
is
a
pointer
to
an
array
of
integers
The
parentheses
are
necessary
since
brackets
have
higher
precedence
than
Without
parentheses
the
declaration
int
daytab
is
an
array
of
pointers
to
integers
More
generally
only
the
first
dimension
subscript
of
an
array
is
free
all
the
others
have
to
be
specified
Section
has
a
further
discussion
of
complicated
declarations
Exercise
There
is
no
error
checking
in
day
of
year
or
month
day
Remedy
this
defect
Initialization
of
Pointer
Arrays
Consider
the
problem
of
writing
a
function
month
name
n
which
returns
a
pointer
to
a
character
string
containing
the
name
of
the
n
th
month
This
is
an
ideal
application
for
an
internal
static
array
month
name
contains
a
private
array
of
character
strings
and
returns
a
pointer
to
the
proper
one
when
called
This
section
shows
how
that
array
of
names
is
initialized
The
syntax
is
similar
to
previous
initializations
month
name
return
name
of
n
th
month
char
month
name
int
n
static
char
name
Illegal
month
January
February
March
April
May
June
July
August
September
October
November
December
return
n
n
name
name
n
The
declaration
of
name
which
is
an
array
of
character
pointers
is
the
same
as
lineptr
in
the
sorting
example
The
initializer
is
a
list
of
character
strings
each
is
assigned
to
the
corresponding
position
in
the
array
The
characters
of
the
i
th
string
are
placed
somewhere
and
a
pointer
to
them
is
stored
in
name
i
Since
the
size
of
the
array
name
is
not
specified
the
compiler
counts
the
initializers
and
fills
in
the
correct
number
Pointers
vs
Multi
dimensional
Arrays
Newcomers
to
C
are
sometimes
confused
about
the
difference
between
a
two
dimensional
array
and
an
array
of
pointers
such
as
name
in
the
example
above
Given
the
definitions
int
a
int
b
then
a
and
b
are
both
syntactically
legal
references
to
a
single
int
But
a
is
a
true
two
dimensional
array
int
sized
locations
have
been
set
aside
and
the
conventional
rectangular
subscript
calculation
row
col
is
used
to
find
the
element
a
row
col
For
b
however
the
definition
only
allocates
pointers
and
does
not
initialize
them
initialization
must
be
done
explicitly
either
statically
or
with
code
Assuming
that
each
element
of
b
does
point
to
a
twenty
element
array
then
there
will
be
ints
set
aside
plus
ten
cells
for
the
pointers
The
important
advantage
of
the
pointer
array
is
that
the
rows
of
the
array
may
be
of
different
lengths
That
is
each
element
of
b
need
not
point
to
a
twenty
element
vector
some
may
point
to
two
elements
some
to
fifty
and
some
to
none
at
all
Although
we
have
phrased
this
discussion
in
terms
of
integers
by
far
the
most
frequent
use
of
arrays
of
pointers
is
to
store
character
strings
of
diverse
lengths
as
in
the
function
month
name
Compare
the
declaration
and
picture
for
an
array
of
pointers
char
name
Illegal
month
Jan
Feb
Mar
with
those
for
a
two
dimensional
array
char
aname
Illegal
month
Jan
Feb
Mar
Exercise
Rewrite
the
routines
day
of
year
and
month
day
with
pointers
instead
of
indexing
Command
line
Arguments
In
environments
that
support
C
there
is
a
way
to
pass
command
line
arguments
or
parameters
to
a
program
when
it
begins
executing
When
main
is
called
it
is
called
with
two
arguments
The
first
conventionally
called
argc
for
argument
count
is
the
number
of
command
line
arguments
the
program
was
invoked
with
the
second
argv
for
argument
vector
is
a
pointer
to
an
array
of
character
strings
that
contain
the
arguments
one
per
string
We
customarily
use
multiple
levels
of
pointers
to
manipulate
these
character
strings
The
simplest
illustration
is
the
program
echo
which
echoes
its
command
line
arguments
on
a
single
line
separated
by
blanks
That
is
the
command
echo
hello
world
prints
the
output
hello
world
By
convention
argv
is
the
name
by
which
the
program
was
invoked
so
argc
is
at
least
If
argc
is
there
are
no
command
line
arguments
after
the
program
name
In
the
example
above
argc
is
and
argv
argv
and
argv
are
echo
hello
and
world
respectively
The
first
optional
argument
is
argv
and
the
last
is
argv
argc
additionally
the
standard
requires
that
argv
argc
be
a
null
pointer
The
first
version
of
echo
treats
argv
as
an
array
of
character
pointers
include
stdio
h
echo
command
line
arguments
st
version
main
int
argc
char
argv
int
i
for
i
i
argc
i
printf
s
s
argv
i
i
argc
printf
n
return
Since
argv
is
a
pointer
to
an
array
of
pointers
we
can
manipulate
the
pointer
rather
than
index
the
array
This
next
variant
is
based
on
incrementing
argv
which
is
a
pointer
to
pointer
to
char
while
argc
is
counted
down
include
stdio
h
echo
command
line
arguments
nd
version
main
int
argc
char
argv
while
argc
printf
s
s
argv
argc
printf
n
return
Since
argv
is
a
pointer
to
the
beginning
of
the
array
of
argument
strings
incrementing
it
by
argv
makes
it
point
at
the
original
argv
instead
of
argv
Each
successive
increment
moves
it
along
to
the
next
argument
argv
is
then
the
pointer
to
that
argument
At
the
same
time
argc
is
decremented
when
it
becomes
zero
there
are
no
arguments
left
to
print
Alternatively
we
could
write
the
printf
statement
as
printf
argc
s
s
argv
This
shows
that
the
format
argument
of
printf
can
be
an
expression
too
As
a
second
example
let
us
make
some
enhancements
to
the
pattern
finding
program
from
Section
If
you
recall
we
wired
the
search
pattern
deep
into
the
program
an
obviously
unsatisfactory
arrangement
Following
the
lead
of
the
UNIX
program
grep
let
us
enhance
the
program
so
the
pattern
to
be
matched
is
specified
by
the
first
argument
on
the
command
line
include
stdio
h
include
string
h
define
MAXLINE
int
getline
char
line
int
max
find
print
lines
that
match
pattern
from
st
arg
main
int
argc
char
argv
char
line
MAXLINE
int
found
if
argc
printf
Usage
find
pattern
n
else
while
getline
line
MAXLINE
if
strstr
line
argv
NULL
printf
s
line
found
return
found
The
standard
library
function
strstr
s
t
returns
a
pointer
to
the
first
occurrence
of
the
string
t
in
the
string
s
or
NULL
if
there
is
none
It
is
declared
in
string
h
The
model
can
now
be
elaborated
to
illustrate
further
pointer
constructions
Suppose
we
want
to
allow
two
optional
arguments
One
says
print
all
the
lines
except
those
that
match
the
pattern
the
second
says
precede
each
printed
line
by
its
line
number
A
common
convention
for
C
programs
on
UNIX
systems
is
that
an
argument
that
begins
with
a
minus
sign
introduces
an
optional
flag
or
parameter
If
we
choose
x
for
except
to
signal
the
inversion
and
n
number
to
request
line
numbering
then
the
command
find
x
npattern
will
print
each
line
that
doesn
t
match
the
pattern
preceded
by
its
line
number
Optional
arguments
should
be
permitted
in
any
order
and
the
rest
of
the
program
should
be
independent
of
the
number
of
arguments
that
we
present
Furthermore
it
is
convenient
for
users
if
option
arguments
can
be
combined
as
in
find
nx
pattern
Here
is
the
program
include
stdio
h
include
string
h
define
MAXLINE
int
getline
char
line
int
max
find
print
lines
that
match
pattern
from
st
arg
main
int
argc
char
argv
char
line
MAXLINE
long
lineno
int
c
except
number
found
while
argc
argv
while
c
argv
switch
c
case
x
except
break
case
n
number
break
default
printf
find
illegal
option
c
n
c
argc
found
break
if
argc
printf
Usage
find
x
n
pattern
n
else
while
getline
line
MAXLINE
lineno
if
strstr
line
argv
NULL
except
if
number
printf
ld
lineno
printf
s
line
found
return
found
argc
is
decremented
and
argv
is
incremented
before
each
optional
argument
At
the
end
of
the
loop
if
there
are
no
errors
argc
tells
how
many
arguments
remain
unprocessed
and
argv
points
to
the
first
of
these
Thus
argc
should
be
and
argv
should
point
at
the
pattern
Notice
that
argv
is
a
pointer
to
an
argument
string
so
argv
is
its
first
character
An
alternate
valid
form
would
be
argv
Because
binds
tighter
than
and
the
parentheses
are
necessary
without
them
the
expression
would
be
taken
as
argv
In
fact
that
is
what
we
have
used
in
the
inner
loop
where
the
task
is
to
walk
along
a
specific
argument
string
In
the
inner
loop
the
expression
argv
increments
the
pointer
argv
It
is
rare
that
one
uses
pointer
expressions
more
complicated
than
these
in
such
cases
breaking
them
into
two
or
three
steps
will
be
more
intuitive
Exercise
Write
the
program
expr
which
evaluates
a
reverse
Polish
expression
from
the
command
line
where
each
operator
or
operand
is
a
separate
argument
For
example
expr
evaluates
Exercise
Modify
the
program
entab
and
detab
written
as
exercises
in
Chapter
to
accept
a
list
of
tab
stops
as
arguments
Use
the
default
tab
settings
if
there
are
no
arguments
Exercise
Extend
entab
and
detab
to
accept
the
shorthand
entab
m
n
to
mean
tab
stops
every
n
columns
starting
at
column
m
Choose
convenient
for
the
user
default
behavior
Exercise
Write
the
program
tail
which
prints
the
last
n
lines
of
its
input
By
default
n
is
set
to
let
us
say
but
it
can
be
changed
by
an
optional
argument
so
that
tail
n
prints
the
last
n
lines
The
program
should
behave
rationally
no
matter
how
unreasonable
the
input
or
the
value
of
n
Write
the
program
so
it
makes
the
best
use
of
available
storage
lines
should
be
stored
as
in
the
sorting
program
of
Section
not
in
a
two
dimensional
array
of
fixed
size
Pointers
to
Functions
In
C
a
function
itself
is
not
a
variable
but
it
is
possible
to
define
pointers
to
functions
which
can
be
assigned
placed
in
arrays
passed
to
functions
returned
by
functions
and
so
on
We
will
illustrate
this
by
modifying
the
sorting
procedure
written
earlier
in
this
chapter
so
that
if
the
optional
argument
n
is
given
it
will
sort
the
input
lines
numerically
instead
of
lexicographically
A
sort
often
consists
of
three
parts
a
comparison
that
determines
the
ordering
of
any
pair
of
objects
an
exchange
that
reverses
their
order
and
a
sorting
algorithm
that
makes
comparisons
and
exchanges
until
the
objects
are
in
order
The
sorting
algorithm
is
independent
of
the
comparison
and
exchange
operations
so
by
passing
different
comparison
and
exchange
functions
to
it
we
can
arrange
to
sort
by
different
criteria
This
is
the
approach
taken
in
our
new
sort
Lexicographic
comparison
of
two
lines
is
done
by
strcmp
as
before
we
will
also
need
a
routine
numcmp
that
compares
two
lines
on
the
basis
of
numeric
value
and
returns
the
same
kind
of
condition
indication
as
strcmp
does
These
functions
are
declared
ahead
of
main
and
a
pointer
to
the
appropriate
one
is
passed
to
qsort
We
have
skimped
on
error
processing
for
arguments
so
as
to
concentrate
on
the
main
issues
include
stdio
h
include
string
h
define
MAXLINES
max
lines
to
be
sorted
char
lineptr
MAXLINES
pointers
to
text
lines
int
readlines
char
lineptr
int
nlines
void
writelines
char
lineptr
int
nlines
void
qsort
void
lineptr
int
left
int
right
int
comp
void
void
int
numcmp
char
char
sort
input
lines
main
int
argc
char
argv
int
nlines
number
of
input
lines
read
int
numeric
if
numeric
sort
if
argc
strcmp
argv
n
numeric
if
nlines
readlines
lineptr
MAXLINES
qsort
void
lineptr
nlines
int
void
void
numeric
numcmp
strcmp
writelines
lineptr
nlines
return
else
printf
input
too
big
to
sort
n
return
In
the
call
to
qsort
strcmp
and
numcmp
are
addresses
of
functions
Since
they
are
known
to
be
functions
the
is
not
necessary
in
the
same
way
that
it
is
not
needed
before
an
array
name
We
have
written
qsort
so
it
can
process
any
data
type
not
just
character
strings
As
indicated
by
the
function
prototype
qsort
expects
an
array
of
pointers
two
integers
and
a
function
with
two
pointer
arguments
The
generic
pointer
type
void
is
used
for
the
pointer
arguments
Any
pointer
can
be
cast
to
void
and
back
again
without
loss
of
information
so
we
can
call
qsort
by
casting
arguments
to
void
The
elaborate
cast
of
the
function
argument
casts
the
arguments
of
the
comparison
function
These
will
generally
have
no
effect
on
actual
representation
but
assure
the
compiler
that
all
is
well
qsort
sort
v
left
v
right
into
increasing
order
void
qsort
void
v
int
left
int
right
int
comp
void
void
int
i
last
void
swap
void
v
int
int
if
left
right
do
nothing
if
array
contains
return
fewer
than
two
elements
swap
v
left
left
right
last
left
for
i
left
i
right
i
if
comp
v
i
v
left
swap
v
last
i
swap
v
left
last
qsort
v
left
last
comp
qsort
v
last
right
comp
The
declarations
should
be
studied
with
some
care
The
fourth
parameter
of
qsort
is
int
comp
void
void
which
says
that
comp
is
a
pointer
to
a
function
that
has
two
void
arguments
and
returns
an
int
The
use
of
comp
in
the
line
if
comp
v
i
v
left
is
consistent
with
the
declaration
comp
is
a
pointer
to
a
function
comp
is
the
function
and
comp
v
i
v
left
is
the
call
to
it
The
parentheses
are
needed
so
the
components
are
correctly
associated
without
them
int
comp
void
void
WRONG
says
that
comp
is
a
function
returning
a
pointer
to
an
int
which
is
very
different
We
have
already
shown
strcmp
which
compares
two
strings
Here
is
numcmp
which
compares
two
strings
on
a
leading
numeric
value
computed
by
calling
atof
include
stdlib
h
numcmp
compare
s
and
s
numerically
int
numcmp
char
s
char
s
double
v
v
v
atof
s
v
atof
s
if
v
v
return
else
if
v
v
return
else
return
The
swap
function
which
exchanges
two
pointers
is
identical
to
what
we
presented
earlier
in
the
chapter
except
that
the
declarations
are
changed
to
void
void
swap
void
v
int
i
int
j
void
temp
temp
v
i
v
i
v
j
v
j
temp
A
variety
of
other
options
can
be
added
to
the
sorting
program
some
make
challenging
exercises
Exercise
Modify
the
sort
program
to
handle
a
r
flag
which
indicates
sorting
in
reverse
decreasing
order
Be
sure
that
r
works
with
n
Exercise
Add
the
option
f
to
fold
upper
and
lower
case
together
so
that
case
distinctions
are
not
made
during
sorting
for
example
a
and
A
compare
equal
Exercise
Add
the
d
directory
order
option
which
makes
comparisons
only
on
letters
numbers
and
blanks
Make
sure
it
works
in
conjunction
with
f
Exercise
Add
a
field
searching
capability
so
sorting
may
bee
done
on
fields
within
lines
each
field
sorted
according
to
an
independent
set
of
options
The
index
for
this
book
was
sorted
with
df
for
the
index
category
and
n
for
the
page
numbers
Complicated
Declarations
C
is
sometimes
castigated
for
the
syntax
of
its
declarations
particularly
ones
that
involve
pointers
to
functions
The
syntax
is
an
attempt
to
make
the
declaration
and
the
use
agree
it
works
well
for
simple
cases
but
it
can
be
confusing
for
the
harder
ones
because
declarations
cannot
be
read
left
to
right
and
because
parentheses
are
over
used
The
difference
between
int
f
f
function
returning
pointer
to
int
and
int
pf
pf
pointer
to
function
returning
int
illustrates
the
problem
is
a
prefix
operator
and
it
has
lower
precedence
than
so
parentheses
are
necessary
to
force
the
proper
association
Although
truly
complicated
declarations
rarely
arise
in
practice
it
is
important
to
know
how
to
understand
them
and
if
necessary
how
to
create
them
One
good
way
to
synthesize
declarations
is
in
small
steps
with
typedef
which
is
discussed
in
Section
As
an
alternative
in
this
section
we
will
present
a
pair
of
programs
that
convert
from
valid
C
to
a
word
description
and
back
again
The
word
description
reads
left
to
right
The
first
dcl
is
the
more
complex
It
converts
a
C
declaration
into
a
word
description
as
in
these
examples
char
argv
argv
pointer
to
char
int
daytab
daytab
pointer
to
array
of
int
int
daytab
daytab
array
of
pointer
to
int
void
comp
comp
function
returning
pointer
to
void
void
comp
comp
pointer
to
function
returning
void
char
x
x
function
returning
pointer
to
array
of
pointer
to
function
returning
char
char
x
x
array
of
pointer
to
function
returning
pointer
to
array
of
char
dcl
is
based
on
the
grammar
that
specifies
a
declarator
which
is
spelled
out
precisely
in
Appendix
A
Section
this
is
a
simplified
form
dcl
optional
s
direct
dcl
direct
dcl
name
dcl
direct
dcl
direct
dcl
optional
size
In
words
a
dcl
is
a
direct
dcl
perhaps
preceded
by
s
A
direct
dcl
is
a
name
or
a
parenthesized
dcl
or
a
direct
dcl
followed
by
parentheses
or
a
direct
dcl
followed
by
brackets
with
an
optional
size
This
grammar
can
be
used
to
parse
functions
For
instance
consider
this
declarator
pfa
pfa
will
be
identified
as
a
name
and
thus
as
a
direct
dcl
Then
pfa
is
also
a
direct
dcl
Then
pfa
is
recognized
as
a
dcl
so
pfa
is
a
direct
dcl
Then
pfa
is
a
direct
dcl
and
thus
a
dcl
We
can
also
illustrate
the
parse
with
a
tree
like
this
where
direct
dcl
has
been
abbreviated
to
dir
dcl
The
heart
of
the
dcl
program
is
a
pair
of
functions
dcl
and
dirdcl
that
parse
a
declaration
according
to
this
grammar
Because
the
grammar
is
recursively
defined
the
functions
call
each
other
recursively
as
they
recognize
pieces
of
a
declaration
the
program
is
called
a
recursivedescent
parser
dcl
parse
a
declarator
void
dcl
void
int
ns
for
ns
gettoken
count
s
ns
dirdcl
while
ns
strcat
out
pointer
to
dirdcl
parse
a
direct
declarator
void
dirdcl
void
int
type
if
tokentype
dcl
dcl
if
tokentype
printf
error
missing
n
else
if
tokentype
NAME
variable
name
strcpy
name
token
else
printf
error
expected
name
or
dcl
n
while
type
gettoken
PARENS
type
BRACKETS
if
type
PARENS
strcat
out
function
returning
else
strcat
out
array
strcat
out
token
strcat
out
of
Since
the
programs
are
intended
to
be
illustrative
not
bullet
proof
there
are
significant
restrictions
on
dcl
It
can
only
handle
a
simple
data
type
line
char
or
int
It
does
not
handle
argument
types
in
functions
or
qualifiers
like
const
Spurious
blanks
confuse
it
It
doesn
t
do
much
error
recovery
so
invalid
declarations
will
also
confuse
it
These
improvements
are
left
as
exercises
Here
are
the
global
variables
and
the
main
routine
include
stdio
h
include
string
h
include
ctype
h
define
MAXTOKEN
enum
NAME
PARENS
BRACKETS
void
dcl
void
void
dirdcl
void
int
gettoken
void
int
tokentype
type
of
last
token
char
token
MAXTOKEN
last
token
string
char
name
MAXTOKEN
identifier
name
char
datatype
MAXTOKEN
data
type
char
int
etc
char
out
main
convert
declaration
to
words
while
gettoken
EOF
st
token
on
line
strcpy
datatype
token
is
the
datatype
out
dcl
parse
rest
of
line
if
tokentype
n
printf
syntax
error
n
printf
s
s
s
n
name
out
datatype
return
The
function
gettoken
skips
blanks
and
tabs
then
finds
the
next
token
in
the
input
a
token
is
a
name
a
pair
of
parentheses
a
pair
of
brackets
perhaps
including
a
number
or
any
other
single
character
int
gettoken
void
return
next
token
int
c
getch
void
void
ungetch
int
char
p
token
while
c
getch
c
t
if
c
if
c
getch
strcpy
token
return
tokentype
PARENS
else
ungetch
c
return
tokentype
else
if
c
for
p
c
p
getch
p
return
tokentype
BRACKETS
else
if
isalpha
c
for
p
c
isalnum
c
getch
p
c
p
ungetch
c
return
tokentype
NAME
else
return
tokentype
c
getch
and
ungetch
are
discussed
in
Chapter
Going
in
the
other
direction
is
easier
especially
if
we
do
not
worry
about
generating
redundant
parentheses
The
program
undcl
converts
a
word
description
like
x
is
a
function
returning
a
pointer
to
an
array
of
pointers
to
functions
returning
char
which
we
will
express
as
x
char
to
char
x
The
abbreviated
input
syntax
lets
us
reuse
the
gettoken
function
undcl
also
uses
the
same
external
variables
as
dcl
does
undcl
convert
word
descriptions
to
declarations
main
int
type
char
temp
MAXTOKEN
while
gettoken
EOF
strcpy
out
token
while
type
gettoken
n
if
type
PARENS
type
BRACKETS
strcat
out
token
else
if
type
sprintf
temp
s
out
strcpy
out
temp
else
if
type
NAME
sprintf
temp
s
s
token
out
strcpy
out
temp
else
printf
invalid
input
at
s
n
token
return
Exercise
Make
dcl
recover
from
input
errors
Exercise
Modify
undcl
so
that
it
does
not
add
redundant
parentheses
to
declarations
Exercise
Expand
dcl
to
handle
declarations
with
function
argument
types
qualifiers
like
const
and
so
on
Chapter
Structures
A
structure
is
a
collection
of
one
or
more
variables
possibly
of
different
types
grouped
together
under
a
single
name
for
convenient
handling
Structures
are
called
records
in
some
languages
notably
Pascal
Structures
help
to
organize
complicated
data
particularly
in
large
programs
because
they
permit
a
group
of
related
variables
to
be
treated
as
a
unit
instead
of
as
separate
entities
One
traditional
example
of
a
structure
is
the
payroll
record
an
employee
is
described
by
a
set
of
attributes
such
as
name
address
social
security
number
salary
etc
Some
of
these
in
turn
could
be
structures
a
name
has
several
components
as
does
an
address
and
even
a
salary
Another
example
more
typical
for
C
comes
from
graphics
a
point
is
a
pair
of
coordinate
a
rectangle
is
a
pair
of
points
and
so
on
The
main
change
made
by
the
ANSI
standard
is
to
define
structure
assignment
structures
may
be
copied
and
assigned
to
passed
to
functions
and
returned
by
functions
This
has
been
supported
by
most
compilers
for
many
years
but
the
properties
are
now
precisely
defined
Automatic
structures
and
arrays
may
now
also
be
initialized
Basics
of
Structures
Let
us
create
a
few
structures
suitable
for
graphics
The
basic
object
is
a
point
which
we
will
assume
has
an
x
coordinate
and
a
y
coordinate
both
integers
The
two
components
can
be
placed
in
a
structure
declared
like
this
struct
point
int
x
int
y
The
keyword
struct
introduces
a
structure
declaration
which
is
a
list
of
declarations
enclosed
in
braces
An
optional
name
called
a
structure
tag
may
follow
the
word
struct
as
with
point
here
The
tag
names
this
kind
of
structure
and
can
be
used
subsequently
as
a
shorthand
for
the
part
of
the
declaration
in
braces
The
variables
named
in
a
structure
are
called
members
A
structure
member
or
tag
and
an
ordinary
i
e
non
member
variable
can
have
the
same
name
without
conflict
since
they
can
always
be
distinguished
by
context
Furthermore
the
same
member
names
may
occur
in
different
structures
although
as
a
matter
of
style
one
would
normally
use
the
same
names
only
for
closely
related
objects
A
struct
declaration
defines
a
type
The
right
brace
that
terminates
the
list
of
members
may
be
followed
by
a
list
of
variables
just
as
for
any
basic
type
That
is
struct
x
y
z
is
syntactically
analogous
to
int
x
y
z
in
the
sense
that
each
statement
declares
x
y
and
z
to
be
variables
of
the
named
type
and
causes
space
to
be
set
aside
for
them
A
structure
declaration
that
is
not
followed
by
a
list
of
variables
reserves
no
storage
it
merely
describes
a
template
or
shape
of
a
structure
If
the
declaration
is
tagged
however
the
tag
can
be
used
later
in
definitions
of
instances
of
the
structure
For
example
given
the
declaration
of
point
above
struct
point
pt
defines
a
variable
pt
which
is
a
structure
of
type
struct
point
A
structure
can
be
initialized
by
following
its
definition
with
a
list
of
initializers
each
a
constant
expression
for
the
members
struct
maxpt
An
automatic
structure
may
also
be
initialized
by
assignment
or
by
calling
a
function
that
returns
a
structure
of
the
right
type
A
member
of
a
particular
structure
is
referred
to
in
an
expression
by
a
construction
of
the
form
structure
name
member
The
structure
member
operator
connects
the
structure
name
and
the
member
name
To
print
the
coordinates
of
the
point
pt
for
instance
printf
d
d
pt
x
pt
y
or
to
compute
the
distance
from
the
origin
to
pt
double
dist
sqrt
double
dist
sqrt
double
pt
x
pt
x
double
pt
y
pt
y
Structures
can
be
nested
One
representation
of
a
rectangle
is
a
pair
of
points
that
denote
the
diagonally
opposite
corners
struct
rect
struct
point
pt
struct
point
pt
The
rect
structure
contains
two
point
structures
If
we
declare
screen
as
struct
rect
screen
then
screen
pt
x
refers
to
the
x
coordinate
of
the
pt
member
of
screen
Structures
and
Functions
The
only
legal
operations
on
a
structure
are
copying
it
or
assigning
to
it
as
a
unit
taking
its
address
with
and
accessing
its
members
Copy
and
assignment
include
passing
arguments
to
functions
and
returning
values
from
functions
as
well
Structures
may
not
be
compared
A
structure
may
be
initialized
by
a
list
of
constant
member
values
an
automatic
structure
may
also
be
initialized
by
an
assignment
Let
us
investigate
structures
by
writing
some
functions
to
manipulate
points
and
rectangles
There
are
at
least
three
possible
approaches
pass
components
separately
pass
an
entire
structure
or
pass
a
pointer
to
it
Each
has
its
good
points
and
bad
points
The
first
function
makepoint
will
take
two
integers
and
return
a
point
structure
makepoint
make
a
point
from
x
and
y
components
struct
point
makepoint
int
x
int
y
struct
point
temp
temp
x
x
temp
y
y
return
temp
Notice
that
there
is
no
conflict
between
the
argument
name
and
the
member
with
the
same
name
indeed
the
re
use
of
the
names
stresses
the
relationship
makepoint
can
now
be
used
to
initialize
any
structure
dynamically
or
to
provide
structure
arguments
to
a
function
struct
rect
screen
struct
point
middle
struct
point
makepoint
int
int
screen
pt
makepoint
screen
pt
makepoint
XMAX
YMAX
middle
makepoint
screen
pt
x
screen
pt
x
screen
pt
y
screen
pt
y
The
next
step
is
a
set
of
functions
to
do
arithmetic
on
points
For
instance
addpoints
add
two
points
struct
addpoint
struct
point
p
struct
point
p
p
x
p
x
p
y
p
y
return
p
Here
both
the
arguments
and
the
return
value
are
structures
We
incremented
the
components
in
p
rather
than
using
an
explicit
temporary
variable
to
emphasize
that
structure
parameters
are
passed
by
value
like
any
others
As
another
example
the
function
ptinrect
tests
whether
a
point
is
inside
a
rectangle
where
we
have
adopted
the
convention
that
a
rectangle
includes
its
left
and
bottom
sides
but
not
its
top
and
right
sides
ptinrect
return
if
p
in
r
if
not
int
ptinrect
struct
point
p
struct
rect
r
return
p
x
r
pt
x
p
x
r
pt
x
p
y
r
pt
y
p
y
r
pt
y
This
assumes
that
the
rectangle
is
presented
in
a
standard
form
where
the
pt
coordinates
are
less
than
the
pt
coordinates
The
following
function
returns
a
rectangle
guaranteed
to
be
in
canonical
form
define
min
a
b
a
b
a
b
define
max
a
b
a
b
a
b
canonrect
canonicalize
coordinates
of
rectangle
struct
rect
canonrect
struct
rect
r
struct
rect
temp
temp
pt
x
min
r
pt
x
r
pt
x
temp
pt
y
min
r
pt
y
r
pt
y
temp
pt
x
max
r
pt
x
r
pt
x
temp
pt
y
max
r
pt
y
r
pt
y
return
temp
If
a
large
structure
is
to
be
passed
to
a
function
it
is
generally
more
efficient
to
pass
a
pointer
than
to
copy
the
whole
structure
Structure
pointers
are
just
like
pointers
to
ordinary
variables
The
declaration
struct
point
pp
says
that
pp
is
a
pointer
to
a
structure
of
type
struct
point
If
pp
points
to
a
point
structure
pp
is
the
structure
and
pp
x
and
pp
y
are
the
members
To
use
pp
we
might
write
for
example
struct
point
origin
pp
pp
origin
printf
origin
is
d
d
n
pp
x
pp
y
The
parentheses
are
necessary
in
pp
x
because
the
precedence
of
the
structure
member
operator
is
higher
then
The
expression
pp
x
means
pp
x
which
is
illegal
here
because
x
is
not
a
pointer
Pointers
to
structures
are
so
frequently
used
that
an
alternative
notation
is
provided
as
a
shorthand
If
p
is
a
pointer
to
a
structure
then
p
member
of
structure
refers
to
the
particular
member
So
we
could
write
instead
printf
origin
is
d
d
n
pp
x
pp
y
Both
and
associate
from
left
to
right
so
if
we
have
struct
rect
r
rp
r
then
these
four
expressions
are
equivalent
r
pt
x
rp
pt
x
r
pt
x
rp
pt
x
The
structure
operators
and
together
with
for
function
calls
and
for
subscripts
are
at
the
top
of
the
precedence
hierarchy
and
thus
bind
very
tightly
For
example
given
the
declaration
struct
int
len
char
str
p
then
p
len
increments
len
not
p
because
the
implied
parenthesization
is
p
len
Parentheses
can
be
used
to
alter
binding
p
len
increments
p
before
accessing
len
and
p
len
increments
p
afterward
This
last
set
of
parentheses
is
unnecessary
In
the
same
way
p
str
fetches
whatever
str
points
to
p
str
increments
str
after
accessing
whatever
it
points
to
just
like
s
p
str
increments
whatever
str
points
to
and
p
str
increments
p
after
accessing
whatever
str
points
to
Arrays
of
Structures
Consider
writing
a
program
to
count
the
occurrences
of
each
C
keyword
We
need
an
array
of
character
strings
to
hold
the
names
and
an
array
of
integers
for
the
counts
One
possibility
is
to
use
two
parallel
arrays
keyword
and
keycount
as
in
char
keyword
NKEYS
int
keycount
NKEYS
But
the
very
fact
that
the
arrays
are
parallel
suggests
a
different
organization
an
array
of
structures
Each
keyword
is
a
pair
char
word
int
cout
and
there
is
an
array
of
pairs
The
structure
declaration
struct
key
char
word
int
count
keytab
NKEYS
declares
a
structure
type
key
defines
an
array
keytab
of
structures
of
this
type
and
sets
aside
storage
for
them
Each
element
of
the
array
is
a
structure
This
could
also
be
written
struct
key
char
word
int
count
struct
key
keytab
NKEYS
Since
the
structure
keytab
contains
a
constant
set
of
names
it
is
easiest
to
make
it
an
external
variable
and
initialize
it
once
and
for
all
when
it
is
defined
The
structure
initialization
is
analogous
to
earlier
ones
the
definition
is
followed
by
a
list
of
initializers
enclosed
in
braces
struct
key
char
word
int
count
keytab
auto
break
case
char
const
continue
default
unsigned
void
volatile
while
The
initializers
are
listed
in
pairs
corresponding
to
the
structure
members
It
would
be
more
precise
to
enclose
the
initializers
for
each
row
or
structure
in
braces
as
in
auto
break
case
but
inner
braces
are
not
necessary
when
the
initializers
are
simple
variables
or
character
strings
and
when
all
are
present
As
usual
the
number
of
entries
in
the
array
keytab
will
be
computed
if
the
initializers
are
present
and
the
is
left
empty
The
keyword
counting
program
begins
with
the
definition
of
keytab
The
main
routine
reads
the
input
by
repeatedly
calling
a
function
getword
that
fetches
one
word
at
a
time
Each
word
is
looked
up
in
keytab
with
a
version
of
the
binary
search
function
that
we
wrote
in
Chapter
The
list
of
keywords
must
be
sorted
in
increasing
order
in
the
table
include
stdio
h
include
ctype
h
include
string
h
define
MAXWORD
int
getword
char
int
int
binsearch
char
struct
key
int
count
C
keywords
main
int
n
char
word
MAXWORD
while
getword
word
MAXWORD
EOF
if
isalpha
word
if
n
binsearch
word
keytab
NKEYS
keytab
n
count
for
n
n
NKEYS
n
if
keytab
n
count
printf
d
s
n
keytab
n
count
keytab
n
word
return
binsearch
find
word
in
tab
tab
n
int
binsearch
char
word
struct
key
tab
int
n
int
cond
int
low
high
mid
low
high
n
while
low
high
mid
low
high
if
cond
strcmp
word
tab
mid
word
high
mid
else
if
cond
low
mid
else
return
mid
return
We
will
show
the
function
getword
in
a
moment
for
now
it
suffices
to
say
that
each
call
to
getword
finds
a
word
which
is
copied
into
the
array
named
as
its
first
argument
The
quantity
NKEYS
is
the
number
of
keywords
in
keytab
Although
we
could
count
this
by
hand
it
s
a
lot
easier
and
safer
to
do
it
by
machine
especially
if
the
list
is
subject
to
change
One
possibility
would
be
to
terminate
the
list
of
initializers
with
a
null
pointer
then
loop
along
keytab
until
the
end
is
found
But
this
is
more
than
is
needed
since
the
size
of
the
array
is
completely
determined
at
compile
time
The
size
of
the
array
is
the
size
of
one
entry
times
the
number
of
entries
so
the
number
of
entries
is
just
size
of
keytab
size
of
struct
key
C
provides
a
compile
time
unary
operator
called
sizeof
that
can
be
used
to
compute
the
size
of
any
object
The
expressions
sizeof
object
and
sizeof
type
name
yield
an
integer
equal
to
the
size
of
the
specified
object
or
type
in
bytes
Strictly
sizeof
produces
an
unsigned
integer
value
whose
type
size
t
is
defined
in
the
header
stddef
h
An
object
can
be
a
variable
or
array
or
structure
A
type
name
can
be
the
name
of
a
basic
type
like
int
or
double
or
a
derived
type
like
a
structure
or
a
pointer
In
our
case
the
number
of
keywords
is
the
size
of
the
array
divided
by
the
size
of
one
element
This
computation
is
used
in
a
define
statement
to
set
the
value
of
NKEYS
define
NKEYS
sizeof
keytab
sizeof
struct
key
Another
way
to
write
this
is
to
divide
the
array
size
by
the
size
of
a
specific
element
define
NKEYS
sizeof
keytab
sizeof
keytab
This
has
the
advantage
that
it
does
not
need
to
be
changed
if
the
type
changes
A
sizeof
can
not
be
used
in
a
if
line
because
the
preprocessor
does
not
parse
type
names
But
the
expression
in
the
define
is
not
evaluated
by
the
preprocessor
so
the
code
here
is
legal
Now
for
the
function
getword
We
have
written
a
more
general
getword
than
is
necessary
for
this
program
but
it
is
not
complicated
getword
fetches
the
next
word
from
the
input
where
a
word
is
either
a
string
of
letters
and
digits
beginning
with
a
letter
or
a
single
nonwhite
space
character
The
function
value
is
the
first
character
of
the
word
or
EOF
for
end
of
file
or
the
character
itself
if
it
is
not
alphabetic
getword
get
next
word
or
character
from
input
int
getword
char
word
int
lim
int
c
getch
void
void
ungetch
int
char
w
word
while
isspace
c
getch
if
c
EOF
w
c
if
isalpha
c
w
return
c
for
lim
w
if
isalnum
w
getch
ungetch
w
break
w
return
word
getword
uses
the
getch
and
ungetch
that
we
wrote
in
Chapter
When
the
collection
of
an
alphanumeric
token
stops
getword
has
gone
one
character
too
far
The
call
to
ungetch
pushes
that
character
back
on
the
input
for
the
next
call
getword
also
uses
isspace
to
skip
whitespace
isalpha
to
identify
letters
and
isalnum
to
identify
letters
and
digits
all
are
from
the
standard
header
ctype
h
Exercise
Our
version
of
getword
does
not
properly
handle
underscores
string
constants
comments
or
preprocessor
control
lines
Write
a
better
version
Pointers
to
Structures
To
illustrate
some
of
the
considerations
involved
with
pointers
to
and
arrays
of
structures
let
us
write
the
keyword
counting
program
again
this
time
using
pointers
instead
of
array
indices
The
external
declaration
of
keytab
need
not
change
but
main
and
binsearch
do
need
modification
include
stdio
h
include
ctype
h
include
string
h
define
MAXWORD
int
getword
char
int
struct
key
binsearch
char
struct
key
int
count
C
keywords
pointer
version
main
char
word
MAXWORD
struct
key
p
while
getword
word
MAXWORD
EOF
if
isalpha
word
if
p
binsearch
word
keytab
NKEYS
NULL
p
count
for
p
keytab
p
keytab
NKEYS
p
if
p
count
printf
d
s
n
p
count
p
word
return
binsearch
find
word
in
tab
tab
n
struct
key
binsearch
char
word
struck
key
tab
int
n
int
cond
struct
key
low
tab
struct
key
high
tab
n
struct
key
mid
while
low
high
mid
low
high
low
if
cond
strcmp
word
mid
word
high
mid
else
if
cond
low
mid
else
return
mid
return
NULL
There
are
several
things
worthy
of
note
here
First
the
declaration
of
binsearch
must
indicate
that
it
returns
a
pointer
to
struct
key
instead
of
an
integer
this
is
declared
both
in
the
function
prototype
and
in
binsearch
If
binsearch
finds
the
word
it
returns
a
pointer
to
it
if
it
fails
it
returns
NULL
Second
the
elements
of
keytab
are
now
accessed
by
pointers
This
requires
significant
changes
in
binsearch
The
initializers
for
low
and
high
are
now
pointers
to
the
beginning
and
just
past
the
end
of
the
table
The
computation
of
the
middle
element
can
no
longer
be
simply
mid
low
high
WRONG
because
the
addition
of
pointers
is
illegal
Subtraction
is
legal
however
so
high
low
is
the
number
of
elements
and
thus
mid
low
high
low
sets
mid
to
the
element
halfway
between
low
and
high
The
most
important
change
is
to
adjust
the
algorithm
to
make
sure
that
it
does
not
generate
an
illegal
pointer
or
attempt
to
access
an
element
outside
the
array
The
problem
is
that
tab
and
tab
n
are
both
outside
the
limits
of
the
array
tab
The
former
is
strictly
illegal
and
it
is
illegal
to
dereference
the
latter
The
language
definition
does
guarantee
however
that
pointer
arithmetic
that
involves
the
first
element
beyond
the
end
of
an
array
that
is
tab
n
will
work
correctly
In
main
we
wrote
for
p
keytab
p
keytab
NKEYS
p
If
p
is
a
pointer
to
a
structure
arithmetic
on
p
takes
into
account
the
size
of
the
structure
so
p
increments
p
by
the
correct
amount
to
get
the
next
element
of
the
array
of
structures
and
the
test
stops
the
loop
at
the
right
time
Don
t
assume
however
that
the
size
of
a
structure
is
the
sum
of
the
sizes
of
its
members
Because
of
alignment
requirements
for
different
objects
there
may
be
unnamed
holes
in
a
structure
Thus
for
instance
if
a
char
is
one
byte
and
an
int
four
bytes
the
structure
struct
char
c
int
i
might
well
require
eight
bytes
not
five
The
sizeof
operator
returns
the
proper
value
Finally
an
aside
on
program
format
when
a
function
returns
a
complicated
type
like
a
structure
pointer
as
in
struct
key
binsearch
char
word
struct
key
tab
int
n
the
function
name
can
be
hard
to
see
and
to
find
with
a
text
editor
Accordingly
an
alternate
style
is
sometimes
used
struct
key
binsearch
char
word
struct
key
tab
int
n
This
is
a
matter
of
personal
taste
pick
the
form
you
like
and
hold
to
it
Self
referential
Structures
Suppose
we
want
to
handle
the
more
general
problem
of
counting
the
occurrences
of
all
the
words
in
some
input
Since
the
list
of
words
isn
t
known
in
advance
we
can
t
conveniently
sort
it
and
use
a
binary
search
Yet
we
can
t
do
a
linear
search
for
each
word
as
it
arrives
to
see
if
it
s
already
been
seen
the
program
would
take
too
long
More
precisely
its
running
time
is
likely
to
grow
quadratically
with
the
number
of
input
words
How
can
we
organize
the
data
to
copy
efficiently
with
a
list
or
arbitrary
words
One
solution
is
to
keep
the
set
of
words
seen
so
far
sorted
at
all
times
by
placing
each
word
into
its
proper
position
in
the
order
as
it
arrives
This
shouldn
t
be
done
by
shifting
words
in
a
linear
array
though
that
also
takes
too
long
Instead
we
will
use
a
data
structure
called
a
binary
tree
The
tree
contains
one
node
per
distinct
word
each
node
contains
A
pointer
to
the
text
of
the
word
A
count
of
the
number
of
occurrences
A
pointer
to
the
left
child
node
A
pointer
to
the
right
child
node
No
node
may
have
more
than
two
children
it
might
have
only
zero
or
one
The
nodes
are
maintained
so
that
at
any
node
the
left
subtree
contains
only
words
that
are
lexicographically
less
than
the
word
at
the
node
and
the
right
subtree
contains
only
words
that
are
greater
This
is
the
tree
for
the
sentence
now
is
the
time
for
all
good
men
to
come
to
the
aid
of
their
party
as
built
by
inserting
each
word
as
it
is
encountered
To
find
out
whether
a
new
word
is
already
in
the
tree
start
at
the
root
and
compare
the
new
word
to
the
word
stored
at
that
node
If
they
match
the
question
is
answered
affirmatively
If
the
new
record
is
less
than
the
tree
word
continue
searching
at
the
left
child
otherwise
at
the
right
child
If
there
is
no
child
in
the
required
direction
the
new
word
is
not
in
the
tree
and
in
fact
the
empty
slot
is
the
proper
place
to
add
the
new
word
This
process
is
recursive
since
the
search
from
any
node
uses
a
search
from
one
of
its
children
Accordingly
recursive
routines
for
insertion
and
printing
will
be
most
natural
Going
back
to
the
description
of
a
node
it
is
most
conveniently
represented
as
a
structure
with
four
components
struct
tnode
the
tree
node
char
word
points
to
the
text
int
count
number
of
occurrences
struct
tnode
left
left
child
struct
tnode
right
right
child
This
recursive
declaration
of
a
node
might
look
chancy
but
it
s
correct
It
is
illegal
for
a
structure
to
contain
an
instance
of
itself
but
struct
tnode
left
declares
left
to
be
a
pointer
to
a
tnode
not
a
tnode
itself
Occasionally
one
needs
a
variation
of
self
referential
structures
two
structures
that
refer
to
each
other
The
way
to
handle
this
is
struct
t
struct
s
p
p
points
to
an
s
struct
s
struct
t
q
q
points
to
a
t
The
code
for
the
whole
program
is
surprisingly
small
given
a
handful
of
supporting
routines
like
getword
that
we
have
already
written
The
main
routine
reads
words
with
getword
and
installs
them
in
the
tree
with
addtree
include
stdio
h
include
ctype
h
include
string
h
define
MAXWORD
struct
tnode
addtree
struct
tnode
char
void
treeprint
struct
tnode
int
getword
char
int
word
frequency
count
main
struct
tnode
root
char
word
MAXWORD
root
NULL
while
getword
word
MAXWORD
EOF
if
isalpha
word
root
addtree
root
word
treeprint
root
return
The
function
addtree
is
recursive
A
word
is
presented
by
main
to
the
top
level
the
root
of
the
tree
At
each
stage
that
word
is
compared
to
the
word
already
stored
at
the
node
and
is
percolated
down
to
either
the
left
or
right
subtree
by
a
recursive
call
to
adtree
Eventually
the
word
either
matches
something
already
in
the
tree
in
which
case
the
count
is
incremented
or
a
null
pointer
is
encountered
indicating
that
a
node
must
be
created
and
added
to
the
tree
If
a
new
node
is
created
addtree
returns
a
pointer
to
it
which
is
installed
in
the
parent
node
struct
tnode
talloc
void
char
strdup
char
addtree
add
a
node
with
w
at
or
below
p
struct
treenode
addtree
struct
tnode
p
char
w
int
cond
if
p
NULL
a
new
word
has
arrived
p
talloc
make
a
new
node
p
word
strdup
w
p
count
p
left
p
right
NULL
else
if
cond
strcmp
w
p
word
p
count
repeated
word
else
if
cond
less
than
into
left
subtree
p
left
addtree
p
left
w
else
greater
than
into
right
subtree
p
right
addtree
p
right
w
return
p
Storage
for
the
new
node
is
fetched
by
a
routine
talloc
which
returns
a
pointer
to
a
free
space
suitable
for
holding
a
tree
node
and
the
new
word
is
copied
into
a
hidden
space
by
strdup
We
will
discuss
these
routines
in
a
moment
The
count
is
initialized
and
the
two
children
are
made
null
This
part
of
the
code
is
executed
only
at
the
leaves
of
the
tree
when
a
new
node
is
being
added
We
have
unwisely
omitted
error
checking
on
the
values
returned
by
strdup
and
talloc
treeprint
prints
the
tree
in
sorted
order
at
each
node
it
prints
the
left
subtree
all
the
words
less
than
this
word
then
the
word
itself
then
the
right
subtree
all
the
words
greater
If
you
feel
shaky
about
how
recursion
works
simulate
treeprint
as
it
operates
on
the
tree
shown
above
treeprint
in
order
print
of
tree
p
void
treeprint
struct
tnode
p
if
p
NULL
treeprint
p
left
printf
d
s
n
p
count
p
word
treeprint
p
right
A
practical
note
if
the
tree
becomes
unbalanced
because
the
words
don
t
arrive
in
random
order
the
running
time
of
the
program
can
grow
too
much
As
a
worst
case
if
the
words
are
already
in
order
this
program
does
an
expensive
simulation
of
linear
search
There
are
generalizations
of
the
binary
tree
that
do
not
suffer
from
this
worst
case
behavior
but
we
will
not
describe
them
here
Before
leaving
this
example
it
is
also
worth
a
brief
digression
on
a
problem
related
to
storage
allocators
Clearly
it
s
desirable
that
there
be
only
one
storage
allocator
in
a
program
even
though
it
allocates
different
kinds
of
objects
But
if
one
allocator
is
to
process
requests
for
say
pointers
to
chars
and
pointers
to
struct
tnodes
two
questions
arise
First
how
does
it
meet
the
requirement
of
most
real
machines
that
objects
of
certain
types
must
satisfy
alignment
restrictions
for
example
integers
often
must
be
located
at
even
addresses
Second
what
declarations
can
cope
with
the
fact
that
an
allocator
must
necessarily
return
different
kinds
of
pointers
Alignment
requirements
can
generally
be
satisfied
easily
at
the
cost
of
some
wasted
space
by
ensuring
that
the
allocator
always
returns
a
pointer
that
meets
all
alignment
restrictions
The
alloc
of
Chapter
does
not
guarantee
any
particular
alignment
so
we
will
use
the
standard
library
function
malloc
which
does
In
Chapter
we
will
show
one
way
to
implement
malloc
The
question
of
the
type
declaration
for
a
function
like
malloc
is
a
vexing
one
for
any
language
that
takes
its
type
checking
seriously
In
C
the
proper
method
is
to
declare
that
malloc
returns
a
pointer
to
void
then
explicitly
coerce
the
pointer
into
the
desired
type
with
a
cast
malloc
and
related
routines
are
declared
in
the
standard
header
stdlib
h
Thus
talloc
can
be
written
as
include
stdlib
h
talloc
make
a
tnode
struct
tnode
talloc
void
return
struct
tnode
malloc
sizeof
struct
tnode
strdup
merely
copies
the
string
given
by
its
argument
into
a
safe
place
obtained
by
a
call
on
malloc
char
strdup
char
s
make
a
duplicate
of
s
char
p
p
char
malloc
strlen
s
for
if
p
NULL
strcpy
p
s
return
p
malloc
returns
NULL
if
no
space
is
available
strdup
passes
that
value
on
leaving
errorhandling
to
its
caller
Storage
obtained
by
calling
malloc
may
be
freed
for
re
use
by
calling
free
see
Chapters
and
Exercise
Write
a
program
that
reads
a
C
program
and
prints
in
alphabetical
order
each
group
of
variable
names
that
are
identical
in
the
first
characters
but
different
somewhere
thereafter
Don
t
count
words
within
strings
and
comments
Make
a
parameter
that
can
be
set
from
the
command
line
Exercise
Write
a
cross
referencer
that
prints
a
list
of
all
words
in
a
document
and
for
each
word
a
list
of
the
line
numbers
on
which
it
occurs
Remove
noise
words
like
the
and
and
so
on
Exercise
Write
a
program
that
prints
the
distinct
words
in
its
input
sorted
into
decreasing
order
of
frequency
of
occurrence
Precede
each
word
by
its
count
Table
Lookup
In
this
section
we
will
write
the
innards
of
a
table
lookup
package
to
illustrate
more
aspects
of
structures
This
code
is
typical
of
what
might
be
found
in
the
symbol
table
management
routines
of
a
macro
processor
or
a
compiler
For
example
consider
the
define
statement
When
a
line
like
define
IN
is
encountered
the
name
IN
and
the
replacement
text
are
stored
in
a
table
Later
when
the
name
IN
appears
in
a
statement
like
state
IN
it
must
be
replaced
by
There
are
two
routines
that
manipulate
the
names
and
replacement
texts
install
s
t
records
the
name
s
and
the
replacement
text
t
in
a
table
s
and
t
are
just
character
strings
lookup
s
searches
for
s
in
the
table
and
returns
a
pointer
to
the
place
where
it
was
found
or
NULL
if
it
wasn
t
there
The
algorithm
is
a
hash
search
the
incoming
name
is
converted
into
a
small
non
negative
integer
which
is
then
used
to
index
into
an
array
of
pointers
An
array
element
points
to
the
beginning
of
a
linked
list
of
blocks
describing
names
that
have
that
hash
value
It
is
NULL
if
no
names
have
hashed
to
that
value
A
block
in
the
list
is
a
structure
containing
pointers
to
the
name
the
replacement
text
and
the
next
block
in
the
list
A
null
next
pointer
marks
the
end
of
the
list
struct
nlist
table
entry
struct
nlist
next
next
entry
in
chain
char
name
defined
name
char
defn
replacement
text
The
pointer
array
is
just
define
HASHSIZE
static
struct
nlist
hashtab
HASHSIZE
pointer
table
The
hashing
function
which
is
used
by
both
lookup
and
install
adds
each
character
value
in
the
string
to
a
scrambled
combination
of
the
previous
ones
and
returns
the
remainder
modulo
the
array
size
This
is
not
the
best
possible
hash
function
but
it
is
short
and
effective
hash
form
hash
value
for
string
s
unsigned
hash
char
s
unsigned
hashval
for
hashval
s
s
hashval
s
hashval
return
hashval
HASHSIZE
Unsigned
arithmetic
ensures
that
the
hash
value
is
non
negative
The
hashing
process
produces
a
starting
index
in
the
array
hashtab
if
the
string
is
to
be
found
anywhere
it
will
be
in
the
list
of
blocks
beginning
there
The
search
is
performed
by
lookup
If
lookup
finds
the
entry
already
present
it
returns
a
pointer
to
it
if
not
it
returns
NULL
lookup
look
for
s
in
hashtab
struct
nlist
lookup
char
s
struct
nlist
np
for
np
hashtab
hash
s
np
NULL
np
np
next
if
strcmp
s
np
name
return
np
found
return
NULL
not
found
The
for
loop
in
lookup
is
the
standard
idiom
for
walking
along
a
linked
list
for
ptr
head
ptr
NULL
ptr
ptr
next
install
uses
lookup
to
determine
whether
the
name
being
installed
is
already
present
if
so
the
new
definition
will
supersede
the
old
one
Otherwise
a
new
entry
is
created
install
returns
NULL
if
for
any
reason
there
is
no
room
for
a
new
entry
struct
nlist
lookup
char
char
strdup
char
install
put
name
defn
in
hashtab
struct
nlist
install
char
name
char
defn
struct
nlist
np
unsigned
hashval
if
np
lookup
name
NULL
not
found
np
struct
nlist
malloc
sizeof
np
if
np
NULL
np
name
strdup
name
NULL
return
NULL
hashval
hash
name
np
next
hashtab
hashval
hashtab
hashval
np
else
already
there
free
void
np
defn
free
previous
defn
if
np
defn
strdup
defn
NULL
return
NULL
return
np
Exercise
Write
a
function
undef
that
will
remove
a
name
and
definition
from
the
table
maintained
by
lookup
and
install
Exercise
Implement
a
simple
version
of
the
define
processor
i
e
no
arguments
suitable
for
use
with
C
programs
based
on
the
routines
of
this
section
You
may
also
find
getch
and
ungetch
helpful
Typedef
C
provides
a
facility
called
typedef
for
creating
new
data
type
names
For
example
the
declaration
typedef
int
Length
makes
the
name
Length
a
synonym
for
int
The
type
Length
can
be
used
in
declarations
casts
etc
in
exactly
the
same
ways
that
the
int
type
can
be
Length
len
maxlen
Length
lengths
Similarly
the
declaration
typedef
char
String
makes
String
a
synonym
for
char
or
character
pointer
which
may
then
be
used
in
declarations
and
casts
String
p
lineptr
MAXLINES
alloc
int
int
strcmp
String
String
p
String
malloc
Notice
that
the
type
being
declared
in
a
typedef
appears
in
the
position
of
a
variable
name
not
right
after
the
word
typedef
Syntactically
typedef
is
like
the
storage
classes
extern
static
etc
We
have
used
capitalized
names
for
typedefs
to
make
them
stand
out
As
a
more
complicated
example
we
could
make
typedefs
for
the
tree
nodes
shown
earlier
in
this
chapter
typedef
struct
tnode
Treeptr
typedef
struct
tnode
the
tree
node
char
word
points
to
the
text
int
count
number
of
occurrences
struct
tnode
left
left
child
struct
tnode
right
right
child
Treenode
This
creates
two
new
type
keywords
called
Treenode
a
structure
and
Treeptr
a
pointer
to
the
structure
Then
the
routine
talloc
could
become
Treeptr
talloc
void
return
Treeptr
malloc
sizeof
Treenode
It
must
be
emphasized
that
a
typedef
declaration
does
not
create
a
new
type
in
any
sense
it
merely
adds
a
new
name
for
some
existing
type
Nor
are
there
any
new
semantics
variables
declared
this
way
have
exactly
the
same
properties
as
variables
whose
declarations
are
spelled
out
explicitly
In
effect
typedef
is
like
define
except
that
since
it
is
interpreted
by
the
compiler
it
can
cope
with
textual
substitutions
that
are
beyond
the
capabilities
of
the
preprocessor
For
example
typedef
int
PFI
char
char
creates
the
type
PFI
for
pointer
to
function
of
two
char
arguments
returning
int
which
can
be
used
in
contexts
like
PFI
strcmp
numcmp
in
the
sort
program
of
Chapter
Besides
purely
aesthetic
issues
there
are
two
main
reasons
for
using
typedefs
The
first
is
to
parameterize
a
program
against
portability
problems
If
typedefs
are
used
for
data
types
that
may
be
machine
dependent
only
the
typedefs
need
change
when
the
program
is
moved
One
common
situation
is
to
use
typedef
names
for
various
integer
quantities
then
make
an
appropriate
set
of
choices
of
short
int
and
long
for
each
host
machine
Types
like
size
t
and
ptrdiff
t
from
the
standard
library
are
examples
The
second
purpose
of
typedefs
is
to
provide
better
documentation
for
a
program
a
type
called
Treeptr
may
be
easier
to
understand
than
one
declared
only
as
a
pointer
to
a
complicated
structure
Unions
A
union
is
a
variable
that
may
hold
at
different
times
objects
of
different
types
and
sizes
with
the
compiler
keeping
track
of
size
and
alignment
requirements
Unions
provide
a
way
to
manipulate
different
kinds
of
data
in
a
single
area
of
storage
without
embedding
any
machinedependent
information
in
the
program
They
are
analogous
to
variant
records
in
pascal
As
an
example
such
as
might
be
found
in
a
compiler
symbol
table
manager
suppose
that
a
constant
may
be
an
int
a
float
or
a
character
pointer
The
value
of
a
particular
constant
must
be
stored
in
a
variable
of
the
proper
type
yet
it
is
most
convenient
for
table
management
if
the
value
occupies
the
same
amount
of
storage
and
is
stored
in
the
same
place
regardless
of
its
type
This
is
the
purpose
of
a
union
a
single
variable
that
can
legitimately
hold
any
of
one
of
several
types
The
syntax
is
based
on
structures
union
u
tag
int
ival
float
fval
char
sval
u
The
variable
u
will
be
large
enough
to
hold
the
largest
of
the
three
types
the
specific
size
is
implementation
dependent
Any
of
these
types
may
be
assigned
to
u
and
then
used
in
expressions
so
long
as
the
usage
is
consistent
the
type
retrieved
must
be
the
type
most
recently
stored
It
is
the
programmer
s
responsibility
to
keep
track
of
which
type
is
currently
stored
in
a
union
the
results
are
implementation
dependent
if
something
is
stored
as
one
type
and
extracted
as
another
Syntactically
members
of
a
union
are
accessed
as
union
name
member
or
union
pointer
member
just
as
for
structures
If
the
variable
utype
is
used
to
keep
track
of
the
current
type
stored
in
u
then
one
might
see
code
such
as
if
utype
INT
printf
d
n
u
ival
if
utype
FLOAT
printf
f
n
u
fval
if
utype
STRING
printf
s
n
u
sval
else
printf
bad
type
d
in
utype
n
utype
Unions
may
occur
within
structures
and
arrays
and
vice
versa
The
notation
for
accessing
a
member
of
a
union
in
a
structure
or
vice
versa
is
identical
to
that
for
nested
structures
For
example
in
the
structure
array
defined
by
struct
char
name
int
flags
int
utype
union
int
ival
float
fval
char
sval
u
symtab
NSYM
the
member
ival
is
referred
to
as
symtab
i
u
ival
and
the
first
character
of
the
string
sval
by
either
of
symtab
i
u
sval
symtab
i
u
sval
In
effect
a
union
is
a
structure
in
which
all
members
have
offset
zero
from
the
base
the
structure
is
big
enough
to
hold
the
widest
member
and
the
alignment
is
appropriate
for
all
of
the
types
in
the
union
The
same
operations
are
permitted
on
unions
as
on
structures
assignment
to
or
copying
as
a
unit
taking
the
address
and
accessing
a
member
A
union
may
only
be
initialized
with
a
value
of
the
type
of
its
first
member
thus
union
u
described
above
can
only
be
initialized
with
an
integer
value
The
storage
allocator
in
Chapter
shows
how
a
union
can
be
used
to
force
a
variable
to
be
aligned
on
a
particular
kind
of
storage
boundary
Bit
fields
When
storage
space
is
at
a
premium
it
may
be
necessary
to
pack
several
objects
into
a
single
machine
word
one
common
use
is
a
set
of
single
bit
flags
in
applications
like
compiler
symbol
tables
Externally
imposed
data
formats
such
as
interfaces
to
hardware
devices
also
often
require
the
ability
to
get
at
pieces
of
a
word
Imagine
a
fragment
of
a
compiler
that
manipulates
a
symbol
table
Each
identifier
in
a
program
has
certain
information
associated
with
it
for
example
whether
or
not
it
is
a
keyword
whether
or
not
it
is
external
and
or
static
and
so
on
The
most
compact
way
to
encode
such
information
is
a
set
of
one
bit
flags
in
a
single
char
or
int
The
usual
way
this
is
done
is
to
define
a
set
of
masks
corresponding
to
the
relevant
bit
positions
as
in
define
KEYWORD
define
EXTRENAL
define
STATIC
or
enum
KEYWORD
EXTERNAL
STATIC
The
numbers
must
be
powers
of
two
Then
accessing
the
bits
becomes
a
matter
of
bitfiddling
with
the
shifting
masking
and
complementing
operators
that
were
described
in
Chapter
Certain
idioms
appear
frequently
flags
EXTERNAL
STATIC
turns
on
the
EXTERNAL
and
STATIC
bits
in
flags
while
flags
EXTERNAL
STATIC
turns
them
off
and
if
flags
EXTERNAL
STATIC
is
true
if
both
bits
are
off
Although
these
idioms
are
readily
mastered
as
an
alternative
C
offers
the
capability
of
defining
and
accessing
fields
within
a
word
directly
rather
than
by
bitwise
logical
operators
A
bit
field
or
field
for
short
is
a
set
of
adjacent
bits
within
a
single
implementation
defined
storage
unit
that
we
will
call
a
word
For
example
the
symbol
table
defines
above
could
be
replaced
by
the
definition
of
three
fields
struct
unsigned
int
is
keyword
unsigned
int
is
extern
unsigned
int
is
static
flags
This
defines
a
variable
table
called
flags
that
contains
three
bit
fields
The
number
following
the
colon
represents
the
field
width
in
bits
The
fields
are
declared
unsigned
int
to
ensure
that
they
are
unsigned
quantities
Individual
fields
are
referenced
in
the
same
way
as
other
structure
members
flags
is
keyword
flags
is
extern
etc
Fields
behave
like
small
integers
and
may
participate
in
arithmetic
expressions
just
like
other
integers
Thus
the
previous
examples
may
be
written
more
naturally
as
flags
is
extern
flags
is
static
to
turn
the
bits
on
flags
is
extern
flags
is
static
to
turn
them
off
and
if
flags
is
extern
flags
is
static
to
test
them
Almost
everything
about
fields
is
implementation
dependent
Whether
a
field
may
overlap
a
word
boundary
is
implementation
defined
Fields
need
not
be
names
unnamed
fields
a
colon
and
width
only
are
used
for
padding
The
special
width
may
be
used
to
force
alignment
at
the
next
word
boundary
Fields
are
assigned
left
to
right
on
some
machines
and
right
to
left
on
others
This
means
that
although
fields
are
useful
for
maintaining
internally
defined
data
structures
the
question
of
which
end
comes
first
has
to
be
carefully
considered
when
picking
apart
externally
defined
data
programs
that
depend
on
such
things
are
not
portable
Fields
may
be
declared
only
as
ints
for
portability
specify
signed
or
unsigned
explicitly
They
are
not
arrays
and
they
do
not
have
addresses
so
the
operator
cannot
be
applied
on
them
Chapter
Input
and
Output
Input
and
output
are
not
part
of
the
C
language
itself
so
we
have
not
emphasized
them
in
our
presentation
thus
far
Nonetheless
programs
interact
with
their
environment
in
much
more
complicated
ways
than
those
we
have
shown
before
In
this
chapter
we
will
describe
the
standard
library
a
set
of
functions
that
provide
input
and
output
string
handling
storage
management
mathematical
routines
and
a
variety
of
other
services
for
C
programs
We
will
concentrate
on
input
and
output
The
ANSI
standard
defines
these
library
functions
precisely
so
that
they
can
exist
in
compatible
form
on
any
system
where
C
exists
Programs
that
confine
their
system
interactions
to
facilities
provided
by
the
standard
library
can
be
moved
from
one
system
to
another
without
change
The
properties
of
library
functions
are
specified
in
more
than
a
dozen
headers
we
have
already
seen
several
of
these
including
stdio
h
string
h
and
ctype
h
We
will
not
present
the
entire
library
here
since
we
are
more
interested
in
writing
C
programs
that
use
it
The
library
is
described
in
detail
in
Appendix
B
Standard
Input
and
Output
As
we
said
in
Chapter
the
library
implements
a
simple
model
of
text
input
and
output
A
text
stream
consists
of
a
sequence
of
lines
each
line
ends
with
a
newline
character
If
the
system
doesn
t
operate
that
way
the
library
does
whatever
necessary
to
make
it
appear
as
if
it
does
For
instance
the
library
might
convert
carriage
return
and
linefeed
to
newline
on
input
and
back
again
on
output
The
simplest
input
mechanism
is
to
read
one
character
at
a
time
from
the
standard
input
normally
the
keyboard
with
getchar
int
getchar
void
getchar
returns
the
next
input
character
each
time
it
is
called
or
EOF
when
it
encounters
end
of
file
The
symbolic
constant
EOF
is
defined
in
stdio
h
The
value
is
typically
bus
tests
should
be
written
in
terms
of
EOF
so
as
to
be
independent
of
the
specific
value
In
many
environments
a
file
may
be
substituted
for
the
keyboard
by
using
the
convention
for
input
redirection
if
a
program
prog
uses
getchar
then
the
command
line
prog
infile
causes
prog
to
read
characters
from
infile
instead
The
switching
of
the
input
is
done
in
such
a
way
that
prog
itself
is
oblivious
to
the
change
in
particular
the
string
infile
is
not
included
in
the
command
line
arguments
in
argv
Input
switching
is
also
invisible
if
the
input
comes
from
another
program
via
a
pipe
mechanism
on
some
systems
the
command
line
otherprog
prog
runs
the
two
programs
otherprog
and
prog
and
pipes
the
standard
output
of
otherprog
into
the
standard
input
for
prog
The
function
int
putchar
int
is
used
for
output
putchar
c
puts
the
character
c
on
the
standard
output
which
is
by
default
the
screen
putchar
returns
the
character
written
or
EOF
is
an
error
occurs
Again
output
can
usually
be
directed
to
a
file
with
filename
if
prog
uses
putchar
prog
outfile
will
write
the
standard
output
to
outfile
instead
If
pipes
are
supported
prog
anotherprog
puts
the
standard
output
of
prog
into
the
standard
input
of
anotherprog
Output
produced
by
printf
also
finds
its
way
to
the
standard
output
Calls
to
putchar
and
printf
may
be
interleaved
output
happens
in
the
order
in
which
the
calls
are
made
Each
source
file
that
refers
to
an
input
output
library
function
must
contain
the
line
include
stdio
h
before
the
first
reference
When
the
name
is
bracketed
by
and
a
search
is
made
for
the
header
in
a
standard
set
of
places
for
example
on
UNIX
systems
typically
in
the
directory
usr
include
Many
programs
read
only
one
input
stream
and
write
only
one
output
stream
for
such
programs
input
and
output
with
getchar
putchar
and
printf
may
be
entirely
adequate
and
is
certainly
enough
to
get
started
This
is
particularly
true
if
redirection
is
used
to
connect
the
output
of
one
program
to
the
input
of
the
next
For
example
consider
the
program
lower
which
converts
its
input
to
lower
case
include
stdio
h
include
ctype
h
main
lower
convert
input
to
lower
case
int
c
while
c
getchar
EOF
putchar
tolower
c
return
The
function
tolower
is
defined
in
ctype
h
it
converts
an
upper
case
letter
to
lower
case
and
returns
other
characters
untouched
As
we
mentioned
earlier
functions
like
getchar
and
putchar
in
stdio
h
and
tolower
in
ctype
h
are
often
macros
thus
avoiding
the
overhead
of
a
function
call
per
character
We
will
show
how
this
is
done
in
Section
Regardless
of
how
the
ctype
h
functions
are
implemented
on
a
given
machine
programs
that
use
them
are
shielded
from
knowledge
of
the
character
set
Exercise
Write
a
program
that
converts
upper
case
to
lower
or
lower
case
to
upper
depending
on
the
name
it
is
invoked
with
as
found
in
argv
Formatted
Output
printf
The
output
function
printf
translates
internal
values
to
characters
We
have
used
printf
informally
in
previous
chapters
The
description
here
covers
most
typical
uses
but
is
not
complete
for
the
full
story
see
Appendix
B
int
printf
char
format
arg
arg
printf
converts
formats
and
prints
its
arguments
on
the
standard
output
under
control
of
the
format
It
returns
the
number
of
characters
printed
The
format
string
contains
two
types
of
objects
ordinary
characters
which
are
copied
to
the
output
stream
and
conversion
specifications
each
of
which
causes
conversion
and
printing
of
the
next
successive
argument
to
printf
Each
conversion
specification
begins
with
a
and
ends
with
a
conversion
character
Between
the
and
the
conversion
character
there
may
be
in
order
A
minus
sign
which
specifies
left
adjustment
of
the
converted
argument
A
number
that
specifies
the
minimum
field
width
The
converted
argument
will
be
printed
in
a
field
at
least
this
wide
If
necessary
it
will
be
padded
on
the
left
or
right
if
left
adjustment
is
called
for
to
make
up
the
field
width
A
period
which
separates
the
field
width
from
the
precision
A
number
the
precision
that
specifies
the
maximum
number
of
characters
to
be
printed
from
a
string
or
the
number
of
digits
after
the
decimal
point
of
a
floating
point
value
or
the
minimum
number
of
digits
for
an
integer
An
h
if
the
integer
is
to
be
printed
as
a
short
or
l
letter
ell
if
as
a
long
Conversion
characters
are
shown
in
Table
If
the
character
after
the
is
not
a
conversion
specification
the
behavior
is
undefined
Table
Basic
Printf
Conversions
Character
Argument
type
Printed
As
d
i
int
decimal
number
o
int
unsigned
octal
number
without
a
leading
zero
x
X
int
unsigned
hexadecimal
number
without
a
leading
x
or
X
using
abcdef
or
ABCDEF
for
u
int
unsigned
decimal
number
c
int
single
character
s
char
print
characters
from
the
string
until
a
or
the
number
of
characters
given
by
the
precision
f
double
m
dddddd
where
the
number
of
d
s
is
given
by
the
precision
default
e
E
double
m
dddddde
xx
or
m
ddddddE
xx
where
the
number
of
d
s
is
given
by
the
precision
default
g
G
double
use
e
or
E
if
the
exponent
is
less
than
or
greater
than
or
equal
to
the
precision
otherwise
use
f
Trailing
zeros
and
a
trailing
decimal
point
are
not
printed
p
void
pointer
implementation
dependent
representation
no
argument
is
converted
print
a
A
width
or
precision
may
be
specified
as
in
which
case
the
value
is
computed
by
converting
the
next
argument
which
must
be
an
int
For
example
to
print
at
most
max
characters
from
a
string
s
printf
s
max
s
Most
of
the
format
conversions
have
been
illustrated
in
earlier
chapters
One
exception
is
the
precision
as
it
relates
to
strings
The
following
table
shows
the
effect
of
a
variety
of
specifications
in
printing
hello
world
characters
We
have
put
colons
around
each
field
so
you
can
see
it
extent
s
hello
world
s
hello
world
s
hello
wor
s
hello
world
s
hello
world
s
hello
world
s
hello
wor
s
hello
wor
A
warning
printf
uses
its
first
argument
to
decide
how
many
arguments
follow
and
what
their
type
is
It
will
get
confused
and
you
will
get
wrong
answers
if
there
are
not
enough
arguments
of
if
they
are
the
wrong
type
You
should
also
be
aware
of
the
difference
between
these
two
calls
printf
s
FAILS
if
s
contains
printf
s
s
SAFE
The
function
sprintf
does
the
same
conversions
as
printf
does
but
stores
the
output
in
a
string
int
sprintf
char
string
char
format
arg
arg
sprintf
formats
the
arguments
in
arg
arg
etc
according
to
format
as
before
but
places
the
result
in
string
instead
of
the
standard
output
string
must
be
big
enough
to
receive
the
result
Exercise
Write
a
program
that
will
print
arbitrary
input
in
a
sensible
way
As
a
minimum
it
should
print
non
graphic
characters
in
octal
or
hexadecimal
according
to
local
custom
and
break
long
text
lines
Variable
length
Argument
Lists
This
section
contains
an
implementation
of
a
minimal
version
of
printf
to
show
how
to
write
a
function
that
processes
a
variable
length
argument
list
in
a
portable
way
Since
we
are
mainly
interested
in
the
argument
processing
minprintf
will
process
the
format
string
and
arguments
but
will
call
the
real
printf
to
do
the
format
conversions
The
proper
declaration
for
printf
is
int
printf
char
fmt
where
the
declaration
means
that
the
number
and
types
of
these
arguments
may
vary
The
declaration
can
only
appear
at
the
end
of
an
argument
list
Our
minprintf
is
declared
as
void
minprintf
char
fmt
since
we
will
not
return
the
character
count
that
printf
does
The
tricky
bit
is
how
minprintf
walks
along
the
argument
list
when
the
list
doesn
t
even
have
a
name
The
standard
header
stdarg
h
contains
a
set
of
macro
definitions
that
define
how
to
step
through
an
argument
list
The
implementation
of
this
header
will
vary
from
machine
to
machine
but
the
interface
it
presents
is
uniform
The
type
va
list
is
used
to
declare
a
variable
that
will
refer
to
each
argument
in
turn
in
minprintf
this
variable
is
called
ap
for
argument
pointer
The
macro
va
start
initializes
ap
to
point
to
the
first
unnamed
argument
It
must
be
called
once
before
ap
is
used
There
must
be
at
least
one
named
argument
the
final
named
argument
is
used
by
va
start
to
get
started
Each
call
of
va
arg
returns
one
argument
and
steps
ap
to
the
next
va
arg
uses
a
type
name
to
determine
what
type
to
return
and
how
big
a
step
to
take
Finally
va
end
does
whatever
cleanup
is
necessary
It
must
be
called
before
the
program
returns
These
properties
form
the
basis
of
our
simplified
printf
include
stdarg
h
minprintf
minimal
printf
with
variable
argument
list
void
minprintf
char
fmt
va
list
ap
points
to
each
unnamed
arg
in
turn
char
p
sval
int
ival
double
dval
va
start
ap
fmt
make
ap
point
to
st
unnamed
arg
for
p
fmt
p
p
if
p
putchar
p
continue
switch
p
case
d
ival
va
arg
ap
int
printf
d
ival
break
case
f
dval
va
arg
ap
double
printf
f
dval
break
case
s
for
sval
va
arg
ap
char
sval
sval
putchar
sval
break
default
putchar
p
break
va
end
ap
clean
up
when
done
Exercise
Revise
minprintf
to
handle
more
of
the
other
facilities
of
printf
Formatted
Input
Scanf
The
function
scanf
is
the
input
analog
of
printf
providing
many
of
the
same
conversion
facilities
in
the
opposite
direction
int
scanf
char
format
scanf
reads
characters
from
the
standard
input
interprets
them
according
to
the
specification
in
format
and
stores
the
results
through
the
remaining
arguments
The
format
argument
is
described
below
the
other
arguments
each
of
which
must
be
a
pointer
indicate
where
the
corresponding
converted
input
should
be
stored
As
with
printf
this
section
is
a
summary
of
the
most
useful
features
not
an
exhaustive
list
scanf
stops
when
it
exhausts
its
format
string
or
when
some
input
fails
to
match
the
control
specification
It
returns
as
its
value
the
number
of
successfully
matched
and
assigned
input
items
This
can
be
used
to
decide
how
many
items
were
found
On
the
end
of
file
EOF
is
returned
note
that
this
is
different
from
which
means
that
the
next
input
character
does
not
match
the
first
specification
in
the
format
string
The
next
call
to
scanf
resumes
searching
immediately
after
the
last
character
already
converted
There
is
also
a
function
sscanf
that
reads
from
a
string
instead
of
the
standard
input
int
sscanf
char
string
char
format
arg
arg
It
scans
the
string
according
to
the
format
in
format
and
stores
the
resulting
values
through
arg
arg
etc
These
arguments
must
be
pointers
The
format
string
usually
contains
conversion
specifications
which
are
used
to
control
conversion
of
input
The
format
string
may
contain
Blanks
or
tabs
which
are
not
ignored
Ordinary
characters
not
which
are
expected
to
match
the
next
non
white
space
character
of
the
input
stream
Conversion
specifications
consisting
of
the
character
an
optional
assignment
suppression
character
an
optional
number
specifying
a
maximum
field
width
an
optional
h
l
or
L
indicating
the
width
of
the
target
and
a
conversion
character
A
conversion
specification
directs
the
conversion
of
the
next
input
field
Normally
the
result
is
places
in
the
variable
pointed
to
by
the
corresponding
argument
If
assignment
suppression
is
indicated
by
the
character
however
the
input
field
is
skipped
no
assignment
is
made
An
input
field
is
defined
as
a
string
of
non
white
space
characters
it
extends
either
to
the
next
white
space
character
or
until
the
field
width
is
specified
is
exhausted
This
implies
that
scanf
will
read
across
boundaries
to
find
its
input
since
newlines
are
white
space
White
space
characters
are
blank
tab
newline
carriage
return
vertical
tab
and
formfeed
The
conversion
character
indicates
the
interpretation
of
the
input
field
The
corresponding
argument
must
be
a
pointer
as
required
by
the
call
by
value
semantics
of
C
Conversion
characters
are
shown
in
Table
Table
Basic
Scanf
Conversions
Character
Input
Data
Argument
type
d
decimal
integer
int
i
integer
int
The
integer
may
be
in
octal
leading
or
hexadecimal
leading
x
or
X
o
octal
integer
with
or
without
leading
zero
int
u
unsigned
decimal
integer
unsigned
int
x
hexadecimal
integer
with
or
without
leading
x
or
X
int
c
characters
char
The
next
input
characters
default
are
placed
at
the
indicated
spot
The
normal
skip
over
white
space
is
suppressed
to
read
the
next
non
white
space
character
use
s
s
character
string
not
quoted
char
pointing
to
an
array
of
characters
long
enough
for
the
string
and
a
terminating
that
will
be
added
e
f
g
floating
point
number
with
optional
sign
optional
decimal
point
and
optional
exponent
float
literal
no
assignment
is
made
The
conversion
characters
d
i
o
u
and
x
may
be
preceded
by
h
to
indicate
that
a
pointer
to
short
rather
than
int
appears
in
the
argument
list
or
by
l
letter
ell
to
indicate
that
a
pointer
to
long
appears
in
the
argument
list
As
a
first
example
the
rudimentary
calculator
of
Chapter
can
be
written
with
scanf
to
do
the
input
conversion
include
stdio
h
main
rudimentary
calculator
double
sum
v
sum
while
scanf
lf
v
printf
t
f
n
sum
v
return
Suppose
we
want
to
read
input
lines
that
contain
dates
of
the
form
Dec
The
scanf
statement
is
int
day
year
char
monthname
scanf
d
s
d
day
monthname
year
No
is
used
with
monthname
since
an
array
name
is
a
pointer
Literal
characters
can
appear
in
the
scanf
format
string
they
must
match
the
same
characters
in
the
input
So
we
could
read
dates
of
the
form
mm
dd
yy
with
the
scanf
statement
int
day
month
year
scanf
d
d
d
month
day
year
scanf
ignores
blanks
and
tabs
in
its
format
string
Furthermore
it
skips
over
white
space
blanks
tabs
newlines
etc
as
it
looks
for
input
values
To
read
input
whose
format
is
not
fixed
it
is
often
best
to
read
a
line
at
a
time
then
pick
it
apart
with
scanf
For
example
suppose
we
want
to
read
lines
that
might
contain
a
date
in
either
of
the
forms
above
Then
we
could
write
while
getline
line
sizeof
line
if
sscanf
line
d
s
d
day
monthname
year
printf
valid
s
n
line
Dec
form
else
if
sscanf
line
d
d
d
month
day
year
printf
valid
s
n
line
mm
dd
yy
form
else
printf
invalid
s
n
line
invalid
form
Calls
to
scanf
can
be
mixed
with
calls
to
other
input
functions
The
next
call
to
any
input
function
will
begin
by
reading
the
first
character
not
read
by
scanf
A
final
warning
the
arguments
to
scanf
and
sscanf
must
be
pointers
By
far
the
most
common
error
is
writing
scanf
d
n
instead
of
scanf
d
n
This
error
is
not
generally
detected
at
compile
time
Exercise
Write
a
private
version
of
scanf
analogous
to
minprintf
from
the
previous
section
Exercise
Rewrite
the
postfix
calculator
of
Chapter
to
use
scanf
and
or
sscanf
to
do
the
input
and
number
conversion
File
Access
The
examples
so
far
have
all
read
the
standard
input
and
written
the
standard
output
which
are
automatically
defined
for
a
program
by
the
local
operating
system
The
next
step
is
to
write
a
program
that
accesses
a
file
that
is
not
already
connected
to
the
program
One
program
that
illustrates
the
need
for
such
operations
is
cat
which
concatenates
a
set
of
named
files
into
the
standard
output
cat
is
used
for
printing
files
on
the
screen
and
as
a
general
purpose
input
collector
for
programs
that
do
not
have
the
capability
of
accessing
files
by
name
For
example
the
command
cat
x
c
y
c
prints
the
contents
of
the
files
x
c
and
y
c
and
nothing
else
on
the
standard
output
The
question
is
how
to
arrange
for
the
named
files
to
be
read
that
is
how
to
connect
the
external
names
that
a
user
thinks
of
to
the
statements
that
read
the
data
The
rules
are
simple
Before
it
can
be
read
or
written
a
file
has
to
be
opened
by
the
library
function
fopen
fopen
takes
an
external
name
like
x
c
or
y
c
does
some
housekeeping
and
negotiation
with
the
operating
system
details
of
which
needn
t
concern
us
and
returns
a
pointer
to
be
used
in
subsequent
reads
or
writes
of
the
file
This
pointer
called
the
file
pointer
points
to
a
structure
that
contains
information
about
the
file
such
as
the
location
of
a
buffer
the
current
character
position
in
the
buffer
whether
the
file
is
being
read
or
written
and
whether
errors
or
end
of
file
have
occurred
Users
don
t
need
to
know
the
details
because
the
definitions
obtained
from
stdio
h
include
a
structure
declaration
called
FILE
The
only
declaration
needed
for
a
file
pointer
is
exemplified
by
FILE
fp
FILE
fopen
char
name
char
mode
This
says
that
fp
is
a
pointer
to
a
FILE
and
fopen
returns
a
pointer
to
a
FILE
Notice
that
FILE
is
a
type
name
like
int
not
a
structure
tag
it
is
defined
with
a
typedef
Details
of
how
fopen
can
be
implemented
on
the
UNIX
system
are
given
in
Section
The
call
to
fopen
in
a
program
is
fp
fopen
name
mode
The
first
argument
of
fopen
is
a
character
string
containing
the
name
of
the
file
The
second
argument
is
the
mode
also
a
character
string
which
indicates
how
one
intends
to
use
the
file
Allowable
modes
include
read
r
write
w
and
append
a
Some
systems
distinguish
between
text
and
binary
files
for
the
latter
a
b
must
be
appended
to
the
mode
string
If
a
file
that
does
not
exist
is
opened
for
writing
or
appending
it
is
created
if
possible
Opening
an
existing
file
for
writing
causes
the
old
contents
to
be
discarded
while
opening
for
appending
preserves
them
Trying
to
read
a
file
that
does
not
exist
is
an
error
and
there
may
be
other
causes
of
error
as
well
like
trying
to
read
a
file
when
you
don
t
have
permission
If
there
is
any
error
fopen
will
return
NULL
The
error
can
be
identified
more
precisely
see
the
discussion
of
error
handling
functions
at
the
end
of
Section
in
Appendix
B
The
next
thing
needed
is
a
way
to
read
or
write
the
file
once
it
is
open
getc
returns
the
next
character
from
a
file
it
needs
the
file
pointer
to
tell
it
which
file
int
getc
FILE
fp
getc
returns
the
next
character
from
the
stream
referred
to
by
fp
it
returns
EOF
for
end
of
file
or
error
putc
is
an
output
function
int
putc
int
c
FILE
fp
putc
writes
the
character
c
to
the
file
fp
and
returns
the
character
written
or
EOF
if
an
error
occurs
Like
getchar
and
putchar
getc
and
putc
may
be
macros
instead
of
functions
When
a
C
program
is
started
the
operating
system
environment
is
responsible
for
opening
three
files
and
providing
pointers
for
them
These
files
are
the
standard
input
the
standard
output
and
the
standard
error
the
corresponding
file
pointers
are
called
stdin
stdout
and
stderr
and
are
declared
in
stdio
h
Normally
stdin
is
connected
to
the
keyboard
and
stdout
and
stderr
are
connected
to
the
screen
but
stdin
and
stdout
may
be
redirected
to
files
or
pipes
as
described
in
Section
getchar
and
putchar
can
be
defined
in
terms
of
getc
putc
stdin
and
stdout
as
follows
define
getchar
getc
stdin
define
putchar
c
putc
c
stdout
For
formatted
input
or
output
of
files
the
functions
fscanf
and
fprintf
may
be
used
These
are
identical
to
scanf
and
printf
except
that
the
first
argument
is
a
file
pointer
that
specifies
the
file
to
be
read
or
written
the
format
string
is
the
second
argument
int
fscanf
FILE
fp
char
format
int
fprintf
FILE
fp
char
format
With
these
preliminaries
out
of
the
way
we
are
now
in
a
position
to
write
the
program
cat
to
concatenate
files
The
design
is
one
that
has
been
found
convenient
for
many
programs
If
there
are
command
line
arguments
they
are
interpreted
as
filenames
and
processed
in
order
If
there
are
no
arguments
the
standard
input
is
processed
include
stdio
h
cat
concatenate
files
version
main
int
argc
char
argv
FILE
fp
void
filecopy
FILE
FILE
if
argc
no
args
copy
standard
input
filecopy
stdin
stdout
else
while
argc
if
fp
fopen
argv
r
NULL
printf
cat
can
t
open
s
n
argv
return
else
filecopy
fp
stdout
fclose
fp
return
filecopy
copy
file
ifp
to
file
ofp
void
filecopy
FILE
ifp
FILE
ofp
int
c
while
c
getc
ifp
EOF
putc
c
ofp
The
file
pointers
stdin
and
stdout
are
objects
of
type
FILE
They
are
constants
however
not
variables
so
it
is
not
possible
to
assign
to
them
The
function
int
fclose
FILE
fp
is
the
inverse
of
fopen
it
breaks
the
connection
between
the
file
pointer
and
the
external
name
that
was
established
by
fopen
freeing
the
file
pointer
for
another
file
Since
most
operating
systems
have
some
limit
on
the
number
of
files
that
a
program
may
have
open
simultaneously
it
s
a
good
idea
to
free
the
file
pointers
when
they
are
no
longer
needed
as
we
did
in
cat
There
is
also
another
reason
for
fclose
on
an
output
file
it
flushes
the
buffer
in
which
putc
is
collecting
output
fclose
is
called
automatically
for
each
open
file
when
a
program
terminates
normally
You
can
close
stdin
and
stdout
if
they
are
not
needed
They
can
also
be
reassigned
by
the
library
function
freopen
Error
Handling
Stderr
and
Exit
The
treatment
of
errors
in
cat
is
not
ideal
The
trouble
is
that
if
one
of
the
files
can
t
be
accessed
for
some
reason
the
diagnostic
is
printed
at
the
end
of
the
concatenated
output
That
might
be
acceptable
if
the
output
is
going
to
a
screen
but
not
if
it
s
going
into
a
file
or
into
another
program
via
a
pipeline
To
handle
this
situation
better
a
second
output
stream
called
stderr
is
assigned
to
a
program
in
the
same
way
that
stdin
and
stdout
are
Output
written
on
stderr
normally
appears
on
the
screen
even
if
the
standard
output
is
redirected
Let
us
revise
cat
to
write
its
error
messages
on
the
standard
error
include
stdio
h
cat
concatenate
files
version
main
int
argc
char
argv
FILE
fp
void
filecopy
FILE
FILE
char
prog
argv
program
name
for
errors
if
argc
no
args
copy
standard
input
filecopy
stdin
stdout
else
while
argc
if
fp
fopen
argv
r
NULL
fprintf
stderr
s
can
t
open
s
n
prog
argv
exit
else
filecopy
fp
stdout
fclose
fp
if
ferror
stdout
fprintf
stderr
s
error
writing
stdout
n
prog
exit
exit
The
program
signals
errors
in
two
ways
First
the
diagnostic
output
produced
by
fprintf
goes
to
stderr
so
it
finds
its
way
to
the
screen
instead
of
disappearing
down
a
pipeline
or
into
an
output
file
We
included
the
program
name
from
argv
in
the
message
so
if
this
program
is
used
with
others
the
source
of
an
error
is
identified
Second
the
program
uses
the
standard
library
function
exit
which
terminates
program
execution
when
it
is
called
The
argument
of
exit
is
available
to
whatever
process
called
this
one
so
the
success
or
failure
of
the
program
can
be
tested
by
another
program
that
uses
this
one
as
a
sub
process
Conventionally
a
return
value
of
signals
that
all
is
well
non
zero
values
usually
signal
abnormal
situations
exit
calls
fclose
for
each
open
output
file
to
flush
out
any
buffered
output
Within
main
return
expr
is
equivalent
to
exit
expr
exit
has
the
advantage
that
it
can
be
called
from
other
functions
and
that
calls
to
it
can
be
found
with
a
pattern
searching
program
like
those
in
Chapter
The
function
ferror
returns
non
zero
if
an
error
occurred
on
the
stream
fp
int
ferror
FILE
fp
Although
output
errors
are
rare
they
do
occur
for
example
if
a
disk
fills
up
so
a
production
program
should
check
this
as
well
The
function
feof
FILE
is
analogous
to
ferror
it
returns
non
zero
if
end
of
file
has
occurred
on
the
specified
file
int
feof
FILE
fp
We
have
generally
not
worried
about
exit
status
in
our
small
illustrative
programs
but
any
serious
program
should
take
care
to
return
sensible
useful
status
values
Line
Input
and
Output
The
standard
library
provides
an
input
and
output
routine
fgets
that
is
similar
to
the
getline
function
that
we
have
used
in
earlier
chapters
char
fgets
char
line
int
maxline
FILE
fp
fgets
reads
the
next
input
line
including
the
newline
from
file
fp
into
the
character
array
line
at
most
maxline
characters
will
be
read
The
resulting
line
is
terminated
with
Normally
fgets
returns
line
on
end
of
file
or
error
it
returns
NULL
Our
getline
returns
the
line
length
which
is
a
more
useful
value
zero
means
end
of
file
For
output
the
function
fputs
writes
a
string
which
need
not
contain
a
newline
to
a
file
int
fputs
char
line
FILE
fp
It
returns
EOF
if
an
error
occurs
and
non
negative
otherwise
The
library
functions
gets
and
puts
are
similar
to
fgets
and
fputs
but
operate
on
stdin
and
stdout
Confusingly
gets
deletes
the
terminating
n
and
puts
adds
it
To
show
that
there
is
nothing
special
about
functions
like
fgets
and
fputs
here
they
are
copied
from
the
standard
library
on
our
system
fgets
get
at
most
n
chars
from
iop
char
fgets
char
s
int
n
FILE
iop
register
int
c
register
char
cs
cs
s
while
n
c
getc
iop
EOF
if
cs
c
n
break
cs
return
c
EOF
cs
s
NULL
s
fputs
put
string
s
on
file
iop
int
fputs
char
s
FILE
iop
int
c
while
c
s
putc
c
iop
return
ferror
iop
EOF
For
no
obvious
reason
the
standard
specifies
different
return
values
for
ferror
and
fputs
It
is
easy
to
implement
our
getline
from
fgets
getline
read
a
line
return
length
int
getline
char
line
int
max
if
fgets
line
max
stdin
NULL
return
else
return
strlen
line
Exercise
Write
a
program
to
compare
two
files
printing
the
first
line
where
they
differ
Exercise
Modify
the
pattern
finding
program
of
Chapter
to
take
its
input
from
a
set
of
named
files
or
if
no
files
are
named
as
arguments
from
the
standard
input
Should
the
file
name
be
printed
when
a
matching
line
is
found
Exercise
Write
a
program
to
print
a
set
of
files
starting
each
new
one
on
a
new
page
with
a
title
and
a
running
page
count
for
each
file
Miscellaneous
Functions
The
standard
library
provides
a
wide
variety
of
functions
This
section
is
a
brief
synopsis
of
the
most
useful
More
details
and
many
other
functions
can
be
found
in
Appendix
B
String
Operations
We
have
already
mentioned
the
string
functions
strlen
strcpy
strcat
and
strcmp
found
in
string
h
In
the
following
s
and
t
are
char
s
and
c
and
n
are
ints
strcat
s
t
concatenate
t
to
end
of
s
strncat
s
t
n
concatenate
n
characters
of
t
to
end
of
s
strcmp
s
t
return
negative
zero
or
positive
for
s
t
s
t
s
t
strncmp
s
t
n
same
as
strcmp
but
only
in
first
n
characters
strcpy
s
t
copy
t
to
s
strncpy
s
t
n
copy
at
most
n
characters
of
t
to
s
strlen
s
return
length
of
s
strchr
s
c
return
pointer
to
first
c
in
s
or
NULL
if
not
present
strrchr
s
c
return
pointer
to
last
c
in
s
or
NULL
if
not
present
Character
Class
Testing
and
Conversion
Several
functions
from
ctype
h
perform
character
tests
and
conversions
In
the
following
c
is
an
int
that
can
be
represented
as
an
unsigned
char
or
EOF
The
function
returns
int
isalpha
c
non
zero
if
c
is
alphabetic
if
not
isupper
c
non
zero
if
c
is
upper
case
if
not
islower
c
non
zero
if
c
is
lower
case
if
not
isdigit
c
non
zero
if
c
is
digit
if
not
isalnum
c
non
zero
if
isalpha
c
or
isdigit
c
if
not
isspace
c
non
zero
if
c
is
blank
tab
newline
return
formfeed
vertical
tab
toupper
c
return
c
converted
to
upper
case
tolower
c
return
c
converted
to
lower
case
Ungetc
The
standard
library
provides
a
rather
restricted
version
of
the
function
ungetch
that
we
wrote
in
Chapter
it
is
called
ungetc
int
ungetc
int
c
FILE
fp
pushes
the
character
c
back
onto
file
fp
and
returns
either
c
or
EOF
for
an
error
Only
one
character
of
pushback
is
guaranteed
per
file
ungetc
may
be
used
with
any
of
the
input
functions
like
scanf
getc
or
getchar
Command
Execution
The
function
system
char
s
executes
the
command
contained
in
the
character
string
s
then
resumes
execution
of
the
current
program
The
contents
of
s
depend
strongly
on
the
local
operating
system
As
a
trivial
example
on
UNIX
systems
the
statement
system
date
causes
the
program
date
to
be
run
it
prints
the
date
and
time
of
day
on
the
standard
output
system
returns
a
system
dependent
integer
status
from
the
command
executed
In
the
UNIX
system
the
status
return
is
the
value
returned
by
exit
Storage
Management
The
functions
malloc
and
calloc
obtain
blocks
of
memory
dynamically
void
malloc
size
t
n
returns
a
pointer
to
n
bytes
of
uninitialized
storage
or
NULL
if
the
request
cannot
be
satisfied
void
calloc
size
t
n
size
t
size
returns
a
pointer
to
enough
free
space
for
an
array
of
n
objects
of
the
specified
size
or
NULL
if
the
request
cannot
be
satisfied
The
storage
is
initialized
to
zero
The
pointer
returned
by
malloc
or
calloc
has
the
proper
alignment
for
the
object
in
question
but
it
must
be
cast
into
the
appropriate
type
as
in
int
ip
ip
int
calloc
n
sizeof
int
free
p
frees
the
space
pointed
to
by
p
where
p
was
originally
obtained
by
a
call
to
malloc
or
calloc
There
are
no
restrictions
on
the
order
in
which
space
is
freed
but
it
is
a
ghastly
error
to
free
something
not
obtained
by
calling
malloc
or
calloc
It
is
also
an
error
to
use
something
after
it
has
been
freed
A
typical
but
incorrect
piece
of
code
is
this
loop
that
frees
items
from
a
list
for
p
head
p
NULL
p
p
next
WRONG
free
p
The
right
way
is
to
save
whatever
is
needed
before
freeing
for
p
head
p
NULL
p
q
q
p
next
free
p
Section
shows
the
implementation
of
a
storage
allocator
like
malloc
in
which
allocated
blocks
may
be
freed
in
any
order
Mathematical
Functions
There
are
more
than
twenty
mathematical
functions
declared
in
math
h
here
are
some
of
the
more
frequently
used
Each
takes
one
or
two
double
arguments
and
returns
a
double
sin
x
sine
of
x
x
in
radians
cos
x
cosine
of
x
x
in
radians
atan
y
x
arctangent
of
y
x
in
radians
exp
x
exponential
function
e
x
log
x
natural
base
e
logarithm
of
x
x
log
x
common
base
logarithm
of
x
x
pow
x
y
x
y
sqrt
x
square
root
of
x
x
fabs
x
absolute
value
of
x
Random
Number
generation
The
function
rand
computes
a
sequence
of
pseudo
random
integers
in
the
range
zero
to
RAND
MAX
which
is
defined
in
stdlib
h
One
way
to
produce
random
floating
point
numbers
greater
than
or
equal
to
zero
but
less
than
one
is
define
frand
double
rand
RAND
MAX
If
your
library
already
provides
a
function
for
floating
point
random
numbers
it
is
likely
to
have
better
statistical
properties
than
this
one
The
function
srand
unsigned
sets
the
seed
for
rand
The
portable
implementation
of
rand
and
srand
suggested
by
the
standard
appears
in
Section
Exercise
Functions
like
isupper
can
be
implemented
to
save
space
or
to
save
time
Explore
both
possibilities
Chapter
The
UNIX
System
Interface
The
UNIX
operating
system
provides
its
services
through
a
set
of
system
calls
which
are
in
effect
functions
within
the
operating
system
that
may
be
called
by
user
programs
This
chapter
describes
how
to
use
some
of
the
most
important
system
calls
from
C
programs
If
you
use
UNIX
this
should
be
directly
helpful
for
it
is
sometimes
necessary
to
employ
system
calls
for
maximum
efficiency
or
to
access
some
facility
that
is
not
in
the
library
Even
if
you
use
C
on
a
different
operating
system
however
you
should
be
able
to
glean
insight
into
C
programming
from
studying
these
examples
although
details
vary
similar
code
will
be
found
on
any
system
Since
the
ANSI
C
library
is
in
many
cases
modeled
on
UNIX
facilities
this
code
may
help
your
understanding
of
the
library
as
well
This
chapter
is
divided
into
three
major
parts
input
output
file
system
and
storage
allocation
The
first
two
parts
assume
a
modest
familiarity
with
the
external
characteristics
of
UNIX
systems
Chapter
was
concerned
with
an
input
output
interface
that
is
uniform
across
operating
systems
On
any
particular
system
the
routines
of
the
standard
library
have
to
be
written
in
terms
of
the
facilities
provided
by
the
host
system
In
the
next
few
sections
we
will
describe
the
UNIX
system
calls
for
input
and
output
and
show
how
parts
of
the
standard
library
can
be
implemented
with
them
File
Descriptors
In
the
UNIX
operating
system
all
input
and
output
is
done
by
reading
or
writing
files
because
all
peripheral
devices
even
keyboard
and
screen
are
files
in
the
file
system
This
means
that
a
single
homogeneous
interface
handles
all
communication
between
a
program
and
peripheral
devices
In
the
most
general
case
before
you
read
and
write
a
file
you
must
inform
the
system
of
your
intent
to
do
so
a
process
called
opening
the
file
If
you
are
going
to
write
on
a
file
it
may
also
be
necessary
to
create
it
or
to
discard
its
previous
contents
The
system
checks
your
right
to
do
so
Does
the
file
exist
Do
you
have
permission
to
access
it
and
if
all
is
well
returns
to
the
program
a
small
non
negative
integer
called
a
file
descriptor
Whenever
input
or
output
is
to
be
done
on
the
file
the
file
descriptor
is
used
instead
of
the
name
to
identify
the
file
A
file
descriptor
is
analogous
to
the
file
pointer
used
by
the
standard
library
or
to
the
file
handle
of
MS
DOS
All
information
about
an
open
file
is
maintained
by
the
system
the
user
program
refers
to
the
file
only
by
the
file
descriptor
Since
input
and
output
involving
keyboard
and
screen
is
so
common
special
arrangements
exist
to
make
this
convenient
When
the
command
interpreter
the
shell
runs
a
program
three
files
are
open
with
file
descriptors
and
called
the
standard
input
the
standard
output
and
the
standard
error
If
a
program
reads
and
writes
and
it
can
do
input
and
output
without
worrying
about
opening
files
The
user
of
a
program
can
redirect
I
O
to
and
from
files
with
and
prog
infile
outfile
In
this
case
the
shell
changes
the
default
assignments
for
the
file
descriptors
and
to
the
named
files
Normally
file
descriptor
remains
attached
to
the
screen
so
error
messages
can
go
there
Similar
observations
hold
for
input
or
output
associated
with
a
pipe
In
all
cases
the
file
assignments
are
changed
by
the
shell
not
by
the
program
The
program
does
not
know
where
its
input
comes
from
nor
where
its
output
goes
so
long
as
it
uses
file
for
input
and
and
for
output
Low
Level
I
O
Read
and
Write
Input
and
output
uses
the
read
and
write
system
calls
which
are
accessed
from
C
programs
through
two
functions
called
read
and
write
For
both
the
first
argument
is
a
file
descriptor
The
second
argument
is
a
character
array
in
your
program
where
the
data
is
to
go
to
or
to
come
from
The
third
argument
is
the
number
is
the
number
of
bytes
to
be
transferred
int
n
read
read
int
fd
char
buf
int
n
int
n
written
write
int
fd
char
buf
int
n
Each
call
returns
a
count
of
the
number
of
bytes
transferred
On
reading
the
number
of
bytes
returned
may
be
less
than
the
number
requested
A
return
value
of
zero
bytes
implies
end
of
file
and
indicates
an
error
of
some
sort
For
writing
the
return
value
is
the
number
of
bytes
written
an
error
has
occurred
if
this
isn
t
equal
to
the
number
requested
Any
number
of
bytes
can
be
read
or
written
in
one
call
The
most
common
values
are
which
means
one
character
at
a
time
unbuffered
and
a
number
like
or
that
corresponds
to
a
physical
block
size
on
a
peripheral
device
Larger
sizes
will
be
more
efficient
because
fewer
system
calls
will
be
made
Putting
these
facts
together
we
can
write
a
simple
program
to
copy
its
input
to
its
output
the
equivalent
of
the
file
copying
program
written
for
Chapter
This
program
will
copy
anything
to
anything
since
the
input
and
output
can
be
redirected
to
any
file
or
device
include
syscalls
h
main
copy
input
to
output
char
buf
BUFSIZ
int
n
while
n
read
buf
BUFSIZ
write
buf
n
return
We
have
collected
function
prototypes
for
the
system
calls
into
a
file
called
syscalls
h
so
we
can
include
it
in
the
programs
of
this
chapter
This
name
is
not
standard
however
The
parameter
BUFSIZ
is
also
defined
in
syscalls
h
its
value
is
a
good
size
for
the
local
system
If
the
file
size
is
not
a
multiple
of
BUFSIZ
some
read
will
return
a
smaller
number
of
bytes
to
be
written
by
write
the
next
call
to
read
after
that
will
return
zero
It
is
instructive
to
see
how
read
and
write
can
be
used
to
construct
higher
level
routines
like
getchar
putchar
etc
For
example
here
is
a
version
of
getchar
that
does
unbuffered
input
by
reading
the
standard
input
one
character
at
a
time
include
syscalls
h
getchar
unbuffered
single
character
input
int
getchar
void
char
c
return
read
c
unsigned
char
c
EOF
c
must
be
a
char
because
read
needs
a
character
pointer
Casting
c
to
unsigned
char
in
the
return
statement
eliminates
any
problem
of
sign
extension
The
second
version
of
getchar
does
input
in
big
chunks
and
hands
out
the
characters
one
at
a
time
include
syscalls
h
getchar
simple
buffered
version
int
getchar
void
static
char
buf
BUFSIZ
static
char
bufp
buf
static
int
n
if
n
buffer
is
empty
n
read
buf
sizeof
buf
bufp
buf
return
n
unsigned
char
bufp
EOF
If
these
versions
of
getchar
were
to
be
compiled
with
stdio
h
included
it
would
be
necessary
to
undef
the
name
getchar
in
case
it
is
implemented
as
a
macro
Open
Creat
Close
Unlink
Other
than
the
default
standard
input
output
and
error
you
must
explicitly
open
files
in
order
to
read
or
write
them
There
are
two
system
calls
for
this
open
and
creat
sic
open
is
rather
like
the
fopen
discussed
in
Chapter
except
that
instead
of
returning
a
file
pointer
it
returns
a
file
descriptor
which
is
just
an
int
open
returns
if
any
error
occurs
include
fcntl
h
int
fd
int
open
char
name
int
flags
int
perms
fd
open
name
flags
perms
As
with
fopen
the
name
argument
is
a
character
string
containing
the
filename
The
second
argument
flags
is
an
int
that
specifies
how
the
file
is
to
be
opened
the
main
values
are
O
RDONLY
open
for
reading
only
O
WRONLY
open
for
writing
only
O
RDWR
open
for
both
reading
and
writing
These
constants
are
defined
in
fcntl
h
on
System
V
UNIX
systems
and
in
sys
file
h
on
Berkeley
BSD
versions
To
open
an
existing
file
for
reading
fd
open
name
O
RDONLY
The
perms
argument
is
always
zero
for
the
uses
of
open
that
we
will
discuss
It
is
an
error
to
try
to
open
a
file
that
does
not
exist
The
system
call
creat
is
provided
to
create
new
files
or
to
re
write
old
ones
int
creat
char
name
int
perms
fd
creat
name
perms
returns
a
file
descriptor
if
it
was
able
to
create
the
file
and
if
not
If
the
file
already
exists
creat
will
truncate
it
to
zero
length
thereby
discarding
its
previous
contents
it
is
not
an
error
to
creat
a
file
that
already
exists
If
the
file
does
not
already
exist
creat
creates
it
with
the
permissions
specified
by
the
perms
argument
In
the
UNIX
file
system
there
are
nine
bits
of
permission
information
associated
with
a
file
that
control
read
write
and
execute
access
for
the
owner
of
the
file
for
the
owner
s
group
and
for
all
others
Thus
a
three
digit
octal
number
is
convenient
for
specifying
the
permissions
For
example
specifies
read
write
and
execute
permission
for
the
owner
and
read
and
execute
permission
for
the
group
and
everyone
else
To
illustrate
here
is
a
simplified
version
of
the
UNIX
program
cp
which
copies
one
file
to
another
Our
version
copies
only
one
file
it
does
not
permit
the
second
argument
to
be
a
directory
and
it
invents
permissions
instead
of
copying
them
include
stdio
h
include
fcntl
h
include
syscalls
h
define
PERMS
RW
for
owner
group
others
void
error
char
cp
copy
f
to
f
main
int
argc
char
argv
int
f
f
n
char
buf
BUFSIZ
if
argc
error
Usage
cp
from
to
if
f
open
argv
O
RDONLY
error
cp
can
t
open
s
argv
if
f
creat
argv
PERMS
error
cp
can
t
create
s
mode
o
argv
PERMS
while
n
read
f
buf
BUFSIZ
if
write
f
buf
n
n
error
cp
write
error
on
file
s
argv
return
This
program
creates
the
output
file
with
fixed
permissions
of
With
the
stat
system
call
described
in
Section
we
can
determine
the
mode
of
an
existing
file
and
thus
give
the
same
mode
to
the
copy
Notice
that
the
function
error
is
called
with
variable
argument
lists
much
like
printf
The
implementation
of
error
illustrates
how
to
use
another
member
of
the
printf
family
The
standard
library
function
vprintf
is
like
printf
except
that
the
variable
argument
list
is
replaced
by
a
single
argument
that
has
been
initialized
by
calling
the
va
start
macro
Similarly
vfprintf
and
vsprintf
match
fprintf
and
sprintf
include
stdio
h
include
stdarg
h
error
print
an
error
message
and
die
void
error
char
fmt
va
list
args
va
start
args
fmt
fprintf
stderr
error
vprintf
stderr
fmt
args
fprintf
stderr
n
va
end
args
exit
There
is
a
limit
often
about
on
the
number
of
files
that
a
program
may
open
simultaneously
Accordingly
any
program
that
intends
to
process
many
files
must
be
prepared
to
re
use
file
descriptors
The
function
close
int
fd
breaks
the
connection
between
a
file
descriptor
and
an
open
file
and
frees
the
file
descriptor
for
use
with
some
other
file
it
corresponds
to
fclose
in
the
standard
library
except
that
there
is
no
buffer
to
flush
Termination
of
a
program
via
exit
or
return
from
the
main
program
closes
all
open
files
The
function
unlink
char
name
removes
the
file
name
from
the
file
system
It
corresponds
to
the
standard
library
function
remove
Exercise
Rewrite
the
program
cat
from
Chapter
using
read
write
open
and
close
instead
of
their
standard
library
equivalents
Perform
experiments
to
determine
the
relative
speeds
of
the
two
versions
Random
Access
Lseek
Input
and
output
are
normally
sequential
each
read
or
write
takes
place
at
a
position
in
the
file
right
after
the
previous
one
When
necessary
however
a
file
can
be
read
or
written
in
any
arbitrary
order
The
system
call
lseek
provides
a
way
to
move
around
in
a
file
without
reading
or
writing
any
data
long
lseek
int
fd
long
offset
int
origin
sets
the
current
position
in
the
file
whose
descriptor
is
fd
to
offset
which
is
taken
relative
to
the
location
specified
by
origin
Subsequent
reading
or
writing
will
begin
at
that
position
origin
can
be
or
to
specify
that
offset
is
to
be
measured
from
the
beginning
from
the
current
position
or
from
the
end
of
the
file
respectively
For
example
to
append
to
a
file
the
redirection
in
the
UNIX
shell
or
a
for
fopen
seek
to
the
end
before
writing
lseek
fd
L
To
get
back
to
the
beginning
rewind
lseek
fd
L
Notice
the
L
argument
it
could
also
be
written
as
long
or
just
as
if
lseek
is
properly
declared
With
lseek
it
is
possible
to
treat
files
more
or
less
like
arrays
at
the
price
of
slower
access
For
example
the
following
function
reads
any
number
of
bytes
from
any
arbitrary
place
in
a
file
It
returns
the
number
read
or
on
error
include
syscalls
h
get
read
n
bytes
from
position
pos
int
get
int
fd
long
pos
char
buf
int
n
if
lseek
fd
pos
get
to
pos
return
read
fd
buf
n
else
return
The
return
value
from
lseek
is
a
long
that
gives
the
new
position
in
the
file
or
if
an
error
occurs
The
standard
library
function
fseek
is
similar
to
lseek
except
that
the
first
argument
is
a
FILE
and
the
return
is
non
zero
if
an
error
occurred
Example
An
implementation
of
Fopen
and
Getc
Let
us
illustrate
how
some
of
these
pieces
fit
together
by
showing
an
implementation
of
the
standard
library
routines
fopen
and
getc
Recall
that
files
in
the
standard
library
are
described
by
file
pointers
rather
than
file
descriptors
A
file
pointer
is
a
pointer
to
a
structure
that
contains
several
pieces
of
information
about
the
file
a
pointer
to
a
buffer
so
the
file
can
be
read
in
large
chunks
a
count
of
the
number
of
characters
left
in
the
buffer
a
pointer
to
the
next
character
position
in
the
buffer
the
file
descriptor
and
flags
describing
read
write
mode
error
status
etc
The
data
structure
that
describes
a
file
is
contained
in
stdio
h
which
must
be
included
by
include
in
any
source
file
that
uses
routines
from
the
standard
input
output
library
It
is
also
included
by
functions
in
that
library
In
the
following
excerpt
from
a
typical
stdio
h
names
that
are
intended
for
use
only
by
functions
of
the
library
begin
with
an
underscore
so
they
are
less
likely
to
collide
with
names
in
a
user
s
program
This
convention
is
used
by
all
standard
library
routines
define
NULL
define
EOF
define
BUFSIZ
define
OPEN
MAX
max
files
open
at
once
typedef
struct
iobuf
int
cnt
characters
left
char
ptr
next
character
position
char
base
location
of
buffer
int
flag
mode
of
file
access
int
fd
file
descriptor
FILE
extern
FILE
iob
OPEN
MAX
define
stdin
iob
define
stdout
iob
define
stderr
iob
enum
flags
READ
file
open
for
reading
WRITE
file
open
for
writing
UNBUF
file
is
unbuffered
EOF
EOF
has
occurred
on
this
file
ERR
error
occurred
on
this
file
int
fillbuf
FILE
int
flushbuf
int
FILE
define
feof
p
p
flag
EOF
define
ferror
p
p
flag
ERR
define
fileno
p
p
fd
define
getc
p
p
cnt
unsigned
char
p
ptr
fillbuf
p
define
putc
x
p
p
cnt
p
ptr
x
flushbuf
x
p
define
getchar
getc
stdin
define
putcher
x
putc
x
stdout
The
getc
macro
normally
decrements
the
count
advances
the
pointer
and
returns
the
character
Recall
that
a
long
define
is
continued
with
a
backslash
If
the
count
goes
negative
however
getc
calls
the
function
fillbuf
to
replenish
the
buffer
re
initialize
the
structure
contents
and
return
a
character
The
characters
are
returned
unsigned
which
ensures
that
all
characters
will
be
positive
Although
we
will
not
discuss
any
details
we
have
included
the
definition
of
putc
to
show
that
it
operates
in
much
the
same
way
as
getc
calling
a
function
flushbuf
when
its
buffer
is
full
We
have
also
included
macros
for
accessing
the
error
and
end
of
file
status
and
the
file
descriptor
The
function
fopen
can
now
be
written
Most
of
fopen
is
concerned
with
getting
the
file
opened
and
positioned
at
the
right
place
and
setting
the
flag
bits
to
indicate
the
proper
state
fopen
does
not
allocate
any
buffer
space
this
is
done
by
fillbuf
when
the
file
is
first
read
include
fcntl
h
include
syscalls
h
define
PERMS
RW
for
owner
group
others
FILE
fopen
char
name
char
mode
int
fd
FILE
fp
if
mode
r
mode
w
mode
a
return
NULL
for
fp
iob
fp
iob
OPEN
MAX
fp
if
fp
flag
READ
WRITE
break
found
free
slot
if
fp
iob
OPEN
MAX
no
free
slots
return
NULL
if
mode
w
fd
creat
name
PERMS
else
if
mode
a
if
fd
open
name
O
WRONLY
fd
creat
name
PERMS
lseek
fd
L
else
fd
open
name
O
RDONLY
if
fd
couldn
t
access
name
return
NULL
fp
fd
fd
fp
cnt
fp
base
NULL
fp
flag
mode
r
READ
WRITE
return
fp
This
version
of
fopen
does
not
handle
all
of
the
access
mode
possibilities
of
the
standard
though
adding
them
would
not
take
much
code
In
particular
our
fopen
does
not
recognize
the
b
that
signals
binary
access
since
that
is
meaningless
on
UNIX
systems
nor
the
that
permits
both
reading
and
writing
The
first
call
to
getc
for
a
particular
file
finds
a
count
of
zero
which
forces
a
call
of
fillbuf
If
fillbuf
finds
that
the
file
is
not
open
for
reading
it
returns
EOF
immediately
Otherwise
it
tries
to
allocate
a
buffer
if
reading
is
to
be
buffered
Once
the
buffer
is
established
fillbuf
calls
read
to
fill
it
sets
the
count
and
pointers
and
returns
the
character
at
the
beginning
of
the
buffer
Subsequent
calls
to
fillbuf
will
find
a
buffer
allocated
include
syscalls
h
fillbuf
allocate
and
fill
input
buffer
int
fillbuf
FILE
fp
int
bufsize
if
fp
flag
READ
EOF
ERR
READ
return
EOF
bufsize
fp
flag
UNBUF
BUFSIZ
if
fp
base
NULL
no
buffer
yet
if
fp
base
char
malloc
bufsize
NULL
return
EOF
can
t
get
buffer
fp
ptr
fp
base
fp
cnt
read
fp
fd
fp
ptr
bufsize
if
fp
cnt
if
fp
cnt
fp
flag
EOF
else
fp
flag
ERR
fp
cnt
return
EOF
return
unsigned
char
fp
ptr
The
only
remaining
loose
end
is
how
everything
gets
started
The
array
iob
must
be
defined
and
initialized
for
stdin
stdout
and
stderr
FILE
iob
OPEN
MAX
stdin
stdout
stderr
char
char
READ
char
char
WRITE
char
char
WRITE
UNBUF
The
initialization
of
the
flag
part
of
the
structure
shows
that
stdin
is
to
be
read
stdout
is
to
be
written
and
stderr
is
to
be
written
unbuffered
Exercise
Rewrite
fopen
and
fillbuf
with
fields
instead
of
explicit
bit
operations
Compare
code
size
and
execution
speed
Exercise
Design
and
write
flushbuf
fflush
and
fclose
Exercise
The
standard
library
function
int
fseek
FILE
fp
long
offset
int
origin
is
identical
to
lseek
except
that
fp
is
a
file
pointer
instead
of
a
file
descriptor
and
return
value
is
an
int
status
not
a
position
Write
fseek
Make
sure
that
your
fseek
coordinates
properly
with
the
buffering
done
for
the
other
functions
of
the
library
Example
Listing
Directories
A
different
kind
of
file
system
interaction
is
sometimes
called
for
determining
information
about
a
file
not
what
it
contains
A
directory
listing
program
such
as
the
UNIX
command
ls
is
an
example
it
prints
the
names
of
files
in
a
directory
and
optionally
other
information
such
as
sizes
permissions
and
so
on
The
MS
DOS
dir
command
is
analogous
Since
a
UNIX
directory
is
just
a
file
ls
need
only
read
it
to
retrieve
the
filenames
But
is
is
necessary
to
use
a
system
call
to
access
other
information
about
a
file
such
as
its
size
On
other
systems
a
system
call
may
be
needed
even
to
access
filenames
this
is
the
case
on
MSDOS
for
instance
What
we
want
is
provide
access
to
the
information
in
a
relatively
systemindependent
way
even
though
the
implementation
may
be
highly
system
dependent
We
will
illustrate
some
of
this
by
writing
a
program
called
fsize
fsize
is
a
special
form
of
ls
that
prints
the
sizes
of
all
files
named
in
its
commandline
argument
list
If
one
of
the
files
is
a
directory
fsize
applies
itself
recursively
to
that
directory
If
there
are
no
arguments
at
all
it
processes
the
current
directory
Let
us
begin
with
a
short
review
of
UNIX
file
system
structure
A
directory
is
a
file
that
contains
a
list
of
filenames
and
some
indication
of
where
they
are
located
The
location
is
an
index
into
another
table
called
the
inode
list
The
inode
for
a
file
is
where
all
information
about
the
file
except
its
name
is
kept
A
directory
entry
generally
consists
of
only
two
items
the
filename
and
an
inode
number
Regrettably
the
format
and
precise
contents
of
a
directory
are
not
the
same
on
all
versions
of
the
system
So
we
will
divide
the
task
into
two
pieces
to
try
to
isolate
the
non
portable
parts
The
outer
level
defines
a
structure
called
a
Dirent
and
three
routines
opendir
readdir
and
closedir
to
provide
system
independent
access
to
the
name
and
inode
number
in
a
directory
entry
We
will
write
fsize
with
this
interface
Then
we
will
show
how
to
implement
these
on
systems
that
use
the
same
directory
structure
as
Version
and
System
V
UNIX
variants
are
left
as
exercises
The
Dirent
structure
contains
the
inode
number
and
the
name
The
maximum
length
of
a
filename
component
is
NAME
MAX
which
is
a
system
dependent
value
opendir
returns
a
pointer
to
a
structure
called
DIR
analogous
to
FILE
which
is
used
by
readdir
and
closedir
This
information
is
collected
into
a
file
called
dirent
h
define
NAME
MAX
longest
filename
component
system
dependent
typedef
struct
portable
directory
entry
long
ino
inode
number
char
name
NAME
MAX
name
terminator
Dirent
typedef
struct
minimal
DIR
no
buffering
etc
int
fd
file
descriptor
for
the
directory
Dirent
d
the
directory
entry
DIR
DIR
opendir
char
dirname
Dirent
readdir
DIR
dfd
void
closedir
DIR
dfd
The
system
call
stat
takes
a
filename
and
returns
all
of
the
information
in
the
inode
for
that
file
or
if
there
is
an
error
That
is
char
name
struct
stat
stbuf
int
stat
char
struct
stat
stat
name
stbuf
fills
the
structure
stbuf
with
the
inode
information
for
the
file
name
The
structure
describing
the
value
returned
by
stat
is
in
sys
stat
h
and
typically
looks
like
this
struct
stat
inode
information
returned
by
stat
dev
t
st
dev
device
of
inode
ino
t
st
ino
inode
number
short
st
mode
mode
bits
short
st
nlink
number
of
links
to
file
short
st
uid
owners
user
id
short
st
gid
owners
group
id
dev
t
st
rdev
for
special
files
off
t
st
size
file
size
in
characters
time
t
st
atime
time
last
accessed
time
t
st
mtime
time
last
modified
time
t
st
ctime
time
originally
created
Most
of
these
values
are
explained
by
the
comment
fields
The
types
like
dev
t
and
ino
t
are
defined
in
sys
types
h
which
must
be
included
too
The
st
mode
entry
contains
a
set
of
flags
describing
the
file
The
flag
definitions
are
also
included
in
sys
types
h
we
need
only
the
part
that
deals
with
file
type
define
S
IFMT
type
of
file
define
S
IFDIR
directory
define
S
IFCHR
character
special
define
S
IFBLK
block
special
define
S
IFREG
regular
Now
we
are
ready
to
write
the
program
fsize
If
the
mode
obtained
from
stat
indicates
that
a
file
is
not
a
directory
then
the
size
is
at
hand
and
can
be
printed
directly
If
the
name
is
a
directory
however
then
we
have
to
process
that
directory
one
file
at
a
time
it
may
in
turn
contain
sub
directories
so
the
process
is
recursive
The
main
routine
deals
with
command
line
arguments
it
hands
each
argument
to
the
function
fsize
include
stdio
h
include
string
h
include
syscalls
h
include
fcntl
h
flags
for
read
and
write
include
sys
types
h
typedefs
include
sys
stat
h
structure
returned
by
stat
include
dirent
h
void
fsize
char
print
file
name
main
int
argc
char
argv
if
argc
default
current
directory
fsize
else
while
argc
fsize
argv
return
The
function
fsize
prints
the
size
of
the
file
If
the
file
is
a
directory
however
fsize
first
calls
dirwalk
to
handle
all
the
files
in
it
Note
how
the
flag
names
S
IFMT
and
S
IFDIR
are
used
to
decide
if
the
file
is
a
directory
Parenthesization
matters
because
the
precedence
of
is
lower
than
that
of
int
stat
char
struct
stat
void
dirwalk
char
void
fcn
char
fsize
print
the
name
of
file
name
void
fsize
char
name
struct
stat
stbuf
if
stat
name
stbuf
fprintf
stderr
fsize
can
t
access
s
n
name
return
if
stbuf
st
mode
S
IFMT
S
IFDIR
dirwalk
name
fsize
printf
ld
s
n
stbuf
st
size
name
The
function
dirwalk
is
a
general
routine
that
applies
a
function
to
each
file
in
a
directory
It
opens
the
directory
loops
through
the
files
in
it
calling
the
function
on
each
then
closes
the
directory
and
returns
Since
fsize
calls
dirwalk
on
each
directory
the
two
functions
call
each
other
recursively
define
MAX
PATH
dirwalk
apply
fcn
to
all
files
in
dir
void
dirwalk
char
dir
void
fcn
char
char
name
MAX
PATH
Dirent
dp
DIR
dfd
if
dfd
opendir
dir
NULL
fprintf
stderr
dirwalk
can
t
open
s
n
dir
return
while
dp
readdir
dfd
NULL
if
strcmp
dp
name
strcmp
dp
name
continue
skip
self
and
parent
if
strlen
dir
strlen
dp
name
sizeof
name
fprintf
stderr
dirwalk
name
s
s
too
long
n
dir
dp
name
else
sprintf
name
s
s
dir
dp
name
fcn
name
closedir
dfd
Each
call
to
readdir
returns
a
pointer
to
information
for
the
next
file
or
NULL
when
there
are
no
files
left
Each
directory
always
contains
entries
for
itself
called
and
its
parent
these
must
be
skipped
or
the
program
will
loop
forever
Down
to
this
last
level
the
code
is
independent
of
how
directories
are
formatted
The
next
step
is
to
present
minimal
versions
of
opendir
readdir
and
closedir
for
a
specific
system
The
following
routines
are
for
Version
and
System
V
UNIX
systems
they
use
the
directory
information
in
the
header
sys
dir
h
which
looks
like
this
ifndef
DIRSIZ
define
DIRSIZ
endif
struct
direct
directory
entry
ino
t
d
ino
inode
number
char
d
name
DIRSIZ
long
name
does
not
have
Some
versions
of
the
system
permit
much
longer
names
and
have
a
more
complicated
directory
structure
The
type
ino
t
is
a
typedef
that
describes
the
index
into
the
inode
list
It
happens
to
be
unsigned
short
on
the
systems
we
use
regularly
but
this
is
not
the
sort
of
information
to
embed
in
a
program
it
might
be
different
on
a
different
system
so
the
typedef
is
better
A
complete
set
of
system
types
is
found
in
sys
types
h
opendir
opens
the
directory
verifies
that
the
file
is
a
directory
this
time
by
the
system
call
fstat
which
is
like
stat
except
that
it
applies
to
a
file
descriptor
allocates
a
directory
structure
and
records
the
information
int
fstat
int
fd
struct
stat
opendir
open
a
directory
for
readdir
calls
DIR
opendir
char
dirname
int
fd
struct
stat
stbuf
DIR
dp
if
fd
open
dirname
O
RDONLY
fstat
fd
stbuf
stbuf
st
mode
S
IFMT
S
IFDIR
dp
DIR
malloc
sizeof
DIR
NULL
return
NULL
dp
fd
fd
return
dp
closedir
closes
the
directory
file
and
frees
the
space
closedir
close
directory
opened
by
opendir
void
closedir
DIR
dp
if
dp
close
dp
fd
free
dp
Finally
readdir
uses
read
to
read
each
directory
entry
If
a
directory
slot
is
not
currently
in
use
because
a
file
has
been
removed
the
inode
number
is
zero
and
this
position
is
skipped
Otherwise
the
inode
number
and
name
are
placed
in
a
static
structure
and
a
pointer
to
that
is
returned
to
the
user
Each
call
overwrites
the
information
from
the
previous
one
include
sys
dir
h
local
directory
structure
readdir
read
directory
entries
in
sequence
Dirent
readdir
DIR
dp
struct
direct
dirbuf
local
directory
structure
static
Dirent
d
return
portable
structure
while
read
dp
fd
char
dirbuf
sizeof
dirbuf
sizeof
dirbuf
if
dirbuf
d
ino
slot
not
in
use
continue
d
ino
dirbuf
d
ino
strncpy
d
name
dirbuf
d
name
DIRSIZ
d
name
DIRSIZ
ensure
termination
return
d
return
NULL
Although
the
fsize
program
is
rather
specialized
it
does
illustrate
a
couple
of
important
ideas
First
many
programs
are
not
system
programs
they
merely
use
information
that
is
maintained
by
the
operating
system
For
such
programs
it
is
crucial
that
the
representation
of
the
information
appear
only
in
standard
headers
and
that
programs
include
those
headers
instead
of
embedding
the
declarations
in
themselves
The
second
observation
is
that
with
care
it
is
possible
to
create
an
interface
to
system
dependent
objects
that
is
itself
relatively
systemindependent
The
functions
of
the
standard
library
are
good
examples
Exercise
Modify
the
fsize
program
to
print
the
other
information
contained
in
the
inode
entry
Example
A
Storage
Allocator
In
Chapter
we
presented
a
vary
limited
stack
oriented
storage
allocator
The
version
that
we
will
now
write
is
unrestricted
Calls
to
malloc
and
free
may
occur
in
any
order
malloc
calls
upon
the
operating
system
to
obtain
more
memory
as
necessary
These
routines
illustrate
some
of
the
considerations
involved
in
writing
machine
dependent
code
in
a
relatively
machineindependent
way
and
also
show
a
real
life
application
of
structures
unions
and
typedef
Rather
than
allocating
from
a
compiled
in
fixed
size
array
malloc
will
request
space
from
the
operating
system
as
needed
Since
other
activities
in
the
program
may
also
request
space
without
calling
this
allocator
the
space
that
malloc
manages
may
not
be
contiguous
Thus
its
free
storage
is
kept
as
a
list
of
free
blocks
Each
block
contains
a
size
a
pointer
to
the
next
block
and
the
space
itself
The
blocks
are
kept
in
order
of
increasing
storage
address
and
the
last
block
highest
address
points
to
the
first
When
a
request
is
made
the
free
list
is
scanned
until
a
big
enough
block
is
found
This
algorithm
is
called
first
fit
by
contrast
with
best
fit
which
looks
for
the
smallest
block
that
will
satisfy
the
request
If
the
block
is
exactly
the
size
requested
it
is
unlinked
from
the
list
and
returned
to
the
user
If
the
block
is
too
big
it
is
split
and
the
proper
amount
is
returned
to
the
user
while
the
residue
remains
on
the
free
list
If
no
big
enough
block
is
found
another
large
chunk
is
obtained
by
the
operating
system
and
linked
into
the
free
list
Freeing
also
causes
a
search
of
the
free
list
to
find
the
proper
place
to
insert
the
block
being
freed
If
the
block
being
freed
is
adjacent
to
a
free
block
on
either
side
it
is
coalesced
with
it
into
a
single
bigger
block
so
storage
does
not
become
too
fragmented
Determining
the
adjacency
is
easy
because
the
free
list
is
maintained
in
order
of
decreasing
address
One
problem
which
we
alluded
to
in
Chapter
is
to
ensure
that
the
storage
returned
by
malloc
is
aligned
properly
for
the
objects
that
will
be
stored
in
it
Although
machines
vary
for
each
machine
there
is
a
most
restrictive
type
if
the
most
restrictive
type
can
be
stored
at
a
particular
address
all
other
types
may
be
also
On
some
machines
the
most
restrictive
type
is
a
double
on
others
int
or
long
suffices
A
free
block
contains
a
pointer
to
the
next
block
in
the
chain
a
record
of
the
size
of
the
block
and
then
the
free
space
itself
the
control
information
at
the
beginning
is
called
the
header
To
simplify
alignment
all
blocks
are
multiples
of
the
header
size
and
the
header
is
aligned
properly
This
is
achieved
by
a
union
that
contains
the
desired
header
structure
and
an
instance
of
the
most
restrictive
alignment
type
which
we
have
arbitrarily
made
a
long
typedef
long
Align
for
alignment
to
long
boundary
union
header
block
header
struct
union
header
ptr
next
block
if
on
free
list
unsigned
size
size
of
this
block
s
Align
x
force
alignment
of
blocks
typedef
union
header
Header
The
Align
field
is
never
used
it
just
forces
each
header
to
be
aligned
on
a
worst
case
boundary
In
malloc
the
requested
size
in
characters
is
rounded
up
to
the
proper
number
of
header
sized
units
the
block
that
will
be
allocated
contains
one
more
unit
for
the
header
itself
and
this
is
the
value
recorded
in
the
size
field
of
the
header
The
pointer
returned
by
malloc
points
at
the
free
space
not
at
the
header
itself
The
user
can
do
anything
with
the
space
requested
but
if
anything
is
written
outside
of
the
allocated
space
the
list
is
likely
to
be
scrambled
The
size
field
is
necessary
because
the
blocks
controlled
by
malloc
need
not
be
contiguous
it
is
not
possible
to
compute
sizes
by
pointer
arithmetic
The
variable
base
is
used
to
get
started
If
freep
is
NULL
as
it
is
at
the
first
call
of
malloc
then
a
degenerate
free
list
is
created
it
contains
one
block
of
size
zero
and
points
to
itself
In
any
case
the
free
list
is
then
searched
The
search
for
a
free
block
of
adequate
size
begins
at
the
point
freep
where
the
last
block
was
found
this
strategy
helps
keep
the
list
homogeneous
If
a
too
big
block
is
found
the
tail
end
is
returned
to
the
user
in
this
way
the
header
of
the
original
needs
only
to
have
its
size
adjusted
In
all
cases
the
pointer
returned
to
the
user
points
to
the
free
space
within
the
block
which
begins
one
unit
beyond
the
header
static
Header
base
empty
list
to
get
started
static
Header
freep
NULL
start
of
free
list
malloc
general
purpose
storage
allocator
void
malloc
unsigned
nbytes
Header
p
prevp
Header
moreroce
unsigned
unsigned
nunits
nunits
nbytes
sizeof
Header
sizeof
header
if
prevp
freep
NULL
no
free
list
yet
base
s
ptr
freeptr
prevptr
base
base
s
size
for
p
prevp
s
ptr
prevp
p
p
p
s
ptr
if
p
s
size
nunits
big
enough
if
p
s
size
nunits
exactly
prevp
s
ptr
p
s
ptr
else
allocate
tail
end
p
s
size
nunits
p
p
s
size
p
s
size
nunits
freep
prevp
return
void
p
if
p
freep
wrapped
around
free
list
if
p
morecore
nunits
NULL
return
NULL
none
left
The
function
morecore
obtains
storage
from
the
operating
system
The
details
of
how
it
does
this
vary
from
system
to
system
Since
asking
the
system
for
memory
is
a
comparatively
expensive
operation
we
don
t
want
to
do
that
on
every
call
to
malloc
so
morecore
requests
al
least
NALLOC
units
this
larger
block
will
be
chopped
up
as
needed
After
setting
the
size
field
morecore
inserts
the
additional
memory
into
the
arena
by
calling
free
The
UNIX
system
call
sbrk
n
returns
a
pointer
to
n
more
bytes
of
storage
sbrk
returns
if
there
was
no
space
even
though
NULL
could
have
been
a
better
design
The
must
be
cast
to
char
so
it
can
be
compared
with
the
return
value
Again
casts
make
the
function
relatively
immune
to
the
details
of
pointer
representation
on
different
machines
There
is
still
one
assumption
however
that
pointers
to
different
blocks
returned
by
sbrk
can
be
meaningfully
compared
This
is
not
guaranteed
by
the
standard
which
permits
pointer
comparisons
only
within
an
array
Thus
this
version
of
malloc
is
portable
only
among
machines
for
which
general
pointer
comparison
is
meaningful
define
NALLOC
minimum
units
to
request
morecore
ask
system
for
more
memory
static
Header
morecore
unsigned
nu
char
cp
sbrk
int
Header
up
if
nu
NALLOC
nu
NALLOC
cp
sbrk
nu
sizeof
Header
if
cp
char
no
space
at
all
return
NULL
up
Header
cp
up
s
size
nu
free
void
up
return
freep
free
itself
is
the
last
thing
It
scans
the
free
list
starting
at
freep
looking
for
the
place
to
insert
the
free
block
This
is
either
between
two
existing
blocks
or
at
the
end
of
the
list
In
any
case
if
the
block
being
freed
is
adjacent
to
either
neighbor
the
adjacent
blocks
are
combined
The
only
troubles
are
keeping
the
pointers
pointing
to
the
right
things
and
the
sizes
correct
free
put
block
ap
in
free
list
void
free
void
ap
Header
bp
p
bp
Header
ap
point
to
block
header
for
p
freep
bp
p
bp
p
s
ptr
p
p
s
ptr
if
p
p
s
ptr
bp
p
bp
p
s
ptr
break
freed
block
at
start
or
end
of
arena
if
bp
bp
size
p
s
ptr
join
to
upper
nbr
bp
s
size
p
s
ptr
s
size
bp
s
ptr
p
s
ptr
s
ptr
else
bp
s
ptr
p
s
ptr
if
p
p
size
bp
join
to
lower
nbr
p
s
size
bp
s
size
p
s
ptr
bp
s
ptr
else
p
s
ptr
bp
freep
p
Although
storage
allocation
is
intrinsically
machine
dependent
the
code
above
illustrates
how
the
machine
dependencies
can
be
controlled
and
confined
to
a
very
small
part
of
the
program
The
use
of
typedef
and
union
handles
alignment
given
that
sbrk
supplies
an
appropriate
pointer
Casts
arrange
that
pointer
conversions
are
made
explicit
and
even
cope
with
a
badlydesigned
system
interface
Even
though
the
details
here
are
related
to
storage
allocation
the
general
approach
is
applicable
to
other
situations
as
well
Exercise
The
standard
library
function
calloc
n
size
returns
a
pointer
to
n
objects
of
size
size
with
the
storage
initialized
to
zero
Write
calloc
by
calling
malloc
or
by
modifying
it
Exercise
malloc
accepts
a
size
request
without
checking
its
plausibility
free
believes
that
the
block
it
is
asked
to
free
contains
a
valid
size
field
Improve
these
routines
so
they
make
more
pains
with
error
checking
Exercise
Write
a
routine
bfree
p
n
that
will
free
any
arbitrary
block
p
of
n
characters
into
the
free
list
maintained
by
malloc
and
free
By
using
bfree
a
user
can
add
a
static
or
external
array
to
the
free
list
at
any
time
Appendix
A
Reference
Manual
A
Introduction
This
manual
describes
the
C
language
specified
by
the
draft
submitted
to
ANSI
on
October
for
approval
as
American
Standard
for
Information
Systems
programming
Language
C
X
The
manual
is
an
interpretation
of
the
proposed
standard
not
the
standard
itself
although
care
has
been
taken
to
make
it
a
reliable
guide
to
the
language
For
the
most
part
this
document
follows
the
broad
outline
of
the
standard
which
in
turn
follows
that
of
the
first
edition
of
this
book
although
the
organization
differs
in
detail
Except
for
renaming
a
few
productions
and
not
formalizing
the
definitions
of
the
lexical
tokens
or
the
preprocessor
the
grammar
given
here
for
the
language
proper
is
equivalent
to
that
of
the
standard
Throughout
this
manual
commentary
material
is
indented
and
written
in
smaller
type
as
this
is
Most
often
these
comments
highlight
ways
in
which
ANSI
Standard
C
differs
from
the
language
defined
by
the
first
edition
of
this
book
or
from
refinements
subsequently
introduced
in
various
compilers
A
Lexical
Conventions
A
program
consists
of
one
or
more
translation
units
stored
in
files
It
is
translated
in
several
phases
which
are
described
in
Par
A
The
first
phases
do
low
level
lexical
transformations
carry
out
directives
introduced
by
the
lines
beginning
with
the
character
and
perform
macro
definition
and
expansion
When
the
preprocessing
of
Par
A
is
complete
the
program
has
been
reduced
to
a
sequence
of
tokens
A
Tokens
There
are
six
classes
of
tokens
identifiers
keywords
constants
string
literals
operators
and
other
separators
Blanks
horizontal
and
vertical
tabs
newlines
formfeeds
and
comments
as
described
below
collectively
white
space
are
ignored
except
as
they
separate
tokens
Some
white
space
is
required
to
separate
otherwise
adjacent
identifiers
keywords
and
constants
If
the
input
stream
has
been
separated
into
tokens
up
to
a
given
character
the
next
token
is
the
longest
string
of
characters
that
could
constitute
a
token
A
Comments
The
characters
introduce
a
comment
which
terminates
with
the
characters
Comments
do
not
nest
and
they
do
not
occur
within
a
string
or
character
literals
A
Identifiers
An
identifier
is
a
sequence
of
letters
and
digits
The
first
character
must
be
a
letter
the
underscore
counts
as
a
letter
Upper
and
lower
case
letters
are
different
Identifiers
may
have
any
length
and
for
internal
identifiers
at
least
the
first
characters
are
significant
some
implementations
may
take
more
characters
significant
Internal
identifiers
include
preprocessor
macro
names
and
all
other
names
that
do
not
have
external
linkage
Par
A
Identifiers
with
external
linkage
are
more
restricted
implementations
may
make
as
few
as
the
first
six
characters
significant
and
may
ignore
case
distinctions
A
Keywords
The
following
identifiers
are
reserved
for
the
use
as
keywords
and
may
not
be
used
otherwise
auto
double
int
struct
break
else
long
switch
case
enum
register
typedef
char
extern
return
union
const
float
short
unsigned
continue
for
signed
void
default
goto
sizeof
volatile
do
if
static
while
Some
implementations
also
reserve
the
words
fortran
and
asm
The
keywords
const
signed
and
volatile
are
new
with
the
ANSI
standard
enum
and
void
are
new
since
the
first
edition
but
in
common
use
entry
formerly
reserved
but
never
used
is
no
longer
reserved
A
Constants
There
are
several
kinds
of
constants
Each
has
a
data
type
Par
A
discusses
the
basic
types
constant
integer
constant
character
constant
floating
constant
enumeration
constant
A
Integer
Constants
An
integer
constant
consisting
of
a
sequence
of
digits
is
taken
to
be
octal
if
it
begins
with
digit
zero
decimal
otherwise
Octal
constants
do
not
contain
the
digits
or
A
sequence
of
digits
preceded
by
x
or
X
digit
zero
is
taken
to
be
a
hexadecimal
integer
The
hexadecimal
digits
include
a
or
A
through
f
or
F
with
values
through
An
integer
constant
may
be
suffixed
by
the
letter
u
or
U
to
specify
that
it
is
unsigned
It
may
also
be
suffixed
by
the
letter
l
or
L
to
specify
that
it
is
long
The
type
of
an
integer
constant
depends
on
its
form
value
and
suffix
See
Par
A
for
a
discussion
of
types
If
it
is
unsuffixed
and
decimal
it
has
the
first
of
these
types
in
which
its
value
can
be
represented
int
long
int
unsigned
long
int
If
it
is
unsuffixed
octal
or
hexadecimal
it
has
the
first
possible
of
these
types
int
unsigned
int
long
int
unsigned
long
int
If
it
is
suffixed
by
u
or
U
then
unsigned
int
unsigned
long
int
If
it
is
suffixed
by
l
or
L
then
long
int
unsigned
long
int
If
an
integer
constant
is
suffixed
by
UL
it
is
unsigned
long
The
elaboration
of
the
types
of
integer
constants
goes
considerably
beyond
the
first
edition
which
merely
caused
large
integer
constants
to
be
long
The
U
suffixes
are
new
A
Character
Constants
A
character
constant
is
a
sequence
of
one
or
more
characters
enclosed
in
single
quotes
as
in
x
The
value
of
a
character
constant
with
only
one
character
is
the
numeric
value
of
the
character
in
the
machine
s
character
set
at
execution
time
The
value
of
a
multi
character
constant
is
implementation
defined
Character
constants
do
not
contain
the
character
or
newlines
in
order
to
represent
them
and
certain
other
characters
the
following
escape
sequences
may
be
used
newline
NL
LF
n
backslash
horizontal
tab
HT
t
question
mark
vertical
tab
VT
v
single
quote
backspace
BS
b
double
quote
carriage
return
CR
r
octal
number
ooo
ooo
formfeed
FF
f
hex
number
hh
xhh
audible
alert
BEL
a
The
escape
ooo
consists
of
the
backslash
followed
by
or
octal
digits
which
are
taken
to
specify
the
value
of
the
desired
character
A
common
example
of
this
construction
is
not
followed
by
a
digit
which
specifies
the
character
NUL
The
escape
xhh
consists
of
the
backslash
followed
by
x
followed
by
hexadecimal
digits
which
are
taken
to
specify
the
value
of
the
desired
character
There
is
no
limit
on
the
number
of
digits
but
the
behavior
is
undefined
if
the
resulting
character
value
exceeds
that
of
the
largest
character
For
either
octal
or
hexadecimal
escape
characters
if
the
implementation
treats
the
char
type
as
signed
the
value
is
sign
extended
as
if
cast
to
char
type
If
the
character
following
the
is
not
one
of
those
specified
the
behavior
is
undefined
In
some
implementations
there
is
an
extended
set
of
characters
that
cannot
be
represented
in
the
char
type
A
constant
in
this
extended
set
is
written
with
a
preceding
L
for
example
L
x
and
is
called
a
wide
character
constant
Such
a
constant
has
type
wchar
t
an
integral
type
defined
in
the
standard
header
stddef
h
As
with
ordinary
character
constants
hexadecimal
escapes
may
be
used
the
effect
is
undefined
if
the
specified
value
exceeds
that
representable
with
wchar
t
Some
of
these
escape
sequences
are
new
in
particular
the
hexadecimal
character
representation
Extended
characters
are
also
new
The
character
sets
commonly
used
in
the
Americas
and
western
Europe
can
be
encoded
to
fit
in
the
char
type
the
main
intent
in
adding
wchar
t
was
to
accommodate
Asian
languages
A
Floating
Constants
A
floating
constant
consists
of
an
integer
part
a
decimal
part
a
fraction
part
an
e
or
E
an
optionally
signed
integer
exponent
and
an
optional
type
suffix
one
of
f
F
l
or
L
The
integer
and
fraction
parts
both
consist
of
a
sequence
of
digits
Either
the
integer
part
or
the
fraction
part
not
both
may
be
missing
either
the
decimal
point
or
the
e
and
the
exponent
not
both
may
be
missing
The
type
is
determined
by
the
suffix
F
or
f
makes
it
float
L
or
l
makes
it
long
double
otherwise
it
is
double
A
Enumeration
Constants
Identifiers
declared
as
enumerators
see
Par
A
are
constants
of
type
int
A
String
Literals
A
string
literal
also
called
a
string
constant
is
a
sequence
of
characters
surrounded
by
double
quotes
as
in
A
string
has
type
array
of
characters
and
storage
class
static
see
Par
A
below
and
is
initialized
with
the
given
characters
Whether
identical
string
literals
are
distinct
is
implementation
defined
and
the
behavior
of
a
program
that
attempts
to
alter
a
string
literal
is
undefined
Adjacent
string
literals
are
concatenated
into
a
single
string
After
any
concatenation
a
null
byte
is
appended
to
the
string
so
that
programs
that
scan
the
string
can
find
its
end
String
literals
do
not
contain
newline
or
double
quote
characters
in
order
to
represent
them
the
same
escape
sequences
as
for
character
constants
are
available
As
with
character
constants
string
literals
in
an
extended
character
set
are
written
with
a
preceding
L
as
in
L
Wide
character
string
literals
have
type
array
of
wchar
t
Concatenation
of
ordinary
and
wide
string
literals
is
undefined
The
specification
that
string
literals
need
not
be
distinct
and
the
prohibition
against
modifying
them
are
new
in
the
ANSI
standard
as
is
the
concatenation
of
adjacent
string
literals
Wide
character
string
literals
are
new
A
Syntax
Notation
In
the
syntax
notation
used
in
this
manual
syntactic
categories
are
indicated
by
italic
type
and
literal
words
and
characters
in
typewriter
style
Alternative
categories
are
usually
listed
on
separate
lines
in
a
few
cases
a
long
set
of
narrow
alternatives
is
presented
on
one
line
marked
by
the
phrase
one
of
An
optional
terminal
or
nonterminal
symbol
carries
the
subscript
opt
so
that
for
example
expressionopt
means
an
optional
expression
enclosed
in
braces
The
syntax
is
summarized
in
Par
A
Unlike
the
grammar
given
in
the
first
edition
of
this
book
the
one
given
here
makes
precedence
and
associativity
of
expression
operators
explicit
A
Meaning
of
Identifiers
Identifiers
or
names
refer
to
a
variety
of
things
functions
tags
of
structures
unions
and
enumerations
members
of
structures
or
unions
enumeration
constants
typedef
names
and
objects
An
object
sometimes
called
a
variable
is
a
location
in
storage
and
its
interpretation
depends
on
two
main
attributes
its
storage
class
and
its
type
The
storage
class
determines
the
lifetime
of
the
storage
associated
with
the
identified
object
the
type
determines
the
meaning
of
the
values
found
in
the
identified
object
A
name
also
has
a
scope
which
is
the
region
of
the
program
in
which
it
is
known
and
a
linkage
which
determines
whether
the
same
name
in
another
scope
refers
to
the
same
object
or
function
Scope
and
linkage
are
discussed
in
Par
A
A
Storage
Class
There
are
two
storage
classes
automatic
and
static
Several
keywords
together
with
the
context
of
an
object
s
declaration
specify
its
storage
class
Automatic
objects
are
local
to
a
block
Par
and
are
discarded
on
exit
from
the
block
Declarations
within
a
block
create
automatic
objects
if
no
storage
class
specification
is
mentioned
or
if
the
auto
specifier
is
used
Objects
declared
register
are
automatic
and
are
if
possible
stored
in
fast
registers
of
the
machine
Static
objects
may
be
local
to
a
block
or
external
to
all
blocks
but
in
either
case
retain
their
values
across
exit
from
and
reentry
to
functions
and
blocks
Within
a
block
including
a
block
that
provides
the
code
for
a
function
static
objects
are
declared
with
the
keyword
static
The
objects
declared
outside
all
blocks
at
the
same
level
as
function
definitions
are
always
static
They
may
be
made
local
to
a
particular
translation
unit
by
use
of
the
static
keyword
this
gives
them
internal
linkage
They
become
global
to
an
entire
program
by
omitting
an
explicit
storage
class
or
by
using
the
keyword
extern
this
gives
them
external
linkage
A
Basic
Types
There
are
several
fundamental
types
The
standard
header
limits
h
described
in
Appendix
B
defines
the
largest
and
smallest
values
of
each
type
in
the
local
implementation
The
numbers
given
in
Appendix
B
show
the
smallest
acceptable
magnitudes
Objects
declared
as
characters
char
are
large
enough
to
store
any
member
of
the
execution
character
set
If
a
genuine
character
from
that
set
is
stored
in
a
char
object
its
value
is
equivalent
to
the
integer
code
for
the
character
and
is
non
negative
Other
quantities
may
be
stored
into
char
variables
but
the
available
range
of
values
and
especially
whether
the
value
is
signed
is
implementation
dependent
Unsigned
characters
declared
unsigned
char
consume
the
same
amount
of
space
as
plain
characters
but
always
appear
non
negative
explicitly
signed
characters
declared
signed
char
likewise
take
the
same
space
as
plain
characters
unsigned
char
type
does
not
appear
in
the
first
edition
of
this
book
but
is
in
common
use
signed
char
is
new
Besides
the
char
types
up
to
three
sizes
of
integer
declared
short
int
int
and
long
int
are
available
Plain
int
objects
have
the
natural
size
suggested
by
the
host
machine
architecture
the
other
sizes
are
provided
to
meet
special
needs
Longer
integers
provide
at
least
as
much
storage
as
shorter
ones
but
the
implementation
may
make
plain
integers
equivalent
to
either
short
integers
or
long
integers
The
int
types
all
represent
signed
values
unless
specified
otherwise
Unsigned
integers
declared
using
the
keyword
unsigned
obey
the
laws
of
arithmetic
modulo
n
where
n
is
the
number
of
bits
in
the
representation
and
thus
arithmetic
on
unsigned
quantities
can
never
overflow
The
set
of
non
negative
values
that
can
be
stored
in
a
signed
object
is
a
subset
of
the
values
that
can
be
stored
in
the
corresponding
unsigned
object
and
the
representation
for
the
overlapping
values
is
the
same
Any
of
single
precision
floating
point
float
double
precision
floating
point
double
and
extra
precision
floating
point
long
double
may
be
synonymous
but
the
ones
later
in
the
list
are
at
least
as
precise
as
those
before
long
double
is
new
The
first
edition
made
long
float
equivalent
to
double
the
locution
has
been
withdrawn
Enumerations
are
unique
types
that
have
integral
values
associated
with
each
enumeration
is
a
set
of
named
constants
Par
A
Enumerations
behave
like
integers
but
it
is
common
for
a
compiler
to
issue
a
warning
when
an
object
of
a
particular
enumeration
is
assigned
something
other
than
one
of
its
constants
or
an
expression
of
its
type
Because
objects
of
these
types
can
be
interpreted
as
numbers
they
will
be
referred
to
as
arithmetic
types
Types
char
and
int
of
all
sizes
each
with
or
without
sign
and
also
enumeration
types
will
collectively
be
called
integral
types
The
types
float
double
and
long
double
will
be
called
floating
types
The
void
type
specifies
an
empty
set
of
values
It
is
used
as
the
type
returned
by
functions
that
generate
no
value
A
Derived
types
Beside
the
basic
types
there
is
a
conceptually
infinite
class
of
derived
types
constructed
from
the
fundamental
types
in
the
following
ways
arrays
of
objects
of
a
given
type
functions
returning
objects
of
a
given
type
pointers
to
objects
of
a
given
type
structures
containing
a
sequence
of
objects
of
various
types
unions
capable
of
containing
any
of
one
of
several
objects
of
various
types
In
general
these
methods
of
constructing
objects
can
be
applied
recursively
A
Type
Qualifiers
An
object
s
type
may
have
additional
qualifiers
Declaring
an
object
const
announces
that
its
value
will
not
be
changed
declaring
it
volatile
announces
that
it
has
special
properties
relevant
to
optimization
Neither
qualifier
affects
the
range
of
values
or
arithmetic
properties
of
the
object
Qualifiers
are
discussed
in
Par
A
A
Objects
and
Lvalues
An
Object
is
a
named
region
of
storage
an
lvalue
is
an
expression
referring
to
an
object
An
obvious
example
of
an
lvalue
expression
is
an
identifier
with
suitable
type
and
storage
class
There
are
operators
that
yield
lvalues
if
E
is
an
expression
of
pointer
type
then
E
is
an
lvalue
expression
referring
to
the
object
to
which
E
points
The
name
lvalue
comes
from
the
assignment
expression
E
E
in
which
the
left
operand
E
must
be
an
lvalue
expression
The
discussion
of
each
operator
specifies
whether
it
expects
lvalue
operands
and
whether
it
yields
an
lvalue
A
Conversions
Some
operators
may
depending
on
their
operands
cause
conversion
of
the
value
of
an
operand
from
one
type
to
another
This
section
explains
the
result
to
be
expected
from
such
conversions
Par
summarizes
the
conversions
demanded
by
most
ordinary
operators
it
will
be
supplemented
as
required
by
the
discussion
of
each
operator
A
Integral
Promotion
A
character
a
short
integer
or
an
integer
bit
field
all
either
signed
or
not
or
an
object
of
enumeration
type
may
be
used
in
an
expression
wherever
an
integer
may
be
used
If
an
int
can
represent
all
the
values
of
the
original
type
then
the
value
is
converted
to
int
otherwise
the
value
is
converted
to
unsigned
int
This
process
is
called
integral
promotion
A
Integral
Conversions
Any
integer
is
converted
to
a
given
unsigned
type
by
finding
the
smallest
non
negative
value
that
is
congruent
to
that
integer
modulo
one
more
than
the
largest
value
that
can
be
represented
in
the
unsigned
type
In
a
two
s
complement
representation
this
is
equivalent
to
left
truncation
if
the
bit
pattern
of
the
unsigned
type
is
narrower
and
to
zero
filling
unsigned
values
and
sign
extending
signed
values
if
the
unsigned
type
is
wider
When
any
integer
is
converted
to
a
signed
type
the
value
is
unchanged
if
it
can
be
represented
in
the
new
type
and
is
implementation
defined
otherwise
A
Integer
and
Floating
When
a
value
of
floating
type
is
converted
to
integral
type
the
fractional
part
is
discarded
if
the
resulting
value
cannot
be
represented
in
the
integral
type
the
behavior
is
undefined
In
particular
the
result
of
converting
negative
floating
values
to
unsigned
integral
types
is
not
specified
When
a
value
of
integral
type
is
converted
to
floating
and
the
value
is
in
the
representable
range
but
is
not
exactly
representable
then
the
result
may
be
either
the
next
higher
or
next
lower
representable
value
If
the
result
is
out
of
range
the
behavior
is
undefined
A
Floating
Types
When
a
less
precise
floating
value
is
converted
to
an
equally
or
more
precise
floating
type
the
value
is
unchanged
When
a
more
precise
floating
value
is
converted
to
a
less
precise
floating
type
and
the
value
is
within
representable
range
the
result
may
be
either
the
next
higher
or
the
next
lower
representable
value
If
the
result
is
out
of
range
the
behavior
is
undefined
A
Arithmetic
Conversions
Many
operators
cause
conversions
and
yield
result
types
in
a
similar
way
The
effect
is
to
bring
operands
into
a
common
type
which
is
also
the
type
of
the
result
This
pattern
is
called
the
usual
arithmetic
conversions
First
if
either
operand
is
long
double
the
other
is
converted
to
long
double
Otherwise
if
either
operand
is
double
the
other
is
converted
to
double
Otherwise
if
either
operand
is
float
the
other
is
converted
to
float
Otherwise
the
integral
promotions
are
performed
on
both
operands
then
if
either
operand
is
unsigned
long
int
the
other
is
converted
to
unsigned
long
int
Otherwise
if
one
operand
is
long
int
and
the
other
is
unsigned
int
the
effect
depends
on
whether
a
long
int
can
represent
all
values
of
an
unsigned
int
if
so
the
unsigned
int
operand
is
converted
to
long
int
if
not
both
are
converted
to
unsigned
long
int
Otherwise
if
one
operand
is
long
int
the
other
is
converted
to
long
int
Otherwise
if
either
operand
is
unsigned
int
the
other
is
converted
to
unsigned
int
Otherwise
both
operands
have
type
int
There
are
two
changes
here
First
arithmetic
on
float
operands
may
be
done
in
single
precision
rather
than
double
the
first
edition
specified
that
all
floating
arithmetic
was
double
precision
Second
shorter
unsigned
types
when
combined
with
a
larger
signed
type
do
not
propagate
the
unsigned
property
to
the
result
type
in
the
first
edition
the
unsigned
always
dominated
The
new
rules
are
slightly
more
complicated
but
reduce
somewhat
the
surprises
that
may
occur
when
an
unsigned
quantity
meets
signed
Unexpected
results
may
still
occur
when
an
unsigned
expression
is
compared
to
a
signed
expression
of
the
same
size
A
Pointers
and
Integers
An
expression
of
integral
type
may
be
added
to
or
subtracted
from
a
pointer
in
such
a
case
the
integral
expression
is
converted
as
specified
in
the
discussion
of
the
addition
operator
Par
A
Two
pointers
to
objects
of
the
same
type
in
the
same
array
may
be
subtracted
the
result
is
converted
to
an
integer
as
specified
in
the
discussion
of
the
subtraction
operator
Par
A
An
integral
constant
expression
with
value
or
such
an
expression
cast
to
type
void
may
be
converted
by
a
cast
by
assignment
or
by
comparison
to
a
pointer
of
any
type
This
produces
a
null
pointer
that
is
equal
to
another
null
pointer
of
the
same
type
but
unequal
to
any
pointer
to
a
function
or
object
Certain
other
conversions
involving
pointers
are
permitted
but
have
implementation
defined
aspects
They
must
be
specified
by
an
explicit
type
conversion
operator
or
cast
Pars
A
and
A
A
pointer
may
be
converted
to
an
integral
type
large
enough
to
hold
it
the
required
size
is
implementation
dependent
The
mapping
function
is
also
implementation
dependent
A
pointer
to
one
type
may
be
converted
to
a
pointer
to
another
type
The
resulting
pointer
may
cause
addressing
exceptions
if
the
subject
pointer
does
not
refer
to
an
object
suitably
aligned
in
storage
It
is
guaranteed
that
a
pointer
to
an
object
may
be
converted
to
a
pointer
to
an
object
whose
type
requires
less
or
equally
strict
storage
alignment
and
back
again
without
change
the
notion
of
alignment
is
implementation
dependent
but
objects
of
the
char
types
have
least
strict
alignment
requirements
As
described
in
Par
A
a
pointer
may
also
be
converted
to
type
void
and
back
again
without
change
A
pointer
may
be
converted
to
another
pointer
whose
type
is
the
same
except
for
the
addition
or
removal
of
qualifiers
Pars
A
A
of
the
object
type
to
which
the
pointer
refers
If
qualifiers
are
added
the
new
pointer
is
equivalent
to
the
old
except
for
restrictions
implied
by
the
new
qualifiers
If
qualifiers
are
removed
operations
on
the
underlying
object
remain
subject
to
the
qualifiers
in
its
actual
declaration
Finally
a
pointer
to
a
function
may
be
converted
to
a
pointer
to
another
function
type
Calling
the
function
specified
by
the
converted
pointer
is
implementation
dependent
however
if
the
converted
pointer
is
reconverted
to
its
original
type
the
result
is
identical
to
the
original
pointer
A
Void
The
nonexistent
value
of
a
void
object
may
not
be
used
in
any
way
and
neither
explicit
nor
implicit
conversion
to
any
non
void
type
may
be
applied
Because
a
void
expression
denotes
a
nonexistent
value
such
an
expression
may
be
used
only
where
the
value
is
not
required
for
example
as
an
expression
statement
Par
A
or
as
the
left
operand
of
a
comma
operator
Par
A
An
expression
may
be
converted
to
type
void
by
a
cast
For
example
a
void
cast
documents
the
discarding
of
the
value
of
a
function
call
used
as
an
expression
statement
void
did
not
appear
in
the
first
edition
of
this
book
but
has
become
common
since
A
Pointers
to
Void
Any
pointer
to
an
object
may
be
converted
to
type
void
without
loss
of
information
If
the
result
is
converted
back
to
the
original
pointer
type
the
original
pointer
is
recovered
Unlike
the
pointer
to
pointer
conversions
discussed
in
Par
A
which
generally
require
an
explicit
cast
pointers
may
be
assigned
to
and
from
pointers
of
type
void
and
may
be
compared
with
them
This
interpretation
of
void
pointers
is
new
previously
char
pointers
played
the
role
of
generic
pointer
The
ANSI
standard
specifically
blesses
the
meeting
of
void
pointers
with
object
pointers
in
assignments
and
relationals
while
requiring
explicit
casts
for
other
pointer
mixtures
A
Expressions
The
precedence
of
expression
operators
is
the
same
as
the
order
of
the
major
subsections
of
this
section
highest
precedence
first
Thus
for
example
the
expressions
referred
to
as
the
operands
of
Par
A
are
those
expressions
defined
in
Pars
A
A
Within
each
subsection
the
operators
have
the
same
precedence
Left
or
right
associativity
is
specified
in
each
subsection
for
the
operators
discussed
therein
The
grammar
given
in
Par
incorporates
the
precedence
and
associativity
of
the
operators
The
precedence
and
associativity
of
operators
is
fully
specified
but
the
order
of
evaluation
of
expressions
is
with
certain
exceptions
undefined
even
if
the
subexpressions
involve
side
effects
That
is
unless
the
definition
of
the
operator
guarantees
that
its
operands
are
evaluated
in
a
particular
order
the
implementation
is
free
to
evaluate
operands
in
any
order
or
even
to
interleave
their
evaluation
However
each
operator
combines
the
values
produced
by
its
operands
in
a
way
compatible
with
the
parsing
of
the
expression
in
which
it
appears
This
rule
revokes
the
previous
freedom
to
reorder
expressions
with
operators
that
are
mathematically
commutative
and
associative
but
can
fail
to
be
computationally
associative
The
change
affects
only
floating
point
computations
near
the
limits
of
their
accuracy
and
situations
where
overflow
is
possible
The
handling
of
overflow
divide
check
and
other
exceptions
in
expression
evaluation
is
not
defined
by
the
language
Most
existing
implementations
of
C
ignore
overflow
in
evaluation
of
signed
integral
expressions
and
assignments
but
this
behavior
is
not
guaranteed
Treatment
of
division
by
and
all
floating
point
exceptions
varies
among
implementations
sometimes
it
is
adjustable
by
a
non
standard
library
function
A
Pointer
Conversion
If
the
type
of
an
expression
or
subexpression
is
array
of
T
for
some
type
T
then
the
value
of
the
expression
is
a
pointer
to
the
first
object
in
the
array
and
the
type
of
the
expression
is
altered
to
pointer
to
T
This
conversion
does
not
take
place
if
the
expression
is
in
the
operand
of
the
unary
operator
or
of
sizeof
or
as
the
left
operand
of
an
assignment
operator
or
the
operator
Similarly
an
expression
of
type
function
returning
T
except
when
used
as
the
operand
of
the
operator
is
converted
to
pointer
to
function
returning
T
A
Primary
Expressions
Primary
expressions
are
identifiers
constants
strings
or
expressions
in
parentheses
primary
expression
identifier
constant
string
expression
An
identifier
is
a
primary
expression
provided
it
has
been
suitably
declared
as
discussed
below
Its
type
is
specified
by
its
declaration
An
identifier
is
an
lvalue
if
it
refers
to
an
object
Par
A
and
if
its
type
is
arithmetic
structure
union
or
pointer
A
constant
is
a
primary
expression
Its
type
depends
on
its
form
as
discussed
in
Par
A
A
string
literal
is
a
primary
expression
Its
type
is
originally
array
of
char
for
wide
char
strings
array
of
wchar
t
but
following
the
rule
given
in
Par
A
this
is
usually
modified
to
pointer
to
char
wchar
t
and
the
result
is
a
pointer
to
the
first
character
in
the
string
The
conversion
also
does
not
occur
in
certain
initializers
see
Par
A
A
parenthesized
expression
is
a
primary
expression
whose
type
and
value
are
identical
to
those
of
the
unadorned
expression
The
precedence
of
parentheses
does
not
affect
whether
the
expression
is
an
lvalue
A
Postfix
Expressions
The
operators
in
postfix
expressions
group
left
to
right
postfix
expression
primary
expression
postfix
expression
expression
postfix
expression
argument
expression
listopt
postfix
expression
identifier
postfix
expression
identifier
postfix
expression
postfix
expression
argument
expression
list
assignment
expression
assignment
expression
list
assignment
expression
A
Array
References
A
postfix
expression
followed
by
an
expression
in
square
brackets
is
a
postfix
expression
denoting
a
subscripted
array
reference
One
of
the
two
expressions
must
have
type
pointer
to
T
where
T
is
some
type
and
the
other
must
have
integral
type
the
type
of
the
subscript
expression
is
T
The
expression
E
E
is
identical
by
definition
to
E
E
See
Par
A
for
further
discussion
A
Function
Calls
A
function
call
is
a
postfix
expression
called
the
function
designator
followed
by
parentheses
containing
a
possibly
empty
comma
separated
list
of
assignment
expressions
Par
A
which
constitute
the
arguments
to
the
function
If
the
postfix
expression
consists
of
an
identifier
for
which
no
declaration
exists
in
the
current
scope
the
identifier
is
implicitly
declared
as
if
the
declaration
extern
int
identifier
had
been
given
in
the
innermost
block
containing
the
function
call
The
postfix
expression
after
possible
explicit
declaration
and
pointer
generation
Par
A
must
be
of
type
pointer
to
function
returning
T
for
some
type
T
and
the
value
of
the
function
call
has
type
T
In
the
first
edition
the
type
was
restricted
to
function
and
an
explicit
operator
was
required
to
call
through
pointers
to
functions
The
ANSI
standard
blesses
the
practice
of
some
existing
compilers
by
permitting
the
same
syntax
for
calls
to
functions
and
to
functions
specified
by
pointers
The
older
syntax
is
still
usable
The
term
argument
is
used
for
an
expression
passed
by
a
function
call
the
term
parameter
is
used
for
an
input
object
or
its
identifier
received
by
a
function
definition
or
described
in
a
function
declaration
The
terms
actual
argument
parameter
and
formal
argument
parameter
respectively
are
sometimes
used
for
the
same
distinction
In
preparing
for
the
call
to
a
function
a
copy
is
made
of
each
argument
all
argument
passing
is
strictly
by
value
A
function
may
change
the
values
of
its
parameter
objects
which
are
copies
of
the
argument
expressions
but
these
changes
cannot
affect
the
values
of
the
arguments
However
it
is
possible
to
pass
a
pointer
on
the
understanding
that
the
function
may
change
the
value
of
the
object
to
which
the
pointer
points
There
are
two
styles
in
which
functions
may
be
declared
In
the
new
style
the
types
of
parameters
are
explicit
and
are
part
of
the
type
of
the
function
such
a
declaration
os
also
called
a
function
prototype
In
the
old
style
parameter
types
are
not
specified
Function
declaration
is
issued
in
Pars
A
and
A
If
the
function
declaration
in
scope
for
a
call
is
old
style
then
default
argument
promotion
is
applied
to
each
argument
as
follows
integral
promotion
Par
A
is
performed
on
each
argument
of
integral
type
and
each
float
argument
is
converted
to
double
The
effect
of
the
call
is
undefined
if
the
number
of
arguments
disagrees
with
the
number
of
parameters
in
the
definition
of
the
function
or
if
the
type
of
an
argument
after
promotion
disagrees
with
that
of
the
corresponding
parameter
Type
agreement
depends
on
whether
the
function
s
definition
is
new
style
or
old
style
If
it
is
old
style
then
the
comparison
is
between
the
promoted
type
of
the
arguments
of
the
call
and
the
promoted
type
of
the
parameter
if
the
definition
is
newstyle
the
promoted
type
of
the
argument
must
be
that
of
the
parameter
itself
without
promotion
If
the
function
declaration
in
scope
for
a
call
is
new
style
then
the
arguments
are
converted
as
if
by
assignment
to
the
types
of
the
corresponding
parameters
of
the
function
s
prototype
The
number
of
arguments
must
be
the
same
as
the
number
of
explicitly
described
parameters
unless
the
declaration
s
parameter
list
ends
with
the
ellipsis
notation
In
that
case
the
number
of
arguments
must
equal
or
exceed
the
number
of
parameters
trailing
arguments
beyond
the
explicitly
typed
parameters
suffer
default
argument
promotion
as
described
in
the
preceding
paragraph
If
the
definition
of
the
function
is
old
style
then
the
type
of
each
parameter
in
the
definition
after
the
definition
parameter
s
type
has
undergone
argument
promotion
These
rules
are
especially
complicated
because
they
must
cater
to
a
mixture
of
old
and
new
style
functions
Mixtures
are
to
be
avoided
if
possible
The
order
of
evaluation
of
arguments
is
unspecified
take
note
that
various
compilers
differ
However
the
arguments
and
the
function
designator
are
completely
evaluated
including
all
side
effects
before
the
function
is
entered
Recursive
calls
to
any
function
are
permitted
A
Structure
References
A
postfix
expression
followed
by
a
dot
followed
by
an
identifier
is
a
postfix
expression
The
first
operand
expression
must
be
a
structure
or
a
union
and
the
identifier
must
name
a
member
of
the
structure
or
union
The
value
is
the
named
member
of
the
structure
or
union
and
its
type
is
the
type
of
the
member
The
expression
is
an
lvalue
if
the
first
expression
is
an
lvalue
and
if
the
type
of
the
second
expression
is
not
an
array
type
A
postfix
expression
followed
by
an
arrow
built
from
and
followed
by
an
identifier
is
a
postfix
expression
The
first
operand
expression
must
be
a
pointer
to
a
structure
or
union
and
the
identifier
must
name
a
member
of
the
structure
or
union
The
result
refers
to
the
named
member
of
the
structure
or
union
to
which
the
pointer
expression
points
and
the
type
is
the
type
of
the
member
the
result
is
an
lvalue
if
the
type
is
not
an
array
type
Thus
the
expression
E
MOS
is
the
same
as
E
MOS
Structures
and
unions
are
discussed
in
Par
A
In
the
first
edition
of
this
book
it
was
already
the
rule
that
a
member
name
in
such
an
expression
had
to
belong
to
the
structure
or
union
mentioned
in
the
postfix
expression
however
a
note
admitted
that
this
rule
was
not
firmly
enforced
Recent
compilers
and
ANSI
do
enforce
it
A
Postfix
Incrementation
A
postfix
expression
followed
by
a
or
operator
is
a
postfix
expression
The
value
of
the
expression
is
the
value
of
the
operand
After
the
value
is
noted
the
operand
is
incremented
or
decremented
by
The
operand
must
be
an
lvalue
see
the
discussion
of
additive
operators
Par
A
and
assignment
Par
A
for
further
constraints
on
the
operand
and
details
of
the
operation
The
result
is
not
an
lvalue
A
Unary
Operators
Expressions
with
unary
operators
group
right
to
left
unary
expression
postfix
expression
unary
expression
unary
expression
unary
operator
cast
expression
sizeof
unary
expression
sizeof
type
name
unary
operator
one
of
A
Prefix
Incrementation
Operators
A
unary
expression
followed
by
a
or
operator
is
a
unary
expression
The
operand
is
incremented
or
decremented
by
The
value
of
the
expression
is
the
value
after
the
incrementation
decrementation
The
operand
must
be
an
lvalue
see
the
discussion
of
additive
operators
Par
A
and
assignment
Par
A
for
further
constraints
on
the
operands
and
details
of
the
operation
The
result
is
not
an
lvalue
A
Address
Operator
The
unary
operator
takes
the
address
of
its
operand
The
operand
must
be
an
lvalue
referring
neither
to
a
bit
field
nor
to
an
object
declared
as
register
or
must
be
of
function
type
The
result
is
a
pointer
to
the
object
or
function
referred
to
by
the
lvalue
If
the
type
of
the
operand
is
T
the
type
of
the
result
is
pointer
to
T
A
Indirection
Operator
The
unary
operator
denotes
indirection
and
returns
the
object
or
function
to
which
its
operand
points
It
is
an
lvalue
if
the
operand
is
a
pointer
to
an
object
of
arithmetic
structure
union
or
pointer
type
If
the
type
of
the
expression
is
pointer
to
T
the
type
of
the
result
is
T
A
Unary
Plus
Operator
The
operand
of
the
unary
operator
must
have
arithmetic
type
and
the
result
is
the
value
of
the
operand
An
integral
operand
undergoes
integral
promotion
The
type
of
the
result
is
the
type
of
the
promoted
operand
The
unary
is
new
with
the
ANSI
standard
It
was
added
for
symmetry
with
the
unary
A
Unary
Minus
Operator
The
operand
of
the
unary
operator
must
have
arithmetic
type
and
the
result
is
the
negative
of
its
operand
An
integral
operand
undergoes
integral
promotion
The
negative
of
an
unsigned
quantity
is
computed
by
subtracting
the
promoted
value
from
the
largest
value
of
the
promoted
type
and
adding
one
but
negative
zero
is
zero
The
type
of
the
result
is
the
type
of
the
promoted
operand
A
One
s
Complement
Operator
The
operand
of
the
operator
must
have
integral
type
and
the
result
is
the
one
s
complement
of
its
operand
The
integral
promotions
are
performed
If
the
operand
is
unsigned
the
result
is
computed
by
subtracting
the
value
from
the
largest
value
of
the
promoted
type
If
the
operand
is
signed
the
result
is
computed
by
converting
the
promoted
operand
to
the
corresponding
unsigned
type
applying
and
converting
back
to
the
signed
type
The
type
of
the
result
is
the
type
of
the
promoted
operand
A
Logical
Negation
Operator
The
operand
of
the
operator
must
have
arithmetic
type
or
be
a
pointer
and
the
result
is
if
the
value
of
its
operand
compares
equal
to
and
otherwise
The
type
of
the
result
is
int
A
Sizeof
Operator
The
sizeof
operator
yields
the
number
of
bytes
required
to
store
an
object
of
the
type
of
its
operand
The
operand
is
either
an
expression
which
is
not
evaluated
or
a
parenthesized
type
name
When
sizeof
is
applied
to
a
char
the
result
is
when
applied
to
an
array
the
result
is
the
total
number
of
bytes
in
the
array
When
applied
to
a
structure
or
union
the
result
is
the
number
of
bytes
in
the
object
including
any
padding
required
to
make
the
object
tile
an
array
the
size
of
an
array
of
n
elements
is
n
times
the
size
of
one
element
The
operator
may
not
be
applied
to
an
operand
of
function
type
or
of
incomplete
type
or
to
a
bit
field
The
result
is
an
unsigned
integral
constant
the
particular
type
is
implementation
defined
The
standard
header
stddef
h
See
appendix
B
defines
this
type
as
size
t
A
Casts
A
unary
expression
preceded
by
the
parenthesized
name
of
a
type
causes
conversion
of
the
value
of
the
expression
to
the
named
type
cast
expression
unary
expression
type
name
cast
expression
This
construction
is
called
a
cast
The
names
are
described
in
Par
A
The
effects
of
conversions
are
described
in
Par
A
An
expression
with
a
cast
is
not
an
lvalue
A
Multiplicative
Operators
The
multiplicative
operators
and
group
left
to
right
multiplicative
expression
multiplicative
expression
cast
expression
multiplicative
expression
cast
expression
multiplicative
expression
cast
expression
The
operands
of
and
must
have
arithmetic
type
the
operands
of
must
have
integral
type
The
usual
arithmetic
conversions
are
performed
on
the
operands
and
predict
the
type
of
the
result
The
binary
operator
denotes
multiplication
The
binary
operator
yields
the
quotient
and
the
operator
the
remainder
of
the
division
of
the
first
operand
by
the
second
if
the
second
operand
is
the
result
is
undefined
Otherwise
it
is
always
true
that
a
b
b
a
b
is
equal
to
a
If
both
operands
are
non
negative
then
the
remainder
is
non
negative
and
smaller
than
the
divisor
if
not
it
is
guaranteed
only
that
the
absolute
value
of
the
remainder
is
smaller
than
the
absolute
value
of
the
divisor
A
Additive
Operators
The
additive
operators
and
group
left
to
right
If
the
operands
have
arithmetic
type
the
usual
arithmetic
conversions
are
performed
There
are
some
additional
type
possibilities
for
each
operator
additive
expression
multiplicative
expression
additive
expression
multiplicative
expression
additive
expression
multiplicative
expression
The
result
of
the
operator
is
the
sum
of
the
operands
A
pointer
to
an
object
in
an
array
and
a
value
of
any
integral
type
may
be
added
The
latter
is
converted
to
an
address
offset
by
multiplying
it
by
the
size
of
the
object
to
which
the
pointer
points
The
sum
is
a
pointer
of
the
same
type
as
the
original
pointer
and
points
to
another
object
in
the
same
array
appropriately
offset
from
the
original
object
Thus
if
P
is
a
pointer
to
an
object
in
an
array
the
expression
P
is
a
pointer
to
the
next
object
in
the
array
If
the
sum
pointer
points
outside
the
bounds
of
the
array
except
at
the
first
location
beyond
the
high
end
the
result
is
undefined
The
provision
for
pointers
just
beyond
the
end
of
an
array
is
new
It
legitimizes
a
common
idiom
for
looping
over
the
elements
of
an
array
The
result
of
the
operator
is
the
difference
of
the
operands
A
value
of
any
integral
type
may
be
subtracted
from
a
pointer
and
then
the
same
conversions
and
conditions
as
for
addition
apply
If
two
pointers
to
objects
of
the
same
type
are
subtracted
the
result
is
a
signed
integral
value
representing
the
displacement
between
the
pointed
to
objects
pointers
to
successive
objects
differ
by
The
type
of
the
result
is
defined
as
ptrdiff
t
in
the
standard
header
stddef
h
The
value
is
undefined
unless
the
pointers
point
to
objects
within
the
same
array
however
if
P
points
to
the
last
member
of
an
array
then
P
P
has
value
A
Shift
Operators
The
shift
operators
and
group
left
to
right
For
both
operators
each
operand
must
be
integral
and
is
subject
to
integral
the
promotions
The
type
of
the
result
is
that
of
the
promoted
left
operand
The
result
is
undefined
if
the
right
operand
is
negative
or
greater
than
or
equal
to
the
number
of
bits
in
the
left
expression
s
type
shift
expression
additive
expression
shift
expression
additive
expression
shift
expression
additive
expression
The
value
of
E
E
is
E
interpreted
as
a
bit
pattern
left
shifted
E
bits
in
the
absence
of
overflow
this
is
equivalent
to
multiplication
by
E
The
value
of
E
E
is
E
right
shifted
E
bit
positions
The
right
shift
is
equivalent
to
division
by
E
if
E
is
unsigned
or
it
has
a
nonnegative
value
otherwise
the
result
is
implementation
defined
A
Relational
Operators
The
relational
operators
group
left
to
right
but
this
fact
is
not
useful
a
b
c
is
parsed
as
a
b
c
and
evaluates
to
either
or
relational
expression
shift
expression
relational
expression
shift
expression
relational
expression
shift
expression
relational
expression
shift
expression
relational
expression
shift
expression
The
operators
less
greater
less
or
equal
and
greater
or
equal
all
yield
if
the
specified
relation
is
false
and
if
it
is
true
The
type
of
the
result
is
int
The
usual
arithmetic
conversions
are
performed
on
arithmetic
operands
Pointers
to
objects
of
the
same
type
ignoring
any
qualifiers
may
be
compared
the
result
depends
on
the
relative
locations
in
the
address
space
of
the
pointed
to
objects
Pointer
comparison
is
defined
only
for
parts
of
the
same
object
if
two
pointers
point
to
the
same
simple
object
they
compare
equal
if
the
pointers
are
to
members
of
the
same
structure
pointers
to
objects
declared
later
in
the
structure
compare
higher
if
the
pointers
refer
to
members
of
an
array
the
comparison
is
equivalent
to
comparison
of
the
the
corresponding
subscripts
If
P
points
to
the
last
member
of
an
array
then
P
compares
higher
than
P
even
though
P
points
outside
the
array
Otherwise
pointer
comparison
is
undefined
These
rules
slightly
liberalize
the
restrictions
stated
in
the
first
edition
by
permitting
comparison
of
pointers
to
different
members
of
a
structure
or
union
They
also
legalize
comparison
with
a
pointer
just
off
the
end
of
an
array
A
Equality
Operators
equality
expression
relational
expression
equality
expression
relational
expression
equality
expression
relational
expression
The
equal
to
and
the
not
equal
to
operators
are
analogous
to
the
relational
operators
except
for
their
lower
precedence
Thus
a
b
c
d
is
whenever
a
b
and
c
d
have
the
same
truth
value
The
equality
operators
follow
the
same
rules
as
the
relational
operators
but
permit
additional
possibilities
a
pointer
may
be
compared
to
a
constant
integral
expression
with
value
or
to
a
pointer
to
void
See
Par
A
A
Bitwise
AND
Operator
AND
expression
equality
expression
AND
expression
equality
expression
The
usual
arithmetic
conversions
are
performed
the
result
is
the
bitwise
AND
function
of
the
operands
The
operator
applies
only
to
integral
operands
A
Bitwise
Exclusive
OR
Operator
exclusive
OR
expression
AND
expression
exclusive
OR
expression
AND
expression
The
usual
arithmetic
conversions
are
performed
the
result
is
the
bitwise
exclusive
OR
function
of
the
operands
The
operator
applies
only
to
integral
operands
A
Bitwise
Inclusive
OR
Operator
inclusive
OR
expression
exclusive
OR
expression
inclusive
OR
expression
exclusive
OR
expression
The
usual
arithmetic
conversions
are
performed
the
result
is
the
bitwise
inclusive
OR
function
of
the
operands
The
operator
applies
only
to
integral
operands
A
Logical
AND
Operator
logical
AND
expression
inclusive
OR
expression
logical
AND
expression
inclusive
OR
expression
The
operator
groups
left
to
right
It
returns
if
both
its
operands
compare
unequal
to
zero
otherwise
Unlike
guarantees
left
to
right
evaluation
the
first
operand
is
evaluated
including
all
side
effects
if
it
is
equal
to
the
value
of
the
expression
is
Otherwise
the
right
operand
is
evaluated
and
if
it
is
equal
to
the
expression
s
value
is
otherwise
The
operands
need
not
have
the
same
type
but
each
must
have
arithmetic
type
or
be
a
pointer
The
result
is
int
A
Logical
OR
Operator
logical
OR
expression
logical
AND
expression
logical
OR
expression
logical
AND
expression
The
operator
groups
left
to
right
It
returns
if
either
of
its
operands
compare
unequal
to
zero
and
otherwise
Unlike
guarantees
left
to
right
evaluation
the
first
operand
is
evaluated
including
all
side
effects
if
it
is
unequal
to
the
value
of
the
expression
is
Otherwise
the
right
operand
is
evaluated
and
if
it
is
unequal
to
the
expression
s
value
is
otherwise
The
operands
need
not
have
the
same
type
but
each
must
have
arithmetic
type
or
be
a
pointer
The
result
is
int
A
Conditional
Operator
conditional
expression
logical
OR
expression
logical
OR
expression
expression
conditional
expression
The
first
expression
is
evaluated
including
all
side
effects
if
it
compares
unequal
to
the
result
is
the
value
of
the
second
expression
otherwise
that
of
the
third
expression
Only
one
of
the
second
and
third
operands
is
evaluated
If
the
second
and
third
operands
are
arithmetic
the
usual
arithmetic
conversions
are
performed
to
bring
them
to
a
common
type
and
that
type
is
the
type
of
the
result
If
both
are
void
or
structures
or
unions
of
the
same
type
or
pointers
to
objects
of
the
same
type
the
result
has
the
common
type
If
one
is
a
pointer
and
the
other
the
constant
the
is
converted
to
the
pointer
type
and
the
result
has
that
type
If
one
is
a
pointer
to
void
and
the
other
is
another
pointer
the
other
pointer
is
converted
to
a
pointer
to
void
and
that
is
the
type
of
the
result
In
the
type
comparison
for
pointers
any
type
qualifiers
Par
A
in
the
type
to
which
the
pointer
points
are
insignificant
but
the
result
type
inherits
qualifiers
from
both
arms
of
the
conditional
A
Assignment
Expressions
There
are
several
assignment
operators
all
group
right
to
left
assignment
expression
conditional
expression
unary
expression
assignment
operator
assignment
expression
assignment
operator
one
of
All
require
an
lvalue
as
left
operand
and
the
lvalue
must
be
modifiable
it
must
not
be
an
array
and
must
not
have
an
incomplete
type
or
be
a
function
Also
its
type
must
not
be
qualified
with
const
if
it
is
a
structure
or
union
it
must
not
have
any
member
or
recursively
submember
qualified
with
const
The
type
of
an
assignment
expression
is
that
of
its
left
operand
and
the
value
is
the
value
stored
in
the
left
operand
after
the
assignment
has
taken
place
In
the
simple
assignment
with
the
value
of
the
expression
replaces
that
of
the
object
referred
to
by
the
lvalue
One
of
the
following
must
be
true
both
operands
have
arithmetic
type
in
which
case
the
right
operand
is
converted
to
the
type
of
the
left
by
the
assignment
or
both
operands
are
structures
or
unions
of
the
same
type
or
one
operand
is
a
pointer
and
the
other
is
a
pointer
to
void
or
the
left
operand
is
a
pointer
and
the
right
operand
is
a
constant
expression
with
value
or
both
operands
are
pointers
to
functions
or
objects
whose
types
are
the
same
except
for
the
possible
absence
of
const
or
volatile
in
the
right
operand
An
expression
of
the
form
E
op
E
is
equivalent
to
E
E
op
E
except
that
E
is
evaluated
only
once
A
Comma
Operator
expression
assignment
expression
expression
assignment
expression
A
pair
of
expressions
separated
by
a
comma
is
evaluated
left
to
right
and
the
value
of
the
left
expression
is
discarded
The
type
and
value
of
the
result
are
the
type
and
value
of
the
right
operand
All
side
effects
from
the
evaluation
of
the
left
operand
are
completed
before
beginning
the
evaluation
of
the
right
operand
In
contexts
where
comma
is
given
a
special
meaning
for
example
in
lists
of
function
arguments
Par
A
and
lists
of
initializers
Par
A
the
required
syntactic
unit
is
an
assignment
expression
so
the
comma
operator
appears
only
in
a
parenthetical
grouping
for
example
f
a
t
t
c
has
three
arguments
the
second
of
which
has
the
value
A
Constant
Expressions
Syntactically
a
constant
expression
is
an
expression
restricted
to
a
subset
of
operators
constant
expression
conditional
expression
Expressions
that
evaluate
to
a
constant
are
required
in
several
contexts
after
case
as
array
bounds
and
bit
field
lengths
as
the
value
of
an
enumeration
constant
in
initializers
and
in
certain
preprocessor
expressions
Constant
expressions
may
not
contain
assignments
increment
or
decrement
operators
function
calls
or
comma
operators
except
in
an
operand
of
sizeof
If
the
constant
expression
is
required
to
be
integral
its
operands
must
consist
of
integer
enumeration
character
and
floating
constants
casts
must
specify
an
integral
type
and
any
floating
constants
must
be
cast
to
integer
This
necessarily
rules
out
arrays
indirection
address
of
and
structure
member
operations
However
any
operand
is
permitted
for
sizeof
More
latitude
is
permitted
for
the
constant
expressions
of
initializers
the
operands
may
be
any
type
of
constant
and
the
unary
operator
may
be
applied
to
external
or
static
objects
and
to
external
and
static
arrays
subscripted
with
a
constant
expression
The
unary
operator
can
also
be
applied
implicitly
by
appearance
of
unsubscripted
arrays
and
functions
Initializers
must
evaluate
either
to
a
constant
or
to
the
address
of
a
previously
declared
external
or
static
object
plus
or
minus
a
constant
Less
latitude
is
allowed
for
the
integral
constant
expressions
after
if
sizeof
expressions
enumeration
constants
and
casts
are
not
permitted
See
Par
A
A
Declarations
Declarations
specify
the
interpretation
given
to
each
identifier
they
do
not
necessarily
reserve
storage
associated
with
the
identifier
Declarations
that
reserve
storage
are
called
definitions
Declarations
have
the
form
declaration
declaration
specifiers
init
declarator
listopt
The
declarators
in
the
init
declarator
list
contain
the
identifiers
being
declared
the
declarationspecifiers
consist
of
a
sequence
of
type
and
storage
class
specifiers
declaration
specifiers
storage
class
specifier
declaration
specifiersopt
type
specifier
declaration
specifiersopt
type
qualifier
declaration
specifiersopt
init
declarator
list
init
declarator
init
declarator
list
init
declarator
init
declarator
declarator
declarator
initializer
Declarators
will
be
discussed
later
Par
A
they
contain
the
names
being
declared
A
declaration
must
have
at
least
one
declarator
or
its
type
specifier
must
declare
a
structure
tag
a
union
tag
or
the
members
of
an
enumeration
empty
declarations
are
not
permitted
A
Storage
Class
Specifiers
The
storage
class
specifiers
are
storage
class
specifier
auto
register
static
extern
typedef
The
meaning
of
the
storage
classes
were
discussed
in
Par
A
The
auto
and
register
specifiers
give
the
declared
objects
automatic
storage
class
and
may
be
used
only
within
functions
Such
declarations
also
serve
as
definitions
and
cause
storage
to
be
reserved
A
register
declaration
is
equivalent
to
an
auto
declaration
but
hints
that
the
declared
objects
will
be
accessed
frequently
Only
a
few
objects
are
actually
placed
into
registers
and
only
certain
types
are
eligible
the
restrictions
are
implementation
dependent
However
if
an
object
is
declared
register
the
unary
operator
may
not
be
applied
to
it
explicitly
or
implicitly
The
rule
that
it
is
illegal
to
calculate
the
address
of
an
object
declared
register
but
actually
taken
to
be
auto
is
new
The
static
specifier
gives
the
declared
objects
static
storage
class
and
may
be
used
either
inside
or
outside
functions
Inside
a
function
this
specifier
causes
storage
to
be
allocated
and
serves
as
a
definition
for
its
effect
outside
a
function
see
Par
A
A
declaration
with
extern
used
inside
a
function
specifies
that
the
storage
for
the
declared
objects
is
defined
elsewhere
for
its
effects
outside
a
function
see
Par
A
The
typedef
specifier
does
not
reserve
storage
and
is
called
a
storage
class
specifier
only
for
syntactic
convenience
it
is
discussed
in
Par
A
At
most
one
storage
class
specifier
may
be
given
in
a
declaration
If
none
is
given
these
rules
are
used
objects
declared
inside
a
function
are
taken
to
be
auto
functions
declared
within
a
function
are
taken
to
be
extern
objects
and
functions
declared
outside
a
function
are
taken
to
be
static
with
external
linkage
See
Pars
A
A
A
Type
Specifiers
The
type
specifiers
are
type
specifier
void
char
short
int
long
float
double
signed
unsigned
struct
or
union
specifier
enum
specifier
typedef
name
At
most
one
of
the
words
long
or
short
may
be
specified
together
with
int
the
meaning
is
the
same
if
int
is
not
mentioned
The
word
long
may
be
specified
together
with
double
At
most
one
of
signed
or
unsigned
may
be
specified
together
with
int
or
any
of
its
short
or
long
varieties
or
with
char
Either
may
appear
alone
in
which
case
int
is
understood
The
signed
specifier
is
useful
for
forcing
char
objects
to
carry
a
sign
it
is
permissible
but
redundant
with
other
integral
types
Otherwise
at
most
one
type
specifier
may
be
given
in
a
declaration
If
the
type
specifier
is
missing
from
a
declaration
it
is
taken
to
be
int
Types
may
also
be
qualified
to
indicate
special
properties
of
the
objects
being
declared
type
qualifier
const
volatile
Type
qualifiers
may
appear
with
any
type
specifier
A
const
object
may
be
initialized
but
not
thereafter
assigned
to
There
are
no
implementation
dependent
semantics
for
volatile
objects
The
const
and
volatile
properties
are
new
with
the
ANSI
standard
The
purpose
of
const
is
to
announce
objects
that
may
be
placed
in
read
only
memory
and
perhaps
to
increase
opportunities
for
optimization
The
purpose
of
volatile
is
to
force
an
implementation
to
suppress
optimization
that
could
otherwise
occur
For
example
for
a
machine
with
memory
mapped
input
output
a
pointer
to
a
device
register
might
be
declared
as
a
pointer
to
volatile
in
order
to
prevent
the
compiler
from
removing
apparently
redundant
references
through
the
pointer
Except
that
it
should
diagnose
explicit
attempts
to
change
const
objects
a
compiler
may
ignore
these
qualifiers
A
Structure
and
Union
Declarations
A
structure
is
an
object
consisting
of
a
sequence
of
named
members
of
various
types
A
union
is
an
object
that
contains
at
different
times
any
of
several
members
of
various
types
Structure
and
union
specifiers
have
the
same
form
struct
or
union
specifier
struct
or
union
identifieropt
struct
declaration
list
struct
or
union
identifier
struct
or
union
struct
union
A
struct
declaration
list
is
a
sequence
of
declarations
for
the
members
of
the
structure
or
union
struct
declaration
list
struct
declaration
struct
declaration
list
struct
declaration
struct
declaration
specifier
qualifier
list
struct
declarator
list
specifier
qualifier
list
type
specifier
specifier
qualifier
listopt
type
qualifier
specifier
qualifier
listopt
struct
declarator
list
struct
declarator
struct
declarator
list
struct
declarator
Usually
a
struct
declarator
is
just
a
declarator
for
a
member
of
a
structure
or
union
A
structure
member
may
also
consist
of
a
specified
number
of
bits
Such
a
member
is
also
called
a
bit
field
its
length
is
set
off
from
the
declarator
for
the
field
name
by
a
colon
struct
declarator
declarator
declaratoropt
constant
expression
A
type
specifier
of
the
form
struct
or
union
identifier
struct
declaration
list
declares
the
identifier
to
be
the
tag
of
the
structure
or
union
specified
by
the
list
A
subsequent
declaration
in
the
same
or
an
inner
scope
may
refer
to
the
same
type
by
using
the
tag
in
a
specifier
without
the
list
struct
or
union
identifier
If
a
specifier
with
a
tag
but
without
a
list
appears
when
the
tag
is
not
declared
an
incomplete
type
is
specified
Objects
with
an
incomplete
structure
or
union
type
may
be
mentioned
in
contexts
where
their
size
is
not
needed
for
example
in
declarations
not
definitions
for
specifying
a
pointer
or
for
creating
a
typedef
but
not
otherwise
The
type
becomes
complete
on
occurrence
of
a
subsequent
specifier
with
that
tag
and
containing
a
declaration
list
Even
in
specifiers
with
a
list
the
structure
or
union
type
being
declared
is
incomplete
within
the
list
and
becomes
complete
only
at
the
terminating
the
specifier
A
structure
may
not
contain
a
member
of
incomplete
type
Therefore
it
is
impossible
to
declare
a
structure
or
union
containing
an
instance
of
itself
However
besides
giving
a
name
to
the
structure
or
union
type
tags
allow
definition
of
self
referential
structures
a
structure
or
union
may
contain
a
pointer
to
an
instance
of
itself
because
pointers
to
incomplete
types
may
be
declared
A
very
special
rule
applies
to
declarations
of
the
form
struct
or
union
identifier
that
declare
a
structure
or
union
but
have
no
declaration
list
and
no
declarators
Even
if
the
identifier
is
a
structure
or
union
tag
already
declared
in
an
outer
scope
Par
A
this
declaration
makes
the
identifier
the
tag
of
a
new
incompletely
typed
structure
or
union
in
the
current
scope
This
recondite
is
new
with
ANSI
It
is
intended
to
deal
with
mutually
recursive
structures
declared
in
an
inner
scope
but
whose
tags
might
already
be
declared
in
the
outer
scope
A
structure
or
union
specifier
with
a
list
but
no
tag
creates
a
unique
type
it
can
be
referred
to
directly
only
in
the
declaration
of
which
it
is
a
part
The
names
of
members
and
tags
do
not
conflict
with
each
other
or
with
ordinary
variables
A
member
name
may
not
appear
twice
in
the
same
structure
or
union
but
the
same
member
name
may
be
used
in
different
structures
or
unions
In
the
first
edition
of
this
book
the
names
of
structure
and
union
members
were
not
associated
with
their
parent
However
this
association
became
common
in
compilers
well
before
the
ANSI
standard
A
non
field
member
of
a
structure
or
union
may
have
any
object
type
A
field
member
which
need
not
have
a
declarator
and
thus
may
be
unnamed
has
type
int
unsigned
int
or
signed
int
and
is
interpreted
as
an
object
of
integral
type
of
the
specified
length
in
bits
whether
an
int
field
is
treated
as
signed
is
implementation
dependent
Adjacent
field
members
of
structures
are
packed
into
implementation
dependent
storage
units
in
an
implementationdependent
direction
When
a
field
following
another
field
will
not
fit
into
a
partially
filled
storage
unit
it
may
be
split
between
units
or
the
unit
may
be
padded
An
unnamed
field
with
width
forces
this
padding
so
that
the
next
field
will
begin
at
the
edge
of
the
next
allocation
unit
The
ANSI
standard
makes
fields
even
more
implementation
dependent
than
did
the
first
edition
It
is
advisable
to
read
the
language
rules
for
storing
bit
fields
as
implementation
dependent
without
qualification
Structures
with
bit
fields
may
be
used
as
a
portable
way
of
attempting
to
reduce
the
storage
required
for
a
structure
with
the
probable
cost
of
increasing
the
instruction
space
and
time
needed
to
access
the
fields
or
as
a
non
portable
way
to
describe
a
storage
layout
known
at
the
bitlevel
In
the
second
case
it
is
necessary
to
understand
the
rules
of
the
local
implementation
The
members
of
a
structure
have
addresses
increasing
in
the
order
of
their
declarations
A
nonfield
member
of
a
structure
is
aligned
at
an
addressing
boundary
depending
on
its
type
therefore
there
may
be
unnamed
holes
in
a
structure
If
a
pointer
to
a
structure
is
cast
to
the
type
of
a
pointer
to
its
first
member
the
result
refers
to
the
first
member
A
union
may
be
thought
of
as
a
structure
all
of
whose
members
begin
at
offset
and
whose
size
is
sufficient
to
contain
any
of
its
members
At
most
one
of
the
members
can
be
stored
in
a
union
at
any
time
If
a
pointr
to
a
union
is
cast
to
the
type
of
a
pointer
to
a
member
the
result
refers
to
that
member
A
simple
example
of
a
structure
declaration
is
struct
tnode
char
tword
int
count
struct
tnode
left
struct
tnode
right
which
contains
an
array
of
characters
an
integer
and
two
pointers
to
similar
structures
Once
this
declaration
has
bene
given
the
declaration
struct
tnode
s
sp
declares
s
to
be
a
structure
of
the
given
sort
and
sp
to
be
a
pointer
to
a
structure
of
the
given
sort
With
these
declarations
the
expression
sp
count
refers
to
the
count
field
of
the
structure
to
which
sp
points
s
left
refers
to
the
left
subtree
pointer
of
the
structure
s
and
s
right
tword
refers
to
the
first
character
of
the
tword
member
of
the
right
subtree
of
s
In
general
a
member
of
a
union
may
not
be
inspected
unless
the
value
of
the
union
has
been
assigned
using
the
same
member
However
one
special
guarantee
simplifies
the
use
of
unions
if
a
union
contains
several
structures
that
share
a
common
initial
sequence
and
the
union
currently
contains
one
of
these
structures
it
is
permitted
to
refer
to
the
common
initial
part
of
any
of
the
contained
structures
For
example
the
following
is
a
legal
fragment
union
struct
int
type
n
struct
int
type
int
intnode
ni
struct
int
type
float
floatnode
nf
u
u
nf
type
FLOAT
u
nf
floatnode
if
u
n
type
FLOAT
sin
u
nf
floatnode
A
Enumerations
Enumerations
are
unique
types
with
values
ranging
over
a
set
of
named
constants
called
enumerators
The
form
of
an
enumeration
specifier
borrows
from
that
of
structures
and
unions
enum
specifier
enum
identifieropt
enumerator
list
enum
identifier
enumerator
list
enumerator
enumerator
list
enumerator
enumerator
identifier
identifier
constant
expression
The
identifiers
in
an
enumerator
list
are
declared
as
constants
of
type
int
and
may
appear
wherever
constants
are
required
If
no
enumerations
with
appear
then
the
values
of
the
corresponding
constants
begin
at
and
increase
by
as
the
declaration
is
read
from
left
to
right
An
enumerator
with
gives
the
associated
identifier
the
value
specified
subsequent
identifiers
continue
the
progression
from
the
assigned
value
Enumerator
names
in
the
same
scope
must
all
be
distinct
from
each
other
and
from
ordinary
variable
names
but
the
values
need
not
be
distinct
The
role
of
the
identifier
in
the
enum
specifier
is
analogous
to
that
of
the
structure
tag
in
a
struct
specifier
it
names
a
particular
enumeration
The
rules
for
enum
specifiers
with
and
without
tags
and
lists
are
the
same
as
those
for
structure
or
union
specifiers
except
that
incomplete
enumeration
types
do
not
exist
the
tag
of
an
enum
specifier
without
an
enumerator
list
must
refer
to
an
in
scope
specifier
with
a
list
Enumerations
are
new
since
the
first
edition
of
this
book
but
have
been
part
of
the
language
for
some
years
A
Declarators
Declarators
have
the
syntax
declarator
pointeropt
direct
declarator
direct
declarator
identifier
declarator
direct
declarator
constant
expressionopt
direct
declarator
parameter
type
list
direct
declarator
identifier
listopt
pointer
type
qualifier
listopt
type
qualifier
listopt
pointer
type
qualifier
list
type
qualifier
type
qualifier
list
type
qualifier
The
structure
of
declarators
resembles
that
of
indirection
function
and
array
expressions
the
grouping
is
the
same
A
Meaning
of
Declarators
A
list
of
declarators
appears
after
a
sequence
of
type
and
storage
class
specifiers
Each
declarator
declares
a
unique
main
identifier
the
one
that
appears
as
the
first
alternative
of
the
production
for
direct
declarator
The
storage
class
specifiers
apply
directly
to
this
identifier
but
its
type
depends
on
the
form
of
its
declarator
A
declarator
is
read
as
an
assertion
that
when
its
identifier
appears
in
an
expression
of
the
same
form
as
the
declarator
it
yields
an
object
of
the
specified
type
Considering
only
the
type
parts
of
the
declaration
specifiers
Par
A
and
a
particular
declarator
a
declaration
has
the
form
T
D
where
T
is
a
type
and
D
is
a
declarator
The
type
attributed
to
the
identifier
in
the
various
forms
of
declarator
is
described
inductively
using
this
notation
In
a
declaration
T
D
where
D
is
an
unadored
identifier
the
type
of
the
identifier
is
T
In
a
declaration
T
D
where
D
has
the
form
D
then
the
type
of
the
identifier
in
D
is
the
same
as
that
of
D
The
parentheses
do
not
alter
the
type
but
may
change
the
binding
of
complex
declarators
A
Pointer
Declarators
In
a
declaration
T
D
where
D
has
the
form
type
qualifier
listopt
D
and
the
type
of
the
identifier
in
the
declaration
T
D
is
type
modifier
T
the
type
of
the
identifier
of
D
is
type
modifier
type
qualifier
list
pointer
to
T
Qualifiers
following
apply
to
pointer
itself
rather
than
to
the
object
to
which
the
pointer
points
For
example
consider
the
declaration
int
ap
Here
ap
plays
the
role
of
D
a
declaration
int
ap
below
would
give
ap
the
type
array
of
int
the
type
qualifier
list
is
empty
and
the
type
modifier
is
array
of
Hence
the
actual
declaration
gives
ap
the
type
array
to
pointers
to
int
As
other
examples
the
declarations
int
i
pi
const
cpi
i
const
int
ci
pci
declare
an
integer
i
and
a
pointer
to
an
integer
pi
The
value
of
the
constant
pointer
cpi
may
not
be
changed
it
will
always
point
to
the
same
location
although
the
value
to
which
it
refers
may
be
altered
The
integer
ci
is
constant
and
may
not
be
changed
though
it
may
be
initialized
as
here
The
type
of
pci
is
pointer
to
const
int
and
pci
itself
may
be
changed
to
point
to
another
place
but
the
value
to
which
it
points
may
not
be
altered
by
assigning
through
pci
A
Array
Declarators
In
a
declaration
T
D
where
D
has
the
form
D
constant
expressionopt
and
the
type
of
the
identifier
in
the
declaration
T
D
is
type
modifier
T
the
type
of
the
identifier
of
D
is
type
modifier
array
of
T
If
the
constant
expression
is
present
it
must
have
integral
type
and
value
greater
than
If
the
constant
expression
specifying
the
bound
is
missing
the
array
has
an
incomplete
type
An
array
may
be
constructed
from
an
arithmetic
type
from
a
pointer
from
a
structure
or
union
or
from
another
array
to
generate
a
multi
dimensional
array
Any
type
from
which
an
array
is
constructed
must
be
complete
it
must
not
be
an
array
of
structure
of
incomplete
type
This
implies
that
for
a
multi
dimensional
array
only
the
first
dimension
may
be
missing
The
type
of
an
object
of
incomplete
aray
type
is
completed
by
another
complete
declaration
for
the
object
Par
A
or
by
initializing
it
Par
A
For
example
float
fa
afp
declares
an
array
of
float
numbers
and
an
array
of
pointers
to
float
numbers
Also
static
int
x
d
declares
a
static
three
dimensional
array
of
integers
with
rank
X
X
In
complete
detail
x
d
is
an
array
of
three
items
each
item
is
an
array
of
five
arrays
each
of
the
latter
arrays
is
an
array
of
seven
integers
Any
of
the
expressions
x
d
x
d
i
x
d
i
j
x
d
i
j
k
may
reasonably
appear
in
an
expression
The
first
three
have
type
array
the
last
has
type
int
More
specifically
x
d
i
j
is
an
array
of
integers
and
x
d
i
is
an
array
of
arrays
of
integers
The
array
subscripting
operation
is
defined
so
that
E
E
is
identical
to
E
E
Therefore
despite
its
asymmetric
appearance
subscripting
is
a
commutative
operation
Because
of
the
conversion
rules
that
apply
to
and
to
arrays
Pars
A
A
A
if
E
is
an
array
and
E
an
integer
then
E
E
refers
to
the
E
th
member
of
E
In
the
example
x
d
i
j
k
is
equivalent
to
x
d
i
j
k
The
first
subexpression
x
d
i
j
is
converted
by
Par
A
to
type
pointer
to
array
of
integers
by
Par
A
the
addition
involves
multiplication
by
the
size
of
an
integer
It
follows
from
the
rules
that
arrays
are
stored
by
rows
last
subscript
varies
fastest
and
that
the
first
subscript
in
the
declaration
helps
determine
the
amount
of
storage
consumed
by
an
array
but
plays
no
other
part
in
subscript
calculations
A
Function
Declarators
In
a
new
style
function
declaration
T
D
where
D
has
the
form
D
parameter
type
list
and
the
type
of
the
identifier
in
the
declaration
T
D
is
type
modifier
T
the
type
of
the
identifier
of
D
is
type
modifier
function
with
arguments
parameter
type
list
returning
T
The
syntax
of
the
parameters
is
parameter
type
list
parameter
list
parameter
list
parameter
list
parameter
declaration
parameter
list
parameter
declaration
parameter
declaration
declaration
specifiers
declarator
declaration
specifiers
abstract
declaratoropt
In
the
new
style
declaration
the
parameter
list
specifies
the
types
of
the
parameters
As
a
special
case
the
declarator
for
a
new
style
function
with
no
parameters
has
a
parameter
list
consisting
soley
of
the
keyword
void
If
the
parameter
list
ends
with
an
ellipsis
then
the
function
may
accept
more
arguments
than
the
number
of
parameters
explicitly
described
see
Par
A
The
types
of
parameters
that
are
arrays
or
functions
are
altered
to
pointers
in
accordance
with
the
rules
for
parameter
conversions
see
Par
A
The
only
storage
class
specifier
permitted
in
a
parameter
s
declaration
is
register
and
this
specifier
is
ignored
unless
the
function
declarator
heads
a
function
definition
Similarly
if
the
declarators
in
the
parameter
declarations
contain
identifiers
and
the
function
declarator
does
not
head
a
function
definition
the
identifiers
go
out
of
scope
immediately
Abstract
declarators
which
do
not
mention
the
identifiers
are
discussed
in
Par
A
In
an
old
style
function
declaration
T
D
where
D
has
the
form
D
identifier
listopt
and
the
type
of
the
identifier
in
the
declaration
T
D
is
type
modifier
T
the
type
of
the
identifier
of
D
is
type
modifier
function
of
unspecified
arguments
returning
T
The
parameters
if
present
have
the
form
identifier
list
identifier
identifier
list
identifier
In
the
old
style
declarator
the
identifier
list
must
be
absent
unless
the
declarator
is
used
in
the
head
of
a
function
definition
Par
A
No
information
about
the
types
of
the
parameters
is
supplied
by
the
declaration
For
example
the
declaration
int
f
fpi
pfi
declares
a
function
f
returning
an
integer
a
function
fpi
returning
a
pointer
to
an
integer
and
a
pointer
pfi
to
a
function
returning
an
integer
In
none
of
these
are
the
parameter
types
specified
they
are
old
style
In
the
new
style
declaration
int
strcpy
char
dest
const
char
source
rand
void
strcpy
is
a
function
returning
int
with
two
arguments
the
first
a
character
pointer
and
the
second
a
pointer
to
constant
characters
The
parameter
names
are
effectively
comments
The
second
function
rand
takes
no
arguments
and
returns
int
Function
declarators
with
parameter
prototypes
are
by
far
the
most
important
language
change
introduced
by
the
ANSI
standard
They
offer
an
advantage
over
the
old
style
declarators
of
the
first
edition
by
providing
error
detection
and
coercion
of
arguments
across
function
calls
but
at
a
cost
turmoil
and
confusion
during
their
introduction
and
the
necessity
of
accomodating
both
forms
Some
syntactic
ugliness
was
required
for
the
sake
of
compatibility
namely
void
as
an
explicit
marker
of
new
style
functions
without
parameters
The
ellipsis
notation
for
variadic
functions
is
also
new
and
together
with
the
macros
in
the
standard
header
stdarg
h
formalizes
a
mechanism
that
was
officially
forbidden
but
unofficially
condoned
in
the
first
edition
These
notations
were
adapted
from
the
C
language
A
Initialization
When
an
object
is
declared
its
init
declarator
may
specify
an
initial
value
for
the
identifier
being
declared
The
initializer
is
preceded
by
and
is
either
an
expression
or
a
list
of
initializers
nested
in
braces
A
list
may
end
with
a
comma
a
nicety
for
neat
formatting
initializer
assignment
expression
initializer
list
initializer
list
initializer
list
initializer
initializer
list
initializer
All
the
expressions
in
the
initializer
for
a
static
object
or
array
must
be
constant
expressions
as
described
in
Par
A
The
expressions
in
the
initializer
for
an
auto
or
register
object
or
array
must
likewise
be
constant
expressions
if
the
initializer
is
a
brace
enclosed
list
However
if
the
initializer
for
an
automatic
object
is
a
single
expression
it
need
not
be
a
constant
expression
but
must
merely
have
appropriate
type
for
assignment
to
the
object
The
first
edition
did
not
countenance
initialization
of
automatic
structures
unions
or
arrays
The
ANSI
standard
allows
it
but
only
by
constant
constructions
unless
the
initializer
can
be
expressed
by
a
simple
expression
A
static
object
not
explicitly
initialized
is
initialized
as
if
it
or
its
members
were
assigned
the
constant
The
initial
value
of
an
automatic
object
not
explicitly
intialized
is
undefined
The
initializer
for
a
pointer
or
an
object
of
arithmetic
type
is
a
single
expression
perhaps
in
braces
The
expression
is
assigned
to
the
object
The
initializer
for
a
structure
is
either
an
expression
of
the
same
type
or
a
brace
enclosed
list
of
initializers
for
its
members
in
order
Unnamed
bit
field
members
are
ignored
and
are
not
initialized
If
there
are
fewer
initializers
in
the
list
than
members
of
the
structure
the
trailing
members
are
initialized
with
There
may
not
be
more
initializers
than
members
Unnamed
bitfield
members
are
ignored
and
are
not
initialized
The
initializer
for
an
array
is
a
brace
enclosed
list
of
initializers
for
its
members
If
the
array
has
unknown
size
the
number
of
initializers
determines
the
size
of
the
array
and
its
type
becomes
complete
If
the
array
has
fixed
size
the
number
of
initializers
may
not
exceed
the
number
of
members
of
the
array
if
there
are
fewer
the
trailing
members
are
initialized
with
As
a
special
case
a
character
array
may
be
initialized
by
a
string
literal
successive
characters
of
the
string
initialize
successive
members
of
the
array
Similarly
a
wide
character
literal
Par
A
may
initialize
an
array
of
type
wchar
t
If
the
array
has
unknown
size
the
number
of
characters
in
the
string
including
the
terminating
null
character
determines
its
size
if
its
size
is
fixed
the
number
of
characters
in
the
string
not
counting
the
terminating
null
character
must
not
exceed
the
size
of
the
array
The
initializer
for
a
union
is
either
a
single
expression
of
the
same
type
or
a
brace
enclosed
initializer
for
the
first
member
of
the
union
The
first
edition
did
not
allow
initialization
of
unions
The
first
member
rule
is
clumsy
but
is
hard
to
generalize
without
new
syntax
Besides
allowing
unions
to
be
explicitly
initialized
in
at
least
a
primitive
way
this
ANSI
rule
makes
definite
the
semantics
of
static
unions
not
explicitly
initialized
An
aggregate
is
a
structure
or
array
If
an
aggregate
contains
members
of
aggregate
type
the
initialization
rules
apply
recursively
Braces
may
be
elided
in
the
initialization
as
follows
if
the
initializer
for
an
aggregate
s
member
that
itself
is
an
aggregate
begins
with
a
left
brace
then
the
succeding
comma
separated
list
of
initializers
initializes
the
members
of
the
subaggregate
it
is
erroneous
for
there
to
be
more
initializers
than
members
If
however
the
initializer
for
a
subaggregate
does
not
begin
with
a
left
brace
then
only
enough
elements
from
the
list
are
taken
into
account
for
the
members
of
the
subaggregate
any
remaining
members
are
left
to
initialize
the
next
member
of
the
aggregate
of
which
the
subaggregate
is
a
part
For
example
int
x
declares
and
initializes
x
as
a
dimensional
array
with
three
members
since
no
size
was
specified
and
there
are
three
initializers
float
y
is
a
completely
bracketed
initialization
and
initialize
the
first
row
of
the
array
y
namely
y
y
and
y
Likewise
the
next
two
lines
initialize
y
and
y
The
initializer
ends
early
and
therefore
the
elements
of
y
are
initialized
with
Precisely
the
same
effect
could
have
been
achieved
by
float
y
The
initializer
for
y
begins
with
a
left
brace
but
that
for
y
does
not
therefore
three
elements
from
the
list
are
used
Likewise
the
next
three
are
taken
successively
for
y
and
for
y
Also
float
y
initializes
the
first
column
of
y
regarded
as
a
two
dimensional
array
and
leaves
the
rest
Finally
char
msg
Syntax
error
on
line
s
n
shows
a
character
array
whose
members
are
initialized
with
a
string
its
size
includes
the
terminating
null
character
A
Type
names
In
several
contexts
to
specify
type
conversions
explicitly
with
a
cast
to
declare
parameter
types
in
function
declarators
and
as
argument
of
sizeof
it
is
necessary
to
supply
the
name
of
a
data
type
This
is
accomplished
using
a
type
name
which
is
syntactically
a
declaration
for
an
object
of
that
type
omitting
the
name
of
the
object
type
name
specifier
qualifier
list
abstract
declaratoropt
abstract
declarator
pointer
pointeropt
direct
abstract
declarator
direct
abstract
declarator
abstract
declarator
direct
abstract
declaratoropt
constant
expressionopt
direct
abstract
declaratoropt
parameter
type
listopt
It
is
possible
to
identify
uniquely
the
location
in
the
abstract
declarator
where
the
identifier
would
appear
if
the
construction
were
a
declarator
in
a
declaration
The
named
type
is
then
the
same
as
the
type
of
the
hypothetical
identifier
For
example
int
int
int
int
int
int
void
name
respectively
the
types
integer
pointer
to
integer
array
of
pointers
to
integers
pointer
to
an
unspecified
number
of
integers
function
of
unspecified
parameters
returning
pointer
to
integer
and
array
of
unspecified
size
of
pointers
to
functions
with
no
parameters
each
returning
an
integer
A
Typedef
Declarations
whose
storage
class
specifier
is
typedef
do
not
declare
objects
instead
they
define
identifiers
that
name
types
These
identifiers
are
called
typedef
names
typedef
name
identifier
A
typedef
declaration
attributes
a
type
to
each
name
among
its
declarators
in
the
usual
way
see
Par
A
Thereafter
each
such
typedef
name
is
syntactically
equivalent
to
a
type
specifier
keyword
for
the
associated
type
For
example
after
typedef
long
Blockno
Blockptr
typedef
struct
double
r
theta
Complex
the
constructions
Blockno
b
extern
Blockptr
bp
Complex
z
zp
are
legal
declarations
The
type
of
b
is
long
that
of
bp
is
pointer
to
long
and
that
of
z
is
the
specified
structure
zp
is
a
pointer
to
such
a
structure
typedef
does
not
introduce
new
types
only
synonyms
for
types
that
could
be
specified
in
another
way
In
the
example
b
has
the
same
type
as
any
long
object
Typedef
names
may
be
redeclared
in
an
inner
scope
but
a
non
empty
set
of
type
specifiers
must
be
given
For
example
extern
Blockno
does
not
redeclare
Blockno
but
extern
int
Blockno
does
A
Type
Equivalence
Two
type
specifier
lists
are
equivalent
if
they
contain
the
same
set
of
type
specifiers
taking
into
account
that
some
specifiers
can
be
implied
by
others
for
example
long
alone
implies
long
int
Structures
unions
and
enumerations
with
different
tags
are
distinct
and
a
tagless
union
structure
or
enumeration
specifies
a
unique
type
Two
types
are
the
same
if
their
abstract
declarators
Par
A
after
expanding
any
typedef
types
and
deleting
any
function
parameter
specifiers
are
the
same
up
to
the
equivalence
of
type
specifier
lists
Array
sizes
and
function
parameter
types
are
significant
A
Statements
Except
as
described
statements
are
executed
in
sequence
Statements
are
executed
for
their
effect
and
do
not
have
values
They
fall
into
several
groups
statement
labeled
statement
expression
statement
compound
statement
selection
statement
iteration
statement
jump
statement
A
Labeled
Statements
Statements
may
carry
label
prefixes
labeled
statement
identifier
statement
case
constant
expression
statement
default
statement
A
label
consisting
of
an
identifier
declares
the
identifier
The
only
use
of
an
identifier
label
is
as
a
target
of
goto
The
scope
of
the
identifier
is
the
current
function
Because
labels
have
their
own
name
space
they
do
not
interfere
with
other
identifiers
and
cannot
be
redeclared
See
Par
A
Case
labels
and
default
labels
are
used
with
the
switch
statement
Par
A
The
constant
expression
of
case
must
have
integral
type
Labels
themselves
do
not
alter
the
flow
of
control
A
Expression
Statement
Most
statements
are
expression
statements
which
have
the
form
expression
statement
expressionopt
Most
expression
statements
are
assignments
or
function
calls
All
side
effects
from
the
expression
are
completed
before
the
next
statement
is
executed
If
the
expression
is
missing
the
construction
is
called
a
null
statement
it
is
often
used
to
supply
an
empty
body
to
an
iteration
statement
to
place
a
label
A
Compound
Statement
So
that
several
statements
can
be
used
where
one
is
expected
the
compound
statement
also
called
block
is
provided
The
body
of
a
function
definition
is
a
compound
statement
compound
statement
declaration
listopt
statement
listopt
declaration
list
declaration
declaration
list
declaration
statement
list
statement
statement
list
statement
If
an
identifier
in
the
declaration
list
was
in
scope
outside
the
block
the
outer
declaration
is
suspended
within
the
block
see
Par
A
after
which
it
resumes
its
force
An
identifier
may
be
declared
only
once
in
the
same
block
These
rules
apply
to
identifiers
in
the
same
name
space
Par
A
identifiers
in
different
name
spaces
are
treated
as
distinct
Initialization
of
automatic
objects
is
performed
each
time
the
block
is
entered
at
the
top
and
proceeds
in
the
order
of
the
declarators
If
a
jump
into
the
block
is
executed
these
initializations
are
not
performed
Initialization
of
static
objects
are
performed
only
once
before
the
program
begins
execution
A
Selection
Statements
Selection
statements
choose
one
of
several
flows
of
control
selection
statement
if
expression
statement
if
expression
statement
else
statement
switch
expression
statement
In
both
forms
of
the
if
statement
the
expression
which
must
have
arithmetic
or
pointer
type
is
evaluated
including
all
side
effects
and
if
it
compares
unequal
to
the
first
substatement
is
executed
In
the
second
form
the
second
substatement
is
executed
if
the
expression
is
The
else
ambiguity
is
resolved
by
connecting
an
else
with
the
last
encountered
else
less
if
at
the
same
block
nesting
level
The
switch
statement
causes
control
to
be
transferred
to
one
of
several
statements
depending
on
the
value
of
an
expression
which
must
have
integral
type
The
substatement
controlled
by
a
switch
is
typically
compound
Any
statement
within
the
substatement
may
be
labeled
with
one
or
more
case
labels
Par
A
The
controlling
expression
undergoes
integral
promotion
Par
A
and
the
case
constants
are
converted
to
the
promoted
type
No
two
of
these
case
constants
associated
with
the
same
switch
may
have
the
same
value
after
conversion
There
may
also
be
at
most
one
default
label
associated
with
a
switch
Switches
may
be
nested
a
case
or
default
label
is
associated
with
the
smallest
switch
that
contains
it
When
the
switch
statement
is
executed
its
expression
is
evaluated
including
all
side
effects
and
compared
with
each
case
constant
If
one
of
the
case
constants
is
equal
to
the
value
of
the
expression
control
passes
to
the
statement
of
the
matched
case
label
If
no
case
constant
matches
the
expression
and
if
there
is
a
default
label
control
passes
to
the
labeled
statement
If
no
case
matches
and
if
there
is
no
default
then
none
of
the
substatements
of
the
swtich
is
executed
In
the
first
edition
of
this
book
the
controlling
expression
of
switch
and
the
case
constants
were
required
to
have
int
type
A
Iteration
Statements
Iteration
statements
specify
looping
iteration
statement
while
expression
statement
do
statement
while
expression
for
expressionopt
expressionopt
expressionopt
statement
In
the
while
and
do
statements
the
substatement
is
executed
repeatedly
so
long
as
the
value
of
the
expression
remains
unequal
to
the
expression
must
have
arithmetic
or
pointer
type
With
while
the
test
including
all
side
effects
from
the
expression
occurs
before
each
execution
of
the
statement
with
do
the
test
follows
each
iteration
In
the
for
statement
the
first
expression
is
evaluated
once
and
thus
specifies
initialization
for
the
loop
There
is
no
restriction
on
its
type
The
second
expression
must
have
arithmetic
or
pointer
type
it
is
evaluated
before
each
iteration
and
if
it
becomes
equal
to
the
for
is
terminated
The
third
expression
is
evaluated
after
each
iteration
and
thus
specifies
a
reinitialization
for
the
loop
There
is
no
restriction
on
its
type
Side
effects
from
each
expression
are
completed
immediately
after
its
evaluation
If
the
substatement
does
not
contain
continue
a
statement
for
expression
expression
expression
statement
is
equivalent
to
expression
while
expression
statement
expression
Any
of
the
three
expressions
may
be
dropped
A
missing
second
expression
makes
the
implied
test
equivalent
to
testing
a
non
zero
element
A
Jump
statements
Jump
statements
transfer
control
unconditionally
jump
statement
goto
identifier
continue
break
return
expressionopt
In
the
goto
statement
the
identifier
must
be
a
label
Par
A
located
in
the
current
function
Control
transfers
to
the
labeled
statement
A
continue
statement
may
appear
only
within
an
iteration
statement
It
causes
control
to
pass
to
the
loop
continuation
portion
of
the
smallest
enclosing
such
statement
More
precisely
within
each
of
the
statements
while
do
for
contin
contin
contin
while
a
continue
not
contained
in
a
smaller
iteration
statement
is
the
same
as
goto
contin
A
break
statement
may
appear
only
in
an
iteration
statement
or
a
switch
statement
and
terminates
execution
of
the
smallest
enclosing
such
statement
control
passes
to
the
statement
following
the
terminated
statement
A
function
returns
to
its
caller
by
the
return
statement
When
return
is
followed
by
an
expression
the
value
is
returned
to
the
caller
of
the
function
The
expression
is
converted
as
by
assignment
to
the
type
returned
by
the
function
in
which
it
appears
Flowing
off
the
end
of
a
function
is
equivalent
to
a
return
with
no
expression
In
either
case
the
returned
value
is
undefined
A
External
Declarations
The
unit
of
input
provided
to
the
C
compiler
is
called
a
translation
unit
it
consists
of
a
sequence
of
external
declarations
which
are
either
declarations
or
function
definitions
translation
unit
external
declaration
translation
unit
external
declaration
external
declaration
function
definition
declaration
The
scope
of
external
declarations
persists
to
the
end
of
the
translation
unit
in
which
they
are
declared
just
as
the
effect
of
declarations
within
the
blocks
persists
to
the
end
of
the
block
The
syntax
of
external
declarations
is
the
same
as
that
of
all
declarations
except
that
only
at
this
level
may
the
code
for
functions
be
given
A
Function
Definitions
Function
definitions
have
the
form
function
definition
declaration
specifiersopt
declarator
declaration
listopt
compound
statement
The
only
storage
class
specifiers
allowed
among
the
declaration
specifiers
are
extern
or
static
see
Par
A
for
the
distinction
between
them
A
function
may
return
an
arithmetic
type
a
structure
a
union
a
pointer
or
void
but
not
a
function
or
an
array
The
declarator
in
a
function
declaration
must
specify
explicitly
that
the
declared
identifier
has
function
type
that
is
it
must
contain
one
of
the
forms
see
Par
A
direct
declarator
parameter
type
list
direct
declarator
identifier
listopt
where
the
direct
declarator
is
an
identifier
or
a
parenthesized
identifier
In
particular
it
must
not
achieve
function
type
by
means
of
a
typedef
In
the
first
form
the
definition
is
a
new
style
function
and
its
parameters
together
with
their
types
are
declared
in
its
parameter
type
list
the
declaration
list
following
the
function
s
declarator
must
be
absent
Unless
the
parameter
type
list
consists
solely
of
void
showing
that
the
function
takes
no
parameters
each
declarator
in
the
parameter
type
list
must
contain
an
identifier
If
the
parameter
type
list
ends
with
then
the
function
may
be
called
with
more
arguments
than
parameters
the
va
arg
macro
mechanism
defined
in
the
standard
header
stdarg
h
and
described
in
Appendix
B
must
be
used
to
refer
to
the
extra
arguments
Variadic
functions
must
have
at
least
one
named
parameter
In
the
second
form
the
definition
is
old
style
the
identifier
list
names
the
parameters
while
the
declaration
list
attributes
types
to
them
If
no
declaration
is
given
for
a
parameter
its
type
is
taken
to
be
int
The
declaration
list
must
declare
only
parameters
named
in
the
list
initialization
is
not
permitted
and
the
only
storage
class
specifier
possible
is
register
In
both
styles
of
function
definition
the
parameters
are
understood
to
be
declared
just
after
the
beginning
of
the
compound
statement
constituting
the
function
s
body
and
thus
the
same
identifiers
must
not
be
redeclared
there
although
they
may
like
other
identifiers
be
redeclared
in
inner
blocks
If
a
parameter
is
declared
to
have
type
array
of
type
the
declaration
is
adjusted
to
read
pointer
to
type
similarly
if
a
parameter
is
declared
to
have
type
function
returning
type
the
declaration
is
adjusted
to
read
pointer
to
function
returning
type
During
the
call
to
a
function
the
arguments
are
converted
as
necessary
and
assigned
to
the
parameters
see
Par
A
New
style
function
definitions
are
new
with
the
ANSI
standard
There
is
also
a
small
change
in
the
details
of
promotion
the
first
edition
specified
that
the
declarations
of
float
parameters
were
adjusted
to
read
double
The
difference
becomes
noticable
when
a
pointer
to
a
parameter
is
generated
within
a
function
A
complete
example
of
a
new
style
function
definition
is
int
max
int
a
int
b
int
c
int
m
m
a
b
a
b
return
m
c
m
c
Here
int
is
the
declaration
specifier
max
int
a
int
b
int
c
is
the
function
s
declarator
and
is
the
block
giving
the
code
for
the
function
The
corresponding
old
style
definition
would
be
int
max
a
b
c
int
a
b
c
where
now
int
max
a
b
c
is
the
declarator
and
int
a
b
c
is
the
declaration
list
for
the
parameters
A
External
Declarations
External
declarations
specify
the
characteristics
of
objects
functions
and
other
identifiers
The
term
external
refers
to
their
location
outside
functions
and
is
not
directly
connected
with
the
extern
keyword
the
storage
class
for
an
externally
declared
object
may
be
left
empty
or
it
may
be
specified
as
extern
or
static
Several
external
declarations
for
the
same
identifier
may
exist
within
the
same
translation
unit
if
they
agree
in
type
and
linkage
and
if
there
is
at
most
one
definition
for
the
identifier
Two
declarations
for
an
object
or
function
are
deemed
to
agree
in
type
under
the
rule
discussed
in
Par
A
In
addition
if
the
declarations
differ
because
one
type
is
an
incomplete
structure
union
or
enumeration
type
Par
A
and
the
other
is
the
corresponding
completed
type
with
the
same
tag
the
types
are
taken
to
agree
Moreover
if
one
type
is
an
incomplete
array
type
Par
A
and
the
other
is
a
completed
array
type
the
types
if
otherwise
identical
are
also
taken
to
agree
Finally
if
one
type
specifies
an
old
style
function
and
the
other
an
otherwise
identical
new
style
function
with
parameter
declarations
the
types
are
taken
to
agree
If
the
first
external
declarator
for
a
function
or
object
includes
the
static
specifier
the
identifier
has
internal
linkage
otherwise
it
has
external
linkage
Linkage
is
discussed
in
Par
An
external
declaration
for
an
object
is
a
definition
if
it
has
an
initializer
An
external
object
declaration
that
does
not
have
an
initializer
and
does
not
contain
the
extern
specifier
is
a
tentative
definition
If
a
definition
for
an
object
appears
in
a
translation
unit
any
tentative
definitions
are
treated
merely
as
redundant
declarations
If
no
definition
for
the
object
appears
in
the
translation
unit
all
its
tentative
definitions
become
a
single
definition
with
initializer
Each
object
must
have
exactly
one
definition
For
objects
with
internal
linkage
this
rule
applies
separately
to
each
translation
unit
because
internally
linked
objects
are
unique
to
a
translation
unit
For
objects
with
external
linkage
it
applies
to
the
entire
program
Although
the
one
definition
rule
is
formulated
somewhat
differently
in
the
first
edition
of
this
book
it
is
in
effect
identical
to
the
one
stated
here
Some
implementations
relax
it
by
generalizing
the
notion
of
tentative
definition
In
the
alternate
formulation
which
is
usual
in
UNIX
systems
and
recognized
as
a
common
extension
by
the
Standard
all
the
tentative
definitions
for
an
externally
linked
object
throughout
all
the
translation
units
of
the
program
are
considered
together
instead
of
in
each
translation
unit
separately
If
a
definition
occurs
somewhere
in
the
program
then
the
tentative
definitions
become
merely
declarations
but
if
no
definition
appears
then
all
its
tentative
definitions
become
a
definition
with
initializer
A
Scope
and
Linkage
A
program
need
not
all
be
compiled
at
one
time
the
source
text
may
be
kept
in
several
files
containing
translation
units
and
precompiled
routines
may
be
loaded
from
libraries
Communication
among
the
functions
of
a
program
may
be
carried
out
both
through
calls
and
through
manipulation
of
external
data
Therefore
there
are
two
kinds
of
scope
to
consider
first
the
lexical
scope
of
an
identifier
which
is
the
region
of
the
program
text
within
which
the
identifier
s
characteristics
are
understood
and
second
the
scope
associated
with
objects
and
functions
with
external
linkage
which
determines
the
connections
between
identifiers
in
separately
compiled
translation
units
A
Lexical
Scope
Identifiers
fall
into
several
name
spaces
that
do
not
interfere
with
one
another
the
same
identifier
may
be
used
for
different
purposes
even
in
the
same
scope
if
the
uses
are
in
different
name
spaces
These
classes
are
objects
functions
typedef
names
and
enum
constants
labels
tags
of
structures
or
unions
and
enumerations
and
members
of
each
structure
or
union
individually
These
rules
differ
in
several
ways
from
those
described
in
the
first
edition
of
this
manual
Labels
did
not
previously
have
their
own
name
space
tags
of
structures
and
unions
each
had
a
separate
space
and
in
some
implementations
enumerations
tags
did
as
well
putting
different
kinds
of
tags
into
the
same
space
is
a
new
restriction
The
most
important
departure
from
the
first
edition
is
that
each
structure
or
union
creates
a
separate
name
space
for
its
members
so
that
the
same
name
may
appear
in
several
different
structures
This
rule
has
been
common
practice
for
several
years
The
lexical
scope
of
an
object
or
function
identifier
in
an
external
declaration
begins
at
the
end
of
its
declarator
and
persists
to
the
end
of
the
translation
unit
in
which
it
appears
The
scope
of
a
parameter
of
a
function
definition
begins
at
the
start
of
the
block
defining
the
function
and
persists
through
the
function
the
scope
of
a
parameter
in
a
function
declaration
ends
at
the
end
of
the
declarator
The
scope
of
an
identifier
declared
at
the
head
of
a
block
begins
at
the
end
of
its
declarator
and
persists
to
the
end
of
the
block
The
scope
of
a
label
is
the
whole
of
the
function
in
which
it
appears
The
scope
of
a
structure
union
or
enumeration
tag
or
an
enumeration
constant
begins
at
its
appearance
in
a
type
specifier
and
persists
to
the
end
of
a
translation
unit
for
declarations
at
the
external
level
or
to
the
end
of
the
block
for
declarations
within
a
function
If
an
identifier
is
explicitly
declared
at
the
head
of
a
block
including
the
block
constituting
a
function
any
declaration
of
the
identifier
outside
the
block
is
suspended
until
the
end
of
the
block
A
Linkage
Within
a
translation
unit
all
declarations
of
the
same
object
or
function
identifier
with
internal
linkage
refer
to
the
same
thing
and
the
object
or
function
is
unique
to
that
translation
unit
All
declarations
for
the
same
object
or
function
identifier
with
external
linkage
refer
to
the
same
thing
and
the
object
or
function
is
shared
by
the
entire
program
As
discussed
in
Par
A
the
first
external
declaration
for
an
identifier
gives
the
identifier
internal
linkage
if
the
static
specifier
is
used
external
linkage
otherwise
If
a
declaration
for
an
identifier
within
a
block
does
not
include
the
extern
specifier
then
the
identifier
has
no
linkage
and
is
unique
to
the
function
If
it
does
include
extern
and
an
external
declaration
for
is
active
in
the
scope
surrounding
the
block
then
the
identifier
has
the
same
linkage
as
the
external
declaration
and
refers
to
the
same
object
or
function
but
if
no
external
declaration
is
visible
its
linkage
is
external
A
Preprocessing
A
preprocessor
performs
macro
substitution
conditional
compilation
and
inclusion
of
named
files
Lines
beginning
with
perhaps
preceded
by
white
space
communicate
with
this
preprocessor
The
syntax
of
these
lines
is
independent
of
the
rest
of
the
language
they
may
appear
anywhere
and
have
effect
that
lasts
independent
of
scope
until
the
end
of
the
translation
unit
Line
boundaries
are
significant
each
line
is
analyzed
individually
bus
see
Par
A
for
how
to
adjoin
lines
To
the
preprocessor
a
token
is
any
language
token
or
a
character
sequence
giving
a
file
name
as
in
the
include
directive
Par
A
in
addition
any
character
not
otherwise
defined
is
taken
as
a
token
However
the
effect
of
white
spaces
other
than
space
and
horizontal
tab
is
undefined
within
preprocessor
lines
Preprocessing
itself
takes
place
in
several
logically
successive
phases
that
may
in
a
particular
implementation
be
condensed
First
trigraph
sequences
as
described
in
Par
A
are
replaced
by
their
equivalents
Should
the
operating
system
environment
require
it
newline
characters
are
introduced
between
the
lines
of
the
source
file
Each
occurrence
of
a
backslash
character
followed
by
a
newline
is
deleted
this
splicing
lines
Par
A
The
program
is
split
into
tokens
separated
by
white
space
characters
comments
are
replaced
by
a
single
space
Then
preprocessing
directives
are
obeyed
and
macros
Pars
A
A
are
expanded
Escape
sequences
in
character
constants
and
string
literals
Pars
A
A
are
replaced
by
their
equivalents
then
adjacent
string
literals
are
concatenated
The
result
is
translated
then
linked
together
with
other
programs
and
libraries
by
collecting
the
necessary
programs
and
data
and
connecting
external
functions
and
object
references
to
their
definitions
A
Trigraph
Sequences
The
character
set
of
C
source
programs
is
contained
within
seven
bit
ASCII
but
is
a
superset
of
the
ISO
Invariant
Code
Set
In
order
to
enable
programs
to
be
represented
in
the
reduced
set
all
occurrences
of
the
following
trigraph
sequences
are
replaced
by
the
corresponding
single
character
This
replacement
occurs
before
any
other
processing
No
other
such
replacements
occur
Trigraph
sequences
are
new
with
the
ANSI
standard
A
Line
Splicing
Lines
that
end
with
the
backslash
character
are
folded
by
deleting
the
backslash
and
the
following
newline
character
This
occurs
before
division
into
tokens
A
Macro
Definition
and
Expansion
A
control
line
of
the
form
define
identifier
token
sequence
causes
the
preprocessor
to
replace
subsequent
instances
of
the
identifier
with
the
given
sequence
of
tokens
leading
and
trailing
white
space
around
the
token
sequence
is
discarded
A
second
define
for
the
same
identifier
is
erroneous
unless
the
second
token
sequence
is
identical
to
the
first
where
all
white
space
separations
are
taken
to
be
equivalent
A
line
of
the
form
define
identifier
identifier
list
token
sequence
where
there
is
no
space
between
the
first
identifier
and
the
is
a
macro
definition
with
parameters
given
by
the
identifier
list
As
with
the
first
form
leading
and
trailing
white
space
arround
the
token
sequence
is
discarded
and
the
macro
may
be
redefined
only
with
a
definition
in
which
the
number
and
spelling
of
parameters
and
the
token
sequence
is
identical
A
control
line
of
the
form
undef
identifier
causes
the
identifier
s
preprocessor
definition
to
be
forgotten
It
is
not
erroneous
to
apply
undef
to
an
unknown
identifier
When
a
macro
has
been
defined
in
the
second
form
subsequent
textual
instances
of
the
macro
identifier
followed
by
optional
white
space
and
then
by
a
sequence
of
tokens
separated
by
commas
and
a
constitute
a
call
of
the
macro
The
arguments
of
the
call
are
the
commaseparated
token
sequences
commas
that
are
quoted
or
protected
by
nested
parentheses
do
not
separate
arguments
During
collection
arguments
are
not
macro
expanded
The
number
of
arguments
in
the
call
must
match
the
number
of
parameters
in
the
definition
After
the
arguments
are
isolated
leading
and
trailing
white
space
is
removed
from
them
Then
the
token
sequence
resulting
from
each
argument
is
substituted
for
each
unquoted
occurrence
of
the
corresponding
parameter
s
identifier
in
the
replacement
token
sequence
of
the
macro
Unless
the
parameter
in
the
replacement
sequence
is
preceded
by
or
preceded
or
followed
by
the
argument
tokens
are
examined
for
macro
calls
and
expanded
as
necessary
just
before
insertion
Two
special
operators
influence
the
replacement
process
First
if
an
occurrence
of
a
parameter
in
the
replacement
token
sequence
is
immediately
preceded
by
string
quotes
are
placed
around
the
corresponding
parameter
and
then
both
the
and
the
parameter
identifier
are
replaced
by
the
quoted
argument
A
character
is
inserted
before
each
or
character
that
appears
surrounding
or
inside
a
string
literal
or
character
constant
in
the
argument
Second
if
the
definition
token
sequence
for
either
kind
of
macro
contains
a
operator
then
just
after
replacement
of
the
parameters
each
is
deleted
together
with
any
white
space
on
either
side
so
as
to
concatenate
the
adjacent
tokens
and
form
a
new
token
The
effect
is
undefined
if
invalid
tokens
are
produced
or
if
the
result
depends
on
the
order
of
processing
of
the
operators
Also
may
not
appear
at
the
beginning
or
end
of
a
replacement
token
sequence
In
both
kinds
of
macro
the
replacement
token
sequence
is
repeatedly
rescanned
for
more
defined
identifiers
However
once
a
given
identifier
has
been
replaced
in
a
given
expansion
it
is
not
replaced
if
it
turns
up
again
during
rescanning
instead
it
is
left
unchanged
Even
if
the
final
value
of
a
macro
expansion
begins
with
with
it
is
not
taken
to
be
a
preprocessing
directive
The
details
of
the
macro
expansion
process
are
described
more
precisely
in
the
ANSI
standard
than
in
the
first
edition
The
most
important
change
is
the
addition
of
the
and
operators
which
make
quotation
and
concatenation
admissible
Some
of
the
new
rules
especially
those
involving
concatenation
are
bizarre
See
example
below
For
example
this
facility
may
be
used
for
manifest
constants
as
in
define
TABSIZE
int
table
TABSIZE
The
definition
define
ABSDIFF
a
b
a
b
a
b
b
a
defines
a
macro
to
return
the
absolute
value
of
the
difference
between
its
arguments
Unlike
a
function
to
do
the
same
thing
the
arguments
and
returned
value
may
have
any
arithmetic
type
or
even
be
pointers
Also
the
arguments
which
might
have
side
effects
are
evaluated
twice
once
for
the
test
and
once
to
produce
the
value
Given
the
definition
define
tempfile
dir
dir
s
the
macro
call
tempfile
usr
tmp
yields
usr
tmp
s
which
will
subsequently
be
catenated
into
a
single
string
After
define
cat
x
y
x
y
the
call
cat
var
yields
var
However
the
call
cat
cat
is
undefined
the
presence
of
prevents
the
arguments
of
the
outer
call
from
being
expanded
Thus
it
produces
the
token
string
cat
and
the
catenation
of
the
last
token
of
the
first
argument
with
the
first
token
of
the
second
is
not
a
legal
token
If
a
second
level
of
macro
definition
is
introduced
define
xcat
x
y
cat
x
y
things
work
more
smoothly
xcat
xcat
does
produce
because
the
expansion
of
xcat
itself
does
not
involve
the
operator
Likewise
ABSDIFF
ABSDIFF
a
b
c
produces
the
expected
fully
expanded
result
A
File
Inclusion
A
control
line
of
the
form
include
filename
causes
the
replacement
of
that
line
by
the
entire
contents
of
the
file
filename
The
characters
in
the
name
filename
must
not
include
or
newline
and
the
effect
is
undefined
if
it
contains
any
of
or
The
named
file
is
searched
for
in
a
sequence
of
implementation
defined
places
Similarly
a
control
line
of
the
form
include
filename
searches
first
in
association
with
the
original
source
file
a
deliberately
implementationdependent
phrase
and
if
that
search
fails
then
as
in
the
first
form
The
effect
of
using
or
in
the
filename
remains
undefined
but
is
permitted
Finally
a
directive
of
the
form
include
token
sequence
not
matching
one
of
the
previous
forms
is
interpreted
by
expanding
the
token
sequence
as
for
normal
text
one
of
the
two
forms
with
or
must
result
and
is
then
treated
as
previously
described
include
files
may
be
nested
A
Conditional
Compilation
Parts
of
a
program
may
be
compiled
conditionally
according
to
the
following
schematic
syntax
preprocessor
conditional
if
line
text
elif
parts
else
partopt
endif
if
line
if
constant
expression
ifdef
identifier
ifndef
identifier
elif
parts
elif
line
text
elif
partsopt
elif
line
elif
constant
expression
else
part
else
line
text
else
line
else
Each
of
the
directives
if
line
elif
line
else
line
and
endif
appears
alone
on
a
line
The
constant
expressions
in
if
and
subsequent
elif
lines
are
evaluated
in
order
until
an
expression
with
a
non
zero
value
is
found
text
following
a
line
with
a
zero
value
is
discarded
The
text
following
the
successful
directive
line
is
treated
normally
Text
here
refers
to
any
material
including
preprocessor
lines
that
is
not
part
of
the
conditional
structure
it
may
be
empty
Once
a
successful
if
or
elif
line
has
been
found
and
its
text
processed
succeeding
elif
and
else
lines
together
with
their
text
are
discarded
If
all
the
expressions
are
zero
and
there
is
an
else
the
text
following
the
else
is
treated
normally
Text
controlled
by
inactive
arms
of
the
conditional
is
ignored
except
for
checking
the
nesting
of
conditionals
The
constant
expression
in
if
and
elif
is
subject
to
ordinary
macro
replacement
Moreover
any
expressions
of
the
form
defined
identifier
or
defined
identifier
are
replaced
before
scanning
for
macros
by
L
if
the
identifier
is
defined
in
the
preprocessor
and
by
L
if
not
Any
identifiers
remaining
after
macro
expansion
are
replaced
by
L
Finally
each
integer
constant
is
considered
to
be
suffixed
with
L
so
that
all
arithmetic
is
taken
to
be
long
or
unsigned
long
The
resulting
constant
expression
Par
A
is
restricted
it
must
be
integral
and
may
not
contain
sizeof
a
cast
or
an
enumeration
constant
The
control
lines
ifdef
identifier
ifndef
identifier
are
equivalent
to
if
defined
identifier
if
defined
identifier
respectively
elif
is
new
since
the
first
edition
although
it
has
been
available
is
some
preprocessors
The
defined
preprocessor
operator
is
also
new
A
Line
Control
For
the
benefit
of
other
preprocessors
that
generate
C
programs
a
line
in
one
of
the
forms
line
constant
filename
line
constant
causes
the
compiler
to
believe
for
purposes
of
error
diagnostics
that
the
line
number
of
the
next
source
line
is
given
by
the
decimal
integer
constant
and
the
current
input
file
is
named
by
the
identifier
If
the
quoted
filename
is
absent
the
remembered
name
does
not
change
Macros
in
the
line
are
expanded
before
it
is
interpreted
A
Error
Generation
A
preprocessor
line
of
the
form
error
token
sequenceopt
causes
the
preprocessor
to
write
a
diagnostic
message
that
includes
the
token
sequence
A
Pragmas
A
control
line
of
the
form
pragma
token
sequenceopt
causes
the
preprocessor
to
perform
an
implementation
dependent
action
An
unrecognized
pragma
is
ignored
A
Null
directive
A
control
line
of
the
form
has
no
effect
A
Predefined
names
Several
identifiers
are
predefined
and
expand
to
produce
special
information
They
and
also
the
preprocessor
expansion
operator
defined
may
not
be
undefined
or
redefined
LINE
A
decimal
constant
containing
the
current
source
line
number
FILE
A
string
literal
containing
the
name
of
the
file
being
compiled
DATE
A
string
literal
containing
the
date
of
compilation
in
the
form
Mmmm
dd
yyyy
TIME
A
string
literal
containing
the
time
of
compilation
in
the
form
hh
mm
ss
STDC
The
constant
It
is
intended
that
this
identifier
be
defined
to
be
only
in
standardconforming
implementations
error
and
pragma
are
new
with
the
ANSI
standard
the
predefined
preprocessor
macros
are
new
but
some
of
them
have
been
available
in
some
implementations
A
Grammar
Below
is
a
recapitulation
of
the
grammar
that
was
given
throughout
the
earlier
part
of
this
appendix
It
has
exactly
the
same
content
but
is
in
different
order
The
grammar
has
undefined
terminal
symbols
integer
constant
character
constant
floatingconstant
identifier
string
and
enumeration
constant
the
typewriter
style
words
and
symbols
are
terminals
given
literally
This
grammar
can
be
transformed
mechanically
into
input
acceptable
for
an
automatic
parser
generator
Besides
adding
whatever
syntactic
marking
is
used
to
indicate
alternatives
in
productions
it
is
necessary
to
expand
the
one
of
constructions
and
depending
on
the
rules
of
the
parser
generator
to
duplicate
each
production
with
an
opt
symbol
once
with
the
symbol
and
once
without
With
one
further
change
namely
deleting
the
production
typedef
name
identifier
and
making
typedef
name
a
terminal
symbol
this
grammar
is
acceptable
to
the
YACC
parser
generator
It
has
only
one
conflict
generated
by
the
if
else
ambiguity
translation
unit
external
declaration
translation
unit
external
declaration
external
declaration
function
definition
declaration
function
definition
declaration
specifiersopt
declarator
declaration
listopt
compound
statement
declaration
declaration
specifiers
init
declarator
listopt
declaration
list
declaration
declaration
list
declaration
declaration
specifiers
storage
class
specifier
declaration
specifiersopt
type
specifier
declaration
specifiersopt
type
qualifier
declaration
specifiersopt
storage
class
specifier
one
of
auto
register
static
extern
typedef
type
specifier
one
of
void
char
short
int
long
float
double
signed
unsigned
struct
or
union
specifier
enum
specifier
typedef
name
type
qualifier
one
of
const
volatile
struct
or
union
specifier
struct
or
union
identifieropt
struct
declaration
list
struct
or
union
identifier
struct
or
union
one
of
struct
union
struct
declaration
list
struct
declaration
struct
declaration
list
struct
declaration
init
declarator
list
init
declarator
init
declarator
list
init
declarator
init
declarator
declarator
declarator
initializer
struct
declaration
specifier
qualifier
list
struct
declarator
list
specifier
qualifier
list
type
specifier
specifier
qualifier
listopt
type
qualifier
specifier
qualifier
listopt
struct
declarator
list
struct
declarator
struct
declarator
list
struct
declarator
struct
declarator
declarator
declaratoropt
constant
expression
enum
specifier
enum
identifieropt
enumerator
list
enum
identifier
enumerator
list
enumerator
enumerator
list
enumerator
enumerator
identifier
identifier
constant
expression
declarator
pointeropt
direct
declarator
direct
declarator
identifier
declarator
direct
declarator
constant
expressionopt
direct
declarator
parameter
type
list
direct
declarator
identifier
listopt
pointer
type
qualifier
listopt
type
qualifier
listopt
pointer
type
qualifier
list
type
qualifier
type
qualifier
list
type
qualifier
parameter
type
list
parameter
list
parameter
list
parameter
list
parameter
declaration
parameter
list
parameter
declaration
parameter
declaration
declaration
specifiers
declarator
declaration
specifiers
abstract
declaratoropt
identifier
list
identifier
identifier
list
identifier
initializer
assignment
expression
initializer
list
initializer
list
initializer
list
initializer
initializer
list
initializer
type
name
specifier
qualifier
list
abstract
declaratoropt
abstract
declarator
pointer
pointeropt
direct
abstract
declarator
direct
abstract
declarator
abstract
declarator
direct
abstract
declaratoropt
constant
expressionopt
direct
abstract
declaratoropt
parameter
type
listopt
typedef
name
identifier
statement
labeled
statement
expression
statement
compound
statement
selection
statement
iteration
statement
jump
statement
labeled
statement
identifier
statement
case
constant
expression
statement
default
statement
expression
statement
expressionopt
compound
statement
declaration
listopt
statement
listopt
statement
list
statement
statement
list
statement
selection
statement
if
expression
statement
if
expression
statement
else
statement
switch
expression
statement
iteration
statement
while
expression
statement
do
statement
while
expression
for
expressionopt
expressionopt
expressionopt
statement
jump
statement
goto
identifier
continue
break
return
expressionopt
expression
assignment
expression
expression
assignment
expression
assignment
expression
conditional
expression
unary
expression
assignment
operator
assignment
expression
assignment
operator
one
of
conditional
expression
logical
OR
expression
logical
OR
expression
expression
conditional
expression
constant
expression
conditional
expression
logical
OR
expression
logical
AND
expression
logical
OR
expression
logical
AND
expression
logical
AND
expression
inclusive
OR
expression
logical
AND
expression
inclusive
OR
expression
inclusive
OR
expression
exclusive
OR
expression
inclusive
OR
expression
exclusive
OR
expression
exclusive
OR
expression
AND
expression
exclusive
OR
expression
AND
expression
AND
expression
equality
expression
AND
expression
equality
expression
equality
expression
relational
expression
equality
expression
relational
expression
equality
expression
relational
expression
relational
expression
shift
expression
relational
expression
shift
expression
relational
expression
shift
expression
relational
expression
shift
expression
relational
expression
shift
expression
shift
expression
additive
expression
shift
expression
additive
expression
shift
expression
additive
expression
additive
expression
multiplicative
expression
additive
expression
multiplicative
expression
additive
expression
multiplicative
expression
multiplicative
expression
multiplicative
expression
cast
expression
multiplicative
expression
cast
expression
multiplicative
expression
cast
expression
cast
expression
unary
expression
type
name
cast
expression
unary
expression
postfix
expression
unary
expression
unary
expression
unary
operator
cast
expression
sizeof
unary
expression
sizeof
type
name
unary
operator
one
of
postfix
expression
primary
expression
postfix
expression
expression
postfix
expression
argument
expression
listopt
postfix
expression
identifier
postfix
expression
identifier
postfix
expression
postfix
expression
primary
expression
identifier
constant
string
expression
argument
expression
list
assignment
expression
assignment
expression
list
assignment
expression
constant
integer
constant
character
constant
floating
constant
enumeration
constant
The
following
grammar
for
the
preprocessor
summarizes
the
structure
of
control
lines
but
is
not
suitable
for
mechanized
parsing
It
includes
the
symbol
text
which
means
ordinary
program
text
non
conditional
preprocessor
control
lines
or
complete
preprocessor
conditional
instructions
control
line
define
identifier
token
sequence
define
identifier
identifier
identifier
token
sequence
undef
identifier
include
filename
include
filename
line
constant
filename
line
constant
error
token
sequenceopt
pragma
token
sequenceopt
preprocessor
conditional
preprocessor
conditional
if
line
text
elif
parts
else
partopt
endif
if
line
if
constant
expression
ifdef
identifier
ifndef
identifier
elif
parts
elif
line
text
elif
partsopt
elif
line
elif
constant
expression
else
part
else
line
text
else
line
else
Appendix
B
Standard
Library
This
appendix
is
a
summary
of
the
library
defined
by
the
ANSI
standard
The
standard
library
is
not
part
of
the
C
language
proper
but
an
environment
that
supports
standard
C
will
provide
the
function
declarations
and
type
and
macro
definitions
of
this
library
We
have
omitted
a
few
functions
that
are
of
limited
utility
or
easily
synthesized
from
others
we
have
omitted
multibyte
characters
and
we
have
omitted
discussion
of
locale
issues
that
is
properties
that
depend
on
local
language
nationality
or
culture
The
functions
types
and
macros
of
the
standard
library
are
declared
in
standard
headers
assert
h
float
h
math
h
stdarg
h
stdlib
h
ctype
h
limits
h
setjmp
h
stddef
h
string
h
errno
h
locale
h
signal
h
stdio
h
time
h
A
header
can
be
accessed
by
include
header
Headers
may
be
included
in
any
order
and
any
number
of
times
A
header
must
be
included
outside
of
any
external
declaration
or
definition
and
before
any
use
of
anything
it
declares
A
header
need
not
be
a
source
file
External
identifiers
that
begin
with
an
underscore
are
reserved
for
use
by
the
library
as
are
all
other
identifiers
that
begin
with
an
underscore
and
an
upper
case
letter
or
another
underscore
B
Input
and
Output
stdio
h
The
input
and
output
functions
types
and
macros
defined
in
stdio
h
represent
nearly
one
third
of
the
library
A
stream
is
a
source
or
destination
of
data
that
may
be
associated
with
a
disk
or
other
peripheral
The
library
supports
text
streams
and
binary
streams
although
on
some
systems
notably
UNIX
these
are
identical
A
text
stream
is
a
sequence
of
lines
each
line
has
zero
or
more
characters
and
is
terminated
by
n
An
environment
may
need
to
convert
a
text
stream
to
or
from
some
other
representation
such
as
mapping
n
to
carriage
return
and
linefeed
A
binary
stream
is
a
sequence
of
unprocessed
bytes
that
record
internal
data
with
the
property
that
if
it
is
written
then
read
back
on
the
same
system
it
will
compare
equal
A
stream
is
connected
to
a
file
or
device
by
opening
it
the
connection
is
broken
by
closing
the
stream
Opening
a
file
returns
a
pointer
to
an
object
of
type
FILE
which
records
whatever
information
is
necessary
to
control
the
stream
We
will
use
file
pointer
and
stream
interchangeably
when
there
is
no
ambiguity
When
a
program
begins
execution
the
three
streams
stdin
stdout
and
stderr
are
already
open
B
File
Operations
The
following
functions
deal
with
operations
on
files
The
type
size
t
is
the
unsigned
integral
type
produced
by
the
sizeof
operator
FILE
fopen
const
char
filename
const
char
mode
fopen
opens
the
named
file
and
returns
a
stream
or
NULL
if
the
attempt
fails
Legal
values
for
mode
include
r
open
text
file
for
reading
w
create
text
file
for
writing
discard
previous
contents
if
any
a
append
open
or
create
text
file
for
writing
at
end
of
file
r
open
text
file
for
update
i
e
reading
and
writing
w
create
text
file
for
update
discard
previous
contents
if
any
a
append
open
or
create
text
file
for
update
writing
at
end
Update
mode
permits
reading
and
writing
the
same
file
fflush
or
a
file
positioning
function
must
be
called
between
a
read
and
a
write
or
vice
versa
If
the
mode
includes
b
after
the
initial
letter
as
in
rb
or
w
b
that
indicates
a
binary
file
Filenames
are
limited
to
FILENAME
MAX
characters
At
most
FOPEN
MAX
files
may
be
open
at
once
FILE
freopen
const
char
filename
const
char
mode
FILE
stream
freopen
opens
the
file
with
the
specified
mode
and
associates
the
stream
with
it
It
returns
stream
or
NULL
if
an
error
occurs
freopen
is
normally
used
to
change
the
files
associated
with
stdin
stdout
or
stderr
int
fflush
FILE
stream
On
an
output
stream
fflush
causes
any
buffered
but
unwritten
data
to
be
written
on
an
input
stream
the
effect
is
undefined
It
returns
EOF
for
a
write
error
and
zero
otherwise
fflush
NULL
flushes
all
output
streams
int
fclose
FILE
stream
fclose
flushes
any
unwritten
data
for
stream
discards
any
unread
buffered
input
frees
any
automatically
allocated
buffer
then
closes
the
stream
It
returns
EOF
if
any
errors
occurred
and
zero
otherwise
int
remove
const
char
filename
remove
removes
the
named
file
so
that
a
subsequent
attempt
to
open
it
will
fail
It
returns
non
zero
if
the
attempt
fails
int
rename
const
char
oldname
const
char
newname
rename
changes
the
name
of
a
file
it
returns
non
zero
if
the
attempt
fails
FILE
tmpfile
void
tmpfile
creates
a
temporary
file
of
mode
wb
that
will
be
automatically
removed
when
closed
or
when
the
program
terminates
normally
tmpfile
returns
a
stream
or
NULL
if
it
could
not
create
the
file
char
tmpnam
char
s
L
tmpnam
tmpnam
NULL
creates
a
string
that
is
not
the
name
of
an
existing
file
and
returns
a
pointer
to
an
internal
static
array
tmpnam
s
stores
the
string
in
s
as
well
as
returning
it
as
the
function
value
s
must
have
room
for
at
least
L
tmpnam
characters
tmpnam
generates
a
different
name
each
time
it
is
called
at
most
TMP
MAX
different
names
are
guaranteed
during
execution
of
the
program
Note
that
tmpnam
creates
a
name
not
a
file
int
setvbuf
FILE
stream
char
buf
int
mode
size
t
size
setvbuf
controls
buffering
for
the
stream
it
must
be
called
before
reading
writing
or
any
other
operation
A
mode
of
IOFBF
causes
full
buffering
IOLBF
line
buffering
of
text
files
and
IONBF
no
buffering
If
buf
is
not
NULL
it
will
be
used
as
the
buffer
otherwise
a
buffer
will
be
allocated
size
determines
the
buffer
size
setvbuf
returns
non
zero
for
any
error
void
setbuf
FILE
stream
char
buf
If
buf
is
NULL
buffering
is
turned
off
for
the
stream
Otherwise
setbuf
is
equivalent
to
void
setvbuf
stream
buf
IOFBF
BUFSIZ
B
Formatted
Output
The
printf
functions
provide
formatted
output
conversion
int
fprintf
FILE
stream
const
char
format
fprintf
converts
and
writes
output
to
stream
under
the
control
of
format
The
return
value
is
the
number
of
characters
written
or
negative
if
an
error
occurred
The
format
string
contains
two
types
of
objects
ordinary
characters
which
are
copied
to
the
output
stream
and
conversion
specifications
each
of
which
causes
conversion
and
printing
of
the
next
successive
argument
to
fprintf
Each
conversion
specification
begins
with
the
character
and
ends
with
a
conversion
character
Between
the
and
the
conversion
character
there
may
be
in
order
Flags
in
any
order
which
modify
the
specification
o
which
specifies
left
adjustment
of
the
converted
argument
in
its
field
o
which
specifies
that
the
number
will
always
be
printed
with
a
sign
o
space
if
the
first
character
is
not
a
sign
a
space
will
be
prefixed
o
for
numeric
conversions
specifies
padding
to
the
field
width
with
leading
zeros
o
which
specifies
an
alternate
output
form
For
o
the
first
digit
will
become
zero
For
x
or
X
x
or
X
will
be
prefixed
to
a
non
zero
result
For
e
E
f
g
and
G
the
output
will
always
have
a
decimal
point
for
g
and
G
trailing
zeros
will
not
be
removed
A
number
specifying
a
minimum
field
width
The
converted
argument
will
be
printed
in
a
field
at
least
this
wide
and
wider
if
necessary
If
the
converted
argument
has
fewer
characters
than
the
field
width
it
will
be
padded
on
the
left
or
right
if
left
adjustment
has
been
requested
to
make
up
the
field
width
The
padding
character
is
normally
space
but
is
if
the
zero
padding
flag
is
present
A
period
which
separates
the
field
width
from
the
precision
A
number
the
precision
that
specifies
the
maximum
number
of
characters
to
be
printed
from
a
string
or
the
number
of
digits
to
be
printed
after
the
decimal
point
for
e
E
or
f
conversions
or
the
number
of
significant
digits
for
g
or
G
conversion
or
the
number
of
digits
to
be
printed
for
an
integer
leading
s
will
be
added
to
make
up
the
necessary
width
A
length
modifier
h
l
letter
ell
or
L
h
indicates
that
the
corresponding
argument
is
to
be
printed
as
a
short
or
unsigned
short
l
indicates
that
the
argument
is
a
long
or
unsigned
long
L
indicates
that
the
argument
is
a
long
double
Width
or
precision
or
both
may
be
specified
as
in
which
case
the
value
is
computed
by
converting
the
next
argument
s
which
must
be
int
The
conversion
characters
and
their
meanings
are
shown
in
Table
B
If
the
character
after
the
is
not
a
conversion
character
the
behavior
is
undefined
Table
B
Printf
Conversions
Character
Argument
type
Printed
As
d
i
int
signed
decimal
notation
o
int
unsigned
octal
notation
without
a
leading
zero
x
X
unsigned
int
unsigned
hexadecimal
notation
without
a
leading
x
or
X
using
abcdef
for
x
or
ABCDEF
for
X
u
int
unsigned
decimal
notation
c
int
single
character
after
conversion
to
unsigned
char
s
char
characters
from
the
string
are
printed
until
a
is
reached
or
until
the
number
of
characters
indicated
by
the
precision
have
been
printed
f
double
decimal
notation
of
the
form
mmm
ddd
where
the
number
of
d
s
is
given
by
the
precision
The
default
precision
is
a
precision
of
suppresses
the
decimal
point
e
E
double
decimal
notation
of
the
form
m
dddddde
xx
or
m
ddddddE
xx
where
the
number
of
d
s
is
specified
by
the
precision
The
default
precision
is
a
precision
of
suppresses
the
decimal
point
g
G
double
e
or
E
is
used
if
the
exponent
is
less
than
or
greater
than
or
equal
to
the
precision
otherwise
f
is
used
Trailing
zeros
and
a
trailing
decimal
point
are
not
printed
p
void
print
as
a
pointer
implementation
dependent
representation
n
int
the
number
of
characters
written
so
far
by
this
call
to
printf
is
written
into
the
argument
No
argument
is
converted
no
argument
is
converted
print
a
int
printf
const
char
format
printf
is
equivalent
to
fprintf
stdout
int
sprintf
char
s
const
char
format
sprintf
is
the
same
as
printf
except
that
the
output
is
written
into
the
string
s
terminated
with
s
must
be
big
enough
to
hold
the
result
The
return
count
does
not
include
the
int
vprintf
const
char
format
va
list
arg
int
vfprintf
FILE
stream
const
char
format
va
list
arg
int
vsprintf
char
s
const
char
format
va
list
arg
The
functions
vprintf
vfprintf
and
vsprintf
are
equivalent
to
the
corresponding
printf
functions
except
that
the
variable
argument
list
is
replaced
by
arg
which
has
been
initialized
by
the
va
start
macro
and
perhaps
va
arg
calls
See
the
discussion
of
stdarg
h
in
Section
B
B
Formatted
Input
The
scanf
function
deals
with
formatted
input
conversion
int
fscanf
FILE
stream
const
char
format
fscanf
reads
from
stream
under
control
of
format
and
assigns
converted
values
through
subsequent
arguments
each
of
which
must
be
a
pointer
It
returns
when
format
is
exhausted
fscanf
returns
EOF
if
end
of
file
or
an
error
occurs
before
any
conversion
otherwise
it
returns
the
number
of
input
items
converted
and
assigned
The
format
string
usually
contains
conversion
specifications
which
are
used
to
direct
interpretation
of
input
The
format
string
may
contain
Blanks
or
tabs
which
are
not
ignored
Ordinary
characters
not
which
are
expected
to
match
the
next
non
white
space
character
of
the
input
stream
Conversion
specifications
consisting
of
a
an
optional
assignment
suppression
character
an
optional
number
specifying
a
maximum
field
width
an
optional
h
l
or
L
indicating
the
width
of
the
target
and
a
conversion
character
A
conversion
specification
determines
the
conversion
of
the
next
input
field
Normally
the
result
is
placed
in
the
variable
pointed
to
by
the
corresponding
argument
If
assignment
suppression
is
indicated
by
as
in
s
however
the
input
field
is
simply
skipped
no
assignment
is
made
An
input
field
is
defined
as
a
string
of
non
white
space
characters
it
extends
either
to
the
next
white
space
character
or
until
the
field
width
if
specified
is
exhausted
This
implies
that
scanf
will
read
across
line
boundaries
to
find
its
input
since
newlines
are
white
space
White
space
characters
are
blank
tab
newline
carriage
return
vertical
tab
and
formfeed
The
conversion
character
indicates
the
interpretation
of
the
input
field
The
corresponding
argument
must
be
a
pointer
The
legal
conversion
characters
are
shown
in
Table
B
The
conversion
characters
d
i
n
o
u
and
x
may
be
preceded
by
h
if
the
argument
is
a
pointer
to
short
rather
than
int
or
by
l
letter
ell
if
the
argument
is
a
pointer
to
long
The
conversion
characters
e
f
and
g
may
be
preceded
by
l
if
a
pointer
to
double
rather
than
float
is
in
the
argument
list
and
by
L
if
a
pointer
to
a
long
double
Table
B
Scanf
Conversions
Character
Input
Data
Argument
type
d
decimal
integer
int
i
integer
int
The
integer
may
be
in
octal
leading
or
hexadecimal
leading
x
or
X
o
octal
integer
with
or
without
leading
zero
int
u
unsigned
decimal
integer
unsigned
int
x
hexadecimal
integer
with
or
without
leading
x
or
X
int
c
characters
char
The
next
input
characters
are
placed
in
the
indicated
array
up
to
the
number
given
by
the
width
field
the
default
is
No
is
added
The
normal
skip
over
white
space
characters
is
suppressed
in
this
case
to
read
the
next
non
white
space
character
use
s
s
string
of
non
white
space
characters
not
quoted
char
pointing
to
an
array
of
characters
large
enough
to
hold
the
string
and
a
terminating
that
will
be
added
e
f
g
floating
point
number
float
The
input
format
for
float
s
is
an
optional
sign
a
string
of
numbers
possibly
containing
a
decimal
point
and
an
optional
exponent
field
containing
an
E
or
e
followed
by
a
possibly
signed
integer
p
pointer
value
as
printed
by
printf
p
void
n
writes
into
the
argument
the
number
of
characters
read
so
far
by
this
call
int
No
input
is
read
The
converted
item
count
is
not
incremented
matches
the
longest
non
empty
string
of
input
characters
from
the
set
between
brackets
char
A
is
added
includes
in
the
set
matches
the
longest
non
empty
string
of
input
characters
not
from
the
set
between
brackets
char
A
is
added
includes
in
the
set
literal
no
assignment
is
made
int
scanf
const
char
format
scanf
is
identical
to
fscanf
stdin
int
sscanf
const
char
s
const
char
format
sscanf
s
is
equivalent
to
scanf
except
that
the
input
characters
are
taken
from
the
string
s
B
Character
Input
and
Output
Functions
int
fgetc
FILE
stream
fgetc
returns
the
next
character
of
stream
as
an
unsigned
char
converted
to
an
int
or
EOF
if
end
of
file
or
error
occurs
char
fgets
char
s
int
n
FILE
stream
fgets
reads
at
most
the
next
n
characters
into
the
array
s
stopping
if
a
newline
is
encountered
the
newline
is
included
in
the
array
which
is
terminated
by
fgets
returns
s
or
NULL
if
end
of
file
or
error
occurs
int
fputc
int
c
FILE
stream
fputc
writes
the
character
c
converted
to
an
unsigend
char
on
stream
It
returns
the
character
written
or
EOF
for
error
int
fputs
const
char
s
FILE
stream
fputs
writes
the
string
s
which
need
not
contain
n
on
stream
it
returns
nonnegative
or
EOF
for
an
error
int
getc
FILE
stream
getc
is
equivalent
to
fgetc
except
that
if
it
is
a
macro
it
may
evaluate
stream
more
than
once
int
getchar
void
getchar
is
equivalent
to
getc
stdin
char
gets
char
s
gets
reads
the
next
input
line
into
the
array
s
it
replaces
the
terminating
newline
with
It
returns
s
or
NULL
if
end
of
file
or
error
occurs
int
putc
int
c
FILE
stream
putc
is
equivalent
to
fputc
except
that
if
it
is
a
macro
it
may
evaluate
stream
more
than
once
int
putchar
int
c
putchar
c
is
equivalent
to
putc
c
stdout
int
puts
const
char
s
puts
writes
the
string
s
and
a
newline
to
stdout
It
returns
EOF
if
an
error
occurs
non
negative
otherwise
int
ungetc
int
c
FILE
stream
ungetc
pushes
c
converted
to
an
unsigned
char
back
onto
stream
where
it
will
be
returned
on
the
next
read
Only
one
character
of
pushback
per
stream
is
guaranteed
EOF
may
not
be
pushed
back
ungetc
returns
the
character
pushed
back
or
EOF
for
error
B
Direct
Input
and
Output
Functions
size
t
fread
void
ptr
size
t
size
size
t
nobj
FILE
stream
fread
reads
from
stream
into
the
array
ptr
at
most
nobj
objects
of
size
size
fread
returns
the
number
of
objects
read
this
may
be
less
than
the
number
requested
feof
and
ferror
must
be
used
to
determine
status
size
t
fwrite
const
void
ptr
size
t
size
size
t
nobj
FILE
stream
fwrite
writes
from
the
array
ptr
nobj
objects
of
size
size
on
stream
It
returns
the
number
of
objects
written
which
is
less
than
nobj
on
error
B
File
Positioning
Functions
int
fseek
FILE
stream
long
offset
int
origin
fseek
sets
the
file
position
for
stream
a
subsequent
read
or
write
will
access
data
beginning
at
the
new
position
For
a
binary
file
the
position
is
set
to
offset
characters
from
origin
which
may
be
SEEK
SET
beginning
SEEK
CUR
current
position
or
SEEK
END
end
of
file
For
a
text
stream
offset
must
be
zero
or
a
value
returned
by
ftell
in
which
case
origin
must
be
SEEK
SET
fseek
returns
non
zero
on
error
long
ftell
FILE
stream
ftell
returns
the
current
file
position
for
stream
or
on
error
void
rewind
FILE
stream
rewind
fp
is
equivalent
to
fseek
fp
L
SEEK
SET
clearerr
fp
int
fgetpos
FILE
stream
fpos
t
ptr
fgetpos
records
the
current
position
in
stream
in
ptr
for
subsequent
use
by
fsetpos
The
type
fpos
t
is
suitable
for
recording
such
values
fgetpos
returns
nonzero
on
error
int
fsetpos
FILE
stream
const
fpos
t
ptr
fsetpos
positions
stream
at
the
position
recorded
by
fgetpos
in
ptr
fsetpos
returns
non
zero
on
error
B
Error
Functions
Many
of
the
functions
in
the
library
set
status
indicators
when
error
or
end
of
file
occur
These
indicators
may
be
set
and
tested
explicitly
In
addition
the
integer
expression
errno
declared
in
errno
h
may
contain
an
error
number
that
gives
further
information
about
the
most
recent
error
void
clearerr
FILE
stream
clearerr
clears
the
end
of
file
and
error
indicators
for
stream
int
feof
FILE
stream
feof
returns
non
zero
if
the
end
of
file
indicator
for
stream
is
set
int
ferror
FILE
stream
ferror
returns
non
zero
if
the
error
indicator
for
stream
is
set
void
perror
const
char
s
perror
s
prints
s
and
an
implementation
defined
error
message
corresponding
to
the
integer
in
errno
as
if
by
fprintf
stderr
s
s
n
s
error
message
See
strerror
in
Section
B
B
Character
Class
Tests
ctype
h
The
header
ctype
h
declares
functions
for
testing
characters
For
each
function
the
argument
list
is
an
int
whose
value
must
be
EOF
or
representable
as
an
unsigned
char
and
the
return
value
is
an
int
The
functions
return
non
zero
true
if
the
argument
c
satisfies
the
condition
described
and
zero
if
not
isalnum
c
isalpha
c
or
isdigit
c
is
true
isalpha
c
isupper
c
or
islower
c
is
true
iscntrl
c
control
character
isdigit
c
decimal
digit
isgraph
c
printing
character
except
space
islower
c
lower
case
letter
isprint
c
printing
character
including
space
ispunct
c
printing
character
except
space
or
letter
or
digit
isspace
c
space
formfeed
newline
carriage
return
tab
vertical
tab
isupper
c
upper
case
letter
isxdigit
c
hexadecimal
digit
In
the
seven
bit
ASCII
character
set
the
printing
characters
are
x
to
x
E
the
control
characters
are
NUL
to
x
F
US
and
x
F
DEL
In
addition
there
are
two
functions
that
convert
the
case
of
letters
int
tolower
c
convert
c
to
lower
case
int
toupper
c
convert
c
to
upper
case
If
c
is
an
upper
case
letter
tolower
c
returns
the
corresponding
lower
case
letter
toupper
c
returns
the
corresponding
upper
case
letter
otherwise
it
returns
c
B
String
Functions
string
h
There
are
two
groups
of
string
functions
defined
in
the
header
string
h
The
first
have
names
beginning
with
str
the
second
have
names
beginning
with
mem
Except
for
memmove
the
behavior
is
undefined
if
copying
takes
place
between
overlapping
objects
Comparison
functions
treat
arguments
as
unsigned
char
arrays
In
the
following
table
variables
s
and
t
are
of
type
char
cs
and
ct
are
of
type
const
char
n
is
of
type
size
t
and
c
is
an
int
converted
to
char
char
strcpy
s
ct
copy
string
ct
to
string
s
including
return
s
char
strncpy
s
ct
n
copy
at
most
n
characters
of
string
ct
to
s
return
s
Pad
with
s
if
ct
has
fewer
than
n
characters
char
strcat
s
ct
concatenate
string
ct
to
end
of
string
s
return
s
char
strncat
s
ct
n
concatenate
at
most
n
characters
of
string
ct
to
string
s
terminate
s
with
return
s
int
strcmp
cs
ct
compare
string
cs
to
string
ct
return
if
cs
ct
if
cs
ct
or
if
cs
ct
int
strncmp
cs
ct
n
compare
at
most
n
characters
of
string
cs
to
string
ct
return
if
cs
ct
if
cs
ct
or
if
cs
ct
char
strchr
cs
c
return
pointer
to
first
occurrence
of
c
in
cs
or
NULL
if
not
present
char
strrchr
cs
c
return
pointer
to
last
occurrence
of
c
in
cs
or
NULL
if
not
present
size
t
strspn
cs
ct
return
length
of
prefix
of
cs
consisting
of
characters
in
ct
size
t
strcspn
cs
ct
return
length
of
prefix
of
cs
consisting
of
characters
not
in
ct
char
strpbrk
cs
ct
return
pointer
to
first
occurrence
in
string
cs
of
any
character
string
ct
or
NULL
if
not
present
char
strstr
cs
ct
return
pointer
to
first
occurrence
of
string
ct
in
cs
or
NULL
if
not
present
size
t
strlen
cs
return
length
of
cs
char
strerror
n
return
pointer
to
implementation
defined
string
corresponding
to
error
n
char
strtok
s
ct
strtok
searches
s
for
tokens
delimited
by
characters
from
ct
see
below
A
sequence
of
calls
of
strtok
s
ct
splits
s
into
tokens
each
delimited
by
a
character
from
ct
The
first
call
in
a
sequence
has
a
non
NULL
s
it
finds
the
first
token
in
s
consisting
of
characters
not
in
ct
it
terminates
that
by
overwriting
the
next
character
of
s
with
and
returns
a
pointer
to
the
token
Each
subsequent
call
indicated
by
a
NULL
value
of
s
returns
the
next
such
token
searching
from
just
past
the
end
of
the
previous
one
strtok
returns
NULL
when
no
further
token
is
found
The
string
ct
may
be
different
on
each
call
The
mem
functions
are
meant
for
manipulating
objects
as
character
arrays
the
intent
is
an
interface
to
efficient
routines
In
the
following
table
s
and
t
are
of
type
void
cs
and
ct
are
of
type
const
void
n
is
of
type
size
t
and
c
is
an
int
converted
to
an
unsigned
char
void
memcpy
s
ct
n
copy
n
characters
from
ct
to
s
and
return
s
void
memmove
s
ct
n
same
as
memcpy
except
that
it
works
even
if
the
objects
overlap
int
memcmp
cs
ct
n
compare
the
first
n
characters
of
cs
with
ct
return
as
with
strcmp
void
memchr
cs
c
n
return
pointer
to
first
occurrence
of
character
c
in
cs
or
NULL
if
not
present
among
the
first
n
characters
void
memset
s
c
n
place
character
c
into
first
n
characters
of
s
return
s
B
Mathematical
Functions
math
h
The
header
math
h
declares
mathematical
functions
and
macros
The
macros
EDOM
and
ERANGE
found
in
errno
h
are
non
zero
integral
constants
that
are
used
to
signal
domain
and
range
errors
for
the
functions
HUGE
VAL
is
a
positive
double
value
A
domain
error
occurs
if
an
argument
is
outside
the
domain
over
which
the
function
is
defined
On
a
domain
error
errno
is
set
to
EDOM
the
return
value
is
implementation
defined
A
range
error
occurs
if
the
result
of
the
function
cannot
be
represented
as
a
double
If
the
result
overflows
the
function
returns
HUGE
VAL
with
the
right
sign
and
errno
is
set
to
ERANGE
If
the
result
underflows
the
function
returns
zero
whether
errno
is
set
to
ERANGE
is
implementation
defined
In
the
following
table
x
and
y
are
of
type
double
n
is
an
int
and
all
functions
return
double
Angles
for
trigonometric
functions
are
expressed
in
radians
sin
x
sine
of
x
cos
x
cosine
of
x
tan
x
tangent
of
x
asin
x
sin
x
in
range
pi
pi
x
in
acos
x
cos
x
in
range
pi
x
in
atan
x
tan
x
in
range
pi
pi
atan
y
x
tan
y
x
in
range
pi
pi
sinh
x
hyperbolic
sine
of
x
cosh
x
hyperbolic
cosine
of
x
tanh
x
hyperbolic
tangent
of
x
exp
x
exponential
function
e
x
log
x
natural
logarithm
ln
x
x
log
x
base
logarithm
log
x
x
pow
x
y
x
y
A
domain
error
occurs
if
x
and
y
or
if
x
and
y
is
not
an
integer
sqrt
x
sqare
root
of
x
x
ceil
x
smallest
integer
not
less
than
x
as
a
double
floor
x
largest
integer
not
greater
than
x
as
a
double
fabs
x
absolute
value
x
ldexp
x
n
x
n
frexp
x
int
ip
splits
x
into
a
normalized
fraction
in
the
interval
which
is
returned
and
a
power
of
which
is
stored
in
exp
If
x
is
zero
both
parts
of
the
result
are
zero
modf
x
double
ip
splits
x
into
integral
and
fractional
parts
each
with
the
same
sign
as
x
It
stores
the
integral
part
in
ip
and
returns
the
fractional
part
fmod
x
y
floating
point
remainder
of
x
y
with
the
same
sign
as
x
If
y
is
zero
the
result
is
implementation
defined
B
Utility
Functions
stdlib
h
The
header
stdlib
h
declares
functions
for
number
conversion
storage
allocation
and
similar
tasks
double
atof
const
char
s
atof
converts
s
to
double
it
is
equivalent
to
strtod
s
char
NULL
int
atoi
const
char
s
converts
s
to
int
it
is
equivalent
to
int
strtol
s
char
NULL
long
atol
const
char
s
converts
s
to
long
it
is
equivalent
to
strtol
s
char
NULL
double
strtod
const
char
s
char
endp
strtod
converts
the
prefix
of
s
to
double
ignoring
leading
white
space
it
stores
a
pointer
to
any
unconverted
suffix
in
endp
unless
endp
is
NULL
If
the
answer
would
overflow
HUGE
VAL
is
returned
with
the
proper
sign
if
the
answer
would
underflow
zero
is
returned
In
either
case
errno
is
set
to
ERANGE
long
strtol
const
char
s
char
endp
int
base
strtol
converts
the
prefix
of
s
to
long
ignoring
leading
white
space
it
stores
a
pointer
to
any
unconverted
suffix
in
endp
unless
endp
is
NULL
If
base
is
between
and
conversion
is
done
assuming
that
the
input
is
written
in
that
base
If
base
is
zero
the
base
is
or
leading
implies
octal
and
leading
x
or
X
hexadecimal
Letters
in
either
case
represent
digits
from
to
base
a
leading
x
or
X
is
permitted
in
base
If
the
answer
would
overflow
LONG
MAX
or
LONG
MIN
is
returned
depending
on
the
sign
of
the
result
and
errno
is
set
to
ERANGE
unsigned
long
strtoul
const
char
s
char
endp
int
base
strtoul
is
the
same
as
strtol
except
that
the
result
is
unsigned
long
and
the
error
value
is
ULONG
MAX
int
rand
void
rand
returns
a
pseudo
random
integer
in
the
range
to
RAND
MAX
which
is
at
least
void
srand
unsigned
int
seed
srand
uses
seed
as
the
seed
for
a
new
sequence
of
pseudo
random
numbers
The
initial
seed
is
void
calloc
size
t
nobj
size
t
size
calloc
returns
a
pointer
to
space
for
an
array
of
nobj
objects
each
of
size
size
or
NULL
if
the
request
cannot
be
satisfied
The
space
is
initialized
to
zero
bytes
void
malloc
size
t
size
malloc
returns
a
pointer
to
space
for
an
object
of
size
size
or
NULL
if
the
request
cannot
be
satisfied
The
space
is
uninitialized
void
realloc
void
p
size
t
size
realloc
changes
the
size
of
the
object
pointed
to
by
p
to
size
The
contents
will
be
unchanged
up
to
the
minimum
of
the
old
and
new
sizes
If
the
new
size
is
larger
the
new
space
is
uninitialized
realloc
returns
a
pointer
to
the
new
space
or
NULL
if
the
request
cannot
be
satisfied
in
which
case
p
is
unchanged
void
free
void
p
free
deallocates
the
space
pointed
to
by
p
it
does
nothing
if
p
is
NULL
p
must
be
a
pointer
to
space
previously
allocated
by
calloc
malloc
or
realloc
void
abort
void
abort
causes
the
program
to
terminate
abnormally
as
if
by
raise
SIGABRT
void
exit
int
status
exit
causes
normal
program
termination
atexit
functions
are
called
in
reverse
order
of
registration
open
files
are
flushed
open
streams
are
closed
and
control
is
returned
to
the
environment
How
status
is
returned
to
the
environment
is
implementationdependent
but
zero
is
taken
as
successful
termination
The
values
EXIT
SUCCESS
and
EXIT
FAILURE
may
also
be
used
int
atexit
void
fcn
void
atexit
registers
the
function
fcn
to
be
called
when
the
program
terminates
normally
it
returns
non
zero
if
the
registration
cannot
be
made
int
system
const
char
s
system
passes
the
string
s
to
the
environment
for
execution
If
s
is
NULL
system
returns
non
zero
if
there
is
a
command
processor
If
s
is
not
NULL
the
return
value
is
implementation
dependent
char
getenv
const
char
name
getenv
returns
the
environment
string
associated
with
name
or
NULL
if
no
string
exists
Details
are
implementation
dependent
void
bsearch
const
void
key
const
void
base
size
t
n
size
t
size
int
cmp
const
void
keyval
const
void
datum
bsearch
searches
base
base
n
for
an
item
that
matches
key
The
function
cmp
must
return
negative
if
its
first
argument
the
search
key
is
less
than
its
second
a
table
entry
zero
if
equal
and
positive
if
greater
Items
in
the
array
base
must
be
in
ascending
order
bsearch
returns
a
pointer
to
a
matching
item
or
NULL
if
none
exists
void
qsort
void
base
size
t
n
size
t
size
int
cmp
const
void
const
void
qsort
sorts
into
ascending
order
an
array
base
base
n
of
objects
of
size
size
The
comparison
function
cmp
is
as
in
bsearch
int
abs
int
n
abs
returns
the
absolute
value
of
its
int
argument
long
labs
long
n
labs
returns
the
absolute
value
of
its
long
argument
div
t
div
int
num
int
denom
div
computes
the
quotient
and
remainder
of
num
denom
The
results
are
stored
in
the
int
members
quot
and
rem
of
a
structure
of
type
div
t
ldiv
t
ldiv
long
num
long
denom
ldiv
computes
the
quotient
and
remainder
of
num
denom
The
results
are
stored
in
the
long
members
quot
and
rem
of
a
structure
of
type
ldiv
t
B
Diagnostics
assert
h
The
assert
macro
is
used
to
add
diagnostics
to
programs
void
assert
int
expression
If
expression
is
zero
when
assert
expression
is
executed
the
assert
macro
will
print
on
stderr
a
message
such
as
Assertion
failed
expression
file
filename
line
nnn
It
then
calls
abort
to
terminate
execution
The
source
filename
and
line
number
come
from
the
preprocessor
macros
FILE
and
LINE
If
NDEBUG
is
defined
at
the
time
assert
h
is
included
the
assert
macro
is
ignored
B
Variable
Argument
Lists
stdarg
h
The
header
stdarg
h
provides
facilities
for
stepping
through
a
list
of
function
arguments
of
unknown
number
and
type
Suppose
lastarg
is
the
last
named
parameter
of
a
function
f
with
a
variable
number
of
arguments
Then
declare
within
f
a
variable
of
type
va
list
that
will
point
to
each
argument
in
turn
va
list
ap
ap
must
be
initialized
once
with
the
macro
va
start
before
any
unnamed
argument
is
accessed
va
start
va
list
ap
lastarg
Thereafter
each
execution
of
the
macro
va
arg
will
produce
a
value
that
has
the
type
and
value
of
the
next
unnamed
argument
and
will
also
modify
ap
so
the
next
use
of
va
arg
returns
the
next
argument
type
va
arg
va
list
ap
type
The
macro
void
va
end
va
list
ap
must
be
called
once
after
the
arguments
have
been
processed
but
before
f
is
exited
B
Non
local
Jumps
setjmp
h
The
declarations
in
setjmp
h
provide
a
way
to
avoid
the
normal
function
call
and
return
sequence
typically
to
permit
an
immediate
return
from
a
deeply
nested
function
call
int
setjmp
jmp
buf
env
The
macro
setjmp
saves
state
information
in
env
for
use
by
longjmp
The
return
is
zero
from
a
direct
call
of
setjmp
and
non
zero
from
a
subsequent
call
of
longjmp
A
call
to
setjmp
can
only
occur
in
certain
contexts
basically
the
test
of
if
switch
and
loops
and
only
in
simple
relational
expressions
if
setjmp
env
get
here
on
direct
call
else
get
here
by
calling
longjmp
void
longjmp
jmp
buf
env
int
val
longjmp
restores
the
state
saved
by
the
most
recent
call
to
setjmp
using
the
information
saved
in
env
and
execution
resumes
as
if
the
setjmp
function
had
just
executed
and
returned
the
non
zero
value
val
The
function
containing
the
setjmp
must
not
have
terminated
Accessible
objects
have
the
values
they
had
at
the
time
longjmp
was
called
except
that
non
volatile
automatic
variables
in
the
function
calling
setjmp
become
undefined
if
they
were
changed
after
the
setjmp
call
B
Signals
signal
h
The
header
signal
h
provides
facilities
for
handling
exceptional
conditions
that
arise
during
execution
such
as
an
interrupt
signal
from
an
external
source
or
an
error
in
execution
void
signal
int
sig
void
handler
int
int
signal
determines
how
subsequent
signals
will
be
handled
If
handler
is
SIG
DFL
the
implementation
defined
default
behavior
is
used
if
it
is
SIG
IGN
the
signal
is
ignored
otherwise
the
function
pointed
to
by
handler
will
be
called
with
the
argument
of
the
type
of
signal
Valid
signals
include
SIGABRT
abnormal
termination
e
g
from
abort
SIGFPE
arithmetic
error
e
g
zero
divide
or
overflow
SIGILL
illegal
function
image
e
g
illegal
instruction
SIGINT
interactive
attention
e
g
interrupt
SIGSEGV
illegal
storage
access
e
g
access
outside
memory
limits
SIGTERM
termination
request
sent
to
this
program
signal
returns
the
previous
value
of
handler
for
the
specific
signal
or
SIG
ERR
if
an
error
occurs
When
a
signal
sig
subsequently
occurs
the
signal
is
restored
to
its
default
behavior
then
the
signal
handler
function
is
called
as
if
by
handler
sig
If
the
handler
returns
execution
will
resume
where
it
was
when
the
signal
occurred
The
initial
state
of
signals
is
implementation
defined
int
raise
int
sig
raise
sends
the
signal
sig
to
the
program
it
returns
non
zero
if
unsuccessful
B
Date
and
Time
Functions
time
h
The
header
time
h
declares
types
and
functions
for
manipulating
date
and
time
Some
functions
process
local
time
which
may
differ
from
calendar
time
for
example
because
of
time
zone
clock
t
and
time
t
are
arithmetic
types
representing
times
and
struct
tm
holds
the
components
of
a
calendar
time
int
tm
sec
seconds
after
the
minute
int
tm
min
minutes
after
the
hour
int
tm
hour
hours
since
midnight
int
tm
mday
day
of
the
month
int
tm
mon
months
since
January
int
tm
year
years
since
int
tm
wday
days
since
Sunday
int
tm
yday
days
since
January
int
tm
isdst
Daylight
Saving
Time
flag
tm
isdst
is
positive
if
Daylight
Saving
Time
is
in
effect
zero
if
not
and
negative
if
the
information
is
not
available
clock
t
clock
void
clock
returns
the
processor
time
used
by
the
program
since
the
beginning
of
execution
or
if
unavailable
clock
CLK
PER
SEC
is
a
time
in
seconds
time
t
time
time
t
tp
time
returns
the
current
calendar
time
or
if
the
time
is
not
available
If
tp
is
not
NULL
the
return
value
is
also
assigned
to
tp
double
difftime
time
t
time
time
t
time
difftime
returns
time
time
expressed
in
seconds
time
t
mktime
struct
tm
tp
mktime
converts
the
local
time
in
the
structure
tp
into
calendar
time
in
the
same
representation
used
by
time
The
components
will
have
values
in
the
ranges
shown
mktime
returns
the
calendar
time
or
if
it
cannot
be
represented
The
next
four
functions
return
pointers
to
static
objects
that
may
be
overwritten
by
other
calls
char
asctime
const
struct
tm
tp
asctime
tt
converts
the
time
in
the
structure
tp
into
a
string
of
the
form
Sun
Jan
n
char
ctime
const
time
t
tp
ctime
converts
the
calendar
time
tp
to
local
time
it
is
equivalent
to
asctime
localtime
tp
struct
tm
gmtime
const
time
t
tp
gmtime
converts
the
calendar
time
tp
into
Coordinated
Universal
Time
UTC
It
returns
NULL
if
UTC
is
not
available
The
name
gmtime
has
historical
significance
struct
tm
localtime
const
time
t
tp
localtime
converts
the
calendar
time
tp
into
local
time
size
t
strftime
char
s
size
t
smax
const
char
fmt
const
struct
tm
tp
strftime
formats
date
and
time
information
from
tp
into
s
according
to
fmt
which
is
analogous
to
a
printf
format
Ordinary
characters
including
the
terminating
are
copied
into
s
Each
c
is
replaced
as
described
below
using
values
appropriate
for
the
local
environment
No
more
than
smax
characters
are
placed
into
s
strftime
returns
the
number
of
characters
excluding
the
or
zero
if
more
than
smax
characters
were
produced
a
abbreviated
weekday
name
A
full
weekday
name
b
abbreviated
month
name
B
full
month
name
c
local
date
and
time
representation
d
day
of
the
month
H
hour
hour
clock
I
hour
hour
clock
j
day
of
the
year
m
month
M
minute
p
local
equivalent
of
AM
or
PM
S
second
U
week
number
of
the
year
Sunday
as
st
day
of
week
w
weekday
Sunday
is
W
week
number
of
the
year
Monday
as
st
day
of
week
x
local
date
representation
X
local
time
representation
y
year
without
century
Y
year
with
century
Z
time
zone
name
if
any
B
Implementation
defined
Limits
limits
h
and
float
h
The
header
limits
h
defines
constants
for
the
sizes
of
integral
types
The
values
below
are
acceptable
minimum
magnitudes
larger
values
may
be
used
CHAR
BIT
bits
in
a
char
CHAR
MAX
UCHAR
MAX
or
SCHAR
MAX
maximum
value
of
char
CHAR
MIN
or
SCHAR
MIN
maximum
value
of
char
INT
MAX
maximum
value
of
int
INT
MIN
minimum
value
of
int
LONG
MAX
maximum
value
of
long
LONG
MIN
minimum
value
of
long
SCHAR
MAX
maximum
value
of
signed
char
SCHAR
MIN
minimum
value
of
signed
char
SHRT
MAX
maximum
value
of
short
SHRT
MIN
minimum
value
of
short
UCHAR
MAX
maximum
value
of
unsigned
char
UINT
MAX
maximum
value
of
unsigned
int
ULONG
MAX
maximum
value
of
unsigned
long
USHRT
MAX
maximum
value
of
unsigned
short
The
names
in
the
table
below
a
subset
of
float
h
are
constants
related
to
floating
point
arithmetic
When
a
value
is
given
it
represents
the
minimum
magnitude
for
the
corresponding
quantity
Each
implementation
defines
appropriate
values
FLT
RADIX
radix
of
exponent
representation
e
g
FLT
ROUNDS
floating
point
rounding
mode
for
addition
FLT
DIG
decimal
digits
of
precision
FLT
EPSILON
E
smallest
number
x
such
that
x
FLT
MANT
DIG
number
of
base
FLT
RADIX
in
mantissa
FLT
MAX
E
maximum
floating
point
number
FLT
MAX
EXP
maximum
n
such
that
FLT
RADIXn
is
representable
FLT
MIN
E
minimum
normalized
floating
point
number
FLT
MIN
EXP
minimum
n
such
that
n
is
a
normalized
number
DBL
DIG
decimal
digits
of
precision
DBL
EPSILON
E
smallest
number
x
such
that
x
DBL
MANT
DIG
number
of
base
FLT
RADIX
in
mantissa
DBL
MAX
E
maximum
double
floating
point
number
DBL
MAX
EXP
maximum
n
such
that
FLT
RADIXn
is
representable
DBL
MIN
E
minimum
normalized
double
floating
point
number
DBL
MIN
EXP
minimum
n
such
that
n
is
a
normalized
number
Appendix
C
Summary
of
Changes
Since
the
publication
of
the
first
edition
of
this
book
the
definition
of
the
C
language
has
undergone
changes
Almost
all
were
extensions
of
the
original
language
and
were
carefully
designed
to
remain
compatible
with
existing
practice
some
repaired
ambiguities
in
the
original
description
and
some
represent
modifications
that
change
existing
practice
Many
of
the
new
facilities
were
announced
in
the
documents
accompanying
compilers
available
from
AT
T
and
have
subsequently
been
adopted
by
other
suppliers
of
C
compilers
More
recently
the
ANSI
committee
standardizing
the
language
incorporated
most
of
the
changes
and
also
introduced
other
significant
modifications
Their
report
was
in
part
participated
by
some
commercial
compilers
even
before
issuance
of
the
formal
C
standard
This
Appendix
summarizes
the
differences
between
the
language
defined
by
the
first
edition
of
this
book
and
that
expected
to
be
defined
by
the
final
standard
It
treats
only
the
language
itself
not
its
environment
and
library
although
these
are
an
important
part
of
the
standard
there
is
little
to
compare
with
because
the
first
edition
did
not
attempt
to
prescribe
an
environment
or
library
Preprocessing
is
more
carefully
defined
in
the
Standard
than
in
the
first
edition
and
is
extended
it
is
explicitly
token
based
there
are
new
operators
for
concatenation
of
tokens
and
creation
of
strings
there
are
new
control
lines
like
elif
and
pragma
redeclaration
of
macros
by
the
same
token
sequence
is
explicitly
permitted
parameters
inside
strings
are
no
longer
replaced
Splicing
of
lines
by
is
permitted
everywhere
not
just
in
strings
and
macro
definitions
See
Par
A
The
minimum
significance
of
all
internal
identifiers
increased
to
characters
the
smallest
mandated
significance
of
identifiers
with
external
linkage
remains
monocase
letters
Many
implementations
provide
more
Trigraph
sequences
introduced
by
allow
representation
of
characters
lacking
in
some
character
sets
Escapes
for
are
defined
see
Par
A
Observe
that
the
introduction
of
trigraphs
may
change
the
meaning
of
strings
containing
the
sequence
New
keywords
void
const
volatile
signed
enum
are
introduced
The
stillborn
entry
keyword
is
withdrawn
New
escape
sequences
for
use
within
character
constants
and
string
literals
are
defined
The
effect
of
following
by
a
character
not
part
of
an
approved
escape
sequence
is
undefined
See
Par
A
Everyone
s
favorite
trivial
change
and
are
not
octal
digits
The
standard
introduces
a
larger
set
of
suffixes
to
make
the
type
of
constants
explicit
U
or
L
for
integers
F
or
L
for
floating
It
also
refines
the
rules
for
the
type
of
unsiffixed
constants
Par
A
Adjacent
string
literals
are
concatenated
There
is
a
notation
for
wide
character
string
literals
and
character
constants
see
Par
A
Characters
as
well
as
other
types
may
be
explicitly
declared
to
carry
or
not
to
carry
a
sign
by
using
the
keywords
signed
or
unsigned
The
locution
long
float
as
a
synonym
for
double
is
withdrawn
but
long
double
may
be
used
to
declare
an
extraprecision
floating
quantity
For
some
time
type
unsigned
char
has
been
available
The
standard
introduces
the
signed
keyword
to
make
signedness
explicit
for
char
and
other
integral
objects
The
void
type
has
been
available
in
most
implementations
for
some
years
The
Standard
introduces
the
use
of
the
void
type
as
a
generic
pointer
type
previously
char
played
this
role
At
the
same
time
explicit
rules
are
enacted
against
mixing
pointers
and
integers
and
pointers
of
different
type
without
the
use
of
casts
The
Standard
places
explicit
minima
on
the
ranges
of
the
arithmetic
types
and
mandates
headers
limits
h
and
float
h
giving
the
characteristics
of
each
particular
implementation
Enumerations
are
new
since
the
first
edition
of
this
book
The
Standard
adopts
from
C
the
notion
of
type
qualifier
for
example
const
Par
A
Strings
are
no
longer
modifiable
and
so
may
be
placed
in
read
only
memory
The
usual
arithmetic
conversions
are
changed
essentially
from
for
integers
unsigned
always
wins
for
floating
point
always
use
double
to
promote
to
the
smallest
capacious
enough
type
See
Par
A
The
old
assignment
operators
like
are
truly
gone
Also
assignment
operators
are
now
single
tokens
in
the
first
edition
they
were
pairs
and
could
be
separated
by
white
space
A
compiler
s
license
to
treat
mathematically
associative
operators
as
computationally
associative
is
revoked
A
unary
operator
is
introduced
for
symmetry
with
unary
A
pointer
to
a
function
may
be
used
as
a
function
designator
without
an
explicit
operator
See
Par
A
Structures
may
be
assigned
passed
to
functions
and
returned
by
functions
Applying
the
address
of
operator
to
arrays
is
permitted
and
the
result
is
a
pointer
to
the
array
The
sizeof
operator
in
the
first
edition
yielded
type
int
subsequently
many
implementations
made
it
unsigned
The
Standard
makes
its
type
explicitly
implementation
dependent
but
requires
the
type
size
t
to
be
defined
in
a
standard
header
stddef
h
A
similar
change
occurs
in
the
type
ptrdiff
t
of
the
difference
between
pointers
See
Par
A
and
Par
A
The
address
of
operator
may
not
be
applied
to
an
object
declared
register
even
if
the
implementation
chooses
not
to
keep
the
object
in
a
register
The
type
of
a
shift
expression
is
that
of
the
left
operand
the
right
operand
can
t
promote
the
result
See
Par
A
The
Standard
legalizes
the
creation
of
a
pointer
just
beyond
the
end
of
an
array
and
allows
arithmetic
and
relations
on
it
see
Par
A
The
Standard
introduces
borrowing
from
C
the
notion
of
a
function
prototype
declaration
that
incorporates
the
types
of
the
parameters
and
includes
an
explicit
recognition
of
variadic
functions
together
with
an
approved
way
of
dealing
with
them
See
Pars
A
A
B
The
older
style
is
still
accepted
with
restrictions
Empty
declarations
which
have
no
declarators
and
don
t
declare
at
least
a
structure
union
or
enumeration
are
forbidden
by
the
Standard
On
the
other
hand
a
declaration
with
just
a
structure
or
union
tag
redeclares
that
tag
even
if
it
was
declared
in
an
outer
scope
External
data
declarations
without
any
specifiers
or
qualifiers
just
a
naked
declarator
are
forbidden
Some
implementations
when
presented
with
an
extern
declaration
in
an
inner
block
would
export
the
declaration
to
the
rest
of
the
file
The
Standard
makes
it
clear
that
the
scope
of
such
a
declaration
is
just
the
block
The
scope
of
parameters
is
injected
into
a
function
s
compound
statement
so
that
variable
declarations
at
the
top
level
of
the
function
cannot
hide
the
parameters
The
name
spaces
of
identifiers
are
somewhat
different
The
Standard
puts
all
tags
in
a
single
name
space
and
also
introduces
a
separate
name
space
for
labels
see
Par
A
Also
member
names
are
associated
with
the
structure
or
union
of
which
they
are
a
part
This
has
been
common
practice
from
some
time
Unions
may
be
initialized
the
initializer
refers
to
the
first
member
Automatic
structures
unions
and
arrays
may
be
initialized
albeit
in
a
restricted
way
Character
arrays
with
an
explicit
size
may
be
initialized
by
a
string
literal
with
exactly
that
many
characters
the
is
quietly
squeezed
out
The
controlling
expression
and
the
case
labels
of
a
switch
may
have
any
integral
type
