In Praise of Digital Design
and Computer Architecture
Harris and Harris have taken the popular pedagogy from Computer
Organization and Design to the next level of refinement, showing in
detail how to build a MIPS microprocessor in both Verilog and VHDL.
Given the exciting opportunity that students have to run large digital
designs on modern FGPAs, the approach the authors take in this book is
both informative and enlightening.
David A. Patterson University of California, Berkeley
Digital Design and Computer Architecture brings a fresh perspective to
an old discipline. Many textbooks tend to resemble overgrown shrubs,
but Harris and Harris have managed to prune away the deadwood while
preserving the fundamentals and presenting them in a contemporary context. In doing so, they offer a text that will benefit students interested in
designing solutions for tomorrow’s challenges.
Jim Frenzel University of Idaho
Harris and Harris have a pleasant and informative writing style.
Their treatment of the material is at a good level for introducing students
to computer engineering with plenty of helpful diagrams. Combinational
circuits, microarchitecture, and memory systems are handled particularly well.
James Pinter-Lucke Claremont McKenna College
Harris and Harris have written a book that is very clear and easy to
understand. The exercises are well-designed and the real-world examples
are a nice touch. The lengthy and confusing explanations often found in
similar textbooks are not seen here. It’s obvious that the authors have
devoted a great deal of time and effort to create an accessible text.
I strongly recommend Digital Design and Computer Architecture.
Peiyi Zhao Chapman University

Harris and Harris have created the first book that successfully combines
digital system design with computer architecture. Digital Design and
Computer Architecture is a much-welcomed text that extensively explores
digital systems designs and explains the MIPS architecture in fantastic
detail. I highly recommend this book.
James E. Stine, Jr., Oklahoma State University
Digital Design and Computer Architecture is a brilliant book. Harris and
Harris seamlessly tie together all the important elements in microprocessor design—transistors, circuits, logic gates, finite state machines, memories, arithmetic units—and conclude with computer architecture. This
text is an excellent guide for understanding how complex systems can be
flawlessly designed.
Jaeha Kim Rambus, Inc.
Digital Design and Computer Architecture is a very well-written book
that will appeal to both young engineers who are learning these subjects
for the first time and also to the experienced engineers who want to use
this book as a reference. I highly recommend it.
A. Utku Diril Nvidia Corporation

Digital Design and
Computer Architecture

About the Authors
David Money Harris is an associate professor of engineering at Harvey
Mudd College. He received his Ph.D. in electrical engineering from
Stanford University and his M.Eng. in electrical engineering and computer science from MIT. Before attending Stanford, he worked at Intel as
a logic and circuit designer on the Itanium and Pentium II processors.
Since then, he has consulted at Sun Microsystems, Hewlett-Packard,
Evans & Sutherland, and other design companies.
David’s passions include teaching, building chips, and exploring the
outdoors. When he is not at work, he can usually be found hiking,
mountaineering, or rock climbing. He particularly enjoys hiking with his
son, Abraham, who was born at the start of this book project. David
holds about a dozen patents and is the author of three other textbooks
on chip design, as well as two guidebooks to the Southern California
mountains.
Sarah L. Harris is an assistant professor of engineering at Harvey Mudd
College. She received her Ph.D. and M.S. in electrical engineering from
Stanford University. Before attending Stanford, she received a B.S. in electrical and computer engineering from Brigham Young University. Sarah
has also worked with Hewlett-Packard, the San Diego Supercomputer
Center, Nvidia, and Microsoft Research in Beijing.
Sarah loves teaching, exploring and developing new technologies,
traveling, wind surfing, rock climbing, and playing the guitar. Her recent
exploits include researching sketching interfaces for digital circuit design,
acting as a science correspondent for a National Public Radio affiliate,
and learning how to kite surf. She speaks four languages and looks forward to adding a few more to the list in the near future.

Digital Design and
Computer Architecture
David Money Harris
Sarah L. Harris
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Morgan Kaufmann Publishers is an imprint of Elsevier

Publisher: Denise E. M. Penrose
Senior Developmental Editor: Nate McFadden
Publishing Services Manager: George Morrison
Project Manager: Marilyn E Rash
Assistant Editor: Mary E. James
Editorial Assistant: Kimberlee Honjo
Cover and Editorial Illustrations: Duane Bibby
Interior Design: Frances Baca Design
Composition: Integra
Technical Illustrations: Harris and Harris/Integra
Production Services: Graphic World Inc.
Interior Printer: Courier-Westford
Cover Printer: Phoenix Color Corp.
Morgan Kaufmann Publishers is an imprint of Elsevier.
500 Sansome Street, Suite 400, San Francisco, CA 94111
This book is printed on acid-free paper.
© 2007 by Elsevier Inc. All rights reserved.
Designations used by companies to distinguish their products are often claimed as trademarks or
registered trademarks. In all instances in which Morgan Kaufmann Publishers is aware of a claim,
the product names appear in initial capital or all capital letters. Readers, however, should contact
the appropriate companies for more complete information regarding trademarks and registration.
Permissions may be sought directly from Elsevier’s Science & Technology Rights Department
in Oxford, UK: phone: (44) 1865 843830, fax: (44) 1865 853333, e-mail: permissions
@elsevier.co.uk. You may also complete your request on-line via the Elsevier homepage
(http://elsevier.com) by selecting “Customer Support” and then “Obtaining Permissions.”
Library of Congress Cataloging-in-Publication Data
Harris, David Money.
Digital design and computer architecture / David Money Harris and
Sarah L. Harris.—1st ed.
p. cm.
Includes bibliographical references and index.
ISBN 13: 978-0-12-370497-9 (alk. paper)
ISBN 10: 0-12-370497-9
1. Digital electronics. 2. Logic design. 3. Computer architecture. I. Harris, Sarah L. II. Title.
TK7868.D5H298 2007
621.381—dc22
2006030554
For information on all Morgan Kaufmann publications,
visit our Web site at www.mkp.com
Printed in the United States of America
07 08 09 10 11 5 4 3 2 1
Pr
To my family, Jennifer and Abraham
– DMH
To my sisters, Lara and Jenny
– SLH


Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviii
Online Supplements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
How to Use the Software Tools in a Course . . . . . . . . . . . . . . xix
Labs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xx
Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
Chapter 1 From Zero to One . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1 The Game Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 The Art of Managing Complexity . . . . . . . . . . . . . . . . . . . . . 4
1.2.1 Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.2 Discipline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.3 The Three -Y’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 The Digital Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 Number Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.1 Decimal Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.2 Binary Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.3 Hexadecimal Numbers . . . . . . . . . . . . . . . . . . . . . . 11
1.4.4 Bytes, Nibbles, and All That Jazz . . . . . . . . . . . . . . 13
1.4.5 Binary Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.4.6 Signed Binary Numbers . . . . . . . . . . . . . . . . . . . . . . 15
1.5 Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.5.1 NOT Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5.2 Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5.3 AND Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5.4 OR Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.5.5 Other Two-Input Gates . . . . . . . . . . . . . . . . . . . . . . 21
1.5.6 Multiple-Input Gates . . . . . . . . . . . . . . . . . . . . . . . . 21
1.6 Beneath the Digital Abstraction . . . . . . . . . . . . . . . . . . . . . . . 22
1.6.1 Supply Voltage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.6.2 Logic Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.6.3 Noise Margins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.6.4 DC Transfer Characteristics . . . . . . . . . . . . . . . . . . . 23
1.6.5 The Static Discipline . . . . . . . . . . . . . . . . . . . . . . . . 24
ix

1.7 CMOS Transistors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
1.7.1 Semiconductors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.7.2 Diodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.7.3 Capacitors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.7.4 nMOS and pMOS Transistors . . . . . . . . . . . . . . . . . 28
1.7.5 CMOS NOT Gate . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.7.6 Other CMOS Logic Gates . . . . . . . . . . . . . . . . . . . . 31
1.7.7 Transmission Gates . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.7.8 Pseudo-nMOS Logic . . . . . . . . . . . . . . . . . . . . . . . . 33
1.8 Power Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.9 Summary and a Look Ahead . . . . . . . . . . . . . . . . . . . . . . . . . 35
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Chapter 2 Combinational Logic Design . . . . . . . . . . . . . . . . . . . . 51
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.2 Boolean Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.1 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.2 Sum-of-Products Form . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.3 Product-of-Sums Form . . . . . . . . . . . . . . . . . . . . . . . 56
2.3 Boolean Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.3.1 Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.3.2 Theorems of One Variable . . . . . . . . . . . . . . . . . . . . 57
2.3.3 Theorems of Several Variables . . . . . . . . . . . . . . . . . 58
2.3.4 The Truth Behind It All . . . . . . . . . . . . . . . . . . . . . . 60
2.3.5 Simplifying Equations . . . . . . . . . . . . . . . . . . . . . . . 61
2.4 From Logic to Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
2.5 Multilevel Combinational Logic . . . . . . . . . . . . . . . . . . . . . . . 65
2.5.1 Hardware Reduction . . . . . . . . . . . . . . . . . . . . . . . . 66
2.5.2 Bubble Pushing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
2.6 X’s and Z’s, Oh My . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.6.1 Illegal Value: X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.6.2 Floating Value: Z . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
2.7 Karnaugh Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
2.7.1 Circular Thinking . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.7.2 Logic Minimization with K-Maps . . . . . . . . . . . . . . . 73
2.7.3 Don’t Cares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
2.7.4 The Big Picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
2.8 Combinational Building Blocks . . . . . . . . . . . . . . . . . . . . . . . 79
2.8.1 Multiplexers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
2.8.2 Decoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
2.9 Timing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
2.9.1 Propagation and Contamination Delay . . . . . . . . . . 84
2.9.2 Glitches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
x CONTENTS

2.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
Chapter 3 Sequential Logic Design . . . . . . . . . . . . . . . . . . . . . . 103
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.2 Latches and Flip-Flops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.2.1 SR Latch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.2.2 D Latch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.2.3 D Flip-Flop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.2.4 Register . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.2.5 Enabled Flip-Flop . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.2.6 Resettable Flip-Flop . . . . . . . . . . . . . . . . . . . . . . . . 110
3.2.7 Transistor-Level Latch and Flip-Flop Designs . . . . 110
3.2.8 Putting It All Together . . . . . . . . . . . . . . . . . . . . . . 112
3.3 Synchronous Logic Design . . . . . . . . . . . . . . . . . . . . . . . . . . 113
3.3.1 Some Problematic Circuits . . . . . . . . . . . . . . . . . . . 113
3.3.2 Synchronous Sequential Circuits . . . . . . . . . . . . . . . 114
3.3.3 Synchronous and Asynchronous Circuits . . . . . . . . 116
3.4 Finite State Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.4.1 FSM Design Example . . . . . . . . . . . . . . . . . . . . . . . 117
3.4.2 State Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.4.3 Moore and Mealy Machines . . . . . . . . . . . . . . . . . . 126
3.4.4 Factoring State Machines . . . . . . . . . . . . . . . . . . . . 129
3.4.5 FSM Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
3.5 Timing of Sequential Logic. . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.5.1 The Dynamic Discipline . . . . . . . . . . . . . . . . . . . . . 134
3.5.2 System Timing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
3.5.3 Clock Skew . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
3.5.4 Metastability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
3.5.5 Synchronizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
3.5.6 Derivation of Resolution Time . . . . . . . . . . . . . . . . 146
3.6 Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
Chapter 4 Hardware Description Languages . . . . . . . . . . . . . . 167
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
4.1.1 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
4.1.2 Language Origins . . . . . . . . . . . . . . . . . . . . . . . . . . 168
4.1.3 Simulation and Synthesis . . . . . . . . . . . . . . . . . . . . 169
CONTENTS xi

4.2 Combinational Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.2.1 Bitwise Operators . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.2.2 Comments and White Space . . . . . . . . . . . . . . . . . . 174
4.2.3 Reduction Operators . . . . . . . . . . . . . . . . . . . . . . . 174
4.2.4 Conditional Assignment . . . . . . . . . . . . . . . . . . . . . 175
4.2.5 Internal Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 176
4.2.6 Precedence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
4.2.7 Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.2.8 Z’s and X’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.2.9 Bit Swizzling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.2.10 Delays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.2.11 VHDL Libraries and Types . . . . . . . . . . . . . . . . . . 183
4.3 Structural Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
4.4 Sequential Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.4.1 Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.4.2 Resettable Registers . . . . . . . . . . . . . . . . . . . . . . . . 191
4.4.3 Enabled Registers . . . . . . . . . . . . . . . . . . . . . . . . . . 193
4.4.4 Multiple Registers . . . . . . . . . . . . . . . . . . . . . . . . . . 194
4.4.5 Latches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
4.5 More Combinational Logic . . . . . . . . . . . . . . . . . . . . . . . . . 195
4.5.1 Case Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
4.5.2 If Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.5.3 Verilog casez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
4.5.4 Blocking and Nonblocking Assignments . . . . . . . . 201
4.6 Finite State Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
4.7 Parameterized Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
4.8 Testbenches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
4.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Chapter 5 Digital Building Blocks . . . . . . . . . . . . . . . . . . . . . . . 233
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.2 Arithmetic Circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.2.1 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.2.2 Subtraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
5.2.3 Comparators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
5.2.4 ALU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
5.2.5 Shifters and Rotators . . . . . . . . . . . . . . . . . . . . . . . 244
5.2.6 Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
5.2.7 Division . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
5.2.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
xii CONTENTS

5.3 Number Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
5.3.1 Fixed-Point Number Systems . . . . . . . . . . . . . . . . . 249
5.3.2 Floating-Point Number Systems . . . . . . . . . . . . . . . 250
5.4 Sequential Building Blocks. . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.4.1 Counters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.4.2 Shift Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
5.5 Memory Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
5.5.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
5.5.2 Dynamic Random Access Memory . . . . . . . . . . . . . 260
5.5.3 Static Random Access Memory . . . . . . . . . . . . . . . 260
5.5.4 Area and Delay . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
5.5.5 Register Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
5.5.6 Read Only Memory . . . . . . . . . . . . . . . . . . . . . . . . 262
5.5.7 Logic Using Memory Arrays . . . . . . . . . . . . . . . . . 264
5.5.8 Memory HDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
5.6 Logic Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
5.6.1 Programmable Logic Array . . . . . . . . . . . . . . . . . . 266
5.6.2 Field Programmable Gate Array . . . . . . . . . . . . . . 268
5.6.3 Array Implementations . . . . . . . . . . . . . . . . . . . . . . 273
5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
Chapter 6 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
6.2 Assembly Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
6.2.1 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
6.2.2 Operands: Registers, Memory, and
Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
6.3 Machine Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.3.1 R-type Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.3.2 I-type Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 301
6.3.3 J-type Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 302
6.3.4 Interpreting Machine Language Code . . . . . . . . . . 302
6.3.5 The Power of the Stored Program . . . . . . . . . . . . . 303
6.4 Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
6.4.1 Arithmetic/Logical Instructions . . . . . . . . . . . . . . . 304
6.4.2 Branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
6.4.3 Conditional Statements . . . . . . . . . . . . . . . . . . . . . 310
6.4.4 Getting Loopy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
6.4.5 Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
6.4.6 Procedure Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
6.5 Addressing Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
CONTENTS xiii

6.6 Lights, Camera, Action: Compiling, Assembling,
and Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
6.6.1 The Memory Map . . . . . . . . . . . . . . . . . . . . . . . . . 330
6.6.2 Translating and Starting a Program . . . . . . . . . . . . 331
6.7 Odds and Ends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
6.7.1 Pseudoinstructions . . . . . . . . . . . . . . . . . . . . . . . . . 336
6.7.2 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
6.7.3 Signed and Unsigned Instructions . . . . . . . . . . . . . . 338
6.7.4 Floating-Point Instructions . . . . . . . . . . . . . . . . . . . 340
6.8 Real-World Perspective: IA-32 Architecture . . . . . . . . . . . . . 341
6.8.1 IA-32 Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
6.8.2 IA-32 Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
6.8.3 Status Flags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
6.8.4 IA-32 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 344
6.8.5 IA-32 Instruction Encoding . . . . . . . . . . . . . . . . . . 346
6.8.6 Other IA-32 Peculiarities . . . . . . . . . . . . . . . . . . . . 348
6.8.7 The Big Picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
6.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
Chapter 7 Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . 363
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
7.1.1 Architectural State and Instruction Set . . . . . . . . . . 363
7.1.2 Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
7.1.3 MIPS Microarchitectures . . . . . . . . . . . . . . . . . . . . 366
7.2 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
7.3 Single-Cycle Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
7.3.1 Single-Cycle Datapath . . . . . . . . . . . . . . . . . . . . . . 368
7.3.2 Single-Cycle Control . . . . . . . . . . . . . . . . . . . . . . . . 374
7.3.3 More Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 377
7.3.4 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . 380
7.4 Multicycle Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
7.4.1 Multicycle Datapath . . . . . . . . . . . . . . . . . . . . . . . . 382
7.4.2 Multicycle Control . . . . . . . . . . . . . . . . . . . . . . . . . 388
7.4.3 More Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 395
7.4.4 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . 397
7.5 Pipelined Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
7.5.1 Pipelined Datapath . . . . . . . . . . . . . . . . . . . . . . . . . 404
7.5.2 Pipelined Control . . . . . . . . . . . . . . . . . . . . . . . . . . 405
7.5.3 Hazards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
7.5.4 More Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 418
7.5.5 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . 418
xiv CONTENTS

7.6 HDL Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
7.6.1 Single-Cycle Processor . . . . . . . . . . . . . . . . . . . . . . 422
7.6.2 Generic Building Blocks . . . . . . . . . . . . . . . . . . . . . 426
7.6.3 Testbench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
7.7 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
7.8 Advanced Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . 435
7.8.1 Deep Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
7.8.2 Branch Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 437
7.8.3 Superscalar Processor . . . . . . . . . . . . . . . . . . . . . . . 438
7.8.4 Out-of-Order Processor . . . . . . . . . . . . . . . . . . . . . 441
7.8.5 Register Renaming . . . . . . . . . . . . . . . . . . . . . . . . . 443
7.8.6 Single Instruction Multiple Data . . . . . . . . . . . . . . 445
7.8.7 Multithreading . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
7.8.8 Multiprocessors . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
7.9 Real-World Perspective: IA-32 Microarchitecture . . . . . . . . . 447
7.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
Chapter 8 Memory Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
8.2 Memory System Performance Analysis . . . . . . . . . . . . . . . . . 467
8.3 Caches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
8.3.1 What Data Is Held in the Cache? . . . . . . . . . . . . . . 469
8.3.2 How Is the Data Found? . . . . . . . . . . . . . . . . . . . . 470
8.3.3 What Data Is Replaced? . . . . . . . . . . . . . . . . . . . . . 478
8.3.4 Advanced Cache Design . . . . . . . . . . . . . . . . . . . . . 479
8.3.5 The Evolution of MIPS Caches . . . . . . . . . . . . . . . 483
8.4 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
8.4.1 Address Translation . . . . . . . . . . . . . . . . . . . . . . . . 486
8.4.2 The Page Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
8.4.3 The Translation Lookaside Buffer . . . . . . . . . . . . . 490
8.4.4 Memory Protection . . . . . . . . . . . . . . . . . . . . . . . . 491
8.4.5 Replacement Policies . . . . . . . . . . . . . . . . . . . . . . . 492
8.4.6 Multilevel Page Tables . . . . . . . . . . . . . . . . . . . . . . 492
8.5 Memory-Mapped I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
8.6 Real-World Perspective: IA-32 Memory and I/O Systems . . . 499
8.6.1 IA-32 Cache Systems . . . . . . . . . . . . . . . . . . . . . . . 499
8.6.2 IA-32 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . 501
8.6.3 IA-32 Programmed I/O . . . . . . . . . . . . . . . . . . . . . 502
8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
CONTENTS xv

Appendix A Digital System Implementation . . . . . . . . . . . . . . . 515
A.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
A.2 74xx Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
A.2.1 Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.2.2 Other Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.3 Programmable Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.3.1 PROMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.3.2 PLAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
A.3.3 FPGAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
A.4 Application-Specific Integrated Circuits . . . . . . . . . . . . . . . . 523
A.5 Data Sheets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
A.6 Logic Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529
A.7 Packaging and Assembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531
A.8 Transmission lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
A.8.1 Matched Termination . . . . . . . . . . . . . . . . . . . . . . . 536
A.8.2 Open Termination . . . . . . . . . . . . . . . . . . . . . . . . . 538
A.8.3 Short Termination . . . . . . . . . . . . . . . . . . . . . . . . . 539
A.8.4 Mismatched Termination . . . . . . . . . . . . . . . . . . . . 539
A.8.5 When to Use Transmission Line Models . . . . . . . . . 542
A.8.6 Proper Transmission Line Terminations . . . . . . . . . 542
A.8.7 Derivation of Z0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
A.8.8 Derivation of the Reflection Coefficient . . . . . . . . . 545
A.8.9 Putting It All Together . . . . . . . . . . . . . . . . . . . . . . 546
A.9 Economics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
Appendix B MIPS Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 551
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
xvi CONTENTS

xvii
Preface
Why publish yet another book on digital design and computer architecture? There are dozens of good books in print on digital design. There
are also several good books about computer architecture, especially the
classic texts of Patterson and Hennessy. This book is unique in its treatment in that it presents digital logic design from the perspective of computer architecture, starting at the beginning with 1’s and 0’s, and leading
students through the design of a MIPS microprocessor.
We have used several editions of Patterson and Hennessy’s
Computer Organization and Design (COD) for many years at Harvey
Mudd College. We particularly like their coverage of the MIPS architecture and microarchitecture because MIPS is a commercially successful
microprocessor architecture, yet it is simple enough to clearly explain
and build in an introductory class. Because our class has no prerequisites, the first half of the semester is dedicated to digital design, which is
not covered by COD. Other universities have indicated a need for a
book that combines digital design and computer architecture. We have
undertaken to prepare such a book.
We believe that building a microprocessor is a special rite of passage
for engineering and computer science students. The inner workings of a
processor seem almost magical to the uninitiated, yet prove to be
straightforward when carefully explained. Digital design in itself is a
powerful and exciting subject. Assembly language programming unveils
the inner language spoken by the processor. Microarchitecture is the link
that brings it all together.
This book is suitable for a rapid-paced, single-semester introduction to digital design and computer architecture or for a two-quarter or
two-semester sequence giving more time to digest the material and
experiment in the lab. The only prerequisite is basic familiarity with a
high-level programming language such as C, C++, or Java. The material is usually taught at the sophomore- or junior-year level, but may
also be accessible to bright freshmen who have some programming
experience.

FEATURES
This book offers a number of special features.
Side-by-Side Coverage of Verilog and VHDL
Hardware description languages (HDLs) are at the center of modern digital design practices. Unfortunately, designers are evenly split between
the two dominant languages, Verilog and VHDL. This book introduces
HDLs in Chapter 4 as soon as combinational and sequential logic design
has been covered. HDLs are then used in Chapters 5 and 7 to design
larger building blocks and entire processors. Nevertheless, Chapter 4 can
be skipped and the later chapters are still accessible for courses that
choose not to cover HDLs.
This book is unique in its side-by-side presentation of Verilog and
VHDL, enabling the reader to quickly compare and contrast the two
languages. Chapter 4 describes principles applying to both HDLs, then
provides language-specific syntax and examples in adjacent columns.
This side-by-side treatment makes it easy for an instructor to choose
either HDL, and for the reader to transition from one to the other, either
in a class or in professional practice.
Classic MIPS Architecture and Microarchitecture
Chapters 6 and 7 focus on the MIPS architecture adapted from the treatment of Patterson and Hennessy. MIPS is an ideal architecture because it is a
real architecture shipped in millions of products yearly, yet it is streamlined
and easy to learn. Moreover, hundreds of universities around the world
have developed pedagogy, labs, and tools around the MIPS architecture.
Real-World Perspectives
Chapters 6, 7, and 8 illustrate the architecture, microarchitecture, and
memory hierarchy of Intel IA-32 processors. These real-world perspective chapters show how the concepts in the chapter relate to the chips
found in most PCs.
Accessible Overview of Advanced Microarchitecture
Chapter 7 includes an overview of modern high-performance microarchitectural features including branch prediction, superscalar and out-oforder operation, multithreading, and multicore processors. The
treatment is accessible to a student in a first course and shows how the
microarchitectures in the book can be extended to modern processors.
End-of-Chapter Exercises and Interview Questions
The best way to learn digital design is to do it. Each chapter ends with
numerous exercises to practice the material. The exercises are followed
by a set of interview questions that our industrial colleagues have asked
students applying for work in the field. These questions provide a helpful
xviii PREFACE

glimpse into the types of problems job applicants will typically encounter
during the interview process. (Exercise solutions are available via the
book’s companion and instructor Web pages. For more details, see the
next section, Online Supplements.)
ONLINE SUPPLEMENTS
Supplementary materials are available online at textbooks.elsevier.com/
9780123704979. This companion site (accessible to all readers) includes:
 Solutions to odd-numbered exercises
 Links to professional-strength computer-aided design (CAD) tools
from Xilinx® and Synplicity®
 Link to PCSPIM, a Windows-based MIPS simulator
 Hardware description language (HDL) code for the MIPS processor
 Xilinx Project Navigator helpful hints
 Lecture slides in PowerPoint (PPT) format
 Sample course and lab materials
 List of errata
The instructor site (linked to the companion site and accessible to
adopters who register at textbooks.elsevier.com) includes:
 Solutions to even-numbered exercises
 Links to professional-strength computer-aided design (CAD) tools
from Xilinx® and Synplicity®. (Instructors from qualified universities can access free Synplicity tools for use in their classroom and
laboratories. More details are available at the instructor site.)
 Figures from the text in JPG and PPT formats
Additional details on using the Xilinx, Synplicity, and PCSPIM tools in
your course are provided in the next section. Details on the sample lab
materials are also provided here.
HOW TO USE THE SOFTWARE TOOLS IN A COURSE
Xilinx ISE WebPACK
Xilinx ISE WebPACK is a free version of the professional-strength Xilinx
ISE Foundation FPGA design tools. It allows students to enter their digital designs in schematic or using either the Verilog or VHDL hardware
description language (HDL). After entering the design, students can
PREFACE xix
Prelims.qxd
simulate their circuits using ModelSim MXE III Starter, which is
included in the Xilinx WebPACK. Xilinx WebPACK also includes XST, a
logic synthesis tool supporting both Verilog and VHDL.
The difference between WebPACK and Foundation is that WebPACK
supports a subset of the most common Xilinx FPGAs. The difference
between ModelSim MXE III Starter and ModelSim commercial versions
is that Starter degrades performance for simulations with more than
10,000 lines of HDL.
Synplify Pro
Synplify Pro® is a high-performance, sophisticated logic synthesis engine
for FPGA and CPLD designs. Synplify Pro also contains HDL Analyst, a
graphical interface tool that generates schematic views of the HDL
source code. We have found that this is immensely useful in the learning
and debugging process.
Synplicity has generously agreed to donate Synplify Pro to qualified universities and will provide as many licenses as needed to fill
university labs. Instructors should visit the instructor Web page for
this text for more information on how to request Synplify Pro licenses.
For additional information on Synplicity and its other software, visit
www.synplicity.com/university.
PCSPIM
PCSPIM, also called simply SPIM, is a Windows-based MIPS simulator
that runs MIPS assembly code. Students enter their MIPS assembly code
into a text file and run it using PCSPIM. PCSPIM displays the instructions, memory, and register values. Links to the user’s manual and an
example file are available at the companion site (textbooks.elsevier.com/
9780123704979).
LABS
The companion site includes links to a series of labs that cover topics
from digital design through computer architecture. The labs teach students how to use the Xilinx WebPACK or Foundation tools to enter,
simulate, synthesize, and implement their designs. The labs also include
topics on assembly language programming using the PCSPIM simulator.
After synthesis, students can implement their designs using the Digilent
Spartan 3 Starter Board or the XUP-Virtex 2 Pro (V2Pro) Board. Both of
these powerful and competitively priced boards are available from
www.digilentinc.com. The boards contain FPGAs that can be programmed
to implement student designs. We provide labs that describe how to implement a selection of designs using Digilent’s Spartan 3 Board using
xx PREFACE

PREFACE xxi
WebPACK. Unfortunately, Xilinx WebPACK does not support the huge
FPGA on the V2Pro board. Qualified universities may contact the Xilinx
University Program to request a donation of the full Foundation tools.
To run the labs, students will need to download and install the
Xilinx WebPACK, PCSPIM, and possibly Synplify Pro. Instructors may
also choose to install the tools on lab machines. The labs include instructions on how to implement the projects on the Digilent’s Spartan 3
Starter Board. The implementation step may be skipped, but we have
found it of great value. The labs will also work with the XST synthesis
tool, but we recommend using Synplify Pro because the schematics it
produces give students invaluable feedback.
We have tested the labs on Windows, but the tools are also available
for Linux.
BUGS
As all experienced programmers know, any program of significant complexity undoubtedly contains bugs. So too do books. We have taken
great care to find and squash the bugs in this book. However, some
errors undoubtedly do remain. We will maintain a list of errata on the
book’s Web page.
Please send your bug reports to ddcabugs@onehotlogic.com. The first
person to report a substantive bug with a fix that we use in a future printing will be rewarded with a $1 bounty! (Be sure to include your mailing
address.)
ACKNOWLEDGMENTS
First and foremost, we thank David Patterson and John Hennessy for
their pioneering MIPS microarchitectures described in their Computer
Organization and Design textbook. We have taught from various editions
of their book for many years. We appreciate their gracious support of this
book and their permission to build on their microarchitectures.
Duane Bibby, our favorite cartoonist, labored long and hard to illustrate the fun and adventure of digital design. We also appreciate the enthusiasm of Denise Penrose, Nate McFadden, and the rest of the team at
Morgan Kaufmann who made this book happen. Jeff Somers at Graphic
World Publishing Services has ably guided the book through production.
Numerous reviewers have substantially improved the book. They
include John Barr (Ithaca College), Jack V. Briner (Charleston Southern
University), Andrew C. Brown (SK Communications), Carl Baumgaertner
(Harvey Mudd College), A. Utku Diril (Nvidia Corporation), Jim Frenzel
(University of Idaho), Jaeha Kim (Rambus, Inc.), Phillip King

xxii PREFACE
(ShotSpotter, Inc.), James Pinter-Lucke (Claremont McKenna College),
Amir Roth, Z. Jerry Shi (University of Connecticut), James E. Stine
(Oklahoma State University), Luke Teyssier, Peiyi Zhao (Chapman
University), and an anonymous reviewer. Simon Moore was a wonderful
host during David’s sabbatical visit to Cambridge University, where
major sections of this book were written.
We also appreciate the students in our course at Harvey Mudd
College who have given us helpful feedback on drafts of this textbook.
Of special note are Casey Schilling, Alice Clifton, Chris Acon, and
Stephen Brawner.
I, David, particularly thank my wife, Jennifer, who gave birth to our
son Abraham at the beginning of the project. I appreciate her patience and
loving support through yet another project at a busy time in our lives.



1
1.1 The Game Plan
1.2 The Art of Managing
Complexity
1.3 The Digital Abstraction
1.4 Number Systems
1.5 Logic Gates
1.6 Beneath the Digital
Abstraction
1.7 CMOS Transistors*
1.8 Power Consumption*
1.9 Summary and a Look
Ahead
Exercises
Interview Questions
From Zero to One
1.1 THE GAME PLAN
Microprocessors have revolutionized our world during the past three
decades. A laptop computer today has far more capability than a
room-sized mainframe of yesteryear. A luxury automobile contains
about 50 microprocessors. Advances in microprocessors have made
cell phones and the Internet possible, have vastly improved medicine,
and have transformed how war is waged. Worldwide semiconductor
industry sales have grown from US $21 billion in 1985 to $227 billion
in 2005, and microprocessors are a major segment of these sales.
We believe that microprocessors are not only technically, economically, and socially important, but are also an intrinsically fascinating
human invention. By the time you finish reading this book, you will
know how to design and build your own microprocessor. The skills
you learn along the way will prepare you to design many other digital
systems.
We assume that you have a basic familiarity with electricity, some
prior programming experience, and a genuine interest in understanding
what goes on under the hood of a computer. This book focuses on the
design of digital systems, which operate on 1’s and 0’s. We begin with
digital logic gates that accept 1’s and 0’s as inputs and produce 1’s and
0’s as outputs. We then explore how to combine logic gates into more
complicated modules such as adders and memories. Then we shift gears
to programming in assembly language, the native tongue of the microprocessor. Finally, we put gates together to build a microprocessor that
runs these assembly language programs.
A great advantage of digital systems is that the building blocks are
quite simple: just 1’s and 0’s. They do not require grungy mathematics or
a profound knowledge of physics. Instead, the designer’s challenge is to
combine these simple blocks into complicated systems. A microprocessor
may be the first system that you build that is too complex to fit in your
3

head all at once. One of the major themes weaved through this book is
how to manage complexity.
1.2 THE ART OF MANAGING COMPLEXITY
One of the characteristics that separates an engineer or computer scientist
from a layperson is a systematic approach to managing complexity.
Modern digital systems are built from millions or billions of transistors.
No human being could understand these systems by writing equations
describing the movement of electrons in each transistor and solving all of
the equations simultaneously. You will need to learn to manage complexity to understand how to build a microprocessor without getting mired in
a morass of detail.
1.2.1 Abstraction
The critical technique for managing complexity is abstraction: hiding
details when they are not important. A system can be viewed from many
different levels of abstraction. For example, American politicians
abstract the world into cities, counties, states, and countries. A county
contains multiple cities and a state contains many counties. When a
politician is running for president, the politician is mostly interested in
how the state as a whole will vote, rather than how each county votes,
so the state is the most useful level of abstraction. On the other hand,
the Census Bureau measures the population of every city, so the agency
must consider the details of a lower level of abstraction.
Figure 1.1 illustrates levels of abstraction for an electronic computer
system along with typical building blocks at each level. At the lowest
level of abstraction is the physics, the motion of electrons. The behavior
of electrons is described by quantum mechanics and Maxwell’s equations. Our system is constructed from electronic devices such as transistors (or vacuum tubes, once upon a time). These devices have
well-defined connection points called terminals and can be modeled by
the relationship between voltage and current as measured at each terminal. By abstracting to this device level, we can ignore the individual electrons. The next level of abstraction is analog circuits, in which devices
are assembled to create components such as amplifiers. Analog circuits
input and output a continuous range of voltages. Digital circuits such as
logic gates restrict the voltages to discrete ranges, which we will use to
indicate 0 and 1. In logic design, we build more complex structures, such
as adders or memories, from digital circuits.
Microarchitecture links the logic and architecture levels of abstraction.
The architecture level of abstraction describes a computer from the programmer’s perspective. For example, the Intel IA-32 architecture used by
microprocessors in most personal computers (PCs) is defined by a set of
4 CHAPTER ONE From Zero to One
Physics
Devices
Analog
Circuits
Digital
Circuits
Logic
Microarchitecture
Architecture
Operating
Systems
Application
Software
Electrons
Transistors
Diodes
Amplifiers
Filters
AND gates
NOT gates
Adders
Memories
Datapaths
Controllers
Instructions
Registers
Device
Drivers
Programs
Figure 1.1 Levels of abstraction
for electronic computing system

instructions and registers (memory for temporarily storing variables) that
the programmer is allowed to use. Microarchitecture involves combining
logic elements to execute the instructions defined by the architecture.
A particular architecture can be implemented by one of many different
microarchitectures with different price/performance/power trade-offs. For
example, the Intel Core 2 Duo, the Intel 80486, and the AMD Athlon all
implement the IA-32 architecture with different microarchitectures.
Moving into the software realm, the operating system handles lowlevel details such as accessing a hard drive or managing memory. Finally,
the application software uses these facilities provided by the operating system to solve a problem for the user. Thanks to the power of abstraction,
your grandmother can surf the Web without any regard for the quantum
vibrations of electrons or the organization of the memory in her computer.
This book focuses on the levels of abstraction from digital circuits
through computer architecture. When you are working at one level of
abstraction, it is good to know something about the levels of abstraction
immediately above and below where you are working. For example, a
computer scientist cannot fully optimize code without understanding the
architecture for which the program is being written. A device engineer
cannot make wise trade-offs in transistor design without understanding
the circuits in which the transistors will be used. We hope that by the
time you finish reading this book, you can pick the level of abstraction
appropriate to solving your problem and evaluate the impact of your
design choices on other levels of abstraction.
1.2.2 Discipline
Discipline is the act of intentionally restricting your design choices so
that you can work more productively at a higher level of abstraction.
Using interchangeable parts is a familiar application of discipline. One of
the first examples of interchangeable parts was in flintlock rifle manufacturing. Until the early 19th century, rifles were individually crafted by
hand. Components purchased from many different craftsmen were carefully filed and fit together by a highly skilled gunmaker. The discipline of
interchangeable parts revolutionized the industry. By limiting the components to a standardized set with well-defined tolerances, rifles could be
assembled and repaired much faster and with less skill. The gunmaker
no longer concerned himself with lower levels of abstraction such as the
specific shape of an individual barrel or gunstock.
In the context of this book, the digital discipline will be very important. Digital circuits use discrete voltages, whereas analog circuits use continuous voltages. Therefore, digital circuits are a subset of analog circuits
and in some sense must be capable of less than the broader class of analog
circuits. However, digital circuits are much simpler to design. By limiting
1.2 The Art of Managing Complexity
5

ourselves to digital circuits, we can easily combine components into
sophisticated systems that ultimately outperform those built from analog
components in many applications. For example, digital televisions, compact disks (CDs), and cell phones are replacing their analog predecessors.
1.2.3 The Three -Y’s
In addition to abstraction and discipline, designers use the three “-y’s” to
manage complexity: hierarchy, modularity, and regularity. These principles apply to both software and hardware systems.
 Hierarchy involves dividing a system into modules, then further subdividing each of these modules until the pieces are easy to understand.
 Modularity states that the modules have well-defined functions and
interfaces, so that they connect together easily without unanticipated
side effects.
 Regularity seeks uniformity among the modules. Common modules
are reused many times, reducing the number of distinct modules that
must be designed.
To illustrate these “-y’s” we return to the example of rifle manufacturing. A flintlock rifle was one of the most intricate objects in common
use in the early 19th century. Using the principle of hierarchy, we can
break it into components shown in Figure 1.2: the lock, stock, and barrel.
The barrel is the long metal tube through which the bullet is fired.
The lock is the firing mechanism. And the stock is the wooden body that
holds the parts together and provides a secure grip for the user. In turn,
the lock contains the trigger, hammer, flint, frizzen, and pan. Each of
these components could be hierarchically described in further detail.
Modularity teaches that each component should have a well-defined
function and interface. A function of the stock is to mount the barrel
and lock. Its interface consists of its length and the location of its mounting pins. In a modular rifle design, stocks from many different manufacturers can be used with a particular barrel as long as the stock and barrel
are of the correct length and have the proper mounting mechanism. A
function of the barrel is to impart spin to the bullet so that it travels
more accurately. Modularity dictates that there should be no side effects:
the design of the stock should not impede the function of the barrel.
Regularity teaches that interchangeable parts are a good idea. With
regularity, a damaged barrel can be replaced by an identical part. The
barrels can be efficiently built on an assembly line, instead of being
painstakingly hand-crafted.
We will return to these principles of hierarchy, modularity, and regularity throughout the book.
6 CHAPTER ONE From Zero to One
Captain Meriwether Lewis of
the Lewis and Clark
Expedition was one of the
early advocates of interchangeable parts for rifles. In
1806, he explained:
The guns of Drewyer and
Sergt. Pryor were both out
of order. The first was
repared with a new lock, the
old one having become unfit
for use; the second had the
cock screw broken which
was replaced by a duplicate
which had been prepared for
the lock at Harpers Ferry
where she was manufactured. But for the precaution
taken in bringing on those
extra locks, and parts of
locks, in addition to the
ingenuity of John Shields,
most of our guns would at
this moment been entirely
unfit for use; but fortunately
for us I have it in my power
here to record that they are
all in good order.
See Elliott Coues, ed., The
History of the Lewis and
Clark Expedition... (4 vols),
New York: Harper, 1893;
reprint, 3 vols, New York:
Dover, 3:817.
Cha
1.3 THE DIGITAL ABSTRACTION
Most physical variables are continuous. For example, the voltage on a
wire, the frequency of an oscillation, or the position of a mass are all
continuous quantities. Digital systems, on the other hand, represent
information with discrete-valued variables—that is, variables with a
finite number of distinct values.
An early digital system using variables with ten discrete values was
Charles Babbage’s Analytical Engine. Babbage labored from 1834 to
1871,1 designing and attempting to build this mechanical computer. The
Analytical Engine used gears with ten positions labeled 0 through 9,
much like a mechanical odometer in a car. Figure 1.3 shows a prototype
1.3 The Digital Abstraction 7
Barrel
Stoc
Lock
Expanded view of Lock
k
Flint
Cock
Pan
Spring
String
Figure 1.2 Flintlock rifle with
a close-up view of the lock
(Image by Euroams Italia.
www.euroarms.net © 2006).
1 And we thought graduate school was long!
Charles Babbage, 1791–1871.
Attended Cambridge University and married Georgiana
Whitmore in 1814. Invented
the Analytical Engine, the
world’s first mechanical
computer. Also invented the
cowcatcher and the universal
postage rate. Interested in
lock-picking, but abhorred
street musicians (image
courtesy of Fourmilab
Switzerland, www.
fourmilab.ch).

of the Analytical Engine, in which each row processes one digit. Babbage
chose 25 rows of gears, so the machine has 25-digit precision.
Unlike Babbage’s machine, most electronic computers use a binary
(two-valued) representation in which a high voltage indicates a ‘1’ and a
low voltage indicates a ‘0,’ because it is easier to distinguish between
two voltages than ten.
The amount of information D in a discrete valued variable with N
distinct states is measured in units of bits as
D  log2N bits (1.1)
A binary variable conveys log22  1 bit of information. Indeed, the
word bit is short for binary digit. Each of Babbage’s gears carried log210
3.322 bits of information because it could be in one of 23.322  10 unique
positions. A continuous signal theoretically contains an infinite amount of
information because it can take on an infinite number of values. In practice, noise and measurement error limit the information to only 10 to 16
bits for most continuous signals. If the measurement must be made rapidly,
the information content is lower (e.g., 8 bits).
This book focuses on digital circuits using binary variables: 1’s and
0’s. George Boole developed a system of logic operating on binary variables that is now known as Boolean logic. Each of Boole’s variables
could be TRUE or FALSE. Electronic computers commonly use a positive voltage to represent ‘1’ and zero volts to represent ‘0’. In this book,
we will use the terms ‘1,’ TRUE, and HIGH synonymously. Similarly, we
will use ‘0,’ FALSE, and LOW interchangeably.
The beauty of the digital abstraction is that digital designers can
focus on 1’s and 0’s, ignoring whether the Boolean variables are physically represented with specific voltages, rotating gears, or even hydraulic
8 CHAPTER ONE From Zero to One
Figure 1.3 Babbage’s Analytical
Engine, under construction at
the time of his death in 1871
(image courtesy of Science
Museum/Science and Society
Picture Library).
George Boole, 1815–1864. Born
to working-class parents and
unable to afford a formal education, Boole taught himself
mathematics and joined the
faculty of Queen’s College in
Ireland. He wrote An
Investigation of the Laws of
Thought (1854), which introduced binary variables and the
three fundamental logic operations: AND, OR, and NOT
(image courtesy of xxx).
Chap
fluid levels. A computer programmer can work without needing to know
the intimate details of the computer hardware. On the other hand,
understanding the details of the hardware allows the programmer to
optimize the software better for that specific computer.
An individual bit doesn’t carry much information. In the next section, we examine how groups of bits can be used to represent numbers.
In later chapters, we will also use groups of bits to represent letters and
programs.
1.4 NUMBER SYSTEMS
You are accustomed to working with decimal numbers. In digital systems consisting of 1’s and 0’s, binary or hexadecimal numbers are often
more convenient. This section introduces the various number systems
that will be used throughout the rest of the book.
1.4.1 Decimal Numbers
In elementary school, you learned to count and do arithmetic in decimal.
Just as you (probably) have ten fingers, there are ten decimal digits, 0, 1,
2, ..., 9. Decimal digits are joined together to form longer decimal numbers. Each column of a decimal number has ten times the weight of the
previous column. From right to left, the column weights are 1, 10, 100,
1000, and so on. Decimal numbers are referred to as base 10. The base
is indicated by a subscript after the number to prevent confusion when
working in more than one base. For example, Figure 1.4 shows how the
decimal number 974210 is written as the sum of each of its digits multiplied by the weight of the corresponding column.
An N-digit decimal number represents one of 10N possibilities: 0, 1,
2, 3, ..., 10N1. This is called the range of the number. For example, a
three-digit decimal number represents one of 1000 possibilities in the
range of 0 to 999.
1.4.2 Binary Numbers
Bits represent one of two values, 0 or 1, and are joined together to form
binary numbers. Each column of a binary number has twice the weight
1.4 Number Systems 9
974210 = 9 × 103 + 7 × 102 + 4 × 101 + 2 × 100
nine
thousands
1000's column
100's column
10's column
seven
hundreds
four
tens
two
ones
1's column
Figure 1.4 Representation
of a decimal number
C
of the previous column, so binary numbers are base 2. In binary, the column weights (again from right to left) are 1, 2, 4, 8, 16, 32, 64, 128,
256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, and so on. If
you work with binary numbers often, you’ll save time if you remember
these powers of two up to 216.
An N-bit binary number represents one of 2N possibilities: 0, 1, 2,
3, ..., 2N1. Table 1.1 shows 1, 2, 3, and 4-bit binary numbers and their
decimal equivalents.
Example 1.1 BINARY TO DECIMAL CONVERSION
Convert the binary number 101102 to decimal.
Solution: Figure 1.5 shows the conversion.
10 CHAPTER ONE From Zero to One
1-Bit 2-Bit 3-Bit 4-Bit
Binary Binnary Binary Binary Decimal
Numbers Numbers Numbers Numbers Equivalents
0 00 000 0000 0
1 01 001 0001 1
10 010 0010 2
11 011 0011 3
100 0100 4
101 0101 5
110 0110 6
111 0111 7
1000 8
1001 9
1010 10
1011 11
1100 12
1101 13
1110 14
1111 15
Table 1.1 Binary numbers and their decimal equivalent
C
Example 1.2 DECIMAL TO BINARY CONVERSION
Convert the decimal number 8410 to binary.
Solution: Determine whether each column of the binary result has a 1 or a 0. We
can do this starting at either the left or the right column.
Working from the left, start with the largest power of 2 less than the number (in
this case, 64). 84  64, so there is a 1 in the 64’s column, leaving 84  64  20.
20  32, so there is a 0 in the 32’s column. 20  16, so there is a 1 in the 16’s
column, leaving 20  16  4. 4  8, so there is a 0 in the 8’s column. 4  4, so
there is a 1 in the 4’s column, leaving 4  4  0. Thus there must be 0’s in the
2’s and 1’s column. Putting this all together, 8410  10101002.
Working from the right, repeatedly divide the number by 2. The remainder goes
in each column. 84/2  42, so 0 goes in the 1’s column. 42/2  21, so 0 goes in
the 2’s column. 21/2  10 with a remainder of 1 going in the 4’s column. 10/2
5, so 0 goes in the 8’s column. 5/2  2 with a remainder of 1 going in the 16’s
column. 2/2  1, so 0 goes in the 32’s column. Finally 1/2  0 with a remainder
of 1 going in the 64’s column. Again, 8410  10101002
1.4.3 Hexadecimal Numbers
Writing long binary numbers becomes tedious and prone to error.
A group of four bits represents one of 24  16 possibilities. Hence, it is
sometimes more convenient to work in base 16, called hexadecimal.
Hexadecimal numbers use the digits 0 to 9 along with the letters A to F,
as shown in Table 1.2. Columns in base 16 have weights of 1, 16, 162
(or 256), 163 (or 4096), and so on.
Example 1.3 HEXADECIMAL TO BINARY AND DECIMAL CONVERSION
Convert the hexadecimal number 2ED16 to binary and to decimal.
Solution: Conversion between hexadecimal and binary is easy because each hexadecimal digit directly corresponds to four binary digits. 216  00102, E16  11102
and D16  11012, so 2ED16  0010111011012. Conversion to decimal requires
the arithmetic shown in Figure 1.6.
1.4 Number Systems 11
101102 = 1 × 24 + 0 × 23 + 1 × 22 + 1 × 21
+ 0 × 20 = 2210
one
sixteen
1's column
no
eight one
four one
two no
one
16's column
8's column
4's column
2's column
Figure 1.5 Conversion of a
binary number to decimal
“Hexadecimal,” a term
coined by IBM in 1963,
derives from the Greek hexi
(six) and Latin decem (ten). A
more proper term would use
the Latin sexa (six), but sexidecimal sounded too risqué.
Chapter 01.qxd 1/27/
Example 1.4 BINARY TO HEXADECIMAL CONVERSION
Convert the binary number 11110102 to hexadecimal.
Solution: Again, conversion is easy. Start reading from the right. The four least
significant bits are 10102  A16. The next bits are 1112  716. Hence 11110102
 7A16.
12 CHAPTER ONE From Zero to One
Table 1.2 Hexadecimal number system
Hexadecimal Digit Decimal Equivalent Binary Equivalent
0 0 0000
1 1 0001
2 2 0010
3 3 0011
4 4 0100
5 5 0101
6 6 0110
7 7 0111
8 8 1000
9 9 1001
A 10 1010
B 11 1011
C 12 1100
D 13 1101
E 14 1110
F 15 1111
2ED16 = 2 × 162 + E × 161 + D × 160 = 74910
two
two hundred
fifty six's
1's column
fourteen
sixteens
thirteen
ones
256's column
16's column
Figure 1.6 Conversion of
hexadecimal number to decimal
Cha
Example 1.5 DECIMAL TO HEXADECIMAL AND BINARY CONVERSION
Convert the decimal number 33310 to hexadecimal and binary.
Solution: Like decimal to binary conversion, decimal to hexadecimal conversion
can be done from the left or the right.
Working from the left, start with the largest power of 16 less than the number (in
this case, 256). 256 goes into 333 once, so there is a 1 in the 256’s column, leaving
333  256  77. 16 goes into 77 four times, so there is a 4 in the 16’s column,
leaving 77  16  4  13. 1310  D16, so there is a D in the 1’s column. In summary, 33310  14D16. Now it is easy to convert from hexadecimal to binary, as in
Example 1.3. 14D16  1010011012.
Working from the right, repeatedly divide the number by 16. The remainder
goes in each column. 333/16  20 with a remainder of 1310  D16 going in
the 1’s column. 20/16  1 with a remainder of 4 going in the 16’s column.
1/16  0 with a remainder of 1 going in the 256’s column. Again, the result
is 14D16.
1.4.4 Bytes, Nibbles, and All That Jazz
A group of eight bits is called a byte. It represents one of 28  256 possibilities. The size of objects stored in computer memories is customarily
measured in bytes rather than bits.
A group of four bits, or half a byte, is called a nibble. It represents
one of 24  16 possibilities. One hexadecimal digit stores one nibble and
two hexadecimal digits store one full byte. Nibbles are no longer a commonly used unit, but the term is cute.
Microprocessors handle data in chunks called words. The size of a
word depends on the architecture of the microprocessor. When this
chapter was written in 2006, most computers had 32-bit processors,
indicating that they operate on 32-bit words. At the time, computers
handling 64-bit words were on the verge of becoming widely available.
Simpler microprocessors, especially those used in gadgets such as toasters, use 8- or 16-bit words.
Within a group of bits, the bit in the 1’s column is called the least
significant bit (lsb), and the bit at the other end is called the most
significant bit (msb), as shown in Figure 1.7(a) for a 6-bit binary
number. Similarly, within a word, the bytes are identified as least
significant byte (LSB) through most significant byte (MSB), as shown
in Figure 1.7(b) for a four-byte number written with eight hexadecimal digits.
1.4 Number Systems 13
A microprocessor is a processor built on a single chip.
Until the 1970’s, processors
were too complicated to fit on
one chip, so mainframe
processors were built from
boards containing many chips.
Intel introduced the first 4-bit
microprocessor, called the
4004, in 1971. Now, even the
most sophisticated supercomputers are built using microprocessors. We will use the
terms microprocessor and
processor interchangeably
throughout this book.
Chapter 01.qx
By handy coincidence, 210  1024  103. Hence, the term kilo
(Greek for thousand) indicates 210. For example, 210 bytes is one kilobyte (1 KB). Similarly, mega (million) indicates 220  106, and giga
(billion) indicates 230  109. If you know 210  1 thousand, 220  1
million, 230  1 billion, and remember the powers of two up to 29, it is
easy to estimate any power of two in your head.
Example 1.6 ESTIMATING POWERS OF TWO
Find the approximate value of 224 without using a calculator.
Solution: Split the exponent into a multiple of ten and the remainder. 224  220
 24. 220  1 million. 24  16. So 224  16 million. Technically, 224
16,777,216, but 16 million is close enough for marketing purposes.
1024 bytes is called a kilobyte (KB). 1024 bits is called a kilobit (Kb
or Kbit). Similarly, MB, Mb, GB, and Gb are used for millions and billions of bytes and bits. Memory capacity is usually measured in bytes.
Communication speed is usually measured in bits/sec. For example, the
maximum speed of a dial-up modem is usually 56 Kbits/sec.
1.4.5 Binary Addition
Binary addition is much like decimal addition, but easier, as shown in
Figure 1.8. As in decimal addition, if the sum of two numbers is greater
than what fits in a single digit, we carry a 1 into the next column. Figure
1.8 compares addition of decimal and binary numbers. In the right-most
column of Figure 1.8(a), 7  9  16, which cannot fit in a single digit
because it is greater than 9. So we record the 1’s digit, 6, and carry the
10’s digit, 1, over to the next column. Likewise, in binary, if the sum of
two numbers is greater than 1, we carry the 2’s digit over to the next column. For example, in the right-most column of Figure 1.8(b), the sum
14 CHAPTER ONE From Zero to One
101100
least
significant
bit
most
significant
bit
(a) (b)
DEAFDAD8
least
significant
byte
most
significant
byte
1011
+ 0011
1110
carries 11
4277
+ 5499
9776
11
(a) (b)
Figure 1.7 Least and most
significant bits and bytes
Figure 1.8 Addition examples
showing carries: (a) decimal
(b) binary
Chapter 01.qx
1  1  210  102 cannot fit in a single binary digit. So we record the
1’s digit (0) and carry the 2’s digit (1) of the result to the next column. In
the second column, the sum is 1  1  1  310  112. Again, we record
the 1’s digit (1) and carry the 2’s digit (1) to the next column. For obvious reasons, the bit that is carried over to the neighboring column is
called the carry bit.
Example 1.7 BINARY ADDITION
Compute 01112  01012.
Solution: Figure 1.9 shows that the sum is 11002. The carries are indicated in
blue. We can check our work by repeating the computation in decimal. 01112
710. 01012  510. The sum is 1210  11002.
Digital systems usually operate on a fixed number of digits.
Addition is said to overflow if the result is too big to fit in the available
digits. A 4-bit number, for example, has the range [0, 15]. 4-bit binary
addition overflows if the result exceeds 15. The fifth bit is discarded,
producing an incorrect result in the remaining four bits. Overflow can be
detected by checking for a carry out of the most significant column.
Example 1.8 ADDITION WITH OVERFLOW
Compute 11012  01012. Does overflow occur?
Solution: Figure 1.10 shows the sum is 100102. This result overflows the range
of a 4-bit binary number. If it must be stored as four bits, the most significant bit
is discarded, leaving the incorrect result of 00102. If the computation had been
done using numbers with five or more bits, the result 100102 would have been
correct.
1.4.6 Signed Binary Numbers
So far, we have considered only unsigned binary numbers that represent
positive quantities. We will often want to represent both positive and
negative numbers, requiring a different binary number system. Several
schemes exist to represent signed binary numbers; the two most widely
employed are called sign/magnitude and two’s complement.
Sign/Magnitude Numbers
Sign/magnitude numbers are intuitively appealing because they match our
custom of writing negative numbers with a minus sign followed by the
magnitude. An N-bit sign/magnitude number uses the most significant bit
1.4 Number Systems 15
0111
+ 0101
1100
111
1101
+ 0101
10010
11 1
Figure 1.9 Binary addition
example
Figure 1.10 Binary addition
example with overflow
Chapter
as the sign and the remaining N  1 bits as the magnitude (absolute
value). A sign bit of 0 indicates positive and a sign bit of 1 indicates
negative.
Example 1.9 SIGN/MAGNITUDE NUMBERS
Write 5 and 5 as 4-bit sign/magnitude numbers
Solution: Both numbers have a magnitude of 510  1012. Thus, 510  01012 and
510  11012.
Unfortunately, ordinary binary addition does not work for sign/magnitude numbers. For example, using ordinary addition on 510  510
gives 11012  01012  100102, which is nonsense.
An N-bit sign/magnitude number spans the range [2N1  1, 2N1
 1]. Sign/magnitude numbers are slightly odd in that both 0 and 0
exist. Both indicate zero. As you may expect, it can be troublesome to
have two different representations for the same number.
Two’s Complement Numbers
Two’s complement numbers are identical to unsigned binary numbers
except that the most significant bit position has a weight of 2N1
instead of 2N1. They overcome the shortcomings of sign/magnitude
numbers: zero has a single representation, and ordinary addition works.
In two’s complement representation, zero is written as all zeros:
00...0002. The most positive number has a 0 in the most significant position and 1’s elsewhere: 01...1112  2N1  1. The most negative number
has a 1 in the most significant position and 0’s elsewhere: 10...0002
2N1. And 1 is written as all ones: 11...1112.
Notice that positive numbers have a 0 in the most significant position and negative numbers have a 1 in this position, so the most significant bit can be viewed as the sign bit. However, the remaining bits are
interpreted differently for two’s complement numbers than for sign/
magnitude numbers.
The sign of a two’s complement number is reversed in a process
called taking the two’s complement. The process consists of inverting all
of the bits in the number, then adding 1 to the least significant bit
position. This is useful to find the representation of a negative number
or to determine the magnitude of a negative number.
Example 1.10 TWO’S COMPLEMENT REPRESENTATION
OF A NEGATIVE NUMBER
Find the representation of 210 as a 4-bit two’s complement number.
16 CHAPTER ONE From Zero to One
The $7 billion Ariane 5
rocket, launched on June 4,
1996, veered off course 40
seconds after launch, broke
up, and exploded. The failure
was caused when the computer controlling the rocket
overflowed its 16-bit range
and crashed.
The code had been extensively tested on the Ariane 4
rocket. However, the Ariane 5
had a faster engine that produced larger values for the
control computer, leading to
the overflow.
(Photograph courtesy
ESA/CNES/ ARIANESPACEService Optique CS6.)
Chapter 01.qxd 1/27/07 1
Solution: Start with  210  00102. To get 210, invert the bits and add 1.
Inverting 00102 produces 11012. 11012  1  11102. So 210 is 11102.
Example 1.11 VALUE OF NEGATIVE TWO’S COMPLEMENT NUMBERS
Find the decimal value of the two’s complement number 10012.
Solution: 10012 has a leading 1, so it must be negative. To find its magnitude,
invert the bits and add 1. Inverting 10012  01102. 01102  1  01112  710.
Hence, 10012  710.
Two’s complement numbers have the compelling advantage that
addition works properly for both positive and negative numbers. Recall
that when adding N-bit numbers, the carry out of the Nth bit (i.e., the
N  1th result bit), is discarded.
Example 1.12 ADDING TWO’S COMPLEMENT NUMBERS
Compute (a) 210  110 and (b) 710  710 using two’s complement numbers.
Solution: (a) 210  110  11102  00012  11112  110. (b) 710  710
 10012  01112  100002. The fifth bit is discarded, leaving the correct 4-bit
result 00002.
Subtraction is performed by taking the two’s complement of the second number, then adding.
Example 1.13 SUBTRACTING TWO’S COMPLEMENT NUMBERS
Compute (a) 510  310 and (b) 310  510 using 4-bit two’s complement numbers.
Solution: (a) 310  00112. Take its two’s complement to obtain 310  11012.
Now add 510  (310)  01012  11012  00102  210. Note that the carry
out of the most significant position is discarded because the result is stored in
four bits. (b) Take the two’s complement of 510 to obtain 510  1011. Now
add 310  (510)  00112  10112  11102  210.
The two’s complement of 0 is found by inverting all the bits (producing 11...1112) and adding 1, which produces all 0’s, disregarding the
carry out of the most significant bit position. Hence, zero is always
represented with all 0’s. Unlike the sign/magnitude system, the two’s
complement system has no separate 0. Zero is considered positive
because its sign bit is 0.
1.4 Number Systems 17
Chapter 01.qxd 1/27/07 10:20 AM Page
Like unsigned numbers, N-bit two’s complement numbers represent one of 2N possible values. However the values are split between
positive and negative numbers. For example, a 4-bit unsigned number
represents 16 values: 0 to 15. A 4-bit two’s complement number also
represents 16 values: 8 to 7. In general, the range of an N-bit two’s
complement number spans [2N1, 2N1  1]. It should make sense
that there is one more negative number than positive number because
there is no 0. The most negative number 10...0002  2N1 is
sometimes called the weird number. Its two’s complement is found by
inverting the bits (producing 01...1112 and adding 1, which produces
10...0002, the weird number, again). Hence, this negative number has
no positive counterpart.
Adding two N-bit positive numbers or negative numbers may
cause overflow if the result is greater than 2N1  1 or less than
2N1. Adding a positive number to a negative number never causes
overflow. Unlike unsigned numbers, a carry out of the most significant
column does not indicate overflow. Instead, overflow occurs if the two
numbers being added have the same sign bit and the result has the
opposite sign bit.
Example 1.14 ADDING TWO’S COMPLEMENT NUMBERS
WITH OVERFLOW
Compute (a) 410  510 using 4-bit two’s complement numbers. Does the result
overflow?
Solution: (a) 410  510  01002  01012  10012  710. The result overflows
the range of 4-bit positive two’s complement numbers, producing an incorrect
negative result. If the computation had been done using five or more bits, the
result 010012  910 would have been correct.
When a two’s complement number is extended to more bits, the sign
bit must be copied into the most significant bit positions. This process is
called sign extension. For example, the numbers 3 and 3 are written as
4-bit two’s complement numbers 0011 and 1101, respectively. They are
sign-extended to seven bits by copying the sign bit into the three new
upper bits to form 0000011 and 1111101, respectively.
Comparison of Number Systems
The three most commonly used binary number systems are unsigned,
two’s complement, and sign/magnitude. Table 1.3 compares the range of
N-bit numbers in each of these three systems. Two’s complement numbers are convenient because they represent both positive and negative
integers and because ordinary addition works for all numbers.
18 CHAPTER ONE From Zero to One
Chapter 01.qxd 1/27
Subtraction is performed by negating the second number (i.e., taking the
two’s complement), and then adding. Unless stated otherwise, assume
that all signed binary numbers use two’s complement representation.
Figure 1.11 shows a number line indicating the values of 4-bit numbers in each system. Unsigned numbers span the range [0, 15] in regular
binary order. Two’s complement numbers span the range [8, 7]. The
nonnegative numbers [0, 7] share the same encodings as unsigned numbers. The negative numbers [8, 1] are encoded such that a larger
unsigned binary value represents a number closer to 0. Notice that the
weird number, 1000, represents 8 and has no positive counterpart.
Sign/magnitude numbers span the range [7, 7]. The most significant bit
is the sign bit. The positive numbers [1, 7] share the same encodings as
unsigned numbers. The negative numbers are symmetric but have the
sign bit set. 0 is represented by both 0000 and 1000. Thus, N-bit
sign/magnitude numbers represent only 2N  1 integers because of the
two representations for 0.
1.5 LOGIC GATES
Now that we know how to use binary variables to represent information, we explore digital systems that perform operations on these binary
variables. Logic gates are simple digital circuits that take one or more
binary inputs and produce a binary output. Logic gates are drawn with
a symbol showing the input (or inputs) and the output. Inputs are
1.5 Logic Gates 19
–8
1000 1001
–7 –6 –5 –4 –3 –2 –1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
1010 1011 1100 1101 1110 1111 0000 0001 0010 0011 0100 0101 0110 0111 Two's Complement
1000 1111 1110 1101 1100 1011 1010 1001 0000 0001 0010 0011 0100 0101 0110 0111
0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111
Sign/Magnitude
Unsigned
Table 1.3 Range of N-bit numbers
System Range
Unsigned [0, 2N  1]
Sign/Magnitude [2N1  1, 2N1  1]
Two’s Complement [2N1, 2N1  1]
Figure 1.11 Number line and 4-bit binary encodings
Chapter 01.qxd 
usually drawn on the left (or top) and outputs on the right (or bottom).
Digital designers typically use letters near the beginning of the alphabet
for gate inputs and the letter Y for the gate output. The relationship
between the inputs and the output can be described with a truth table or
a Boolean equation. A truth table lists inputs on the left and the corresponding output on the right. It has one row for each possible combination of inputs. A Boolean equation is a mathematical expression using
binary variables.
1.5.1 NOT Gate
A NOT gate has one input, A, and one output, Y, as shown in Figure
1.12. The NOT gate’s output is the inverse of its input. If A is FALSE,
then Y is TRUE. If A is TRUE, then Y is FALSE. This relationship is
summarized by the truth table and Boolean equation in the figure. The
line over A in the Boolean equation is pronounced NOT, so Y  A
_
is
read “Y equals NOT A.” The NOT gate is also called an inverter.
Other texts use a variety of notations for NOT, including Y  A, Y
A, Y  !A or Y  ~A. We will use Y  A
_
exclusively, but don’t be puzzled if you encounter another notation elsewhere.
1.5.2 Buffer
The other one-input logic gate is called a buffer and is shown in Figure
1.13. It simply copies the input to the output.
From the logical point of view, a buffer is no different from a wire,
so it might seem useless. However, from the analog point of view, the
buffer might have desirable characteristics such as the ability to deliver
large amounts of current to a motor or the ability to quickly send its
output to many gates. This is an example of why we need to consider
multiple levels of abstraction to fully understand a system; the digital
abstraction hides the real purpose of a buffer.
The triangle symbol indicates a buffer. A circle on the output is
called a bubble and indicates inversion, as was seen in the NOT gate
symbol of Figure 1.12.
1.5.3 AND Gate
Two-input logic gates are more interesting. The AND gate shown in Figure
1.14 produces a TRUE output, Y, if and only if both A and B are TRUE.
Otherwise, the output is FALSE. By convention, the inputs are listed in the
order 00, 01, 10, 11, as if you were counting in binary. The Boolean equation for an AND gate can be written in several ways: Y  A • B, Y  AB, or
Y  A  B. The  symbol is pronounced “intersection” and is preferred by
logicians. We prefer Y  AB, read “Y equals A and B,” because we are lazy.
20 CHAPTER ONE From Zero to One
NOT
Y = A
A Y
0 1
1 0
A Y
BUF
Y = A
A Y
0 0
1 1
A Y
AND
Y = AB
ABY
000
010
100
111
A
B Y
Figure 1.12 NOT gate
Figure 1.14 AND gate
Figure 1.13 Buffer
According to Larry Wall,
inventor of the Perl programming language, “the three
principal virtues of a programmer are Laziness,
Impatience, and Hubris.”
Chapter 01.q
1.5.4 OR Gate
The OR gate shown in Figure 1.15 produces a TRUE output, Y, if either
A or B (or both) are TRUE. The Boolean equation for an OR gate is
written as Y  A  B or Y  A  B. The  symbol is pronounced
union and is preferred by logicians. Digital designers normally use the 
notation, Y  A  B is pronounced “Y equals A or B”.
1.5.5 Other Two-Input Gates
Figure 1.16 shows other common two-input logic gates. XOR (exclusive
OR, pronounced “ex-OR”) is TRUE if A or B, but not both, are TRUE.
Any gate can be followed by a bubble to invert its operation. The
NAND gate performs NOT AND. Its output is TRUE unless both inputs
are TRUE. The NOR gate performs NOT OR. Its output is TRUE if
neither A nor B is TRUE.
Example 1.15 XNOR GATE
Figure 1.17 shows the symbol and Boolean equation for a two-input XNOR
gate that performs the inverse of an XOR. Complete the truth table.
Solution: Figure 1.18 shows the truth table. The XNOR output is TRUE if both
inputs are FALSE or both inputs are TRUE. The two-input XNOR gate is sometimes called an equality gate because its output is TRUE when the inputs are equal.
1.5.6 Multiple-Input Gates
Many Boolean functions of three or more inputs exist. The most common
are AND, OR, XOR, NAND, NOR, and XNOR. An N-input AND
gate produces a TRUE output when all N inputs are TRUE. An N-input
OR gate produces a TRUE output when at least one input is TRUE.
1.5 Logic Gates 21
OR
Y = A + B
ABY
000
011
101
111
A
B Y
Figure 1.15 OR gate
XOR NAND NOR
Y =A + B Y =AB Y=A+B
ABY
000
011
101
110
ABY
001
011
101
110
ABY
001
010
100
110
A
B Y A
B Y A
B Y
Figure 1.16 More two-input logic gates
A silly way to remember the
OR symbol is that it’s input
side is curved like Pacman’s
mouth, so the gate is hungry
and willing to eat any TRUE
inputs it can find!
Figure 1.17 XNOR gate
XNOR
Y =A + B
ABY
0 0
0 1
1 0
1 1
A
B Y
Chapt
An N-input XOR gate is sometimes called a parity gate and produces a
TRUE output if an odd number of inputs are TRUE. As with two-input
gates, the input combinations in the truth table are listed in counting order.
Example 1.16 THREE-INPUT NOR GATE
Figure 1.19 shows the symbol and Boolean equation for a three-input NOR gate.
Complete the truth table.
Solution: Figure 1.20 shows the truth table. The output is TRUE only if none of
the inputs are TRUE.
Example 1.17 FOUR-INPUT AND GATE
Figure 1.21 shows the symbol and Boolean equation for a four-input AND gate.
Create a truth table.
Solution: Figure 1.22 shows the truth table. The output is TRUE only if all of the
inputs are TRUE.
1.6 BENEATH THE DIGITAL ABSTRACTION
A digital system uses discrete-valued variables. However, the variables are
represented by continuous physical quantities such as the voltage on a wire,
the position of a gear, or the level of fluid in a cylinder. Hence, the designer
must choose a way to relate the continuous value to the discrete value.
For example, consider representing a binary signal A with a voltage on
a wire. Let 0 volts (V) indicate A  0 and 5 V indicate A  1. Any real system must tolerate some noise, so 4.97 V probably ought to be interpreted
as A  1 as well. But what about 4.3 V? Or 2.8 V? Or 2.500000 V?
1.6.1 Supply Voltage
Suppose the lowest voltage in the system is 0 V, also called ground or
GND. The highest voltage in the system comes from the power supply
and is usually called VDD. In 1970’s and 1980’s technology, VDD was
generally 5 V. As chips have progressed to smaller transistors, VDD has
dropped to 3.3 V, 2.5 V, 1.8 V, 1.5 V, 1.2 V, or even lower to save power
and avoid overloading the transistors.
1.6.2 Logic Levels
The mapping of a continuous variable onto a discrete binary variable is
done by defining logic levels, as shown in Figure 1.23. The first gate is
called the driver and the second gate is called the receiver. The output of
22 CHAPTER ONE From Zero to One
Figure 1.19 Three-input NOR
gate
Figure 1.20 Three-input NOR
truth table
Figure 1.21 Four-input AND gate
NOR3
Y = A + B + C
BCY
0 0
0 1
1 0
1 1
A
B Y
C
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
BCY
001
010
100
110
A
0
0
0
0
000
010
100
110
1
1
1
1
AND4
Y = ABCD
A
B
C Y
D
Figure 1.18 XNOR truth table
ABY
0 0
0 1
1 0
1 1
1
0
0
1
Cha
the driver is connected to the input of the receiver. The driver produces a
LOW (0) output in the range of 0 to VOL or a HIGH (1) output in the
range of VOH to VDD. If the receiver gets an input in the range of 0 to
VIL, it will consider the input to be LOW. If the receiver gets an input in
the range of VIH to VDD, it will consider the input to be HIGH. If, for
some reason such as noise or faulty components, the receiver’s input
should fall in the forbidden zone between VIL and VIH, the behavior of
the gate is unpredictable. VOH, VOL, VIH, and VIL are called the output
and input high and low logic levels.
1.6.3 Noise Margins
If the output of the driver is to be correctly interpreted at the input of
the receiver, we must choose VOL  VIL and VOH 	 VIH. Thus, even if
the output of the driver is contaminated by some noise, the input of the
receiver will still detect the correct logic level. The noise margin is the
amount of noise that could be added to a worst-case output such that
the signal can still be interpreted as a valid input. As can be seen in
Figure 1.23, the low and high noise margins are, respectively
NML  VIL  VOL (1.2)
NMH  VOH  VIH (1.3)
Example 1.18
Consider the inverter circuit of Figure 1.24. VO1 is the output voltage of inverter
I1, and VI2 is the input voltage of inverter I2. Both inverters have the following
characteristics: VDD  5 V, VIL  1.35 V, VIH  3.15 V, VOL  0.33 V, and
VOH  3.84 V. What are the inverter low and high noise margins? Can the circuit tolerate 1 V of noise between VO1 and VI2?
Solution: The inverter noise margins are: NML  VIL  VOL  (1.35 V  0.33 V)
 1.02 V, NMH  VOH  VIH  (3.84 V  3.15 V)  0.69 V. The circuit can tolerate 1 V of noise when the output is LOW (NML  1.02 V) but not when the
output is HIGH (NMH  0.69 V). For example, suppose the driver, I1, outputs its
worst-case HIGH value, VO1  VOH  3.84 V. If noise causes the voltage to
droop by 1 V before reaching the input of the receiver, VI2  (3.84 V  1 V)
2.84 V. This is less than the acceptable input HIGH value, VIH  3.15 V, so the
receiver may not sense a proper HIGH input.
1.6.4 DC Transfer Characteristics
To understand the limits of the digital abstraction, we must delve into
the analog behavior of a gate. The DC transfer characteristics of a gate
describe the output voltage as a function of the input voltage when the
1.6 Beneath the Digital Abstraction 23
Figure 1.22 Four-input AND
truth table
BDY
000
010
100
110
C
0
0
0
0
000
010
100
110
1
1
1
1
A
000
010
100
110
0
0
0
0
000
010
100
111
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
VDD stands for the voltage on
the drain of a metal-oxidesemiconductor transistor, used
to build most modern chips.
The power supply voltage is
also sometimes called VCC,
standing for the voltage on
the collector of a bipolar transistor used to build chips in
an older technology. Ground
is sometimes called VSS
because it is the voltage on
the source of a metal-oxidesemiconductor transistor. See
Section 1.7 for more information on transistors.
Chapter 01.qxd 1/27/07 10:2
input is changed slowly enough that the output can keep up. They are
called transfer characteristics because they describe the relationship
between input and output voltages.
An ideal inverter would have an abrupt switching threshold at
VDD/2, as shown in Figure 1.25(a). For V(A)  VDD/2, V(Y)  VDD. For
V(A) 	 VDD/2, V(Y)  0. In such a case, VIH  VIL  VDD/2. VOH
VDD and VOL  0.
A real inverter changes more gradually between the extremes, as
shown in Figure 1.25(b). When the input voltage V(A) is 0, the output
voltage V(Y)  VDD. When V(A)  VDD, V(Y)  0. However, the transition between these endpoints is smooth and may not be centered at
exactly VDD/2. This raises the question of how to define the logic levels.
A reasonable place to choose the logic levels is where the slope of
the transfer characteristic dV(Y)/dV(A) is 1. These two points are
called the unity gain points. Choosing logic levels at the unity gain
points usually maximizes the noise margins. If VIL were reduced, VOH
would only increase by a small amount. But if VIL were increased, VOH
would drop precipitously.
1.6.5 The Static Discipline
To avoid inputs falling into the forbidden zone, digital logic gates are
designed to conform to the static discipline. The static discipline requires
that, given logically valid inputs, every circuit element will produce logically valid outputs.
By conforming to the static discipline, digital designers sacrifice the
freedom of using arbitrary analog circuit elements in return for the
24 CHAPTER ONE From Zero to One
I1 I2
Noise
VO1 VI2 Figure 1.24 Inverter circuit
DC indicates behavior when
an input voltage is held constant or changes slowly
enough for the rest of the system to keep up. The term’s
historical root comes from
direct current, a method of
transmitting power across a
line with a constant voltage.
In contrast, the transient
response of a circuit is the
behavior when an input voltage changes rapidly. Section
2.9 explores transient
response further.
Forbidden
Zone
NML
NMH
Output Characteristics Input Characteristics
VOH
VDD
VOL
GND
VIH
VIL
Logic High
Input Range
Logic Low
Input Range
Logic High
Output Range
Logic Low
Output Range
Driver Receiver
Figure 1.23 Logic levels and
noise margins
Chapter 01
simplicity and robustness of digital circuits. They raise the level of
abstraction from analog to digital, increasing design productivity by
hiding needless detail.
The choice of VDD and logic levels is arbitrary, but all gates that
communicate must have compatible logic levels. Therefore, gates are
grouped into logic families such that all gates in a logic family obey the
static discipline when used with other gates in the family. Logic gates in
the same logic family snap together like Legos in that they use consistent
power supply voltages and logic levels.
Four major logic families that predominated from the 1970’s through
the 1990’s are Transistor-Transistor Logic (TTL), Complementary MetalOxide-Semiconductor Logic (CMOS, pronounced sea-moss), Low Voltage
TTL Logic (LVTTL), and Low Voltage CMOS Logic (LVCMOS). Their
logic levels are compared in Table 1.4. Since then, logic families have
balkanized with a proliferation of even lower power supply voltages.
Appendix A.6 revisits popular logic families in more detail.
1.6 Beneath the Digital Abstraction 25
VDD
V(A)
V(Y)
VOH VDD
VOL
VIL, VIH
0
A Y
VDD
V(A)
V(Y)
VOH
VDD
VOL
VIL VIH
Unity Gain Points
Slope = –1
0
(a) (b)
VDD/ 2
Figure 1.25 DC transfer characteristics and logic levels
Table 1.4 Logic levels of 5 V and 3.3 V logic families
Logic Family VDD VIL VIH VOL VOH
TTL 5 (4.75–5.25) 0.8 2.0 0.4 2.4
CMOS 5 (4.5–6) 1.35 3.15 0.33 3.84
LVTTL 3.3 (3–3.6) 0.8 2.0 0.4 2.4
LVCMOS 3.3 (3–3.6) 0.9 1.8 0.36 2.7

Example 1.19 LOGIC FAMILY COMPATIBILITY
Which of the logic families in Table 1.4 can communicate with each other reliably?
Solution: Table 1.5 lists which logic families have compatible logic levels. Note
that a 5 V logic family such as TTL or CMOS may produce an output voltage as
HIGH as 5 V. If this 5 V signal drives the input of a 3.3 V logic family such as
LVTTL or LVCMOS, it can damage the receiver, unless the receiver is specially
designed to be “5-volt compatible.”
1.7 CMOS TRANSISTORS*
This section and other sections marked with a * are optional and are
not necessary to understand the main flow of the book.
Babbage’s Analytical Engine was built from gears, and early electrical computers used relays or vacuum tubes. Modern computers use transistors because they are cheap, small, and reliable. Transistors are
electrically controlled switches that turn ON or OFF when a voltage or
current is applied to a control terminal. The two main types of transistors are bipolar transistors and metal-oxide-semiconductor field effect
transistors (MOSFETs or MOS transistors, pronounced “moss-fets” or
“M-O-S”, respectively).
In 1958, Jack Kilby at Texas Instruments built the first integrated
circuit containing two transistors. In 1959, Robert Noyce at Fairchild
Semiconductor patented a method of interconnecting multiple transistors
on a single silicon chip. At the time, transistors cost about $10 each.
Thanks to more than three decades of unprecedented manufacturing
advances, engineers can now pack roughly one billion MOSFETs onto a
1 cm2 chip of silicon, and these transistors cost less than 10 microcents
apiece. The capacity and cost continue to improve by an order of magnitude every 8 years or so. MOSFETs are now the building blocks of
26 CHAPTER ONE From Zero to One
Table 1.5 Compatibility of logic families
Receiver
TTL CMOS LVTTL LVCMOS
Driver TTL OK NO: VOH  VIH MAYBEa MAYBEa
CMOS OK OK MAYBEa MAYBEa
LVTTL OK NO: VOH  VIH OK OK
LVCMOS OK NO: VOH  VIH OK OK
a As long as a 5 V HIGH level does not damage the receiver input
Robert Noyce, 1927–1990. Born
in Burlington, Iowa. Received
a B.A. in physics from
Grinnell College and a Ph.D.
in physics from MIT. Nicknamed “Mayor of Silicon
Valley” for his profound
influence on the industry.
Cofounded Fairchild
Semiconductor in 1957 and
Intel in 1968. Coinvented the
integrated circuit. Many engineers from his teams went on
to found other seminal semiconductor companies
(© 2006, Intel Corporation.
Reproduced by permission).

almost all digital systems. In this section, we will peer beneath the digital
abstraction to see how logic gates are built from MOSFETs.
1.7.1 Semiconductors
MOS transistors are built from silicon, the predominant atom in rock
and sand. Silicon (Si) is a group IV atom, so it has four electrons in its
valence shell and forms bonds with four adjacent atoms, resulting in a
crystalline lattice. Figure 1.26(a) shows the lattice in two dimensions for
ease of drawing, but remember that the lattice actually forms a cubic
crystal. In the figure, a line represents a covalent bond. By itself, silicon is
a poor conductor because all the electrons are tied up in covalent bonds.
However, it becomes a better conductor when small amounts of impurities, called dopant atoms, are carefully added. If a group V dopant such
as arsenic (As) is added, the dopant atoms have an extra electron that is
not involved in the bonds. The electron can easily move about the lattice,
leaving an ionized dopant atom (As) behind, as shown in Figure
1.26(b). The electron carries a negative charge, so we call arsenic an
n-type dopant. On the other hand, if a group III dopant such as boron
(B) is added, the dopant atoms are missing an electron, as shown in
Figure 1.26(c). This missing electron is called a hole. An electron from a
neighboring silicon atom may move over to fill the missing bond, forming an ionized dopant atom (B) and leaving a hole at the neighboring
silicon atom. In a similar fashion, the hole can migrate around the lattice.
The hole is a lack of negative charge, so it acts like a positively charged
particle. Hence, we call boron a p-type dopant. Because the conductivity
of silicon changes over many orders of magnitude depending on the
concentration of dopants, silicon is called a semiconductor.
1.7.2 Diodes
The junction between p-type and n-type silicon is called a diode. The
p-type region is called the anode and the n-type region is called the cathode, as illustrated in Figure 1.27. When the voltage on the anode rises
above the voltage on the cathode, the diode is forward biased, and
1.7 CMOS Transistors 27
Si Si Si
Si Si Si
Si Si Si
(a)
Si As Si
Si Si Si
Si Si Si
(b)
-
+
Free electron
Si B Si
Si Si Si
Si Si Si
(c)
+
-
Free hole
Figure 1.26 Silicon lattice and
dopant atoms
C
current flows through the diode from the anode to the cathode. But
when the anode voltage is lower than the voltage on the cathode, the
diode is reverse biased, and no current flows. The diode symbol intuitively shows that current only flows in one direction.
1.7.3 Capacitors
A capacitor consists of two conductors separated by an insulator. When
a voltage V is applied to one of the conductors, the conductor accumulates electric charge Q and the other conductor accumulates the opposite
charge Q. The capacitance C of the capacitor is the ratio of charge to
voltage: C  Q/V. The capacitance is proportional to the size of the conductors and inversely proportional the distance between them. The symbol for a capacitor is shown in Figure 1.28.
Capacitance is important because charging or discharging a conductor takes time and energy. More capacitance means that a circuit will be
slower and require more energy to operate. Speed and energy will be discussed throughout this book.
1.7.4 nMOS and pMOS Transistors
A MOSFET is a sandwich of several layers of conducting and insulating
materials. MOSFETs are built on thin flat wafers of silicon of about
15 to 30 cm in diameter. The manufacturing process begins with a bare
wafer. The process involves a sequence of steps in which dopants are
implanted into the silicon, thin films of silicon dioxide and silicon are
grown, and metal is deposited. Between each step, the wafer is patterned
so that the materials appear only where they are desired. Because transistors are a fraction of a micron2 in length and the entire wafer is
processed at once, it is inexpensive to manufacture billions of transistors
at a time. Once processing is complete, the wafer is cut into rectangles
called chips or dice that contain thousands, millions, or even billions of
transistors. The chip is tested, then placed in a plastic or ceramic package with metal pins to connect it to a circuit board.
The MOSFET sandwich consists of a conducting layer called the
gate on top of an insulating layer of silicon dioxide (SiO2) on top of the
silicon wafer, called the substrate. Historically, the gate was constructed
from metal, hence the name metal-oxide-semiconductor. Modern manufacturing processes use polycrystalline silicon for the gate, because it
does not melt during subsequent high-temperature processing steps.
Silicon dioxide is better known as glass and is often simply called oxide
in the semiconductor industry. The metal-oxide-semiconductor sandwich forms a capacitor, in which a thin layer of insulating oxide called a
dielectric separates the metal and semiconductor plates.
28 CHAPTER ONE From Zero to One
p-type n-type
anode cathode
Figure 1.27 The p-n junction
diode structure and symbol
C
Figure 1.28 Capacitor symbol
Technicians in an Intel clean
room wear Gore-Tex bunny
suits to prevent particulates
from their hair, skin, and
clothing from contaminating
the microscopic transistors on
silicon wafers (© 2006, Intel
Corporation. Reproduced by
permission).
A 40-pin dual-inline package
(DIP) contains a small chip
(scarcely visible) in the center
that is connected to 40 metal
pins, 20 on a side, by gold
wires thinner than a strand of
hair (photograph by Kevin
Mapp. © Harvey Mudd
College). 2 1 
m  1 micron  106 m.
Chapt
There are two flavors of MOSFETs: nMOS and pMOS (pronounced
“n-moss” and “p-moss”). Figure 1.29 shows cross-sections of each type,
made by sawing through a wafer and looking at it from the side. The
n-type transistors, called nMOS, have regions of n-type dopants adjacent
to the gate called the source and the drain and are built on a p-type
semiconductor substrate. The pMOS transistors are just the opposite,
consisting of p-type source and drain regions in an n-type substrate.
A MOSFET behaves as a voltage-controlled switch in which the gate
voltage creates an electric field that turns ON or OFF a connection
between the source and drain. The term field effect transistor comes
from this principle of operation. Let us start by exploring the operation
of an nMOS transistor.
The substrate of an nMOS transistor is normally tied to GND, the
lowest voltage in the system. First, consider the situation when the gate is
also at 0 V, as shown in Figure 1.30(a). The diodes between the source or
drain and the substrate are reverse biased because the source or drain
voltage is nonnegative. Hence, there is no path for current to flow between
the source and drain, so the transistor is OFF. Now, consider when the gate
is raised to VDD, as shown in Figure 1.30(b). When a positive voltage is
applied to the top plate of a capacitor, it establishes an electric field that
attracts positive charge on the top plate and negative charge to the bottom
plate. If the voltage is sufficiently large, so much negative charge is
attracted to the underside of the gate that the region inverts from p-type to
effectively become n-type. This inverted region is called the channel. Now
the transistor has a continuous path from the n-type source through the
n-type channel to the n-type drain, so electrons can flow from source to
drain. The transistor is ON. The gate voltage required to turn on a transistor is called the threshold voltage, Vt, and is typically 0.3 to 0.7 V.
1.7 CMOS Transistors 29
n
p
source gate drain
substrate
SiO2
n
source gate drain
Polysilicon
n p p
gate
source drain
gate
source drain
substrate
(a) nMOS (b) pMOS
Figure 1.29 nMOS and pMOS transistors
The source and drain terminals are physically symmetric.
However, we say that charge
flows from the source to the
drain. In an nMOS transistor,
the charge is carried by electrons, which flow from negative voltage to positive
voltage. In a pMOS transistor,
the charge is carried by holes,
which flow from positive voltage to negative voltage. If we
draw schematics with the
most positive voltage at the
top and the most negative at
the bottom, the source of
(negative) charges in an
nMOS transistor is the bottom
terminal and the source of
(positive) charges in a pMOS
transistor is the top terminal.
A technician holds a 12-inch
wafer containing hundreds
of microprocessor chips
(© 2006, Intel Corporation.
Reproduced by permission).

pMOS transistors work in just the opposite fashion, as might be
guessed from the bubble on their symbol. The substrate is tied to VDD.
When the gate is also at VDD, the pMOS transistor is OFF. When the gate
is at GND, the channel inverts to p-type and the pMOS transistor is ON.
Unfortunately, MOSFETs are not perfect switches. In particular,
nMOS transistors pass 0’s well but pass 1’s poorly. Specifically, when the
gate of an nMOS transistor is at VDD, the drain will only swing between
0 and VDD  Vt. Similarly, pMOS transistors pass 1’s well but 0’s
poorly. However, we will see that it is possible to build logic gates that
use transistors only in their good mode.
nMOS transistors need a p-type substrate, and pMOS transistors
need an n-type substrate. To build both flavors of transistors on the
same chip, manufacturing processes typically start with a p-type wafer,
then implant n-type regions called wells where the pMOS transistors
should go. These processes that provide both flavors of transistors are
called Complementary MOS or CMOS. CMOS processes are used to
build the vast majority of all transistors fabricated today.
In summary, CMOS processes give us two types of electrically
controlled switches, as shown in Figure 1.31. The voltage at the gate (g)
regulates the flow of current between the source (s) and drain (d). nMOS
transistors are OFF when the gate is 0 and ON when the gate is 1.
30 CHAPTER ONE From Zero to One
n
p
gate
source drain
substrate
n
(a)
GND
GND
n
p
source gate drain
substrate
n
(b)
VDD
GND
-------
channel
+++++++
Figure 1.30 nMOS transistor operation
Figure 1.31 Switch models of
MOSFETs
g
s
d
g
d
s
nMOS
pMOS
g = 0
s
d
d
s
OFF
ON
g = 1
s
d
d
s
ON
OFF
Gordon Moore, 1929–. Born in
San Francisco. Received a
B.S. in chemistry from UC
Berkeley and a Ph.D. in chemistry and physics from
Caltech. Cofounded Intel in
1968 with Robert Noyce.
Observed in 1965 that the
number of transistors on a
computer chip doubles every
year. This trend has become
known as Moore’s Law. Since
1975, transistor counts have
doubled every two years.
A corollary of Moore’s
Law is that microprocessor
performance doubles every
18 to 24 months. Semiconductor sales have also
increased exponentially.
Unfortunately, power consumption has increased
exponentially as well
(© 2006, Intel Corporation.
Reproduced by permission).
C
pMOS transistors are just the opposite: ON when the gate is 0 and OFF
when the gate is 1.
1.7.5 CMOS NOT Gate
Figure 1.32 shows a schematic of a NOT gate built with CMOS transistors. The triangle indicates GND, and the flat bar indicates VDD; these
labels will be omitted from future schematics. The nMOS transistor, N1,
is connected between GND and the Y output. The pMOS transistor, P1,
is connected between VDD and the Y output. Both transistor gates are
controlled by the input, A.
If A  0, N1 is OFF and P1 is ON. Hence, Y is connected to VDD
but not to GND, and is pulled up to a logic 1. P1 passes a good 1. If
A  1, N1 is ON and P1 is OFF, and Y is pulled down to a logic 0. N1
passes a good 0. Checking against the truth table in Figure 1.12, we see
that the circuit is indeed a NOT gate.
1.7.6 Other CMOS Logic Gates
Figure 1.33 shows a schematic of a two-input NAND gate. In schematic
diagrams, wires are always joined at three-way junctions. They are
joined at four-way junctions only if a dot is shown. The nMOS transistors N1 and N2 are connected in series; both nMOS transistors must be
ON to pull the output down to GND. The pMOS transistors P1 and P2
are in parallel; only one pMOS transistor must be ON to pull the output
up to VDD. Table 1.6 lists the operation of the pull-down and pull-up
networks and the state of the output, demonstrating that the gate does
function as a NAND. For example, when A  1 and B  0, N1 is ON,
but N2 is OFF, blocking the path from Y to GND. P1 is OFF, but P2 is
ON, creating a path from VDD to Y. Therefore, Y is pulled up to 1.
Figure 1.34 shows the general form used to construct any inverting
logic gate, such as NOT, NAND, or NOR. nMOS transistors are good at
passing 0’s, so a pull-down network of nMOS transistors is placed between
the output and GND to pull the output down to 0. pMOS transistors are
1.7 CMOS Transistors 31
Figure 1.32 NOT gate schematic
Figure 1.33 Two-input NAND
gate schematic
VDD
A Y
GND
N1
P1
A
B
Y
N2
N1
P2 P1
Table 1.6 NAND gate operation
A B Pull-Down Network Pull-Up Network Y
0 0 OFF ON 1
0 1 OFF ON 1
1 0 OFF ON 1
1 1 ON OFF 0
pMOS
pull-up
network
output
inputs
nMOS
pull-down
network
Figure 1.34 General form of an
inverting logic gate
Chap
good at passing 1’s, so a pull-up network of pMOS transistors is placed
between the output and VDD to pull the output up to 1. The networks may
consist of transistors in series or in parallel. When transistors are in parallel, the network is ON if either transistor is ON. When transistors are in
series, the network is ON only if both transistors are ON. The slash across
the input wire indicates that the gate may receive multiple inputs.
If both the pull-up and pull-down networks were ON simultaneously, a short circuit would exist between VDD and GND. The output of
the gate might be in the forbidden zone and the transistors would consume large amounts of power, possibly enough to burn out. On the other
hand, if both the pull-up and pull-down networks were OFF simultaneously, the output would be connected to neither VDD nor GND. We say
that the output floats. Its value is again undefined. Floating outputs are
usually undesirable, but in Section 2.6 we will see how they can occasionally be used to the designer’s advantage.
In a properly functioning logic gate, one of the networks should be
ON and the other OFF at any given time, so that the output is pulled
HIGH or LOW but not shorted or floating. We can guarantee this by
using the rule of conduction complements. When nMOS transistors are
in series, the pMOS transistors must be in parallel. When nMOS transistors are in parallel, the pMOS transistors must be in series.
Example 1.20 THREE-INPUT NAND SCHEMATIC
Draw a schematic for a three-input NAND gate using CMOS transistors.
Solution: The NAND gate should produce a 0 output only when all three inputs
are 1. Hence, the pull-down network should have three nMOS transistors in
series. By the conduction complements rule, the pMOS transistors must be in
parallel. Such a gate is shown in Figure 1.35; you can verify the function by
checking that it has the correct truth table.
Example 1.21 TWO-INPUT NOR SCHEMATIC
Draw a schematic for a two-input NOR gate using CMOS transistors.
Solution: The NOR gate should produce a 0 output if either input is 1. Hence,
the pull-down network should have two nMOS transistors in parallel. By the
conduction complements rule, the pMOS transistors must be in series. Such a
gate is shown in Figure 1.36.
Example 1.22 TWO-INPUT AND SCHEMATIC
Draw a schematic for a two-input AND gate.
32 CHAPTER ONE From Zero to One
A
B
Y
C
A
B
Y
Figure 1.35 Three-input NAND
gate schematic
Figure 1.36 Two-input NOR gate
schematic
Experienced designers claim
that electronic devices operate
because they contain magic
smoke. They confirm this theory with the observation that
if the magic smoke is ever let
out of the device, it ceases to
work.

Solution: It is impossible to build an AND gate with a single CMOS gate.
However, building NAND and NOT gates is easy. Thus, the best way to build an
AND gate using CMOS transistors is to use a NAND followed by a NOT, as
shown in Figure 1.37.
1.7.7 Transmission Gates
At times, designers find it convenient to use an ideal switch that can pass
both 0 and 1 well. Recall that nMOS transistors are good at passing 0
and pMOS transistors are good at passing 1, so the parallel combination
of the two passes both values well. Figure 1.38 shows such a circuit,
called a transmission gate or pass gate. The two sides of the switch are
called A and B because a switch is bidirectional and has no preferred
input or output side. The control signals are called enables, EN and EN
___
.
When EN  0 and EN
___  1, both transistors are OFF. Hence, the transmission gate is OFF or disabled, so A and B are not connected. When EN
 1 and EN
___  0, the transmission gate is ON or enabled, and any logic
value can flow between A and B.
1.7.8 Pseudo-nMOS Logic
An N-input CMOS NOR gate uses N nMOS transistors in parallel and
N pMOS transistors in series. Transistors in series are slower than
transistors in parallel, just as resistors in series have more resistance
than resistors in parallel. Moreover, pMOS transistors are slower than
nMOS transistors because holes cannot move around the silicon lattice
as fast as electrons. Therefore the parallel nMOS transistors are fast
and the series pMOS transistors are slow, especially when many are in
series.
Pseudo-nMOS logic replaces the slow stack of pMOS transistors
with a single weak pMOS transistor that is always ON, as shown in
Figure 1.39. This pMOS transistor is often called a weak pull-up. The
physical dimensions of the pMOS transistor are selected so that the
pMOS transistor will pull the output, Y, HIGH weakly—that is, only if
none of the nMOS transistors are ON. But if any nMOS transistor is
ON, it overpowers the weak pull-up and pulls Y down close enough to
GND to produce a logic 0.
The advantage of pseudo-nMOS logic is that it can be used to build
fast NOR gates with many inputs. For example, Figure 1.40 shows a
pseudo-nMOS four-input NOR. Pseudo-nMOS gates are useful for certain
memory and logic arrays discussed in Chapter 5. The disadvantage is that
a short circuit exists between VDD and GND when the output is LOW; the
weak pMOS and nMOS transistors are both ON. The short circuit draws
continuous power, so pseudo-nMOS logic must be used sparingly.
1.7 CMOS Transistors 33
Figure 1.40 Pseudo-nMOS fourinput NOR gate
Figure 1.37 Two-input AND gate
schematic
Figure 1.38 Transmission gate
Figure 1.39 Generic pseudonMOS gate
A
B Y
A B
EN
EN
Y
inputs nMOS
pull-down
network
weak
A B
Y
weak
C D
Chap
Pseudo-nMOS gates got their name from the 1970’s, when manufacturing processes only had nMOS transistors. A weak nMOS transistor
was used to pull the output HIGH because pMOS transistors were not
available.
1.8 POWER CONSUMPTION*
Power consumption is the amount of energy used per unit time. Power
consumption is of great importance in digital systems. The battery life of
portable systems such as cell phones and laptop computers is limited by
power consumption. Power is also significant for systems that are
plugged in, because electricity costs money and because the system will
overheat if it draws too much power.
Digital systems draw both dynamic and static power. Dynamic
power is the power used to charge capacitance as signals change between
0 and 1. Static power is the power used even when signals do not change
and the system is idle.
Logic gates and the wires that connect them have capacitance. The
energy drawn from the power supply to charge a capacitance C to voltage VDD is CVDD
2. If the voltage on the capacitor switches at frequency
f (i.e., f times per second), it charges the capacitor f/2 times and discharges it f/2 times per second. Discharging does not draw energy from
the power supply, so the dynamic power consumption is
Pdynamic  CV2
DDf (1.4)
Electrical systems draw some current even when they are idle. When
transistors are OFF, they leak a small amount of current. Some circuits,
such as the pseudo-nMOS gate discussed in Section 1.7.8, have a path
from VDD to GND through which current flows continuously. The total
static current, IDD, is also called the leakage current or the quiescent
supply current flowing between VDD and GND. The static power consumption is proportional to this static current:
Pstatic  IDDVDD (1.5)
Example 1.23 POWER CONSUMPTION
A particular cell phone has a 6 watt-hour (W-hr) battery and operates at 1.2 V.
Suppose that, when it is in use, the cell phone operates at 300 MHz and the
average amount of capacitance in the chip switching at any given time is 10 nF
(108 Farads). When in use, it also broadcasts 3 W of power out of its antenna.
When the phone is not in use, the dynamic power drops to almost zero because
the signal processing is turned off. But the phone also draws 40 mA of quiescent
current whether it is in use or not. Determine the battery life of the phone (a) if it
is not being used, and (b) if it is being used continuously.
1
2
34 CHAPTER ONE From Zero to One
Cha
Solution: The static power is Pstatic  (0.040 A)(1.2 V)  48 mW. If the phone is
not being used, this is the only power consumption, so the battery life is (6 Whr)/(0.048 W)  125 hours (about 5 days). If the phone is being used, the
dynamic power is Pdynamic  (0.5)(108 F)(1.2 V)2(3  108 Hz)  2.16 W.
Together with the static and broadcast power, the total active power is 2.16 W 
0.048 W  3 W  5.2 W, so the battery life is 6 W-hr/5.2 W  1.15 hours. This
example somewhat oversimplifies the actual operation of a cell phone, but it
illustrates the key ideas of power consumption.
1.9 SUMMARY AND A LOOK AHEAD
There are 10 kinds of people in this world: those who can count in
binary and those who can’t.
This chapter has introduced principles for understanding and designing
complex systems. Although the real world is analog, digital designers
discipline themselves to use a discrete subset of possible signals. In particular, binary variables have just two states: 0 and 1, also called FALSE
and TRUE or LOW and HIGH. Logic gates compute a binary output
from one or more binary inputs. Some of the common logic gates are:
 NOT: TRUE when input is FALSE
 AND: TRUE when all inputs are TRUE
 OR: TRUE when any inputs are TRUE
 XOR: TRUE when an odd number of inputs are TRUE
Logic gates are commonly built from CMOS transistors, which
behave as electrically controlled switches. nMOS transistors turn ON
when the gate is 1. pMOS transistors turn ON when the gate is 0.
In Chapters 2 through 5, we continue the study of digital logic.
Chapter 2 addresses combinational logic, in which the outputs depend
only on the current inputs. The logic gates introduced already are examples of combinational logic. You will learn to design circuits involving
multiple gates to implement a relationship between inputs and outputs
specified by a truth table or Boolean equation. Chapter 3 addresses
sequential logic, in which the outputs depend on both current and past
inputs. Registers are common sequential elements that remember their
previous input. Finite state machines, built from registers and combinational logic, are a powerful way to build complicated systems in a systematic fashion. We also study timing of digital systems to analyze how
fast the systems can operate. Chapter 4 describes hardware description
languages (HDLs). HDLs are related to conventional programming
languages but are used to simulate and build hardware rather than
software. Most digital systems today are designed with HDLs. Verilog
1.9 Summary and a Look Ahead 35
Chapter 01.q
and VHDL are the two prevalent languages, and they are covered sideby-side in this book. Chapter 5 studies other combinational and sequential building blocks such as adders, multipliers, and memories.
Chapter 6 shifts to computer architecture. It describes the MIPS
processor, an industry-standard microprocessor used in consumer electronics, some Silicon Graphics workstations, and many communications
systems such as televisions, networking hardware and wireless links. The
MIPS architecture is defined by its registers and assembly language
instruction set. You will learn to write programs in assembly language
for the MIPS processor so that you can communicate with the processor
in its native language.
Chapters 7 and 8 bridge the gap between digital logic and computer
architecture. Chapter 7 investigates microarchitecture, the arrangement
of digital building blocks, such as adders and registers, needed to construct a processor. In that chapter, you learn to build your own MIPS
processor. Indeed, you learn three microarchitectures illustrating different
trade-offs of performance and cost. Processor performance has increased
exponentially, requiring ever more sophisticated memory systems to feed
the insatiable demand for data. Chapter 8 delves into memory system
architecture and also describes how computers communicate with peripheral devices such as keyboards and printers.
36 CHAPTER ONE From Zero to One

Exercises 37
Exercises
Exercise 1.1 Explain in one paragraph at least three levels of abstraction that are
used by
a) biologists studying the operation of cells.
b) chemists studying the composition of matter.
Exercise 1.2 Explain in one paragraph how the techniques of hierarchy,
modularity, and regularity may be used by
a) automobile designers.
b) businesses to manage their operations.
Exercise 1.3 Ben Bitdiddle is building a house. Explain how he can use the
principles of hierarchy, modularity, and regularity to save time and money
during construction.
Exercise 1.4 An analog voltage is in the range of 0–5 V. If it can be measured with
an accuracy of  50 mV, at most how many bits of information does it convey?
Exercise 1.5 A classroom has an old clock on the wall whose minute hand
broke off.
a) If you can read the hour hand to the nearest 15 minutes, how many bits
of information does the clock convey about the time?
b) If you know whether it is before or after noon, how many additional bits
of information do you know about the time?
Exercise 1.6 The Babylonians developed the sexagesimal (base 60) number system about 4000 years ago. How many bits of information is conveyed with one
sexagesimal digit? How do you write the number 400010 in sexagesimal?
Exercise 1.7 How many different numbers can be represented with 16 bits?
Exercise 1.8 What is the largest unsigned 32-bit binary number?
Exercise 1.9 What is the largest 16-bit binary number that can be represented
with
a) unsigned numbers?
b) two’s complement numbers?
c) sign/magnitude numbers?

38 CHAPTER ONE From Zero to One
Exercise 1.10 What is the smallest (most negative) 16-bit binary number that
can be represented with
a) unsigned numbers?
b) two’s complement numbers?
c) sign/magnitude numbers?
Exercise 1.11 Convert the following unsigned binary numbers to decimal.
a) 10102
b) 1101102
c) 111100002
d) 00011000101001112
Exercise 1.12 Repeat Exercise 1.11, but convert to hexadecimal.
Exercise 1.13 Convert the following hexadecimal numbers to decimal.
a) A516
b) 3B16
c) FFFF16
d) D000000016
Exercise 1.14 Repeat Exercise 1.13, but convert to unsigned binary.
Exercise 1.15 Convert the following two’s complement binary numbers to decimal.
a) 10102
b) 1101102
c) 011100002
d) 100111112
Exercise 1.16 Repeat Exercise 1.15, assuming the binary numbers are in
sign/magnitude form rather than two’s complement representation.
Exercise 1.17 Convert the following decimal numbers to unsigned binary
numbers.
a) 4210
b) 6310

c) 22910
d) 84510
Exercise 1.18 Repeat Exercise 1.17, but convert to hexadecimal.
Exercise 1.19 Convert the following decimal numbers to 8-bit two’s complement
numbers or indicate that the decimal number would overflow the range.
a) 4210
b) 6310
c) 12410
d) 12810
e) 13310
Exercise 1.20 Repeat Exercise 1.19, but convert to 8-bit sign/magnitude
numbers.
Exercise 1.21 Convert the following 4-bit two’s complement numbers to 8-bit
two’s complement numbers.
a) 01012
b) 10102
Exercise 1.22 Repeat Exercise 1.21 if the numbers are unsigned rather than
two’s complement.
Exercise 1.23 Base 8 is referred to as octal. Convert each of the numbers from
Exercise 1.17 to octal.
Exercise 1.24 Convert each of the following octal numbers to binary,
hexadecimal, and decimal.
a) 428
b) 638
c) 2558
d) 30478
Exercise 1.25 How many 5-bit two’s complement numbers are greater than 0?
How many are less than 0? How would your answers differ for sign/magnitude
numbers?
Exercises 39
Ch
Exercise 1.26 How many bytes are in a 32-bit word? How many nibbles are in
the word?
Exercise 1.27 How many bytes are in a 64-bit word?
Exercise 1.28 A particular DSL modem operates at 768 kbits/sec. How many
bytes can it receive in 1 minute?
Exercise 1.29 Hard disk manufacturers use the term “megabyte” to mean 106
bytes and “gigabyte” to mean 109 bytes. How many real GBs of music can you
store on a 50 GB hard disk?
Exercise 1.30 Estimate the value of 231 without using a calculator.
Exercise 1.31 A memory on the Pentium II microprocessor is organized as a rectangular array of bits with 28 rows and 29 columns. Estimate how many bits it
has without using a calculator.
Exercise 1.32 Draw a number line analogous to Figure 1.11 for 3-bit unsigned,
two’s complement, and sign/magnitude numbers.
Exercise 1.33 Perform the following additions of unsigned binary numbers.
Indicate whether or not the sum overflows a 4-bit result.
a) 10012  01002
b) 11012  10112
Exercise 1.34 Perform the following additions of unsigned binary numbers.
Indicate whether or not the sum overflows an 8-bit result.
a) 100110012  010001002
b) 110100102  101101102
Exercise 1.35 Repeat Exercise 1.34, assuming that the binary numbers are in
two’s complement form.
Exercise 1.36 Convert the following decimal numbers to 6-bit two’s complement
binary numbers and add them. Indicate whether or not the sum overflows a 6-bit
result.
a) 1610  910
b) 2710  3110
c) 410  1910
40 CHAPTER ONE From Zero to One
C
d) 310  3210
e) 1610  910
f) 2710  3110
Exercise 1.37 Perform the following additions of unsigned hexadecimal
numbers. Indicate whether or not the sum overflows an 8-bit (two hex digit)
result.
a) 716  916
b) 1316  2816
c) AB16  3E16
d) 8F16  AD16
Exercise 1.38 Convert the following decimal numbers to 5-bit two’s complement
binary numbers and subtract them. Indicate whether or not the difference overflows a 5-bit result.
a) 910  710
b) 1210  1510
c) 610  1110
d) 410  810
Exercise 1.39 In a biased N-bit binary number system with bias B, positive and
negative numbers are represented as their value plus the bias B. For example, for
5-bit numbers with a bias of 15, the number 0 is represented as 01111, 1 as
10000, and so forth. Biased number systems are sometimes used in floating point
mathematics, which will be discussed in Chapter 5. Consider a biased 8-bit
binary number system with a bias of 12710.
a) What decimal value does the binary number 100000102 represent?
b) What binary number represents the value 0?
c) What is the representation and value of the most negative number?
d) What is the representation and value of the most positive number?
Exercise 1.40 Draw a number line analogous to Figure 1.11 for 3-bit biased
numbers with a bias of 3 (see Exercise 1.39 for a definition of biased numbers).
Exercise 1.41 In a binary coded decimal (BCD) system, 4 bits are used to represent a decimal digit from 0 to 9. For example, 3710 is written as 00110111BCD.
Exercises 41
Chapter 01.
a) Write 28910 in BCD.
b) Convert 100101010001BCD to decimal.
c) Convert 01101001BCD to binary.
d) Explain why BCD might be a useful way to represent numbers.
Exercise 1.42 A flying saucer crashes in a Nebraska cornfield. The FBI investigates the wreckage and finds an engineering manual containing an equation in
the Martian number system: 325  42  411. If this equation is correct, how
many fingers would you expect Martians have?
Exercise 1.43 Ben Bitdiddle and Alyssa P. Hacker are having an argument. Ben
says, “All integers greater than zero and exactly divisible by six have exactly two
1’s in their binary representation.” Alyssa disagrees. She says, “No, but all such
numbers have an even number of 1’s in their representation.” Do you agree with
Ben or Alyssa or both or neither? Explain.
Exercise 1.44 Ben Bitdiddle and Alyssa P. Hacker are having another argument.
Ben says, “I can get the two’s complement of a number by subtracting 1, then
inverting all the bits of the result.” Alyssa says, “No, I can do it by examining
each bit of the number, starting with the least significant bit. When the first 1 is
found, invert each subsequent bit.” Do you agree with Ben or Alyssa or both or
neither? Explain.
Exercise 1.45 Write a program in your favorite language (e.g., C, Java, Perl) to
convert numbers from binary to decimal. The user should type in an unsigned
binary number. The program should print the decimal equivalent.
Exercise 1.46 Repeat Exercise 1.45 but convert from decimal to hexadecimal.
Exercise 1.47 Repeat Exercise 1.45 but convert from an arbitrary base b1 to
another base b2, as specified by the user. Support bases up to 16, using the letters
of the alphabet for digits greater than 9. The user should enter b1, b2, and then
the number to convert in base b1. The program should print the equivalent number in base b2.
Exercise 1.48 Draw the symbol, Boolean equation, and truth table for
a) a three-input OR gate.
b) a three-input exclusive OR (XOR) gate.
c) a four-input XNOR gate
42 CHAPTER ONE From Zero to One
C
Exercise 1.49 A majority gate produces a TRUE output if and only if more than
half of its inputs are TRUE. Complete a truth table for the three-input majority
gate shown in Figure 1.41.
Exercise 1.50 A three-input AND-OR (AO) gate shown in Figure 1.42 produces
a TRUE output if both A and B are TRUE, or if C is TRUE. Complete a truth
table for the gate.
Exercise 1.51 A three-input OR-AND-INVERT (OAI) gate shown in Figure
1.43 produces a FALSE input if C is TRUE and A or B is TRUE. Otherwise it
produces a TRUE output. Complete a truth table for the gate.
Exercise 1.52 There are 16 different truth tables for Boolean functions of two
variables. List each truth table. Give each one a short descriptive name (such as
OR, NAND, and so on).
Exercise 1.53 How many different truth tables exist for Boolean functions of N
variables?
Exercise 1.54 Is it possible to assign logic levels so that a device with the transfer characteristics shown in Figure 1.44 would serve as an inverter? If so, what
are the input and output low and high levels (VIL, VOL, VIH, and VOH) and
noise margins (NML and NMH)? If not, explain why not.
Exercises 43
A
B Y
C
MAJ
A
B Y C
A
B Y C
Figure 1.41 Three-input majority gate
Figure 1.42 Three-input AND-OR gate
Figure 1.43 Three-input OR-AND-INVERT gate

Exercise 1.55 Repeat Exercise 1.54 for the transfer characteristics shown in
Figure 1.45.
Exercise 1.56 Is it possible to assign logic levels so that a device with the transfer characteristics shown in Figure 1.46 would serve as a buffer? If so, what are
the input and output low and high levels (VIL, VOL, VIH, and VOH) and noise
margins (NML and NMH)? If not, explain why not.
44 CHAPTER ONE From Zero to One
Figure 1.45 DC transfer characteristics
Vin
Vout
012345
0
1
2
3
4
5
Figure 1.44 DC transfer characteristics
Vin
Vout
012345
0
1
2
3
4
5

Exercise 1.57 Ben Bitdiddle has invented a circuit with the transfer characteristics shown in Figure 1.47 that he would like to use as a buffer. Will it work?
Why or why not? He would like to advertise that it is compatible with LVCMOS
and LVTTL logic. Can Ben’s buffer correctly receive inputs from those logic
families? Can its output properly drive those logic families? Explain.
Exercise 1.58 While walking down a dark alley, Ben Bitdiddle encounters a twoinput gate with the transfer function shown in Figure 1.48. The inputs are A and
B and the output is Y.
a) What kind of logic gate did he find?
b) What are the approximate high and low logic levels?
Exercises 45
Vin
Vout
012345
0
1
2
3
4
5
Vin
Vout
0
0
0.6
1.2
1.8
2.4
3.0
3.3
0.6 1.2 1.8 2.4 3.0 3.3
Figure 1.46 DC transfer characteristics
Figure 1.47 Ben’s buffer DC transfer characteristics

Exercise 1.59 Repeat Exercise 1.58 for Figure 1.49.
Exercise 1.60 Sketch a transistor-level circuit for the following CMOS gates. Use
a minimum number of transistors.
a) A four-input NAND gate.
b) A three-input OR-AND-INVERT gate (see Exercise 1.51).
c) A three-input AND-OR gate (see Exercise 1.50).
Exercise 1.61 A minority gate produces a TRUE output if and only if fewer
than half of its inputs are TRUE. Otherwise it produces a FALSE output. Sketch
a transistor-level circuit for a CMOS minority gate. Use a minimum number of
transistors.
46 CHAPTER ONE From Zero to One
Figure 1.48 Two-input DC transfer characteristics
Figure 1.49 Two-input DC transfer characteristics
0
1
2
3
0
1
2
3
0
1
2
3
A B
Y
0
1
2
3
0
1
2
3
0
1
2
3
A B
Y

Exercise 1.62 Write a truth table for the function performed by the gate in
Figure 1.50. The truth table should have two inputs, A and B. What is the name
of this function?
Exercise 1.63 Write a truth table for the function performed by the gate in
Figure 1.51. The truth table should have three inputs, A, B, and C.
Exercise 1.64 Implement the following three-input gates using only pseudonMOS logic gates. Your gates receive three inputs, A, B, and C. Use a minimum
number of transistors.
a) three-input NOR gate
b) three-input NAND gate
a) three-input AND gate
Exercise 1.65 Resistor-Transistor Logic (RTL) uses nMOS transistors to pull
the gate output LOW and a weak resistor to pull the output HIGH when none
of the paths to ground are active. A NOT gate built using RTL is shown in
Figure 1.52. Sketch a three-input RTL NOR gate. Use a minimum number of
transistors.
Exercises 47
A
B
A
B
A
A
B
B
Y
A
B
C
C
A B
Y
A
Y
weak
Figure 1.50 Mystery schematic
Figure 1.51 Mystery schematic
Figure 1.52 RTL NOT gate

Interview Questions
These questions have been asked at interviews for digital design jobs.
Question 1.1 Sketch a transistor-level circuit for a CMOS four-input NOR gate.
Question 1.2 The king receives 64 gold coins in taxes but has reason to believe
that one is counterfeit. He summons you to identify the fake coin. You have a
balance that can hold coins on each side. How many times do you need to use
the balance to find the lighter, fake coin?
Question 1.3 The professor, the teaching assistant, the digital design student, and
the freshman track star need to cross a rickety bridge on a dark night. The
bridge is so shakey that only two people can cross at a time. They have only one
flashlight among them and the span is too long to throw the flashlight, so somebody must carry it back to the other people. The freshman track star can cross
the bridge in 1 minute. The digital design student can cross the bridge in 2 minutes. The teaching assistant can cross the bridge in 5 minutes. The professor
always gets distracted and takes 10 minutes to cross the bridge. What is the
fastest time to get everyone across the bridge?
48 CHAPTER ONE From Zero to One



2
2.1 Introduction
2.2 Boolean Equations
2.3 Boolean Algebra
2.4 From Logic to Gates
2.5 Multilevel Combinational
Logic
2.6 X’s and Z’s, Oh My
2.7 Karnaugh Maps
2.8 Combinational Building
Blocks
2.9 Timing
2.10 Summary
Exercises
Interview Questions
Combinational Logic Design
2.1 INTRODUCTION
In digital electronics, a circuit is a network that processes discretevalued variables. A circuit can be viewed as a black box, shown in
Figure 2.1, with
 one or more discrete-valued input terminals
 one or more discrete-valued output terminals
 a functional specification describing the relationship between inputs
and outputs
 a timing specification describing the delay between inputs changing
and outputs responding.
Peering inside the black box, circuits are composed of nodes and elements. An element is itself a circuit with inputs, outputs, and a specification. A node is a wire, whose voltage conveys a discrete-valued variable.
Nodes are classified as input, output, or internal. Inputs receive values
from the external world. Outputs deliver values to the external world.
Wires that are not inputs or outputs are called internal nodes. Figure 2.2
51
inputs outputs
functional spec
timing spec
Figure 2.1 Circuit as a black
box with inputs, outputs, and
specifications
Figure 2.2 Elements and nodes
A E1
E2
B E3
C
n1
Y
Z
Chap
illustrates a circuit with three elements, E1, E2, and E3, and six nodes.
Nodes A, B, and C are inputs. Y and Z are outputs. n1 is an internal
node between E1 and E3.
Digital circuits are classified as combinational or sequential. A combinational circuit’s outputs depend only on the current values of the
inputs; in other words, it combines the current input values to compute
the output. For example, a logic gate is a combinational circuit. A
sequential circuit’s outputs depend on both current and previous values
of the inputs; in other words, it depends on the input sequence. A combinational circuit is memoryless, but a sequential circuit has memory. This
chapter focuses on combinational circuits, and Chapter 3 examines
sequential circuits.
The functional specification of a combinational circuit expresses
the output values in terms of the current input values. The timing
specification of a combinational circuit consists of lower and upper
bounds on the delay from input to output. We will initially concentrate
on the functional specification, then return to the timing specification
later in this chapter.
Figure 2.3 shows a combinational circuit with two inputs and one
output. On the left of the figure are the inputs, A and B, and on the right
is the output, Y. The symbol CL inside the box indicates that it is implemented using only combinational logic. In this example, the function, F,
is specified to be OR: Y  F(A, B)  A  B. In words, we say the
output, Y, is a function of the two inputs, A and B, namely Y  A OR B.
Figure 2.4 shows two possible implementations for the combinational logic circuit in Figure 2.3. As we will see repeatedly throughout
the book, there are often many implementations for a single function.
You choose which to use given the building blocks at your disposal and
your design constraints. These constraints often include area, speed,
power, and design time.
Figure 2.5 shows a combinational circuit with multiple outputs. This
particular combinational circuit is called a full adder and we will revisit
it in Section 5.2.1. The two equations specify the function of the outputs, S and Cout, in terms of the inputs, A, B, and Cin.
To simplify drawings, we often use a single line with a slash through
it and a number next to it to indicate a bus, a bundle of multiple signals.
The number specifies how many signals are in the bus. For example,
Figure 2.6(a) represents a block of combinational logic with three inputs
and two outputs. If the number of bits is unimportant or obvious from
the context, the slash may be shown without a number. Figure 2.6(b)
indicates two blocks of combinational logic with an arbitrary number of
outputs from one block serving as inputs to the second block.
The rules of combinational composition tell us how we can build
a large combinational circuit from smaller combinational circuit
52 CHAPTER TWO Combinational Logic Design
Figure 2.3 Combinational logic
circuit
Figure 2.4 Two OR
implementations
A
B Y
Y = F(A, B) = A + B
CL
A
B Y
(a)
Y
(b)
A
B
Figure 2.5 Multiple-output
combinational circuit
Figure 2.6 Slash notation for
multiple signals
A S
S = A ⊕ B ⊕ Cin
Cout = AB + ACin + BCin
B
Cin
CL Cout
CL 3
(a)
CL CL
(b)
2
Chap
elements. A circuit is combinational if it consists of interconnected
circuit elements such that
 Every circuit element is itself combinational.
 Every node of the circuit is either designated as an input to the
circuit or connects to exactly one output terminal of a circuit
element.
 The circuit contains no cyclic paths: every path through the circuit
visits each circuit node at most once.
Example 2.1 COMBINATIONAL CIRCUITS
Which of the circuits in Figure 2.7 are combinational circuits according to the
rules of combinational composition?
Solution: Circuit (a) is combinational. It is constructed from two combinational
circuit elements (inverters I1 and I2). It has three nodes: n1, n2, and n3. n1 is an
input to the circuit and to I1; n2 is an internal node, which is the output of I1
and the input to I2; n3 is the output of the circuit and of I2. (b) is not combinational, because there is a cyclic path: the output of the XOR feeds back to one of
its inputs. Hence, a cyclic path starting at n4 passes through the XOR to n5,
which returns to n4. (c) is combinational. (d) is not combinational, because node
n6 connects to the output terminals of both I3 and I4. (e) is combinational,
illustrating two combinational circuits connected to form a larger combinational
circuit. (f) does not obey the rules of combinational composition because it has a
cyclic path through the two elements. Depending on the functions of the
elements, it may or may not be a combinational circuit.
Large circuits such as microprocessors can be very complicated, so
we use the principles from Chapter 1 to manage the complexity. Viewing
a circuit as a black box with a well-defined interface and function is an
2.1 Introduction 53
The rules of combinational
composition are sufficient but
not strictly necessary. Certain
circuits that disobey these
rules are still combinational,
so long as the outputs depend
only on the current values of
the inputs. However, determining whether oddball circuits are combinational is
more difficult, so we will usually restrict ourselves to combinational composition as a
way to build combinational
circuits.
Figure 2.7 Example circuits
(a)
n1 n2 n3 I1 I2
(c)
CL
CL
(e)
n4
n5
(b)
n6
I3
I4
(d)
CL
CL
(f)
Cha
application of abstraction and modularity. Building the circuit out of
smaller circuit elements is an application of hierarchy. The rules of
combinational composition are an application of discipline.
The functional specification of a combinational circuit is usually
expressed as a truth table or a Boolean equation. In the next sections, we
describe how to derive a Boolean equation from any truth table and how
to use Boolean algebra and Karnaugh maps to simplify equations. We
show how to implement these equations using logic gates and how to
analyze the speed of these circuits.
2.2 BOOLEAN EQUATIONS
Boolean equations deal with variables that are either TRUE or FALSE,
so they are perfect for describing digital logic. This section defines some
terminology commonly used in Boolean equations, then shows how to
write a Boolean equation for any logic function given its truth table.
2.2.1 Terminology
The complement of a variable, A, is its inverse, . The variable or its
complement is called a literal. For example, A, , B, and are literals.
We call A the true form of the variable and the complementary form;
“true form” does not mean that A is TRUE, but merely that A does not
have a line over it.
The AND of one or more literals is called a product or an implicant.
, , and B are all implicants for a function of three variables.
A minterm is a product involving all of the inputs to the function.
is a minterm for a function of the three variables A, B, and C, but is
not, because it does not involve C. Similarly, the OR of one or more
literals is called a sum. A maxterm is a sum involving all of the inputs
to the function. is a maxterm for a function of the three
variables A, B, and C.
The order of operations is important when interpreting Boolean
equations. Does Y  A  BC mean Y  (A OR B) AND C or Y  A
OR (B AND C)? In Boolean equations, NOT has the highest precedence,
followed by AND, then OR. Just as in ordinary equations, products are
performed before sums. Therefore, the equation is read as Y  A OR
(B AND C). Equation 2.1 gives another example of order of operations.
(2.1)
2.2.2 Sum-of-Products Form
A truth table of N inputs contains 2N rows, one for each possible value
of the inputs. Each row in a truth table is associated with a minterm
AB  BCD  ((A)B)  (BC(D))
A  B  C
AB
ABC
AB ABC
A
A B
A
54 CHAPTER TWO Combinational Logic Design
Chapter 02
that is TRUE for that row. Figure 2.8 shows a truth table of two
inputs, A and B. Each row shows its corresponding minterm. For example, the minterm for the first row is because is TRUE when
A  0, B  0.
We can write a Boolean equation for any truth table by summing
each of the minterms for which the output, Y, is TRUE. For example,
in Figure 2.8, there is only one row (or minterm) for which the
output Y is TRUE, shown circled in blue. Thus, . Figure 2.9
shows a truth table with more than one row in which the output is
TRUE. Taking the sum of each of the circled minterms gives
.
This is called the sum-of-products canonical form of a function
because it is the sum (OR) of products (ANDs forming minterms).
Although there are many ways to write the same function, such as
, we will sort the minterms in the same order that they
appear in the truth table, so that we always write the same Boolean
expression for the same truth table.
Example 2.2 SUM-OF-PRODUCTS FORM
Ben Bitdiddle is having a picnic. He won’t enjoy it if it rains or if there are ants.
Design a circuit that will output TRUE only if Ben enjoys the picnic.
Solution: First define the inputs and outputs. The inputs are A and R, which
indicate if there are ants and if it rains. A is TRUE when there are ants and
FALSE when there are no ants. Likewise, R is TRUE when it rains and FALSE
when the sun smiles on Ben. The output is E, Ben’s enjoyment of the picnic. E is
TRUE if Ben enjoys the picnic and FALSE if he suffers. Figure 2.10 shows the
truth table for Ben’s picnic experience.
Using sum-of-products form, we write the equation as: . We can build the
equation using two inverters and a two-input AND gate, shown in Figure 2.11(a).
You may recognize this truth table as the NOR function from Section 1.5.5: E  A
NOR . Figure 2.11(b) shows the NOR implementation. In
Section 2.3, we show that the two equations, and , are equivalent.
The sum-of-products form provides a Boolean equation for any truth
table with any number of variables. Figure 2.12 shows a random threeinput truth table. The sum-of-products form of the logic function is
(2.2)
Unfortunately, sum-of-products form does not necessarily generate
the simplest equation. In Section 2.3 we show how to write the same
function using fewer terms.
Y  ABC  ABC  ABC
AR A  R
R  A  R
E AR
Y  BA  BA
Y  AB  AB
Y  AB
AB AB
2.2 Boolean Equations 55
Figure 2.8 Truth table and
minterms
Figure 2.9 Truth table with
multiple TRUE minterms
0
ABY
0 0
0 1
1 0
1 1
0
1
0
minterm
A B
A B
A B
A B
ABY
0 0
0 1
1 0
1 1
0
1
0
1
minterm
A B
A B
A B
A B
Canonical form is just a fancy
word for standard form. You
can use the term to impress
your friends and scare your
enemies.
Chapter 02.qxd 
2.2.3 Product-of-Sums Form
An alternative way of expressing Boolean functions is the product-ofsums canonical form. Each row of a truth table corresponds to a maxterm that is FALSE for that row. For example, the maxterm for the
first row of a two-input truth table is (A  B) because (A  B) is
FALSE when A  0, B  0. We can write a Boolean equation for any
circuit directly from the truth table as the AND of each of the maxterms for which the output is FALSE.
Example 2.3 PRODUCT-OF-SUMS FORM
Write an equation in product-of-sums form for the truth table in Figure 2.13.
Solution: The truth table has two rows in which the output is FALSE. Hence, the
function can be written in product-of-sums form as . The
first maxterm, (A  B), guarantees that Y  0 for A  0, B  0, because any
value AND 0 is 0. Likewise, the second maxterm, , guarantees that Y  0
for A  1, B  0. Figure 2.13 is the same truth table as Figure 2.9, showing that
the same function can be written in more than one way.
Similarly, a Boolean equation for Ben’s picnic from Figure 2.10
can be written in product-of-sums form by circling the three rows of
0’s to obtain . This is uglier than the sumof-products equation, , but the two equations are logically
equivalent.
Sum-of-products produces the shortest equations when the output is
TRUE on only a few rows of a truth table; product-of-sums is simpler
when the output is FALSE on only a few rows of a truth table.
2.3 BOOLEAN ALGEBRA
In the previous section, we learned how to write a Boolean expression
given a truth table. However, that expression does not necessarily lead to
the simplest set of logic gates. Just as you use algebra to simplify mathematical equations, you can use Boolean algebra to simplify Boolean
equations. The rules of Boolean algebra are much like those of ordinary
algebra but are in some cases simpler, because variables have only two
possible values: 0 or 1.
Boolean algebra is based on a set of axioms that we assume are correct. Axioms are unprovable in the sense that a definition cannot be
proved. From these axioms, we prove all the theorems of Boolean algebra. These theorems have great practical significance, because they teach
us how to simplify logic to produce smaller and less costly circuits.
E  AR
E  (A  R)(A  R)(A  R)
(A  B)
Y  (A  B)(A  B)
56 CHAPTER TWO Combinational Logic Design
Figure 2.13 Truth table with
multiple FALSE maxterms
A + B
ABY
0 0
0 1
1 0
1 1
0
1
0
1
maxterm
A + B
A + B
A + B
Figure 2.12 Random three-input
truth table
BCY
0 0
0 1
1 0
1 1
1
0
0
0
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
Figure 2.11 Ben’s circuit
A
R
E
(a)
A
R E
(b)
Figure 2.10 Ben’s truth table
ARE
0 0
0 1
1 0
1 1
1
0
0
0
Chapter 02.qxd 1/31/
Axioms and theorems of Boolean algebra obey the principle of duality. If the symbols 0 and 1 and the operators • (AND) and  (OR) are
interchanged, the statement will still be correct. We use the prime ()
symbol to denote the dual of a statement.
2.3.1 Axioms
Table 2.1 states the axioms of Boolean algebra. These five axioms and
their duals define Boolean variables and the meanings of NOT, AND,
and OR. Axiom A1 states that a Boolean variable B is 0 if it is not 1.
The axiom’s dual, A1, states that the variable is 1 if it is not 0. Together,
A1 and A1 tell us that we are working in a Boolean or binary field of
0’s and 1’s. Axioms A2 and A2 define the NOT operation. Axioms A3
to A5 define AND; their duals, A3 to A5 define OR.
2.3.2 Theorems of One Variable
Theorems T1 to T5 in Table 2.2 describe how to simplify equations
involving one variable.
The identity theorem, T1, states that for any Boolean variable B, B
AND 1  B. Its dual states that B OR 0  B. In hardware, as shown in
Figure 2.14, T1 means that if one input of a two-input AND gate is
always 1, we can remove the AND gate and replace it with a wire connected to the variable input (B). Likewise, T1 means that if one input of
a two-input OR gate is always 0, we can replace the OR gate with a wire
connected to B. In general, gates cost money, power, and delay, so
replacing a gate with a wire is beneficial.
The null element theorem, T2, says that B AND 0 is always equal
to 0. Therefore, 0 is called the null element for the AND operation,
because it nullifies the effect of any other input. The dual states that B
OR 1 is always equal to 1. Hence, 1 is the null element for the OR
operation. In hardware, as shown in Figure 2.15, if one input of an
AND gate is 0, we can replace the AND gate with a wire that is tied
2.3 Boolean Algebra 57
Axiom Dual Name
A1 B  0 if B  1 A1 B  1 if B  0 Binary field
A2 A2 NOT
A3 0 • 0  0 A3 1  1  1 AND/OR
A4 1 • 1  1 A4 0  0  0 AND/OR
A5 0 • 1  1 • 0  0 A5 1  0  0  1  1 AND/OR
0  1 1  0
Table 2.1 Axioms of Boolean algebra
The null element theorem
leads to some outlandish statements that are actually true! It
is particularly dangerous when
left in the hands of advertisers:
YOU WILL GET A MILLION
DOLLARS or we’ll send you a
toothbrush in the mail. (You’ll
most likely be receiving a
toothbrush in the mail.)
Figure 2.14 Identity theorem in
hardware: (a) T1, (b) T1
1 =
(a)
B B
= 0
B B
(b)
Chapter 02.qxd 1/31
LOW (to 0). Likewise, if one input of an OR gate is 1, we can replace
the OR gate with a wire that is tied HIGH (to 1).
Idempotency, T3, says that a variable AND itself is equal to just
itself. Likewise, a variable OR itself is equal to itself. The theorem gets
its name from the Latin roots: idem (same) and potent (power). The
operations return the same thing you put into them. Figure 2.16 shows
that idempotency again permits replacing a gate with a wire.
Involution, T4, is a fancy way of saying that complementing a variable twice results in the original variable. In digital electronics, two
wrongs make a right. Two inverters in series logically cancel each other
out and are logically equivalent to a wire, as shown in Figure 2.17. The
dual of T4 is itself.
The complement theorem, T5 (Figure 2.18), states that a variable
AND its complement is 0 (because one of them has to be 0). And, by duality, a variable OR its complement is 1 (because one of them has to be 1).
2.3.3 Theorems of Several Variables
Theorems T6 to T12 in Table 2.3 describe how to simplify equations
involving more than one Boolean variable.
Commutativity and associativity, T6 and T7, work the same as in
traditional algebra. By commutativity, the order of inputs for an AND or
OR function does not affect the value of the output. By associativity, the
specific groupings of inputs do not affect the value of the output.
The distributivity theorem, T8, is the same as in traditional algebra,
but its dual, T8, is not. By T8, AND distributes over OR, and by T8, OR
distributes over AND. In traditional algebra, multiplication distributes
over addition but addition does not distribute over multiplication, so that
(B  C)  (B  D)  B  (C  D).
The covering, combining, and consensus theorems, T9 to T11, permit us to eliminate redundant variables. With some thought, you should
be able to convince yourself that these theorems are correct.
58 CHAPTER TWO Combinational Logic Design
Theorem Dual Name
T1 B • 1  B T1 B  0  B Identity
T2 B • 0  0 T2 B  1  1 Null Element
T3 B • B  B T3 B  B  B Idempotency
T4 B
=  B Involution
T5 B • B  0 T5 B  B  1 Complements
Table 2.2 Boolean theorems of one variable
Figure 2.16 Idempotency
theorem in hardware: (a) T3,
(b) T3
B =
(a)
B B
= B
B B
(b)
Figure 2.17 Involution theorem
in hardware: T4
Figure 2.18 Complement
theorem in hardware: (a) T5,
(b) T5
B = B
B =
(a)
B
0
= B
B
1
(b)
Figure 2.15 Null element
theorem in hardware: (a) T2,
(b) T2
0 =
(a)
B 0
= 1
B 1
(b)
Chapter 02.qxd 1
De Morgan’s Theorem, T12, is a particularly powerful tool in digital
design. The theorem explains that the complement of the product of all
the terms is equal to the sum of the complement of each term. Likewise,
the complement of the sum of all the terms is equal to the product of the
complement of each term.
According to De Morgan’s theorem, a NAND gate is equivalent to
an OR gate with inverted inputs. Similarly, a NOR gate is equivalent to
an AND gate with inverted inputs. Figure 2.19 shows these De Morgan
equivalent gates for NAND and NOR gates. The two symbols shown for
each function are called duals. They are logically equivalent and can be
used interchangeably.
2.3 Boolean Algebra 59
Theorem Dual Name
T6 B • C  C • B T6 B  C  C  B Commutativity
T7 (B • C) • D  B • (C • D) T7 (B  C)  D  B  (C  D) Associativity
T8 (B • C)  (B • D)  B • (C  D) T8 (B  C) • (B  D)  B  (C • D) Distributivity
T9 B • (B  C)  B T9 B  (B • C)  B Covering
T10 T10 Combining
T11 T11 Consensus
T12 T12 De Morgan’s
 (B0  B1  B2 ...)  (B0 • B1 • B2) Theorem
B0 • B1 • B2... B0  B1  B2...
 B • C  B • D  (B  C) • (B  D)
(B • C)  (B • D)  (C • D) (B  C) • (B  D) • (C  D)
(B • C)  (B • C)  B (B  C) • (B  C)  B
Table 2.3 Boolean theorems of several variables
Augustus De Morgan, died 1871.
A British mathematician, born
in India. Blind in one eye. His
father died when he was 10.
Attended Trinity College,
Cambridge, at age 16, and
was appointed Professor of
Mathematics at the newly
founded London University at
age 22. Wrote widely on many
mathematical subjects, including logic, algebra, and paradoxes. De Morgan’s crater on
the moon is named for him.
He proposed a riddle for the
year of his birth: “I was x
years of age in the year x2.”
Figure 2.19 De Morgan equivalent gates
ABY
001
011
101
110
NAND
A
B Y
A
B Y
NOR
A
B Y
A
B Y
ABY
001
010
100
110
Y = AB = A + B Y = A + B = A B
Chapter 02.qxd 1/31/07 9:55 PM Page 59
The inversion circle is called a bubble. Intuitively, you can imagine that
“pushing” a bubble through the gate causes it to come out at the other side
and flips the body of the gate from AND to OR or vice versa. For example,
the NAND gate in Figure 2.19 consists of an AND body with a bubble on
the output. Pushing the bubble to the left results in an OR body with
bubbles on the inputs. The underlying rules for bubble pushing are
 Pushing bubbles backward (from the output) or forward (from the
inputs) changes the body of the gate from AND to OR or vice versa.
 Pushing a bubble from the output back to the inputs puts bubbles
on all gate inputs.
 Pushing bubbles on all gate inputs forward toward the output puts a
bubble on the output.
Section 2.5.2 uses bubble pushing to help analyze circuits.
Example 2.4 DERIVE THE PRODUCT-OF-SUMS FORM
Figure 2.20 shows the truth table for a Boolean function, Y, and its complement,
. Using De Morgan’s Theorem, derive the product-of-sums canonical form of Y
from the sum-of-products form of .
Solution: Figure 2.21 shows the minterms (circled) contained in . The sum-ofproducts canonical form of is
(2.3)
Taking the complement of both sides and applying De Morgan’s Theorem twice,
we get:
(2.4)
2.3.4 The Truth Behind It All
The curious reader might wonder how to prove that a theorem is true. In
Boolean algebra, proofs of theorems with a finite number of variables
are easy: just show that the theorem holds for all possible values of these
variables. This method is called perfect induction and can be done with a
truth table.
Example 2.5 PROVING THE CONSENSUS THEOREM
Prove the consensus theorem, T11, from Table 2.3.
Solution: Check both sides of the equation for all eight combinations of B, C,
and D. The truth table in Figure 2.22 illustrates these combinations. Because
BC  BD  CD  BC  BD for all cases, the theorem is proved.
Y  Y  AB  AB  (AB)(AB)  (A  B)(A  B)
Y  AB  AB
Y
Y
Y
Y
60 CHAPTER TWO Combinational Logic Design
FIGURE 2.20 Truth table
showing Y and Y
–
Figure 2.21 Truth table showing
minterms for Y
–
ABY
0 0
0 1
1 0
1 1
0
0
1
1
Y
1
1
0
0
ABY
0 0
0 1
1 0
1 1
0
0
1
1
Y
1
1
0
0
minterm
A B
A B
A B
A B
Chapter 02.qxd 1
2.3.5 Simplifying Equations
The theorems of Boolean algebra help us simplify Boolean equations.
For example, consider the sum-of-products expression from the truth
table of Figure 2.9: . By Theorem T10, the equation simplifies to . This may have been obvious looking at the truth table.
In general, multiple steps may be necessary to simplify more complex
equations.
The basic principle of simplifying sum-of-products equations is to
combine terms using the relationship , where P may be
any implicant. How far can an equation be simplified? We define an
equation in sum-of-products form to be minimized if it uses the fewest
possible implicants. If there are several equations with the same number
of implicants, the minimal one is the one with the fewest literals.
An implicant is called a prime implicant if it cannot be combined
with any other implicants to form a new implicant with fewer literals.
The implicants in a minimal equation must all be prime implicants.
Otherwise, they could be combined to reduce the number of literals.
Example 2.6 EQUATION MINIMIZATION
Minimize Equation 2.2: .
Solution: We start with the original equation and apply Boolean theorems step
by step, as shown in Table 2.4.
Have we simplified the equation completely at this point? Let’s take a closer
look. From the original equation, the minterms and differ only in the
variable A. So we combined the minterms to form . However, if we look at
the original equation, we note that the last two minterms and also
differ by a single literal (C and ). Thus, using the same method, we could have
combined these two minterms to form the minterm . We say that implicants
and share the minterm .
So, are we stuck with simplifying only one of the minterm pairs, or can we simplify
both? Using the idempotency theorem, we can duplicate terms as many times as we
want: B  B  B  B  B … . Using this principle, we simplify the equation completely to its two prime implicants, , as shown in Table 2.5. BC  AB
BC AB ABC
AB
C
ABC ABC
BC
ABC ABC
ABC  ABC  AB C
PA  PA  P
Y  B
Y  AB  AB
2.3 Boolean Algebra 61
Figure 2.22 Truth table proving
T11
0 0
0 1
1 0
1 1
BCD
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
BC + BD + CD BC + BD
0
1
0
1
0
0
1
1
0
1
0
1
0
0
1
1
Chapter 02.q
Although it is a bit counterintuitive, expanding an implicant (for
example, turning AB into ) is sometimes useful in minimizing equations. By doing this, you can repeat one of the expanded
minterms to be combined (shared) with another minterm.
You may have noticed that completely simplifying a Boolean equation with the theorems of Boolean algebra can take some trial and error.
Section 2.7 describes a methodical technique called Karnaugh maps that
makes the process easier.
Why bother simplifying a Boolean equation if it remains logically
equivalent? Simplifying reduces the number of gates used to physically
implement the function, thus making it smaller, cheaper, and possibly
faster. The next section describes how to implement Boolean equations
with logic gates.
2.4 FROM LOGIC TO GATES
A schematic is a diagram of a digital circuit showing the elements and
the wires that connect them together. For example, the schematic in
Figure 2.23 shows a possible hardware implementation of our favorite
logic function, Equation 2.2:
Y  ABC  ABC  ABC.
ABC  ABC
62 CHAPTER TWO Combinational Logic Design
Step Equation Justification
1 T3: Idempotency
2 T8: Distributivity
3 T5: Complements
4 BC  AB T1: Identity
BC(1)  AB(1)
BC(A  A)  AB(C  C)
ABC  ABC  ABC  ABC
ABC  ABC  ABC
Table 2.5 Improved equation minimization
Step Equation Justification
1 T8: Distributivity
2 T5: Complements
3 BC  ABC T1: Identity
BC (1)  ABC
BC (A  A)  ABC
ABC  ABC  ABC
Table 2.4 Equation minimization
Chapter 02.qxd 1/31/
By drawing schematics in a consistent fashion, we make them easier
to read and debug. We will generally obey the following guidelines:
 Inputs are on the left (or top) side of a schematic.
 Outputs are on the right (or bottom) side of a schematic.
 Whenever possible, gates should flow from left to right.
 Straight wires are better to use than wires with multiple corners
(jagged wires waste mental effort following the wire rather than
thinking of what the circuit does).
 Wires always connect at a T junction.
 A dot where wires cross indicates a connection between the wires.
 Wires crossing without a dot make no connection.
The last three guidelines are illustrated in Figure 2.24.
Any Boolean equation in sum-of-products form can be drawn as a
schematic in a systematic way similar to Figure 2.23. First, draw
columns for the inputs. Place inverters in adjacent columns to provide
the complementary inputs if necessary. Draw rows of AND gates for
each of the minterms. Then, for each output, draw an OR gate
connected to the minterms related to that output. This style is called
a programmable logic array (PLA) because the inverters, AND gates,
and OR gates are arrayed in a systematic fashion. PLAs will be
discussed further in Section 5.6.
Figure 2.25 shows an implementation of the simplified equation we
found using Boolean algebra in Example 2.6. Notice that the simplified
circuit has significantly less hardware than that of Figure 2.23. It may
also be faster, because it uses gates with fewer inputs.
We can reduce the number of gates even further (albeit by a single
inverter) by taking advantage of inverting gates. Observe that is an BC
2.4 From Logic to Gates 63
Figure 2.23 Schematic of
Y A
–
B
–
C
–  AB
–
C
– AB–
C
A C B
Y
minterm: ABC
minterm: ABC
minterm: ABC
ABC
Figure 2.24 Wire connections
wires connect
at a T junction
wires connect
at a dot
wires crossing
without a dot do
not connect
A B C
Y
Figure 2.25 Schematic of
Y B
–
C
– AB
–
Chapter 02.q
1 Black light, twinkies, and beer.
AND with inverted inputs. Figure 2.26 shows a schematic using this
optimization to eliminate the inverter on C. Recall that by De Morgan’s
theorem the AND with inverted inputs is equivalent to a NOR gate.
Depending on the implementation technology, it may be cheaper to use
the fewest gates or to use certain types of gates in preference to others.
For example, NANDs and NORs are preferred over ANDs and ORs in
CMOS implementations.
Many circuits have multiple outputs, each of which computes a separate Boolean function of the inputs. We can write a separate truth table
for each output, but it is often convenient to write all of the outputs on a
single truth table and sketch one schematic with all of the outputs.
Example 2.7 MULTIPLE-OUTPUT CIRCUITS
The dean, the department chair, the teaching assistant, and the dorm social
chair each use the auditorium from time to time. Unfortunately, they occasionally conflict, leading to disasters such as the one that occurred when the dean’s
fundraising meeting with crusty trustees happened at the same time as the
dorm’s BTB1 party. Alyssa P. Hacker has been called in to design a room reservation system.
The system has four inputs, A3, . . . , A0, and four outputs, Y3, . . . , Y0. These
signals can also be written as A3:0 and Y3:0. Each user asserts her input when she
requests the auditorium for the next day. The system asserts at most one output,
granting the auditorium to the highest priority user. The dean, who is paying for
the system, demands highest priority (3). The department chair, teaching assistant, and dorm social chair have decreasing priority.
Write a truth table and Boolean equations for the system. Sketch a circuit that
performs this function.
Solution: This function is called a four-input priority circuit. Its symbol and truth
table are shown in Figure 2.27.
We could write each output in sum-of-products form and reduce the equations
using Boolean algebra. However, the simplified equations are clear by inspection from the functional description (and the truth table): Y3 is TRUE whenever A3 is asserted, so Y3  A3. Y2 is TRUE if A2 is asserted and A3 is not
asserted, so . Y1 is TRUE if A1 is asserted and neither of the higher
priority inputs is asserted: . And Y0 is TRUE whenever A0 and
no other input is asserted: . The schematic is shown in Figure
2.28. An experienced designer can often implement a logic circuit by inspection. Given a clear specification, simply turn the words into equations and the
equations into gates.
Y0  A3A2A1 A0
Y1  A3A2 A1
Y2  A3A2
64 CHAPTER TWO Combinational Logic Design
Y
A B C
Figure 2.26 Schematic using
fewer gates
Chap
Notice that if A3 is asserted in the priority circuit, the outputs don’t
care what the other inputs are. We use the symbol X to describe inputs
that the output doesn’t care about. Figure 2.29 shows that the four-input
priority circuit truth table becomes much smaller with don’t cares. From
this truth table, we can easily read the Boolean equations in sum-ofproducts form by ignoring inputs with X’s. Don’t cares can also appear
in truth table outputs, as we will see in Section 2.7.3.
2.5 MULTILEVEL COMBINATIONAL LOGIC
Logic in sum-of-products form is called two-level logic because it consists of literals connected to a level of AND gates connected to a level of
2.5 Multilevel Combinational Logic 65
A0
A1
Priority
Circuit
A2
A3
0 0
0 1
1 0
1 1
0
0
0
0
0
0
0
0
0
0
1
1
0
1
0
0
Y0
Y1
Y2
Y3
0 0
0 0
0 0
0 0
0 1 000 1 0 0
0 1
1 0
1 1
0 0
0 1
0 1
0 1
1 0
1 0 0 1
1 0
1 1
0 0
0 1
1 0
1 0
1 1
1 1
1 1 1 0
1 1 1 1
0
0
0
1
1
1
1
0
0
0
0
0
0
0
0
0
1 00 0
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
1 00 0
1 00 0
A3 A2 A1 A0 Y3 Y2 Y1 Y0
Figure 2.27 Priority circuit
Figure 2.28 Priority circuit
schematic
A3A2A1A0
Y3
Y2
Y1
Y0
Figure 2.29 Priority circuit truth table
with don’t cares (X’s)
A1 A0
0 0
0 1
1 X
X X
0
0
0
0
0
0
0
1
0
0
1
0
0
1
0
0
A3 A2
0 0
0 0
0 0
0 1
1 X X X 1 000
Y3 Y2 Y1 Y0
X is an overloaded symbol
that means “don’t care” in
truth tables and “contention”
in logic simulation (see
Section 2.6.1). Think about
the context so you don’t mix
up the meanings. Some
authors use D or ? instead
for “don’t care” to avoid
this ambiguity.

OR gates. Designers often build circuits with more than two levels of
logic gates. These multilevel combinational circuits may use less hardware than their two-level counterparts. Bubble pushing is especially
helpful in analyzing and designing multilevel circuits.
2.5.1 Hardware Reduction
Some logic functions require an enormous amount of hardware when
built using two-level logic. A notable example is the XOR function of
multiple variables. For example consider building a three-input XOR
using the two-level techniques we have studied so far.
Recall that an N-input XOR produces a TRUE output when an odd
number of inputs are TRUE. Figure 2.30 shows the truth table for a
three-input XOR with the rows circled that produce TRUE outputs.
From the truth table, we read off a Boolean equation in sum-of-products
form in Equation 2.5. Unfortunately, there is no way to simplify this
equation into fewer implicants.
(2.5)
On the other hand, A  B  C  (A  B)  C (prove this to yourself by perfect induction if you are in doubt). Therefore, the three-input
XOR can be built out of a cascade of two-input XORs, as shown in
Figure 2.31.
Similarly, an eight-input XOR would require 128 eight-input AND
gates and one 128-input OR gate for a two-level sum-of-products implementation. A much better option is to use a tree of two-input XOR
gates, as shown in Figure 2.32.
Y  ABC  ABC  ABC  ABC
66 CHAPTER TWO Combinational Logic Design
Figure 2.30 Three-input XOR:
(a) functional specification
and (b) two-level logic
implementation
B C
0 0
0 1
1 0
1 1
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
1
1
0
1
0
0
1
Y
XOR3
Y = A ⊕ B ⊕ C
A
B Y
C
A B C
Y
(a) (b)
Chapter 0
Selecting the best multilevel implementation of a specific logic function is not a simple process. Moreover, “best” has many meanings: fewest
gates, fastest, shortest design time, least cost, least power consumption. In
Chapter 5, you will see that the “best” circuit in one technology is not
necessarily the best in another. For example, we have been using ANDs
and ORs, but in CMOS, NANDs and NORs are more efficient. With
some experience, you will find that you can create a good multilevel
design by inspection for most circuits. You will develop some of this
experience as you study circuit examples through the rest of this book. As
you are learning, explore various design options and think about the
trade-offs. Computer-aided design (CAD) tools are also available to
search a vast space of possible multilevel designs and seek the one that
best fits your constraints given the available building blocks.
2.5.2 Bubble Pushing
You may recall from Section 1.7.6 that CMOS circuits prefer NANDs
and NORs over ANDs and ORs. But reading the equation by inspection
from a multilevel circuit with NANDs and NORs can get pretty hairy.
Figure 2.33 shows a multilevel circuit whose function is not immediately clear by inspection. Bubble pushing is a helpful way to redraw
these circuits so that the bubbles cancel out and the function can be
more easily determined. Building on the principles from Section 2.3.3,
the guidelines for bubble pushing are as follows:
 Begin at the output of the circuit and work toward the inputs.
 Push any bubbles on the final output back toward the inputs so that
you can read an equation in terms of the output (for example, Y)
instead of the complement of the output .
 Working backward, draw each gate in a form so that bubbles cancel.
If the current gate has an input bubble, draw the preceding gate with
an output bubble. If the current gate does not have an input bubble,
draw the preceding gate without an output bubble.
Figure 2.34 shows how to redraw Figure 2.33 according to the
bubble pushing guidelines. Starting at the output, Y, the NAND gate
has a bubble on the output that we wish to eliminate. We push the
output bubble back to form an OR with inverted inputs, shown in
(Y)
2.5 Multilevel Combinational Logic 67
Figure 2.31 Three-input XOR
using two two-input XORs
A
B
Y C
Figure 2.32 Eight-input XOR
using seven two-input XORs
Figure 2.33 Multilevel circuit
using NANDs and NORs
A
B
C
D
Y
Cha
Figure 2.34(a). Working to the left, the rightmost gate has an input
bubble that cancels with the output bubble of the middle NAND gate,
so no change is necessary, as shown in Figure 2.34(b). The middle gate
has no input bubble, so we transform the leftmost gate to have no
output bubble, as shown in Figure 2.34(c). Now all of the bubbles in
the circuit cancel except at the inputs, so the function can be read by
inspection in terms of ANDs and ORs of true or complementary
inputs: .
For emphasis of this last point, Figure 2.35 shows a circuit logically equivalent to the one in Figure 2.34. The functions of internal
nodes are labeled in blue. Because bubbles in series cancel, we
can ignore the bubble on the output of the middle gate and the input
of the rightmost gate to produce the logically equivalent circuit of
Figure 2.35.
Y  ABC  D
68 CHAPTER TWO Combinational Logic Design
A
B
C Y
D
(a)
no output
bubble
bubble on
A input and output
B
C
D
Y
(b)
A
B
C
D
Y
(c)
Y = ABC + D
no bubble on
input and output
Figure 2.34 Bubble-pushed
circuit
Figure 2.35 Logically equivalent
bubble-pushed circuit
A
B
C
D
Y
AB
ABC
Y = ABC + D
Ch
Example 2.8 BUBBLE PUSHING FOR CMOS LOGIC
Most designers think in terms of AND and OR gates, but suppose you would
like to implement the circuit in Figure 2.36 in CMOS logic, which favors
NAND and NOR gates. Use bubble pushing to convert the circuit to NANDs,
NORs, and inverters.
Solution: A brute force solution is to just replace each AND gate with a NAND
and an inverter, and each OR gate with a NOR and an inverter, as shown in
Figure 2.37. This requires eight gates. Notice that the inverter is drawn with the
bubble on the front rather than back, to emphasize how the bubble can cancel
with the preceding inverting gate.
For a better solution, observe that bubbles can be added to the output of a gate
and the input of the next gate without changing the function, as shown in Figure
2.38(a). The final AND is converted to a NAND and an inverter, as shown in
Figure 2.38(b). This solution requires only five gates.
2.6 X’S AND Z’S, OH MY
Boolean algebra is limited to 0’s and 1’s. However, real circuits can also
have illegal and floating values, represented symbolically by X and Z.
2.6.1 Illegal Value: X
The symbol X indicates that the circuit node has an unknown or illegal
value. This commonly happens if it is being driven to both 0 and 1 at the
same time. Figure 2.39 shows a case where node Y is driven both HIGH
and LOW. This situation, called contention, is considered to be an error
2.6 X’s and Z’s, Oh My 69
Figure 2.36 Circuit using ANDs
and ORs
Figure 2.37 Poor circuit using
NANDs and NORs
(a) (b)
Figure 2.38 Better circuit using
NANDs and NORs
Figure 2.39 Circuit with
contention
A = 1
Y = X
B = 0

and must be avoided. The actual voltage on a node with contention may
be somewhere between 0 and VDD, depending on the relative strengths
of the gates driving HIGH and LOW. It is often, but not always, in the
forbidden zone. Contention also can cause large amounts of power to
flow between the fighting gates, resulting in the circuit getting hot and
possibly damaged.
X values are also sometimes used by circuit simulators to indicate
an uninitialized value. For example, if you forget to specify the
value of an input, the simulator may assume it is an X to warn you of
the problem.
As mentioned in Section 2.4, digital designers also use the symbol
X to indicate “don’t care” values in truth tables. Be sure not to mix up
the two meanings. When X appears in a truth table, it indicates that
the value of the variable in the truth table is unimportant. When
X appears in a circuit, it means that the circuit node has an unknown
or illegal value.
2.6.2 Floating Value: Z
The symbol Z indicates that a node is being driven neither HIGH nor
LOW. The node is said to be floating, high impedance, or high Z. A typical
misconception is that a floating or undriven node is the same as a logic 0.
In reality, a floating node might be 0, might be 1, or might be at some voltage in between, depending on the history of the system. A floating node
does not always mean there is an error in the circuit, so long as some other
circuit element does drive the node to a valid logic level when the value of
the node is relevant to circuit operation.
One common way to produce a floating node is to forget to
connect a voltage to a circuit input, or to assume that an unconnected
input is the same as an input with the value of 0. This mistake may
cause the circuit to behave erratically as the floating input randomly
changes from 0 to 1. Indeed, touching the circuit may be enough to
trigger the change by means of static electricity from the body. We have
seen circuits that operate correctly only as long as the student keeps a
finger pressed on a chip.
The tristate buffer, shown in Figure 2.40, has three possible output states: HIGH (1), LOW (0), and floating (Z). The tristate buffer
has an input, A, an output, Y, and an enable, E. When the enable is
TRUE, the tristate buffer acts as a simple buffer, transferring the input
value to the output. When the enable is FALSE, the output is allowed
to float (Z).
The tristate buffer in Figure 2.40 has an active high enable. That is,
when the enable is HIGH (1), the buffer is enabled. Figure 2.41 shows a
tristate buffer with an active low enable. When the enable is LOW (0),
70 CHAPTER TWO Combinational Logic Design
Figure 2.40 Tristate buffer
Figure 2.41 Tristate buffer with
active low enable
EAY
00Z
01Z
100
111
A
E
Y
EAY
000
011
10Z
11Z
A
E
Y

the buffer is enabled. We show that the signal is active low by putting a
bubble on its input wire. We often indicate an active low input by drawing a bar over its name, , or appending the word “bar” after its name,
Ebar.
Tristate buffers are commonly used on busses that connect multiple chips. For example, a microprocessor, a video controller, and an
Ethernet controller might all need to communicate with the memory
system in a personal computer. Each chip can connect to a shared
memory bus using tristate buffers, as shown in Figure 2.42. Only one
chip at a time is allowed to assert its enable signal to drive a
value onto the bus. The other chips must produce floating outputs
so that they do not cause contention with the chip talking to the
memory. Any chip can read the information from the shared bus at
any time. Such tristate busses were once common. However, in modern computers, higher speeds are possible with point-to-point links, in
which chips are connected to each other directly rather than over
a shared bus.
2.7 KARNAUGH MAPS
After working through several minimizations of Boolean equations using
Boolean algebra, you will realize that, if you’re not careful, you sometimes end up with a completely different equation instead of a simplified
equation. Karnaugh maps (K-maps) are a graphical method for simplifying Boolean equations. They were invented in 1953 by Maurice
Karnaugh, a telecommunications engineer at Bell Labs. K-maps work
well for problems with up to four variables. More important, they give
insight into manipulating Boolean equations.
E
2.7 Karnaugh Maps 71
Figure 2.42 Tristate bus
connecting multiple chips
en1
to bus
from bus
en2
to bus
from bus
en3
to bus
from bus
en4
to bus
from bus
Processor
Video
Ethernet
shared bus
Memory
Maurice Karnaugh, 1924–.
Graduated with a bachelor’s
degree in physics from the
City College of New York in
1948 and earned a Ph.D. in
physics from Yale in 1952.
Worked at Bell Labs and IBM
from 1952 to 1993 and as a
computer science professor at
the Polytechnic University of
New York from 1980 to 1999.

Recall that logic minimization involves combining terms. Two terms
containing an implicant, P, and the true and complementary forms of
some variable, A, are combined to eliminate A: .
Karnaugh maps make these combinable terms easy to see by putting
them next to each other in a grid.
Figure 2.43 shows the truth table and K-map for a three-input
function. The top row of the K-map gives the four possible values
for the A and B inputs. The left column gives the two possible
values for the C input. Each square in the K-map corresponds to a
row in the truth table and contains the value of the output, Y, for that
row. For example, the top left square corresponds to the first row in
the truth table and indicates that the output value Y  1 when ABC
000. Just like each row in a truth table, each square in a K-map
represents a single minterm. For the purpose of explanation,
Figure 2.43(c) shows the minterm corresponding to each square in
the K-map.
Each square, or minterm, differs from an adjacent square by a
change in a single variable. This means that adjacent squares share
all the same literals except one, which appears in true form in one
square and in complementary form in the other. For example,
the squares representing the minterms and are adjacent
and differ only in the variable C. You may have noticed that the
A and B combinations in the top row are in a peculiar order: 00, 01,
11, 10. This order is called a Gray code. It differs from ordinary
binary order (00, 01, 10, 11) in that adjacent entries differ only in
a single variable. For example, 01 : 11 only changes A from 0 to 1,
while 01 : 10 would change A from 1 to 0 and B from 0 to 1.
Hence, writing the combinations in binary order would not have
produced our desired property of adjacent squares differing only in
one variable.
The K-map also “wraps around.” The squares on the far right are
effectively adjacent to the squares on the far left, in that they differ only
in one variable, A. In other words, you could take the map and roll it
into a cylinder, then join the ends of the cylinder to form a torus (i.e., a
donut), and still guarantee that adjacent squares would differ only in one
variable.
2.7.1 Circular Thinking
In the K-map in Figure 2.43, only two minterms are present in the
equation, and , as indicated by the 1’s in the left column.
Reading the minterms from the K-map is exactly equivalent to reading
equations in sum-of-products form directly from the truth table.
ABC ABC
ABC ABC
PA  PA  P
72 CHAPTER TWO Combinational Logic Design
Gray codes were patented
(U.S. Patent 2,632,058) by
Frank Gray, a Bell Labs
researcher, in 1953. They are
especially useful in mechanical encoders because a slight
misalignment causes an error
in only one bit.
Gray codes generalize to
any number of bits. For
example, a 3-bit Gray code
sequence is:
000, 001, 011, 010,
110, 111, 101, 100
Lewis Carroll posed a related
puzzle in Vanity Fair in 1879.
“The rules of the Puzzle are
simple enough. Two words
are proposed, of the same
length; and the puzzle
consists of linking these
together by interposing
other words, each of which
shall differ from the next
word in one letter only. That
is to say, one letter may be
changed in one of the given
words, then one letter in the
word so obtained, and so
on, till we arrive at the
other given word.”
For example, SHIP to DOCK:
SHIP, SLIP, SLOP,
SLOT, SOOT, LOOT,
LOOK, LOCK, DOCK.
Can you find a shorter
sequence?
Chap
As before, we can use Boolean algebra to minimize equations in sum-ofproducts form.
(2.6)
K-maps help us do this simplification graphically by circling 1’s in
adjacent squares, as shown in Figure 2.44. For each circle, we write the
corresponding implicant. Remember from Section 2.2 that an implicant is
the product of one or more literals. Variables whose true and complementary forms are both in the circle are excluded from the implicant. In this
case, the variable C has both its true form (1) and its complementary form
(0) in the circle, so we do not include it in the implicant. In other words, Y
is TRUE when A  B  0, independent of C. So the implicant is . This
K-map gives the same answer we reached using Boolean algebra.
2.7.2 Logic Minimization with K-Maps
K-maps provide an easy visual way to minimize logic. Simply circle all
the rectangular blocks of 1’s in the map, using the fewest possible
number of circles. Each circle should be as large as possible. Then read
off the implicants that were circled.
More formally, recall that a Boolean equation is minimized when it
is written as a sum of the fewest number of prime implicants. Each circle
on the K-map represents an implicant. The largest possible circles are
prime implicants.
AB
Y  ABC  ABC  AB(C  C)  AB
2.7 Karnaugh Maps 73
Figure 2.43 Three-input function: (a) truth table, (b) K-map, (c) K-map showing minterms
B C
0 0
0 1
1 0
1 1
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
0
0
0
0
Y
(a)
C 00 01
0
1
Y
11 10
AB
1
1
0
0
0
0
0
0
(b)
C 00 01
0
1
Y
11 10
AB
ABC
ABC
ABC
ABC
ABC
ABC
ABC
ABC
(c)
C 00 01
0
1
Y
11 10
AB
1
0
0
0
0
0
0
1 Figure 2.44 K-map minimization
Chapter
For example, in the K-map of Figure 2.44, and are
implicants, but not prime implicants. Only is a prime implicant in
that K-map. Rules for finding a minimized equation from a K-map are
as follows:
 Use the fewest circles necessary to cover all the 1’s.
 All the squares in each circle must contain 1’s.
 Each circle must span a rectangular block that is a power of 2
(i.e., 1, 2, or 4) squares in each direction.
 Each circle should be as large as possible.
 A circle may wrap around the edges of the K-map.
 A 1 in a K-map may be circled multiple times if doing so allows
fewer circles to be used.
Example 2.9 MINIMIZATION OF A THREE-VARIABLE FUNCTION
USING A K-MAP
Suppose we have the function Y  F(A, B, C) with the K-map shown in
Figure 2.45. Minimize the equation using the K-map.
Solution: Circle the 1’s in the K-map using as few circles as possible, as shown in
Figure 2.46. Each circle in the K-map represents a prime implicant, and the dimension of each circle is a power of two (2  1 and 2  2). We form the prime implicant for each circle by writing those variables that appear in the circle only in true or
only in complementary form.
For example, in the 2  1 circle, the true and complementary forms of B are
included in the circle, so we do not include B in the prime implicant. However, only
the true form of A(A) and complementary form of C are in this circle, so we
include these variables in the prime implicant . Similarly, the 2  2 circle covers
all squares where B  0, so the prime implicant is .
Notice how the top-right square (minterm) is covered twice to make the prime
implicant circles as large as possible. As we saw with Boolean algebra
techniques, this is equivalent to sharing a minterm to reduce the size of the
B
AC
(C)
AB
ABC ABC
74 CHAPTER TWO Combinational Logic Design
Figure 2.45 K-map for
Example 2.9
00 01
Y
11 10
AB
1
1
0
0
1
0
1
1
0
1
C
Chapter 
implicant. Also notice how the circle covering four squares wraps around
the sides of the K-map.
Example 2.10 SEVEN-SEGMENT DISPLAY DECODER
A seven-segment display decoder takes a 4-bit data input, D3:0, and produces
seven outputs to control light-emitting diodes to display a digit from 0 to 9. The
seven outputs are often called segments a through g, or Sa–Sg, as defined in
Figure 2.47. The digits are shown in Figure 2.48. Write a truth table for the outputs, and use K-maps to find Boolean equations for outputs Sa and Sb. Assume
that illegal input values (10–15) produce a blank readout.
Solution: The truth table is given in Table 2.6. For example, an input of 0000
should turn on all segments except Sg.
Each of the seven outputs is an independent function of four variables. The
K-maps for outputs Sa and Sb are shown in Figure 2.49. Remember that adjacent
squares may differ in only a single variable, so we label the rows and columns in
Gray code order: 00, 01, 11, 10. Be careful to also remember this ordering when
entering the output values into the squares.
Next, circle the prime implicants. Use the fewest number of circles necessary to
cover all the 1’s. A circle can wrap around the edges (vertical and horizontal),
and a 1 may be circled more than once. Figure 2.50 shows the prime implicants
and the simplified Boolean equations.
Note that the minimal set of prime implicants is not unique. For example, the
0000 entry in the Sa K-map was circled along with the 1000 entry to produce
the minterm. The circle could have included the 0010 entry instead,
producing a minterm, as shown with dashed lines in Figure 2.51.
Figure 2.52 illustrates (see page 78) a common error in which a nonprime implicant was chosen to cover the 1 in the upper left corner. This minterm,
, gives a sum-of-products equation that is not minimal. The minterm
could have been combined with either of the adjacent ones to form a larger circle, as was done in the previous two figures.
D3D2D1D0
D3D2D0
D2D1D0
2.7 Karnaugh Maps 75
Figure 2.46 Solution for
Example 2.9
Figure 2.47 Seven-segment
display decoder icon
00 01
Y
11 10
AB
1
1
0
0
1
0
1
1
0
1
C
AC
Y = AC + B
B
4 7
7-segment
display
decoder
a
b
c
d
g
e
f
D S

76 CHAPTER TWO Combinational Logic Design
Figure 2.49 Karnaugh maps for
Sa and Sb
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
D3:2 00
00
10 01 11
1
1
1
0
0
0
1
01 1
1
1
1
0
0
0
0
0
11
10
00
00
D 10 1:0
D3:2
D1:0
Sa Sb
D3:0 Sa Sb Sc Sd Se Sf Sg
0000 1 1 1 1 1 1 0
0001 0 1 1 0 0 0 0
0010 1 1 0 1 1 0 1
0011 1 1 1 1 0 0 1
0100 0 1 1 0 0 1 1
0101 1 0 1 1 0 1 1
0110 1 0 1 1 1 1 1
0111 1 1 1 0 0 0 0
1000 1 1 1 1 1 1 1
1001 1 1 1 0 0 1 1
others 0 0 0 0 0 0 0
Table 2.6 Seven-segment display decoder truth table
Figure 2.48 Seven-segment
display digits
0123456789

2.7.3 Don’t Cares
Recall that “don’t care” entries for truth table inputs were introduced
in Section 2.4 to reduce the number of rows in the table when some
variables do not affect the output. They are indicated by the symbol X,
which means that the entry can be either 0 or 1.
Don’t cares also appear in truth table outputs where the output
value is unimportant or the corresponding input combination can never
happen. Such outputs can be treated as either 0’s or 1’s at the designer’s
discretion.
2.7 Karnaugh Maps 77
Figure 2.50 K-map solution for Example 2.10
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sa = D3D1 + D3D2D0 + D3D2D1 + D2D1D0
D3D1
D3D2D0
D2D1D0
Sa
D3D2D1
01 11
1
1
1
0
0
0
1
01 1
1
1
1
0
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sb = D3D2 + D2D1 + D3D1D0 + D3D1D0
D3D2
D2D1
Sb
D3D1D0
D3D1D0
Figure 2.51 Alternative K-map
for Sa showing different set of
prime implicants
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sa = D3D1 + D3D2D0 + D3D2D1 + D3D2D0
D3D1
D3D2D0 D3D2D1
D3D2D0
Sa

In a K-map, X’s allow for even more logic minimization. They can
be circled if they help cover the 1’s with fewer or larger circles, but they
do not have to be circled if they are not helpful.
Example 2.11 SEVEN-SEGMENT DISPLAY DECODER WITH DON’T CARES
Repeat Example 2.10 if we don’t care about the output values for illegal input
values of 10 to 15.
Solution: The K-map is shown in Figure 2.53 with X entries representing don’t
care. Because don’t cares can be 0 or 1, we circle a don’t care if it allows us to
cover the 1’s with fewer or bigger circles. Circled don’t cares are treated as 1’s,
whereas uncircled don’t cares are 0’s. Observe how a 2  2 square wrapping
around all four corners is circled for segment Sa. Use of don’t cares simplifies the
logic substantially.
2.7.4 The Big Picture
Boolean algebra and Karnaugh maps are two methods for logic simplification. Ultimately, the goal is to find a low-cost method of implementing
a particular logic function.
In modern engineering practice, computer programs called logic
synthesizers produce simplified circuits from a description of the logic
function, as we will see in Chapter 4. For large problems, logic synthesizers are much more efficient than humans. For small problems, a
human with a bit of experience can find a good solution by inspection.
Neither of the authors has ever used a Karnaugh map in real life to
78 CHAPTER TWO Combinational Logic Design
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sa
D3D1
D3D2D0 D3D2D1
D3D2D1D0
Sa = D3D1 + D3D2D0 + D3D2D1 + D3D2D1D0
Figure 2.52 Alternative K-map
for Sa showing incorrect
nonprime implicant

solve a practical problem. But the insight gained from the principles
underlying Karnaugh maps is valuable. And Karnaugh maps often
appear at job interviews!
2.8 COMBINATIONAL BUILDING BLOCKS
Combinational logic is often grouped into larger building blocks to
build more complex systems. This is an application of the principle of
abstraction, hiding the unnecessary gate-level details to emphasize the
function of the building block. We have already studied three such
building blocks: full adders (from Section 2.1), priority circuits (from
Section 2.4), and seven-segment display decoders (from Section 2.7).
This section introduces two more commonly used building blocks:
multiplexers and decoders. Chapter 5 covers other combinational
building blocks.
2.8.1 Multiplexers
Multiplexers are among the most commonly used combinational circuits. They choose an output from among several possible inputs based
on the value of a select signal. A multiplexer is sometimes affectionately
called a mux.
2:1 Multiplexer
Figure 2.54 shows the schematic and truth table for a 2:1 multiplexer
with two data inputs, D0 and D1, a select input, S, and one output, Y.
The multiplexer chooses between the two data inputs based on the
select: if S  0, Y  D0, and if S  1, Y  D1. S is also called a control
signal because it controls what the multiplexer does.
2.8 Combinational Building Blocks 79
Figure 2.53 K-map solution
with don’t cares
01 11
1
0
0
1
X
X
1
01 1
1
1
1
1
X
X
X
X
11
10
00
00
10 01 11
1
1
1
0
X
X
1
01 1
1
1
1
0
X
X
X
X
11
10
00
00
10
D3:2
D1:0
Sa D3:2
D1:0
Sb
Sa = D1 + D3 + D2D0 + D2D0 Sb = D3 + D3D2 + D1D0 + D1D0
Figure 2.54 2:1 multiplexer
symbol and truth table
Y
0 0
0 1
1 0
1 1
0
1
0
1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
1
1
0
1
S
D0
Y
D1
S D1 D0
Chap
A 2:1 multiplexer can be built from sum-of-products logic as shown
in Figure 2.55. The Boolean equation for the multiplexer may be derived
with a Karnaugh map or read off by inspection (Y is 1 if S  0 AND D0
is 1 OR if S  1 AND D1 is 1).
Alternatively, multiplexers can be built from tristate buffers, as
shown in Figure 2.56. The tristate enables are arranged such that, at
all times, exactly one tristate buffer is active. When S  0, tristate T0
is enabled, allowing D0 to flow to Y. When S  1, tristate T1 is
enabled, allowing D1 to flow to Y.
Wider Multiplexers
A 4:1 multiplexer has four data inputs and one output, as shown
in Figure 2.57. Two select signals are needed to choose among the four
data inputs. The 4:1 multiplexer can be built using sum-of-products
logic, tristates, or multiple 2:1 multiplexers, as shown in Figure 2.58.
The product terms enabling the tristates can be formed using AND
gates and inverters. They can also be formed using a decoder, which we
will introduce in Section 2.8.2.
Wider multiplexers, such as 8:1 and 16:1 multiplexers, can be built
by expanding the methods shown in Figure 2.58. In general, an N:1
multiplexer needs log2N select lines. Again, the best implementation
choice depends on the target technology.
Multiplexer Logic
Multiplexers can be used as lookup tables to perform logic functions.
Figure 2.59 shows a 4:1 multiplexer used to implement a two-input
80 CHAPTER TWO Combinational Logic Design
D1
Y
D0
S
S 00 01
0
1
Y
11 10
D1:0
0
0
1
0
1
1
0
1
Y = D0S + D1S Figure 2.55 2:1 multiplexer
implementation using two-level
logic
Figure 2.56 Multiplexer using
tristate buffers
Figure 2.57 4:1 multiplexer
Y
D0
S
T0
T1
Y = D0S + D1S
D1
00
S1:0
D0
D1 Y 01
10
11
D2
D3
2
Shorting together the outputs
of multiple gates technically
violates the rules for combinational circuits given in Section
2.1. But because exactly one of
the outputs is driven at any
time, this exception is allowed.
Chap
AND gate. The inputs, A and B, serve as select lines. The multiplexer
data inputs are connected to 0 or 1 according to the corresponding row
of the truth table. In general, a 2N-input multiplexer can be programmed to perform any N-input logic function by applying 0’s and 1’s
to the appropriate data inputs. Indeed, by changing the data inputs, the
multiplexer can be reprogrammed to perform a different function.
With a little cleverness, we can cut the multiplexer size in half, using
only a 2N1-input multiplexer to perform any N-input logic function.
The strategy is to provide one of the literals, as well as 0’s and 1’s, to the
multiplexer data inputs.
To illustrate this principle, Figure 2.60 shows two-input AND and
XOR functions implemented with 2:1 multiplexers. We start with an
ordinary truth table, and then combine pairs of rows to eliminate the
rightmost input variable by expressing the output in terms of this
variable. For example, in the case of AND, when A  0, Y  0, regardless of B. When A  1, Y  0 if B  0 and Y  1 if B  1, so Y  B.
We then use the multiplexer as a lookup table according to the new,
smaller truth table.
Example 2.12 LOGIC WITH MULTIPLEXERS
Alyssa P. Hacker needs to implement the function to finish her senior project, but when she looks in her lab kit, the only part she has left
is an 8:1 multiplexer. How does she implement the function?
Solution: Figure 2.61 shows Alyssa’s implementation using a single 8:1 multiplexer. The multiplexer acts as a lookup table where each row in the truth table
corresponds to a multiplexer input.
Y  AB  BC  ABC
2.8 Combinational Building Blocks 81
Figure 2.58 4:1 multiplexer
implementations: (a) twolevel logic, (b) tristates,
(c) hierarchical
(a)
Y
D0
D1
D2
D3
(b) (c)
S0
Y
0
1
0
1
0
1
S1
D0
D1
D2
D3
Y
S1S0
S1S0
S1S0
S1S0
D0
D2
D3
D1
S1 S0
ABY
000
010
100
1 1 1
Y = AB
00
Y 01
10
11
A B
Figure 2.59 4:1 multiplexer
implementation of two-input
AND function
Chapter 02.
Example 2.13 LOGIC WITH MULTIPLEXERS, REPRISED
Alyssa turns on her circuit one more time before the final presentation and
blows up the 8:1 multiplexer. (She accidently powered it with 20 V instead of 5
V after not sleeping all night.) She begs her friends for spare parts and they
give her a 4:1 multiplexer and an inverter. Can she build her circuit with only
these parts?
Solution: Alyssa reduces her truth table to four rows by letting the output
depend on C. (She could also have chosen to rearrange the columns of the truth
table to let the output depend on A or B.) Figure 2.62 shows the new design.
2.8.2 Decoders
A decoder has N inputs and 2N outputs. It asserts exactly one of its
outputs depending on the input combination. Figure 2.63 shows a
2:4 decoder. When A1:0  00, Y0 is 1. When A1:0  01, Y1 is 1. And so
forth. The outputs are called one-hot, because exactly one is “hot”
(HIGH) at a given time.
82 CHAPTER TWO Combinational Logic Design
Figure 2.61 Alyssa’s circuit:
(a) truth table, (b) 8:1
multiplexer implementation
AB Y
0 0 1
010
100
1 1 1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
C
(a)
Y = AB + BC + ABC
A B C
(b)
000
001
010
011
100
101
110
111
Y
Figure 2.60 Multiplexer logic
using variable inputs
ABY
000
011
101
110
A Y
0
1
A
B
Y
1 B
Y = A ⊕ B
0 B B
(b)
ABY
000
010
100
111
Y = AB
A Y
0 0 0
1
A
B
Y
1 B
(a)
Ch
Example 2.14 DECODER IMPLEMENTATION
Implement a 2:4 decoder with AND, OR, and NOT gates.
Solution: Figure 2.64 shows an implementation for the 2:4 decoder using four
AND gates. Each gate depends on either the true or the complementary form of
each input. In general, an N:2N decoder can be constructed from 2N N-input AND
gates that accept the various combinations of true or complementary inputs. Each
output in a decoder represents a single minterm. For example, Y0 represents the
minterm . This fact will be handy when using decoders with other digital
building blocks.
A1A0
Decoder Logic
Decoders can be combined with OR gates to build logic functions.
Figure 2.65 shows the two-input XNOR function using a 2:4 decoder
and a single OR gate. Because each output of a decoder represents a
single minterm, the function is built as the OR of all the minterms in the
function. In Figure 2.65, Y  AB  AB  AB .
2.8 Combinational Building Blocks 83
AB Y
001
010
100
111
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
C
(a)
A Y
0 0
0 1
101
110
B
C
C
00
Y 01
10
11
A B
C
(b) (c)
Figure 2.62 Alyssa’s new circuit
Figure 2.63 2:4 decoder
2:4
Decoder
A1
A0
Y3
Y2
Y1
00 Y0
01
10
11
0 0
0 1
1 0
1 1
0
0
0
1
A1 A0 Y3 Y2 Y1 Y0
0
0
1
0
0
1
0
0
1
0
0
0
Figure 2.64 2:4 decoder implementation
A1 A0
Y3
Y2
Y1
Y0
Figure 2.65 Logic function using
decoder
2:4
Decoder
A
B
00
01
10
11
Y = A B
Y
AB
AB
AB
AB
Minterm
⊕
Chap
When using decoders to build logic, it is easiest to express functions
as a truth table or in canonical sum-of-products form. An N-input function with M 1’s in the truth table can be built with an N:2N decoder and
an M-input OR gate attached to all of the minterms containing 1’s in the
truth table. This concept will be applied to the building of Read Only
Memories (ROMs) in Section 5.5.6.
2.9 TIMING
In previous sections, we have been concerned primarily with whether the
circuit works—ideally, using the fewest gates. However, as any seasoned
circuit designer will attest, one of the most challenging issues in circuit
design is timing: making a circuit run fast.
An output takes time to change in response to an input change.
Figure 2.66 shows the delay between an input change and the
subsequent output change for a buffer. The figure is called a timing
diagram; it portrays the transient response of the buffer circuit when
an input changes. The transition from LOW to HIGH is called the
rising edge. Similarly, the transition from HIGH to LOW (not shown
in the figure) is called the falling edge. The blue arrow indicates
that the rising edge of Y is caused by the rising edge of A. We measure
delay from the 50% point of the input signal, A, to the 50% point
of the output signal, Y. The 50% point is the point at which the
signal is half-way (50%) between its LOW and HIGH values as it
transitions.
2.9.1 Propagation and Contamination Delay
Combinational logic is characterized by its propagation delay and
contamination delay. The propagation delay, tpd, is the maximum time
from when an input changes until the output or outputs reach their
final value. The contamination delay, tcd, is the minimum time from
when an input changes until any output starts to change its value.
84 CHAPTER TWO Combinational Logic Design
Figure 2.66 Circuit delay A
Y
Time
delay
A Y
When designers speak of calculating the delay of a circuit,
they generally are referring to
the worst-case value (the
propagation delay), unless it
is clear otherwise from the
context.

Figure 2.67 illustrates a buffer’s propagation delay and contamination delay in blue and gray, respectively. The figure shows that A is
initially either HIGH or LOW and changes to the other state at a particular time; we are interested only in the fact that it changes, not
what value it has. In response, Y changes some time later. The arcs
indicate that Y may start to change tcd after A transitions and that Y
definitely settles to its new value within tpd.
The underlying causes of delay in circuits include the time required
to charge the capacitance in a circuit and the speed of light. tpd and tcd
may be different for many reasons, including
 different rising and falling delays
 multiple inputs and outputs, some of which are faster than others
 circuits slowing down when hot and speeding up when cold
Calculating tpd and tcd requires delving into the lower levels of
abstraction beyond the scope of this book. However, manufacturers normally supply data sheets specifying these delays for each gate.
Along with the factors already listed, propagation and contamination delays are also determined by the path a signal takes from input to
output. Figure 2.68 shows a four-input logic circuit. The critical path,
shown in blue, is the path from input A or B to output Y. It is the
longest, and therefore the slowest, path, because the input travels
2.9 Timing 85
Figure 2.67 Propagation and
contamination delay
A Y
A
Y
Time
tpd
tcd
A
B
C
D Y
Critical Path
Short Path
n1
n2 Figure 2.68 Short path and
critical path
Circuit delays are ordinarily
on the order of picoseconds
(1 ps  1012 seconds) to
nanoseconds (1 ns  109
seconds). Trillions of picoseconds have elapsed in the time
you spent reading this sidebar.
Chapt
through three gates to the output. This path is critical because it limits
the speed at which the circuit operates. The short path through the circuit, shown in gray, is from input D to output Y. This is the shortest, and
therefore the fastest, path through the circuit, because the input travels
through only a single gate to the output.
The propagation delay of a combinational circuit is the sum of the
propagation delays through each element on the critical path. The
contamination delay is the sum of the contamination delays through
each element on the short path. These delays are illustrated in Figure
2.69 and are described by the following equations:
(2.7)
(2.8)
Example 2.15 FINDING DELAYS
Ben Bitdiddle needs to find the propagation delay and contamination delay of
the circuit shown in Figure 2.70. According to his data book, each gate has
a propagation delay of 100 picoseconds (ps) and a contamination delay
of 60 ps.
Solution: Ben begins by finding the critical path and the shortest path through
the circuit. The critical path, highlighted in blue in Figure 2.71, is from input A
tcd  tcdAND
tpd  2tpdAND  tpdOR
86 CHAPTER TWO Combinational Logic Design
A = 1 0
Y = 1 0
D
Y
delay
Time
A
Y
delay
A = 1
B = 1
C = 0
D = 1 0 Y = 1 0
Short Path
Critical Path
Time
n1
n2
n1
n2
n1
n2
B = 1
C = 0
D = 1
Figure 2.69 Critical and short path waveforms
Although we are ignoring wire
delay in this analysis, digital
circuits are now so fast that
the delay of long wires can be
as important as the delay of
the gates. The speed of light
delay in wires is covered in
Appendix A.
Cha
or B through three gates to the output, Y. Hence, tpd is three times the propagation delay of a single gate, or 300 ps.
The shortest path, shown in gray in Figure 2.72, is from input C, D, or E
through two gates to the output, Y. There are only two gates in the shortest path,
so tcd is 120 ps.
Example 2.16 MULTIPLEXER TIMING: CONTROL-CRITICAL
VS. DATA-CRITICAL
Compare the worst-case timing of the three four-input multiplexer designs
shown in Figure 2.58 in Section 2.8.1. Table 2.7 lists the propagation delays for
the components. What is the critical path for each design? Given your timing
analysis, why might you choose one design over the other?
Solution: One of the critical paths for each of the three design options is
highlighted in blue in Figures 2.73 and 2.74. tpd_sy indicates the propagation
delay from input S to output Y; tpd_dy indicates the propagation delay from
input D to output Y; tpd is the worst of the two: max(tpd_sy, tpd_dy).
For both the two-level logic and tristate implementations in Figure 2.73, the
critical path is from one of the control signals, S, to the output, Y: tpd  tpd_sy.
These circuits are control critical, because the critical path is from the control
signals to the output. Any additional delay in the control signals will add directly
to the worst-case delay. The delay from D to Y in Figure 2.73(b) is only 50 ps,
compared with the delay from S to Y of 125 ps.
2.9 Timing 87
A
B
C
D
E
Y Figure 2.70 Ben’s circuit
Figure 2.71 Ben’s critical path
A
B
C
D
E
Y
Figure 2.72 Ben’s shortest path
A
B
C
D
E
Y
C
Figure 2.74 shows the hierarchical implementation of the 4:1 multiplexer using
two stages of 2:1 multiplexers. The critical path is from any of the D inputs to
the output. This circuit is data critical, because the critical path is from the data
input to the output: (tpd  tpd_dy).
If data inputs arrive well before the control inputs, we would prefer the design
with the shortest control-to-output delay (the hierarchical design in Figure 2.74).
Similarly, if the control inputs arrive well before the data inputs, we would
prefer the design with the shortest data-to-output delay (the tristate design in
Figure 2.73(b)).
The best choice depends not only on the critical path through the circuit and
the input arrival times, but also on the power, cost, and availability of parts.
2.9.2 Glitches
So far we have discussed the case where a single input transition causes a
single output transition. However, it is possible that a single input transition can cause multiple output transitions. These are called glitches or
hazards. Although glitches usually don’t cause problems, it is important
to realize that they exist and recognize them when looking at timing diagrams. Figure 2.75 shows a circuit with a glitch and the Karnaugh map
of the circuit.
The Boolean equation is correctly minimized, but let’s look at what
happens when A  0, C  1, and B transitions from 1 to 0. Figure 2.76
(see page 90) illustrates this scenario. The short path (shown in gray)
goes through two gates, the AND and OR gates. The critical path (shown
in blue) goes through an inverter and two gates, the AND and OR gates.
88 CHAPTER TWO Combinational Logic Design
Gate tpd (ps)
NOT 30
2-input AND 60
3-input AND 80
4-input OR 90
tristate (A to Y) 50
tristate (enable to Y) 35
Table 2.7 Timing specifications for
multiplexer circuit elements
Hazards have another meaning related to microarchitecture in Chapter 7, so we will
stick with the term glitches
for multiple output transitions
to avoid confusion.
Cha
2.9 Timing 89
Figure 2.73 4:1 multiplexer
propagation delays:
(a) two-level logic,
(b) tristate
tpd_sy = tpd_INV + tpd_AND3 + tpd_OR4
= 30 ps + 80 ps + 90 ps
= 200 ps
S1
D0
D1
D2
D3
Out
S0
(a)
tpd_dy = tpd_AND3 + tpd_OR4
= 170 ps
D2
D3
Out
S1 S0
tpd_sy = tpd_INV + tpd_AND2 + tpd_TRI_SY
 = 30 ps + 60 ps + 35 ps
 = 125 ps (b)
tpd_dy = tpd_TRI_AY
= 50 ps
D0
D1
S0
D0
D1
D2
D3
S1
Y
tpd_s0y = tpd_TRLSY + tpd_TRI_AY = 85 ns
2:1 mux
2:1 mux
2:1 mux
tpd_dy = 2 tpd_TRI_AY = 100 ns
Figure 2.74 4:1 multiplexer propagation
delays: hierarchical using 2:1 multiplexers
As B transitions from 1 to 0, n2 (on the short path) falls before n1
(on the critical path) can rise. Until n1 rises, the two inputs to the OR
gate are 0, and the output Y drops to 0. When n1 eventually rises, Y
returns to 1. As shown in the timing diagram of Figure 2.76, Y starts at
1 and ends at 1 but momentarily glitches to 0.
A
B
C
Y
00 01
1
Y
11 10
AB
1
1
0
1
0
1
0
0
C
0
Y = AB + BC
Figure 2.75 Circuit with a glitch

90 CHAPTER TWO Combinational Logic Design
Figure 2.77 Input change
crosses implicant boundary
00 01
1
Y
11 10
AB
1
1
0
1
0
1
0
0
C
0
Y = AB + BC
Figure 2.76 Timing of a glitch
A = 0
C = 1
B = 1 0
Y = 1 0 1
Short Path
Critical Path
B
Y
Time
1 0
0 1
glitch
n1
n2
n2
n1
As long as we wait for the propagation delay to elapse before we
depend on the output, glitches are not a problem, because the output
eventually settles to the right answer.
If we choose to, we can avoid this glitch by adding another gate to
the implementation. This is easiest to understand in terms of the K-map.
Figure 2.77 shows how an input transition on B from ABC  001 to
ABC  011 moves from one prime implicant circle to another. The transition across the boundary of two prime implicants in the K-map
indicates a possible glitch.
As we saw from the timing diagram in Figure 2.76, if the circuitry
implementing one of the prime implicants turns off before the
circuitry of the other prime implicant can turn on, there is a glitch.
To fix this, we add another circle that covers that prime implicant
boundary, as shown in Figure 2.78. You might recognize this as the
consensus theorem, where the added term, , is the consensus or
redundant term.
AC
Ch
2.10 Summary 91
00 01
1
Y
11 10
AB
1
1
0
1
0
1
0
0
C
0
AC Y = AB + BC + AC
B = 1 0
Y = 1
A = 0
C = 1
Figure 2.78 K-map without
glitch
Figure 2.79 Circuit without
glitch
Figure 2.79 shows the glitch-proof circuit. The added AND gate is
highlighted in blue. Now a transition on B when A  0 and C  1 does
not cause a glitch on the output, because the blue AND gate outputs 1
throughout the transition.
In general, a glitch can occur when a change in a single variable
crosses the boundary between two prime implicants in a K-map. We can
eliminate the glitch by adding redundant implicants to the K-map
to cover these boundaries. This of course comes at the cost of extra
hardware.
However, simultaneous transitions on multiple variables can also
cause glitches. These glitches cannot be fixed by adding hardware.
Because the vast majority of interesting systems have simultaneous (or
near-simultaneous) transitions on multiple variables, glitches are a fact
of life in most circuits. Although we have shown how to eliminate one
kind of glitch, the point of discussing glitches is not to eliminate them
but to be aware that they exist. This is especially important when looking at timing diagrams on a simulator or oscilloscope.
2.10 SUMMARY
A digital circuit is a module with discrete-valued inputs and outputs and
a specification describing the function and timing of the module. This
chapter has focused on combinational circuits, circuits whose outputs
depend only on the current values of the inputs.
Ch
The function of a combinational circuit can be given by a truth
table or a Boolean equation. The Boolean equation for any truth table
can be obtained systematically using sum-of-products or product-ofsums form. In sum-of-products form, the function is written as the
sum (OR) of one or more implicants. Implicants are the product
(AND) of literals. Literals are the true or complementary forms of the
input variables.
Boolean equations can be simplified using the rules of Boolean algebra. In particular, they can be simplified into minimal sum-of-products
form by combining implicants that differ only in the true and complementary forms of one of the literals: . Karnaugh maps are
a visual tool for minimizing functions of up to four variables. With practice, designers can usually simplify functions of a few variables by
inspection. Computer-aided design tools are used for more complicated
functions; such methods and tools are discussed in Chapter 4.
Logic gates are connected to create combinational circuits that perform the desired function. Any function in sum-of-products form can be
built using two-level logic with the literals as inputs: NOT gates form
the complementary literals, AND gates form the products, and OR gates
form the sum. Depending on the function and the building blocks available, multilevel logic implementations with various types of gates may be
more efficient. For example, CMOS circuits favor NAND and NOR
gates because these gates can be built directly from CMOS transistors
without requiring extra NOT gates. When using NAND and NOR
gates, bubble pushing is helpful to keep track of the inversions.
Logic gates are combined to produce larger circuits such as multiplexers, decoders, and priority circuits. A multiplexer chooses one of the data
inputs based on the select input. A decoder sets one of the outputs HIGH
according to the input. A priority circuit produces an output indicating the
highest priority input. These circuits are all examples of combinational
building blocks. Chapter 5 will introduce more building blocks, including
other arithmetic circuits. These building blocks will be used extensively to
build a microprocessor in Chapter 7.
The timing specification of a combinational circuit consists of the
propagation and contamination delays through the circuit. These indicate the longest and shortest times between an input change and the
consequent output change. Calculating the propagation delay of a circuit
involves identifying the critical path through the circuit, then adding up
the propagation delays of each element along that path. There are many
different ways to implement complicated combinational circuits; these
ways offer trade-offs between speed and cost.
The next chapter will move to sequential circuits, whose outputs
depend on previous as well as current values of the inputs. In other
words, sequential circuits have memory of the past.
PA  PA  P
92 CHAPTER TWO Combinational Logic Design
Ch
Exercises 93
Exercises
Exercise 2.1 Write a Boolean equation in sum-of-products canonical form for
each of the truth tables in Figure 2.80.
Exercise 2.2 Write a Boolean equation in product-of-sums canonical form for
the truth tables in Figure 2.80.
Exercise 2.3 Minimize each of the Boolean equations from Exercise 2.1.
Exercise 2.4 Sketch a reasonably simple combinational circuit implementing
each of the functions from Exercise 2.3. Reasonably simple means that you are
not wasteful of gates, but you don’t waste vast amounts of time checking every
possible implementation of the circuit either.
Exercise 2.5 Repeat Exercise 2.4 using only NOT gates and AND and OR
gates.
Exercise 2.6 Repeat Exercise 2.4 using only NOT gates and NAND and NOR
gates.
Exercise 2.7 Simplify the following Boolean equations using Boolean theorems.
Check for correctness using a truth table or K-map.
(a)
(b)
(c) Y ABCD  ABC  ABCD  ABD ABCD  BCD  A
Y AB  ABC  (A  C)
Y  AC  ABC
BCY
0 0
0 1
1 0
1 1
1
0
1
0
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
1
BCY
0 0
0 1
1 0
1 1
1
0
0
0
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
1
ABY
0 0
0 1
1 0
1 1
1
0
1
1
CDY
0 0
0 1
1 0
1 1
1
1
1
1
B
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
A
0 0
0 1
1 0
1 1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
0
1
0
0
0
1
0
CDY
0 0
0 1
1 0
1 1
1
0
0
1
B
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
1
1
0
A
0 0
0 1
1 0
1 1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
0
1
1
0
1
0
0
1
(a) (b) (c) (d) (e)
Figure 2.80 Truth tables
Chapter 02.qx
94 CHAPTER TWO Combinational Logic Design
Exercise 2.8 Sketch a reasonably simple combinational circuit implementing
each of the functions from Exercise 2.7.
Exercise 2.9 Simplify each of the following Boolean equations. Sketch a reasonably simple combinational circuit implementing the simplified equation.
(a)
(b)
(c) Y  ABC  ABD  ABE  ACD  ACE 

Exercise 2.10 Give an example of a truth table requiring between 3 billion and
5 billion rows that can be constructed using fewer than 40 (but at least 1)
two-input gates.
Exercise 2.11 Give an example of a circuit with a cyclic path that is nevertheless
combinational.
Exercise 2.12 Alyssa P. Hacker says that any Boolean function can be written in
minimal sum-of-products form as the sum of all of the prime implicants of the
function. Ben Bitdiddle says that there are some functions whose minimal equation does not involve all of the prime implicants. Explain why Alyssa is right or
provide a counterexample demonstrating Ben’s point.
Exercise 2.13 Prove that the following theorems are true using perfect induction.
You need not prove their duals.
(a) The idempotency theorem (T3)
(b) The distributivity theorem (T8)
(c) The combining theorem (T10)
Exercise 2.14 Prove De Morgan’s Theorem (T12) for three variables, B2, B1, B0,
using perfect induction.
Exercise 2.15 Write Boolean equations for the circuit in Figure 2.81. You need
not minimize the equations.
BCE BDE CDE
(A  D  E) BCD
Y  A AB AB  A  B
Y  BC  ABC  BC
Chapter 02.qxd 1/31/
Exercises 95
Exercise 2.16 Minimize the Boolean equations from Exercise 2.15 and sketch an
improved circuit with the same function.
Exercise 2.17 Using De Morgan equivalent gates and bubble pushing methods,
redraw the circuit in Figure 2.82 so that you can find the Boolean equation by
inspection. Write the Boolean equation.
Exercise 2.18 Repeat Exercise 2.17 for the circuit in Figure 2.83.
A B C D
Y Z
A
B
C
D
E Y
Figure 2.81 Circuit schematic
Figure 2.82 Circuit schematic

96 CHAPTER TWO Combinational Logic Design
Exercise 2.19 Find a minimal Boolean equation for the function in Figure 2.84.
Remember to take advantage of the don’t care entries.
Exercise 2.20 Sketch a circuit for the function from Exercise 2.19.
Exercise 2.21 Does your circuit from Exercise 2.20 have any potential glitches
when one of the inputs changes? If not, explain why not. If so, show how to
modify the circuit to eliminate the glitches.
Exercise 2.22 Ben Bitdiddle will enjoy his picnic on sunny days that have no
ants. He will also enjoy his picnic any day he sees a hummingbird, as well as on
days where there are ants and ladybugs. Write a Boolean equation for his enjoyment (E) in terms of sun (S), ants (A), hummingbirds (H), and ladybugs (L).
CDY
00X
01X
10X
110
B
0 0
0 1
1 0
1 1
0
X
0
X
0
0
0
0
1
1
1
1
A
0
0
0
0
0
0
0
0
001
010
10X
111
0 0
0 1
1 0
1 1
1
1
X
1
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
Figure 2.84 Truth table
A
B
C
D
E
F
G
Y
Figure 2.83 Circuit schematic

Exercises 97
Exercise 2.23 Complete the design of the seven-segment decoder segments Sc
through Sg (see Example 2.10):
(a) Derive Boolean equations for the outputs Sc through Sg assuming that inputs
greater than 9 must produce blank (0) outputs.
(b) Derive Boolean equations for the outputs Sc through Sg assuming that inputs
greater than 9 are don’t cares.
(c) Sketch a reasonably simple gate-level implementation of part (b). Multiple
outputs can share gates where appropriate.
Exercise 2.24 A circuit has four inputs and two outputs. The inputs, A3:0, represent a number from 0 to 15. Output P should be TRUE if the number is prime
(0 and 1 are not prime, but 2, 3, 5, and so on, are prime). Output D should be
TRUE if the number is divisible by 3. Give simplified Boolean equations for each
output and sketch a circuit.
Exercise 2.25 A priority encoder has 2N inputs. It produces an N-bit binary output indicating the most significant bit of the input that is TRUE, or 0 if none of
the inputs are TRUE. It also produces an output NONE that is TRUE if none of
the input bits are TRUE. Design an eight-input priority encoder with inputs A7:0
and outputs Y2:0 and NONE. For example, if the input is 00100000, the output
Y should be 101 and NONE should be 0. Give a simplified Boolean equation for
each output, and sketch a schematic.
Exercise 2.26 Design a modified priority encoder (see Exercise 2.25) that
receives an 8-bit input, A7:0, and produces two 3-bit outputs, Y2:0 and Z2:0.
Y indicates the most significant bit of the input that is TRUE. Z indicates the
second most significant bit of the input that is TRUE. Y should be 0 if none of
the inputs are TRUE. Z should be 0 if no more than one of the inputs is TRUE.
Give a simplified Boolean equation for each output, and sketch a schematic.
Exercise 2.27 An M-bit thermometer code for the number k consists of k 1’s in
the least significant bit positions and M  k 0’s in all the more significant bit
positions. A binary-to-thermometer code converter has N inputs and 2N1
outputs. It produces a 2N1 bit thermometer code for the number specified by
the input. For example, if the input is 110, the output should be 0111111.
Design a 3:7 binary-to-thermometer code converter. Give a simplified Boolean
equation for each output, and sketch a schematic.
Exercise 2.28 Write a minimized Boolean equation for the function performed
by the circuit in Figure 2.85.

98 CHAPTER TWO Combinational Logic Design
Exercise 2.30 Implement the function from Figure 2.80(b) using
(a) an 8:1 multiplexer
(b) a 4:1 multiplexer and one inverter
(c) a 2:1 multiplexer and two other logic gates
Exercise 2.31 Implement the function from Exercise 2.9(a) using
(a) an 8:1 multiplexer
(b) a 4:1 multiplexer and no other gates
(c) a 2:1 multiplexer, one OR gate, and an inverter
Exercise 2.32 Determine the propagation delay and contamination delay of the
circuit in Figure 2.83. Use the gate delays given in Table 2.8.
Exercise 2.29 Write a minimized Boolean equation for the function performed
by the circuit in Figure 2.86.
Figure 2.85 Multiplexer circuit
0
1
00
C, D
01
10
11
A
Y
Figure 2.86 Multiplexer circuit
00
C, D
01
10
11
Y
00
A, B
01
10
11

Exercises 99
Exercise 2.33 Sketch a schematic for a fast 3:8 decoder. Suppose gate delays are
given in Table 2.8 (and only the gates in that table are available). Design your
decoder to have the shortest possible critical path, and indicate what that path is.
What are its propagation delay and contamination delay?
Exercise 2.34 Redesign the circuit from Exercise 2.24 to be as fast as possible.
Use only the gates from Table 2.8. Sketch the new circuit and indicate the critical
path. What are its propagation delay and contamination delay?
Exercise 2.35 Redesign the priority encoder from Exercise 2.25 to be as fast
as possible. You may use any of the gates from Table 2.8. Sketch the new
circuit and indicate the critical path. What are its propagation delay and
contamination delay?
Exercise 2.36 Design an 8:1 multiplexer with the shortest possible delay from
the data inputs to the output. You may use any of the gates from Table 2.7 on
page 88. Sketch a schematic. Using the gate delays from the table, determine
this delay.
Gate tpd (ps) tcd (ps)
NOT 15 10
2-input NAND 20 15
3-input NAND 30 25
2-input NOR 30 25
3-input NOR 45 35
2-input AND 30 25
3-input AND 40 30
2-input OR 40 30
3-input OR 55 45
2-input XOR 60 40
Table 2.8 Gate delays for Exercises 2.32–2.35

100 CHAPTER TWO Combinational Logic Design
Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 2.1 Sketch a schematic for the two-input XOR function using only
NAND gates. How few can you use?
Question 2.2 Design a circuit that will tell whether a given month has 31 days
in it. The month is specified by a 4-bit input, A3:0. For example, if the inputs
are 0001, the month is January, and if the inputs are 1100, the month is
December. The circuit output, Y, should be HIGH only when the month specified by the inputs has 31 days in it. Write the simplified equation, and draw
the circuit diagram using a minimum number of gates. (Hint: Remember to
take advantage of don’t cares.)
Question 2.3 What is a tristate buffer? How and why is it used?
Question 2.4 A gate or set of gates is universal if it can be used to construct any
Boolean function. For example, the set {AND, OR, NOT} is universal.
(a) Is an AND gate by itself universal? Why or why not?
(b) Is the set {OR, NOT} universal? Why or why not?
(c) Is a NAND gate by itself universal? Why or why not?
Question 2.5 Explain why a circuit’s contamination delay might be less than
(instead of equal to) its propagation delay.



3
3.1 Introduction
3.2 Latches and Flip-Flops
3.3 Synchronous Logic Design
3.4 Finite State Machines
3.5 Timing of Sequential Logic
3.6 Parallelism
3.7 Summary
Exercises
Interview Questions
Sequential Logic Design
3.1 INTRODUCTION
In the last chapter, we showed how to analyze and design combinational
logic. The output of combinational logic depends only on current input
values. Given a specification in the form of a truth table or Boolean
equation, we can create an optimized circuit to meet the specification.
In this chapter, we will analyze and design sequential logic. The outputs of sequential logic depend on both current and prior input values.
Hence, sequential logic has memory. Sequential logic might explicitly
remember certain previous inputs, or it might distill the prior inputs into
a smaller amount of information called the state of the system. The state
of a digital sequential circuit is a set of bits called state variables that
contain all the information about the past necessary to explain the future
behavior of the circuit.
The chapter begins by studying latches and flip-flops, which are
simple sequential circuits that store one bit of state. In general, sequential
circuits are complicated to analyze. To simplify design, we discipline
ourselves to build only synchronous sequential circuits consisting of combinational logic and banks of flip-flops containing the state of the circuit.
The chapter describes finite state machines, which are an easy way to
design sequential circuits. Finally, we analyze the speed of sequential
circuits and discuss parallelism as a way to increase clock speed.
3.2 LATCHES AND FLIP-FLOPS
The fundamental building block of memory is a bistable element, an
element with two stable states. Figure 3.1(a) shows a simple bistable
element consisting of a pair of inverters connected in a loop. Figure
3.1(b) shows the same circuit redrawn to emphasize the symmetry. The
inverters are cross-coupled, meaning that the input of I1 is the output of
I2 and vice versa. The circuit has no inputs, but it does have two outputs,
103

Q and . Analyzing this circuit is different from analyzing a combinational circuit because it is cyclic: Q depends on , and depends on Q.
Consider the two cases, Q is 0 or Q is 1. Working through the
consequences of each case, we have:
 Case I: Q  0
As shown in Figure 3.2(a), I2 receives a FALSE input, Q, so it produces a TRUE output on . I1 receives a TRUE input, , so it
produces a FALSE output on Q. This is consistent with the original
assumption that Q  0, so the case is said to be stable.
 Case II: Q  1
As shown in Figure 3.2(b), I2 receives a TRUE input and produces a
FALSE output on . I1 receives a FALSE input and produces a TRUE
output on Q. This is again stable.
Because the cross-coupled inverters have two stable states, Q  0
and Q  1, the circuit is said to be bistable. A subtle point is that the
circuit has a third possible state with both outputs approximately
halfway between 0 and 1. This is called a metastable state and will be
discussed in Section 3.5.4.
An element with N stable states conveys log2N bits of information,
so a bistable element stores one bit. The state of the cross-coupled
inverters is contained in one binary state variable, Q. The value of Q
tells us everything about the past that is necessary to explain the future
behavior of the circuit. Specifically, if Q  0, it will remain 0 forever,
and if Q  1, it will remain 1 forever. The circuit does have another
node, , but does not contain any additional information because if
Q is known, is also known. On the other hand, is also an acceptable choice for the state variable.
Q Q
Q Q
Q
Q Q
Q Q
Q
104 CHAPTER THREE Sequential Logic Design
(b)
Q
Q
I1
I2
Q Q
(a)
I2 I1
Figure 3.1 Cross-coupled
inverter pair
(b)
Q
Q
I1
I2
1
0
0
1
(a)
I1 Q
I2
0
1 Q
1
0
Figure 3.2 Bistable operation
of cross-coupled inverters
Just as Y is commonly used
for the output of combinational logic, Q is commonly
used for the output of sequential logic.
Chapter 0
When power is first applied to a sequential circuit, the initial state is
unknown and usually unpredictable. It may differ each time the circuit is
turned on.
Although the cross-coupled inverters can store a bit of information,
they are not practical because the user has no inputs to control the state.
However, other bistable elements, such as latches and flip-flops, provide
inputs to control the value of the state variable. The remainder of this
section considers these circuits.
3.2.1 SR Latch
One of the simplest sequential circuits is the SR latch, which is
composed of two cross-coupled NOR gates, as shown in Figure 3.3.
The latch has two inputs, S and R, and two outputs, Q and . The
SR latch is similar to the cross-coupled inverters, but its state can
be controlled through the S and R inputs, which set and reset the
output Q.
A good way to understand an unfamiliar circuit is to work out its
truth table, so that is where we begin. Recall that a NOR gate produces
a FALSE output when either input is TRUE. Consider the four possible
combinations of R and S.
 Case I: R  1, S  0
N1 sees at least one TRUE input, R, so it produces a FALSE output
on Q. N2 sees both Q and S FALSE, so it produces a TRUE
output on .
 Case II: R  0, S  1
N1 receives inputs of 0 and . Because we don’t yet know , we
can’t determine the output Q. N2 receives at least one TRUE input,
S, so it produces a FALSE output on . Now we can revisit N1,
knowing that both inputs are FALSE, so the output Q is TRUE.
 Case III: R  1, S  1
N1 and N2 both see at least one TRUE input (R or S), so each
produces a FALSE output. Hence Q and are both FALSE.
 Case IV: R  0, S  0
N1 receives inputs of 0 and . Because we don’t yet know , we
can’t determine the output. N2 receives inputs of 0 and Q. Because
we don’t yet know Q, we can’t determine the output. Now we are
stuck. This is reminiscent of the cross-coupled inverters. But we
know that Q must either be 0 or 1. So we can solve the problem by
checking what happens in each of these subcases.
Q Q
Q
Q
Q Q
Q
Q
3.2 Latches and Flip-Flops 105
R
S
N1 Q
N2 Q
Figure 3.3 SR latch schematic
Chapter 03.q
 Case IVa: Q  0
Because S and Q are FALSE, N2 produces a TRUE output on , as
shown in Figure 3.4(a). Now N1 receives one TRUE input, , so its
output, Q, is FALSE, just as we had assumed.
 Case IVb: Q  1
Because Q is TRUE, N2 produces a FALSE output on , as shown
in Figure 3.4(b). Now N1 receives two FALSE inputs, R and , so
its output, Q, is TRUE, just as we had assumed.
Putting this all together, suppose Q has some known prior value,
which we will call Qprev, before we enter Case IV. Qprev is either 0 or 1,
and represents the state of the system. When R and S are 0, Q will
remember this old value, Qprev, and will be its complement, .
This circuit has memory.
The truth table in Figure 3.5 summarizes these four cases. The
inputs S and R stand for Set and Reset. To set a bit means to make it
TRUE. To reset a bit means to make it FALSE. The outputs, Q and ,
are normally complementary. When R is asserted, Q is reset to 0 and
does the opposite. When S is asserted, Q is set to 1 and does the
opposite. When neither input is asserted, Q remembers its old value,
Qprev. Asserting both S and R simultaneously doesn’t make much sense
because it means the latch should be set and reset at the same time,
which is impossible. The poor confused circuit responds by making both
outputs 0.
The SR latch is represented by the symbol in Figure 3.6. Using the
symbol is an application of abstraction and modularity. There are various ways to build an SR latch, such as using different logic gates or transistors. Nevertheless, any circuit element with the relationship specified
by the truth table in Figure 3.5 and the symbol in Figure 3.6 is called an
SR latch.
Like the cross-coupled inverters, the SR latch is a bistable element
with one bit of state stored in Q. However, the state can be controlled
through the S and R inputs. When R is asserted, the state is reset to 0.
When S is asserted, the state is set to 1. When neither is asserted, the
state retains its old value. Notice that the entire history of inputs can be
Q
Q
Q
Q Qprev
Q
Q
Q
Q
106 CHAPTER THREE Sequential Logic Design
R
S
N1 Q
N2
0
0
(b)
1
0 1
0
Q
R
S
N1 Q
N2
0
0
(a)
0
1 0
1
Q
Figure 3.4 Bistable states of SR
latch
SRQ
0 0 Qprev
010
101
110
1
0
0
Case
IV
I
II
III
Q
Qprev
Figure 3.5 SR latch truth table
S
R Q
Q
Figure 3.6 SR latch symbol
Cha
accounted for by the single state variable Q. No matter what pattern of
setting and resetting occurred in the past, all that is needed to predict
the future behavior of the SR latch is whether it was most recently set
or reset.
3.2.2 D Latch
The SR latch is awkward because it behaves strangely when both S and
R are simultaneously asserted. Moreover, the S and R inputs conflate
the issues of what and when. Asserting one of the inputs determines
not only what the state should be but also when it should change.
Designing circuits becomes easier when these questions of what and
when are separated. The D latch in Figure 3.7(a) solves these problems.
It has two inputs. The data input, D, controls what the next state
should be. The clock input, CLK, controls when the state should
change.
Again, we analyze the latch by writing the truth table, given in
Figure 3.7(b). For convenience, we first consider the internal nodes , S,
and R. If CLK  0, both S and R are FALSE, regardless of the value
of D. If CLK  1, one AND gate will produce TRUE and the other
FALSE, depending on the value of D. Given S and R, Q and are determined using Figure 3.5. Observe that when CLK  0, Q remembers its
old value, Qprev. When CLK  1, Q  D. In all cases, is the complement of Q, as would seem logical. The D latch avoids the strange case of
simultaneously asserted R and S inputs.
Putting it all together, we see that the clock controls when data
flows through the latch. When CLK  1, the latch is transparent. The
data at D flows through to Q as if the latch were just a buffer. When
CLK  0, the latch is opaque. It blocks the new data from flowing
through to Q, and Q retains the old value. Hence, the D latch is sometimes called a transparent latch or a level-sensitive latch. The D latch
symbol is given in Figure 3.7(c).
The D latch updates its state continuously while CLK  1. We shall
see later in this chapter that it is useful to update the state only at a specific instant in time. The D flip-flop described in the next section does
just that.
Q
Q
D
3.2 Latches and Flip-Flops 107
S
R Q Q
D
CLK D R
S
(a)
Q Q
CLK
D Q
(c)
Q
S R Q
0 0 Qprev
0 1 0
1 0 1
1
0
CLK D
0 X
1 0
1 1
D
X
1
0
(b)
Qprev
Q
Figure 3.7 D latch: (a) schematic, (b) truth table, (c) symbol
Some people call a latch open
or closed rather than transparent or opaque. However,
we think those terms are
ambiguous—does open mean
transparent like an open
door, or opaque, like an open
circuit?
Chapter 
3.2.3 D Flip-Flop
A D flip-flop can be built from two back-to-back D latches controlled by
complementary clocks, as shown in Figure 3.8(a). The first latch, L1, is
called the master. The second latch, L2, is called the slave. The node
between them is named N1. A symbol for the D flip-flop is given in
Figure 3.8(b). When the output is not needed, the symbol is often
condensed as in Figure 3.8(c).
When CLK  0, the master latch is transparent and the slave is
opaque. Therefore, whatever value was at D propagates through to N1.
When CLK  1, the master goes opaque and the slave becomes transparent. The value at N1 propagates through to Q, but N1 is cut off from
D. Hence, whatever value was at D immediately before the clock rises
from 0 to 1 gets copied to Q immediately after the clock rises. At all
other times, Q retains its old value, because there is always an opaque
latch blocking the path between D and Q.
In other words, a D flip-flop copies D to Q on the rising edge of the
clock, and remembers its state at all other times. Reread this definition
until you have it memorized; one of the most common problems for
beginning digital designers is to forget what a flip-flop does. The rising
edge of the clock is often just called the clock edge for brevity. The D
input specifies what the new state will be. The clock edge indicates when
the state should be updated.
A D flip-flop is also known as a master-slave flip-flop, an edge-triggered
flip-flop, or a positive edge-triggered flip-flop. The triangle in the symbols
denotes an edge-triggered clock input. The output is often omitted when
it is not needed.
Example 3.1 FLIP-FLOP TRANSISTOR COUNT
How many transistors are needed to build the D flip-flop described in this section?
Solution: A NAND or NOR gate uses four transistors. A NOT gate uses two
transistors. An AND gate is built from a NAND and a NOT, so it uses six
transistors. The SR latch uses two NOR gates, or eight transistors. The D
latch uses an SR latch, two AND gates, and a NOT gate, or 22 transistors.
The D flip-flop uses two D latches and a NOT gate, or 46 transistors. Section
3.2.7 describes a more efficient CMOS implementation using transmission
gates.
3.2.4 Register
An N-bit register is a bank of N flip-flops that share a common CLK
input, so that all bits of the register are updated at the same time.
Q
Q
108 CHAPTER THREE Sequential Logic Design
(a)
CLK
D Q
CLK
D D Q Q N1
CLK
L1 L2
master slave
(b)
D Q
(c)
Q Q Q
Q
Figure 3.8 D flip-flop:
(a) schematic, (b) symbol,
(c) condensed symbol
The precise distinction
between flip-flops and latches
is somewhat muddled and has
evolved over time. In common
industry usage, a flip-flop is
edge-triggered. In other
words, it is a bistable element
with a clock input. The state
of the flip-flop changes only
in response to a clock edge,
such as when the clock rises
from 0 to 1. Bistable elements
without an edge-triggered
clock are commonly called
latches.
The term flip-flop or latch
by itself usually refers to a D
flip-flop or D latch, respectively, because these are the
types most commonly used in
practice.
Ch
Registers are the key building block of most sequential circuits. Figure
3.9 shows the schematic and symbol for a four-bit register with inputs
D3:0 and outputs Q3:0. D3:0 and Q3:0 are both 4-bit busses.
3.2.5 Enabled Flip-Flop
An enabled flip-flop adds another input called EN or ENABLE to
determine whether data is loaded on the clock edge. When EN is
TRUE, the enabled flip-flop behaves like an ordinary D flip-flop.
When EN is FALSE, the enabled flip-flop ignores the clock and
retains its state. Enabled flip-flops are useful when we wish to load a
new value into a flip-flop only some of the time, rather than on every
clock edge.
Figure 3.10 shows two ways to construct an enabled flip-flop from a
D flip-flop and an extra gate. In Figure 3.10(a), an input multiplexer
chooses whether to pass the value at D, if EN is TRUE, or to recycle the
old state from Q, if EN is FALSE. In Figure 3.10(b), the clock is gated.
3.2 Latches and Flip-Flops 109
CLK
D Q
D Q
D Q
D Q
D3
D2
D1
D0
Q3
Q2
Q1
Q0
(a)
D3:0 Q3:0
4 4
CLK
(b)
Figure 3.9 A 4-bit register:
(a) schematic and (b) symbol
(b)
D Q
CLK EN
D Q D Q
EN
(a) (c)
D Q
EN CLK
D
Q
0
1
Figure 3.10 Enabled flip-flop:
(a, b) schematics, (c) symbol

If EN is TRUE, the CLK input to the flip-flop toggles normally. If EN is
FALSE, the CLK input is also FALSE and the flip-flop retains its old
value. Notice that EN must not change while CLK  1, lest the flip-flop
see a clock glitch (switch at an incorrect time). Generally, performing
logic on the clock is a bad idea. Clock gating delays the clock and can
cause timing errors, as we will see in Section 3.5.3, so do it only if you
are sure you know what you are doing. The symbol for an enabled flipflop is given in Figure 3.10(c).
3.2.6 Resettable Flip-Flop
A resettable flip-flop adds another input called RESET. When RESET
is FALSE, the resettable flip-flop behaves like an ordinary D flip-flop.
When RESET is TRUE, the resettable flip-flop ignores D and resets the
output to 0. Resettable flip-flops are useful when we want to force a
known state (i.e., 0) into all the flip-flops in a system when we first
turn it on.
Such flip-flops may be synchronously or asynchronously resettable.
Synchronously resettable flip-flops reset themselves only on the rising
edge of CLK. Asynchronously resettable flip-flops reset themselves as
soon as RESET becomes TRUE, independent of CLK.
Figure 3.11(a) shows how to construct a synchronously resettable
flip-flop from an ordinary D flip-flop and an AND gate. When
is FALSE, the AND gate forces a 0 into the input of the flip-flop. When
is TRUE, the AND gate passes D to the flip-flop. In this
example, is an active low signal, meaning that the reset signal
performs its function when it is 0, not 1. By adding an inverter, the
circuit could have accepted an active high RESET signal instead.
Figures 3.11(b) and 3.11(c) show symbols for the resettable flip-flop
with active high RESET.
Asynchronously resettable flip-flops require modifying the internal
structure of the flip-flop and are left to you to design in Exercise 3.10;
however, they are frequently available to the designer as a standard
component.
As you might imagine, settable flip-flops are also occasionally
used. They load a 1 into the flip-flop when SET is asserted, and they
too come in synchronous and asynchronous flavors. Resettable and
settable flip-flops may also have an enable input and may be grouped
into N-bit registers.
3.2.7 Transistor-Level Latch and Flip-Flop Designs*
Example 3.1 showed that latches and flip-flops require a large number
of transistors when built from logic gates. But the fundamental role of a
RESET
RESET
RESET
110 CHAPTER THREE Sequential Logic Design
(a)
D Q
CLK
D Q RESET
D Q
RESET
(b) (c)
r
Figure 3.11 Synchronously
resettable flip-flop:
(a) schematic, (b, c) symbols
C
latch is to be transparent or opaque, much like a switch. Recall from
Section 1.7.7 that a transmission gate is an efficient way to build a
CMOS switch, so we might expect that we could take advantage of
transmission gates to reduce the transistor count.
A compact D latch can be constructed from a single transmission gate,
as shown in Figure 3.12(a). When CLK  1 and , the transmission gate is ON, so D flows to Q and the latch is transparent. When CLK
 0 and , the transmission gate is OFF, so Q is isolated from D
and the latch is opaque. This latch suffers from two major limitations:
 Floating output node: When the latch is opaque, Q is not held at its
value by any gates. Thus Q is called a floating or dynamic node. After
some time, noise and charge leakage may disturb the value of Q.
 No buffers: The lack of buffers has caused malfunctions on several commercial chips. A spike of noise that pulls D to a negative
voltage can turn on the nMOS transistor, making the latch transparent, even when CLK  0. Likewise, a spike on D above VDD
can turn on the pMOS transistor even when CLK  0. And the
transmission gate is symmetric, so it could be driven backward
with noise on Q affecting the input D. The general rule is that
neither the input of a transmission gate nor the state node of a
sequential circuit should ever be exposed to the outside world,
where noise is likely.
Figure 3.12(b) shows a more robust 12-transistor D latch used on
modern commercial chips. It is still built around a clocked transmission
gate, but it adds inverters I1 and I2 to buffer the input and output. The
state of the latch is held on node N1. Inverter I3 and the tristate buffer,
T1, provide feedback to turn N1 into a static node. If a small amount of
noise occurs on N1 while CLK  0, T1 will drive N1 back to a valid
logic value.
Figure 3.13 shows a D flip-flop constructed from two static latches
controlled by and CLK. Some redundant internal inverters have
been removed, so the flip-flop requires only 20 transistors.
CLK
CLK1
CLK 0
3.2 Latches and Flip-Flops 111
(a)
CLK
D Q
CLK
CLK
D
Q
N1
(b)
CLK
CLK
CLK
I1
I2
I3
T1
Figure 3.12 D latch schematic
This circuit assumes CLK and
are both available. If
not, two more transistors are
needed for a CLK inverter.
CLK
CLK
D N1
CLK
CLK
CLK
I1 I2
T1
I3
CLK
CLK
T2
CLK
CLK
I4 Q N2
Figure 3.13 D flip-flop
schematic
Chapter 0
3.2.8 Putting It All Together
Latches and flip-flops are the fundamental building blocks of sequential
circuits. Remember that a D latch is level-sensitive, whereas a D flip-flop
is edge-triggered. The D latch is transparent when CLK  1, allowing
the input D to flow through to the output Q. The D flip-flop copies D to
Q on the rising edge of CLK. At all other times, latches and flip-flops
retain their old state. A register is a bank of several D flip-flops that
share a common CLK signal.
Example 3.2 FLIP-FLOP AND LATCH COMPARISON
Ben Bitdiddle applies the D and CLK inputs shown in Figure 3.14 to a D latch
and a D flip-flop. Help him determine the output, Q, of each device.
Solution: Figure 3.15 shows the output waveforms, assuming a small delay for
Q to respond to input changes. The arrows indicate the cause of an output
change. The initial value of Q is unknown and could be 0 or 1, as indicated by
the pair of horizontal lines. First consider the latch. On the first rising edge of
CLK, D  0, so Q definitely becomes 0. Each time D changes while CLK  1,
Q also follows. When D changes while CLK  0, it is ignored. Now consider
the flip-flop. On each rising edge of CLK, D is copied to Q. At all other times,
Q retains its state.
112 CHAPTER THREE Sequential Logic Design
CLK
D
Q (latch)
Q (flop)
Figure 3.14 Example waveforms
CLK
D
Q (latch)
Q (flop)
Figure 3.15 Solution waveforms
Chap
3.3 SYNCHRONOUS LOGIC DESIGN
In general, sequential circuits include all circuits that are not combinational—that is, those whose output cannot be determined simply by looking at the current inputs. Some sequential circuits are just plain kooky. This
section begins by examining some of those curious circuits. It then introduces the notion of synchronous sequential circuits and the dynamic discipline. By disciplining ourselves to synchronous sequential circuits, we can
develop easy, systematic ways to analyze and design sequential systems.
3.3.1 Some Problematic Circuits
Example 3.3 ASTABLE CIRCUITS
Alyssa P. Hacker encounters three misbegotten inverters who have tied themselves in a loop, as shown in Figure 3.16. The output of the third inverter is fed
back to the first inverter. Each inverter has a propagation delay of 1 ns.
Determine what the circuit does.
Solution: Suppose node X is initially 0. Then Y  1, Z  0, and hence X  1,
which is inconsistent with our original assumption. The circuit has no stable
states and is said to be unstable or astable. Figure 3.17 shows the behavior of the
circuit. If X rises at time 0, Y will fall at 1 ns, Z will rise at 2 ns, and X will fall
again at 3 ns. In turn, Y will rise at 4 ns, Z will fall at 5 ns, and X will rise again
at 6 ns, and then the pattern will repeat. Each node oscillates between 0 and 1
with a period (repetition time) of 6 ns. This circuit is called a ring oscillator.
The period of the ring oscillator depends on the propagation delay of each
inverter. This delay depends on how the inverter was manufactured, the power
supply voltage, and even the temperature. Therefore, the ring oscillator period is
difficult to accurately predict. In short, the ring oscillator is a sequential circuit
with zero inputs and one output that changes periodically.
3.3 Synchronous Logic Design 113
X Y Z
Figure 3.16 Three-inverter loop
X
Y
Z
012345678 time (ns)
Figure 3.17 Ring oscillator
waveforms
Example 3.4 RACE CONDITIONS
Ben Bitdiddle designed a new D latch that he claims is better than the one in
Figure 3.17 because it uses fewer gates. He has written the truth table to find
Cha
the output, Q, given the two inputs, D and CLK, and the old state of the latch,
Qprev. Based on this truth table, he has derived Boolean equations. He obtains
Qprev by feeding back the output, Q. His design is shown in Figure 3.18. Does
his latch work correctly, independent of the delays of each gate?
Solution: Figure 3.19 shows that the circuit has a race condition that causes
it to fail when certain gates are slower than others. Suppose CLK  D  1.
The latch is transparent and passes D through to make Q  1. Now, CLK
falls. The latch should remember its old value, keeping Q  1. However, suppose the delay through the inverter from CLK to is rather long compared to the delays of the AND and OR gates. Then nodes N1 and Q may
both fall before rises. In such a case, N2 will never rise, and Q becomes
stuck at 0.
This is an example of asynchronous circuit design in which outputs are directly
fed back to inputs. Asynchronous circuits are infamous for having race conditions where the behavior of the circuit depends on which of two paths through
logic gates is fastest. One circuit may work, while a seemingly identical one built
from gates with slightly different delays may not work. Or the circuit may work
only at certain temperatures or voltages at which the delays are just right. These
mistakes are extremely difficult to track down.
3.3.2 Synchronous Sequential Circuits
The previous two examples contain loops called cyclic paths, in
which outputs are fed directly back to inputs. They are sequential
rather than combinational circuits. Combinational logic has no
cyclic paths and no races. If inputs are applied to combinational
logic, the outputs will always settle to the correct value within a
propagation delay. However, sequential circuits with cyclic paths can
have undesirable races or unstable behavior. Analyzing such circuits
for problems is time-consuming, and many bright people have made
mistakes.
CLK
CLK
114 CHAPTER THREE Sequential Logic Design
CLK
N1
N2
Q
CLK
Figure 3.19 Latch waveforms
illustrating race condition
Q
0
1
0
CLK D
0 0
0 0
0 1
Qprev
0
1
0
1
0
0
0 1
1 0
1 0
1
0
1
1
1
1 1
1 1
0
1
CLK
D
CLK
Qprev
Q
N1 = CLK·D
N2 = CLK·Qprev
Q = CLK·D + CLK·Qprev
Figure 3.18 An improved (?)
D latch
Chap
To avoid these problems, designers break the cyclic paths by inserting registers somewhere in the path. This transforms the circuit into a
collection of combinational logic and registers. The registers contain the
state of the system, which changes only at the clock edge, so we say the
state is synchronized to the clock. If the clock is sufficiently slow, so
that the inputs to all registers settle before the next clock edge, all races
are eliminated. Adopting this discipline of always using registers in the
feedback path leads us to the formal definition of a synchronous
sequential circuit.
Recall that a circuit is defined by its input and output terminals and
its functional and timing specifications. A sequential circuit has a finite
set of discrete states {S0, S1, . . . , Sk  1}. A synchronous sequential circuit
has a clock input, whose rising edges indicate a sequence of times at
which state transitions occur. We often use the terms current state and
next state to distinguish the state of the system at the present from the
state to which it will enter on the next clock edge. The functional specification details the next state and the value of each output for each possible combination of current state and input values. The timing
specification consists of an upper bound, tpcq, and a lower bound, tccq,
on the time from the rising edge of the clock until the output changes,
as well as setup and hold times, tsetup and thold, that indicate when the
inputs must be stable relative to the rising edge of the clock.
The rules of synchronous sequential circuit composition teach us
that a circuit is a synchronous sequential circuit if it consists of interconnected circuit elements such that
 Every circuit element is either a register or a combinational circuit
 At least one circuit element is a register
 All registers receive the same clock signal
 Every cyclic path contains at least one register.
Sequential circuits that are not synchronous are called asynchronous.
A flip-flop is the simplest synchronous sequential circuit. It
has one input, D, one clock, CLK, one output, Q, and two states,
{0, 1}. The functional specification for a flip-flop is that the next
state is D and that the output, Q, is the current state, as shown in
Figure 3.20.
We often call the current state variable S and the next state variable S.
In this case, the prime after S indicates next state, not inversion. The timing of sequential circuits will be analyzed in Section 3.5.
Two other common types of synchronous sequential circuits are
called finite state machines and pipelines. These will be covered later in
this chapter.
3.3 Synchronous Logic Design 115
tpcq stands for the time of
propagation from clock to Q,
where Q indicates the output
of a synchronous sequential
circuit. tccq stands for the time
of contamination from clock to
Q. These are analogous to tpd
and tcd in combinational logic.
D Q
Next
State
Current
State
S ′ S
CLK
Figure 3.20 Flip-flop current
state and next state
This definition of a synchronous sequential circuit is
sufficient, but more restrictive
than necessary. For example,
in high-performance microprocessors, some registers
may receive delayed or gated
clocks to squeeze out the last
bit of performance or power.
Similarly, some microprocessors use latches instead of
registers. However, the
definition is adequate for all
of the synchronous sequential
circuits covered in this book
and for most commercial
digital systems.
Chapt
Example 3.5 SYNCHRONOUS SEQUENTIAL CIRCUITS
Which of the circuits in Figure 3.21 are synchronous sequential circuits?
Solution: Circuit (a) is combinational, not sequential, because it has no registers. (b) is a simple sequential circuit with no feedback. (c) is neither a combinational circuit nor a synchronous sequential circuit, because it has a latch
that is neither a register nor a combinational circuit. (d) and (e) are synchronous sequential logic; they are two forms of finite state machines, which are
discussed in Section 3.4. (f) is neither combinational nor synchronous sequential, because it has a cyclic path from the output of the combinational logic
back to the input of the same logic but no register in the path. (g) is synchronous sequential logic in the form of a pipeline, which we will study in Section
3.6. (h) is not, strictly speaking, a synchronous sequential circuit, because the
second register receives a different clock signal than the first, delayed by two
inverter delays.
3.3.3 Synchronous and Asynchronous Circuits
Asynchronous design in theory is more general than synchronous design,
because the timing of the system is not limited by clocked registers. Just
as analog circuits are more general than digital circuits because analog
circuits can use any voltage, asynchronous circuits are more general
than synchronous circuits because they can use any kind of feedback.
However, synchronous circuits have proved to be easier to design and
use than asynchronous circuits, just as digital are easier than analog circuits. Despite decades of research on asynchronous circuits, virtually all
digital systems are essentially synchronous.
116 CHAPTER THREE Sequential Logic Design
CL
CLK
CL
(a)
CL
(b)
CLK
CL
CLK
(c)
(d)
CL
CLK
CL
(e)
CL
(f)
CLK
(g)
CL
CLK
CL
CLK CLK CLK
(h)
CL
Figure 3.21 Example circuits

Of course, asynchronous circuits are occasionally necessary when communicating between systems with different clocks or when receiving inputs
at arbitrary times, just as analog circuits are necessary when communicating with the real world of continuous voltages. Furthermore, research in
asynchronous circuits continues to generate interesting insights, some of
which can improve synchronous circuits too.
3.4 FINITE STATE MACHINES
Synchronous sequential circuits can be drawn in the forms shown in
Figure 3.22. These forms are called finite state machines (FSMs). They get
their name because a circuit with k registers can be in one of a finite number (2k) of unique states. An FSM has M inputs, N outputs, and k bits of
state. It also receives a clock and, optionally, a reset signal. An FSM consists of two blocks of combinational logic, next state logic and output
logic, and a register that stores the state. On each clock edge, the FSM
advances to the next state, which was computed based on the current state
and inputs. There are two general classes of finite state machines, characterized by their functional specifications. In Moore machines, the outputs
depend only on the current state of the machine. In Mealy machines, the
outputs depend on both the current state and the current inputs. Finite
state machines provide a systematic way to design synchronous sequential
circuits given a functional specification. This method will be explained in
the remainder of this section, starting with an example.
3.4.1 FSM Design Example
To illustrate the design of FSMs, consider the problem of inventing a
controller for a traffic light at a busy intersection on campus.
Engineering students are moseying between their dorms and the labs on
Academic Ave. They are busy reading about FSMs in their favorite
3.4 Finite State Machines 117
CLK
M next k N
state
logic
output
logic
(a)
inputs outputs state
next
state k
(b)
CLK
M next k N
state
logic
output
logic
inputs outputs state
next
state k
Figure 3.22 Finite state
machines: (a) Moore machine,
(b) Mealy machine
Moore and Mealy machines
are named after their promoters, researchers who developed
automata theory, the mathematical underpinnings of state
machines, at Bell Labs.
Edward F. Moore
(1925–2003), not to be
confused with Intel founder
Gordon Moore, published his
seminal article, Gedankenexperiments on Sequential
Machines in 1956. He subsequently became a professor of
mathematics and computer
science at the University of
Wisconsin.
George H. Mealy published
A Method of Synthesizing
Sequential Circuits in 1955.
He subsequently wrote the
first Bell Labs operating
system for the IBM 704
computer. He later joined
Harvard University.

textbook and aren’t looking where they are going. Football players are
hustling between the athletic fields and the dining hall on Bravado
Boulevard. They are tossing the ball back and forth and aren’t looking
where they are going either. Several serious injuries have already
occurred at the intersection of these two roads, and the Dean of Students
asks Ben Bitdiddle to install a traffic light before there are fatalities.
Ben decides to solve the problem with an FSM. He installs two traffic
sensors, TA and TB, on Academic Ave. and Bravado Blvd., respectively.
Each sensor indicates TRUE if students are present and FALSE if the street
is empty. He also installs two traffic lights, LA and LB, to control traffic.
Each light receives digital inputs specifying whether it should be green,
yellow, or red. Hence, his FSM has two inputs, TA and TB, and two outputs, LA and LB. The intersection with lights and sensors is shown in
Figure 3.23. Ben provides a clock with a 5-second period. On each clock
tick (rising edge), the lights may change based on the traffic sensors. He
also provides a reset button so that Physical Plant technicians can put the
controller in a known initial state when they turn it on. Figure 3.24 shows
a black box view of the state machine.
Ben’s next step is to sketch the state transition diagram, shown in
Figure 3.25, to indicate all the possible states of the system and the transitions between these states. When the system is reset, the lights are green
on Academic Ave. and red on Bravado Blvd. Every 5 seconds, the controller examines the traffic pattern and decides what to do next. As long
118 CHAPTER THREE Sequential Logic Design
TA
TB
LA
LB
CLK
Reset
Traffic
Light
Controller
Figure 3.24 Black box view of
finite state machine
TA
LA
TA
LB
TB
TB
LA
LB
Academic Ave. Bravado Blvd.
Dorms
Fields
Athletic
Dining
Hall
Labs
Figure 3.23 Campus map

as traffic is present on Academic Ave., the lights do not change. When
there is no longer traffic on Academic Ave., the light on Academic Ave.
becomes yellow for 5 seconds before it turns red and Bravado Blvd.’s light
turns green. Similarly, the Bravado Blvd. light remains green as long as
traffic is present on the boulevard, then turns yellow and eventually red.
In a state transition diagram, circles represent states and arcs represent transitions between states. The transitions take place on the rising
edge of the clock; we do not bother to show the clock on the diagram,
because it is always present in a synchronous sequential circuit. Moreover,
the clock simply controls when the transitions should occur, whereas the
diagram indicates which transitions occur. The arc labeled Reset pointing
from outer space into state S0 indicates that the system should enter that
state upon reset, regardless of what previous state it was in. If a state has
multiple arcs leaving it, the arcs are labeled to show what input triggers
each transition. For example, when in state S0, the system will remain in
that state if TA is TRUE and move to S1 if TA is FALSE. If a state has a
single arc leaving it, that transition always occurs regardless of the inputs.
For example, when in state S1, the system will always move to S2. The
value that the outputs have while in a particular state are indicated in the
state. For example, while in state S2, LA is red and LB is green.
Ben rewrites the state transition diagram as a state transition table
(Table 3.1), which indicates, for each state and input, what the next
state, S, should be. Note that the table uses don’t care symbols (X)
whenever the next state does not depend on a particular input. Also note
that Reset is omitted from the table. Instead, we use resettable flip-flops
that always go to state S0 on reset, independent of the inputs.
The state transition diagram is abstract in that it uses states labeled
{S0, S1, S2, S3} and outputs labeled {red, yellow, green}. To build a real
circuit, the states and outputs must be assigned binary encodings. Ben
chooses the simple encodings given in Tables 3.2 and 3.3. Each state and
each output is encoded with two bits: S1:0, LA1:0, and LB1:0.
3.4 Finite State Machines 119
S0
LA: green
LB: red
LA: red
LB: yellow
LA: yellow
LB: red
LA: red
LB: green
S1
S3 S2
TA
TA
TB
TB
Reset
Figure 3.25 State transition
diagram

Ben updates the state transition table to use these binary encodings, as
shown in Table 3.4. The revised state transition table is a truth table specifying the next state logic. It defines next state, S, as a function of the current state, S, and the inputs. The revised output table is a truth table
specifying the output logic. It defines the outputs, LA and LB, as functions
of the current state, S.
From this table, it is straightforward to read off the Boolean equations for the next state in sum-of-products form.
(3.1)
The equations can be simplified using Karnaugh maps, but often
doing it by inspection is easier. For example, the TB and terms in the
S
1 equation are clearly redundant. Thus S
1 reduces to an XOR operation. Equation 3.2 gives the next state equations.
TB
S
0S1S0TA S1S0TB
S
1 S1S0 S1S0TB S1S0TB
120 CHAPTER THREE Sequential Logic Design
Table 3.3 Output encoding
Output Encoding L1:0
green 00
yellow 01
red 10
Table 3.4 State transition table with binary encodings
Current State Inputs Next State
S1 S0 TA TB S
1 S0
00 0 X0 1
00 1 X0 0
01 XX1 0
10 X01 1
10 X11 0
11 XX0 0
Table 3.1 State transition table
Current Inputs Next State
State S TA TB S
S0 0 X S1
S0 1 X S0
S1 X X S2
S2 X 0 S3
S2 X 1 S2
S3 X X S0
Table 3.2 State encoding
State Encoding S1:0
S0 00
S1 01
S2 10
S3 11
Ch
(3.2)
Similarly, Ben writes an output table (Table 3.5) indicating, for each
state, what the output should be in that state. Again, it is straightforward to read off and simplify the Boolean equations for the outputs. For
example, observe that LA1 is TRUE only on the rows where S1 is TRUE.
(3.3)
Finally, Ben sketches his Moore FSM in the form of Figure 3.22(a).
First, he draws the 2-bit state register, as shown in Figure 3.26(a). On
each clock edge, the state register copies the next state, S
1:0, to become
the state, S1:0. The state register receives a synchronous or asynchronous
reset to initialize the FSM at startup. Then, he draws the next state logic,
based on Equation 3.2, which computes the next state, based on the current state and inputs, as shown in Figure 3.26(b). Finally, he draws the
output logic, based on Equation 3.3, which computes the outputs based
on the current state, as shown in Figure 3.26(c).
Figure 3.27 shows a timing diagram illustrating the traffic light controller going through a sequence of states. The diagram shows CLK,
Reset, the inputs TA and TB, next state S, state S, and outputs LA and LB.
Arrows indicate causality; for example, changing the state causes the outputs to change, and changing the inputs causes the next state to change.
Dashed lines indicate the rising edge of CLK when the state changes.
The clock has a 5-second period, so the traffic lights change at most
once every 5 seconds. When the finite state machine is first turned on, its
state is unknown, as indicated by the question marks. Therefore, the system should be reset to put it into a known state. In this timing diagram,
LB0S1 S0
LB1S1
LA0S1S0
LA1S1
S
0 S1S0TAS1S0TB
S
1S1 ⊕ S0
3.4 Finite State Machines 121
Table 3.5 Output table
Current State Outputs
S1 S0 LA1 LA0 LB1 LB0
00 0 0 10
01 0 1 10
10 1 0 00
11 1 0 01
Chapte
122 CHAPTER THREE Sequential Logic Design
(a)
S1
S0
S'1
S'0
CLK
state register
Reset
r
S1
S0
S'1
S'0
CLK
next state logic state register
Reset
TA
TB
inputs
(b)
S1 S0
r
S1
S0
S'1
S'0
CLK
next state logic output
logic
state register
Reset
LA1
LB1
LB0
LA0
TA
TB
inputs outputs
(c)
S1 S0
r
Figure 3.26 State machine circuit for traffic light controller
CLK
Reset
TA
TB
S'1:0
S1:0
LA1:0
LB1:0
Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 5 Cycle 6 Cycle 7 Cycle 8 Cycle 9 Cycle 10
S1 (01) S2 (10) S3 (11) S0 (00)
t (sec)
??
??
S0 (00)
S0 (00) S1 (01) S2 (10) S3 (11) S1 (01)
??
??
0 5 10 15 20 25 30 35 40 45
Green (00)
Red (10)
S0 (00)
Yellow (01) Red (10) Green (00)
Green (00) Yellow (01) Red (10)
Figure 3.27 Timing diagram for traffic light controller
This schematic uses some
AND gates with bubbles on
the inputs. They might be
constructed with AND gates
and input inverters, with
NOR gates and inverters for
the non-bubbled inputs, or
with some other combination
of gates. The best choice
depends on the particular
implementation technology.

S immediately resets to S0, indicating that asynchronously resettable flipflops are being used. In state S0, light LA is green and light LB is red.
In this example, traffic arrives immediately on Academic Ave.
Therefore, the controller remains in state S0, keeping LA green even
though traffic arrives on Bravado Blvd. and starts waiting. After 15 seconds, the traffic on Academic Ave. has all passed through and TA falls.
At the following clock edge, the controller moves to state S1, turning LA
yellow. In another 5 seconds, the controller proceeds to state S2 in which
LA turns red and LB turns green. The controller waits in state S2 until all
the traffic on Bravado Blvd. has passed through. It then proceeds to state
S3, turning LB yellow. 5 seconds later, the controller enters state S0,
turning LB red and LA green. The process repeats.
3.4.2 State Encodings
In the previous example, the state and output encodings were selected arbitrarily. A different choice would have resulted in a different circuit. A natural question is how to determine the encoding that produces the circuit with
the fewest logic gates or the shortest propagation delay. Unfortunately, there
is no simple way to find the best encoding except to try all possibilities,
which is infeasible when the number of states is large. However, it is often
possible to choose a good encoding by inspection, so that related states or
outputs share bits. Computer-aided design (CAD) tools are also good at
searching the set of possible encodings and selecting a reasonable one.
One important decision in state encoding is the choice between
binary encoding and one-hot encoding. With binary encoding, as was
used in the traffic light controller example, each state is represented as a
binary number. Because K binary numbers can be represented by log2K
bits, a system with K states only needs log2K bits of state.
In one-hot encoding, a separate bit of state is used for each state. It is
called one-hot because only one bit is “hot” or TRUE at any time. For
example, a one-hot encoded FSM with three states would have state encodings of 001, 010, and 100. Each bit of state is stored in a flip-flop, so onehot encoding requires more flip-flops than binary encoding. However, with
one-hot encoding, the next-state and output logic is often simpler, so fewer
gates are required. The best encoding choice depends on the specific FSM.
Example 3.6 FSM STATE ENCODING
A divide-by-N counter has one output and no inputs. The output Y is HIGH for
one clock cycle out of every N. In other words, the output divides the frequency
of the clock by N. The waveform and state transition diagram for a divide-by-3
counter is shown in Figure 3.28. Sketch circuit designs for such a counter using
binary and one-hot state encodings.
3.4 Finite State Machines 123
Despite Ben’s best efforts, students don’t pay attention to
traffic lights and collisions
continue to occur. The Dean
of Students next asks him to
design a catapult to throw
engineering students directly
from their dorm roofs
through the open windows
of the lab, bypassing the
troublesome intersection all
together. But that is the subject of another textbook.

Solution: Tables 3.6 and 3.7 show the abstract state transition and output tables
before encoding.
Table 3.8 compares binary and one-hot encodings for the three states.
The binary encoding uses two bits of state. Using this encoding, the state transition table is shown in Table 3.9. Note that there are no inputs; the next state
depends only on the current state. The output table is left as an exercise to the
reader. The next-state and output equations are:
(3.4)
(3.5)
The one-hot encoding uses three bits of state. The state transition table for this
encoding is shown in Table 3.10 and the output table is again left as an exercise
to the reader. The next-state and output equations are as follows:
(3.6)
(3.7)
Figure 3.29 shows schematics for each of these designs. Note that the hardware
for the binary encoded design could be optimized to share the same gate for
Y and S
0. Also observe that the one-hot encoding requires both settable (s) and
resettable (r) flip-flops to initialize the machine to S0 on reset. The best implementation choice depends on the relative cost of gates and flip-flops, but the
one-hot design is usually preferable for this specific example.
A related encoding is the one-cold encoding, in which K states are
represented with K bits, exactly one of which is FALSE.
Y S0
 S
0 S2
 S
1 S0
 S
2 S1
Y S1S0
 S
0S1S0
 S
1S1 S0
124 CHAPTER THREE Sequential Logic Design
CLK
Y
(a)
S0
Y: 1
S1
Y: 0
S2
Y: 0
Reset
(b)
Figure 3.28 Divide-by-3 counter
(a) waveform and (b) state
transition diagram
Table 3.6 Divide-by-3 counter
state transition table
Current Next
State State
S0 S1
S1 S2
S2 S0
Table 3.7 Divide-by-3 counter
output table
Current
State Output
S0 1
S1 0
S2 0
Chapter
3.4 Finite State Machines 125
Table 3.8 Binary and one-hot encodings for divide-by-3 counter
State Binary Encoding One-Hot Encoding
S2 S1 S0 S1 S0
S0 0 0 1 0 1
S1 0 1 0 1 0
S2 1 0 0 0 0
Table 3.9 State transition table with binary
encoding
Current State Next State
S1 S0 S1 S0
00 0 1
01 1 0
10 0 0
Table 3.10 State transition table with one-hot encoding
Current State Next State
S2 S1 S0 S
2 S
1 S
0
001 0 1 0
010 1 0 0
100 0 0 1
S1
S0
S'1
S'0
CLK
Reset
Y
next state logic state register output logic output
(a)
Reset
CLK
rrs
S1 S2 S0
Y
(b)
S1 S0
r
Figure 3.29 Divide-by-3 circuits
for (a) binary and (b) one-hot
encodings

3.4.3 Moore and Mealy Machines
So far, we have shown examples of Moore machines, in which the output
depends only on the state of the system. Hence, in state transition diagrams for Moore machines, the outputs are labeled in the circles. Recall
that Mealy machines are much like Moore machines, but the outputs can
depend on inputs as well as the current state. Hence, in state transition
diagrams for Mealy machines, the outputs are labeled on the arcs instead
of in the circles. The block of combinational logic that computes the outputs uses the current state and inputs, as was shown in Figure 3.22(b).
Example 3.7 MOORE VERSUS MEALY MACHINES
Alyssa P. Hacker owns a pet robotic snail with an FSM brain. The snail crawls
from left to right along a paper tape containing a sequence of 1’s and 0’s. On each
clock cycle, the snail crawls to the next bit. The snail smiles when the last four
bits that it has crawled over are, from left to right, 1101. Design the FSM to compute when the snail should smile. The input A is the bit underneath the snail’s
antennae. The output Y is TRUE when the snail smiles. Compare Moore and
Mealy state machine designs. Sketch a timing diagram for each machine showing
the input, states, and output as your snail crawls along the sequence 111011010.
Solution: The Moore machine requires five states, as shown in Figure 3.30(a).
Convince yourself that the state transition diagram is correct. In particular, why
is there an arc from S4 to S2 when the input is 1?
In comparison, the Mealy machine requires only four states, as shown in Figure
3.30(b). Each arc is labeled as A/Y. A is the value of the input that causes that
transition, and Y is the corresponding output.
Tables 3.11 and 3.12 show the state transition and output tables for the Moore
machine. The Moore machine requires at least three bits of state. Consider using a
binary state encoding: S0  000, S1  001, S2  010, S3  011, and S4  100.
Tables 3.13 and 3.14 rewrite the state transition and output tables with these
encodings (These four tables follow on page 128).
From these tables, we find the next state and output equations by inspection.
Note that these equations are simplified using the fact that states 101, 110, and
111 do not exist. Thus, the corresponding next state and output for the nonexistent states are don’t cares (not shown in the tables). We use the don’t cares
to minimize our equations.
(3.8)
S
0S2S1S0 A S1S0A
S
1S1S0A S1S0S2A
S
2S1S0A
126 CHAPTER THREE Sequential Logic Design
An easy way to remember the
difference between the two
types of finite state machines
is that a Moore machine typically has more states than a
Mealy machine for a given
problem.
Chapter 
(3.9)
Table 3.15 shows the combined state transition and output table for the Mealy
machine. The Mealy machine requires at least two bits of state. Consider using a
binary state encoding: S0  00, S1  01, S2  10, and S3  11. Table 3.16
rewrites the state transition and output table with these encodings.
From these tables, we find the next state and output equations by inspection.
(3.10)
(3.11)
The Moore and Mealy machine schematics are shown in Figure 3.31(a) and
3.31(b), respectively.
The timing diagrams for the Moore and Mealy machines are shown in Figure
3.32 (see page 131). The two machines follow a different sequence of states.
Moreover, the Mealy machine’s output rises a cycle sooner because it responds to
the input rather than waiting for the state change. If the Mealy output were
delayed through a flip-flop, it would match the Moore output. When choosing
your FSM design style, consider when you want your outputs to respond.
Y S1S0A
S0
  S1S0A S1 S0AS1S0A
S
1S1S0  S1S0A
Y  S2
3.4 Finite State Machines 127
Reset
(a)
S0
0
S1
0
S2
0
S3
0
S4
1
0
110 1
1
1 0 0
0
Reset
(b)
S0 S1 S2 S3
0/0
1/0 1/0 0/0
1/1
0/0 1/0
0/0
Figure 3.30 FSM state
transition diagrams: (a) Moore
machine, (b) Mealy machine
Chapter 
128 CHAPTER THREE Sequential Logic Design
Table 3.14 Moore output table
with state encodings
Current State Output
S2 S1 S0 Y
000 0
001 0
010 0
011 0
100 1
Table 3.13 Moore state transition table with state encodings
Current State Input Next State
S2 S1 S0 A S
2 S
1 S
0
000 0 000
000 1 001
001 0 000
001 1 010
010 0 011
010 1 010
011 0 000
011 1 100
100 0 000
100 1 010
Table 3.11 Moore state transition table
Current State Input Next State
S AS
S0 0 S0
S0 1 S1
S1 0 S0
S1 1 S2
S2 0 S3
S2 1 S2
S3 0 S0
S3 1 S4
S4 0 S0
S4 1 S2
Table 3.12 Moore output table
Current Output
State
S Y
S0 0
S1 0
S2 0
S3 0
S4 1

3.4.4 Factoring State Machines
Designing complex FSMs is often easier if they can be broken down into
multiple interacting simpler state machines such that the output of some
machines is the input of others. This application of hierarchy and modularity is called factoring of state machines.
3.4 Finite State Machines 129
Table 3.15 Mealy state transition and output table
Current State Input Next State Output
S AS Y
S0 0 S0 0
S0 1 S1 0
S1 0 S0 0
S1 1 S2 0
S2 0 S3 0
S2 1 S2 0
S3 0 S0 0
S3 1 S1 1
Table 3.16 Mealy state transition and output table with
state encodings
Current State Input Next State Output
S1 S0 A S
1 S
0 Y
00 000 0
00 101 0
01 000 0
01 110 0
10 011 0
10 110 0
11 000 0
11 101 1

130 CHAPTER THREE Sequential Logic Design
Example 3.8 UNFACTORED AND FACTORED STATE MACHINES
Modify the traffic light controller from Section 3.4.1 to have a parade mode,
which keeps the Bravado Boulevard light green while spectators and the band
march to football games in scattered groups. The controller receives two more
inputs: P and R. Asserting P for at least one cycle enters parade mode. Asserting
R for at least one cycle leaves parade mode. When in parade mode, the controller proceeds through its usual sequence until LB turns green, then remains in
that state with LB green until parade mode ends.
First, sketch a state transition diagram for a single FSM, as shown in Figure
3.33(a). Then, sketch the state transition diagrams for two interacting FSMs, as
S2
S1
S0
S'2
S'1
S'0
Y
CLK
Reset
A
(a)
S2
S1
S0
(b)
S'1
S'0
CLK
Reset
S1
S0
A
Y
S0
S1
Figure 3.31 FSM schematics for
(a) Moore and (b) Mealy
machines

3.4 Finite State Machines 131
Mealy Machine
Moore Machine
CLK
Reset
A
S
Y
S
Y
Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 5 Cycle 6 Cycle 7 Cycle 8 Cycle 9 Cycle 10
S0
S0
S1
S1
??
??
S2
S2
S3
S3
S4
S1
S0
S0
1 11 11 1 0 00
S2 S3
S3
S4
S1
S2
S2 S2
Figure 3.32 Timing diagrams for Moore and Mealy machines
Controller
TA FSM
(a)
TB
LA
LB
(b)
Mode
FSM
Lights
FSM
P
M
Controller
FSM
LA
LB
TA
TB
R
P
R
Figure 3.33 (a) single and
(b) factored designs for
modified traffic light
controller FSM
shown in Figure 3.33(b). The Mode FSM asserts the output M when it is in
parade mode. The Lights FSM controls the lights based on M and the traffic
sensors, TA and TB.
Solution: Figure 3.34(a) shows the single FSM design. States S0 to S3 handle
normal mode. States S4 to S7 handle parade mode. The two halves of the diagram are almost identical, but in parade mode, the FSM remains in S6 with a
green light on Bravado Blvd. The P and R inputs control movement between
these two halves. The FSM is messy and tedious to design. Figure 3.34(b) shows
the factored FSM design. The mode FSM has two states to track whether the
lights are in normal or parade mode. The Lights FSM is modified to remain in
S2 while M is TRUE.

3.4.5 FSM Review
Finite state machines are a powerful way to systematically design
sequential circuits from a written specification. Use the following procedure to design an FSM:
 Identify the inputs and outputs.
 Sketch a state transition diagram.
132 CHAPTER THREE Sequential Logic Design
S0
LA: green
LB: red LA: green
LB: red
LA: yellow
LB: red LA: yellow
LB: red
LA: red
LB: green LA: red
LB: green
LA: red
LB: yellow LA: red
LB: yellow
S1
S3 S2
TA
TA
TB
TB
Reset
S4 S5
S7 S6
TA
TA
P
P P
P
P
P
R
R
R
R
R
P
R
P
PTA
PTA
P
RTA
RTA
R
RTB
RTB
(a)
LA: green
LB: red
LA: yellow
LB: red
LA: red
LB: green
LA: red
LB: yellow
S0 S1
S3 S2
TA
TA
M + TB
MTB
Reset
Lights FSM
(b)
S0
M: 0
S1
M: 1
P
Reset P
Mode FSM
R
R
Figure 3.34 State transition
diagrams: (a) unfactored,
(b) factored
Ch
 For a Moore machine:
– Write a state transition table.
– Write an output table.
 For a Mealy machine:
– Write a combined state transition and output table.
 Select state encodings—your selection affects the hardware design.
 Write Boolean equations for the next state and output logic.
 Sketch the circuit schematic.
We will repeatedly use FSMs to design complex digital systems throughout this book.
3.5 TIMING OF SEQUENTIAL LOGIC
Recall that a flip-flop copies the input D to the output Q on the rising edge
of the clock. This process is called sampling D on the clock edge. If D is
stable at either 0 or 1 when the clock rises, this behavior is clearly defined.
But what happens if D is changing at the same time the clock rises?
This problem is similar to that faced by a camera when snapping a
picture. Imagine photographing a frog jumping from a lily pad into the
lake. If you take the picture before the jump, you will see a frog on a lily
pad. If you take the picture after the jump, you will see ripples in the
water. But if you take it just as the frog jumps, you may see a blurred
image of the frog stretching from the lily pad into the water. A camera is
characterized by its aperture time, during which the object must remain
still for a sharp image to be captured. Similarly, a sequential element has
an aperture time around the clock edge, during which the input must be
stable for the flip-flop to produce a well-defined output.
The aperture of a sequential element is defined by a setup time and
a hold time, before and after the clock edge, respectively. Just as the
static discipline limited us to using logic levels outside the forbidden
zone, the dynamic discipline limits us to using signals that change outside the aperture time. By taking advantage of the dynamic discipline,
we can think of time in discrete units called clock cycles, just as we
think of signal levels as discrete 1’s and 0’s. A signal may glitch and
oscillate wildly for some bounded amount of time. Under the dynamic
discipline, we are concerned only about its final value at the end of the
clock cycle, after it has settled to a stable value. Hence, we can simply
write A[n], the value of signal A at the end of the nth clock cycle,
where n is an integer, rather than A(t), the value of A at some instant t,
where t is any real number.
The clock period has to be long enough for all signals to settle. This
sets a limit on the speed of the system. In real systems, the clock does not
3.5 Timing of Sequential Logic 133
Chap
reach all flip-flops at precisely the same time. This variation in time,
called clock skew, further increases the necessary clock period.
Sometimes it is impossible to satisfy the dynamic discipline, especially when interfacing with the real world. For example, consider a circuit with an input coming from a button. A monkey might press the
button just as the clock rises. This can result in a phenomenon called
metastability, where the flip-flop captures a value partway between
0 and 1 that can take an unlimited amount of time to resolve into a good
logic value. The solution to such asynchronous inputs is to use a synchronizer, which has a very small (but nonzero) probability of producing
an illegal logic value.
We expand on all of these ideas in the rest of this section.
3.5.1 The Dynamic Discipline
So far, we have focused on the functional specification of sequential
circuits. Recall that a synchronous sequential circuit, such as a flip-flop
or FSM, also has a timing specification, as illustrated in Figure 3.35.
When the clock rises, the output (or outputs) may start to change after
the clock-to-Q contamination delay, tccq, and must definitely settle to the
final value within the clock-to-Q propagation delay, tpcq. These represent the fastest and slowest delays through the circuit, respectively. For
the circuit to sample its input correctly, the input (or inputs) must have
stabilized at least some setup time, tsetup, before the rising edge of the
clock and must remain stable for at least some hold time, thold, after the
rising edge of the clock. The sum of the setup and hold times is called
the aperture time of the circuit, because it is the total time for which the
input must remain stable.
The dynamic discipline states that the inputs of a synchronous
sequential circuit must be stable during the setup and hold aperture time
around the clock edge. By imposing this requirement, we guarantee that
the flip-flops sample signals while they are not changing. Because we
are concerned only about the final values of the inputs at the time
they are sampled, we can treat signals as discrete in time as well as in
logic levels.
134 CHAPTER THREE Sequential Logic Design
CLK
tccq
tpcq
tsetup
output(s)
input(s)
thold
Figure 3.35 Timing specification
for synchronous sequential
circuit

3.5.2 System Timing
The clock period or cycle time, Tc, is the time between rising edges of a
repetitive clock signal. Its reciprocal, fc  1/Tc, is the clock frequency.
All else being the same, increasing the clock frequency increases the
work that a digital system can accomplish per unit time. Frequency is
measured in units of Hertz (Hz), or cycles per second: 1 megahertz
(MHz)  106 Hz, and 1 gigahertz (GHz)  109 Hz.
Figure 3.36(a) illustrates a generic path in a synchronous sequential
circuit whose clock period we wish to calculate. On the rising edge of
the clock, register R1 produces output (or outputs) Q1. These signals
enter a block of combinational logic, producing D2, the input (or inputs)
to register R2. The timing diagram in Figure 3.36(b) shows that each
output signal may start to change a contamination delay after its input
change and settles to the final value within a propagation delay after its
input settles. The gray arrows represent the contamination delay through
R1 and the combinational logic, and the blue arrows represent the propagation delay through R1 and the combinational logic. We analyze the
timing constraints with respect to the setup and hold time of the second
register, R2.
Setup Time Constraint
Figure 3.37 is the timing diagram showing only the maximum delay
through the path, indicated by the blue arrows. To satisfy the setup
time of R2, D2 must settle no later than the setup time before the next
clock edge.
3.5 Timing of Sequential Logic 135
CL
CLK CLK
R1 R2
Q1 D2
(a)
CLK
Q1
D2
(b)
Tc
Figure 3.36 Path between
registers and timing diagram
CLK
Q1
D2
Tc
tpcq tpd tsetup
CL
CLK CLK
Q1 D2
R1 R2
Figure 3.37 Maximum delay
for setup time constraint
In the three decades from
when one of the authors’
families bought an Apple II
computer to the present time
of writing, microprocessor
clock frequencies have
increased from 1 MHz to
several GHz, a factor of
more than 1000. This speedup
partially explains the revolutionary changes computers
have made in society.
Cha
Hence, we find an equation for the minimum clock period:
(3.12)
In commercial designs, the clock period is often dictated by the Director
of Engineering or by the marketing department (to ensure a competitive
product). Moreover, the flip-flop clock-to-Q propagation delay and
setup time, tpcq and tsetup, are specified by the manufacturer. Hence, we
rearrange Equation 3.12 to solve for the maximum propagation delay
through the combinational logic, which is usually the only variable
under the control of the individual designer.
(3.13)
The term in parentheses, tpcq  tsetup, is called the sequencing overhead.
Ideally, the entire cycle time, Tc, would be available for useful computation in the combinational logic, tpd. However, the sequencing overhead
of the flip-flop cuts into this time. Equation 3.13 is called the setup time
constraint or max-delay constraint, because it depends on the setup
time and limits the maximum delay through combinational logic.
If the propagation delay through the combinational logic is too
great, D2 may not have settled to its final value by the time R2 needs it
to be stable and samples it. Hence, R2 may sample an incorrect result or
even an illegal logic level, a level in the forbidden region. In such a case,
the circuit will malfunction. The problem can be solved by increasing the
clock period or by redesigning the combinational logic to have a shorter
propagation delay.
Hold Time Constraint
The register R2 in Figure 3.36(a) also has a hold time constraint. Its
input, D2, must not change until some time, thold, after the rising edge of
the clock. According to Figure 3.38, D2 might change as soon as tccq 
tcd after the rising edge of the clock.
tpd  Tc  (tpcq tsetup)
Tc  tpcq  tpd  tsetup
136 CHAPTER THREE Sequential Logic Design
CLK
Q1
D2
tccq tcd
thold
CL
CLK CLK
Q1 D2
R1 R2
Figure 3.38 Minimum delay for
hold time constraint
C
Hence, we find
(3.14)
Again, tccq and thold are characteristics of the flip-flop that are usually
outside the designer’s control. Rearranging, we can solve for the minimum contamination delay through the combinational logic:
(3.15)
Equation 3.15 is also called the min-delay constraint because it limits the
minimum delay through combinational logic.
We have assumed that any logic elements can be connected to each
other without introducing timing problems. In particular, we would
expect that two flip-flops may be directly cascaded as in Figure 3.39
without causing hold time problems.
In such a case, tcd  0 because there is no combinational logic between
flip-flops. Substituting into Equation 3.15 yields the requirement that
(3.16)
In other words, a reliable flip-flop must have a hold time shorter
than its contamination delay. Often, flip-flops are designed with thold
0, so that Equation 3.16 is always satisfied. Unless noted otherwise, we
will usually make that assumption and ignore the hold time constraint in
this book.
Nevertheless, hold time constraints are critically important. If they
are violated, the only solution is to increase the contamination delay
through the logic, which requires redesigning the circuit. Unlike setup
time constraints, they cannot be fixed by adjusting the clock period.
Redesigning an integrated circuit and manufacturing the corrected
design takes months and millions of dollars in today’s advanced technologies, so hold time violations must be taken extremely seriously.
Putting It All Together
Sequential circuits have setup and hold time constraints that dictate the
maximum and minimum delays of the combinational logic between flipflops. Modern flip-flops are usually designed so that the minimum delay
through the combinational logic is 0—that is, flip-flops can be placed
back-to-back. The maximum delay constraint limits the number of consecutive gates on the critical path of a high-speed circuit, because a high
clock frequency means a short clock period.
Example 3.9 TIMING ANALYSIS
Ben Bitdiddle designed the circuit in Figure 3.40. According to the data sheets
for the components he is using, flip-flops have a clock-to-Q contamination delay
thold  tccq
tcd  thold tccq
tccq  tcd  thold
3.5 Timing of Sequential Logic 137
CLK
Figure 3.39 Back-to-back
flip-flops
Cha
of 30 ps and a propagation delay of 80 ps. They have a setup time of 50 ps and
a hold time of 60 ps. Each logic gate has a propagation delay of 40 ps and a contamination delay of 25 ps. Help Ben determine the maximum clock frequency
and whether any hold time violations could occur. This process is called timing
analysis.
Solution: Figure 3.41(a) shows waveforms illustrating when the signals might
change. The inputs, A to D, are registered, so they change shortly after CLK
rises only.
The critical path occurs when B  1, C  0, D  0, and A rises from 0 to 1,
triggering n1 to rise, X to rise and Y to fall, as shown in Figure 3.41(b). This
path involves three gate delays. For the critical path, we assume that each gate
requires its full propagation delay. Y must setup before the next rising edge of
the CLK. Hence, the minimum cycle time is
(3.17)
The maximum clock frequency is fc  1/Tc  4 GHz.
A short path occurs when A  0 and C rises, causing X to rise, as shown in
Figure 3.41(c). For the short path, we assume that each gate switches after
only a contamination delay. This path involves only one gate delay, so it may
occur after tccq  tcd  30  25  55 ps. But recall that the flip-flop has a
hold time of 60 ps, meaning that X must remain stable for 60 ps after the
rising edge of CLK for the flip-flop to reliably sample its value. In this case,
X  0 at the first rising edge of CLK, so we want the flip-flop to capture
X  0. Because X did not hold stable long enough, the actual value of X is
unpredictable. The circuit has a hold time violation and may behave erratically at any clock frequency.
Tc  tpcq3 tpd  tsetup 80  3 40 50 250 ps
138 CHAPTER THREE Sequential Logic Design
CLK CLK
A
B
C
D
n1
X'
Y'
X
Y
Figure 3.40 Sample circuit for
timing analysis
Chapter 03.q
Example 3.10 FIXING HOLD TIME VIOLATIONS
Alyssa P. Hacker proposes to fix Ben’s circuit by adding buffers to slow down
the short paths, as shown in Figure 3.42. The buffers have the same delays as
other gates. Determine the maximum clock frequency and whether any hold time
problems could occur.
Solution: Figure 3.43 shows waveforms illustrating when the signals might
change. The critical path from A to Y is unaffected, because it does not pass
through any buffers. Therefore, the maximum clock frequency is still 4 GHz.
However, the short paths are slowed by the contamination delay of the buffer.
Now X will not change until tccq  2tcd  30  2  25  80 ps. This is after
the 60 ps hold time has elapsed, so the circuit now operates correctly.
This example had an unusually long hold time to illustrate the point of hold time
problems. Most flip-flops are designed with thold 	 tccq to avoid such problems.
However, several high-performance microprocessors, including the Pentium 4, use
an element called a pulsed latch in place of a flip-flop. The pulsed latch behaves like
a flip-flop but has a short clock-to Q delay and a long hold time. In general, adding
buffers can usually, but not always, solve hold time problems without slowing the
critical path.
3.5 Timing of Sequential Logic 139
A–D
CLK
n1
X'
Y'
0 50 100 150 200 250 t (ps)
(a)
A
n1
X'
Y'
tpcq
tpd
tpd
tpd
tsetup
C
X'
tccq
tcd
thold
(b)
(c)
Figure 3.41 Timing diagram:
(a) general case, (b) critical
path, (c) short path
Ch
3.5.3 Clock Skew*
In the previous analysis, we assumed that the clock reaches all registers
at exactly the same time. In reality, there is some variation in this time.
This variation in clock edges is called clock skew. For example, the wires
from the clock source to different registers may be of different lengths,
resulting in slightly different delays, as shown in Figure 3.44. Noise also
results in different delays. Clock gating, described in Section 3.2.5,
further delays the clock. If some clocks are gated and others are not,
there will be substantial skew between the gated and ungated clocks. In
140 CHAPTER THREE Sequential Logic Design
CLK CLK
A
B
C
D
n1
X'
Y'
X
Y Buffers added to fix
hold time violation
n2
n3
Figure 3.42 Corrected circuit
to fix hold time problem
A–D
CLK
n1–n3
X'
Y'
0 50 100 150 200 250 t (ps)
Figure 3.43 Timing diagram
with buffers to fix hold time
problem
t skew
CLK1
CLK2
CL
CLK1 CLK2
R1 R2
Q1 D2
CLK
delay
CLK
Figure 3.44 Clock skew caused
by wire delay

Figure 3.44, CLK2 is early with respect to CLK1, because the clock wire
between the two registers follows a scenic route. If the clock had been
routed differently, CLK1 might have been early instead. When doing
timing analysis, we consider the worst-case scenario, so that we can
guarantee that the circuit will work under all circumstances.
Figure 3.45 adds skew to the timing diagram from Figure 3.36. The
heavy clock line indicates the latest time at which the clock signal might
reach any register; the hashed lines show that the clock might arrive up
to tskew earlier.
First, consider the setup time constraint shown in Figure 3.46. In the
worst case, R1 receives the latest skewed clock and R2 receives the earliest skewed clock, leaving as little time as possible for data to propagate
between the registers.
The data propagates through the register and combinational logic
and must setup before R2 samples it. Hence, we conclude that
(3.18)
(3.19)
Next, consider the hold time constraint shown in Figure 3.47. In
the worst case, R1 receives an early skewed clock, CLK1, and R2
receives a late skewed clock, CLK2. The data zips through the register
tpd  Tc (tpcq tsetup tskew)
Tc  tpcqtpd tsetuptskew
3.5 Timing of Sequential Logic 141
CL
CLK CLK
R1 R2
Q1 D2
(a)
CLK
Q1
D2
(b)
Tc
tskew
Figure 3.45 Timing diagram
with clock skew
CLK1
Q1
D2
Tc
tpcq tpd tsetuptskew
CL
CLK1 CLK2
R1 R2
Q1 D2
CLK2
Figure 3.46 Setup time
constraint with clock skew
C
and combinational logic but must not arrive until a hold time after the
late clock. Thus, we find that
(3.20)
(3.21)
In summary, clock skew effectively increases both the setup time and
the hold time. It adds to the sequencing overhead, reducing the time
available for useful work in the combinational logic. It also increases
the required minimum delay through the combinational logic. Even if
thold  0, a pair of back-to-back flip-flops will violate Equation 3.21
if tskew 
 tccq. To prevent serious hold time failures, designers must not
permit too much clock skew. Sometimes flip-flops are intentionally
designed to be particularly slow (i.e., large tccq), to prevent hold time
problems even when the clock skew is substantial.
Example 3.11 TIMING ANALYSIS WITH CLOCK SKEW
Revisit Example 3.9 and assume that the system has 50 ps of clock skew.
Solution: The critical path remains the same, but the setup time is effectively
increased by the skew. Hence, the minimum cycle time is
(3.22)
The maximum clock frequency is fc  1/Tc  3.33 GHz.
The short path also remains the same at 55 ps. The hold time is effectively
increased by the skew to 60  50  110 ps, which is much greater than 55 ps.
Hence, the circuit will violate the hold time and malfunction at any frequency.
The circuit violated the hold time constraint even without skew. Skew in the
system just makes the violation worse.
 80  3  40  50  50  300 ps
Tc  tpcq  3tpd  tsetuptskew
tcd  thold  tskew tccq
tccq  tcd  thold  tskew
142 CHAPTER THREE Sequential Logic Design
tcd
thold
Q1
D2
tskew
CL
CLK1 CLK2
R1 R2
Q1 D2
CLK2
CLK1
tccq
Figure 3.47 Hold time
constraint with clock skew
Chapter
Example 3.12 FIXING HOLD TIME VIOLATIONS
Revisit Example 3.10 and assume that the system has 50 ps of clock skew.
Solution: The critical path is unaffected, so the maximum clock frequency
remains 3.33 GHz.
The short path increases to 80 ps. This is still less than thold  tskew  110 ps, so
the circuit still violates its hold time constraint.
To fix the problem, even more buffers could be inserted. Buffers would need to
be added on the critical path as well, reducing the clock frequency. Alternatively,
a better flip-flop with a shorter hold time might be used.
3.5.4 Metastability
As noted earlier, it is not always possible to guarantee that the input to a
sequential circuit is stable during the aperture time, especially when the
input arrives from the external world. Consider a button connected to
the input of a flip-flop, as shown in Figure 3.48. When the button is not
pressed, D  0. When the button is pressed, D  1. A monkey presses
the button at some random time relative to the rising edge of CLK. We
want to know the output Q after the rising edge of CLK. In Case I,
when the button is pressed much before CLK, Q  1.
In Case II, when the button is not pressed until long after CLK, Q  0.
But in Case III, when the button is pressed sometime between tsetup before
CLK and thold after CLK, the input violates the dynamic discipline and the
output is undefined.
Metastable State
In reality, when a flip-flop samples an input that is changing during its
aperture, the output Q may momentarily take on a voltage between 0
and VDD that is in the forbidden zone. This is called a metastable state.
Eventually, the flip-flop will resolve the output to a stable state of either
0 or 1. However, the resolution time required to reach the stable state is
unbounded.
The metastable state of a flip-flop is analogous to a ball on the summit of a hill between two valleys, as shown in Figure 3.49. The two valleys are stable states, because a ball in the valley will remain there as
long as it is not disturbed. The top of the hill is called metastable
because the ball would remain there if it were perfectly balanced. But
because nothing is perfect, the ball will eventually roll to one side or the
other. The time required for this change to occur depends on how nearly
well balanced the ball originally was. Every bistable device has a
metastable state between the two stable states.
3.5 Timing of Sequential Logic 143
D Q
CLK
button
CLK
tsetup thold
aperture
D
Q
D
Q
D
Q
???
Case I Case II Case III
Figure 3.48 Input changing
before, after, or during
aperture
Chapt
Resolution Time
If a flip-flop input changes at a random time during the clock cycle, the
resolution time, tres, required to resolve to a stable state is also a random
variable. If the input changes outside the aperture, then tres  tpcq. But if
the input happens to change within the aperture, tres can be substantially
longer. Theoretical and experimental analyses (see Section 3.5.6) have
shown that the probability that the resolution time, tres, exceeds some
arbitrary time, t, decreases exponentially with t:
(3.23)
where Tc is the clock period, and T0 and  are characteristic of the flipflop. The equation is valid only for t substantially longer than tpcq.
Intuitively, T0/Tc describes the probability that the input changes at
a bad time (i.e., during the aperture time); this probability decreases with
the cycle time, Tc.  is a time constant indicating how fast the flip-flop
moves away from the metastable state; it is related to the delay through
the cross-coupled gates in the flip-flop.
In summary, if the input to a bistable device such as a flip-flop
changes during the aperture time, the output may take on a metastable
value for some time before resolving to a stable 0 or 1. The amount of
time required to resolve is unbounded, because for any finite time, t, the
probability that the flip-flop is still metastable is nonzero. However, this
probability drops off exponentially as t increases. Therefore, if we wait
long enough, much longer than tpcq, we can expect with exceedingly
high probability that the flip-flop will reach a valid logic level.
3.5.5 Synchronizers
Asynchronous inputs to digital systems from the real world are
inevitable. Human input is asynchronous, for example. If handled carelessly, these asynchronous inputs can lead to metastable voltages
within the system, causing erratic system failures that are extremely
difficult to track down and correct. The goal of a digital system
designer should be to ensure that, given asynchronous inputs, the probability of encountering a metastable voltage is sufficiently small.
“Sufficiently” depends on the context. For a digital cell phone, perhaps
one failure in 10 years is acceptable, because the user can always turn
the phone off and back on if it locks up. For a medical device, one failure in the expected life of the universe (1010 years) is a better target. To
guarantee good logic levels, all asynchronous inputs should be passed
through synchronizers.
P(tres 
 t)  T0
Tc
et

144 CHAPTER THREE Sequential Logic Design
metastable
stable stable
Figure 3.49 Stable and
metastable states
Cha
A synchronizer, shown in Figure 3.50, is a device that receives an
asynchronous input, D, and a clock, CLK. It produces an output, Q,
within a bounded amount of time; the output has a valid logic level with
extremely high probability. If D is stable during the aperture, Q should
take on the same value as D. If D changes during the aperture, Q may
take on either a HIGH or LOW value but must not be metastable.
Figure 3.51 shows a simple way to build a synchronizer out of two
flip-flops. F1 samples D on the rising edge of CLK. If D is changing at
that time, the output D2 may be momentarily metastable. If the clock
period is long enough, D2 will, with high probability, resolve to a valid
logic level before the end of the period. F2 then samples D2, which is
now stable, producing a good output Q.
We say that a synchronizer fails if Q, the output of the synchronizer,
becomes metastable. This may happen if D2 has not resolved to a valid
level by the time it must setup at F2—that is, if tres 
 Tctsetup.
According to Equation 3.23, the probability of failure for a single input
change at a random time is
(3.24)
The probability of failure, P(failure), is the probability that the output,
Q, will be metastable upon a single change in D. If D changes once per
second, the probability of failure per second is just P(failure). However,
if D changes N times per second, the probability of failure per second is
N times as great:
P(failure)/sec N (3.25) T0
Tc
e
 Tctsetup

P(failure) T0
Tc
e
Tctsetup

3.5 Timing of Sequential Logic 145
D Q
CLKSYNC
Figure 3.50 Synchronizer
symbol
D
Q
D2 Q
D2
Tc
tsetup tpcq
CLK CLK
CLK
tres
metastable
F1 F2
Figure 3.51 Simple synchronizer
Chapter
System reliability is usually measured in mean time between failures
(MTBF). As the name might suggest, MTBF is the average amount of
time between failures of the system. It is the reciprocal of the probability
that the system will fail in any given second
(3.26)
Equation 3.26 shows that the MTBF improves exponentially as the
synchronizer waits for a longer time, Tc. For most systems, a synchronizer that waits for one clock cycle provides a safe MTBF. In exceptionally high-speed systems, waiting for more cycles may be necessary.
Example 3.13 SYNCHRONIZER FOR FSM INPUT
The traffic light controller FSM from Section 3.4.1 receives asynchronous inputs
from the traffic sensors. Suppose that a synchronizer is used to guarantee stable
inputs to the controller. Traffic arrives on average 0.2 times per second. The flipflops in the synchronizer have the following characteristics:   200 ps, T0
150 ps, and tsetup  500 ps. How long must the synchronizer clock period be for
the MTBF to exceed 1 year?
Solution: 1 year    107 seconds. Solve Equation 3.26.
(3.27)
This equation has no closed form solution. However, it is easy enough to solve by
guess and check. In a spreadsheet, try a few values of Tc and calculate the MTBF
until discovering the value of Tc that gives an MTBF of 1 year: Tc  3.036 ns.
3.5.6 Derivation of Resolution Time*
Equation 3.23 can be derived using a basic knowledge of circuit theory,
differential equations, and probability. This section can be skipped if you
are not interested in the derivation or if you are unfamiliar with the
mathematics.
A flip-flop output will be metastable after some time, t, if the flipflop samples a changing input (causing a metastable condition) and the
output does not resolve to a valid level within that time after the clock
edge. Symbolically, this can be expressed as
P(t (3.28) res 
 t) P(samples changing input) P(unresolved)
107  Tc e
Tc5001012
2001012
(0.2)(1501012)
MTBF  1
P(failure)/sec  Tc e
Tc  tsetup

NT0
146 CHAPTER THREE Sequential Logic Design
Chapter 03.qxd
We consider each probability term individually. The asynchronous
input signal switches between 0 and 1 in some time, tswitch, as shown in
Figure 3.52. The probability that the input changes during the aperture
around the clock edge is
(3.29)
If the flip-flop does enter metastability—that is, with probability
P(samples changing input)—the time to resolve from metastability
depends on the inner workings of the circuit. This resolution time determines P(unresolved), the probability that the flip-flop has not yet
resolved to a valid logic level after a time t. The remainder of this section
analyzes a simple model of a bistable device to estimate this probability.
A bistable device uses storage with positive feedback. Figure 3.53(a)
shows this feedback implemented with a pair of inverters; this circuit’s
behavior is representative of most bistable elements. A pair of inverters
behaves like a buffer. Let us model it as having the symmetric DC transfer
characteristics shown in Figure 3.53(b), with a slope of G. The buffer can
deliver only a finite amount of output current; we can model this as an
output resistance, R. All real circuits also have some capacitance, C, that
must be charged up. Charging the capacitor through the resistor causes
an RC delay, preventing the buffer from switching instantaneously.
Hence, the complete circuit model is shown in Figure 3.53(c), where
vout(t) is the voltage of interest conveying the state of the bistable device.
The metastable point for this circuit is vout(t)  vin(t)  VDD/2; if the
circuit began at exactly that point, it would remain there indefinitely in the
P(samples changing input) tswitch  tsetup  thold
Tc
3.5 Timing of Sequential Logic 147
CLK
D
Tc
tswitch
tsetup thold Figure 3.52 Input timing
(a)
v(t)
vout(t)
R
C
(c)
vin(t) i(t)
G
(b)
vin
vout
slope = G
VDD 0
VDD
VDD/2
VDD/2 ∆V Figure 3.53 Circuit model
of bistable device
Cha
absence of noise. Because voltages are continuous variables, the chance that
the circuit will begin at exactly the metastable point is vanishingly small.
However, the circuit might begin at time 0 near metastability at vout(0)
VDD/2  V for some small offset V. In such a case, the positive feedback
will eventually drive vout(t) to VDD if V 
 0 and to 0 if V 	 0. The time
required to reach VDD or 0 is the resolution time of the bistable device.
The DC transfer characteristic is nonlinear, but it appears linear near
the metastable point, which is the region of interest to us. Specifically, if
vin(t)  VDD/2  V/G, then vout(t)  VDD/2  V for small V. The
current through the resistor is i(t)  (vout(t)  vin(t))/R. The capacitor
charges at a rate dvin(t)/dt  i(t)/C. Putting these facts together, we find
the governing equation for the output voltage.
(3.30)
This is a linear first-order differential equation. Solving it with the initial
condition vout(0)  VDD/2  V gives
(3.31)
Figure 3.54 plots trajectories for vout(t) given various starting points.
vout(t) moves exponentially away from the metastable point VDD/2 until
it saturates at VDD or 0. The output voltage eventually resolves to 1 or 0.
The amount of time this takes depends on the initial voltage offset (V)
from the metastable point (VDD/2).
Solving Equation 3.31 for the resolution time tres, such that vout(tres)
 VDD or 0, gives
(3.32)
t (3.33) res RC
G  1 ln VDD
2|V|
|V|e
(G1)tres
RC  VDD
2
vout (t)  VDD
2  Ve
(G1)t
RC
dvout(t)
dt  (G1)
RC vout(t) VDD
2 
148 CHAPTER THREE Sequential Logic Design
0
VDD/2
VDD
t
vout(t)
Figure 3.54 Resolution
trajectories
Chapter 03.qxd 1/2
In summary, the resolution time increases if the bistable device has high
resistance or capacitance that causes the output to change slowly.
It decreases if the bistable device has high gain, G. The resolution time
also increases logarithmically as the circuit starts closer to the metastable
point (V → 0).
Define  as . Solving Equation 3.33 for V finds the initial
offset, Vres, that gives a particular resolution time, tres:
(3.34)
Suppose that the bistable device samples the input while it is changing. It measures a voltage, vin(0), which we will assume is uniformly distributed between 0 and VDD. The probability that the output has not
resolved to a legal value after time tres depends on the probability that
the initial offset is sufficiently small. Specifically, the initial offset on vout
must be less than Vres, so the initial offset on vin must be less than
Vres/G. Then the probability that the bistable device samples the input
at a time to obtain a sufficiently small initial offset is
(3.35)
Putting this all together, the probability that the resolution time exceeds
some time, t, is given by the following equation:
(3.36)
Observe that Equation 3.36 is in the form of Equation 3.23, where
T0  (tswitch  tsetup  thold)/G and   RC/(G  1). In summary, we
have derived Equation 3.23 and shown how T0 and  depend on physical properties of the bistable device.
3.6 PARALLELISM
The speed of a system is measured in latency and throughput of tokens
moving through a system. We define a token to be a group of inputs that
are processed to produce a group of outputs. The term conjures up the
notion of placing subway tokens on a circuit diagram and moving them
around to visualize data moving through the circuit. The latency of a system is the time required for one token to pass through the system from
start to end. The throughput is the number of tokens that can be produced per unit time.
Ptres 
 t  tswitch  tsetup  thold
GTc
e
 t

P(unresolved) Pvin(0)  VDD
2 	
Vres
G  2Vres
GVDD
Vres  VDD
2 etres/
RC
G1
3.6 Parallelism 149
Chapter 03.
Example 3.14 COOKIE THROUGHPUT AND LATENCY
Ben Bitdiddle is throwing a milk and cookies party to celebrate the installation
of his traffic light controller. It takes him 5 minutes to roll cookies and place
them on his tray. It then takes 15 minutes for the cookies to bake in the oven.
Once the cookies are baked, he starts another tray. What is Ben’s throughput and
latency for a tray of cookies?
Solution: In this example, a tray of cookies is a token. The latency is 1/3 hour
per tray. The throughput is 3 trays/hour.
As you might imagine, the throughput can be improved by processing
several tokens at the same time. This is called parallelism, and it comes in
two forms: spatial and temporal. With spatial parallelism, multiple copies
of the hardware are provided so that multiple tasks can be done at the
same time. With temporal parallelism, a task is broken into stages, like an
assembly line. Multiple tasks can be spread across the stages. Although
each task must pass through all stages, a different task will be in each
stage at any given time so multiple tasks can overlap. Temporal parallelism is commonly called pipelining. Spatial parallelism is sometimes just
called parallelism, but we will avoid that naming convention because it is
ambiguous.
Example 3.15 COOKIE PARALLELISM
Ben Bitdiddle has hundreds of friends coming to his party and needs to bake
cookies faster. He is considering using spatial and/or temporal parallelism.
Spatial Parallelism: Ben asks Alyssa P. Hacker to help out. She has her own
cookie tray and oven.
Temporal Parallelism: Ben gets a second cookie tray. Once he puts one cookie
tray in the oven, he starts rolling cookies on the other tray rather than waiting
for the first tray to bake.
What is the throughput and latency using spatial parallelism? Using temporal
parallelism? Using both?
Solution: The latency is the time required to complete one task from start to
finish. In all cases, the latency is 1/3 hour. If Ben starts with no cookies, the
latency is the time needed for him to produce the first cookie tray.
The throughput is the number of cookie trays per hour. With spatial parallelism, Ben
and Alyssa each complete one tray every 20 minutes. Hence, the throughput doubles,
to 6 trays/hour. With temporal parallelism, Ben puts a new tray in the oven every 15
minutes, for a throughput of 4 trays/hour. These are illustrated in Figure 3.55.
If Ben and Alyssa use both techniques, they can bake 8 trays/hour.
150 CHAPTER THREE Sequential Logic Design

Consider a task with latency L. In a system with no parallelism, the
throughput is 1/L. In a spatially parallel system with N copies of the
hardware, the throughput is N/L. In a temporally parallel system, the
task is ideally broken into N steps, or stages, of equal length. In such a
case, the throughput is also N/L, and only one copy of the hardware is
required. However, as the cookie example showed, finding N steps of
equal length is often impractical. If the longest step has a latency L1, the
pipelined throughput is 1/L1.
Pipelining (temporal parallelism) is particularly attractive because it
speeds up a circuit without duplicating the hardware. Instead, registers
are placed between blocks of combinational logic to divide the logic into
shorter stages that can run with a faster clock. The registers prevent a
token in one pipeline stage from catching up with and corrupting the
token in the next stage.
Figure 3.56 shows an example of a circuit with no pipelining. It contains four blocks of logic between the registers. The critical path passes
through blocks 2, 3, and 4. Assume that the register has a clock-to-Q
propagation delay of 0.3 ns and a setup time of 0.2 ns. Then the cycle
time is Tc  0.3  3  2  4  0.2  9.5 ns. The circuit has a latency
of 9.5 ns and a throughput of 1/9.5 ns  105 MHz.
Figure 3.57 shows the same circuit partitioned into a two-stage
pipeline by adding a register between blocks 3 and 4. The first stage has
a minimum clock period of 0.3  3  2  0.2  5.5 ns. The second
3.6 Parallelism 151
Spatial
Parallelism
Temporal
Parallelism
Ben 1 Ben 1
Roll
Bake
Ben 2 Ben 2
Ben 3 Ben 3
Ben 1 Ben 1
Alyssa 1 Alyssa 1
Ben 2 Ben 2
Alyssa 2 Alyssa 2
Time
0 5 10 15 20 25 30 35 40 45 50
Tray 1
Tray 2
Tray 3
Tray 4
Latency:
time to
first tray
Legend
Tray 1
Tray 2
Tray 3
Figure 3.55 Spatial and temporal parallelism in the cookie kitchen
Chap
stage has a minimum clock period of 0.3  4  0.2  4.5 ns. The clock
must be slow enough for all stages to work. Hence, Tc  5.5 ns. The
latency is two clock cycles, or 11 ns. The throughput is 1/5.5 ns  182
MHz. This example shows that, in a real circuit, pipelining with two
stages almost doubles the throughput and slightly increases the latency.
In comparison, ideal pipelining would exactly double the throughput at
no penalty in latency. The discrepancy comes about because the circuit
cannot be divided into two exactly equal halves and because the registers
introduce more sequencing overhead.
Figure 3.58 shows the same circuit partitioned into a three-stage
pipeline. Note that two more registers are needed to store the results of
blocks 1 and 2 at the end of the first pipeline stage. The cycle time is now
limited by the third stage to 4.5 ns. The latency is three cycles, or 13.5 ns.
The throughput is 1/4.5 ns  222 MHz. Again, adding a pipeline stage
improves throughput at the expense of some latency.
Although these techniques are powerful, they do not apply to all
situations. The bane of parallelism is dependencies. If a current task is
152 CHAPTER THREE Sequential Logic Design
CL CL CL
CL
CLK CLK
tpd1 = 2.4 ns
tpd2 = 3 ns
tpd3 = 2 ns tpd4 = 4 ns
1
2
3 4
Stage 1: 5.5 ns Stage 2: 4.5 ns
CLK
Figure 3.57 Circuit with twostage pipeline
CL CL CL
CL
CLK CLK
tpd1 = 2.4 ns
tpd2 = 3 ns
tpd3 = 2 ns tpd4 = 4 ns
1
2
3 4
Stage 1: 3.5 ns Stage 3: 4.5 ns
CLK CLK
Stage 2: 2.5 ns
Figure 3.58 Circuit with threestage pipeline
CL CL CL
CL
CLK CLK
tpd1 = 2.4 ns
tpd2 = 3 ns
tpd3 = 2 ns tpd4 = 4 ns
1
2
3 4
Tc = 9.5 ns
Figure 3.56 Circuit with no
pipelining
Chap
dependent on the result of a prior task, rather than just prior steps in
the current task, the task cannot start until the prior task has completed. For example, if Ben wants to check that the first tray of cookies
tastes good before he starts preparing the second, he has a dependency
that prevents pipelining or parallel operation. Parallelism is one of the
most important techniques for designing high-performance microprocessors. Chapter 7 discusses pipelining further and shows examples
of handling dependencies.
3.7 SUMMARY
This chapter has described the analysis and design of sequential logic. In
contrast to combinational logic, whose outputs depend only on the current inputs, sequential logic outputs depend on both current and prior
inputs. In other words, sequential logic remembers information about
prior inputs. This memory is called the state of the logic.
Sequential circuits can be difficult to analyze and are easy to design
incorrectly, so we limit ourselves to a small set of carefully designed
building blocks. The most important element for our purposes is the
flip-flop, which receives a clock and an input, D, and produces an output, Q. The flip-flop copies D to Q on the rising edge of the clock and
otherwise remembers the old state of Q. A group of flip-flops sharing a
common clock is called a register. Flip-flops may also receive reset or
enable control signals.
Although many forms of sequential logic exist, we discipline ourselves to use synchronous sequential circuits because they are easy to
design. Synchronous sequential circuits consist of blocks of combinational logic separated by clocked registers. The state of the circuit is
stored in the registers and updated only on clock edges.
Finite state machines are a powerful technique for designing sequential circuits. To design an FSM, first identify the inputs and outputs of
the machine and sketch a state transition diagram, indicating the states
and the transitions between them. Select an encoding for the states, and
rewrite the diagram as a state transition table and output table, indicating the next state and output given the current state and input. From
these tables, design the combinational logic to compute the next state
and output, and sketch the circuit.
Synchronous sequential circuits have a timing specification including
the clock-to-Q propagation and contamination delays, tpcq and tccq, and
the setup and hold times, tsetup and thold. For correct operation, their
inputs must be stable during an aperture time that starts a setup time
before the rising edge of the clock and ends a hold time after the
rising edge of the clock. The minimum cycle time, Tc, of the system is
equal to the propagation delay, tpd, through the combinational logic plus
3.7 Summary 153
Anyone who could invent
logic whose outputs depend
on future inputs would be
fabulously wealthy!

154 CHAPTER THREE Sequential Logic Design
tpcq  tsetup of the register. For correct operation, the contamination
delay through the register and combinational logic must be greater than
thold. Despite the common misconception to the contrary, hold time does
not affect the cycle time.
Overall system performance is measured in latency and throughput.
The latency is the time required for a token to pass from start to end.
The throughput is the number of tokens that the system can process per
unit time. Parallelism improves the system throughput.

Exercises 155
Exercises
Exercise 3.1 Given the input waveforms shown in Figure 3.59, sketch the output, Q, of an SR latch.
S
R
Figure 3.59 Input waveform of SR latch
Exercise 3.2 Given the input waveforms shown in Figure 3.60, sketch the output, Q, of a D latch.
Exercise 3.3 Given the input waveforms shown in Figure 3.60, sketch the output, Q, of a D flip-flop.
Exercise 3.4 Is the circuit in Figure 3.61 combinational logic or sequential logic?
Explain in a simple fashion what the relationship is between the inputs and
outputs. What would you call this circuit?
Exercise 3.5 Is the circuit in Figure 3.62 combinational logic or sequential logic?
Explain in a simple fashion what the relationship is between the inputs and
outputs. What would you call this circuit?
CLK
D
Figure 3.60 Input waveform of D latch or flip-flop
S
R
Q
Q
Figure 3.61 Mystery circuit

156 CHAPTER THREE Sequential Logic Design
Exercise 3.6 The toggle (T) flip-flop has one input, CLK, and one output, Q.
On each rising edge of CLK, Q toggles to the complement of its previous value.
Draw a schematic for a T flip-flop using a D flip-flop and an inverter.
Exercise 3.7 A JK flip-flop receives a clock and two inputs, J and K. On the
rising edge of the clock, it updates the output, Q. If J and K are both 0,
Q retains its old value. If only J is 1, Q becomes 1. If only K is 1, Q becomes
0. If both J and K are 1, Q becomes the opposite of its present state.
(a) Construct a JK flip-flop using a D flip-flop and some combinational logic.
(b) Construct a D flip-flop using a JK flip-flop and some combinational logic.
(c) Construct a T flip-flop (see Exercise 3.6) using a JK flip-flop.
Exercise 3.8 The circuit in Figure 3.63 is called a Muller C-element. Explain in a
simple fashion what the relationship is between the inputs and output.
Q
Q
S
R
S
R
D
R
CLK
Figure 3.62 Mystery circuit
A
B
A
B
C
weak
Figure 3.63 Muller C-element
Exercise 3.9 Design an asynchronously resettable D latch using logic gates.
Exercise 3.10 Design an asynchronously resettable D flip-flop using logic gates.
Exercise 3.11 Design a synchronously settable D flip-flop using logic gates.

Exercises 157
Exercise 3.12 Design an asynchronously settable D flip-flop using logic gates.
Exercise 3.13 Suppose a ring oscillator is built from N inverters connected in a
loop. Each inverter has a minimum delay of tcd and a maximum delay of tpd. If
N is odd, determine the range of frequencies at which the oscillator might
operate.
Exercise 3.14 Why must N be odd in Exercise 3.13?
Exercise 3.15 Which of the circuits in Figure 3.64 are synchronous sequential
circuits? Explain.
Exercise 3.16 You are designing an elevator controller for a building with 25
floors. The controller has two inputs: UP and DOWN. It produces an output
indicating the floor that the elevator is on. There is no floor 13. What is the minimum number of bits of state in the controller?
Exercise 3.17 You are designing an FSM to keep track of the mood of four students working in the digital design lab. Each student’s mood is either HAPPY
(the circuit works), SAD (the circuit blew up), BUSY (working on the circuit),
CLUELESS (confused about the circuit), or ASLEEP (face down on the circuit
board). How many states does the FSM have? What is the minimum number of
bits necessary to represent these states?
Exercise 3.18 How would you factor the FSM from Exercise 3.17 into multiple
simpler machines? How many states does each simpler machine have? What is
the minimum total number of bits necessary in this factored design?
(a)
CL CL
CLK
CL
CL
CL
CL
CLK
CL
(b)
(c) (d)
CL CL
CLK
Figure 3.64 Circuits

Exercise 3.19 Describe in words what the state machine in Figure 3.65 does.
Using binary state encodings, complete a state transition table and output table
for the FSM. Write Boolean equations for the next state and output and sketch
a schematic of the FSM.
Exercise 3.20 Describe in words what the state machine in Figure 3.66 does.
Using binary state encodings, complete a state transition table and output table
for the FSM. Write Boolean equations for the next state and output and sketch
a schematic of the FSM.
Exercise 3.21 Accidents are still occurring at the intersection of Academic
Avenue and Bravado Boulevard. The football team is rushing into the intersection the moment light B turns green. They are colliding with sleep-deprived CS
majors who stagger into the intersection just before light A turns red. Extend the
traffic light controller from Section 3.4.1 so that both lights are red for 5 seconds
before either light turns green again. Sketch your improved Moore machine state
transition diagram, state encodings, state transition table, output table, next
state and output equations, and your FSM schematic.
Exercise 3.22 Alyssa P. Hacker’s snail from Section 3.4.3 has a daughter with a
Mealy machine FSM brain. The daughter snail smiles whenever she slides over
the pattern 1101 or the pattern 1110. Sketch the state transition diagram for this
happy snail using as few states as possible. Choose state encodings and write a
158 CHAPTER THREE Sequential Logic Design
S0
Q: 0
S1
Q: 0
S2
Q: 1
Reset
A B
A
B
Figure 3.65 State transition diagram
S0 S1 S2
Reset
A/0 B/0
A/0
B/0 AB/1
A + B/0
Figure 3.66 State transition diagram

Exercises 159
combined state transition and output table using your encodings. Write the next
state and output equations and sketch your FSM schematic.
Exercise 3.23 You have been enlisted to design a soda machine dispenser for
your department lounge. Sodas are partially subsidized by the student chapter of
the IEEE, so they cost only 25 cents. The machine accepts nickels, dimes, and
quarters. When enough coins have been inserted, it dispenses the soda and
returns any necessary change. Design an FSM controller for the soda machine.
The FSM inputs are Nickel, Dime, and Quarter, indicating which coin was
inserted. Assume that exactly one coin is inserted on each cycle. The outputs are
Dispense, ReturnNickel, ReturnDime, and ReturnTwoDimes. When the FSM
reaches 25 cents, it asserts Dispense and the necessary Return outputs required
to deliver the appropriate change. Then it should be ready to start accepting
coins for another soda.
Exercise 3.24 Gray codes have a useful property in that consecutive numbers
differ in only a single bit position. Table 3.17 lists a 3-bit Gray code representing
the numbers 0 to 7. Design a 3-bit modulo 8 Gray code counter FSM with no
inputs and three outputs. (A modulo N counter counts from 0 to N1, then
repeats. For example, a watch uses a modulo 60 counter for the minutes and seconds that counts from 0 to 59.) When reset, the output should be 000. On each
clock edge, the output should advance to the next Gray code. After reaching
100, it should repeat with 000.
Table 3.17 3-bit Gray code
Number Gray code
0 000
1 001
2 011
3 010
4 110
5 111
6 101
7 100
Exercise 3.25 Extend your modulo 8 Gray code counter from Exercise 3.24 to be
an UP/DOWN counter by adding an UP input. If UP  1, the counter advances
to the next number. If UP  0, the counter retreats to the previous number.
Cha
Exercise 3.26 Your company, Detect-o-rama, would like to design an FSM that
takes two inputs, A and B, and generates one output, Z. The output in cycle n,
Zn, is either the Boolean AND or OR of the corresponding input An and the previous input An1, depending on the other input, Bn:
(a) Sketch the waveform for Z given the inputs shown in Figure 3.67.
(b) Is this FSM a Moore or a Mealy machine?
(c) Design the FSM. Show your state transition diagram, encoded state transition table, next state and output equations, and schematic.
 ZnAn An1 if Bn 1
Zn An An1 if Bn  0
160 CHAPTER THREE Sequential Logic Design
CLK
A
B
Figure 3.67 FSM input waveforms
Exercise 3.27 Design an FSM with one input, A, and two outputs, X and Y. X
should be 1 if A has been 1 for at least three cycles altogether (not necessarily
consecutively). Y should be 1 if A has been 1 for at least two consecutive cycles.
Show your state transition diagram, encoded state transition table, next state
and output equations, and schematic.
Exercise 3.28 Analyze the FSM shown in Figure 3.68. Write the state transition
and output tables and sketch the state transition diagram. Describe in words
what the FSM does.
CLK CLK
X
Q
Figure 3.68 FSM schematic
Chapter
Exercises 161
Exercise 3.29 Repeat Exercise 3.28 for the FSM shown in Figure 3.69. Recall
that the r and s register inputs indicate set and reset, respectively.
Exercise 3.30 Ben Bitdiddle has designed the circuit in Figure 3.70 to compute
a registered four-input XOR function. Each two-input XOR gate has a propagation delay of 100 ps and a contamination delay of 55 ps. Each flip-flop has a
setup time of 60 ps, a hold time of 20 ps, a clock-to-Q maximum delay of
70 ps, and a clock-to-Q minimum delay of 50 ps.
(a) If there is no clock skew, what is the maximum operating frequency of the
circuit?
(b) How much clock skew can the circuit tolerate if it must operate at 2 GHz?
(c) How much clock skew can the circuit tolerate before it might experience
a hold time violation?
(d) Alyssa P. Hacker points out that she can redesign the combinational logic
between the registers to be faster and tolerate more clock skew. Her
improved circuit also uses three two-input XORs, but they are arranged
differently. What is her circuit? What is its maximum frequency if there is
no clock skew? How much clock skew can the circuit tolerate before it
might experience a hold time violation?
CLK
A
CLK
CLK
Q
reset
r
r
s
Figure 3.69 FSM schematic
CLK
CLK
Figure 3.70 Registered four-input XOR circuit
Exercise 3.31 You are designing an adder for the blindingly fast 2-bit
RePentium Processor. The adder is built from two full adders such that the
carry out of the first adder is the carry in to the second adder, as shown in
Figure 3.71. Your adder has input and output registers and must complete the

addition in one clock cycle. Each full adder has the following propagation
delays: 20 ps from Cin to Cout or to Sum (S), 25 ps from A or B to Cout, and 30
ps from A or B to S. The adder has a contamination delay of 15 ps from Cin to
either output and 22 ps from A or B to either output. Each flip-flop has a setup
time of 30 ps, a hold time of 10 ps, a clock-to-Q propagation delay of 35 ps,
and a clock-to-Q contamination delay of 21 ps.
(a) If there is no clock skew, what is the maximum operating frequency of the
circuit?
(b) How much clock skew can the circuit tolerate if it must operate at 8 GHz?
(c) How much clock skew can the circuit tolerate before it might experience a
hold time violation?
Exercise 3.32 A field programmable gate array (FPGA) uses configurable logic
blocks (CLBs) rather than logic gates to implement combinational logic. The
Xilinx Spartan 3 FPGA has propagation and contamination delays of 0.61 and
0.30 ns, respectively for each CLB. It also contains flip-flops with propagation
and contamination delays of 0.72 and 0.50 ns, and setup and hold times of 0.53
and 0 ns, respectively.
(a) If you are building a system that needs to run at 40 MHz, how many
consecutive CLBs can you use between two flip-flops? Assume there is no
clock skew and no delay through wires between CLBs.
(b) Suppose that all paths between flip-flops pass through at least one CLB.
How much clock skew can the FPGA have without violating the hold time?
Exercise 3.33 A synchronizer is built from a pair of flip-flops with tsetup  50 ps,
T0  20 ps, and   30 ps. It samples an asynchronous input that changes 108
times per second. What is the minimum clock period of the synchronizer to
achieve a mean time between failures (MTBF) of 100 years?
162 CHAPTER THREE Sequential Logic Design
A
B
Cin
Cout
S
A
B
Cin
Cout
S
CLK
C
A0
B0
A1
B1
S0
S1
CLK
Figure 3.71 2-bit adder schematic
Cha
Exercises 163
Exercise 3.34 You would like to build a synchronizer that can receive
asynchronous inputs with an MTBF of 50 years. Your system is running at
1 GHz, and you use sampling flip-flops with   100 ps, T0  110 ps, and
tsetup  70 ps. The synchronizer receives a new asynchronous input on
average 0.5 times per second (i.e., once every 2 seconds). What is the
required probability of failure to satisfy this MTBF? How many clock cycles
would you have to wait before reading the sampled input signal to give that
probability of error?
Exercise 3.35 You are walking down the hallway when you run into your
lab partner walking in the other direction. The two of you first step one
way and are still in each other’s way. Then you both step the other way
and are still in each other’s way. Then you both wait a bit, hoping the
other person will step aside. You can model this situation as a metastable
point and apply the same theory that has been applied to synchronizers
and flip-flops. Suppose you create a mathematical model for yourself and
your lab partner. You start the unfortunate encounter in the metastable
state. The probability that you remain in this state after t seconds is .
 indicates your response rate; today, your brain has been blurred by lack of
sleep and has   20 seconds.
(a) How long will it be until you have 99% certainty that you will have
resolved from metastability (i.e., figured out how to pass one another)?
(b) You are not only sleepy, but also ravenously hungry. In fact, you will
starve to death if you don’t get going to the cafeteria within 3 minutes.
What is the probability that your lab partner will have to drag you to
the morgue?
Exercise 3.36 You have built a synchronizer using flip-flops with T0  20 ps
and   30 ps. Your boss tells you that you need to increase the MTBF by a factor of 10. By how much do you need to increase the clock period?
Exercise 3.37 Ben Bitdiddle invents a new and improved synchronizer in
Figure 3.72 that he claims eliminates metastability in a single cycle. He
explains that the circuit in box M is an analog “metastability detector” that
produces a HIGH output if the input voltage is in the forbidden zone
between VIL and VIH. The metastability detector checks to determine
e t

CLK
D r
CLK
Q
M
D2
Figure 3.72 “New and improved” synchronizer
Chapter
164 CHAPTER THREE Sequential Logic Design
whether the first flip-flop has produced a metastable output on D2. If so,
it asynchronously resets the flip-flop to produce a good 0 at D2. The second
flip-flop then samples D2, always producing a valid logic level on Q.
Alyssa P. Hacker tells Ben that there must be a bug in the circuit, because
eliminating metastability is just as impossible as building a perpetual motion
machine. Who is right? Explain, showing Ben’s error or showing why Alyssa
is wrong.

Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 3.1 Draw a state machine that can detect when it has received the serial
input sequence 01010.
Question 3.2 Design a serial (one bit at a time) two’s complementer FSM with
two inputs, Start and A, and one output, Q. A binary number of arbitrary length
is provided to input A, starting with the least significant bit. The corresponding
bit of the output appears at Q on the same cycle. Start is asserted for one cycle
to initialize the FSM before the least significant bit is provided.
Question 3.3 What is the difference between a latch and a flip-flop? Under what
circumstances is each one preferable?
Question 3.4 Design a 5-bit counter finite state machine.
Question 3.5 Design an edge detector circuit. The output should go HIGH for
one cycle after the input makes a 0 → 1 transition.
Question 3.6 Describe the concept of pipelining and why it is used.
Question 3.7 Describe what it means for a flip-flop to have a negative hold time.
Question 3.8 Given signal A, shown in Figure 3.73, design a circuit that
produces signal B.
Interview Questions 165
A
B
Figure 3.73 Signal waveforms
Question 3.9 Consider a block of logic between two registers. Explain the timing
constraints. If you add a buffer on the clock input of the receiver (the second
flip-flop), does the setup time constraint get better or worse?


4
4.1 Introduction
4.2 Combinational Logic
4.3 Structural Modeling
4.4 Sequential Logic
4.5 More Combinational Logic
4.6 Finite State Machines
4.7 Parameterized Modules*
4.8 Testbenches
4.9 Summary
Exercises
Interview Questions
Hardware Description Languages
4.1 INTRODUCTION
Thus far, we have focused on designing combinational and sequential
digital circuits at the schematic level. The process of finding an efficient
set of logic gates to perform a given function is labor intensive and error
prone, requiring manual simplification of truth tables or Boolean equations and manual translation of finite state machines (FSMs) into gates.
In the 1990’s, designers discovered that they were far more productive if
they worked at a higher level of abstraction, specifying just the logical
function and allowing a computer-aided design (CAD) tool to produce
the optimized gates. The specifications are generally given in a hardware
description language (HDL). The two leading hardware description languages are Verilog and VHDL.
Verilog and VHDL are built on similar principles but have different
syntax. Discussion of these languages in this chapter is divided into two
columns for literal side-by-side comparison, with Verilog on the left and
VHDL on the right. When you read the chapter for the first time, focus
on one language or the other. Once you know one, you’ll quickly master
the other if you need it.
Subsequent chapters show hardware in both schematic and HDL
form. If you choose to skip this chapter and not learn one of the HDLs,
you will still be able to master the principles of computer organization
from the schematics. However, the vast majority of commercial systems
are now built using HDLs rather than schematics. If you expect to do
digital design at any point in your professional life, we urge you to learn
one of the HDLs.
4.1.1 Modules
A block of hardware with inputs and outputs is called a module. An AND
gate, a multiplexer, and a priority circuit are all examples of hardware
modules. The two general styles for describing module functionality are
167

behavioral and structural. Behavioral models describe what a module
does. Structural models describe how a module is built from simpler
pieces; it is an application of hierarchy. The Verilog and VHDL code in
HDL Example 4.1 illustrate behavioral descriptions of a module that
computes the Boolean function from Example 2.6,
In both languages, the module is named sillyfunction and has three
inputs, a, b, and c, and one output, y.
y  abcabc  abc.
168 CHAPTER FOUR Hardware Description Languages
Verilog
module sillyfunction (input a, b, c,
output y);
assign y  ~a & ~b & ~c |
a & ~b & ~c |
a & ~b & c;
endmodule
A Verilog module begins with the module name and a listing
of the inputs and outputs. The assign statement describes
combinational logic. ~ indicates NOT, & indicates AND, and
| indicates OR.
Verilog signals such as the inputs and outputs are
Boolean variables (0 or 1). They may also have floating and
undefined values, as discussed in Section 4.2.8.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity sillyfunction is
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of sillyfunction is
begin
y  ((not a) and (not b) and (not c)) or
(a and (not b) and (not c)) or
(a and (not b) and c);
end;
VHDL code has three parts: the library use clause, the entity
declaration, and the architecture body. The library use clause
is required and will be discussed in Section 4.2.11. The entity
declaration lists the module name and its inputs and outputs.
The architecture body defines what the module does.
VHDL signals, such as inputs and outputs, must have a
type declaration. Digital signals should be declared to be
STD_LOGIC type. STD_LOGIC signals can have a value of ‘0’ or
‘1’, as well as floating and undefined values that will be
described in Section 4.2.8. The STD_LOGIC type is defined in
the IEEE.STD_LOGIC_1164 library, which is why the library
must be used.
VHDL lacks a good default order of operations, so
Boolean equations should be parenthesized.
HDL Example 4.1 COMBINATIONAL LOGIC
A module, as you might expect, is a good application of modularity.
It has a well defined interface, consisting of its inputs and outputs, and it
performs a specific function. The particular way in which it is coded is
unimportant to others that might use the module, as long as it performs
its function.
4.1.2 Language Origins
Universities are almost evenly split on which of these languages is taught
in a first course, and industry is similarly split on which language is preferred. Compared to Verilog, VHDL is more verbose and cumbersome,
Chapt
as you might expect of a language developed by committee. U.S. military
contractors, the European Space Agency, and telecommunications companies use VHDL extensively.
Both languages are fully capable of describing any hardware system,
and both have their quirks. The best language to use is the one that is
already being used at your site or the one that your customers demand.
Most CAD tools today allow the two languages to be mixed, so that different modules can be described in different languages.
4.1.3 Simulation and Synthesis
The two major purposes of HDLs are logic simulation and synthesis.
During simulation, inputs are applied to a module, and the outputs are
checked to verify that the module operates correctly. During synthesis,
the textual description of a module is transformed into logic gates.
Simulation
Humans routinely make mistakes. Such errors in hardware designs are
called bugs. Eliminating the bugs from a digital system is obviously important, especially when customers are paying money and lives depend on the
correct operation. Testing a system in the laboratory is time-consuming.
Discovering the cause of errors in the lab can be extremely difficult,
because only signals routed to the chip pins can be observed. There is no
way to directly observe what is happening inside a chip. Correcting errors
after the system is built can be devastatingly expensive. For example,
correcting a mistake in a cutting-edge integrated circuit costs more than
a million dollars and takes several months. Intel’s infamous FDIV (floating
point division) bug in the Pentium processor forced the company to recall
chips after they had shipped, at a total cost of $475 million. Logic simulation is essential to test a system before it is built.
4.1 Introduction 169
The term “bug” predates the
invention of the computer.
Thomas Edison called the “little faults and difficulties” with
his inventions “bugs” in 1878.
The first real computer bug
was a moth, which got caught
between the relays of the
Harvard Mark II electromechanical computer in 1947.
It was found by Grace Hopper,
who logged the incident, along
with the moth itself and the
comment “first actual case of
bug being found.”
Source: Notebook entry courtesy Naval Historical Center,
US Navy; photo No. NII
96566-KN
1 The Institute of Electrical and Electronics Engineers (IEEE) is a professional society
responsible for many computing standards including WiFi (802.11), Ethernet (802.3),
and floating-point numbers (754) (see Chapter 5).
VHDL
VHDL is an acronym for the VHSIC Hardware Description
Language. VHSIC is in turn an acronym for the Very High
Speed Integrated Circuits program of the US Department of
Defense.
VHDL was originally developed in 1981 by the Department of Defense to describe the structure and function of
hardware. Its roots draw from the Ada programming language. The IEEE standardized it in 1987 (IEEE STD 1076)
and has updated the standard several times since. The language was first envisioned for documentation but was
quickly adopted for simulation and synthesis.
Verilog
Verilog was developed by Gateway Design Automation as a
proprietary language for logic simulation in 1984. Gateway
was acquired by Cadence in 1989 and Verilog was made an
open standard in 1990 under the control of Open Verilog
International. The language became an IEEE standard1 in
1995 (IEEE STD 1364) and was updated in 2001.

Figure 4.1 shows waveforms from a simulation2 of the previous
sillyfunction module demonstrating that the module works correctly.
y is TRUE when a, b, and c are 000, 100, or 101, as specified by the
Boolean equation.
Synthesis
Logic synthesis transforms HDL code into a netlist describing the hardware (e.g., the logic gates and the wires connecting them). The logic synthesizer might perform optimizations to reduce the amount of hardware
required. The netlist may be a text file, or it may be drawn as a
schematic to help visualize the circuit. Figure 4.2 shows the results of
synthesizing the sillyfunction module.3 Notice how the three threeinput AND gates are simplified into two two-input AND gates, as we
discovered in Example 2.6 using Boolean algebra.
Circuit descriptions in HDL resemble code in a programming
language. However, you must remember that the code is intended to
represent hardware. Verilog and VHDL are rich languages with many
commands. Not all of these commands can be synthesized into hardware. For example, a command to print results on the screen during simulation does not translate into hardware. Because our primary interest is
170 CHAPTER FOUR Hardware Description Languages
0 ns
0
a 0
Now:
800 ns
b
c
y 0
0
160 320 ns 480 640 ns 800
Figure 4.1 Simulation waveforms
un5_y
un8_y
y
c y
b
a
Figure 4.2 Synthesized circuit
2 The simulation was performed with the Xilinx ISE Simulator, which is part of the Xilinx
ISE 8.2 software. The simulator was selected because it is used commercially, yet is freely
available to universities.
3 Synthesis was performed with Synplify Pro from Synplicity. The tool was selected
because it is the leading commercial tool for synthesizing HDL to field-programmable
gate arrays (see Section 5.6.2) and because it is available inexpensively for universities.

to build hardware, we will emphasize a synthesizable subset of the languages. Specifically, we will divide HDL code into synthesizable modules
and a testbench. The synthesizable modules describe the hardware. The
testbench contains code to apply inputs to a module, check whether the
output results are correct, and print discrepancies between expected and
actual outputs. Testbench code is intended only for simulation and
cannot be synthesized.
One of the most common mistakes for beginners is to think of HDL
as a computer program rather than as a shorthand for describing digital
hardware. If you don’t know approximately what hardware your HDL
should synthesize into, you probably won’t like what you get. You might
create far more hardware than is necessary, or you might write code that
simulates correctly but cannot be implemented in hardware. Instead,
think of your system in terms of blocks of combinational logic, registers,
and finite state machines. Sketch these blocks on paper and show how
they are connected before you start writing code.
In our experience, the best way to learn an HDL is by example.
HDLs have specific ways of describing various classes of logic; these
ways are called idioms. This chapter will teach you how to write
the proper HDL idioms for each type of block and then how to put
the blocks together to produce a working system. When you need to
describe a particular kind of hardware, look for a similar example
and adapt it to your purpose. We do not attempt to rigorously
define all the syntax of the HDLs, because that is deathly boring and
because it tends to encourage thinking of HDLs as programming
languages, not shorthand for hardware. The IEEE Verilog and
VHDL specifications, and numerous dry but exhaustive textbooks,
contain all of the details, should you find yourself needing more information on a particular topic. (See Further Readings section at back of
the book.)
4.2 COMBINATIONAL LOGIC
Recall that we are disciplining ourselves to design synchronous sequential circuits, which consist of combinational logic and registers. The outputs of combinational logic depend only on the current inputs. This
section describes how to write behavioral models of combinational logic
with HDLs.
4.2.1 Bitwise Operators
Bitwise operators act on single-bit signals or on multi-bit busses. For
example, the inv module in HDL Example 4.2 describes four inverters
connected to 4-bit busses.
4.2 Combinational Logic 171

The endianness of a bus is purely arbitrary. (See the sidebar in Section
6.2.2 for the origin of the term.) Indeed, endianness is also irrelevant to
this example, because a bank of inverters doesn’t care what the order of
the bits are. Endianness matters only for operators, such as addition,
where the sum of one column carries over into the next. Either ordering is
acceptable, as long as it is used consistently. We will consistently use the
little-endian order, [N 1:0] in Verilog and (N 1 downto 0) in VHDL,
for an N-bit bus.
After each code example in this chapter is a schematic produced
from the Verilog code by the Synplify Pro synthesis tool. Figure 4.3
shows that the inv module synthesizes to a bank of four inverters,
indicated by the inverter symbol labeled y[3:0]. The bank of inverters
connects to 4-bit input and output busses. Similar hardware is produced
from the synthesized VHDL code.
The gates module in HDL Example 4.3 demonstrates bitwise operations acting on 4-bit busses for other basic logic functions.
172 CHAPTER FOUR Hardware Description Languages
y[3:0]
a[3:0] y[3:0] [3:0] [3:0]
Figure 4.3 inv synthesized circuit
Verilog
module inv (input [3:0] a,
output [3:0] y);
assign y  ~a;
endmodule
a[3:0] represents a 4-bit bus. The bits, from most significant
to least significant, are a[3], a[2], a[1], and a[0]. This is
called little-endian order, because the least significant bit has
the smallest bit number. We could have named the bus
a[4:1], in which case a[4] would have been the most significant. Or we could have used a[0:3], in which case the bits,
from most significant to least significant, would be a[0],
a[1], a[2], and a[3]. This is called big-endian order.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity inv is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of inv is
begin
y  not a;
end;
VHDL uses STD_LOGIC_VECTOR, to indicate busses of
STD_LOGIC. STD_LOGIC_VECTOR (3 downto 0) represents a 4-bit
bus. The bits, from most significant to least significant, are
3, 2, 1, and 0. This is called little-endian order, because the
least significant bit has the smallest bit number. We could
have declared the bus to be STD_LOGIC_VECTOR (4 downto 1),
in which case bit 4 would have been the most significant. Or
we could have written STD_LOGIC_VECTOR (0 to 3), in which
case the bits, from most significant to least significant, would
be 0, 1, 2, and 3. This is called big-endian order.
HDL Example 4.2 INVERTERS
Ch
4.2 Combinational Logic 173
y1[3:0]
y2[3:0]
y3[3:0]
y4[3:0]
y5[3:0]
y5[3:0] [3:0]
y4[3:0] [3:0]
y3[3:0] [3:0]
y2[3:0] [3:0]
y1[3:0] [3:0]
b[3:0] [3:0] a[3:0] [3:0] [3:0] [3:0] [3:0]
[3:0] [3:0] [3:0]
[3:0]
[3:0]
Figure 4.4 gates synthesized circuit
Verilog
module gates (input [3:0] a, b,
output [3:0] y1, y2,
y3, y4, y5);
/* Five different two-input logic
gates acting on 4 bit busses */
assign y1  a & b; // AND
assign y2  a | b; // OR
assign y3  a  b; // XOR
assign y4  ~(a & b); // NAND
assign y5  ~(a | b); // NOR
endmodule
~, , and | are examples of Verilog operators, whereas a, b,
and y1 are operands. A combination of operators and
operands, such as a & b, or ~(a | b), is called an expression.
A complete command such as assign y4  ~(a & b); is
called a statement.
assign out  in1 op in2; is called a continuous assignment statement. Continuous assignment statements end with a
semicolon. Anytime the inputs on the right side of the  in a
continuous assignment statement change, the output on the left
side is recomputed. Thus, continuous assignment statements
describe combinational logic.
HDL Example 4.3 LOGIC GATES
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity gates is
port (a, b: in STD_LOGIC_VECTOR (3 downto 0);
y1, y2, y3, y4,
y5: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of gates is
begin
— — Five different two-input logic gates
— —acting on 4 bit busses
y1  a and b;
y2  a or b;
y3  a xor b;
y4  a nand b;
y5  a nor b;
end;
not, xor, and or are examples of VHDL operators, whereas a,
b, and y1 are operands. A combination of operators and
operands, such as a and b, or a nor b, is called an expression.
A complete command such as y4  a nand b; is called a
statement.
out  in1 op in2; is called a concurrent signal assignment statement. VHDL assignment statements end with a
semicolon. Anytime the inputs on the right side of the  in
a concurrent signal assignment statement change, the output
on the left side is recomputed. Thus, concurrent signal
assignment statements describe combinational logic.
Chapter 04.qxd 1/3
4.2.3 Reduction Operators
Reduction operators imply a multiple-input gate acting on a single bus.
HDL Example 4.4 describes an eight-input AND gate with inputs a7,
a6, . . . , a0.
174 CHAPTER FOUR Hardware Description Languages
Verilog
Verilog comments are just like those in C or Java. Comments
beginning with /* continue, possibly across multiple lines, to
the next */. Comments beginning with // continue to the end
of the line.
Verilog is case-sensitive. y1 and Y1 are different signals
in Verilog.
VHDL
VHDL comments begin with — — and continue to the end of the
line. Comments spanning multiple lines must use — — at the
beginning of each line.
VHDL is not case-sensitive. y1 and Y1 are the same signal in VHDL. However, other tools that may read your file
might be case sensitive, leading to nasty bugs if you blithely
mix upper and lower case.
4.2.2 Comments and White Space
The gates example showed how to format comments. Verilog and
VHDL are not picky about the use of white space (i.e., spaces, tabs, and
line breaks). Nevertheless, proper indenting and use of blank lines is
helpful to make nontrivial designs readable. Be consistent in your use of
capitalization and underscores in signal and module names. Module and
signal names must not begin with a digit.
Verilog
module and8 (input [7:0] a,
output y);
assign y  &a;
// &a is much easier to write than
// assign y  a[7] & a[6] & a[5] & a[4] &
// a[3] & a[2] & a[1] & a[0];
endmodule
As one would expect, |, , ~&, and ~ | reduction operators
are available for OR, XOR, NAND, and NOR as well.
Recall that a multi-input XOR performs parity, returning
TRUE if an odd number of inputs are TRUE.
VHDL
VHDL does not have reduction operators. Instead, it provides the generate command (see Section 4.7). Alternatively,
the operation can be written explicitly, as shown below.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity and8 is
port (a: in STD_LOGIC_VECTOR (7 downto 0);
y: out STD_LOGIC);
end;
architecture synth of and8 is
begin
y  a(7) and a(6) and a(5) and a(4) and
a(3) and a(2) and a(1) and a(0);
end;
HDL Example 4.4 EIGHT-INPUT AND
Chap
4.2.4 Conditional Assignment
Conditional assignments select the output from among alternatives
based on an input called the condition. HDL Example 4.5 illustrates a
2:1 multiplexer using conditional assignment.
4.2 Combinational Logic 175
y
y
a[7:0] [7:0]
[0]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
Figure 4.5 and8 synthesized circuit
Verilog
The conditional operator ?: chooses, based on a first expression, between a second and third expression. The first
expression is called the condition. If the condition is 1, the
operator chooses the second expression. If the condition is 0,
the operator chooses the third expression.
?: is especially useful for describing a multiplexer because,
based on the first input, it selects between two others. The following code demonstrates the idiom for a 2:1 multiplexer with
4-bit inputs and outputs using the conditional operator.
module mux2 (input [3:0] d0, d1,
input s,
output [3:0] y);
assign y  s ? d1 : d0;
endmodule
If s is 1, then y  d1. If s is 0, then y  d0.
?: is also called a ternary operator, because it takes three
inputs. It is used for the same purpose in the C and Java
programming languages.
VHDL
Conditional signal assignments perform different operations
depending on some condition. They are especially useful for
describing a multiplexer. For example, a 2:1 multiplexer can
use conditional signal assignment to select one of two 4-bit
inputs.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is
port (d0, d1: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of mux2 is
begin
y  d0 when s  ‘0’ else d1;
end;
The conditional signal assignment sets y to d0 if s is 0.
Otherwise it sets y to d1.
HDL Example 4.5 2:1 MULTIPLEXER
y[3:0]
0
1
y[3:0]
s
d1[3:0]
d0[3:0] [3:0]
[3:0]
[3:0]
Figure 4.6 mux2 synthesized circuit
Chapt
HDL Example 4.6 shows a 4:1 multiplexer based on the same
principle as the 2:1 multiplexer in HDL Example 4.5.
Figure 4.7 shows the schematic for the 4:1 multiplexer produced by
Synplify Pro. The software uses a different multiplexer symbol than this
text has shown so far. The multiplexer has multiple data (d) and one-hot
enable (e) inputs. When one of the enables is asserted, the associated
data is passed to the output. For example, when s[1]  s[0]  0, the
bottom AND gate, un1_s_5, produces a 1, enabling the bottom input of
the multiplexer and causing it to select d0[3:0].
4.2.5 Internal Variables
Often it is convenient to break a complex function into intermediate
steps. For example, a full adder, which will be described in Section 5.2.1,
176 CHAPTER FOUR Hardware Description Languages
Verilog
A 4:1 multiplexer can select one of four inputs using nested
conditional operators.
module mux4 (input [3:0] d0, d1, d2, d3,
input [1:0] s,
output [3:0] y);
assign y  s[1] ? (s[0] ? d3 : d2)
: (s[0] ? d1 : d0);
endmodule
If s[1] is 1, then the multiplexer chooses the first expression,
(s[0] ? d3 : d2). This expression in turn chooses either d3
or d2 based on s[0] (y  d3 if s[0] is 1 and d2 if s[0] is 0).
If s[1] is 0, then the multiplexer similarly chooses the second
expression, which gives either d1 or d0 based on s[0].
VHDL
A 4:1 multiplexer can select one of four inputs using multiple
else clauses in the conditional signal assignment.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux4 is
port (d0, d1,
d2, d3: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC_VECTOR (1 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synthl of mux4 is
begin
y  d0 when s  “00” else
d1 when s  “01” else
d2 when s  “10” else
d3;
end;
VHDL also supports selected signal assignment statements to
provide a shorthand when selecting from one of several
possibilities. This is analogous to using a case statement in
place of multiple if/else statements in some programming
languages. The 4:1 multiplexer can be rewritten with selected
signal assignment as follows:
architecture synth2 of mux4 is
begin
with a select y 
d0 when “00”,
d1 when “01”,
d2 when “10”,
d3 when others;
end;
HDL Example 4.6 4:1 MULTIPLEXER
Chapter 0
is a circuit with three inputs and two outputs defined by the following
equations:
(4.1)
If we define intermediate signals, P and G,
(4.2)
we can rewrite the full adder as follows:
(4.3)
P and G are called internal variables, because they are neither inputs nor
outputs but are used only internal to the module. They are similar to
local variables in programming languages. HDL Example 4.7 shows
how they are used in HDLs.
HDL assignment statements (assign in Verilog and  in VHDL)
take place concurrently. This is different from conventional programming languages such as C or Java, in which statements are evaluated in
the order in which they are written. In a conventional language, it is
CoutG  PCin
S P⊕ Cin
GAB
PA⊕B
Cout AB  ACin  BCin
S A ⊕ B⊕Cin
4.2 Combinational Logic 177
un1_s_2
un1_s_3
un1_s_4
un1_s_5
y[3:0]
e
d
e
d
e
d
e
d
y[3:0]
s[1:0] [1:0]
d3[3:0]
d2[3:0]
d1[3:0]
d0[3:0]
[0]
[1]
[1]
[0]
[0]
[1]
[0]
[1]
[3:0]
[3:0] [3:0]
[3:0]
[3:0]
Figure 4.7 mux4 synthesized
circuit
Check this by filling out the
truth table to convince
yourself it is correct.
Chapter 04
important that S  P  Cin comes after P  A  B, because statements are executed sequentially. In an HDL, the order does not matter.
Like hardware, HDL assignment statements are evaluated any time the
inputs, signals on the right hand side, change their value, regardless of
the order in which the assignment statements appear in a module.
4.2.6 Precedence
Notice that we parenthesized the cout computation in HDL Example 4.7
to define the order of operations as Cout  G  (P  Cin), rather than Cout
 (G  P)  Cin. If we had not used parentheses, the default operation order
is defined by the language. HDL Example 4.8 specifies operator precedence
from highest to lowest for each language. The tables include arithmetic,
shift, and comparison operators that will be defined in Chapter 5.
178 CHAPTER FOUR Hardware Description Languages
p
g s
un1_cout cout
cout
s
cin
b
a
Figure 4.8 fulladder synthesized circuit
Verilog
In Verilog, wires are used to represent internal variables
whose values are defined by assign statements such as
assign p  a  b; Wires technically have to be declared
only for multibit busses, but it is good practice to include
them for all internal variables; their declaration could have
been omitted in this example.
module fulladder(input a, b, cin,
output s, cout);
wire p, g;
assign p  a  b;
assign g  a & b;
assign s  p  cin;
assign cout  g | (p & cin);
endmodule
VHDL
In VHDL, signals are used to represent internal variables
whose values are defined by concurrent signal assignment
statements such as p  a xor b;
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fulladder is
port(a, b, cin: in STD_LOGIC;
s, cout: out STD_LOGIC);
end;
architecture synth of fulladder is
signal p, g: STD_LOGIC;
begin
p  a xor b;
g  a and b;
s  p xor cin;
cout  g or (p and cin);
end;
HDL Example 4.7 FULL ADDER
Chapter 04.qxd 1/31/0
4.2 Combinational Logic 179
Verilog VHDL
HDL Example 4.8 OPERATOR PRECEDENCE
Table 4.2 VHDL operator precedence
Op Meaning
not NOT
*, /, mod, rem MUL, DIV, MOD, REM
, , PLUS, MINUS,
& CONCATENATE
rol, ror, Rotate,
srl, sll, Shift logical,
sra, sla Shift arithmetic
, /, , Comparison
, , 
and, or, nand, Logical Operations
nor, xor
H
i
g
h
e
s
t
L
o
w
e
s
t
The operator precedence for Verilog is much like you would
expect in other programming languages. In particular, AND
has precedence over OR. We could take advantage of this
precedence to eliminate the parentheses.
assign cout  g | p & cin;
Multiplication has precedence over addition in VHDL, as
you would expect. However, unlike Verilog, all of the logical
operations (and, or, etc.) have equal precedence, unlike what
one might expect in Boolean algebra. Thus, parentheses are
necessary; otherwise cout  g or p and cin would be
interpreted from left to right as cout  (g or p) and cin.
4.2.7 Numbers
Numbers can be specified in a variety of bases. Underscores in numbers
are ignored and can be helpful in breaking long numbers into more readable chunks. HDL Example 4.9 explains how numbers are written in
each language.
4.2.8 Z’s and X’s
HDLs use z to indicate a floating value. z is particularly useful for describing a tristate buffer, whose output floats when the enable is 0. Recall from
Section 2.6 that a bus can be driven by several tristate buffers, exactly one
of which should be enabled. HDL Example 4.10 shows the idiom for a
tristate buffer. If the buffer is enabled, the output is the same as the input.
If the buffer is disabled, the output is assigned a floating value (z).
Table 4.1 Verilog operator precedence
Op Meaning
~ NOT
*, /, % MUL, DIV, MOD
,  PLUS, MINUS
,  Logical Left/Right Shift
,  Arithmetic Left/Right Shift
, , ,  Relative Comparison
, ! Equality Comparison
&, ~& AND, NAND
, ~ XOR, XNOR
|, ~| OR, NOR
?: Conditional
H
i
g
h
e
s
t
L
o
w
e
s
t
Chapter 04.qxd 1
180 CHAPTER FOUR Hardware Description Languages
Verilog
Verilog numbers can specify their base and size (the number
of bits used to represent them). The format for declaring
constants is NBvalue, where N is the size in bits, B is the base,
and value gives the value. For example 9h25 indicates a
9-bit number with a value of 2516  3710  0001001012.
Verilog supports b for binary (base 2), o for octal (base 8),
d for decimal (base 10), and h for hexadecimal (base 16). If
the base is omitted, the base defaults to decimal.
If the size is not given, the number is assumed to have as
many bits as the expression in which it is being used. Zeros
are automatically padded on the front of the number to
bring it up to full size. For example, if w is a 6-bit bus, assign
w  b11 gives w the value 000011. It is better practice to
explicitly give the size.
VHDL
In VHDL, STD_LOGIC numbers are written in binary and
enclosed in single quotes: ‘0’ and ‘1’ indicate logic 0 and 1.
STD_LOGIC_VECTOR numbers are written in binary
or hexadecimal and enclosed in double quotation marks. The
base is binary by default and can be explicitly defined with
the prefix X for hexadecimal or B for binary.
HDL Example 4.9 NUMBERS
Table 4.3 Verilog numbers
Numbers Bits Base Val Stored
3b101 3 2 5 101
b11 ? 2 3 000 ... 0011
8b11 8 2 3 00000011
8b1010_1011 8 2 171 10101011
3d6 3 10 6 110
6o42 6 8 34 100010
8hAB 8 16 171 10101011
42 ? 10 42 00 ... 0101010
Table 4.4 VHDL numbers
Numbers Bits Base Val Stored
“101” 3 2 5 101
B“101” 3 2 5 101
X“AB” 8 16 161 10101011
Verilog
module tristate (input [3:0] a,
input en,
output [3:0] y);
assign y  en ? a : 4bz;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity tristate is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
en: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of tristate is
begin
y  “ZZZZ” when en  ‘0’ else a;
end;
HDL Example 4.10 TRISTATE BUFFER
Chapte
4.2 Combinational Logic 181
y_1[3:0]
y[3:0]
en
a[3:0] [3:0] [3:0]
Figure 4.9 tristate synthesized circuit
Similarly, HDLs use x to indicate an invalid logic level. If a bus is
simultaneously driven to 0 and 1 by two enabled tristate buffers (or other
gates), the result is x, indicating contention. If all the tristate buffers driving a bus are simultaneously OFF, the bus will float, indicated by z.
At the start of simulation, state nodes such as flip-flop outputs are
initialized to an unknown state (x in Verilog and u in VHDL). This is
helpful to track errors caused by forgetting to reset a flip-flop before its
output is used.
If a gate receives a floating input, it may produce an x output when
it can’t determine the correct output value. Similarly, if it receives an
illegal or uninitialized input, it may produce an x output. HDL
Example 4.11 shows how Verilog and VHDL combine these different
signal values in logic gates.
Verilog
Verilog signal values are 0, 1, z, and x. Verilog constants
starting with z or x are padded with leading z’s or x’s (instead
of 0’s) to reach their full length when necessary.
Table 4.5 shows a truth table for an AND gate using all
four possible signal values. Note that the gate can sometimes
determine the output despite some inputs being unknown.
For example 0&z returns 0 because the output of an AND
gate is always 0 if either input is 0. Otherwise, floating or
invalid inputs cause invalid outputs, displayed as x in Verilog.
VHDL
VHDL STD_LOGIC signals are ‘0’, ‘1’, ‘z’, ‘x’, and ‘u’.
Table 4.6 shows a truth table for an AND gate using all
five possible signal values. Notice that the gate can sometimes determine the output despite some inputs being
unknown. For example, ‘0’ and ‘z’ returns ‘0’ because the
output of an AND gate is always ‘0’ if either input is ‘0.’
Otherwise, floating or invalid inputs cause invalid outputs,
displayed as ‘x’ in VHDL. Uninitialized inputs cause uninitialized outputs, displayed as ‘u’ in VHDL.
HDL Example 4.11 TRUTH TABLES WITH UNDEFINED AND FLOATING INPUTS
Table 4.5 Verilog AND gate truth table with z and x
& A
0 1zx
0 0 0 00
B 1 0 1 xx
z 0 x xx
x 0 x xx
Table 4.6 VHDL AND gate truth table with z, x, and u
AND A
01zxu
0 00000
1 01xxu
B z 0xxxu
x 0xxxu
u 0uuuu

Seeing x or u values in simulation is almost always an indication of a
bug or bad coding practice. In the synthesized circuit, this corresponds
to a floating gate input, uninitialized state, or contention. The x or u
may be interpreted randomly by the circuit as 0 or 1, leading to unpredictable behavior.
4.2.9 Bit Swizzling
Often it is necessary to operate on a subset of a bus or to concatenate
(join together) signals to form busses. These operations are collectively
known as bit swizzling. In HDL Example 4.12, y is given the 9-bit value
c2c1d0d0d0c0101 using bit swizzling operations.
4.2.10 Delays
HDL statements may be associated with delays specified in arbitrary
units. They are helpful during simulation to predict how fast a circuit
will work (if you specify meaningful delays) and also for debugging
purposes to understand cause and effect (deducing the source of a bad
output is tricky if all signals change simultaneously in the simulation
results). These delays are ignored during synthesis; the delay of a gate
produced by the synthesizer depends on its tpd and tcd specifications, not
on numbers in HDL code.
HDL Example 4.13 adds delays to the original function from HDL
Example 4.1, It assumes that inverters have a delay
of 1 ns, three-input AND gates have a delay of 2 ns, and three-input OR
gates have a delay of 4 ns. Figure 4.10 shows the simulation waveforms,
with y lagging 7 ns after the inputs. Note that y is initially unknown at
the beginning of the simulation.
y  abcabc  abc.
182 CHAPTER FOUR Hardware Description Languages
Verilog
assign y  {c[2:1], {3{d[0]}}, c[0], 3’b101};
The {} operator is used to concatenate busses. {3{d[0]}}
indicates three copies of d[0].
Don’t confuse the 3-bit binary constant 3’b101 with a bus
named b. Note that it was critical to specify the length of 3
bits in the constant; otherwise, it would have had an unknown
number of leading zeros that might appear in the middle of y.
If y were wider than 9 bits, zeros would be placed in the
most significant bits.
VHDL
y  c(2 downto 1) & d(0) & d(0) & d(0) &
c(0) & “101”;
The & operator is used to concatenate busses. y must be a
9-bit STD_LOGIC_VECTOR. Do not confuse & with the and operator in VHDL.
HDL Example 4.12 BIT SWIZZLING
Chapt
4.2 Combinational Logic 183
4.2.11 VHDL Libraries and Types*
(This section may be skipped by Verilog users.) Unlike Verilog, VHDL
enforces a strict data typing system that can protect the user from some
errors but that is also clumsy at times.
Despite its fundamental importance, the STD_LOGIC type is not built
into VHDL. Instead, it is part of the IEEE.STD_LOGIC_1164 library.
Thus, every file must contain the library statements shown in the previous examples.
Verilog
‘timescale 1ns/1ps
module example (input a, b, c,
output y);
wire ab, bb, cb, n1, n2, n3;
assign #1 {ab, bb, cb}  ~ {a, b, c};
assign #2 n1  ab & bb & cb;
assign #2 n2  a & bb & cb;
assign #2 n3  a & bb & c;
assign #4 y  n1 | n2 | n3;
endmodule
Verilog files can include a timescale directive that indicates
the value of each time unit. The statement is of the form
‘timescale unit/precision. In this file, each unit is 1 ns, and
the simulation has 1 ps precision. If no timescale directive is
given in the file, a default unit and precision (usually 1 ns for
both) is used. In Verilog, a # symbol is used to indicate the
number of units of delay. It can be placed in assign statements, as well as non-blocking () and blocking () assignments, which will be discussed in Section 4.5.4.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity example is
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of example is
signal ab, bb, cb, n1, n2, n3: STD_LOGIC;
begin
ab  not a after 1 ns;
bb  not b after 1 ns;
cb  not c after 1 ns;
n1  ab and bb and cb after 2 ns;
n2  a and bb and cb after 2 ns;
n3  a and bb and c after 2 ns;
y  n1 or n2 or n3 after 4 ns;
end;
In VHDL, the after clause is used to indicate delay. The
units, in this case, are specified as nanoseconds.
HDL Example 4.13 LOGIC GATES WITH DELAYS
Figure 4.10 Example simulation waveforms with delays (from the ModelSim simulator)
Chapter 04.qxd
Moreover, IEEE.STD_LOGIC_1164 lacks basic operations such as
addition, comparison, shifts, and conversion to integers for the
STD_LOGIC_VECTOR data. Most CAD vendors have adopted yet more
libraries containing these functions: IEEE.STD_LOGIC_UNSIGNED and
IEEE.STD_LOGIC_SIGNED. See Section 1.4 for a discussion of unsigned
and signed numbers and examples of these operations.
VHDL also has a BOOLEAN type with two values: true and false.
BOOLEAN values are returned by comparisons (such as the equality comparison, s  ‘0’) and are used in conditional statements such as when.
Despite the temptation to believe a BOOLEAN true value should be
equivalent to a STD_LOGIC ‘1’ and BOOLEAN false should mean
STD_LOGIC ‘0’, these types are not interchangeable. Thus, the following
code is illegal:
y  d1 when s else d0;
q  (state  S2);
Instead, we must write
y  d1 when (s  ‘1’) else d0;
q  ‘1’ when (state  S2) else ‘0’;
Although we do not declare any signals to be BOOLEAN, they are
automatically implied by comparisons and used by conditional statements.
Similarly, VHDL has an INTEGER type that represents both positive
and negative integers. Signals of type INTEGER span at least the values
231 to 231  1. Integer values are used as indices of busses. For example, in the statement
y  a(3) and a(2) and a(1) and a(0);
0, 1, 2, and 3 are integers serving as an index to choose bits of the a
signal. We cannot directly index a bus with a STD_LOGIC or
STD_LOGIC_VECTOR signal. Instead, we must convert the signal to an
INTEGER. This is demonstrated in HDL Example 4.14 for an 8:1 multiplexer that selects one bit from a vector using a 3-bit index. The
CONV_INTEGER function is defined in the IEEE.STD_LOGIC_UNSIGNED
library and performs the conversion from STD_LOGIC_VECTOR to
INTEGER for positive (unsigned) values.
VHDL is also strict about out ports being exclusively for output.
For example, the following code for two and three-input AND gates is
illegal VHDL because v is an output and is also used to compute w.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity and23 is
port(a, b, c: in STD_LOGIC;
v, w: out STD_LOGIC);
end;
184 CHAPTER FOUR Hardware Description Languages
Chapter 0
architecture synth of and23 is
begin
v  a and b;
w  v and c;
end;
VHDL defines a special port type, buffer, to solve this problem.
A signal connected to a buffer port behaves as an output but may also
be used within the module. The corrected entity definition follows.
Verilog does not have this limitation and does not require buffer ports.
entity and23 is
port(a, b, c: in STD_LOGIC;
v: buffer STD_LOGIC;
w: out STD_LOGIC);
end;
VHDL supports enumeration types as an abstract way of representing information without assigning specific binary encodings. For example, the divide-by-3 FSM described in Section 3.4.2 uses three states. We
can give the states names using the enumeration type rather than referring to them by binary values. This is powerful because it allows VHDL
to search for the best state encoding during synthesis, rather than
depending on an arbitrary encoding specified by the user.
type statetype is (S0, S1, S2);
signal state, nextstate: statetype;
4.3 STRUCTURAL MODELING
The previous section discussed behavioral modeling, describing a module
in terms of the relationships between inputs and outputs. This section
4.3 Structural Modeling 185
library IEEE;
use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
entity mux8 is
port(d: in STD_LOGIC_VECTOR(7 downto 0);
s: in STD_LOGIC_VECTOR(2 downto 0);
y: out STD_LOGIC);
end;
architecture synth of mux8 is
begin
y  d(CONV_INTEGER(s));
end;
HDL Example 4.14 8:1 MULTIPLEXER WITH TYPE CONVERSION
Figure follows on next page.
Cha
186 CHAPTER FOUR Hardware Description Languages
un1_s_3
un1_s_4
un1_s_5
un1_s_6
un1_s_7
un1_s_8
un1_s_9
un1_s_10
y
e
d
e
d
e
d
e
d
e
d
e
d
e
d
e
d
y
s[2:0] [2:0]
d[7:0] [7:0]
[0]
[1]
[2]
[0]
[1]
[2]
[0]
[1]
[2]
[1]
[0]
[2]
[0]
[1]
[2]
[2]
[0]
[1]
[0]
[2]
[1]
[1]
[2]
[0]
[0]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
Figure 4.11 mux8 synthesized
circuit

examines structural modeling, describing a module in terms of how it is
composed of simpler modules.
For example, HDL Example 4.15 shows how to assemble a 4:1
multiplexer from three 2:1 multiplexers. Each copy of the 2:1 multiplexer
4.3 Structural Modeling 187
v w
w
v
c
b
a Figure 4.12 and23 synthesized
circuit
Verilog
module mux4 (input [3:0] d0, d1, d2, d3,
input [1:0] s,
output [3:0] y);
wire [3:0] low, high;
mux2 lowmux (d0, d1, s[0], low);
mux2 highmux (d2, d3, s[0], high);
mux2 finalmux (low, high, s[1], y);
endmodule
The three mux2 instances are called lowmux, highmux, and
finalmux. The mux2 module must be defined elsewhere in the
Verilog code.
HDL Example 4.15 STRUCTURAL MODEL OF 4:1 MULTIPLEXER
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux4 is
port (d0, d1,
d2, d3: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC_VECTOR (1 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture struct of mux4 is
component mux2
port (d0,
d1: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
signal low, high: STD_LOGIC_VECTOR (3 downto 0);
begin
lowmux: mux2 port map (d0, d1, s(0), low);
highmux: mux2 port map (d2, d3, s(0), high);
finalmux: mux2 port map (low, high, s(1), y);
end;
The architecture must first declare the mux2 ports using the
component declaration statement. This allows VHDL tools to
check that the component you wish to use has the same ports
as the entity that was declared somewhere else in another
entity statement, preventing errors caused by changing the
entity but not the instance. However, component declaration
makes VHDL code rather cumbersome.
Note that this architecture of mux4 was named struct,
whereas architectures of modules with behavioral descriptions from Section 4.2 were named synth. VHDL allows
multiple architectures (implementations) for the same entity;
the architectures are distinguished by name. The names
themselves have no significance to the CAD tools, but struct
and synth are common. Synthesizable VHDL code generally
contains only one architecture for each entity, so we will not
discuss the VHDL syntax to configure which architecture is
used when multiple architectures are defined.

188 CHAPTER FOUR Hardware Description Languages
mux2
lowmux
mux2
highmux
mux2
finalmux
y[3:0]
s[1:0] [1:0]
d3[3:0]
d2[3:0]
d1[3:0]
d0[3:0]
[0] s
d0[3:0]
d1[3:0]
y[3:0]
[0] s [3:0] d0[3:0]
[3:0] d1[3:0]
y[3:0]
[1] s [3:0] d0[3:0]
[3:0] d1[3:0]
[3:0] y[3:0]
Figure 4.13 mux4 synthesized circuit
Verilog
module mux2 (input [3:0] d0, d1,
input s,
output [3:0] y);
tristate t0 (d0, ~s, y);
tristate t1 (d1, s, y);
endmodule
In Verilog, expressions such as ~s are permitted in the port
list for an instance. Arbitrarily complicated expressions are
legal but discouraged because they make the code difficult to
read.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is
port (d0, d1: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture struct of mux2 is
component tristate
port (a: in STD_LOGIC_VECTOR (3 downto 0);
en: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
signal sbar: STD_LOGIC;
begin
sbar  not s;
t0: tristate port map (d0, sbar, y);
t1: tristate port map (d1, s, y);
end;
In VHDL, expressions such as not s are not permitted in the
port map for an instance. Thus, sbar must be defined as a
separate signal.
HDL Example 4.16 STRUCTURAL MODEL OF 2:1 MULTIPLEXER
is called an instance. Multiple instances of the same module are distinguished by distinct names, in this case lowmux, highmux, and finalmux.
This is an example of regularity, in which the 2:1 multiplexer is reused
many times.
HDL Example 4.16 uses structural modeling to construct a 2:1
multiplexer from a pair of tristate buffers.
C
4.3 Structural Modeling 189
HDL Example 4.17 shows how modules can access part of a bus. An
8-bit wide 2:1 multiplexer is built using two of the 4-bit 2:1 multiplexers
already defined, operating on the low and high nibbles of the byte.
In general, complex systems are designed hierarchically. The overall
system is described structurally by instantiating its major components.
Each of these components is described structurally from its building
blocks, and so forth recursively until the pieces are simple enough to
describe behaviorally. It is good style to avoid (or at least to minimize)
mixing structural and behavioral descriptions within a single module.
tristate
t0
tristate
t1
y[3:0] s
d1[3:0]
d0[3:0]
en [3:0]
a[3:0]
[3:0]
y[3:0]
en [3:0]
a[3:0]
[3:0] y[3:0]
Figure 4.14 mux2 synthesized circuit
Verilog
module mux2_8 (input [7:0] d0, d1,
input s,
output [7:0] y);
mux2 lsbmux (d0[3:0], d1[3:0], s, y[3:0]);
mux2 msbmux (d0[7:4], d1[7:4], s, y[7:4]);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2_8 is
port (d0, d1: in STD_LOGIC_VECTOR (7 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (7 downto 0));
end;
architecture struct of mux2_8 is
component mux2
port (d0, d1: in STD_LOGIC_VECTOR(3
downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
begin
lsbmux: mux2
port map (d0 (3 downto 0), d1 (3 downto 0),
s, y (3 downto 0));
msbhmux: mux2
port map (d0 (7 downto 4), d1 (7 downto 4),
s, y (7 downto 4));
end;
HDL Example 4.17 ACCESSING PARTS OF BUSSES

4.4 SEQUENTIAL LOGIC
HDL synthesizers recognize certain idioms and turn them into specific
sequential circuits. Other coding styles may simulate correctly but synthesize into circuits with blatant or subtle errors. This section presents
the proper idioms to describe registers and latches.
4.4.1 Registers
The vast majority of modern commercial systems are built with registers
using positive edge-triggered D flip-flops. HDL Example 4.18 shows the
idiom for such flip-flops.
In Verilog always statements and VHDL process statements, signals keep their old value until an event in the sensitivity list takes place
that explicitly causes them to change. Hence, such code, with appropriate sensitivity lists, can be used to describe sequential circuits with memory. For example, the flip-flop includes only clk in the sensitive list. It
remembers its old value of q until the next rising edge of the clk, even if
d changes in the interim.
In contrast, Verilog continuous assignment statements (assign) and
VHDL concurrent assignment statements () are reevaluated anytime
any of the inputs on the right hand side changes. Therefore, such code
necessarily describes combinational logic.
190 CHAPTER FOUR Hardware Description Languages
mux2
lsbmux
mux2
msbmux
y[7:0] [7:0]
s
d1[7:0]
[7:0]
d0[7:0] [7:0]
s
[3:0]
d0[3:0]
[3:0]
d1[3:0]
[3:0] y[3:0]
s
[7:4]
d0[3:0]
[7:4]
d1[3:0]
[7:4]
y[3:0]
Figure 4.15 mux2_8 synthesized circuit
C
4.4 Sequential Logic 191
d[3:0] q[3:0]
clk [3:0] [3:0] D[3:0] Q[3:0]
Figure 4.16 flop synthesized circuit
Verilog
module flop (input clk,
input [3:0] d,
output reg [3:0] q);
always @ (posedge clk)
q  d;
endmodule
A Verilog always statement is written in the form
always @ (sensitivity list)
statement;
The statement is executed only when the event specified in
the sensitivity list occurs. In this example, the statement is
q  d (pronounced “q gets d”). Hence, the flip-flop copies d
to q on the positive edge of the clock and otherwise remembers the old state of q.
 is called a nonblocking assignment. Think of it as a
regular  sign for now; we’ll return to the more subtle
points in Section 4.5.4. Note that  is used instead of
assign inside an always statement.
All signals on the left hand side of  or  in an always
statement must be declared as reg. In this example, q is both
an output and a reg, so it is declared as output reg [3:0] q.
Declaring a signal as reg does not mean the signal is actually
the output of a register! All it means is that the signal
appears on the left hand side of an assignment in an always
statement. We will see later examples of always statements
describing combinational logic in which the output is
declared reg but does not come from a flip-flop.
HDL Example 4.18 REGISTER
4.4.2 Resettable Registers
When simulation begins or power is first applied to a circuit, the output of
a flop or register is unknown. This is indicated with x in Verilog and ‘u’ in
VHDL. Generally, it is good practice to use resettable registers so that on
powerup you can put your system in a known state. The reset may be
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity flop is
port (clk: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0) ;
q: out STD_LOGIC_VECTOR (3 downto 0)) ;
end;
architecture synth of flop is
begin
process (clk) begin
if clk’event and clk  ‘1’ then
q  d;
end if;
end process;
end;
A VHDL process is written in the form
process (sensitivity list) begin
statement;
end process;
The statement is executed when any of the variables in the
sensitivity list change. In this example, the if statement is
executed when clk changes, indicated by clk’event. If the
change is a rising edge (clk  ‘1’ after the event), then
q  d (pronounced “q gets d”). Hence, the flip-flop copies d
to q on the positive edge of the clock and otherwise remembers the old state of q.
An alternative VHDL idiom for a flip-flop is
process (clk) begin
if RISING_EDGE (clk) then
q  d;
end if;
end process;
RISING_EDGE (clk) is synonymous with clk’event and
clk  1.
Chapter 04.qx
either asynchronous or synchronous. Recall that asynchronous reset
occurs immediately, whereas synchronous reset clears the output only on
the next rising edge of the clock. HDL Example 4.19 demonstrates the
idioms for flip-flops with asynchronous and synchronous resets. Note that
distinguishing synchronous and asynchronous reset in a schematic can be
difficult. The schematic produced by Synplify Pro places asynchronous
reset at the bottom of a flip-flop and synchronous reset on the left side.
192 CHAPTER FOUR Hardware Description Languages
Verilog
module flopr (input clk,
input reset,
input [3:0] d,
output reg [3:0] q);
// asynchronous reset
always @ (posedge clk, posedge reset)
if (reset) q  4’b0;
else q  d;
endmodule
module flopr (input clk,
input reset,
input [3:0] d,
output reg [3:0] q);
// synchronous reset
always @ (posedge clk)
if (reset) q  4’b0;
else q  d;
endmodule
Multiple signals in an always statement sensitivity list are
separated with a comma or the word or. Notice that posedge
reset is in the sensitivity list on the asynchronously resettable flop, but not on the synchronously resettable flop.
Thus, the asynchronously resettable flop immediately
responds to a rising edge on reset, but the synchronously
resettable flop responds to reset only on the rising edge of
the clock.
Because the modules have the same name, flopr, you
may include only one or the other in your design.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity flopr is
port (clk,
reset: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0);
q: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture asynchronous of flopr is
begin
process (clk, reset) begin
if reset  ‘1’ then
q  “0000”;
elsif clk’ event and clk  ‘1’ then
q  d;
end if;
end process;
end;
architecture synchronous of flopr is
begin
process (clk) begin
if clk’event and clk  ‘1’ then
if reset  ‘1’ then
q  “0000”;
else q  d;
end if;
end if;
end process;
end;
Multiple signals in a process sensitivity list are separated with a
comma. Notice that reset is in the sensitivity list on the asynchronously resettable flop, but not on the synchronously resettable flop. Thus, the asynchronously resettable flop immediately
responds to a rising edge on reset, but the synchronously resettable flop responds to reset only on the rising edge of the clock.
Recall that the state of a flop is initialized to ‘u’ at
startup during VHDL simulation.
As mentioned earlier, the name of the architecture
(asynchronous or synchronous, in this example) is ignored by
the VHDL tools but may be helpful to the human reading the
code. Because both architectures describe the entity flopr,
you may include only one or the other in your design.
HDL Example 4.19 RESETTABLE REGISTER
Chapter 04.q
4.4 Sequential Logic 193
4.4.3 Enabled Registers
Enabled registers respond to the clock only when the enable is asserted.
HDL Example 4.20 shows an asynchronously resettable enabled register
that retains its old value if both reset and en are FALSE.
R
d[3:0] q[3:0]
reset
clk [3:0] Q[3:0] [3:0] D[3:0]
(a)
d[3:0] q[3:0]
reset
clk [3:0] Q[3:0] [3:0] D[3:0]
R
(b)
Figure 4.17 flopr synthesized circuit (a) asynchronous reset, (b) synchronous reset
Verilog
module flopenr (input clk,
input reset,
input en,
input [3:0] d,
output reg [3:0] q);
// asynchronous reset
always @ (posedge clk, posedge reset)
if (reset) q  4’b0;
else if (en) q  d;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity flopenr is
port (clk,
reset,
en: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0);
q: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture asynchronous of flopenr is
— — asynchronous reset
begin
process (clk, reset) begin
if reset  ‘1’ then
q  “0000”;
elsif clk’event and clk  ‘1’ then
if en  ‘1’ then
q  d;
end if;
end if;
end process;
end;
HDL Example 4.20 RESETTABLE ENABLED REGISTER
Chapter
4.4.4 Multiple Registers
A single always/process statement can be used to describe multiple
pieces of hardware. For example, consider the synchronizer from Section
3.5.5 made of two back-to-back flip-flops, as shown in Figure 4.19.
HDL Example 4.21 describes the synchronizer. On the rising edge of
clk, d is copied to n1. At the same time, n1 is copied to q.
194 CHAPTER FOUR Hardware Description Languages
R
d[3:0] q[3:0]
en
reset
clk
[3:0] [3:0] D[3:0] Q[3:0]
E
Figure 4.18 flopenr synthesized circuit
CLK CLK
D
N1
Q
Figure 4.19 Synchronizer circuit
Verilog
module sync (input clk,
input d,
output reg q);
reg n1;
always @ (posedge clk)
begin
n1  d;
q  n1;
end
endmodule
n1 must be declared as a reg because it is an internal signal
used on the left hand side of  in an always statement.
Also notice that the begin/end construct is necessary because
multiple statements appear in the always statement. This is
analogous to { } in C or Java. The begin/end was not
needed in the flopr example because if/else counts as a
single statement.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity sync is
port (clk: in STD_LOGIC;
d: in STD_LOGIC;
q: out STD_LOGIC);
end;
architecture good of sync is
signal n1: STD_LOGIC;
begin
process (clk) begin
if clk’event and clk  ‘1’ then
n1  d;
q  n1;
end if;
end process;
end;
n1 must be declared as a signal because it is an internal
signal used in the module.
HDL Example 4.21 SYNCHRONIZER
n1 q
d q
clk
QD QD
Figure 4.20 sync synthesized circuit
Chapte
4.5 More Combinational Logic 195
4.4.5 Latches
Recall from Section 3.2.2 that a D latch is transparent when the clock is
HIGH, allowing data to flow from input to output. The latch becomes
opaque when the clock is LOW, retaining its old state. HDL Example
4.22 shows the idiom for a D latch.
Not all synthesis tools support latches well. Unless you know that your
tool does support latches and you have a good reason to use them, avoid
them and use edge-triggered flip-flops instead. Furthermore, take care that
your HDL does not imply any unintended latches, something that is easy to
do if you aren’t attentive. Many synthesis tools warn you when a latch is
created; if you didn’t expect one, track down the bug in your HDL.
4.5 MORE COMBINATIONAL LOGIC
In Section 4.2, we used assignment statements to describe combinational
logic behaviorally. Verilog always statements and VHDL process statements are used to describe sequential circuits, because they remember the
old state when no new state is prescribed. However, always/process
lat
q[3:0]
q[3:0] d[3:0]
clk
[3:0] D[3:0] [3:0]
Q[3:0] C
Figure 4.21 latch synthesized circuit
Verilog
module latch (input clk,
input [3:0] d,
output reg [3:0] q);
always @ (clk, d)
if (clk) q  d;
endmodule
The sensitivity list contains both clk and d, so the always
statement evaluates any time clk or d changes. If clk is
HIGH, d flows through to q.
q must be declared to be a reg because it appears on the
left hand side of  in an always statement. This does not
always mean that q is the output of a register.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity latch is
port (clk: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0);
q: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of latch is
begin
process (clk, d) begin
if clk  ‘1’ then q  d;
end if;
end process;
end;
The sensitivity list contains both clk and d, so the process
evaluates anytime clk or d changes. If clk is HIGH, d flows
through to q.
HDL Example 4.22 D LATCH
Chap
statements can also be used to describe combinational logic behaviorally
if the sensitivity list is written to respond to changes in all of the inputs
and the body prescribes the output value for every possible input combination. HDL Example 4.23 uses always/process statements to describe
a bank of four inverters (see Figure 4.3 for the synthesized circuit).
HDLs support blocking and nonblocking assignments in an
always/process statement. A group of blocking assignments are evaluated in the order in which they appear in the code, just as one would
expect in a standard programming language. A group of nonblocking
assignments are evaluated concurrently; all of the statements are evaluated before any of the signals on the left hand sides are updated.
HDL Example 4.24 defines a full adder using intermediate signals
p and g to compute s and cout. It produces the same circuit from
Figure 4.8, but uses always/process statements in place of assignment
statements.
These two examples are poor applications of always/process statements for modeling combinational logic because they require more lines
than the equivalent approach with assignment statements from Section
4.2.1. Moreover, they pose the risk of inadvertently implying sequential
logic if the inputs are left out of the sensitivity list. However, case and
if statements are convenient for modeling more complicated combinational logic. case and if statements must appear within always/process
statements and are examined in the next sections.
196 CHAPTER FOUR Hardware Description Languages
Verilog
module inv (input [3:0] a,
output reg [3:0] y);
always @ (*)
y  ~a;
endmodule
always @ (*) reevaluates the statements inside the always
statement any time any of the signals on the right hand side
of  or  inside the always statement change. Thus, @ (*)
is a safe way to model combinational logic. In this particular
example, @ (a) would also have sufficed.
The  in the always statement is called a blocking assignment, in contrast to the  nonblocking assignment. In
Verilog, it is good practice to use blocking assignments for
combinational logic and nonblocking assignments for sequential logic. This will be discussed further in Section 4.5.4.
Note that y must be declared as reg because it appears
on the left hand side of a  or  sign in an always statement. Nevertheless, y is the output of combinational logic,
not a register.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity inv is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture proc of inv is
begin
process (a) begin
y  not a;
end process;
end;
The begin and end process statements are required in VHDL
even though the process contains only one assignment.
HDL Example 4.23 INVERTER USING always/process
Chapter 
4.5 More Combinational Logic 197
Verilog
In a Verilog always statement,  indicates a blocking assignment and  indicates a nonblocking assignment (also called
a concurrent assignment).
Do not confuse either type with continuous assignment
using the assign statement. assign statements must be used
outside always statements and are also evaluated concurrently.
VHDL
In a VHDL process statement, :  indicates a blocking
assignment and  indicates a nonblocking assignment (also
called a concurrent assignment). This is the first section
where :  is introduced.
Nonblocking assignments are made to outputs and to
signals. Blocking assignments are made to variables, which
are declared in process statements (see HDL Example
4.24).
 can also appear outside process statements, where it
is also evaluated concurrently.
Verilog
module fulladder (input a, b, cin,
output reg s, cout);
reg p, g;
always @ (*)
begin
p  a  b; // blocking
g  a & b; // blocking
s  p  cin; // blocking
cout  g | (p & cin); // blocking
end
endmodule
In this case, an @ (a, b, cin) would have been equivalent to
@ (*). However, @ (*) is better because it avoids common
mistakes of missing signals in the stimulus list.
For reasons that will be discussed in Section 4.5.4, it is
best to use blocking assignments for combinational logic.
This example uses blocking assignments, first computing
p, then g, then s, and finally cout.
Because p and g appear on the left hand side of an
assignment in an always statement, they must be declared to
be reg.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fulladder is
port (a, b, cin: in STD_LOGIC;
s, cout: out STD_LOGIC);
end;
architecture synth of fulladder is
begin
process (a, b, cin)
variable p, g: STD_LOGIC;
begin
p : a xor b; — — blocking
g : a and b; — — blocking
s  p xor cin;
cout  g or (p and cin);
end process;
end;
The process sensitivity list must include a, b, and cin because
combinational logic should respond to changes of any input.
For reasons that will be discussed in Section 4.5.4, it is
best to use blocking assignments for intermediate variables in
combinational logic. This example uses blocking assignments
for p and g so that they get their new values before being
used to compute s and cout that depend on them.
Because p and g appear on the left hand side of a
blocking assignment (:) in a process statement, they must
be declared to be variable rather than signal. The variable
declaration appears before the begin in the process where
the variable is used.
HDL Example 4.24 FULL ADDER USING always/process
Chapter 04.qxd 1/
4.5.1 Case Statements
A better application of using the always/process statement for combinational logic is a seven-segment display decoder that takes advantage of the
case statement that must appear inside an always/process statement.
As you might have noticed in Example 2.10, the design process for
large blocks of combinational logic is tedious and prone to error. HDLs
offer a great improvement, allowing you to specify the function at a
higher level of abstraction, and then automatically synthesize the function into gates. HDL Example 4.25 uses case statements to describe a
seven-segment display decoder based on its truth table. (See Example
2.10 for a description of the seven-segment display decoder.) The case
198 CHAPTER FOUR Hardware Description Languages
Verilog
module sevenseg (input [3:0] data,
output reg [6:0] segments);
always @ (*)
case (data)
// abc_defg
0: segments  7’b111_1110;
1: segments  7’b011_0000;
2: segments  7’b110_1101;
3: segments  7’b111_1001;
4: segments  7’b011_0011;
5: segments  7’b101_1011;
6: segments  7’b101_1111;
7: segments  7’b111_0000;
8: segments  7’b111_1111;
9: segments  7’b111_1011;
default: segments  7’b000_0000;
endcase
endmodule
The case statement checks the value of data. When data is 0,
the statement performs the action after the colon, setting
segments to 1111110. The case statement similarly checks
other data values up to 9 (note the use of the default base,
base 10).
The default clause is a convenient way to define the
output for all cases not explicitly listed, guaranteeing combinational logic.
In Verilog, case statements must appear inside always
statements.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity seven_seg_decoder is
port (data: in STD_LOGIC_VECTOR (3 downto 0);
segments: out STD_LOGIC_VECTOR (6 downto 0));
end;
architecture synth of seven_seg_decoder is
begin
process (data) begin
case data is
— — abcdefg
when X“0”  segments  “1111110”;
when X“1”  segments  “0110000”;
when X“2”  segments  “1101101”;
when X“3”  segments  “1111001”;
when X“4”  segments  “0110011”;
when X“5”  segments  “1011011”;
when X“6”  segments  “1011111”;
when X“7”  segments  “1110000”;
when X“8”  segments  “1111111”;
when X“9”  segments  “1111011”;
when others  segments  “0000000”;
end case;
end process;
end;
The case statement checks the value of data. When data is 0,
the statement performs the action after the , setting
segments to 1111110. The case statement similarly checks
other data values up to 9 (note the use of X for hexadecimal
numbers). The others clause is a convenient way to define
the output for all cases not explicitly listed, guaranteeing
combinational logic.
Unlike Verilog, VHDL supports selected signal assignment statements (see HDL Example 4.6), which are much like
case statements but can appear outside processes. Thus, there
is less reason to use processes to describe combinational logic.
HDL Example 4.25 SEVEN-SEGMENT DISPLAY DECODER
Chapter 04.qxd 1/31/07 8:17 PM Pag
4.5 More Combinational Logic 199
statement performs different actions depending on the value of its
input. A case statement implies combinational logic if all possible input
combinations are defined; otherwise it implies sequential logic, because
the output will keep its old value in the undefined cases.
Synplify Pro synthesizes the seven-segment display decoder into a
read-only memory (ROM) containing the 7 outputs for each of the 16
possible inputs. ROMs are discussed further in Section 5.5.6.
If the default or others clause were left out of the case statement,
the decoder would have remembered its previous output anytime data
were in the range of 10–15. This is strange behavior for hardware.
Ordinary decoders are also commonly written with case statements.
HDL Example 4.26 describes a 3:8 decoder.
4.5.2 If Statements
always/process statements may also contain if statements. The if
statement may be followed by an else statement. If all possible input
rom
segments_1[6:0]
data[3:0] segments[6:0] [6:0]
DOUT[6:0]
[3:0]
A[3:0]
Figure 4.22 sevenseg synthesized circuit
Verilog
module decoder3_8 (input [2:0] a,
output reg [7:0] y);
always @ (*)
case (a)
3’b000: y  8’b00000001;
3’b001: y  8’b00000010;
3’b010: y  8’b00000100;
3’b011: y  8’b00001000;
3’b100: y  8’b00010000;
3’b101: y  8’b00100000;
3’b110: y  8’b01000000;
3’b111: y  8’b10000000;
endcase
endmodule
No default statement is needed because all cases are covered.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity decoder3_8 is
port (a: in STD_LOGIC_VECTOR (2 downto 0);
y: out STD_LOGIC_VECTOR (7 downto 0));
end;
architecture synth of decoder3_8 is
begin
process (a) begin
case a is
when “000”  y  “00000001”;
when “001”  y  “00000010”;
when “010”  y  “00000100”;
when “011”  y  “00001000”;
when “100”  y  “00010000”;
when “101”  y  “00100000”;
when “110”  y  “01000000”;
when “111”  y  “10000000”;
end case;
end process;
end;
No others clause is needed because all cases are covered.
HDL Example 4.26 3:8 DECODER
Chapter 04.qxd 1/31/07 8
200 CHAPTER FOUR Hardware Description Languages
y41
y34
y35
y36
y37
y38
y39
y40
y[7:0]
a[2:0] [2:0]
[0]
[1]
[2]
[0]
[1]
[2]
[2]
[0]
[1]
[0]
[2]
[1]
[1]
[2]
[0]
[1]
[0]
[2]
[0]
[1]
[2]
[0]
[1]
[2]
Figure 4.23 decoder3_8 synthesized circuit

4.5 More Combinational Logic 201
combinations are handled, the statement implies combinational logic;
otherwise, it produces sequential logic (like the latch in Section 4.4.5).
HDL Example 4.27 uses if statements to describe a priority circuit,
defined in Section 2.4. Recall that an N-input priority circuit sets the
output TRUE that corresponds to the most significant input that is
TRUE.
4.5.3 Verilog casez*
(This section may be skipped by VHDL users.) Verilog also provides the
casez statement to describe truth tables with don’t cares (indicated with
? in the casez statement). HDL Example 4.28 shows how to describe a
priority circuit with casez.
Synplify Pro synthesizes a slightly different circuit for this module,
shown in Figure 4.25, than it did for the priority circuit in Figure 4.24.
However, the circuits are logically equivalent.
4.5.4 Blocking and Nonblocking Assignments
The guidelines on page 203 explain when and how to use each type of
assignment. If these guidelines are not followed, it is possible to write
Verilog
module priority (input [3:0] a,
output reg [3:0] y);
always @ (*)
if (a[3]) y  4’b1000;
else if (a[2]) y  4’b0100;
else if (a[1]) y  4’b0010;
else if (a[0]) y  4’b0001;
else y  4’b0000;
endmodule
In Verilog, if statements must appear inside of always
statements.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity priority is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of priority is
begin
process (a) begin
if a(3)  ‘1’ then y  “1000”;
elsif a(2)  ‘1’ then y  “0100”;
elsif a(1)  ‘1’ then y  “0010”;
elsif a(0)  ‘1’ then y  “0001”;
else y  “0000”;
end if;
end process;
end;
Unlike Verilog, VHDL supports conditional signal assignment statements (see HDL Example 4.6), which are much
like if statements but can appear outside processes. Thus,
there is less reason to use processes to describe combinational logic. (Figure follows on next page.)
HDL Example 4.27 PRIORITY CIRCUIT
Chapter 04.qxd
code that appears to work in simulation but synthesizes to incorrect
hardware. The optional remainder of this section explains the principles
behind the guidelines.
Combinational Logic*
The full adder from HDL Example 4.24 is correctly modeled using
blocking assignments. This section explores how it operates and how it
would differ if nonblocking assignments had been used.
Imagine that a, b, and cin are all initially 0. p, g, s, and cout are
thus 0 as well. At some time, a changes to 1, triggering the
always/process statement. The four blocking assignments evaluate in
202 CHAPTER FOUR Hardware Description Languages
module priority_casez(input [3:0] a,
output reg [3:0] y);
always @ (*)
casez (a)
4’b1???: y  4’b1000;
4’b01??: y  4’b0100;
4’b001?: y  4’b0010;
4’b0001: y  4’b0001;
default: y  4’b0000;
endcase
endmodule
HDL Example 4.28 PRIORITY CIRCUIT USING casez
y_1[2]
un13_y
un21_y
y_1[1]
y_1[0]
y[3:0]
a[3:0] [3:0]
[3]
[2:0]
[2] [2] [3]
[2]
[3]
[1]
[2]
[3]
[1] [1]
[0] [0]
Figure 4.24 priority
synthesized circuit
(See figure 4.25.)
Chapt
4.5 More Combinational Logic 203
y23[0]
y24[0]
y25
a[3:0] y[3:0] [3:0] [3]
[2]
[2]
[1]
[0]
[3]
[1]
[2]
[3]
[0]
[1]
[2]
[3]
Figure 4.25 priority_casez
synthesized circuit
Verilog
1. Use always @ (posedge clk) and nonblocking assignments
to model synchronous sequential logic.
always @ (posedge clk)
begin
nl  d; // nonblocking
q  nl; // nonblocking
end
2. Use continuous assignments to model simple combinational logic.
assign y  s ? d1 : d0;
3. Use always @ (*) and blocking assignments to model
more complicated combinational logic where the always
statement is helpful.
always @ (*)
begin
p  a  b; // blocking
g  a & b; // blocking
s  p  cin;
cout  g | (p & cin);
end
4. Do not make assignments to the same signal in more
than one always statement or continuous assignment
statement.
VHDL
1. Use process (clk) and nonblocking assignments to model
synchronous sequential logic.
process (clk) begin
if clk’event and clk  ‘1’ then
nl  d; — — nonblocking
q  nl; — — nonblocking
end if;
end process;
2. Use concurrent assignments outside process statements to
model simple combinational logic.
y  d0 when s  ‘0’ else d1;
3. Use process (in1, in2, ... ) to model more complicated
combinational logic where the process is helpful. Use
blocking assignments for internal variables.
process (a, b, cin)
variable p, g: STD_LOGIC;
begin
p : a xor b; — — blocking
g : a and b; — — blocking
s  p xor cin;
cout  g or (p and cin);
end process;
4. Do not make assignments to the same variable in more
than one process or concurrent assignment statement.
BLOCKING AND NONBLOCKING ASSIGNMENT GUIDELINES
Chapter 04.qxd 1/3
the order shown here. (In the VHDL code, s and cout are assigned concurrently.) Note that p and g get their new values before s and cout are
computed because of the blocking assignments. This is important because
we want to compute s and cout using the new values of p and g.
1. p 1  0  1
2. g 1 • 0  0
3. s 1  0  1
4. cout 0  1 • 0  0
In contrast, HDL Example 4.29 illustrates the use of nonblocking
assignments.
Now consider the same case of a rising from 0 to 1 while b and cin
are 0. The four nonblocking assignments evaluate concurrently:
p 1  01 g 1• 0 0 s 0  0 0 cout 00 • 0  0
Observe that s is computed concurrently with p and hence uses the
old value of p, not the new value. Therefore, s remains 0 rather than
; ; ; ;
;
;
;
;
204 CHAPTER FOUR Hardware Description Languages
Verilog
// nonblocking assignments (not recommended)
module fulladder (input a, b, cin,
output reg s, cout);
reg p, g;
always @ (*)
begin
p  a  b; // nonblocking
g  a & b; // nonblocking
s  p  cin;
cout  g | (p & cin);
end
endmodule
Because p and g appear on the left hand side of an assignment
in an always statement, they must be declared to be reg.
VHDL
— — nonblocking assignments (not recommended)
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fulladder is
port (a, b, cin: in STD_LOGIC;
s, cout: out STD_LOGIC);
end;
architecture nonblocking of fulladder is
signal p, g: STD_LOGIC;
begin
process (a, b, cin, p, g) begin
p  a xor b; — — nonblocking
g  a and b; — — nonblocking
s  p xor cin;
cout  g or (p and cin);
end process;
end;
Because p and g appear on the left hand side of a nonblocking assignment in a process statement, they must be declared
to be signal rather than variable. The signal declaration
appears before the begin in the architecture, not the
process.
HDL Example 4.29 FULL ADDER USING NONBLOCKING ASSIGNMENTS
Chapter 04.qxd 1/31/07 8
4.5 More Combinational Logic 205
becoming 1. However, p does change from 0 to 1. This change triggers
the always/process statement to evaluate a second time, as follows:
p 1 0 1 g 1• 0  0 s 1  0 1 cout 0 1•0  0
This time, p is already 1, so s correctly changes to 1. The nonblocking
assignments eventually reach the right answer, but the always/process
statement had to evaluate twice. This makes simulation slower, though it
synthesizes to the same hardware.
Another drawback of nonblocking assignments in modeling combinational logic is that the HDL will produce the wrong result if you
forget to include the intermediate variables in the sensitivity list.
; ; ; ;
Worse yet, some synthesis tools will synthesize the correct hardware
even when a faulty sensitivity list causes incorrect simulation. This leads
to a mismatch between the simulation results and what the hardware
actually does.
Sequential Logic*
The synchronizer from HDL Example 4.21 is correctly modeled using nonblocking assignments. On the rising edge of the clock, d is copied to n1 at
the same time that n1 is copied to q, so the code properly describes two
registers. For example, suppose initially that d  0, n1  1, and q  0.
On the rising edge of the clock, the following two assignments occur
concurrently, so that after the clock edge, n1  0 and q  1.
nl d  0 q n1  1
HDL Example 4.30 tries to describe the same module using blocking
assignments. On the rising edge of clk, d is copied to n1. Then this new
value of n1 is copied to q, resulting in d improperly appearing at both n1
and q. The assignments occur one after the other so that after the clock
edge, q  n1  0.
1. n1 d  0
2. q n1 ;  0
;
; ;
Verilog
If the sensitivity list of the always statement in HDL Example
4.29 were written as always @ (a, b, cin) rather than always
@ (*), then the statement would not reevaluate when p or g
changes. In the previous example, s would be incorrectly left
at 0, not 1.
VHDL
If the sensitivity list of the process statement in HDL
Example 4.29 were written as process (a, b, cin) rather
than process (a, b, cin, p, g), then the statement would not
reevaluate when p or g changes. In the previous example,
s would be incorrectly left at 0, not 1.
Chapter 04.qxd 1/3
Because n1 is invisible to the outside world and does not influence
the behavior of q, the synthesizer optimizes it away entirely, as shown in
Figure 4.26.
The moral of this illustration is to exclusively use nonblocking
assignment in always statements when modeling sequential logic. With
sufficient cleverness, such as reversing the orders of the assignments, you
could make blocking assignments work correctly, but blocking assignments offer no advantages and only introduce the risk of unintended
behavior. Certain sequential circuits will not work with blocking assignments no matter what the order.
4.6 FINITE STATE MACHINES
Recall that a finite state machine (FSM) consists of a state register and two
blocks of combinational logic to compute the next state and the output
given the current state and the input, as was shown in Figure 3.22. HDL
descriptions of state machines are correspondingly divided into three parts
to model the state register, the next state logic, and the output logic.
206 CHAPTER FOUR Hardware Description Languages
Verilog
// Bad implementation using blocking assignments
module syncbad (input clk,
input d,
output reg q);
reg n1;
always @ (posedge clk)
begin
n1  d; // blocking
q  n1; // blocking
end
endmodule
VHDL
— — Bad implementation using blocking assignment
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity syncbad is
port (clk: in STD_LOGIC;
d: in STD_LOGIC;
q: out STD_LOGIC);
end;
architecture bad of syncbad is
begin
process (clk)
variable n1: STD_LOGIC;
begin
if clk’event and clk  ‘1’ then
n1 : d; — — blocking
q  n1;
end if;
end process;
end;
HDL Example 4.30 BAD SYNCHRONIZER WITH BLOCKING ASSIGNMENTS
q
d q
clk
D Q
Figure 4.26 syncbad synthesized circuit
Chapt
4.6 Finite State Machines 207
HDL Example 4.31 describes the divide-by-3 FSM from Section
3.4.2. It provides an asynchronous reset to initialize the FSM. The state
register uses the ordinary idiom for flip-flops. The next state and output
logic blocks are combinational.
The Synplify Pro Synthesis tool just produces a block diagram and
state transition diagram for state machines; it does not show the logic
gates or the inputs and outputs on the arcs and states. Therefore, be
careful that you have specified the FSM correctly in your HDL code. The
state transition diagram in Figure 4.27 for the divide-by-3 FSM is analogous to the diagram in Figure 3.28(b). The double circle indicates that
Verilog
module divideby3FSM (input clk,
input reset,
output y);
reg [1:0] state, nextstate;
parameter S0  2b00;
parameter S1  2b01;
parameter S2  2b10;
// state register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// next state logic
always @ (*)
case (state)
S0: nextstate  S1;
S1: nextstate  S2;
S2: nextstate  S0;
default: nextstate  S0;
endcase
// output logic
assign y  (state  S0);
endmodule
The parameter statement is used to define constants within a
module. Naming the states with parameters is not required,
but it makes changing state encodings much easier and
makes the code more readable.
Notice how a case statement is used to define the state
transition table. Because the next state logic should be combinational, a default is necessary even though the state 2b11
should never arise.
The output, y, is 1 when the state is S0. The equality
comparison a  b evaluates to 1 if a equals b and 0 otherwise. The inequality comparison a ! b does the inverse,
evaluating to 1 if a does not equal b.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity divideby3FSM is
port (clk, reset: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of divideby3FSM is
type statetype is (S0, S1, S2);
signal state, nextstate: statetype;
begin
— — state register
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
— — next state logic
nextstate  S1 when state  S0 else
S2 when state  S1 else
S0;
— — output logic
y  ‘1’ when state  S0 else ‘0’;
end;
This example defines a new enumeration data type,
statetype, with three possibilities: S0, S1, and S2. state and
nextstate are statetype signals. By using an enumeration
instead of choosing the state encoding, VHDL frees the
synthesizer to explore various state encodings to choose the
best one.
The output, y, is 1 when the state is S0. The inequalitycomparison uses /. To produce an output of 1 when the state
is anything but S0, change the comparison to state / S0.
HDL Example 4.31 DIVIDE-BY-3 FINITE STATE MACHINE
Notice that the synthesis tool
uses a 3-bit encoding (Q[2:0])
instead of the 2-bit encoding
suggested in the Verilog code.
Chapter 04.qxd 1/31/07 8:1
S0 is the reset state. Gate-level implementations of the divide-by-3 FSM
were shown in Section 3.4.2.
If, for some reason, we had wanted the output to be HIGH in states
S0 and S1, the output logic would be modified as follows.
The next two examples describe the snail pattern recognizer FSM
from Section 3.4.3. The code shows how to use case and if statements to handle next state and output logic that depend on the inputs
as well as the current state. We show both Moore and Mealy modules. In the Moore machine (HDL Example 4.32), the output depends
only on the current state, whereas in the Mealy machine (HDL
Example 4.33), the output logic depends on both the current state
and inputs.
208 CHAPTER FOUR Hardware Description Languages
S0
S1
S2
statemachine
state[2:0]
y [0]
reset
clk C
[2:0] Q[2:0] R
Figure 4.27 divideby3fsm
synthesized circuit
Verilog
// Output Logic
assign y  (state  S0 | state  S1);
VHDL
— — output logic
y  ‘1’ when (state  S0 or state  S1) else ‘0’;
Chapter 
Verilog
module patternMoore (input clk,
input reset,
input a,
output y);
reg [2:0] state, nextstate;
parameter S0  3b000;
parameter S1  3b001;
parameter S2  3b010;
parameter S3  3b011;
parameter S4  3b100;
// state register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// next state logic
always @ (*)
case (state)
S0: if (a) nextstate  S1;
else nextstate  S0;
S1: if (a) nextstate  S2;
else nextstate  S0;
S2: if (a) nextstate  S2;
else nextstate  S3;
S3: if (a) nextstate  S4;
else nextstate  S0;
S4: if (a) nextstate  S2;
else nextstate  S0;
default: nextstate  S0;
endcase
// output logic
assign y  (state  S4);
endmodule
Note how nonblocking assignments () are used in the
state register to describe sequential logic, whereas blocking
assignments () are used in the next state logic to describe
combinational logic.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity patternMoore is
port (clk, reset: in STD_LOGIC;
a: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of patternMoore is
type statetype is (S0, S1, S2, S3, S4);
signal state, nextstate: statetype;
begin
— — state register
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
— — next state logic
process (state, a) begin
case state is
when S0  if a  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if a  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if a  ‘1’ then
nextstate  S2;
else nextstate  S3;
end if;
when S3  if a  ‘1’ then
nextstate  S4;
else nextstate  S0;
end if;
when S4  if a  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when others  nextstate  S0;
end case;
end process;
— — output logic
y  ‘1’ when state  S4 else ‘0’;
end;
HDL Example 4.32 PATTERN RECOGNIZER MOORE FSM
S0
S1
S2
S3
S4
statemachine
state[4:0]
y
[4] a
reset
clk
I[0]
[4:0]
C Q[4:0]
R
Figure 4.28 patternMoore synthesized circuit
Chapter 04.qxd 1/31/07 8:17 PM Page 209
210 CHAPTER FOUR Hardware Description Languages
S0
S1
S3 S2
statemachine
state[3:0]
y
y a
reset
clk
I[0]
[3:0]
C Q[3:0]
R
[3]
Figure 4.29 patternMealy synthesized circuit
Verilog
module patternMealy (input clk,
input reset,
input a,
output y);
reg [1:0] state, nextstate;
parameter S0  2b00;
parameter S1  2b01;
parameter S2  2b10;
parameter S3  2b11;
// state register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// next state logic
always @ (*)
case (state)
S0: if (a) nextstate  S1;
else nextstate  S0;
S1: if (a) nextstate  S2;
else nextstate  S0;
S2: if (a) nextstate  S2;
else nextstate  S3;
S3: if (a) nextstate  S1;
else nextstate  S0;
default: nextstate  S0;
endcase
// output logic
assign y  (a & state  S3);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity patternMealy is
port (clk, reset: in STD_LOGIC;
a: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of patternMealy is
type statetype is (S0, S1, S2, S3);
signal state, nextstate: statetype;
begin
— — state register
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
— — next state logic
process(state, a) begin
case state is
when S0  if a  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if a  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if a  ‘1’ then
nextstate  S2;
else nextstate  S3;
end if;
when S3  if a  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when others  nextstate  S0;
end case;
end process;
— — output logic
y  ‘1’ when (a  ‘1’ and state  S3) else ‘0’;
end;
HDL Example 4.33 PATTERN RECOGNIZER MEALY FSM
Chapter 04.qxd 1/31/07 8:17 PM Page 210
4.7 Parameterized Modules 211
Verilog
module mux2
# (parameter width  8)
(input [width1:0] d0, d1,
input s,
output [width1:0] y);
assign y  s ? d1 : d0;
endmodule
Verilog allows a # (parameter ... ) statement before the
inputs and outputs to define parameters. The parameter
statement includes a default value (8) of the parameter,
width. The number of bits in the inputs and outputs can
depend on this parameter.
module mux4_8 (input [7:0] d0, d1, d2, d3,
input [1:0] s,
output [7:0] y);
wire [7:0] low, hi;
mux2 lowmux (d0, d1, s[0], low);
mux2 himux (d2, d3, s[1], hi);
mux2 outmux (low, hi, s[1], y);
endmodule
The 8-bit 4:1 multiplexer instantiates three 2:1 multiplexers
using their default widths.
In contrast, a 12-bit 4:1 multiplexer, mux4_12, would
need to override the default width using #() before the
instance name, as shown below.
module mux4_12 (input [11:0] d0, d1, d2, d3,
input [1:0] s,
output [11:0] y);
wire [11:0] low, hi;
mux2 #(12) lowmux(d0, d1, s[0], low);
mux2 #(12) himux(d2, d3, s[1], hi);
mux2 #(12) outmux(low, hi, s[1], y);
endmodule
Do not confuse the use of the # sign indicating delays with
the use of # ( ... ) in defining and overriding parameters.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is
generic(width: integer : 8);
port (d0,
d1: in STD_LOGIC_VECTOR (width1 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (width1 downto 0));
end;
architecture synth of mux2 is
begin
y  d0 when s  ‘0’ else d1;
end;
The generic statement includes a default value (8) of width.
The value is an integer.
entity mux4_8 is
port (d0, d1, d2,
d3: in STD_LOGIC_VECTOR (7 downto 0);
s: in STD_LOGIC_VECTOR (1 downto 0);
y: out STD_LOGIC_VECTOR (7 downto 0));
end;
architecture struct of mux4_8 is
component mux2
generic (width: integer);
port (d0,
d1: in STD_LOGIC_VECTOR (width1 downto 0) ;
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (width1 downto 0));
end component;
signal low, hi: STD_LOGIC_VECTOR(7 downto 0);
begin
lowmux: mux2 port map(d0, d1, s(0), low);
himux: mux2 port map(d2, d3, s(0), hi);
outmux: mux2 port map(low, hi, s(1), y);
end;
The 8-bit 4:1 multiplexer, mux4_8, instantiates three 2:1
multiplexers using their default widths.
In contrast, a 12-bit 4:1 multiplexer, mux4_12, would
need to override the default width using generic map, as
shown below.
lowmux: mux2 generic map (12)
port map (d0, d1, s(0), low);
himux: mux2 generic map (12)
port map (d2, d3, s(0), hi);
outmux: mux2 generic map (12)
port map (low, hi, s(1), y);
HDL Example 4.34 PARAMETERIZED N-BIT MULTIPLEXERS
4.7 PARAMETERIZED MODULES*
So far all of our modules have had fixed-width inputs and outputs. For
example, we had to define separate modules for 4- and 8-bit wide 2:1 multiplexers. HDLs permit variable bit widths using parameterized modules.
Chapt
HDL Example 4.34 declares a parameterized 2:1 multiplexer with a default
width of 8, then uses it to create 8- and 12-bit 4:1 multiplexers.
HDL Example 4.35 shows a decoder, which is an even better application of parameterized modules. A large N:2N decoder is cumbersome to
212 CHAPTER FOUR Hardware Description Languages
mux2_12
lowmux
mux2_12
himux
mux2_12
outmux
y[11:0]
s[1:0] [1:0]
d3[11:0]
d2[11:0]
d1[11:0]
d0[11:0]
[0]
s
[11:0]
d0[11:0]
[11:0]
d1[11:0]
y[11:0]
[1]
s
[11:0]
d0[11:0]
[11:0]
d1[11:0]
y[11:0]
[1]
s
[11:0]
d0[11:0]
[11:0]
d1[11:0]
[11:0]
y[11:0]
Figure 4.30 mux4_12 synthesized circuit
Verilog
module decoder # (parameter N  3)
(input [N1:0] a,
output reg [2**N1:0] y);
always @ (*)
begin
y  0;
y[a]  1;
end
endmodule
2**N indicates 2N.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
use IEEE.STD_LOGIC_ARITH.all;
entity decoder is
generic (N: integer : 3);
port (a: in STD_LOGIC_VECTOR (N1 downto 0);
y: out STD_LOGIC_VECTOR (2**N1 downto 0));
end;
architecture synth of decoder is
begin
process (a)
variable tmp: STD_LOGIC_VECTOR (2**N1 downto 0);
begin
tmp : CONV_STD_LOGIC_VECTOR(0, 2**N);
tmp (CONV_INTEGER(a)) : ‘1’;
y  tmp;
end process;
end;
2**N indicates 2N.
CONV_STD_LOGIC_VECTOR(0, 2**N) produces a
STD_LOGIC_VECTOR of length 2N containing all 0’s. It requires
the STD_LOGIC_ARITH library. The function is useful in other
parameterized functions, such as resettable flip-flops that
need to be able to produce constants with a parameterized
number of bits. The bit index in VHDL must be an integer,
so the CONV_INTEGER function is used to convert a from a
STD_LOGIC_VECTOR to an integer.
HDL Example 4.35 PARAMETERIZED N:2N DECODER
Chapter
4.7 Parameterized Modules 213
specify with case statements, but easy using parameterized code that simply sets the appropriate output bit to 1. Specifically, the decoder uses
blocking assignments to set all the bits to 0, then changes the appropriate
bit to 1.
HDLs also provide generate statements to produce a variable
amount of hardware depending on the value of a parameter. generate
supports for loops and if statements to determine how many of what
types of hardware to produce. HDL Example 4.36 demonstrates how to
use generate statements to produce an N-input AND function from a
cascade of two-input AND gates.
Use generate statements with caution; it is easy to produce a large
amount of hardware unintentionally!
Verilog
module andN
# (parameter width  8)
(input [width1:0] a,
output y);
genvar i;
wire [width1:1] x;
generate
for (i1; iwidth; ii1) begin:forloop
if (i  1)
assign x[1]  a[0] & a[1];
else
assign x[i]  a[i] & x[i1];
end
endgenerate
assign y  x[width1];
endmodule
The for statement loops through i  1, 2, . . . , width1 to
produce many consecutive AND gates. The begin in a
generate for loop must be followed by a : and an arbitrary
label (forloop, in this case).
Of course, writing assign y  &a would be much
easier!
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity andN is
generic (width: integer : 8);
port (a: in STD_LOGIC_VECTOR (width1 downto 0);
y: out STD_LOGIC);
end;
architecture synth of andN is
signal i: integer;
signal x: STD_LOGIC_VECTOR (width1 downto 1);
begin
AllBits: for i in 1 to width1 generate
LowBit: if i  1 generate
A1: x(1)  a(0) and a(1);
end generate;
OtherBits: if i / 1 generate
Ai: x(i)  a(i) and x(i1);
end generate;
end generate;
y  x(width1);
end;
HDL Example 4.36 PARAMETERIZED N-INPUT AND GATE
x[1] x[2] x[3] x[4] x[5] x[6] x[7]
y [7]
a[7:0] [7:0]
[0] [1] [1]
[2] [2] [1]
[3] [3] [2]
[4] [4] [3]
[5] [5] [4]
[6] [6] [5]
[7] [7] [6]
Figure 4.31 andN synthesized circuit
Chapter 04.qxd 1/
4.8 TESTBENCHES
A testbench is an HDL module that is used to test another module, called
the device under test (DUT). The testbench contains statements to apply
inputs to the DUT and, ideally, to check that the correct outputs are produced. The input and desired output patterns are called test vectors.
Consider testing the sillyfunction module from Section 4.1.1 that
computes . This is a simple module, so we can
perform exhaustive testing by applying all eight possible test vectors.
HDL Example 4.37 demonstrates a simple testbench. It instantiates
the DUT, then applies the inputs. Blocking assignments and delays are
used to apply the inputs in the appropriate order. The user must view the
results of the simulation and verify by inspection that the correct outputs
are produced. Testbenches are simulated the same as other HDL modules.
However, they are not synthesizeable.
y a b c  ab c  abc
214 CHAPTER FOUR Hardware Description Languages
Verilog
module testbench1 ();
reg a, b, c;
wire y;
// instantiate device under test
sillyfunction dut (a, b, c, y);
// apply inputs one at a time
initial begin
a  0; b  0; c  0; #10;
c  1; #10;
b  1; c  0; #10;
c  1; #10;
a  1; b  0; c  0; #10;
c  1; #10;
b  1; c  0; #10;
c  1; #10;
end
endmodule
The initial statement executes the statements in its body at
the start of simulation. In this case, it first applies the input
pattern 000 and waits for 10 time units. It then applies 001
and waits 10 more units, and so forth until all eight possible
inputs have been applied. initial statements should be used
only in testbenches for simulation, not in modules intended
to be synthesized into actual hardware. Hardware has no
way of magically executing a sequence of special steps when
it is first turned on.
Like signals in always statements, signals in initial
statements must be declared to be reg.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity testbench1 is — — no inputs or outputs
end;
architecture sim of testbench1 is
component sillyfunction
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end component;
signal a, b, c, y: STD_LOGIC;
begin
— — instantiate device under test
dut: sillyfunction port map (a, b, c, y);
— — apply inputs one at a time
process begin
a  ‘0’; b  ‘0’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
b  ‘1’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
a  ‘1’; b  ‘0’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
b  ‘1’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
wait; — — wait forever
end process;
end;
The process statement first applies the input pattern 000 and
waits for 10 ns. It then applies 001 and waits 10 more ns,
and so forth until all eight possible inputs have been applied.
At the end, the process waits indefinitely; otherwise, the
process would begin again, repeatedly applying the pattern
of test vectors.
HDL Example 4.37 TESTBENCH
Some tools also call the
module to be tested the unit
under test (UUT).
Chapter 04.qxd 1/31/07 8:17 PM 
4.8 Testbenches 215
Checking for correct outputs is tedious and error-prone. Moreover,
determining the correct outputs is much easier when the design is fresh in
your mind; if you make minor changes and need to retest weeks later,
determining the correct outputs becomes a hassle. A much better approach
is to write a self-checking testbench, shown in HDL Example 4.38.
Writing code for each test vector also becomes tedious, especially for
modules that require a large number of vectors. An even better approach
is to place the test vectors in a separate file. The testbench simply reads
the test vectors from the file, applies the input test vector to the DUT,
waits, checks that the output values from the DUT match the output
vector, and repeats until reaching the end of the test vectors file.
Verilog
module testbench2 ();
reg a, b, c;
wire y;
// instantiate device under test
sillyfunction dut (a, b, c, y);
// apply inputs one at a time
// checking results
initial begin
a  0; b  0; c  0; #10;
if (y ! 1) $display(“000 failed.”);
c  1; #10;
if (y ! 0) $display(“001 failed.”);
b  1; c  0; #10;
if (y ! 0) $display(“010 failed.”);
c  1; #10;
if (y ! 0) $display(“011 failed.”);
a  1; b  0; c  0; #10;
if (y ! 1) $display(“100 failed.”);
c  1; #10;
if (y ! 1) $display(“101 failed.”);
b  1; c  0; #10;
if (y ! 0) $display(“110 failed.”);
c  1; #10;
if (y ! 0) $display(“111 failed.”);
end
endmodule
This module checks y against expectations after each input
test vector is applied. In Verilog, comparison using  or !
is effective between signals that do not take on the values of
x and z. Testbenches use the  and ! operators for
comparisons of equality and inequality, respectively, because
these operators work correctly with operands that could be x
or z. It uses the $display system task to print a message on
the simulator console if an error occurs. $display is meaningful only in simulation, not synthesis.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity testbench2 is — — no inputs or outputs
end;
architecture sim of testbench2 is
component sillyfunction
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end component;
signal a, b, c, y: STD_LOGIC;
begin
— — instantiate device under test
dut: sillyfunction port map (a, b, c, y);
— — apply inputs one at a time
— — checking results
process begin
a  ‘0’; b  ‘0’; c  ‘0’; wait for 10 ns;
assert y  ‘1’ report “000 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘0’ report “001 failed.”;
b  ‘1’; c  ‘0’; wait for 10 ns;
assert y  ‘0’ report “010 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘0’ report “011 failed.”;
a  ‘1’; b  ‘0’; c  ‘0’; wait for 10 ns;
assert y  ‘1’ report “100 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘1’ report “101 failed.”;
b  ‘1’; c  ‘0’; wait for 10 ns;
assert y  ‘0’ report “110 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘0’ report “111 failed.”;
wait; — — wait forever
end process;
end;
The assert statement checks a condition and prints the message
given in the report clause if the condition is not satisfied. assert
is meaningful only in simulation, not in synthesis.
HDL Example 4.38 SELF-CHECKING TESTBENCH
Chapter 04.qxd 1/31/07 8:17 PM Page 215
HDL Example 4.39 demonstrates such a testbench. The testbench generates a clock using an always/process statement with no stimulus list,
so that it is continuously reevaluated. At the beginning of the simulation, it
reads the test vectors from a text file and pulses reset for two cycles.
example.tv is a text file containing the inputs and expected output written
in binary:
000_1
001_0
010_0
011_0
100_1
101_1
110_0
111_0
216 CHAPTER FOUR Hardware Description Languages
Verilog
module testbench3 ();
reg clk, reset;
reg a, b, c, yexpected;
wire y;
reg [31:0] vectornum, errors;
reg [3:0] testvectors [10000:0];
// instantiate device under test
sillyfunction dut (a, b, c, y);
// generate clock
always
begin
clk  1; #5; clk  0; #5;
end
// at start of test, load vectors
// and pulse reset
initial
begin
$readmemb (“example.tv”, testvectors);
vectornum  0; errors  0;
reset  1; #27; reset  0;
end
// apply test vectors on rising edge of clk
always @ (posedge clk)
begin
#1; {a, b, c, yexpected}
testvectors[vectornum];
end
// check results on falling edge of clk
always @ (negedge clk)
if (~reset) begin // skip during reset
if (y ! yexpected) begin
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use STD.TEXTIO.all;
entity testbench3 is — — no inputs or outputs
end;
architecture sim of testbench3 is
component sillyfunction
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end component;
signal a, b, c, y: STD_LOGIC;
signal clk, reset: STD_LOGIC;
signal yexpected: STD_LOGIC;
constant MEMSIZE: integer : 10000;
type tvarray is array (MEMSIZE downto 0) of
STD_LOGIC_VECTOR (3 downto 0);
signal testvectors: tvarray;
shared variable vectornum, errors: integer;
begin
— — instantiate device under test
dut: sillyfunction port map (a, b, c, y);
— — generate clock
process begin
clk  ‘1’; wait for 5 ns;
clk  ‘0’; wait for 5 ns;
end process;
— — at start of test, load vectors
— — and pulse reset
process is
file tv: TEXT;
variable i, j: integer;
variable L: line;
variable ch: character;
HDL Example 4.39 TESTBENCH WITH TEST VECTOR FILE
Chapter 04.q
4.8 Testbenches 217
$display (“Error: inputs  %b”, {a, b, c});
$display (“ outputs  %b (%b expected)”,
y, yexpected);
errors  errors  1;
end
vectornum  vectornum  1;
if (testvectors[vectornum]  4’bx) begin
$display (“%d tests completed with %d errors”,
vectornum, errors);
$finish;
end
end
endmodule
$readmemb reads a file of binary numbers into the testvectors
array. $readmemh is similar but reads a file of hexadecimal
numbers.
The next block of code waits one time unit after the
rising edge of the clock (to avoid any confusion if clock and
data change simultaneously), then sets the three inputs and
the expected output based on the four bits in the current test
vector. The next block of code checks the output of the DUT
at the negative edge of the clock, after the inputs have had
time to propagate through the DUT to produce the output, y.
The testbench compares the generated output, y, with the
expected output, yexpected, and prints an error if they don’t
match. %b and %d indicate to print the values in binary and
decimal, respectively. For example, $display (“%b %b”, y,
yexpected); prints the two values, y and yexpected, in
binary. %h prints a value in hexadecimal.
This process repeats until there are no more valid test
vectors in the testvectors array. $finish terminates the
simulation.
Note that even though the Verilog module supports up
to 10,001 test vectors, it will terminate the simulation after
executing the eight vectors in the file.
begin
— — read file of test vectors
i : 0;
FILE_OPEN (tv, “example.tv”, READ_MODE);
while not endfile (tv) loop
readline (tv, L);
for j in 0 to 3 loop
read (L, ch);
if (ch  ‘_’) then read (L, ch);
end if;
if (ch  ‘0’) then
testvectors (i) (j)  ‘0’;
else testvectors (i) (j)  ‘1’;
end if;
end loop;
i : i  1;
end loop;
vectornum : 0; errors : 0;
reset  ‘1’; wait for 27 ns; reset  ‘0’;
wait;
end process;
— — apply test vectors on rising edge of clk
process (clk) begin
if (clk’event and clk  ‘1’) then
a  testvectors (vectornum) (0) after 1 ns;
b  testvectors (vectornum) (1) after 1 ns;
c  testvectors (vectornum) (2) after 1 ns;
yexpected  testvectors (vectornum) (3)
after 1 ns;
end if;
end process;
— — check results on falling edge of clk
process (clk) begin
if (clk’event and clk  ‘0’ and reset  ‘0’) then
assert y  yexpected
report “Error: y  ” & STD_LOGIC’image(y);
if (y / yexpected) then
errors : errors  1;
end if;
vectornum : vectornum  1;
if (is_x (testvectors(vectornum))) then
if (errors  0) then
report “Just kidding — —” &
integer’image (vectornum) &
“tests completed successfully.”
severity failure;
else
report integer’image (vectornum) &
“tests completed, errors  ” &
integer’image (errors)
severity failure;
end if;
end if;
end if;
end process;
end;
The VHDL code is rather ungainly and uses file reading commands beyond the scope of this chapter, but it gives the sense
of what a self-checking testbench looks like.
Chapter 04.qxd 1/31/07 8:17 PM Page 
218 CHAPTER FOUR Hardware Description Languages
New inputs are applied on the rising edge of the clock, and the output
is checked on the falling edge of the clock. This clock (and reset) would
also be provided to the DUT if sequential logic were being tested. Errors
are reported as they occur. At the end of the simulation, the testbench
prints the total number of test vectors applied and the number of errors
detected.
The testbench in HDL Example 4.39 is overkill for such a simple
circuit. However, it can easily be modified to test more complex circuits
by changing the example.tv file, instantiating the new DUT, and changing a few lines of code to set the inputs and check the outputs.
4.9 SUMMARY
Hardware description languages (HDLs) are extremely important tools
for modern digital designers. Once you have learned Verilog or VHDL,
you will be able to specify digital systems much faster than if you had to
draw the complete schematics. The debug cycle is also often much faster,
because modifications require code changes instead of tedious schematic
rewiring. However, the debug cycle can be much longer using HDLs if
you don’t have a good idea of the hardware your code implies.
HDLs are used for both simulation and synthesis. Logic simulation
is a powerful way to test a system on a computer before it is turned into
hardware. Simulators let you check the values of signals inside your system that might be impossible to measure on a physical piece of hardware. Logic synthesis converts the HDL code into digital logic circuits.
The most important thing to remember when you are writing HDL
code is that you are describing real hardware, not writing a computer
program. The most common beginner’s mistake is to write HDL code
without thinking about the hardware you intend to produce. If you
don’t know what hardware you are implying, you are almost certain not
to get what you want. Instead, begin by sketching a block diagram of
your system, identifying which portions are combinational logic, which
portions are sequential circuits or finite state machines, and so forth.
Then write HDL code for each portion, using the correct idioms to
imply the kind of hardware you need.

Exercises 219
Exercises
The following exercises may be done using your favorite HDL. If you have a simulator available, test your design. Print the waveforms and explain how they prove
that it works. If you have a synthesizer available, synthesize your code. Print the
generated circuit diagram, and explain why it matches your expectations.
Exercise 4.1 Sketch a schematic of the circuit described by the following HDL
code. Simplify the schematic so that it shows a minimum number of gates.
Exercise 4.2 Sketch a schematic of the circuit described by the following HDL
code. Simplify the schematic so that it shows a minimum number of gates.
Exercise 4.3 Write an HDL module that computes a four-input XOR function.
The input is a3:0, and the output is y.
Verilog
module exercise1 (input a, b, c,
output y, z);
assign y  a & b & c | a & b & ~c | a & ~b & c;
assign z  a & b | ~a & ~b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity exercise1 is
port (a, b, c: in STD_LOGIC;
y, z: out STD_LOGIC);
end;
architecture synth of exercisel is
begin
y  (a and b and c) or (a and b and (not c)) or
(a and (not b) and c);
z  (a and b) or ((not a) and (not b));
end;
Verilog
module exercise2 (input [3:0] a,
output reg [1:0] y);
always @ (*)
if (a[0]) y  2’b11;
else if (a[1]) y  2’b10;
else if (a[2]) y  2’b01;
else if (a[3]) y  2’b00;
else y  a[1:0];
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity exercise2 is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (1 downto 0));
end;
architecture synth of exercise2 is
begin
process (a) begin
if a(0)  ‘1’ then y  “11”;
elsif a(1)  ‘1’ then y  “10”;
elsif a(2)  ‘1’ then y  “01”;
elsif a(3)  ‘1’ then y  “00”;
else y  a(1 downto 0);
end if;
end process;
end;
Chapter 04.qxd 1/3
220 CHAPTER FOUR Hardware Description Languages
Exercise 4.4 Write a self-checking testbench for Exercise 4.3. Create a test vector
file containing all 16 test cases. Simulate the circuit and show that it works. Introduce an error in the test vector file and show that the testbench reports a mismatch.
Exercise 4.5 Write an HDL module called minority. It receives three inputs,
a, b, and c. It produces one output, y, that is TRUE if at least two of the inputs
are FALSE.
Exercise 4.6 Write an HDL module for a hexadecimal seven-segment display
decoder. The decoder should handle the digits A, B, C, D, E, and F as well as 0–9.
Exercise 4.7 Write a self-checking testbench for Exercise 4.6. Create a test vector file
containing all 16 test cases. Simulate the circuit and show that it works. Introduce an
error in the test vector file and show that the testbench reports a mismatch.
Exercise 4.8 Write an 8:1 multiplexer module called mux8 with inputs s2:0, d0,
d1, d2, d3, d4, d5, d6, d7, and output y.
Exercise 4.9 Write a structural module to compute the logic function,
, using multiplexer logic. Use the 8:1 multiplexer from
Exercise 4.8.
Exercise 4.10 Repeat Exercise 4.9 using a 4:1 multiplexer and as many NOT
gates as you need.
Exercise 4.11 Section 4.5.4 pointed out that a synchronizer could be correctly
described with blocking assignments if the assignments were given in the proper
order. Think of a simple sequential circuit that cannot be correctly described
with blocking assignments, regardless of order.
Exercise 4.12 Write an HDL module for an eight-input priority circuit.
Exercise 4.13 Write an HDL module for a 2:4 decoder.
Exercise 4.14 Write an HDL module for a 6:64 decoder using three instances of
the 2:4 decoders from Exercise 4.13 and a bunch of three-input AND gates.
Exercise 4.15 Write HDL modules that implement the Boolean equations from
Exercise 2.7.
Exercise 4.16 Write an HDL module that implements the circuit from
Exercise 2.18.
Exercise 4.17 Write an HDL module that implements the logic function from
Exercise 2.19. Pay careful attention to how you handle don’t cares.
y abb cabc
Cha
Exercises 221
Exercise 4.18 Write an HDL module that implements the functions from
Exercise 2.24.
Exercise 4.19 Write an HDL module that implements the priority encoder from
Exercise 2.25.
Exercise 4.20 Write an HDL module that implements the binary-to-thermometer
code converter from Exercise 2.27.
Exercise 4.21 Write an HDL module implementing the days-in-month function
from Question 2.2.
Exercise 4.22 Sketch the state transition diagram for the FSM described by the
following HDL code.
Verilog
module fsm2 (input clk, reset,
input a, b,
output y);
reg [1:0] state, nextstate;
parameter S0  2’b00;
parameter S1  2’b01;
parameter S2  2’b10;
parameter S3  2’b11;
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
always @ (*)
case (state)
S0: if (a  b) nextstate  S1;
else nextstate  S0;
S1: if (a & b) nextstate  S2;
else nextstate  S0;
S2: if (a | b) nextstate  S3;
else nextstate  S0;
S3: if (a | b) nextstate  S3;
else nextstate  S0;
endcase
assign y  (state  S1) | (state  S2);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fsm2 is
port (clk, reset: in STD_LOGIC;
a, b: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of fsm2 is
type statetype is (S0, S1, S2, S3);
signal state, nextstate: statetype;
begin
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
process (state, a, b) begin
case state is
when S0  if (a xor b)  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if (a and b)  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if (a or b)  ‘1’ then
nextstate  S3;
else nextstate  S0;
end if;
when S3  if (a or b)  ‘1’ then
nextstate  S3;
else nextstate  S0;
end if;
end case;
end process;
y  ‘1’ when ((state  S1) or (state  S2))
else ‘0’;
end;
Chapter 04.qxd 1/31/07 8:17 PM Page 221
222 CHAPTER FOUR Hardware Description Languages
Exercise 4.23 Sketch the state transition diagram for the FSM described by the
following HDL code. An FSM of this nature is used in a branch predictor on
some microprocessors.
Verilog
module fsm1 (input clk, reset,
input taken, back,
output predicttaken);
reg [4:0] state, nextstate;
parameter S0  5’b00001;
parameter S1  5’b00010;
parameter S2  5’b00100;
parameter S3  5’b01000;
parameter S4  5’b10000;
always @ (posedge clk, posedge reset)
if (reset) state  S2;
else state  nextstate;
always @ (*)
case (state)
S0: if (taken) nextstate  S1;
else nextstate  S0;
S1: if (taken) nextstate  S2;
else nextstate  S0;
S2: if (taken) nextstate  S3;
else nextstate  S1;
S3: if (taken) nextstate  S4;
else nextstate  S2;
S4: if (taken) nextstate  S4;
else nextstate  S3;
default: nextstate  S2;
endcase
assign predicttaken  (state  S4) | |
(state  S3) | |
(state  S2 && back);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fsm1 is
port (clk, reset: in STD_LOGIC;
taken, back: in STD_LOGIC;
predicttaken: out STD_LOGIC);
end;
architecture synth of fsm1 is
type statetype is (S0, S1, S2, S3, S4);
signal state, nextstate: statetype;
begin
process (clk, reset) begin
if reset  ‘1’ then state  S2;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
process (state, taken) begin
case state is
when S0  if taken  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if taken  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if taken  ‘1’ then
nextstate  S3;
else nextstate  S1;
end if;
when S3  if taken  ‘1’ then
nextstate  S4;
else nextstate  S2;
end if;
when S4  if taken  ‘1’ then
nextstate  S4;
else nextstate  S3;
end if;
when others  nextstate  S2;
end case;
end process;
— — output logic
predicttaken  ‘1’ when
((state  S4) or (state  S3) or
(state  S2 and back  ‘1’))
else ‘0’;
end;
Chapter 04.qxd 1/31/07 8:17 PM Page 222
Exercises 223
Exercise 4.24 Write an HDL module for an SR latch.
Exercise 4.25 Write an HDL module for a JK flip-flop. The flip-flop has inputs,
clk, J, and K, and output Q. On the rising edge of the clock, Q keeps its old
value if J  K  0. It sets Q to 1 if J  1, resets Q to 0 if K  1, and inverts Q if
J  K  1.
Exercise 4.26 Write an HDL module for the latch from Figure 3.18. Use one
assignment statement for each gate. Specify delays of 1 unit or 1 ns to each gate.
Simulate the latch and show that it operates correctly. Then increase the inverter
delay. How long does the delay have to be before a race condition causes the
latch to malfunction?
Exercise 4.27 Write an HDL module for the traffic light controller from
Section 3.4.1.
Exercise 4.28 Write three HDL modules for the factored parade mode traffic
light controller from Example 3.8. The modules should be called controller,
mode, and lights, and they should have the inputs and outputs shown in
Figure 3.33(b).
Exercise 4.29 Write an HDL module describing the circuit in Figure 3.40.
Exercise 4.30 Write an HDL module for the FSM with the state transition
diagram given in Figure 3.65 from Exercise 3.19.
Exercise 4.31 Write an HDL module for the FSM with the state transition
diagram given in Figure 3.66 from Exercise 3.20.
Exercise 4.32 Write an HDL module for the improved traffic light controller
from Exercise 3.21.
Exercise 4.33 Write an HDL module for the daughter snail from Exercise 3.22.
Exercise 4.34 Write an HDL module for the soda machine dispenser from
Exercise 3.23.
Exercise 4.35 Write an HDL module for the Gray code counter from
Exercise 3.24.
Exercise 4.36 Write an HDL module for the UP/DOWN Gray code counter
from Exercise 3.25.
Exercise 4.37 Write an HDL module for the FSM from Exercise 3.26.
Chapte
224 CHAPTER FOUR Hardware Description Languages
Exercise 4.38 Write an HDL module for the FSM from Exercise 3.27.
Exercise 4.39 Write an HDL module for the serial two’s complementer from
Question 3.2.
Exercise 4.40 Write an HDL module for the circuit in Exercise 3.28.
Exercise 4.41 Write an HDL module for the circuit in Exercise 3.29.
Exercise 4.42 Write an HDL module for the circuit in Exercise 3.30.
Exercise 4.43 Write an HDL module for the circuit in Exercise 3.31. You may
use the full adder from Section 4.2.5.
Verilog Exercises
The following exercises are specific to Verilog.
Exercise 4.44 What does it mean for a signal to be declared reg in Verilog?
Exercise 4.45 Rewrite the syncbad module from HDL Example 4.30. Use
nonblocking assignments, but change the code to produce a correct
synchronizer with two flip-flops.
Exercise 4.46 Consider the following two Verilog modules. Do they have the
same function? Sketch the hardware each one implies.
module code1 (input clk, a, b, c,
output reg y);
reg x;
always @ (posedge clk) begin
x  a & b;
y  x | c;
end
endmodule
module code2 (input a, b, c, clk,
output reg y);
reg x;
always @ (posedge clk) begin
y  x | c;
x  a & b;
end
endmodule
Exercise 4.47 Repeat Exercise 4.46 if the  is replaced by  in every assignment.
Chapte
Exercises 225
Exercise 4.48 The following Verilog modules show errors that the authors have
seen students make in the laboratory. Explain the error in each module and show
how to fix it.
(a) module latch (input clk,
input [3:0] d,
output reg [3:0] q);
always @ (clk)
if (clk) q  d;
endmodule
(b) module gates (input [3:0] a, b,
output reg [3:0] y1, y2, y3, y4, y5);
always @ (a)
begin
y1  a & b;
y2  a | b;
y3  a  b;
y4  ~(a & b);
y5  ~(a | b);
end
endmodule
(c) module mux2 (input [3:0] d0, d1,
input s,
output reg [3:0] y);
always @ (posedge s)
if (s) y  d1;
else y  d0;
endmodule
(d) module twoflops (input clk,
input d0, d1,
output reg q0, q1);
always @ (posedge clk)
q1  d1;
q0  d0;
endmodule
Chapter 04.
226 CHAPTER FOUR Hardware Description Languages
(e) module FSM (input clk,
input a,
output reg out1, out2);
reg state;
// next state logic and register (sequential)
always @ (posedge clk)
if (state  0) begin
if (a) state  1;
end else begin
if (~a) state  0;
end
always @ (*) // output logic (combinational)
if (state  0) out1  1;
else out2  1;
endmodule
(f) module priority (input [3:0] a,
output reg [3:0] y);
always @ (*)
if (a[3]) y  4’b1000;
else if (a[2]) y  4’b0100;
else if (a[1]) y  4b0010;
else if (a[0]) y  4b0001;
endmodule
(g) module divideby3FSM (input clk,
input reset,
output out);
reg [1:0] state, nextstate;
parameter S0  2b00;
parameter S1  2b01;
parameter S2  2b10;
// State Register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// Next State Logic
always @ (state)
case (state)
S0: nextstate  S1;
S1: nextstate  S2;
2: nextstate  S0;
endcase
// Output Logic
assign out  (state  S2);
endmodule
Chapter 04.qxd 1/31/07 
Exercises 227
(h) module mux2tri (input [3:0] d0, d1,
input s,
output [3:0] y);
tristate t0 (d0, s, y);
tristate t1 (d1, s, y);
endmodule
(i) module floprsen (input clk,
input reset,
input set,
input [3:0] d,
output reg [3:0] q);
always @ (posedge clk, posedge reset)
if (reset) q  0;
else q  d;
always @ (set)
if (set) q  1;
endmodule
(j) module and3 (input a, b, c,
output reg y);
reg tmp;
always @ (a, b, c)
begin
tmp  a & b;
y  tmp & c;
end
endmodule
VHDL Exercises
The following exercises are specific to VHDL.
Exercise 4.49 In VHDL, why is it necessary to write
q  ‘1’ when state  S0 else ‘0’;
rather than simply
q  (state  S0);
Chapter 0
228 CHAPTER FOUR Hardware Description Languages
Exercise 4.50 Each of the following VHDL modules contains an error. For
brevity, only the architecture is shown; assume that the library use clause and
entity declaration are correct. Explain the error and show how to fix it.
(a) architecture synth of latch is
begin
process (clk) begin
if clk  ‘1’ then q  d;
end if;
end process;
end;
(b) architecture proc of gates is
begin
process (a) begin
y1  a and b;
y2  a or b;
y3  a xor b;
y4  a nand b;
y5  a nor b;
end process;
end;
(c) architecture synth of flop is
begin
process (clk)
if clk’event and clk  ‘1’ then
q  d;
end;
(d) architecture synth of priority is
begin
process (a) begin
if a (3) ‘1’ then y  “1000”;
elsif a (2)  ‘1’ then y  “0100”;
elsif a (1)  ‘1’ then y  “0010”;
elsif a (0)  ‘1’ then y  “0001”;
end if;
end process;
end;
(e) architecture synth of divideby3FSM is
type statetype is (S0, S1, S2);
signal state, nextstate: statetype;
begin
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
Chapter 04.qxd 1/31/0
Exercises 229
process (state) begin
case state is
when S0  nextstate  S1;
when S1  nextstate  S2;
when S2  nextstate  S0;
end case;
end process;
q  ‘1’ when state
 S0 else ‘0’;
end;
(f) architecture struct of mux2 is
component tristate
port (a: in STD_LOGIC_VECTOR (3 downto 0);
en: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
begin
t0: tristate port map (d0, s, y);
t1: tristate port map (d1, s, y);
end;
(g) architecture asynchronous of flopr is
begin
process (clk, reset) begin
if reset
 ‘1’ then
q  ‘0’;
elsif clk’event and clk
 ‘1’ then
q  d;
end if;
end process;
process (set) begin
if set
 ‘1’ then
q  ‘1’;
end if;
end process;
end;
(h) architecture synth of mux3 is
begin
y  d2 when s(1) else
d1 when s(0) else d0;
end;
Chapter 04.qxd 
Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 4.1 Write a line of HDL code that gates a 32-bit bus called data
with another signal called sel to produce a 32-bit result. If sel is TRUE,
result  data. Otherwise, result should be all 0’s.
Question 4.2 Explain the difference between blocking and nonblocking
assignments in Verilog. Give examples.
Question 4.3 What does the following Verilog statement do?
result  | (data[15:0] & 16hC820);
230 CHAPTER FOUR Hardware Description Languages
Ch


5
5.1 Introduction
5.2 Arithmetic Circuits
5.3 Number Systems
5.4 Sequential Building Blocks
5.5 Memory Arrays
5.6 Logic Arrays
5.7 Summary
Exercises
Interview Questions
Digital Building Blocks
5.1 INTRODUCTION
Up to this point, we have examined the design of combinational and
sequential circuits using Boolean equations, schematics, and HDLs. This
chapter introduces more elaborate combinational and sequential building
blocks used in digital systems. These blocks include arithmetic circuits,
counters, shift registers, memory arrays, and logic arrays. These building
blocks are not only useful in their own right, but they also demonstrate the
principles of hierarchy, modularity, and regularity. The building blocks are
hierarchically assembled from simpler components such as logic gates, multiplexers, and decoders. Each building block has a well-defined interface
and can be treated as a black box when the underlying implementation is
unimportant. The regular structure of each building block is easily
extended to different sizes. In Chapter 7, we use many of these building
blocks to build a microprocessor.
5.2 ARITHMETIC CIRCUITS
Arithmetic circuits are the central building blocks of computers.
Computers and digital logic perform many arithmetic functions: addition, subtraction, comparisons, shifts, multiplication, and division. This
section describes hardware implementations for all of these operations.
5.2.1 Addition
Addition is one of the most common operations in digital systems. We
first consider how to add two 1-bit binary numbers. We then extend to
N-bit binary numbers. Adders also illustrate trade-offs between speed
and complexity.
Half Adder
We begin by building a 1-bit half adder. As shown in Figure 5.1, the half
adder has two inputs, A and B, and two outputs, S and Cout. S is the
233
A B
0
0
1
1
0
1
1
0
out SC
0
0
0
1
0
1
0
1
S = A ⊕ B
Cout = AB
Half
Adder
A B
S
Cout +
FIGURE 5.1 1-bit half adder

234 CHAPTER FIVE Digital Building Blocks
0001
+0101
0110
1
Figure 5.2 Carry bit
A B
0
0
1
1
0
1
1
0
out SC
0
0
0
1
S=A ⊕ B ⊕ Cin
Cout =AB+ACin +BCin
Full
Adder
Cin
0
0
1
1
0
1
0
1
0
1
0
1
0
0
0
0
1
1
1
1
1
0
0
1
0
1
1
1
A B
S
Cout Cin +
FIGURE 5.3 1-bit full adder
A B
S
Cout Cin +
N
N N
Figure 5.4 Carry propagate
adder
sum of A and B. If A and B are both 1, S is 2, which cannot be represented with a single binary digit. Instead, it is indicated with a carry out,
Cout, in the next column. The half adder can be built from an XOR gate
and an AND gate.
In a multi-bit adder, Cout is added or carried in to the next most significant bit. For example, in Figure 5.2, the carry bit shown in blue is the
output, Cout, of the first column of 1-bit addition and the input, Cin, to
the second column of addition. However, the half adder lacks a Cin input
to accept Cout of the previous column. The full adder, described in the
next section, solves this problem.
Full Adder
A full adder, introduced in Section 2.1, accepts the carry in, Cin, as shown
in Figure 5.3. The figure also shows the output equations for S and Cout.
Carry Propagate Adder
An N-bit adder sums two N-bit inputs, A and B, and a carry in, Cin, to
produce an N-bit result, S, and a carry out, Cout. It is commonly called a
carry propagate adder (CPA) because the carry out of one bit propagates
into the next bit. The symbol for a CPA is shown in Figure 5.4; it is
drawn just like a full adder except that A, B, and S are busses rather
than single bits. Three common CPA implementations are called ripplecarry adders, carry-lookahead adders, and prefix adders.
Ripple-Carry Adder
The simplest way to build an N-bit carry propagate adder is to chain
together N full adders. The Cout of one stage acts as the Cin of the next
stage, as shown in Figure 5.5 for 32-bit addition. This is called a ripplecarry adder. It is a good application of modularity and regularity: the full
adder module is reused many times to form a larger system. The ripplecarry adder has the disadvantage of being slow when N is large. S31
depends on C30, which depends on C29, which depends on C28, and so
forth all the way back to Cin, as shown in blue in Figure 5.5. We say that
the carry ripples through the carry chain. The delay of the adder, tripple,
grows directly with the number of bits, as given in Equation 5.1, where
tFA is the delay of a full adder.
t (5.1) ripple  NtFA
S31
A30 B30
S30
A1 B1
S1
A0 B0
S0
C30 C29 C1 C0
Cout + + + +
A31 B31
Cin
Figure 5.5 32-bit ripple-carry adder
Schematics typically show
signals flowing from left to
right. Arithmetic circuits
break this rule because the
carries flow from right to
left (from the least significant column to the most
significant column).
C
Carry-Lookahead Adder
The fundamental reason that large ripple-carry adders are slow is that the
carry signals must propagate through every bit in the adder. A carrylookahead adder is another type of carry propagate adder that solves this
problem by dividing the adder into blocks and providing circuitry to
quickly determine the carry out of a block as soon as the carry in is
known. Thus it is said to look ahead across the blocks rather than waiting
to ripple through all the full adders inside a block. For example, a 32-bit
adder may be divided into eight 4-bit blocks.
Carry-lookahead adders use generate (G) and propagate (P) signals
that describe how a column or block determines the carry out. The ith
column of an adder is said to generate a carry if it produces a carry out
independent of the carry in. The ith column of an adder is guaranteed to
generate a carry, Ci, if Ai and Bi are both 1. Hence Gi, the generate signal for column i, is calculated as Gi  AiBi. The column is said to propagate a carry if it produces a carry out whenever there is a carry in. The
ith column will propagate a carry in, Ci1, if either Ai or Bi is 1. Thus,
Pi  Ai  Bi. Using these definitions, we can rewrite the carry logic for a
particular column of the adder. The ith column of an adder will generate
a carryout, Ci, if it either generates a carry, Gi, or propagates a carry in,
Pi Ci1. In equation form,
(5.2)
The generate and propagate definitions extend to multiple-bit
blocks. A block is said to generate a carry if it produces a carry out
independent of the carry in to the block. The block is said to propagate a
carry if it produces a carry out whenever there is a carry in to the block.
We define Gi:j and Pi:j as generate and propagate signals for blocks spanning columns i through j.
A block generates a carry if the most significant column generates a
carry, or if the most significant column propagates a carry and the previous column generated a carry, and so forth. For example, the generate
logic for a block spanning columns 3 through 0 is
(5.3)
A block propagates a carry if all the columns in the block propagate the
carry. For example, the propagate logic for a block spanning columns 3
through 0 is
(5.4)
Using the block generate and propagate signals, we can quickly compute
the carry out of the block, Ci, using the carry in to the block, Cj.
C (5.5) i  Gi:j  Pi:j Cj
P3:0  P3 P2 P1 P0
G3:0  G3  P3 (G2  P2 (G1  P1 G0))
Ci  Ai Bi  (Ai  Bi) Ci1  Gi  Pi Ci1
5.2 Arithmetic Circuits 235
Throughout the ages, people
have used many devices to
perform arithmetic. Toddlers
count on their fingers (and
some adults stealthily do too).
The Chinese and Babylonians
invented the abacus as early as
2400 BC. Slide rules, invented
in 1630, were in use until the
1970’s, when scientific hand
calculators became prevalent.
Computers and digital calculators are ubiquitous today.
What will be next?
Chapter 05.
Figure 5.6(a) shows a 32-bit carry-lookahead adder composed of
eight 4-bit blocks. Each block contains a 4-bit ripple-carry adder and
some lookahead logic to compute the carry out of the block given the
carry in, as shown in Figure 5.6(b). The AND and OR gates needed to
compute the single-bit generate and propagate signals, Gi and Pi, from
Ai and Bi are left out for brevity. Again, the carry-lookahead adder
demonstrates modularity and regularity.
All of the CLA blocks compute the single-bit and block generate and
propagate signals simultaneously. The critical path starts with computing G0 and G3:0 in the first CLA block. Cin then advances directly to
Cout through the AND/OR gate in each block until the last. For a large
adder, this is much faster than waiting for the carries to ripple through
each consecutive bit of the adder. Finally, the critical path through the
last block contains a short ripple-carry adder. Thus, an N-bit adder
divided into k-bit blocks has a delay
tCLA  tpg  tpg (5.6) block 
N
k  1  tAND OR  ktFA
236 CHAPTER FIVE Digital Building Blocks
B0
+ + + +
P3:0
G3
P3
G2
P2
G1
P1
G0
P3
P2
P1
P0
G3:0
Cin
Cout
A0
S0
C0
B1 A1
S1
C1
B2 A2
S2
C2
B3 A3
S3
Cin
(b)
(a)
B3:0 A3:0
S3:0
Cin
B7:4 A7:4
S7:4
C7 C3
B27:24 A27:24
S27:24
C23
B31:28 A31:28
S31:28
4-bit CLA
Block
4-bit CLA
Block
4-bit CLA
Block
4-bit CLA
Block
C27
Cout
Figure 5.6 (a) 32-bit carrylookahead adder (CLA), (b) 4-bit
CLA block
Chapte
where tpg is the delay of the individual generate/propagate gates (a single
AND or OR gate) to generate P and G, tpg_block is the delay to find the generate/propagate signals Pi:j and Gi:j for a k-bit block, and tAND_OR is the
delay from Cin to Cout through the AND/OR logic of the k-bit CLA block.
For N  16, the carry-lookahead adder is generally much faster than the
ripple-carry adder. However, the adder delay still increases linearly with N.
Example 5.1 RIPPLE-CARRY ADDER AND CARRY-LOOKAHEAD
ADDER DELAY
Compare the delays of a 32-bit ripple-carry adder and a 32-bit carry-lookahead
adder with 4-bit blocks. Assume that each two-input gate delay is 100 ps and
that a full adder delay is 300 ps.
Solution: According to Equation 5.1, the propagation delay of the 32-bit ripplecarry adder is 32  300 ps  9.6 ns.
The CLA has tpg  100 ps, tpg_block  6  100 ps  600 ps, and tAND_OR  2 
100 ps  200 ps. According to Equation 5.6, the propagation delay of the 32-bit
carry-lookahead adder with 4-bit blocks is thus 100 ps  600 ps  (32/4  1) 
200 ps  (4  300 ps)  3.3 ns, almost three times faster than the ripple-carry
adder.
Prefix Adder*
Prefix adders extend the generate and propagate logic of the carrylookahead adder to perform addition even faster. They first compute G
and P for pairs of columns, then for blocks of 4, then for blocks of 8,
then 16, and so forth until the generate signal for every column is
known. The sums are computed from these generate signals.
In other words, the strategy of a prefix adder is to compute the carry
in, Ci1, for each column, i, as quickly as possible, then to compute the
sum, using
Si  (Ai  Bi)  Ci1 (5.7)
Define column i  1 to hold Cin, so G1  Cin and P1  0. Then
Ci1  Gi1:1 because there will be a carry out of column i1 if
the block spanning columns i1 through 1 generates a carry. The
generated carry is either generated in column i1 or generated in a
previous column and propagated. Thus, we rewrite Equation 5.7 as
Si  (Ai  Bi)  Gi1:1 (5.8)
Hence, the main challenge is to rapidly compute all the block generate
signals G1:1, G0:1, G1:1, G2:1, . . . , GN2:1. These signals, along
with P1:1, P0:1, P1:1, P2:1, . . . , PN2:1, are called prefixes.
5.2 Arithmetic Circuits 237
Early computers used ripple
carry adders, because
components were expensive
and ripple carry adders used
the least hardware. Virtually
all modern PCs use prefix
adders on critical paths,
because transistors are now
cheap and speed is of great
importance.
Chapter 05.qxd 1/27/07 10:27 AM Page 237
Figure 5.7 shows an N  16-bit prefix adder. The adder begins
with a precomputation to form Pi and Gi for each column from Ai and
Bi using AND and OR gates. It then uses log2N  4 levels of black
cells to form the prefixes of Gi:j and Pi:j. A black cell takes inputs from
the upper part of a block spanning bits i:k and from the lower part
spanning bits k1:j. It combines these parts to form generate and
propagate signals for the entire block spanning bits i:j, using the
equations.
(5.9)
(5.10)
In other words, a block spanning bits i:j will generate a carry if the
upper part generates a carry or if the upper part propagates a carry generated in the lower part. The block will propagate a carry if both the
Pi:j  Pi:k Pk1:j
Gi:j  Gi:k  Pi:k Gk1:j
238 CHAPTER FIVE Digital Building Blocks
0:–1
–1
2:1
–1 1:–12:
012
4:3
3
6:5
5:36:3
456
–1 5:–16: –1 3:–14:
8:7
7
10:9
9:710:7
8910
12:11
11
14:13
14:11 13:11
121314
13:714:7 11:712:7
14:-1 13:–1 12:-1 11:–1 10:–1 9:–1 8:–1 7:–1
15
0123456789101112131415
Ai Bi
Pi:i Gi:i
Pi:kPk–1:j Gi:k Gk–1:j
Pi:j Gi:j
Gi–1:–1Ai Bi
Si
i
i:j
Legend i
Figure 5.7 16-bit prefix adder
Chapter
upper and lower parts propagate the carry. Finally, the prefix adder computes the sums using Equation 5.8.
In summary, the prefix adder achieves a delay that grows logarithmically rather than linearly with the number of columns in the adder.
This speedup is significant, especially for adders with 32 or more bits,
but it comes at the expense of more hardware than a simple carrylookahead adder. The network of black cells is called a prefix tree.
The general principle of using prefix trees to perform computations
in time that grows logarithmically with the number of inputs is a powerful technique. With some cleverness, it can be applied to many other
types of circuits (see, for example, Exercise 5.7).
The critical path for an N-bit prefix adder involves the precomputation
of Pi and Gi followed by log2N stages of black prefix cells to obtain all the
prefixes. Gi1:1 then proceeds through the final XOR gate at the bottom
to compute Si
. Mathematically, the delay of an N-bit prefix adder is
(5.11)
where tpg_prefix is the delay of a black prefix cell.
Example 5.2 PREFIX ADDER DELAY
Compute the delay of a 32-bit prefix adder. Assume that each two-input gate
delay is 100 ps.
Solution: The propagation delay of each black prefix cell, tpg_prefix, is 200 ps
(i.e., two gate delays). Thus, using Equation 5.11, the propagation delay of
the 32-bit prefix adder is 100 ps  log2(32)  200 ps  100 ps  1.2 ns,
which is about three times faster than the carry-lookahead adder and eight
times faster than the ripple-carry adder from Example 5.1. In practice, the
benefits are not quite this great, but prefix adders are still substantially faster
than the alternatives.
Putting It All Together
This section introduced the half adder, full adder, and three types of
carry propagate adders: ripple-carry, carry-lookahead, and prefix adders.
Faster adders require more hardware and therefore are more expensive
and power-hungry. These trade-offs must be considered when choosing
an appropriate adder for a design.
Hardware description languages provide the  operation to specify
a CPA. Modern synthesis tools select among many possible implementations, choosing the cheapest (smallest) design that meets the speed
requirements. This greatly simplifies the designer’s job. HDL Example
5.1 describes a CPA with carries in and out.
tPA  tpg  log2N(tpgprefix )  tXOR
5.2 Arithmetic Circuits 239
Chapt
240 CHAPTER FIVE Digital Building Blocks
Verilog
module adder #(parameter N  8)
(input [N1:0] a, b,
input cin,
output [N1:0] s,
output cout);
assign {cout, s}  a  b  cin;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity adder is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
cin: in STD_LOGIC;
s: out STD_LOGIC_VECTOR(N1 downto 0);
cout: out STD_LOGIC);
end;
architecture synth of adder is
signal result: STD_LOGIC_VECTOR(N downto 0);
begin
result  (“0” & a)  (“0” & b)  cin;
s  result (N1 downto 0);
cout  result (N);
end;
HDL Example 5.1 ADDER
5.2.2 Subtraction
Recall from Section 1.4.6 that adders can add positive and negative
numbers using two’s complement number representation. Subtraction is
almost as easy: flip the sign of the second number, then add. Flipping the
sign of a two’s complement number is done by inverting the bits and
adding 1.
To compute Y  A  B, first create the two’s complement of B:
Invert the bits of B to obtain and add 1 to get . Add this
quantity to A to get . This sum can be performed with a single CPA by adding with Cin  1. Figure 5.9
shows the symbol for a subtractor and the underlying hardware for performing Y  A  B. HDL Example 5.2 describes a subtractor.
5.2.3 Comparators
A comparator determines whether two binary numbers are equal or if
one is greater or less than the other. A comparator receives two N-bit
binary numbers, A and B. There are two common types of comparators.
A  B
Y  A  B  1  A  B
B B  B  1
A B
–
Y
(a)
N N
N
+
Y
A B
(b)
N N
N
N
Figure 5.9 Subtractor:
(a) symbol, (b) implementation
+
cout [8]
s[7:0] [7:0]
cin
b[7:0]
a[7:0]
[7:0]
[8:0]
[7:0] [7:0]
Figure 5.8 Synthesized adder
Chapter 05.qxd 1/27/0
5.2 Arithmetic Circuits 241
Verilog
module subtractor #(parameter N  8)
(input [N1:0] a, b,
output [N1:0] y);
assign y  a  b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity subtractor is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
y: out STD_LOGIC_VECTOR(N1 downto 0));
end;
architecture synth of subtractor is
begin
y  a  b;
end;
HDL Example 5.2 SUBTRACTOR
An equality comparator produces a single output indicating whether A is
equal to B (A  B). A magnitude comparator produces one or more
outputs indicating the relative values of A and B.
The equality comparator is the simpler piece of hardware. Figure
5.11 shows the symbol and implementation of a 4-bit equality comparator. It first checks to determine whether the corresponding bits in each
column of A and B are equal, using XNOR gates. The numbers are
equal if all of the columns are equal.
Magnitude comparison is usually done by computing A  B and
looking at the sign (most significant bit) of the result, as shown in Figure
5.12. If the result is negative (i.e., the sign bit is 1), then A is less than B.
Otherwise A is greater than or equal to B.
HDL Example 5.3 shows how to use various comparison operations.
A3
B3
A2
B2
A1
B1
A0
B0
Equal
(b)
Figure 5.11 4-bit equality
comparator: (a) symbol,
(b) implementation
+ y[7:0] [7:0] b[7:0]
a[7:0] [7:0]
[7:0]
1
Figure 5.10 Synthesized subtractor
(a)
=
A B
Equal
4 4
A < B
–
BA
[N –1]
N
N N
Figure 5.12 N-bit magnitude
comparator
Chapter 05.qx
242 CHAPTER FIVE Digital Building Blocks
Verilog
module comparators # (parameter N  8)
(input [N1:0] a, b,
output eq, neq,
output lt, lte,
output gt, gte);
assign eq  (a  b);
assign neq  (a ! b);
assign lt  (a  b);
assign lte  (a  b);
assign gt  (a  b);
assign gte  (a  b);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
entity comparators is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
eq, neq, lt,
lte, gt, gte: out STD_LOGIC);
end;
architecture synth of comparators is
begin
eq  ‘1’ when (a  b) else ‘0’;
neq  ‘1’ when (a / b) else ‘0’;
lt  ‘1’ when (a  b) else ‘0’;
lte  ‘1’ when (a  b) else ‘0’;
gt  ‘1’ when (a  b) else ‘0’;
gte  ‘1’ when (a  b) else ‘0’;
end;
HDL Example 5.3 COMPARATORS
5.2.4 ALU
An Arithmetic/Logical Unit (ALU) combines a variety of mathematical
and logical operations into a single unit. For example, a typical ALU
might perform addition, subtraction, magnitude comparison, AND, and
OR operations. The ALU forms the heart of most computer systems.
Figure 5.14 shows the symbol for an N-bit ALU with N-bit inputs
and outputs. The ALU receives a control signal, F, that specifies which
=
<
<
gte
gt
lte
lt
neq
eq
b[7:0]
a[7:0] [7:0]
[7:0]
[7:0]
[7:0]
[7:0]
[7:0]
Figure 5.13 Synthesized comparators
ALU
N N
N
3
A B
Y
F
Figure 5.14 ALU symbol
Chapter 05.qxd 1/27/07 10
function to perform. Control signals will generally be shown in blue to
distinguish them from the data. Table 5.1 lists typical functions that the
ALU can perform. The SLT function is used for magnitude comparison
and will be discussed later in this section.
Figure 5.15 shows an implementation of the ALU. The ALU contains an N-bit adder and N two-input AND and OR gates. It also
contains an inverter and a multiplexer to optionally invert input B when
the F2 control signal is asserted. A 4:1 multiplexer chooses the desired
function based on the F1:0 control signals.
5.2 Arithmetic Circuits 243
+
A B
Cout
Y
F2
F1:0
[N–1] S
N N
N
N
N N N N
N
2
1
0
3
2
1
0
extend
Zero
BB
Figure 5.15 N-bit ALU
Table 5.1 ALU operations
F2:0 Function
000 A AND B
001 A OR B
010 A  B
011 not used
100 A AND
101 A OR
110 A  B
111 SLT
B
B
C
More specifically, the arithmetic and logical blocks in the ALU
operate on A and BB. BB is either B or , depending on F2. If F1:0  00,
the output multiplexer chooses A AND BB. If F1:0  01, the ALU
computes A OR BB. If F1:0  10, the ALU performs addition or subtraction. Note that F2 is also the carry in to the adder. Also remember that
in two’s complement arithmetic. If F2  0, the ALU computes A  B. If F2  1, the ALU computes .
When F2:0  111, the ALU performs the set if less than (SLT) operation. When A  B, Y  1. Otherwise, Y  0. In other words, Y is set to
1 if A is less than B.
SLT is performed by computing S  A  B. If S is negative (i.e., the
sign bit is set), A  B. The zero extend unit produces an N-bit output by
concatenating its 1-bit input with 0’s in the most significant bits. The
sign bit (the N1th bit) of S is the input to the zero extend unit.
Example 5.3 SET LESS THAN
Configure a 32-bit ALU for the SLT operation. Suppose A  2510 and B  3210.
Show the control signals and output, Y.
Solution: Because A  B, we expect Y to be 1. For SLT, F2:0  111. With F2  1,
this configures the adder unit as a subtractor with an output, S, of 2510  3210
710  1111 . . . 10012. With F1:0  11, the final multiplexer sets Y  S31  1.
Some ALUs produce extra outputs, called flags, that indicate information about the ALU output. For example, an overflow flag indicates
that the result of the adder overflowed. A zero flag indicates that the
ALU output is 0.
The HDL for an N-bit ALU is left to Exercise 5.9. There are many
variations on this basic ALU that support other functions, such as XOR
or equality comparison.
5.2.5 Shifters and Rotators
Shifters and rotators move bits and multiply or divide by powers of 2. As
the name implies, a shifter shifts a binary number left or right by a specified
number of positions. There are several kinds of commonly used shifters:
 Logical shifter—shifts the number to the left (LSL) or right (LSR)
and fills empty spots with 0’s.
Ex: 11001 LSR 2  00110; 11001 LSL 2  00100
 Arithmetic shifter—is the same as a logical shifter, but on right shifts
fills the most significant bits with a copy of the old most significant
bit (msb). This is useful for multiplying and dividing signed numbers
A  B  1  A  B
B  1  B
B
244 CHAPTER FIVE Digital Building Blocks
Chapter 05.qxd 1/27/07 10:27 A
(see Sections 5.2.6 and 5.2.7). Arithmetic shift left (ASL) is the same
as logical shift left (LSL).
Ex: 11001 ASR 2  11110; 11001 ASL 2  00100
 Rotator—rotates number in circle such that empty spots are filled
with bits shifted off the other end.
Ex: 11001 ROR 2  01110; 11001 ROL 2  00111
An N-bit shifter can be built from N N:1 multiplexers. The input is
shifted by 0 to N1 bits, depending on the value of the log2N-bit select
lines. Figure 5.16 shows the symbol and hardware of 4-bit shifters. The
operators , , and  typically indicate shift left, logical shift
right, and arithmetic shift right, respectively. Depending on the value of
the 2-bit shift amount, shamt1:0, the output, Y, receives the input, A,
shifted by 0 to 3 bits. For all shifters, when shamt1:0  00, Y  A.
Exercise 5.14 covers rotator designs.
A left shift is a special case of multiplication. A left shift by N bits
multiplies the number by 2N. For example, 0000112  4  1100002 is
equivalent to 310  24  4810.
5.2 Arithmetic Circuits 245
Figure 5.16 4-bit shifters: (a) shift left, (b) logical shift right, (c) arithmetic shift right
shamt A3 A2 A1 A0 1:0
Y3
Y2
Y1
Y0
(a)
<<
S1:0
S1:0
S1:0
S1:0
00
01
10
11
00
01
10
11
00
01
10
11
00
01
10
11
2
4 4
2
(b)
A3 A2 A1 A0
Y3
Y2
Y1
Y0
shamt1:0
00
01
10
11
A3:0 Y3:0
shamt1:0
>>
S1:0
S1:0
S1:0
S1:0
00
01
10
11
00
01
10
11
00
01
10
11
2
4 4
2
(c)
A3 A2 A1 A0
Y3
Y2
Y1
Y0
shamt1:0
>>>
S1:0
S1:0
S1:0
S1:0
00
01
10
11
00
01
10
11
00
01
10
11
00
01
10
11
2
4 4
2
A3:0 Y3:0
shamt1:0
A3:0 Y3:0
shamt1:0
Chapter 05
An arithmetic right shift is a special case of division. An arithmetic
right shift by N bits divides the number by 2N. For example, 111002
 2  111112 is equivalent to 410/22  110.
5.2.6 Multiplication*
Multiplication of unsigned binary numbers is similar to decimal multiplication but involves only 1’s and 0’s. Figure 5.17 compares multiplication
in decimal and binary. In both cases, partial products are formed by
multiplying a single digit of the multiplier with the entire multiplicand.
The shifted partial products are summed to form the result.
In general, an N  N multiplier multiplies two N-bit numbers and
produces a 2N-bit result. The partial products in binary multiplication
are either the multiplicand or all 0’s. Multiplication of 1-bit binary numbers is equivalent to the AND operation, so AND gates are used to form
the partial products.
Figure 5.18 shows the symbol, function, and implementation of a 4
 4 multiplier. The multiplier receives the multiplicand and multiplier,
A and B, and produces the product, P. Figure 5.18(b) shows how partial
products are formed. Each partial product is a single multiplier bit (B3,
B2, B1, or B0) AND the multiplicand bits (A3, A2, A1, A0). With N-bit
246 CHAPTER FIVE Digital Building Blocks
Figure 5.17 Multiplication:
(a) decimal, (b) binary
Figure 5.18 4  4 multiplier:
(a) symbol, (b) function,
(c) implementation
230
× 42
(a)
460
+ 920
9660
2 30 × 42 = 9660
multiplier
multiplicand
partial
products
result
0101
0111
5 × 7 = 35
0101
0101
0101
0000
×
+
0100011
(b)
(a)
x
A B
P
4 4
8
(b)
× B3 B2 B1 B0
A3B0 A2B0 A1B0 A0B0
A3 A2 A1 A0
A3B1 A2B1 A1B1 A0B1
A3B2 A2B2 A1B2 A0B2
+ A3B3 A2B3 A1B3 A0B3
P7 P6 P5 P4 P3 P2 P1 P0
0
P2
0
0
(c)
0
P7 P6 P5 P4 P3 P1 P0
A3 A2 A1 A0
B0
B1
B2
B3
Chap
operands, there are N partial products and N  1 stages of 1-bit adders.
For example, for a 4  4 multiplier, the partial product of the first row
is B0 AND (A3, A2, A1, A0). This partial product is added to the shifted
second partial product, B1 AND (A3, A2, A1, A0). Subsequent rows of
AND gates and adders form and add the remaining partial products.
The HDL for a multiplier is in HDL Example 5.4. As with adders,
many different multiplier designs with different speed/cost trade-offs
exist. Synthesis tools may pick the most appropriate design given the
timing constraints.
5.2 Arithmetic Circuits 247
Verilog
module multiplier # (parameter N  8)
(input [N1:0] a, b,
output [2*N1:0] y);
assign y  a * b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity multiplier is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
y: out STD_LOGIC_VECTOR(2*N1 downto 0));
end;
architecture synth of multiplier is
begin
y  a * b;
end;
HDL Example 5.4 MULTIPLIER
5.2.7 Division*
Binary division can be performed using the following algorithm for normalized unsigned numbers in the range [2N1, 2N1]:
R  A
for i  N1 to 0
D  R  B
if D  0 then Qi  0, R  R // R  B
else Qi  1, R  D // R 	 B
if i =/ 0 then R  2R
The partial remainder, R, is initialized to the dividend, A. The divisor, B,
is repeatedly subtracted from this partial remainder to determine whether
it fits. If the difference, D, is negative (i.e., the sign bit of D is 1), then the
quotient bit, Qi, is 0 and the difference is discarded. Otherwise, Qi is 1,
Figure 5.19 Synthesized multiplier
* y[15:0] [15:0]
b[7:0]
a[7:0] [7:0]
[7:0]
Chapter 05.qxd 1/27/0
and the partial remainder is updated to be the difference. In any event,
the partial remainder is then doubled (left-shifted by one column), and
the process repeats. The result satisfies
Figure 5.20 shows a schematic of a 4-bit array divider. The divider
computes A/B and produces a quotient, Q, and a remainder, R. The legend shows the symbol and schematic for each block in the array divider.
The signal P indicates whether R  B is negative. It is obtained from the
Cout output of the leftmost block in the row, which is the sign of the
difference.
The delay of an N-bit array divider increases proportionally to N2
because the carry must ripple through all N stages in a row before the
sign is determined and the multiplexer selects R or D. This repeats for all
N rows. Division is a slow and expensive operation in hardware and
therefore should be used as infrequently as possible.
5.2.8 Further Reading
Computer arithmetic could be the subject of an entire text. Digital
Arithmetic, by Ercegovac and Lang, is an excellent overview of the
entire field. CMOS VLSI Design, by Weste and Harris, covers highperformance circuit designs for arithmetic operations.
A
B  Q  R
B2
(N1)
.
248 CHAPTER FIVE Digital Building Blocks
Figure 5.20 Array divider
+
R B
D
R′
P
Cout Cin
0
1
R B
P R′
Cout Cin 1
A3 A2 A1 A0
Q3
1
1 0
Q2
1
1 0
Q1
1
1 0
Q0
B3 B2 B1 B0
R3 R2 R1 R0
Legend
Chapte
5.3 NUMBER SYSTEMS
Computers operate on both integers and fractions. So far, we have only
considered representing signed or unsigned integers, as introduced in
Section 1.4. This section introduces fixed- and floating-point number
systems that can also represent rational numbers. Fixed-point numbers
are analogous to decimals; some of the bits represent the integer part,
and the rest represent the fraction. Floating-point numbers are analogous to scientific notation, with a mantissa and an exponent.
5.3.1 Fixed-Point Number Systems
Fixed-point notation has an implied binary point between the integer and
fraction bits, analogous to the decimal point between the integer and fraction digits of an ordinary decimal number. For example, Figure 5.21(a)
shows a fixed-point number with four integer bits and four fraction bits.
Figure 5.21(b) shows the implied binary point in blue, and Figure 5.21(c)
shows the equivalent decimal value.
Signed fixed-point numbers can use either two’s complement or
sign/magnitude notations. Figure 5.22 shows the fixed-point representation of 2.375 using both notations with four integer and four fraction
bits. The implicit binary point is shown in blue for clarity. In sign/magnitude form, the most significant bit is used to indicate the sign. The two’s
complement representation is formed by inverting the bits of the
absolute value and adding a 1 to the least significant (rightmost) bit. In
this case, the least significant bit position is in the 24 column.
Like all binary number representations, fixed-point numbers are just a
collection of bits. There is no way of knowing the existence of the binary
point except through agreement of those people interpreting the number.
Example 5.4 ARITHMETIC WITH FIXED-POINT NUMBERS
Compute 0.75  0.625 using fixed-point numbers.
Solution: First convert 0.625, the magnitude of the second number, to fixedpoint binary notation. 0.625 	 21, so there is a 1 in the 21 column, leaving
0.6250.5  0.125. Because 0.125  22, there is a 0 in the 22 column. Because
0.125 	 23, there is a 1 in the 23 column, leaving 0.125  0.125  0. Thus, there
must be a 0 in the 24 column. Putting this all together, 0.62510  0000.10102
Use two’s complement representation for signed numbers so that addition works
correctly. Figure 5.23 shows the conversion of 0.625 to fixed-point two’s complement notation.
Figure 5.24 shows the fixed-point binary addition and the decimal equivalent for
comparison. Note that the leading 1 in the binary fixed-point addition of Figure
5.24(a) is discarded from the 8-bit result.
5.3 Number Systems 249
Figure 5.21 Fixed-point notation
of 6.75 with four integer bits
and four fraction bits
(a) 01101100
(b) 0110.1100
(c) 22 + 21 + 2–1 + 2–2 = 6.75
Figure 5.22 Fixed-point
representation of 2.375:
(a) absolute value, (b) sign
and magnitude, (c) two’s
complement
(a) 0010.0110
(b) 1010.0110
(c) 1101.1010
Fixed-point number systems
are commonly used for banking and financial applications
that require precision but not
a large range.
Chapter 05.qxd 1/
250 CHAPTER FIVE Digital Building Blocks
Figure 5.23 Fixed-point two’s
complement conversion
0000.1010
1111.0101
+ 1 Add 1
1111.0110 Two's Complement
One's Complement
Binary Magnitude
Figure 5.24 Addition: (a) binary
fixed-point (b) decimal
equivalent
0000.1100
10000.0010
+ 1111.0110
0.75
 0.125
+ (–0.625)
(a) (b)
Figure 5.25 Floating-point
numbers
Figure 5.26 32-bit floatingpoint version 1
± M × BE
0 00000111 111 0010 0000 0000 0000 0000
Sign Exponent Mantissa
1 bit 8 bits 23 bits
5.3.2 Floating-Point Number Systems*
Floating-point numbers are analogous to scientific notation. They circumvent the limitation of having a constant number of integer and fractional bits, allowing the representation of very large and very small
numbers. Like scientific notation, floating-point numbers have a sign,
mantissa (M), base (B), and exponent (E), as shown in Figure 5.25. For
example, the number 4.1  103 is the decimal scientific notation for
4100. It has a mantissa of 4.1, a base of 10, and an exponent of 3. The
decimal point floats to the position right after the most significant digit.
Floating-point numbers are base 2 with a binary mantissa. 32 bits are
used to represent 1 sign bit, 8 exponent bits, and 23 mantissa bits.
Example 5.5 32-BIT FLOATING-POINT NUMBERS
Show the floating-point representation of the decimal number 228.
Solution: First convert the decimal number into binary: 22810  111001002
1.110012  27. Figure 5.26 shows the 32-bit encoding, which will be modified
later for efficiency. The sign bit is positive (0), the 8 exponent bits give the value
7, and the remaining 23 bits are the mantissa.
In binary floating-point, the first bit of the mantissa (to the left of
the binary point) is always 1 and therefore need not be stored. It is
called the implicit leading one. Figure 5.27 shows the modified floatingpoint representation of 22810  111001002  20  1.110012  27.
The implicit leading one is not included in the 23-bit mantissa for
efficiency. Only the fraction bits are stored. This frees up an extra bit
for useful data.
Chap
We make one final modification to the exponent field. The exponent
needs to represent both positive and negative exponents. To do so, floating-point uses a biased exponent, which is the original exponent plus a
constant bias. 32-bit floating-point uses a bias of 127. For example, for
the exponent 7, the biased exponent is 7  127  134  100001102.
For the exponent 4, the biased exponent is: 4  127  123
011110112. Figure 5.28 shows 1.110012  27 represented in floatingpoint notation with an implicit leading one and a biased exponent of
134 (7  127). This notation conforms to the IEEE 754 floating-point
standard.
Special Cases: 0, 
, and NaN
The IEEE floating-point standard has special cases to represent numbers
such as zero, infinity, and illegal results. For example, representing the
number zero is problematic in floating-point notation because of the
implicit leading one. Special codes with exponents of all 0’s or all 1’s are
reserved for these special cases. Table 5.2 shows the floating-point
representations of 0, 
, and NaN. As with sign/magnitude numbers,
floating-point has both positive and negative 0. NaN is used for numbers that don’t exist, such as or log2 (5).
Single- and Double-Precision Formats
So far, we have examined 32-bit floating-point numbers. This format is
also called single-precision, single, or float. The IEEE 754 standard also
√1
5.3 Number Systems 251
Figure 5.27 Floating-point
version 2 0 00000111 110 0100 0000 0000 0000 0000
Sign Exponent Fraction
1 bit 8 bits 23 bits
Figure 5.28 IEEE 754 floatingpoint notation
0 10000110
Sign Biased
Exponent
Fraction
1 bit 8 bits 23 bits
110 0100 0000 0000 0000 0000
Table 5.2 IEEE 754 floating-point notations for 0, , and NaN
Number Sign Exponent Fraction
0 X 00000000 00000000000000000000000
 0 11111111 00000000000000000000000
 1 11111111 00000000000000000000000
NaN X 11111111 non-zero
As may be apparent, there are
many reasonable ways to represent floating-point numbers.
For many years, computer
manufacturers used incompatible floating-point formats.
Results from one computer
could not directly be interpreted by another computer.
The Institute of Electrical
and Electronics Engineers
solved this problem by defining the IEEE 754 floatingpoint standard in 1985
defining floating-point numbers. This floating-point format is now almost universally
used and is the one discussed
in this section.
Chapter 05.
defines 64-bit double-precision (also called double) numbers that provide greater precision and greater range. Table 5.3 shows the number of
bits used for the fields in each format.
Excluding the special cases mentioned earlier, normal single-precision
numbers span a range of 
1.175494  1038 to 
3.402824  1038.
They have a precision of about seven significant decimal digits (because
224 ≈ 107). Similarly, normal double-precision numbers span a range
of 
2.22507385850720  10308 to 
1.79769313486232  10308 and
have a precision of about 15 significant decimal digits.
Rounding
Arithmetic results that fall outside of the available precision must round
to a neighboring number. The rounding modes are: (1) round down,
(2) round up, (3) round toward zero, and (4) round to nearest. The
default rounding mode is round to nearest. In the round to nearest
mode, if two numbers are equally near, the one with a 0 in the least
significant position of the fraction is chosen.
Recall that a number overflows when its magnitude is too large to
be represented. Likewise, a number underflows when it is too tiny to be
represented. In round to nearest mode, overflows are rounded up to 

and underflows are rounded down to 0.
Floating-Point Addition
Addition with floating-point numbers is not as simple as addition with
two’s complement numbers. The steps for adding floating-point numbers
with the same sign are as follows:
1. Extract exponent and fraction bits.
2. Prepend leading 1 to form the mantissa.
3. Compare exponents.
4. Shift smaller mantissa if necessary.
5. Add mantissas.
6. Normalize mantissa and adjust exponent if necessary.
7. Round result.
8. Assemble exponent and fraction back into floating-point number.
252 CHAPTER FIVE Digital Building Blocks
Floating-point cannot represent some numbers exactly,
like 1.7. However, when you
type 1.7 into your calculator,
you see exactly 1.7, not
1.69999. .. . To handle this,
some applications, such as
calculators and financial
software, use binary coded
decimal (BCD) numbers or
formats with a base 10 exponent. BCD numbers encode
each decimal digit using four
bits with a range of 0 to 9.
For example the BCD fixedpoint notation of 1.7 with
four integer bits and four
fraction bits would be
0001.0111. Of course,
nothing is free. The cost is
increased complexity in
arithmetic hardware and
wasted encodings (A–F
encodings are not used), and
thus decreased performance.
So for compute-intensive
applications, floating-point
is much faster.
Table 5.3 Single- and double-precision floating-point formats
Format Total Bits Sign Bits Exponent Bits Fraction Bits
single 32 1 8 23
double 64 1 11 52
Chap
Figure 5.29 shows the floating-point addition of 7.875 (1.11111  22)
and 0.1875 (1.1  23). The result is 8.0625 (1.0000001  23). After the
fraction and exponent bits are extracted and the implicit leading 1 is
prepended in steps 1 and 2, the exponents are compared by subtracting the
smaller exponent from the larger exponent. The result is the number of bits
by which the smaller number is shifted to the right to align the implied
binary point (i.e., to make the exponents equal) in step 4. The aligned numbers are added. Because the sum has a mantissa that is greater than or
equal to 2.0, the result is normalized by shifting it to the right one bit and
incrementing the exponent. In this example, the result is exact, so no
rounding is necessary. The result is stored in floating-point notation by
removing the implicit leading one of the mantissa and prepending the
sign bit.
5.3 Number Systems 253
Figure 5.29 Floating-point
addition
111 1100 0000 0000 0000 0000
Step 1
10000001
Exponent
01111100 100 0000 0000 0000 0000 0000
1.111 1100 0000 0000 0000 0000
Step 2
10000001
01111100 1.100 0000 0000 0000 0000 0000
1.111 1100 0000 0000 0000 0000
Step 3
10000001
– 01111100 1.100 0000 0000 0000 0000 0000
101 (shift amount)
1.111 1100 0000 0000 0000 0000
Step 4
10000001
10000001 0.000 0110 0000 0000 0000 0000
1.111 1100 0000 0000 0000 0000
Step 5
10000001
10000001 + 0.000 0110 0000 0000 0000 0000
10.000 0010 0000 0000 0000 0000
Step 6
Step 7
Floating-point numbers
1.000 0001 0000 0000 0000 0000
10000001
1
10.000 0010 0000 0000 0000 0000 >> 1
10000010
0
0
Step 8 0
(No rounding necessary)
Fraction
111 1100 0000 0000 0000 0000
100 0000 0000 0000 0000 0000
10000001
01111100
10000010 000 0001 0000 0000 0000 0000
00000
+
Floating-point arithmetic is
usually done in hardware to
make it fast. This hardware,
called the floating-point unit
(FPU), is typically distinct
from the central processing
unit (CPU). The infamous
floating-point division (FDIV)
bug in the Pentium FPU cost
Intel $475 million to recall
and replace defective chips.
The bug occurred simply
because a lookup table was
not loaded correctly.
C
5.4 SEQUENTIAL BUILDING BLOCKS
This section examines sequential building blocks, including counters and
shift registers.
5.4.1 Counters
An N-bit binary counter, shown in Figure 5.30, is a sequential arithmetic
circuit with clock and reset inputs and an N-bit output, Q. Reset initializes the output to 0. The counter then advances through all 2N possible
outputs in binary order, incrementing on the rising edge of the clock.
Figure 5.31 shows an N-bit counter composed of an adder and a resettable register. On each cycle, the counter adds 1 to the value stored in the register. HDL Example 5.5 describes a binary counter with asynchronous reset.
Other types of counters, such as Up/Down counters, are explored in
Exercises 5.37 through 5.40.
254 CHAPTER FIVE Digital Building Blocks
Figure 5.31 N-bit counter
Figure 5.30 Counter symbol
Q
CLK
Reset
N
N
1
CLK
Reset
B
S
A N
Q3:0 N
r
Verilog
module counter #(parameter N  8)
(input clk,
input reset,
output reg [N1:0] q);
always @ (posedge clk or posedge reset)
if (reset) q  0;
else q  q  1;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
use IEEE.STD_LOGIC_ARITH.ALL;
entity counter is
generic (N: integer : 8);
port (clk,
reset: in STD_LOGIC;
q: buffer STD_LOGIC_VECTOR(N1 downto 0));
end;
architecture synth of counter is
begin
process (clk, reset) begin
if reset  ‘1’ then
q  CONV_STD_LOGIC_VECTOR (0, N);
elsif clk’event and clk  ‘1’ then
q  q  1;
end if;
end process;
end;
HDL Example 5.5 COUNTER
Figure 5.32 Synthesized counter
+ R
q[7:0] [7:0]
reset
clk
[7:0]
1 Q[7:0] [7:0] D[7:0]
Chapter 05
5.4.2 Shift Registers
A shift register has a clock, a serial input, Sin, a serial output, Sout, and
N parallel outputs, QN-1:0, as shown in Figure 5.33. On each rising
edge of the clock, a new bit is shifted in from Sin and all the subsequent contents are shifted forward. The last bit in the shift register is
available at Sout. Shift registers can be viewed as serial-to-parallel converters. The input is provided serially (one bit at a time) at Sin. After
N cycles, the past N inputs are available in parallel at Q.
A shift register can be constructed from N flip-flops connected in
series, as shown in Figure 5.34. Some shift registers also have a reset signal to initialize all of the flip-flops.
A related circuit is a parallel-to-serial converter that loads N bits in
parallel, then shifts them out one at a time. A shift register can be modified to perform both serial-to-parallel and parallel-to-serial operations
by adding a parallel input, DN-1:0, and a control signal, Load, as shown
in Figure 5.35. When Load is asserted, the flip-flops are loaded in parallel from the D inputs. Otherwise, the shift register shifts normally. HDL
Example 5.6 describes such a shift register.
Scan Chains*
Shift registers are often used to test sequential circuits using a technique called scan chains. Testing combinational circuits is relatively
straightforward. Known inputs called test vectors are applied, and the
outputs are checked against the expected result. Testing sequential circuits is more difficult, because the circuits have state. Starting from a
known initial condition, a large number of cycles of test vectors may
be needed to put the circuit into a desired state. For example, testing
that the most significant bit of a 32-bit counter advances from 0 to 1
requires resetting the counter, then applying 231 (about two billion)
clock pulses!
5.4 Sequential Building Blocks 255
Don’t confuse shift registers
with the shifters from Section
5.2.5. Shift registers are
sequential logic blocks that
shift in a new bit on each
clock edge. Shifters are
unclocked combinational
logic blocks that shift an
input by a specified amount.
Figure 5.33 Shift register
symbol
N Q
Sin Sout
Figure 5.34 Shift register
schematic
Figure 5.35 Shift register with
parallel load
CLK
Sin Sout
Q0 Q1 Q2 QN–1
CLK
0
1
0
1
0
1
0
1
D0 D1 D2 DN–1
Q0 Q1 Q2 QN – 1
Sin Sout
Load

To solve this problem, designers like to be able to directly observe and
control all the state of the machine. This is done by adding a test mode in
which the contents of all flip-flops can be read out or loaded with desired
values. Most systems have too many flip-flops to dedicate individual pins
to read and write each flip-flop. Instead, all the flip-flops in the system are
connected together into a shift register called a scan chain. In normal
operation, the flip-flops load data from their D input and ignore the scan
chain. In test mode, the flip-flops serially shift their contents out and shift
256 CHAPTER FIVE Digital Building Blocks
Verilog
module shiftreg # (parameter N  8)
(input clk,
input reset, load,
input sin,
input [N1:0] d,
output reg [N1:0] q,
output sout);
always @ (posedge clk or posedge reset)
if (reset) q  0;
else if (load) q  d;
else q  {q[N2:0], sin};
assign sout  q[N1];
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_ARITH.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity shiftreg is
generic (N: integer : 8);
port(clk, reset,
load: in STD_LOGIC;
sin: in STD_LOGIC;
d: in STD_LOGIC_VECTOR(N1 downto 0);
q: buffer STD_LOGIC_VECTOR(N1 downto 0);
sout: out STD_LOGIC);
end;
architecture synth of shiftreg is
begin
process (clk, reset) begin
if reset  ‘1’ then
q  CONV_STD_LOGIC_VECTOR (0, N);
elsif clk’event and clk  ‘1’ then
if load  ‘1’ then
q  d;
else
q  q(N2 downto 0) & sin;
end if;
end if;
end process;
sout  q(N1);
end;
HDL Example 5.6 SHIFT REGISTER WITH PARALLEL LOAD
Figure 5.36 Synthesized shiftreg
0
1 R
sout
[7]
q[7:0]
d[7:0]
sin
load
reset
clk
[6:0]
[7:0] [7:0] [7:0] D[7:0] Q[7:0]
Chapter 05.qxd 1/27/0
5.5 Memory Arrays 257
Figure 5.37 Scannable flip-flop: (a) schematic, (b) symbol, and (c) N-bit scannable register
0
1
Test
D
Sin
Q
Sout
(a)
D Q
Sin Sout
Test
(b)
D Q
Sin Sout
Test
D Q
Sin Sout
Test
D Q
Sin Sout
Test
D Q
Sin Sout
Test
(c)
Test
CLK
CLK
CLK
D0
Q0
D1
Q1
D2
Q2
DN–1
QN–1
Sin Sout
in new contents using Sin and Sout. The load multiplexer is usually integrated into the flip-flop to produce a scannable flip-flop. Figure 5.37
shows the schematic and symbol for a scannable flip-flop and illustrates
how the flops are cascaded to build an N-bit scannable register.
For example, the 32-bit counter could be tested by shifting in the
pattern 011111 . . . 111 in test mode, counting for one cycle in normal
mode, then shifting out the result, which should be 100000 . . . 000.
This requires only 32  1  32  65 cycles.
5.5 MEMORY ARRAYS
The previous sections introduced arithmetic and sequential circuits for
manipulating data. Digital systems also require memories to store the
data used and generated by such circuits. Registers built from flipflops are a kind of memory that stores small amounts of data. This
section describes memory arrays that can efficiently store large
amounts of data.
The section begins with an overview describing characteristics
shared by all memory arrays. It then introduces three types of memory
arrays: dynamic random access memory (DRAM), static random access
memory (SRAM), and read only memory (ROM). Each memory differs
in the way it stores data. The section briefly discusses area and delay
trade-offs and shows how memory arrays are used, not only to store
data but also to perform logic functions. The section finishes with the
HDL for a memory array.
5.5.1 Overview
Figure 5.38 shows a generic symbol for a memory array. The memory is
organized as a two-dimensional array of memory cells. The memory
reads or writes the contents of one of the rows of the array. This row is
Address
Data
Array N
M
Figure 5.38 Generic memory
array symbol
C
specified by an Address. The value read or written is called Data.
An array with N-bit addresses and M-bit data has 2N rows and M
columns. Each row of data is called a word. Thus, the array contains 2N
M-bit words.
Figure 5.39 shows a memory array with two address bits and three
data bits. The two address bits specify one of the four rows (data words)
in the array. Each data word is three bits wide. Figure 5.39(b) shows
some possible contents of the memory array.
The depth of an array is the number of rows, and the width is the
number of columns, also called the word size. The size of an array is
given as depth  width. Figure 5.39 is a 4-word  3-bit array, or simply
4  3 array. The symbol for a 1024-word  32-bit array is shown in
Figure 5.40. The total size of this array is 32 kilobits (Kb).
Bit Cells
Memory arrays are built as an array of bit cells, each of which stores
1 bit of data. Figure 5.41 shows that each bit cell is connected to a
wordline and a bitline. For each combination of address bits, the memory asserts a single wordline that activates the bit cells in that row. When
the wordline is HIGH, the stored bit transfers to or from the bitline.
Otherwise, the bitline is disconnected from the bit cell. The circuitry to
store the bit varies with memory type.
To read a bit cell, the bitline is initially left floating (Z). Then the
wordline is turned ON, allowing the stored value to drive the bitline to 0
or 1. To write a bit cell, the bitline is strongly driven to the desired value.
Then the wordline is turned ON, connecting the bitline to the stored bit.
The strongly driven bitline overpowers the contents of the bit cell, writing the desired value into the stored bit.
Organization
Figure 5.42 shows the internal organization of a 4  3 memory array.
Of course, practical memories are much larger, but the behavior of larger
arrays can be extrapolated from the smaller array. In this example, the
array stores the data from Figure 5.39(b).
During a memory read, a wordline is asserted, and the corresponding row of bit cells drives the bitlines HIGH or LOW. During a
memory write, the bitlines are driven HIGH or LOW first and then a
wordline is asserted, allowing the bitline values to be stored in that
row of bit cells. For example, to read Address 10, the bitlines are left
floating, the decoder asserts wordline2, and the data stored in that
row of bit cells, 100, reads out onto the Data bitlines. To write the
value 001 to Address 11, the bitlines are driven to the value 001,
then wordline3 is asserted and the new value (001) is stored in the
bit cells.
258 CHAPTER FIVE Digital Building Blocks
(a)
Address
Data
Array 2
3
(b)
Address
11
10
01
00
depth
0
1
1
0
1
0
1
1
0
0
0
1
width
Data
Figure 5.39 4  3 memory
array: (a) symbol, (b) function
Address
Data
1024-word ×
32-bit
Array
10
32
Figure 5.40 32 Kb array: depth
210  1024 words, width  32 bits
stored
bit
wordline
bitline
Figure 5.41 Bit cell
Cha
Memory Ports
All memories have one or more ports. Each port gives read and/or write
access to one memory address. The previous examples were all singleported memories.
Multiported memories can access several addresses simultaneously. Figure 5.43 shows a three-ported memory with two read ports
and one write port. Port 1 reads the data from address A1 onto the
read data output RD1. Port 2 reads the data from address A2 onto
RD2. Port 3 writes the data from the write data input, WD3, into
address A3 on the rising edge of the clock if the write enable, WE3, is
asserted.
Memory Types
Memory arrays are specified by their size (depth  width) and the number and type of ports. All memory arrays store data as an array of bit
cells, but they differ in how they store bits.
Memories are classified based on how they store bits in the bit cell.
The broadest classification is random access memory (RAM) versus read
only memory (ROM). RAM is volatile, meaning that it loses its data
when the power is turned off. ROM is nonvolatile, meaning that it
retains its data indefinitely, even without a power source.
RAM and ROM received their names for historical reasons that are
no longer very meaningful. RAM is called random access memory
because any data word is accessed with the same delay as any other.
A sequential access memory, such as a tape recorder, accesses nearby
data more quickly than faraway data (e.g., at the other end of the tape).
5.5 Memory Arrays 259
wordline3 11
10
2:4
Decoder
Address
01
00
stored
bit = 0
stored
bit = 1
stored
bit = 0
stored
bit = 1
stored
bit = 0
stored
bit = 0
stored
bit = 1
stored
bit = 1
stored
bit = 0
stored
bit = 0
stored
bit = 1
stored
bit = 1
wordline2
wordline1
wordline0
bitline2 bitline1 bitline0
Data2 Data1 Data0
2
Figure 5.42 4  3 memory array
A1
A3
WD3
WE3
A2
CLK
Array
RD2
RD1 M
M
N
N
N
M
Figure 5.43 Three-ported
memory

ROM is called read only memory because, historically, it could only be
read but not written. These names are confusing, because ROMs are
randomly accessed too. Worse yet, most modern ROMs can be written
as well as read! The important distinction to remember is that RAMs are
volatile and ROMs are nonvolatile.
The two major types of RAMs are dynamic RAM (DRAM) and
static RAM (SRAM). Dynamic RAM stores data as a charge on a capacitor, whereas static RAM stores data using a pair of cross-coupled
inverters. There are many flavors of ROMs that vary by how they are
written and erased. These various types of memories are discussed in the
subsequent sections.
5.5.2 Dynamic Random Access Memory
Dynamic RAM (DRAM, pronounced “dee-ram”) stores a bit as the
presence or absence of charge on a capacitor. Figure 5.44 shows a
DRAM bit cell. The bit value is stored on a capacitor. The nMOS
transistor behaves as a switch that either connects or disconnects the
capacitor from the bitline. When the wordline is asserted, the
nMOS transistor turns ON, and the stored bit value transfers to or
from the bitline.
As shown in Figure 5.45(a), when the capacitor is charged to VDD,
the stored bit is 1; when it is discharged to GND (Figure 5.45(b)), the
stored bit is 0. The capacitor node is dynamic because it is not actively
driven HIGH or LOW by a transistor tied to VDD or GND.
Upon a read, data values are transferred from the capacitor to the
bitline. Upon a write, data values are transferred from the bitline to
the capacitor. Reading destroys the bit value stored on the capacitor, so
the data word must be restored (rewritten) after each read. Even when
DRAM is not read, the contents must be refreshed (read and rewritten)
every few milliseconds, because the charge on the capacitor gradually
leaks away.
5.5.3 Static Random Access Memory (SRAM)
Static RAM (SRAM, pronounced “es-ram”) is static because stored
bits do not need to be refreshed. Figure 5.46 shows an SRAM bit cell.
The data bit is stored on cross-coupled inverters like those described
in Section 3.2. Each cell has two outputs, bitline and . When the
wordline is asserted, both nMOS transistors turn on, and data values
are transferred to or from the bitlines. Unlike DRAM, if noise
degrades the value of the stored bit, the cross-coupled inverters restore
the value.
bitline
260 CHAPTER FIVE Digital Building Blocks
Robert Dennard, 1932–.
Invented DRAM in 1966 at
IBM. Although many were
skeptical that the idea would
work, by the mid-1970s
DRAM was in virtually all
computers. He claims to have
done little creative work until,
arriving at IBM, they handed
him a patent notebook and
said, “put all your ideas in
there.” Since 1965, he has
received 35 patents in
semiconductors and microelectronics. (Photo courtesy
of IBM.)
wordline
bitline
stored
bit
Figure 5.44 DRAM bit cell

5.5.4 Area and Delay
Flip-flops, SRAMs, and DRAMs are all volatile memories, but each has
different area and delay characteristics. Table 5.4 shows a comparison of
these three types of volatile memory. The data bit stored in a flip-flop is
available immediately at its output. But flip-flops take at least 20 transistors to build. Generally, the more transistors a device has, the more area,
power, and cost it requires. DRAM latency is longer than that of SRAM
because its bitline is not actively driven by a transistor. DRAM must
wait for charge to move (relatively) slowly from the capacitor to the bitline. DRAM also has lower throughput than SRAM, because it must
refresh data periodically and after a read.
Memory latency and throughput also depend on memory size; larger
memories tend to be slower than smaller ones if all else is the same. The
best memory type for a particular design depends on the speed, cost, and
power constraints.
5.5.5 Register Files
Digital systems often use a number of registers to store temporary variables. This group of registers, called a register file, is usually built as a
small, multiported SRAM array, because it is more compact than an
array of flip-flops.
Figure 5.47 shows a 32-register  32-bit three-ported register file
built from a three-ported memory similar to that of Figure 5.43. The
5.5 Memory Arrays 261
Figure 5.45 DRAM stored values
stored
bit
wordline
bitline bitline
Figure 5.46 SRAM bit cell
Table 5.4 Memory comparison
Memory Transistors per Latency
Type Bit Cell
flip-flop ~20 fast
SRAM 6 medium
DRAM 1 slow
5
5
5
32
32
32
CLK
A1
A3
WD3
RD2
RD1 WE3
A2
Register
File
Figure 5.47 32  32 register
file with two read ports and one
write port
wordline
bitline
(a)
stored + +
bit = 1
wordline
bitline
(b)
stored
bit = 0

register file has two read ports (A1/RD1 and A2/RD2) and one write
port (A3/WD3). The 5-bit addresses, A1, A2, and A3, can each access all
25  32 registers. So, two registers can be read and one register written
simultaneously.
5.5.6 Read Only Memory
Read only memory (ROM) stores a bit as the presence or absence of
a transistor. Figure 5.48 shows a simple ROM bit cell. To read the
cell, the bitline is weakly pulled HIGH. Then the wordline is
turned ON. If the transistor is present, it pulls the bitline LOW. If it
is absent, the bitline remains HIGH. Note that the ROM bit cell is
a combinational circuit and has no state to “forget” if power is
turned off.
The contents of a ROM can be indicated using dot notation. Figure
5.49 shows the dot notation for a 4-word  3-bit ROM containing the
data from Figure 5.39. A dot at the intersection of a row (wordline) and
a column (bitline) indicates that the data bit is 1. For example, the top
wordline has a single dot on Data1, so the data word stored at Address
11 is 010.
Conceptually, ROMs can be built using two-level logic with a
group of AND gates followed by a group of OR gates. The AND gates
produce all possible minterms and hence form a decoder. Figure 5.50
shows the ROM of Figure 5.49 built using a decoder and OR gates.
Each dotted row in Figure 5.49 is an input to an OR gate in Figure
5.50. For data bits with a single dot, in this case Data0, no OR gate is
needed. This representation of a ROM is interesting because it shows
how the ROM can perform any two-level logic function. In practice,
ROMs are built from transistors instead of logic gates, to reduce their
size and cost. Section 5.6.3 explores the transistor-level implementation further.
262 CHAPTER FIVE Digital Building Blocks
wordline
bitline
wordline
bitline
bit cell
containing 0
bit cell
containing 1
Figure 5.48 ROM bit cells
containing 0 and 1
11
10
2:4
Decoder
Address
Data2 Data1 Data0
01
00
2
Figure 5.49 4  3 ROM: dot
notation
C
The contents of the ROM bit cell in Figure 5.48 are specified during
manufacturing by the presence or absence of a transistor in each bit cell.
A programmable ROM (PROM, pronounced like the dance) places a
transistor in every bit cell but provides a way to connect or disconnect
the transistor to ground.
Figure 5.51 shows the bit cell for a fuse-programmable ROM. The
user programs the ROM by applying a high voltage to selectively
blow fuses. If the fuse is present, the transistor is connected to GND
and the cell holds a 0. If the fuse is destroyed, the transistor is disconnected from ground and the cell holds a 1. This is also called a onetime programmable ROM, because the fuse cannot be repaired once it
is blown.
Reprogrammable ROMs provide a reversible mechanism for connecting or disconnecting the transistor to GND. Erasable PROMs
(EPROMs, pronounced “e-proms”) replace the nMOS transistor and
fuse with a floating-gate transistor. The floating gate is not physically
attached to any other wires. When suitable high voltages are applied,
electrons tunnel through an insulator onto the floating gate, turning
on the transistor and connecting the bitline to the wordline (decoder
output). When the EPROM is exposed to intense ultraviolet (UV) light
for about half an hour, the electrons are knocked off the floating gate,
turning the transistor off. These actions are called programming and
erasing, respectively. Electrically erasable PROMs (EEPROMs, pronounced “e-e-proms” or “double-e proms”) and Flash memory use
similar principles but include circuitry on the chip for erasing as well
as programming, so no UV light is necessary. EEPROM bit cells are
individually erasable; Flash memory erases larger blocks of bits and is
cheaper because fewer erasing circuits are needed. In 2006, Flash
5.5 Memory Arrays 263
11
10
2:4
Decoder
01
00
Data2 Data1 Data0
Address 2
Figure 5.50 4  3 ROM implementation using gates
wordline
bitline
bit cell containing 0
intact
fuse
wordline
bitline
bit cell containing 1
blown
fuse
Figure 5.51 Fuse-programmable
ROM bit cell
Fujio Masuoka, 1944–. Received
a Ph.D. in electrical engineering
from Tohoku University, Japan.
Developed memories and highspeed circuits at Toshiba from
1971 to 1994. Invented Flash
memory as an unauthorized
project pursued during nights
and weekends in the late 1970s.
Flash received its name because
the process of erasing the
memory reminds one of the flash
of a camera. Toshiba was slow to
commercialize the idea; Intel was
first to market in 1988. Flash has
grown into a $25 billion per year
market. Dr. Masuoka later joined
the faculty at Tohoku University.

memory costs less than $25 per GB, and the price continues to drop
by 30 to 40% per year. Flash has become an extremely popular way to
store large amounts of data in portable battery-powered systems such
as cameras and music players.
In summary, modern ROMs are not really read only; they can be
programmed (written) as well. The difference between RAM and ROM
is that ROMs take a longer time to write but are nonvolatile.
5.5.7 Logic Using Memory Arrays
Although they are used primarily for data storage, memory arrays can
also perform combinational logic functions. For example, the Data2
output of the ROM in Figure 5.49 is the XOR of the two Address
inputs. Likewise Data0 is the NAND of the two inputs. A 2N-word 
M-bit memory can perform any combinational function of N inputs
and M outputs. For example, the ROM in Figure 5.49 performs three
functions of two inputs.
Memory arrays used to perform logic are called lookup tables
(LUTs). Figure 5.52 shows a 4-word  1-bit memory array used as a
lookup table to perform the function Y  AB. Using memory to perform
logic, the user can look up the output value for a given input combination (address). Each address corresponds to a row in the truth table, and
each data bit corresponds to an output value.
5.5.8 Memory HDL
HDL Example 5.7 describes a 2N-word  M-bit RAM. The RAM has a
synchronous enabled write. In other words, writes occur on the rising
264 CHAPTER FIVE Digital Building Blocks
stored
bit = 1
stored
bit = 0
00
01
2:4
Decoder
A
stored
bit = 0
bitline
stored
bit = 0
Y
B
10
11
4-word x1-bit Array
ABY
0 0
0 1
1 0
1 1
0
0
0
1
Truth
Table
A1
A0
Figure 5.52 4-word  1-bit memory array used as a lookup table
Programmable ROMs can be
configured with a device
programmer like the one
shown below. The device
programmer is attached to a
computer, which specifies the
type of ROM and the data
values to program. The device
programmer blows fuses or
injects charge onto a floating
gate on the ROM. Thus the
programming process is sometimes called burning a ROM.
Flash memory drives with
Universal Serial Bus (USB)
connectors have replaced
floppy disks and CDs for
sharing files because Flash
costs have dropped so
dramatically.
C
edge of the clock if the write enable, we, is asserted. Reads occur
immediately. When power is first applied, the contents of the RAM are
unpredictable.
HDL Example 5.8 describes a 4-word  3-bit ROM. The contents
of the ROM are specified in the HDL case statement. A ROM as small
as this one may be synthesized into logic gates rather than an array.
Note that the seven-segment decoder from HDL Example 4.25 synthesizes into a ROM in Figure 4.22.
5.5 Memory Arrays 265
Verilog
module ram # (parameter N  6, M  32)
(input clk,
input we,
input [N1:0] adr,
input [M1:0] din,
output [M1:0] dout);
reg [M1:0] mem [2**N1:0];
always @ (posedge clk)
if (we) mem [adr]  din;
assign dout  mem[adr];
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_ARITH.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity ram_array is
generic (N_
: integer : 6; M: integer : 32);
port (clk,
we: in STD_LOGIC;
adr: in STD_LOGIC_VECTOR(N1 downto 0);
din: in STD_LOGIC_VECTOR(M1 downto 0);
dout: out STD_LOGIC_VECTOR(M1 downto 0));
end;
architecture synth of ram_array is
type mem_array is array ((2**N1) downto 0)
of STD_LOGIC_VECTOR (M1 downto 0);
signal mem: mem_array;
begin
process (clk) begin
if clk’ event and clk  ‘1’ then
if we  ‘1’ then
mem (CONV_INTEGER (adr))  din;
end if;
end if;
end process;
dout  mem (CONV_INTEGER (adr));
end;
HDL Example 5.7 RAM
ram1
mem[31:0]
dout[31:0] [31:0] din[31:0]
adr[5:0]
we
clk
[5:0]
RADDR[5:0]
[31:0]
DATA[31:0]
DOUT[31:0] [5:0]
WADDR[5:0]
WE[0]
CLK
Figure 5.53 Synthesized ram
Chapter 05.qxd 1/27/
5.6 LOGIC ARRAYS
Like memory, gates can be organized into regular arrays. If the connections are made programmable, these logic arrays can be configured to
perform any function without the user having to connect wires in specific ways. The regular structure simplifies design. Logic arrays are mass
produced in large quantities, so they are inexpensive. Software tools
allow users to map logic designs onto these arrays. Most logic arrays are
also reconfigurable, allowing designs to be modified without replacing
the hardware. Reconfigurability is valuable during development and is
also useful in the field, because a system can be upgraded by simply
downloading the new configuration.
This section introduces two types of logic arrays: programmable
logic arrays (PLAs), and field programmable gate arrays (FPGAs). PLAs,
the older technology, perform only combinational logic functions.
FPGAs can perform both combinational and sequential logic.
5.6.1 Programmable Logic Array
Programmable logic arrays (PLAs) implement two-level combinational
logic in sum-of-products (SOP) form. PLAs are built from an AND
array followed by an OR array, as shown in Figure 5.54. The inputs (in
true and complementary form) drive an AND array, which produces
implicants, which in turn are ORed together to form the outputs. An
M  N  P-bit PLA has M inputs, N implicants, and P outputs.
266 CHAPTER FIVE Digital Building Blocks
Verilog
module rom (input [1:0] adr,
output reg [2:0] dout);
always @ (adr)
case (adr)
2b00: dout  3b011;
2b01: dout  3b110;
2b10: dout  3b100;
2b11: dout  3b010;
endcase
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity rom is
port (adr: in STD_LOGIC_VECTOR(1 downto 0);
dout: out STD_LOGIC_VECTOR(2 downto 0));
end;
architecture synth of rom is
begin
process (adr) begin
case adr is
when “00”  dout  “011”;
when “01”  dout  “110”;
when “10”  dout  “100”;
when “11”  dout  “010”;
end case;
end process;
end;
HDL Example 5.8 ROM
Chapter 05.q
Figure 5.55 shows the dot notation for a 3  3  2-bit PLA performing the functions and . Each row in the
AND array forms an implicant. Dots in each row of the AND array indicate which literals comprise the implicant. The AND array in Figure
5.55 forms three implicants: , and . Dots in the OR array
indicate which implicants are part of the output function.
Figure 5.56 shows how PLAs can be built using two-level logic. An
alternative implementation is given in Section 5.6.3.
ROMs can be viewed as a special case of PLAs. A 2M-word 
N-bit ROM is simply an M  2M  N-bit PLA. The decoder behaves
as an AND plane that produces all 2M minterms. The ROM array
behaves as an OR plane that produces the outputs. If the function
does not depend on all 2M minterms, a PLA is likely to be smaller
than a ROM. For example, an 8-word  2-bit ROM is required to
perform the same functions performed by the 3  3  2-bit PLA
shown in Figures 5.55 and 5.56.
ABC, ABC AB
X  ABC  ABC Y  AB
5.6 Logic Arrays 267
AND
Array
OR
Array
Inputs
Outputs
Implicants
N
M
P
Figure 5.54 M  N  P-bit PLA
X Y
ABC
AB
ABC
ABC
AND Array
OR Array
Figure 5.55 3  3  2-bit PLA:
dot notation
Ch
Programmable logic devices (PLDs) are souped-up PLAs that add
registers and various other features to the basic AND/OR planes.
However, PLDs and PLAs have largely been displaced by FPGAs, which
are more flexible and efficient for building large systems.
5.6.2 Field Programmable Gate Array
A field programmable gate array (FPGA) is an array of reconfigurable
gates. Using software programming tools, a user can implement designs
on the FPGA using either an HDL or a schematic. FPGAs are more powerful and more flexible than PLAs for several reasons. They can implement both combinational and sequential logic. They can also implement
multilevel logic functions, whereas PLAs can only implement two-level
logic. Modern FPGAs integrate other useful functions such as built-in
multipliers and large RAM arrays.
FPGAs are built as an array of configurable logic blocks (CLBs).
Figure 5.57 shows the block diagram of the Spartan FPGA introduced by
Xilinx in 1998. Each CLB can be configured to perform combinational or
sequential functions. The CLBs are surrounded by input/output blocks
(IOBs) for interfacing with external devices. The IOBs connect CLB
inputs and outputs to pins on the chip package. CLBs can connect to
other CLBs and IOBs through programmable routing channels.
The remaining blocks shown in the figure aid in programming the device.
Figure 5.58 shows a single CLB for the Spartan FPGA. Other brands
of FPGAs are organized somewhat differently, but the same general principles apply. The CLB contains lookup tables (LUTs), configurable multiplexers, and registers. The FPGA is configured by specifying the
contents of the lookup tables and the select signals for the multiplexers.
268 CHAPTER FIVE Digital Building Blocks
FPGAs are the brains of many
consumer products, including
automobiles, medical equipment, and media devices like
MP3 players. The Mercedes
Benz S-Class series, for example, has over a dozen Xilinx
FPGAs or PLDs for uses ranging from entertainment to
navigation to cruise control
systems. FPGAs allow for
quick time to market and
make debugging or adding
features late in the design
process easier.
X Y
ABC
AND ARRAY
OR ARRAY
ABC
AB
ABC
Figure 5.56 3  3  2-bit PLA
using two-level logic

Each Spartan CLB has three LUTs: the four-input F- and G-LUTs,
and the three-input H-LUT. By loading the appropriate values into the
lookup tables, the F- and G-LUTs can each be configured to perform any
function of up to four variables, and the H-LUT can perform any function of up to three variables.
Configuring the FPGA also involves choosing the select signals that
determine how the multiplexers route data through the CLB. For example,
depending on the multiplexer configuration, the H-LUT may receive one of
its inputs from either DIN or the F-LUT. Similarly, it receives another input
from either SR or the G-LUT. The third input always comes from H1.
5.6 Logic Arrays 269
Figure 5.57 Spartan block diagram

The FPGA produces two combinational outputs, X and Y. Depending
on the multiplexer configuration, X comes from either the F- or H-LUT.
Y comes from either the G- or H-LUT. These outputs can be connected to
other CLBs via the routing channels
The CLB also contains two flip-flops. Depending on the configuration, the flip-flop inputs may come from DIN or from the F-, G-, or
H-LUT. The flip-flop outputs, XQ and YQ, also can be connected to
other CLBs via the routing channels.
In summary, the CLB can perform up to two combinational and/or
two registered functions. All of the functions can involve at least four
variables, and some can involve up to nine.
The designer configures an FPGA by first creating a schematic or
HDL description of the design. The design is then synthesized onto
the FPGA. The synthesis tool determines how the LUTs, multiplexers,
and routing channels should be configured to perform the
specified functions. This configuration information is then downloaded to the FPGA.
Because Xilinx FPGAs store their configuration information in SRAM,
they can be easily reprogrammed. They may download the SRAM contents
from a computer in the laboratory or from an EEPROM chip when the
270 CHAPTER FIVE Digital Building Blocks
Figure 5.58 Spartan CLB

system is turned on. Some manufacturers include EEPROM directly on the
FPGA or use one-time programmable fuses to configure the FPGA.
Example 5.6 FUNCTIONS BUILT USING CLBS
Explain how to configure a CLB to perform the following functions: (a)
and ; (b) Y  JKLMPQR; (c) a divide-by-3 counter
with binary state encoding (see Figure 3.29(a)).
Solution: (a) Configure the F-LUT to compute X and the G-LUT to compute Y,
as shown in Figure 5.59. Inputs F3, F2, and F1 are A, B, and C, respectively
(these connections are set by the routing channels). Inputs G2 and G1 are A
and B. F4, G4, and G3 are don’t cares (and may be connected to 0). Configure
the final multiplexers to select X from the F-LUT and Y from the G-LUT. In
general, a CLB can compute any two functions, of up to four variables each, in
this fashion.
(b) Configure the F-LUT to compute F  JKLM and the G-LUT to compute
G  PQR. Then configure the H-LUT to compute H  FG. Configure the
final multiplexer to select Y from the H-LUT. This configuration is shown in
Figure 5.60. In general, a CLB can compute certain functions of up to nine
variables in this way.
(c) The FSM has two bits of state (S1:0) and one output (Y). The next state
depends on the two bits of current state. Use the F-LUT and G-LUT to compute
the next state from the current state, as shown in Figure 5.61. Use the two flipflops to hold this state. The flip-flops have a dedicated reset input from the SR
signal in the CLB. The registered outputs are fed back to the inputs using the
routing channels, as indicated by the dashed blue lines. In general, another CLB
might be necessary to compute the output Y. However, in this case, Y  , so Y
can come from the same F-LUT used to compute . Hence, the entire FSM fits
in a single CLB. In general, an FSM requires at least one CLB for every two bits
of state, and it may require more CLBs for the output or next state logic if they
are too complex to fit in a single LUT.
S
0
S
0
X  ABC  ABC Y  AB
5.6 Logic Arrays 271
F4
F3
F2
F1
F
F2 F1 F
0 0
0 1
1 0
1 1
0
1
0
0
F3
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
1
0
X
X
X
X
X
X
X
X
F4
(A) (B) (C) (X )
G2 G1 G
0 0
0 1
1 0
1 1
0
0
1
0
G3
X
X
X
X
X
X
X
X
G4
(A) (B) (Y ) G4
G3
G2
G1
G 0
A
B
0
A
B
C
0
Y
X
Figure 5.59 CLB configuration
for two functions of up to four
inputs each
Chapter
Example 5.7 CLB DELAY
Alyssa P. Hacker is building a finite state machine that must run at 200
MHz. She uses a Spartan 3 FPGA with the following specifications: tCLB
0.61 ns per CLB; tsetup  0.53 ns and tpcq  0.72 ns for all flip-flops. What
is the maximum number of CLBs her design can use? You can ignore interconnect delay.
Solution: Alyssa uses Equation 3.13 to solve for the maximum propagation delay
of the logic: tpd  Tc  (tpcq  tsetup).
Thus, tpd  5 ns  (0.72 ns 0.53 ns), so tpd  3.75 ns. The delay of each CLB,
tCLB, is 0.61 ns, and the maximum number of CLBs, N, is NtCLB  3.75 ns.
Thus, N  6.
272 CHAPTER FIVE Digital Building Blocks
F4
F3
F2
F1
F
F2 F1 F
0 0
0 1
1 0
1 1
1
0
0
0
F3
X
X
X
X
X
X
X
X
F4
(S0) (S0')
G2 G1 G
0 0
0 1
1 0
1 1
0
1
0
0
G3
X
X
X
X
X
X
X
X
G4 G4
G3
G2
G1
G 0
S0
0
0
0
YQ
XQ
(S1) ( S1) ( S0)( S1')
S1
clk
clk
Reset
Reset
Y
S1'
S0'
Figure 5.61 CLB configuration for FSM with two bits of state
F4
F3
F2
F1
F
F2 F1 F
0 0
0 1
1 0
1 1
0
0
0
0
F3
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
F4
(K) (L) (M)
G3G4 G2 G1 G
(P) (Q) G4
G3
G2
G1
G P
Q
R
0
K
L
M
J
(J ) (R)
(Y )
0 1
1 0
1 1
0
0
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
1
X
X
X
X
X
X
X
0X 000
0 0
0 1
1 0
1 1
0
0
0
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1
1
0
H1 FGH
0 1
1 0
1 1
0
0
1
X
X
X
X 000
G
H1
F
0 H Y
Figure 5.60 CLB configuration for one function of more than four inputs
Chapte
5.6.3 Array Implementations*
To minimize their size and cost, ROMs and PLAs commonly use pseudonMOS or dynamic circuits (see Section 1.7.8) instead of conventional
logic gates.
Figure 5.62(a) shows the dot notation for a 4  3-bit ROM that performs the following functions: X  A  B, , and Z  .
These are the same functions as those of Figure 5.49, with the address
inputs renamed A and B and the data outputs renamed X, Y, and Z. The
pseudo-nMOS implementation is given in Figure 5.62(b). Each decoder
output is connected to the gates of the nMOS transistors in its row.
Remember that in pseudo-nMOS circuits, the weak pMOS transistor
pulls the output HIGH only if there is no path to GND through the pulldown (nMOS) network.
Pull-down transistors are placed at every junction without a dot.
The dots from the dot notation diagram of Figure 5.62(a) are left faintly
visible in Figure 5.62(b) for easy comparison. The weak pull-up transistors pull the output HIGH for each wordline without a pull-down transistor. For example, when AB  11, the 11 wordline is HIGH and
transistors on X and Z turn on and pull those outputs LOW. The Y output has no transistor connecting to the 11 wordline, so Y is pulled
HIGH by the weak pull-up.
PLAs can also be built using pseudo-nMOS circuits, as shown in
Figure 5.63 for the PLA from Figure 5.55. Pull-down (nMOS) transistors are placed on the complement of dotted literals in the AND array
and on dotted rows in the OR array. The columns in the OR array are
sent through an inverter before they are fed to the output bits. Again, the
blue dots from the dot notation diagram of Figure 5.55 are left faintly
visible in Figure 5.63 for easy comparison.
Y  A  B AB
5.6 Logic Arrays 273
11
10
2:4
Decoder
01
00
A1
A0
X
(a)
A
B
Y Z
11
10
2:4
Decoder
01
00
A1
A0
A
B
weak
(b)
XYZ
Figure 5.62 ROM implementation: (a) dot notation, (b) pseudo-nMOS circuit
Many ROMs and PLAs use
dynamic circuits in place of
pseudo-nMOS circuits.
Dynamic gates turn the pMOS
transistor ON for only part of
the time, saving power when
the pMOS is OFF and the
result is not needed. Aside
from this, dynamic and
pseudo-nMOS memory arrays
are similar in design and
behavior.
Chapt
5.7 SUMMARY
This chapter introduced digital building blocks used in many digital systems. These blocks include arithmetic circuits such as adders, subtractors, comparators, shifters, multipliers, and dividers; sequential circuits
such as counters and shift registers; and arrays for memory and logic.
The chapter also explored fixed-point and floating-point representations
of fractional numbers. In Chapter 7, we use these building blocks to
build a microprocessor.
Adders form the basis of most arithmetic circuits. A half adder adds
two 1-bit inputs, A and B, and produces a sum and a carry out. A full
adder extends the half adder to also accept a carry in. N full adders can
be cascaded to form a carry propagate adder (CPA) that adds two N-bit
numbers. This type of CPA is called a ripple-carry adder because the
carry ripples through each of the full adders. Faster CPAs can be constructed using lookahead or prefix techniques.
A subtractor negates the second input and adds it to the first.
A magnitude comparator subtracts one number from another and
determines the relative value based on the sign of the result. A multiplier forms partial products using AND gates, then sums these bits
using full adders. A divider repeatedly subtracts the divisor from the
partial remainder and checks the sign of the difference to determine the
quotient bits. A counter uses an adder and a register to increment a
running count.
Fractional numbers are represented using fixed-point or floating-point
forms. Fixed-point numbers are analogous to decimals, and floating-point
numbers are analogous to scientific notation. Fixed-point numbers use ordinary arithmetic circuits, whereas floating-point numbers require more elaborate hardware to extract and process the sign, exponent, and mantissa.
274 CHAPTER FIVE Digital Building Blocks
X Y
ABC
AB
ABC
ABC
AND Array
OR Array
weak
weak
Figure 5.63 3  3  2-bit PLA
using pseudo-nMOS circuits

Large memories are organized into arrays of words. The memories
have one or more ports to read and/or write the words. Volatile memories, such as SRAM and DRAM, lose their state when the power is
turned off. SRAM is faster than DRAM but requires more transistors.
A register file is a small multiported SRAM array. Nonvolatile memories, called ROMs, retain their state indefinitely. Despite their names,
most modern ROMs can be written.
Arrays are also a regular way to build logic. Memory arrays can be
used as lookup tables to perform combinational functions. PLAs are
composed of dedicated connections between configurable AND and
OR arrays; they only implement combinational logic. FPGAs are composed of many small lookup tables and registers; they implement
combinational and sequential logic. The lookup table contents and
their interconnections can be configured to perform any logic function.
Modern FPGAs are easy to reprogram and are large and cheap enough
to build highly sophisticated digital systems, so they are widely used in
low- and medium-volume commercial products as well as in education.
5.7 Summary 275

EXERCISES
Exercise 5.1 What is the delay for the following types of 64-bit adders? Assume
that each two-input gate delay is 150 ps and that a full adder delay is 450 ps.
(a) a ripple-carry adder
(b) a carry-lookahead adder with 4-bit blocks
(c) a prefix adder
Exercise 5.2 Design two adders: a 64-bit ripple-carry adder and a 64-bit carrylookahead adder with 4-bit blocks. Use only two-input gates. Each two-input
gate is 15 m2, has a 50 ps delay, and has 20 pF of total gate capacitance. You
may assume that the static power is negligible.
(a) Compare the area, delay, and power of the adders (operating at 100 MHz).
(b) Discuss the trade-offs between power, area, and delay.
Exercise 5.3 Explain why a designer might choose to use a ripple-carry adder
instead of a carry-lookahead adder.
Exercise 5.4 Design the 16-bit prefix adder of Figure 5.7 in an HDL. Simulate
and test your module to prove that it functions correctly.
Exercise 5.5 The prefix network shown in Figure 5.7 uses black cells to compute all of the prefixes. Some of the block propagate signals are not actually necessary. Design a “gray cell” that receives G and P signals for bits i:k and k  1:j
but produces only Gi:j, not Pi:j. Redraw the prefix network, replacing black cells
with gray cells wherever possible.
Exercise 5.6 The prefix network shown in Figure 5.7 is not the only way to calculate all of the prefixes in logarithmic time. The Kogge-Stone network is another
common prefix network that performs the same function using a different connection of black cells. Research Kogge-Stone adders and draw a schematic similar to
Figure 5.7 showing the connection of black cells in a Kogge-Stone adder.
Exercise 5.7 Recall that an N-input priority encoder has log2N outputs that
encodes which of the N inputs gets priority (see Exercise 2.25).
(a) Design an N-input priority encoder that has delay that increases logarithmically with N. Sketch your design and give the delay of the circuit in terms of
the delay of its circuit elements.
(b) Code your design in HDL. Simulate and test your module to prove that it
functions correctly.
276 CHAPTER FIVE Digital Building Blocks
C
Exercise 5.8 Design the following comparators for 32-bit numbers. Sketch the
schematics.
(a) not equal
(b) greater than
(c) less than or equal to
Exercise 5.9 Design the 32-bit ALU shown in Figure 5.15 using your favorite
HDL. You can make the top-level module either behavioral or structural.
Exercise 5.10 Add an Overflow output to the 32-bit ALU from Exercise 5.9. The
output is TRUE when the result of the adder overflows. Otherwise, it is FALSE.
(a) Write a Boolean equation for the Overflow output.
(b) Sketch the Overflow circuit.
(c) Design the modified ALU in an HDL.
Exercise 5.11 Add a Zero output to the 32-bit ALU from Exercise 5.9. The output is TRUE when Y  0.
Exercise 5.12 Write a testbench to test the 32-bit ALU from Exercise 5.9, 5.10,
or 5.11. Then use it to test the ALU. Include any test vector files necessary. Be
sure to test enough corner cases to convince a reasonable skeptic that the ALU
functions correctly.
Exercise 5.13 Design a shifter that always shifts a 32-bit input left by 2 bits. The
input and output are both 32 bits. Explain the design in words and sketch a
schematic. Implement your design in your favourite HDL.
Exercise 5.14 Design 4-bit left and right rotators. Sketch a schematic of your
design. Implement your design in your favourite HDL.
Exercise 5.15 Design an 8-bit left shifter using only 24 2:1 multiplexers. The
shifter accepts an 8-bit input, A, and a 3-bit shift amount, shamt2:0. It produces
an 8-bit output, Y. Sketch the schematic.
Exercise 5.16 Explain how to build any N-bit shifter or rotator using only
Nlog2N 2:1 multiplexers.
Exercise 5.17 The funnel shifter in Figure 5.64 can perform any N-bit shift or
rotate operation. It shifts a 2N-bit input right by k bits. The output, Y, is the
N least significant bits of the result. The most significant N bits of the input are
Exercises 277
Ch
called B and the least significant N bits are called C. By choosing appropriate
values of B, C, and k, the funnel shifter can perform any type of shift or rotate.
Explain what these values should be in terms of A, shamt, and N for
(a) logical right shift of A by shamt.
(b) arithmetic right shift of A by shamt.
(c) left shift of A by shamt.
(d) right rotate of A by shamt.
(e) left rotate of A by shamt.
278 CHAPTER FIVE Digital Building Blocks
B C
k + N – 1 k
2N – 1 N – 1 0
Y
N – 1 0
Figure 5.64 Funnel shifter
Exercise 5.18 Find the critical path for the 4  4 multiplier from Figure 5.18 in
terms of an AND gate delay (tAND) and a full adder delay (tFA). What is the
delay of an N  N multiplier built in the same way?
Exercise 5.19 Design a multiplier that handles two’s complement numbers.
Exercise 5.20 A sign extension unit extends a two’s complement number from M
to N (N  M) bits by copying the most significant bit of the input into the upper
bits of the output (see Section 1.4.6). It receives an M-bit input, A, and produces
an N-bit output, Y. Sketch a circuit for a sign extension unit with a 4-bit input
and an 8-bit output. Write the HDL for your design.
Exercise 5.21 A zero extension unit extends an unsigned number from M to N
bits (N  M) by putting zeros in the upper bits of the output. Sketch a circuit for
a zero extension unit with a 4-bit input and an 8-bit output. Write the HDL for
your design.
Exercise 5.22 Compute 111001.0002/001100.0002 in binary using the standard
division algorithm from elementary school. Show your work.

Exercise 5.23 What is the range of numbers that can be represented by the
following number systems?
(a) 24-bit unsigned fixed-point numbers with 12 integer bits and 12 fraction bits
(b) 24-bit sign and magnitude fixed-point numbers with 12 integer bits and
12 fraction bits
(c) 24-bit two’s complement fixed-point numbers with 12 integer bits and
12 fraction bits
Exercise 5.24 Express the following base 10 numbers in 16-bit fixed-point
sign/magnitude format with eight integer bits and eight fraction bits. Express
your answer in hexadecimal.
(a) 13.5625
(b) 42.3125
(c) 17.15625
Exercise 5.25 Express the base 10 numbers in Exercise 5.24 in 16-bit fixedpoint two’s complement format with eight integer bits and eight fraction bits.
Express your answer in hexadecimal.
Exercise 5.26 Express the base 10 numbers in Exercise 5.24 in IEEE 754
single-precision floating-point format. Express your answer in hexadecimal.
Exercise 5.27 Convert the following two’s complement binary fixed-point
numbers to base 10.
(a) 0101.1000
(b) 1111.1111
(c) 1000.0000
Exercise 5.28 When adding two floating-point numbers, the number with the
smaller exponent is shifted. Why is this? Explain in words and give an example
to justify your explanation.
Exercise 5.29 Add the following IEEE 754 single-precision floating-point
numbers.
(a) C0D20004  72407020
(b) C0D20004  40DC0004
(c) (5FBE4000  3FF80000)  DFDE4000
(Why is the result counterintuitive? Explain.)
Exercises 279
Ch
Exercise 5.30 Expand the steps in section 5.3.2 for performing floating-point
addition to work for negative as well as positive floating-point numbers.
Exercise 5.31 Consider IEEE 754 single-precision floating-point numbers.
(a) How many numbers can be represented by IEEE 754 single-precision
floating-point format? You need not count 
 or NaN.
(b) How many additional numbers could be represented if 
 and NaN were
not represented?
(c) Explain why 
 and NaN are given special representations.
Exercise 5.32 Consider the following decimal numbers: 245 and 0.0625.
(a) Write the two numbers using single-precision floating-point notation. Give
your answers in hexadecimal.
(b) Perform a magnitude comparison of the two 32-bit numbers from part (a).
In other words, interpret the two 32-bit numbers as two’s complement
numbers and compare them. Does the integer comparison give the correct
result?
(c) You decide to come up with a new single-precision floating-point notation.
Everything is the same as the IEEE 754 single-precision floating-point standard, except that you represent the exponent using two’s complement
instead of a bias. Write the two numbers using your new standard. Give
your answers in hexadecimal.
(e) Does integer comparison work with your new floating-point notation from
part (d)?
(f) Why is it convenient for integer comparison to work with floating-point
numbers?
Exercise 5.33 Design a single-precision floating-point adder using your
favorite HDL. Before coding the design in an HDL, sketch a schematic of your
design. Simulate and test your adder to prove to a skeptic that it functions correctly. You may consider positive numbers only and use round toward zero
(truncate). You may also ignore the special cases given in Table 5.2.
Exercise 5.34 In this problem, you will explore the design of a 32-bit floatingpoint multiplier. The multiplier has two 32-bit floating-point inputs and produces a 32-bit floating-point output. You may consider positive numbers only
280 CHAPTER FIVE Digital Building Blocks

and use round toward zero (truncate). You may also ignore the special cases
given in Table 5.2.
(a) Write the steps necessary to perform 32-bit floating-point multiplication.
(b) Sketch the schematic of a 32-bit floating-point multiplier.
(c) Design a 32-bit floating-point multiplier in an HDL. Simulate and test your
multiplier to prove to a skeptic that it functions correctly.
Exercise 5.35 In this problem, you will explore the design of a 32-bit prefix
adder.
(a) Sketch a schematic of your design.
(b) Design the 32-bit prefix adder in an HDL. Simulate and test your adder to
prove that it functions correctly.
(c) What is the delay of your 32-bit prefix adder from part (a)? Assume that
each two-input gate delay is 100 ps.
(d) Design a pipelined version of the 32-bit prefix adder. Sketch the schematic
of your design. How fast can your pipelined prefix adder run? Make the
design run as fast as possible.
(e) Design the pipelined 32-bit prefix adder in an HDL.
Exercise 5.36 An incrementer adds 1 to an N-bit number. Build an 8-bit incrementer using half adders.
Exercise 5.37 Build a 32-bit synchronous Up/Down counter. The inputs are
Reset and Up. When Reset is 1, the outputs are all 0. Otherwise, when Up  1,
the circuit counts up, and when Up  0, the circuit counts down.
Exercise 5.38 Design a 32-bit counter that adds 4 at each clock edge. The
counter has reset and clock inputs. Upon reset, the counter output is all 0.
Exercise 5.39 Modify the counter from Exercise 5.38 such that the counter will
either increment by 4 or load a new 32-bit value, D, on each clock edge, depending on a control signal, PCSrc. When PCSrc  1, the counter loads the new
value D.
Exercise 5.40 An N-bit Johnson counter consists of an N-bit shift register with a
reset signal. The output of the shift register (Sout) is inverted and fed back to the
input (Sin). When the counter is reset, all of the bits are cleared to 0.
(a) Show the sequence of outputs, Q3:0, produced by a 4-bit Johnson counter
starting immediately after the counter is reset.
Exercises 281
Cha
(b) How many cycles elapse until an N-bit Johnson counter repeats its
sequence? Explain.
(c) Design a decimal counter using a 5-bit Johnson counter, ten AND gates, and
inverters. The decimal counter has a clock, a reset, and ten one-hot outputs,
Y9:0. When the counter is reset, Y0 is asserted. On each subsequent cycle,
the next output should be asserted. After ten cycles, the counter should
repeat. Sketch a schematic of the decimal counter.
(d) What advantages might a Johnson counter have over a conventional counter?
Exercise 5.41 Write the HDL for a 4-bit scannable flip-flop like the one shown in
Figure 5.37. Simulate and test your HDL module to prove that it functions
correctly.
Exercise 5.42 The English language has a good deal of redundancy that
allows us to reconstruct garbled transmissions. Binary data can also be
transmitted in redundant form to allow error correction. For example, the
number 0 could be coded as 00000 and the number 1 could be coded as
11111. The value could then be sent over a noisy channel that might flip up
to two of the bits. The receiver could reconstruct the original data because
a 0 will have at least three of the five received bits as 0’s; similarly a 1 will
have at least three 1’s.
(a) Propose an encoding to send 00, 01, 10, or 11 encoded using five bits of
information such that all errors that corrupt one bit of the encoded data can
be corrected. Hint: the encodings 00000 and 11111 for 00 and 11, respectively, will not work.
(b) Design a circuit that receives your five-bit encoded data and decodes it to
00, 01, 10, or 11, even if one bit of the transmitted data has been changed.
(c) Suppose you wanted to change to an alternative 5-bit encoding. How might
you implement your design to make it easy to change the encoding without
having to use different hardware?
Exercise 5.43 Flash EEPROM, simply called Flash memory, is a fairly recent
invention that has revolutionized consumer electronics. Research and explain
how Flash memory works. Use a diagram illustrating the floating gate. Describe
how a bit in the memory is programmed. Properly cite your sources.
Exercise 5.44 The extraterrestrial life project team has just discovered aliens
living on the bottom of Mono Lake. They need to construct a circuit to
classify the aliens by potential planet of origin based on measured features
282 CHAPTER FIVE Digital Building Blocks

available from the NASA probe: greenness, brownness, sliminess, and
ugliness. Careful consultation with xenobiologists leads to the following
conclusions:
 If the alien is green and slimy or ugly, brown, and slimy, it might be from
Mars.
 If the critter is ugly, brown, and slimy, or green and neither ugly nor slimy, it
might be from Venus.
 If the beastie is brown and neither ugly nor slimy or is green and slimy, it
might be from Jupiter.
Note that this is an inexact science; for example, a life form which is mottled
green and brown and is slimy but not ugly might be from either Mars or
Jupiter.
(a) Program a 4  4  3 PLA to identify the alien. You may use dot notation.
(b) Program a 16  3 ROM to identify the alien. You may use dot notation.
(c) Implement your design in an HDL.
Exercise 5.45 Implement the following functions using a single 16  3 ROM.
Use dot notation to indicate the ROM contents.
(a)
(b)
(c) Z  A  B  C  D
Exercise 5.46 Implement the functions from Exercise 5.45 using an 4  8  3
PLA. You may use dot notation.
Exercise 5.47 Specify the size of a ROM that you could use to program each of
the following combinational circuits. Is using a ROM to implement these functions a good design choice? Explain why or why not.
(a) a 16-bit adder/subtractor with Cin and Cout
(b) an 8  8 multiplier
(c) a 16-bit priority encoder (see Exercise 2.25)
Exercise 5.48 Consider the ROM circuits in Figure 5.65. For each row, can the
circuit in column I be replaced by an equivalent circuit in column II by proper
programming of the latter’s ROM?
Y  AB  BD
X  AB  BCD  AB
Exercises 283
Chapte
284 CHAPTER FIVE Digital Building Blocks
K+1
I
A RD
ROM
CLK
N N
K
In
A RD
ROM
N N A RD
ROM
N
A RD
ROM
K+1
CLK
K
A RD Out
ROM
K+1 K+1
CLK
K
In A RD
ROM
K+1 K+1 Out
CLK
K
In A RD
ROM
K+1 K+N N A RD
ROM
N Out
CLK
K
In A RD
ROM
K+1 K+N N Out
CLK
N
In A RD
ROM
N+1 N N A RD
ROM
N Out
CLK
N
In A RD
ROM
N+1 N N Out
II
(a)
(b)
(c)
(d)
Figure 5.65 ROM circuits
Exercise 5.49 Give an example of a nine-input function that can be performed
using only one Spartan FPGA CLB. Give an example of an eight-input function
that cannot be performed using only one CLB.
Exercise 5.50 How many Spartan FPGA CLBs are required to perform each of
the following functions? Show how to configure one or more CLBs to perform
the function. You should be able to do this by inspection, without performing
logic synthesis.
(a) The combinational function from Exercise 2.7(c).
(b) The combinational function from Exercise 2.9(c).
(c) The two-output function from Exercise 2.15.
(d) The function from Exercise 2.24.
(e) A four-input priority encoder (see Exercise 2.25).
(f) An eight-input priority encoder (see Exercise 2.25).
(g) A 3:8 decoder.

(h) A 4-bit carry propagate adder (with no carry in or out).
(i) The FSM from Exercise 3.19.
(j) The Gray code counter from Exercise 3.24.
Exercise 5.51 Consider the Spartan CLB shown in Figure 5.58. It has the following specifications: tpd  tcd  2.7 ns per CLB; tsetup  3.9 ns, thold  0 ns, and
tpcq  2.8 ns for all flip-flops.
(a) What is the minimum number of Spartan CLBs required to implement the
FSM of Figure 3.26?
(b) Without clock skew, what is the fastest clock frequency at which this FSM
will run reliably?
(c) With 5 ns of clock skew, what is the fastest frequency at which the FSM will
run reliably?
Exercise 5.52 You would like to use an FPGA to implement an M&M sorter
with a color sensor and motors to put red candy in one jar and green candy in
another. The design is to be implemented as an FSM using a Spartan XC3S200
FPGA, a chip from the Spartan 3 series family. It is considerably faster than the
original Spartan FPGA. According to the data sheet, the FPGA has timing characteristics shown in Table 5.5. Assume that the design is small enough that wire
delay is negligible.
Exercises 285
Table 5.5 Spartan 3 XC3S200 timing
Name Value (ns)
tpcq 0.72
tsetup 0.53
thold 0
tpd (per CLB) 0.61
tskew 0
You would like your FSM to run at 100 MHz. What is the maximum number
of CLBs on the critical path? What is the fastest speed at which the FSM will run?
Chapt
Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 5.1 What is the largest possible result of multiplying two unsigned
N-bit numbers?
Question 5.2 Binary coded decimal (BCD) representation uses four bits to
encode each decimal digit. For example 4210 is represented as 01000010BCD.
Explain in words why processors might use BCD representation.
Question 5.3 Design hardware to add two 8-bit unsigned BCD numbers (see
Question 5.2). Sketch a schematic for your design, and write an HDL module for
the BCD adder. The inputs are A, B, and Cin, and the outputs are S and Cout. Cin
and Cout are 1-bit carries, and A, B, and S are 8-bit BCD numbers.
286 CHAPTER FIVE Digital Building Blocks



6
6.1 Introduction
6.2 Assembly Language
6.3 Machine Language
6.4 Programming
6.5 Addressing Modes
6.6 Lights, Camera, Action:
Compiling, Assembling,
and Loading
6.7 Odds and Ends*
6.8 Real World Perspective:
IA-32 Architecture*
6.9 Summary
Exercises
Interview Questions
Architecture
6.1 INTRODUCTION
The previous chapters introduced digital design principles and building
blocks. In this chapter, we jump up a few levels of abstraction to define
the architecture of a computer (see Figure 1.1). The architecture is the
programmer’s view of a computer. It is defined by the instruction set
(language), and operand locations (registers and memory). Many different architectures exist, such as IA-32, MIPS, SPARC, and PowerPC.
The first step in understanding any computer architecture is to
learn its language. The words in a computer’s language are called
instructions. The computer’s vocabulary is called the instruction set. All
programs running on a computer use the same instruction set. Even
complex software applications, such as word processing and spreadsheet applications, are eventually compiled into a series of simple
instructions such as add, subtract, and jump. Computer instructions
indicate both the operation to perform and the operands to use.
The operands may come from memory, from registers, or from the
instruction itself.
Computer hardware understands only 1’s and 0’s, so instructions
are encoded as binary numbers in a format called machine language.
Just as we use letters to encode human language, computers use binary
numbers to encode machine language. Microprocessors are digital systems that read and execute machine language instructions. However,
humans consider reading machine language to be tedious, so we prefer
to represent the instructions in a symbolic format, called assembly
language.
The instruction sets of different architectures are more like different
dialects than different languages. Almost all architectures define basic
instructions, such as add, subtract, and jump, that operate on memory
or registers. Once you have learned one instruction set, understanding
others is fairly straightforward.
289

A computer architecture does not define the underlying hardware
implementation. Often, many different hardware implementations of a
single architecture exist. For example, Intel and Advanced Micro Devices
(AMD) sell various microprocessors belonging to the same IA-32 architecture. They all can run the same programs, but they use different
underlying hardware and therefore offer trade-offs in performance, price,
and power. Some microprocessors are optimized for high-performance
servers, whereas others are optimized for long battery life in laptop computers. The specific arrangement of registers, memories, ALUs, and other
building blocks to form a microprocessor is called the microarchitecture
and will be the subject of Chapter 7. Often, many different microarchitectures exist for a single architecture.
In this text, we introduce the MIPS architecture that was first developed by John Hennessy and his colleagues at Stanford in the 1980s. MIPS
processors are used by, among others, Silicon Graphics, Nintendo, and
Cisco. We start by introducing the basic instructions, operand locations,
and machine language formats. We then introduce more instructions used
in common programming constructs, such as branches, loops, array
manipulations, and procedure calls.
Throughout the chapter, we motivate the design of the MIPS architecture using four principles articulated by Patterson and Hennessy: (1) simplicity favors regularity; (2) make the common case fast; (3) smaller is
faster; and (4) good design demands good compromises.
6.2 ASSEMBLY LANGUAGE
Assembly language is the human-readable representation of the computer’s native language. Each assembly language instruction specifies
both the operation to perform and the operands on which to operate. We
introduce simple arithmetic instructions and show how these operations
are written in assembly language. We then define the MIPS instruction
operands: registers, memory, and constants.
We assume that you already have some familiarity with a high-level
programming language such as C, C, or Java. (These languages are
practically identical for most of the examples in this chapter, but where
they differ, we will use C.)
6.2.1 Instructions
The most common operation computers perform is addition. Code
Example 6.1 shows code for adding variables b and c and writing the
result to a. The program is shown on the left in a high-level language
(using the syntax of C, C, and Java), and then rewritten on the right
in MIPS assembly language. Note that statements in a C program end
with a semicolon.
290 CHAPTER SIX Architecture
What is the best architecture
to study when first learning
the subject?
Commercially successful
architectures such as IA-32
are satisfying to study because
you can use them to write
programs on real computers.
Unfortunately, many of these
architectures are full of warts
and idiosyncrasies accumulated over years of haphazard
development by different
engineering teams, making
the architectures difficult to
understand and implement.
Many textbooks teach
imaginary architectures that
are simplified to illustrate the
key concepts.
We follow the lead of David
Patterson and John Hennessy
in their text, Computer
Organization and Design, by
focusing on the MIPS architecture. Hundreds of millions of
MIPS microprocessors have
shipped, so the architecture is
commercially very important.
Yet it is a clean architecture
with little odd behavior. At the
end of this chapter, we briefly
visit the IA-32 architecture to
compare and contrast it with
MIPS.
Chap
6.2 Assembly Language 291
High-Level Code
a  b  c;
MIPS Assembly Code
add a, b, c
Code Example 6.1 ADDITION
High-Level Code
a  b  c;
MIPS Assembly Code
sub a, b, c
Code Example 6.2 SUBTRACTION
High-Level Code
a  b  c  d; // single-line comment
/* multiple-line
comment */
MIPS Assembly Code
sub t, c, d # t  c  d
add a, b, t # a  b  t
Code Example 6.3 MORE COMPLEX CODE
The first part of the assembly instruction, add, is called the
mnemonic and indicates what operation to perform. The operation is
performed on b and c, the source operands, and the result is written to
a, the destination operand.
Code Example 6.2 shows that subtraction is similar to addition. The
instruction format is the same as the add instruction except for the operation specification, sub. This consistent instruction format is an example
of the first design principle:
Design Principle 1: Simplicity favors regularity.
Instructions with a consistent number of operands—in this case, two
sources and one destination—are easier to encode and handle in hardware. More complex high-level code translates into multiple MIPS
instructions, as shown in Code Example 6.3.
In the high-level language examples, single-line comments begin with
// and continue until the end of the line. Multiline comments begin with
/* and end with */. In assembly language, only single-line comments are
used. They begin with # and continue until the end of the line. The
assembly language program in Code Example 6.3 requires a temporary
variable, t, to store the intermediate result. Using multiple assembly
mnemonic (pronounced
ni-mon-ik) comes from the
Greek word E	
,
to remember. The assembly
language mnemonic is easier
to remember than a machine
language pattern of 0’s and
1’s representing the same
operation.
Chapter 
language instructions to perform more complex operations is an example
of the second design principle of computer architecture:
Design Principle 2: Make the common case fast.
The MIPS instruction set makes the common case fast by including
only simple, commonly used instructions. The number of instructions is
kept small so that the hardware required to decode the instruction and
its operands can be simple, small, and fast. More elaborate operations
that are less common are performed using sequences of multiple simple
instructions. Thus, MIPS is a reduced instruction set computer (RISC)
architecture. Architectures with many complex instructions, such as
Intel’s IA-32 architecture, are complex instruction set computers (CISC).
For example, IA-32 defines a “string move” instruction that copies a
string (a series of characters) from one part of memory to another. Such
an operation requires many, possibly even hundreds, of simple instructions in a RISC machine. However, the cost of implementing complex
instructions in a CISC architecture is added hardware and overhead that
slows down the simple instructions.
A RISC architecture minimizes the hardware complexity and the necessary instruction encoding by keeping the set of distinct instructions small.
For example, an instruction set with 64 simple instructions would need
log264  6 bits to encode the operation. An instruction set with 256 complex instructions would need log2256  8 bits of encoding per instruction.
In a CISC machine, even though the complex instructions may be used
only rarely, they add overhead to all instructions, even the simple ones.
6.2.2 Operands: Registers, Memory, and Constants
An instruction operates on operands. In Code Example 6.1 the variables a, b, and c are all operands. But computers operate on 1’s and 0’s,
not variable names. The instructions need a physical location from
which to retrieve the binary data. Operands can be stored in registers or
memory, or they may be constants stored in the instruction itself.
Computers use various locations to hold operands, to optimize for
speed and data capacity. Operands stored as constants or in registers
are accessed quickly, but they hold only a small amount of data.
Additional data must be accessed from memory, which is large but slow.
MIPS is called a 32-bit architecture because it operates on 32-bit data.
(The MIPS architecture has been extended to 64 bits in commercial
products, but we will consider only the 32-bit form in this book.)
Registers
Instructions need to access operands quickly so that they can run fast.
But operands stored in memory take a long time to retrieve. Therefore,
292 CHAPTER SIX Architecture
Ch
most architectures specify a small number of registers that hold commonly used operands. The MIPS architecture uses 32 registers, called the
register set or register file. The fewer the registers, the faster they can be
accessed. This leads to the third design principle:
Design Principle 3: Smaller is faster.
Looking up information from a small number of relevant books on
your desk is a lot faster than searching for the information in the stacks at a
library. Likewise, reading data from a small set of registers (for example,
32) is faster than reading it from 1000 registers or a large memory. A small
register file is typically built from a small SRAM array (see Section 5.5.3).
The SRAM array uses a small decoder and bitlines connected to relatively
few memory cells, so it has a shorter critical path than a large memory does.
Code Example 6.4 shows the add instruction with register operands.
MIPS register names are preceded by the $ sign. The variables a, b, and c
are arbitrarily placed in $s0, $s1, and $s2. The name $s1 is pronounced
“register s1” or “dollar s1”. The instruction adds the 32-bit values contained in $s1 (b) and $s2 (c) and writes the 32-bit result to $s0 (a).
MIPS generally stores variables in 18 of the 32 registers: $s0 – $s7,
and $t0 – $t9. Register names beginning with $s are called saved registers. Following MIPS convention, these registers store variables such as
a, b, and c. Saved registers have special connotations when they are
used with procedure calls (see Section 6.4.6). Register names beginning
with $t are called temporary registers. They are used for storing temporary variables. Code Example 6.5 shows MIPS assembly code using a
temporary register, $t0, to store the intermediate calculation of cd.
6.2 Assembly Language 293
High-Level Code
a  b  c;
MIPS Assembly Code
# $s0  a, $s1  b, $s2 = c
add $s0, $s1, $s2 # a = b + c
Code Example 6.4 REGISTER OPERANDS
High-Level Code
a  b  c  d;
MIPS Assembly Code
# $s0  a, $s1  b, $s2  c, $s3  d
sub $t0, $s2, $s3 # t  c  d
add $s0, $s1, $t0 # a  b  t
Code Example 6.5 TEMPORARY REGISTERS
Chapter 06.qx
Example 6.1 TRANSLATING HIGH-LEVEL CODE TO ASSEMBLY
LANGUAGE
Translate the following high-level code into assembly language. Assume variables
a–c are held in registers $s0–$s2 and f–j are in $s3–$s7.
a  b  c;
f  (g  h)  (i  j);
Solution: The program uses four assembly language instructions.
# MIPS assembly code
# $s0  a, $s1  b, $s2  c, $s3  f, $s4  g, $s5  h,
# $s6  i, $s7  j
sub $s0, $s1, $s2 # a  b  c
add $t0, $s4, $s5 # $t0  g  h
add $t1, $s6, $s7 # $t1  i  j
sub $s3, $t0, $t1 # f  (g  h)  (i  j)
The Register Set
The MIPS architecture defines 32 registers. Each register has a name and a
number ranging from 0 to 31. Table 6.1 lists the name, number, and use for
each register. $0 always contains the value 0 because this constant is so frequently used in computer programs. We have also discussed the $s and $t
registers. The remaining registers will be described throughout this chapter.
294 CHAPTER SIX Architecture
Table 6.1 MIPS register set
Name Number Use
$0 0 the constant value 0
$at 1 assembler temporary
$v0–$v1 2–3 procedure return values
$a0–$a3 4–7 procedure arguments
$t0–$t7 8–15 temporary variables
$s0–$s7 16–23 saved variables
$t8–$t9 24–25 temporary variables
$k0–$k1 26–27 operating system (OS) temporaries
$gp 28 global pointer
$sp 29 stack pointer
$fp 30 frame pointer
$ra 31 procedure return address
Chapter 06.qxd 1/31/
Memory
If registers were the only storage space for operands, we would be confined to simple programs with no more than 32 variables. However, data
can also be stored in memory. When compared to the register file, memory has many data locations, but accessing it takes a longer amount of
time. Whereas the register file is small and fast, memory is large and
slow. For this reason, commonly used variables are kept in registers. By
using a combination of memory and registers, a program can access a
large amount of data fairly quickly. As described in Section 5.5, memories are organized as an array of data words. The MIPS architecture uses
32-bit memory addresses and 32-bit data words.
MIPS uses a byte-addressable memory. That is, each byte in memory
has a unique address. However, for explanation purposes only, we first
introduce a word-addressable memory, and afterward describe the MIPS
byte-addressable memory.
Figure 6.1 shows a memory array that is word-addressable. That is,
each 32-bit data word has a unique 32-bit address. Both the 32-bit word
address and the 32-bit data value are written in hexadecimal in Figure 6.1.
For example, data 0xF2F1AC07 is stored at memory address 1.
Hexadecimal constants are written with the prefix 0x. By convention,
memory is drawn with low memory addresses toward the bottom and
high memory addresses toward the top.
MIPS uses the load word instruction, lw, to read a data word from
memory into a register. Code Example 6.6 loads memory word 1 into $s3.
The lw instruction specifies the effective address in memory as
the sum of a base address and an offset. The base address (written in
parentheses in the instruction) is a register. The offset is a constant
(written before the parentheses). In Code Example 6.6, the base address
6.2 Assembly Language 295
Data
00000003 40F30788
01 E E2 8 42
F2F 1AC07
ABC DEF 7 8
00000002
00000001
00000000
Word
Address
Word 3
Word 2
Word 1
Word 0
Figure 6.1 Word-addressable
memory
Assembly Code
# This assembly code (unlike MIPS) assumes word-addressable memory
lw $s3, 1($0) # read memory word 1 into $s3
Code Example 6.6 READING WORD-ADDRESSABLE MEMORY

is $0, which holds the value 0, and the offset is 1, so the lw instruction
reads from memory address ($0 + 1)  1. After the load word instruction
(lw) is executed, $s3 holds the value 0xF2F1AC07, which is the data
value stored at memory address 1 in Figure 6.1.
Similarly, MIPS uses the store word instruction, sw, to write a data
word from a register into memory. Code Example 6.7 writes the contents of register $s7 into memory word 5. These examples have used $0
as the base address for simplicity, but remember that any register can be
used to supply the base address.
The previous two code examples have shown a computer architecture with a word-addressable memory. The MIPS memory model,
however, is byte-addressable, not word-addressable. Each data byte
has a unique address. A 32-bit word consists of four 8-bit bytes.
So each word address is a multiple of 4, as shown in Figure 6.2.
Again, both the 32-bit word address and the data value are given in
hexadecimal.
Code Example 6.8 shows how to read and write words in the
MIPS byte-addressable memory. The word address is four times
the word number. The MIPS assembly code reads words 0, 2, and 3
and writes words 1, 8, and 100. The offset can be written in decimal
or hexadecimal.
The MIPS architecture also provides the lb and sb instructions that
load and store single bytes in memory rather than words. They are similar
to lw and sw and will be discussed further in Section 6.4.5.
Byte-addressable memories are organized in a big-endian or littleendian fashion, as shown in Figure 6.3. In both formats, the most
significant byte (MSB) is on the left and the least significant byte (LSB) is
on the right. In big-endian machines, bytes are numbered starting with 0
296 CHAPTER SIX Architecture
Assembly Code
# This assembly code (unlike MIPS) assumes word-addressable memory
sw $s7, 5($0) # write $s7 to memory word 5
Code Example 6.7 WRITING WORD-ADDRESSABLE MEMORY
Word
Address Data
0000000C
00000008
00000004
00000000
width = 4 bytes
40F30788
01 EE 2 8 42
F2F1 AC07
ABCD EF 7 8
Word 3
Word 2
Word 1
Word 0
Figure 6.2 Byte-addressable
memory
0 123
MSB LSB
4 567
8 9AB
C DEF
Byte
Address
0 3 210
4 7 654
8 B A98
C F EDC
Byte
Address
Word
Address
Big-Endian Little-Endian
MSB LSB
Figure 6.3 Big- and littleendian memory addressing
C
at the big (most significant) end. In little-endian machines, bytes are
numbered starting with 0 at the little (least significant) end. Word
addresses are the same in both formats and refer to the same four bytes.
Only the addresses of bytes within a word differ.
Example 6.2 BIG- AND LITTLE-ENDIAN MEMORY
Suppose that $s0 initially contains 0x23456789. After the following program is
run on a big-endian system, what value does $s0 contain? In a little-endian
system? lb $s0, 1($0) loads the data at byte address (1  $0)  1 into the least
significant byte of $s0. lb is discussed in detail in Section 6.4.5.
sw $s0, 0($0)
lb $s0, 1($0)
Solution: Figure 6.4 shows how big- and little-endian machines store the value
0x23456789 in memory word 0. After the load byte instruction, lb $s0, 1($0),
$s0 would contain 0x00000045 on a big-endian system and 0x00000067 on a
little-endian system.
6.2 Assembly Language 297
MIPS Assembly Code
1w $s0, 0($0) # read data word 0 (0xABCDEF78) into $s0
1w $s1, 8($0) # read data word 2 (0x01EE2842) into $s1
1w $s2, 0xC($0) # read data word 3 (0x40F30788) into $s2
sw $s3, 4($0) # write $s3 to data word 1
sw $s4, 0x20($0) # write $s4 to data word 8
sw $s5, 400($0) # write $s5 to data word 100
Code Example 6.8 ACCESSING BYTE-ADDRESSABLE MEMORY
The terms big-endian and littleendian come from Jonathan
Swift’s Gulliver’s Travels, first
published in 1726 under the
pseudonym of Isaac Bickerstaff.
In his stories the Lilliputian king
required his citizens (the LittleEndians) to break their eggs on
the little end. The Big-Endians
were rebels who broke their
eggs on the big end.
The terms were first
applied to computer architectures by Danny Cohen in his
paper “On Holy Wars and a
Plea for Peace” published on
April Fools Day, 1980
(USC/ISI IEN 137). (Photo
courtesy The Brotherton
Collection, IEEDS University
Library.)
1 SPIM, the MIPS simulator that comes with this text, uses the endianness of the machine
it is run on. For example, when using SPIM on an Intel IA-32 machine, the memory is
little-endian. With an older Macintosh or Sun SPARC machine, memory is big-endian.
23 45 67 89
0123
0 23 45 67 89
3 210
Word
Address
Big-Endian Little-Endian
Byte Address
Data Value
Byte Address
Data Value
MSB LSB MSB LSB
Figure 6.4 Big-endian and little-endian data storage
IBM’s PowerPC (formerly found in Macintosh computers) uses
big-endian addressing. Intel’s IA-32 architecture (found in PCs) uses
little-endian addressing. Some MIPS processors are little-endian, and
some are big-endian.1 The choice of endianness is completely arbitrary
but leads to hassles when sharing data between big-endian and littleendian computers. In examples in this text, we will use little-endian
format whenever byte ordering matters.
Ch
In the MIPS architecture, word addresses for lw and sw must be
word aligned. That is, the address must be divisible by 4. Thus, the
instruction lw $s0, 7($0) is an illegal instruction. Some architectures,
such as IA-32, allow non-word-aligned data reads and writes, but MIPS
requires strict alignment for simplicity. Of course, byte addresses for
load byte and store byte, lb and sb, need not be word aligned.
Constants/Immediates
Load word and store word, lw and sw, also illustrate the use of constants in MIPS instructions. These constants are called immediates,
because their values are immediately available from the instruction and
do not require a register or memory access. Add immediate, addi, is
another common MIPS instruction that uses an immediate operand.
addi adds the immediate specified in the instruction to a value in a register, as shown in Code Example 6.9.
The immediate specified in an instruction is a 16-bit two’s complement number in the range [32768, 32767]. Subtraction is equivalent to
adding a negative number, so, in the interest of simplicity, there is no
subi instruction in the MIPS architecture.
Recall that the add and sub instructions use three register operands.
But the lw, sw, and addi instructions use two register operands and a
constant. Because the instruction formats differ, lw and sw instructions
violate design principle 1: simplicity favors regularity. However, this
issue allows us to introduce the last design principle:
298 CHAPTER SIX Architecture
High-Level Code
a  a  4;
b  a  12;
MIPS Assembly Code
# $s0  a, $s1  b
addi $s0, $s0, 4 # a  a  4
addi $s1, $s0, 12 # b  a  12
Code Example 6.9 IMMEDIATE OPERANDS
Design Principle 4: Good design demands good compromises.
A single instruction format would be simple but not flexible. The
MIPS instruction set makes the compromise of supporting three instruction formats. One format, used for instructions such as add and sub, has
three register operands. Another, used for instructions such as lw and
addi, has two register operands and a 16-bit immediate. A third, to be
discussed later, has a 26-bit immediate and no registers. The next section
discusses the three MIPS instruction formats and shows how they are
encoded into binary.
Chapter 
6.3 MACHINE LANGUAGE
Assembly language is convenient for humans to read. However, digital
circuits understand only 1’s and 0’s. Therefore, a program written in
assembly language is translated from mnemonics to a representation
using only 1’s and 0’s, called machine language.
MIPS uses 32-bit instructions. Again, simplicity favors regularity,
and the most regular choice is to encode all instructions as words that
can be stored in memory. Even though some instructions may not
require all 32 bits of encoding, variable-length instructions would add
too much complexity. Simplicity would also encourage a single
instruction format, but, as already mentioned, that is too restrictive.
MIPS makes the compromise of defining three instruction formats:
R-type, I-type, and J-type. This small number of formats allows for
some regularity among all the types, and thus simpler hardware, while
also accommodating different instruction needs, such as the need to
encode large constants in the instruction. R-type instructions operate
on three registers. I-type instructions operate on two registers and
a 16-bit immediate. J-type (jump) instructions operate on one
26-bit immediate. We introduce all three formats in this section but
leave the discussion of J-type instructions for Section 6.4.2.
6.3.1 R-type Instructions
The name R-type is short for register-type. R-type instructions use
three registers as operands: two as sources, and one as a destination.
Figure 6.5 shows the R-type machine instruction format. The 32-bit
instruction has six fields: op, rs, rt, rd, shamt, and funct. Each
field is five or six bits, as indicated.
The operation the instruction performs is encoded in the two fields
highlighted in blue: op (also called opcode or operation code) and funct
(also called the function). All R-type instructions have an opcode of 0.
The specific R-type operation is determined by the funct field.
For example, the opcode and funct fields for the add instruction are
0 (0000002) and 32 (1000002), respectively. Similarly, the sub instruction has an opcode and funct field of 0 and 34.
The operands are encoded in the three fields: rs, rt, and rd. The first
two registers, rs and rt, are the source registers; rd is the destination
6.3 Machine Language 299
op rs rt rd shamt funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
R-type Figure 6.5 R-type machine
instruction format

register. The fields contain the register numbers that were given in
Table 6.1. For example, $s0 is register 16.
The fifth field, shamt, is used only in shift operations. In those
instructions, the binary value stored in the 5-bit shamt field indicates the
amount to shift. For all other R-type instructions, shamt is 0.
Figure 6.6 shows the machine code for the R-type instructions add
and sub. Notice that the destination is the first register in an assembly
language instruction, but it is the third register field (rd) in the machine
language instruction. For example, the assembly instruction add $s0,
$s1, $s2 has rs  $s1 (17), rt  $s2 (18), and rd  $s0 (16).
Tables B.1 and B.2 in Appendix B define the opcode values for all
MIPS instructions and the funct field values for R-type instructions.
Example 6.3 TRANSLATING ASSEMBLY LANGUAGE TO MACHINE
LANGUAGE
Translate the following assembly language statement into machine language.
add $t0, $s4, $s5
Solution: According to Table 6.1, $t0, $s4, and $s5 are registers 8, 20, and
21. According to Tables B.1 and B.2, add has an opcode of 0 and a funct
code of 32. Thus, the fields and machine code are given in Figure 6.7. The
easiest way to write the machine language in hexadecimal is to first write it in
binary, then look at consecutive groups of four bits, which correspond to
hexadecimal digits (indicated in blue). Hence, the machine language instruction is 0x02954020.
300 CHAPTER SIX Architecture
000000 10001 10010 10000 00000 100000
000000
add $s0, $s1, $s2
sub $t0, $t3, $t5
Assembly Code Machine Code
0 17 18 16 0 32
0 11 13 8 0 34
Field Values
(0x02328020)
(0x016D4022)
op rs rt rd shamt funct op rs rt rd shamt funct
01011 01101 01000 00000 100010
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits 6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
Figure 6.6 Machine code for R-type instructions
add $t0, $s4, $s5 000000 10100 10101 01000 00000 100000
Assembly Code Machine Code
0 20 21 8 0 32
Field Values
(0x 02954020)
0 4 2 95 0 2 0
op rs rt rd shamt funct op rs rt rd shamt funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
Figure 6.7 Machine code for the R-type instruction of Example 6.3
rs is short for “register
source.” rt comes after rs
alphabetically and usually
indicates the second register
source.
Cha
6.3.2 I-Type Instructions
The name I-type is short for immediate-type. I-type instructions
use two register operands and one immediate operand. Figure 6.8
shows the I-type machine instruction format. The 32-bit instruction
has four fields: op, rs, rt, and imm. The first three fields, op, rs, and
rt, are like those of R-type instructions. The imm field holds the 16-bit
immediate.
The operation is determined solely by the opcode, highlighted in
blue. The operands are specified in the three fields, rs, rt, and imm. rs
and imm are always used as source operands. rt is used as a destination
for some instructions (such as addi and lw) but as another source for
others (such as sw).
Figure 6.9 shows several examples of encoding I-type instructions.
Recall that negative immediate values are represented using 16-bit
two’s complement notation. rt is listed first in the assembly language
instruction when it is used as a destination, but it is the second register
field in the machine language instruction.
6.3 Machine Language 301
op rs rt imm
6 bits 5 bits 5 bits 16 bits
I-type Figure 6.8 I-type instruction
format
(0x22300005)
(0x2268FFF4)
(0x8C0A0020)
(0xAD310004)
001000 10001 10000 0000 0000 0000 0101
Assembly Code Machine Code
8 17 16
Field Values
op rs rt imm op rs rt imm
addi $s0, $s1, 5
addi $t0, $s3, –12
lw $t2, 32($0)
sw $s1, 4($t1)
8 19 8 –12
35 0 10 32
43 9 17
001000 10011 01000 1111 1111 1111 0100
100011 00000 01010 0000 0000 0010 0000
101011 01001 10001 0000 0000 0000 0100
6 bits 5 bits 5 bits 16 bits 6 bits 5 bits 5 bits 16 bits
4
5
Figure 6.9 Machine code for I-type instructions
Example 6.4 TRANSLATING I-TYPE ASSEMBLY INSTRUCTIONS INTO
MACHINE CODE
Translate the following I-type instruction into machine code.
lw $s3, 24($s4)
Solution: According to Table 6.1, $s3 and $s4 are registers 19 and 20, respectively.
Table B.1 indicates that lw has an opcode of 35. rs specifies the base address, $s4,
and rt specifies the destination register, $s3. The immediate, imm, encodes the
16-bit offset, 24. Thus, the fields and machine code are given in Figure 6.10.

I-type instructions have a 16-bit immediate field, but the immediates
are used in 32-bit operations. For example, lw adds a 16-bit offset to a
32-bit base register. What should go in the upper half of the 32 bits? For
positive immediates, the upper half should be all 0’s, but for negative
immediates, the upper half should be all 1’s. Recall from Section 1.4.6
that this is called sign extension. An N-bit two’s complement number is
sign-extended to an M-bit number (M  N) by copying the sign bit
(most significant bit) of the N-bit number into all of the upper bits of the
M-bit number. Sign-extending a two’s complement number does not
change its value.
Most MIPS instructions sign-extend the immediate. For example,
addi, lw, and sw do sign extension to support both positive and negative immediates. An exception to this rule is that logical operations
(andi, ori, xori) place 0’s in the upper half; this is called zero extension rather than sign extension. Logical operations are discussed further
in Section 6.4.1.
6.3.3 J-type Instructions
The name J-type is short for jump-type. This format is used only with
jump instructions (see Section 6.4.2). This instruction format uses a single 26-bit address operand, addr, as shown in Figure 6.11. Like other
formats, J-type instructions begin with a 6-bit opcode. The remaining
bits are used to specify an address, addr. Further discussion and
machine code examples of J-type instructions are given in Sections 6.4.2
and 6.5.
6.3.4 Interpreting Machine Language Code
To interpret machine language, one must decipher the fields of each
32-bit instruction word. Different instructions use different formats, but
all formats start with a 6-bit opcode field. Thus, the best place to begin is
to look at the opcode. If it is 0, the instruction is R-type; otherwise it is
I-type or J-type.
302 CHAPTER SIX Architecture
100011 10100 10011 1111 1111 1110 1000
op rs rt imm op rs rt imm
lw $s3, –24($s4)
Assembly Code Machine Code
 35 20 19 –24
Field Values
(0x8E93FFE8)
6 bits 5 bits 5 bits 16 bits 8 E 93 FF E8
Figure 6.10 Machine code for the I-type instruction
op addr
6 bits 26 bits
J-type Figure 6.11 J-type instruction
format

Example 6.5 TRANSLATING MACHINE LANGUAGE TO ASSEMBLY
LANGUAGE
Translate the following machine language code into assembly language.
0x2237FFF1
0x02F34022
Solution: First, we represent each instruction in binary and look at the six most
significant bits to find the opcode for each instruction, as shown in Figure 6.12.
The opcode determines how to interpret the rest of the bits. The opcodes are
0010002 (810) and 0000002 (010), indicating an addi and R-type instruction,
respectively. The funct field of the R-type instruction is 1000102 (3410), indicating that it is a sub instruction. Figure 6.12 shows the assembly code equivalent
of the two machine instructions.
6.3 Machine Language 303
addi $s7, $s1, –15
Machine Code Assembly Code
 8 17 23 –15
Field Values
(0x2237FFF1)
op rs rt imm op rs rt imm
2 2 37 FF F1
(0x02F34022) 0 23 19 8 0 34 sub $t0, $s7, $s3
op
0 2 F340 22
001000
000000 10111 10011 01000 00000 100010
10001 10111 1111 1111 1111 0001
op rs rt rd shamt funct rs rt rd shamt funct
Figure 6.12 Machine code to assembly code translation
6.3.5 The Power of the Stored Program
A program written in machine language is a series of 32-bit numbers representing the instructions. Like other binary numbers, these instructions
can be stored in memory. This is called the stored program concept, and
it is a key reason why computers are so powerful. Running a different
program does not require large amounts of time and effort to reconfigure
or rewire hardware; it only requires writing the new program to memory.
Instead of dedicated hardware, the stored program offers general purpose
computing. In this way, a computer can execute applications ranging
from a calculator to a word processor to a video player simply by changing the stored program.
Instructions in a stored program are retrieved, or fetched, from
memory and executed by the processor. Even large, complex programs
are simplified to a series of memory reads and instruction executions.
Figure 6.13 shows how machine instructions are stored in memory.
In MIPS programs, the instructions are normally stored starting at
address 0x00400000. Remember that MIPS memory is byte addressable,
so 32-bit (4-byte) instruction addresses advance by 4 bytes, not 1.

To run or execute the stored program, the processor fetches the
instructions from memory sequentially. The fetched instructions are then
decoded and executed by the digital hardware. The address of the current
instruction is kept in a 32-bit register called the program counter (PC).
The PC is separate from the 32 registers shown previously in Table 6.1.
To execute the code in Figure 6.13, the operating system sets the PC
to address 0x00400000. The processor reads the instruction at that
memory address and executes the instruction, 0x8C0A0020. The processor then increments the PC by 4, to 0x00400004, fetches and executes
that instruction, and repeats.
The architectural state of a microprocessor holds the state of a program. For MIPS, the architectural state consists of the register file and
PC. If the operating system saves the architectural state at some point in
the program, it can interrupt the program, do something else, then
restore the state such that the program continues properly, unaware that
it was ever interrupted. The architectural state is also of great importance when we build a microprocessor in Chapter 7.
6.4 PROGRAMMING
Software languages such as C or Java are called high-level programming
languages, because they are written at a more abstract level than assembly
language. Many high-level languages use common software constructs such
as arithmetic and logical operations, if/else statements, for and while
loops, array indexing, and procedure calls. In this section, we explore how
to translate these high-level constructs into MIPS assembly code.
6.4.1 Arithmetic/Logical Instructions
The MIPS architecture defines a variety of arithmetic and logical
instructions. We introduce these instructions briefly here, because they
are necessary to implement higher-level constructs.
304 CHAPTER SIX Architecture
Figure 6.13 Stored program
addi $t0, $s3, –12
Assembly Code Machine Code
lw $t2, 32($0)
add $s0, $s1, $s2
sub $t0, $t3, $t5
0x8C0A0020
0x02328020
0x2268FFF4
0x016D4022
Address Instructions
0040000C 01 6 D4 0 22
226 8FFF4
023 28020
8C0 A0 0 20
00400008
00400004
00400000
Stored Program
Main Memory
PC
Ada Lovelace, 1815–1852.
Wrote the first computer program. It calculated the Bernoulli
numbers using Charles
Babbage’s Analytical Engine.
She was the only legitimate
child of the poet Lord Byron.

Logical Instructions
MIPS logical operations include and, or, xor, and nor. These R-type
instructions operate bit-by-bit on two source registers and write the
result to the destination register. Figure 6.14 shows examples of these
operations on the two source values 0xFFFF0000 and 0x46A1F0B7.
The figure shows the values stored in the destination register, rd, after
the instruction executes.
The and instruction is useful for masking bits (i.e., forcing
unwanted bits to 0). For example, in Figure 6.14, 0xFFFF0000 AND
0x46A1F0B7  0x46A10000. The and instruction masks off the bottom two bytes and places the unmasked top two bytes of $s2, 0x46A1,
in $s3. Any subset of register bits can be masked.
The or instruction is useful for combining bits from two registers.
For example, 0x347A0000 OR 0x000072FC  0x347A72FC, a combination of the two values.
MIPS does not provide a NOT instruction, but A NOR $0  NOT
A, so the NOR instruction can substitute.
Logical operations can also operate on immediates. These I-type
instructions are andi, ori, and xori. nori is not provided, because the
same functionality can be easily implemented using the other instructions, as will be explored in Exercise 6.11. Figure 6.15 shows examples
of the andi, ori, and xori instructions. The figure gives the values of
6.4 Programming 305
$s1 1111 1111 1111 1111 0000 0000 0000 0000
$s2 0100 0110 1010 0001 1111 0000 1011 0111
$s3 0100 0110 1010 0001 0000 0000 0000 0000
$s4 1111 1111 1111 1111 1111 0000 1011 0111
$s5 1011 1001 0101 1110 1111 0000 1011 0111
$s6 0000 0000 0000 0000 0000 1111 0100 1000
Source Registers
Assembly Code Result
and $s3, $s1, $s2
or $s4, $s1, $s2
xor $s5, $s1, $s2
nor $s6, $s1, $s2
Figure 6.14 Logical operations
$s1 0000 0000 0000 0000 0000 0000 1111 1111
Assembly Code
imm 0000 0000 0000 0000 1111 1010 0011 0100
$s2 0000 0000 0000 0000 0000 0000 0011 0100
$s3 0000 0000 0000 0000 1111 1010 1111 1111
$s4 0000 0000 0000 0000 1111 1010 1100 1011
andi $s2, $s1, 0xFA34
Source Values
Result
ori $s3, $s1, 0xFA34
xori $s4, $s1, 0xFA34
zero-extended Figure 6.15 Logical operations
with immediates
Cha
the source register and immediate, and the value of the destination register, rt, after the instruction executes. Because these instructions operate
on a 32-bit value from a register and a 16-bit immediate, they first zeroextend the immediate to 32 bits.
Shift Instructions
Shift instructions shift the value in a register left or right by up to
31 bits. Shift operations multiply or divide by powers of two. MIPS shift
operations are sll (shift left logical), srl (shift right logical), and sra
(shift right arithmetic).
As discussed in Section 5.2.5, left shifts always fill the least significant bits with 0’s. However, right shifts can be either logical (0’s shift
into the most significant bits) or arithmetic (the sign bit shifts into the
most significant bits). Figure 6.16 shows the machine code for the
R-type instructions sll, srl, and sra. rt (i.e., $s1) holds the 32-bit
value to be shifted, and shamt gives the amount by which to shift (4).
The shifted result is placed in rd.
Figure 6.17 shows the register values for the shift instructions sll,
srl, and sra. Shifting a value left by N is equivalent to multiplying it by
2N. Likewise, arithmetically shifting a value right by N is equivalent to
dividing it by 2N, as discussed in Section 5.2.5.
MIPS also has variable-shift instructions: sllv (shift left logical variable), srlv (shift right logical variable), and srav (shift right arithmetic
variable). Figure 6.18 shows the machine code for these instructions.
306 CHAPTER SIX Architecture
sll $t0, $s1, 4
srl $s2, $s1, 4
sra $s3, $s1, 4
000000 00000 10001 01000 00100 000000
op rs rt rd shamt funct op rs rt rd shamt funct
Assembly Code Machine Code
 0 0 17 8 4 0
Field Values
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits 6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
 0 0 17 18 4 2
 0 0 17 19 4 3
000000 00000 10001 10010 00100 000010
000000 00000 10001 10011 00100 000011
(0x00114100)
(0x00119102)
(0x00119903)
Figure 6.16 Shift instruction machine code
$s1 1111 0011 0000 0000 0000 0010 1010 1000
Assembly Code
shamt 00100
$t0 0011 0000 0000 0000 0010 1010 1000 0000
$s2 0000 1111 0011 0000 0000 0000 0010 1010
$s3
sll $t0, $s1, 4
Source Values
Result
srl $s2, $s1, 4
sra $s3, $s1, 4 1111 1111 0011 0000 0000 0000 0010 1010
Figure 6.17 Shift operations

rt (i.e., $s1) holds the value to be shifted, and the five least significant
bits of rs (i.e., $s2) give the amount to shift. The shifted result is placed
in rd, as before. The shamt field is ignored and should be all 0’s. Figure
6.19 shows register values for each type of variable-shift instruction.
Generating Constants
The addi instruction is helpful for assigning 16-bit constants, as shown
in Code Example 6.10.
6.4 Programming 307
sllv $s3, $s1, $s2
srlv $s4, $s1, $s2
srav $s5, $s1, $s2
op rs rt rd shamt funct
Machine Code Assembly Code Field Values
op rs rt rd shamt funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits 6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
(0x02519804)
(0x0251A006)
(0x0251A807)
401917180
602017180
702117180
000000 10010 10001 10011 00000 000100
000000 10010 10001 10100 00000 000110
000000 10010 10001 10101 00000 000111
Figure 6.18 Variable-shift instruction machine code
$s1
$s3 0000 0000
$s4 0000 0000
$s5 1111 1111
Assembly Code
sllv $s3, $s1, $s2
srlv $s4, $s1, $s2
srav $s5, $s1, $s2
Source Values
Result
$s2
1111 0011 0000 0100 0000 0010 1010 1000
0000 0000 0000 0000 0000 0000 0000 1000
0000 0100 0000 00101010 1000
1111 0011 0000 0100 0000 0010
1111 0011 0000 0100 0000 0010
Figure 6.19 Variable-shift
operations
High-Level Code
int a  0x4f3c;
MIPS Assembly code
# $s0  a
addi $s0, $0, 0x4f3c # a  0x4f3c
Code Example 6.10 16-BIT CONSTANT
High-Level Code
int a  0x6d5e4f3c;
MIPS Assembly Code
# $s0  a
lui $s0, 0x6d5e # a  0x6d5e0000
ori $s0, $s0, 0x4f3c # a  0x6d5e4f3c
Code Example 6.11 32-BIT CONSTANT
Chapter
To assign 32-bit constants, use a load upper immediate instruction
(lui) followed by an or immediate (ori) instruction, as shown in Code
Example 6.11. lui loads a 16-bit immediate into the upper half of a register and sets the lower half to 0. As mentioned earlier, ori merges a 16-
bit immediate into the lower half.
Multiplication and Division Instructions*
Multiplication and division are somewhat different from other arithmetic operations. Multiplying two 32-bit numbers produces a 64-bit
product. Dividing two 32-bit numbers produces a 32-bit quotient and a
32-bit remainder.
The MIPS architecture has two special-purpose registers, hi and lo,
which are used to hold the results of multiplication and division. mult
$s0, $s1 multiplies the values in $s0 and $s1. The 32 most significant
bits are placed in hi and the 32 least significant bits are placed in lo.
Similarly, div $s0, $s1 computes $s0/$s1. The quotient is placed in
lo and the remainder is placed in hi.
6.4.2 Branching
An advantage of a computer over a calculator is its ability to make decisions. A computer performs different tasks depending on the input. For
example, if/else statements, case statements, while loops, and for
loops all conditionally execute code depending on some test.
To sequentially execute instructions, the program counter increments by 4 after each instruction. Branch instructions modify the program counter to skip over sections of code or to go back to repeat
previous code. Conditional branch instructions perform a test and
branch only if the test is TRUE. Unconditional branch instructions,
called jumps, always branch.
Conditional Branches
The MIPS instruction set has two conditional branch instructions: branch
if equal (beq) and branch if not equal (bne). beq branches when the values in two registers are equal, and bne branches when they are not equal.
Code Example 6.12 illustrates the use of beq. Note that branches are
written as beq $rs, $rt, imm, where $rs is the first source register. This
order is reversed from most I-type instructions.
When the program in Code Example 6.12 reaches the branch if
equal instruction (beq), the value in $s0 is equal to the value in $s1, so
the branch is taken. That is, the next instruction executed is the add
instruction just after the label called target. The two instructions
directly after the branch and before the label are not executed.
Assembly code uses labels to indicate instruction locations in the
program. When the assembly code is translated into machine code, these
308 CHAPTER SIX Architecture
The int data type in C refers
to a word of data representing
a two’s complement integer.
MIPS uses 32-bit words, so an
int represents a number in the
range [231, 2311].
hi and lo are not among the
usual 32 MIPS registers, so
special instructions are needed
to access them. mfhi $s2 (move
from hi) copies the value in hi
to $s2. mflo $s3 (move from
lo) copies the value in lo to
$s3. hi and lo are technically
part of the architectural state;
however, we generally ignore
these registers in this book.

6.4 Programming 309
MIPS Assembly Code
addi $s0, $0, 4 # $s0  0  4  4
addi $s1, $0, 1 # $s1  0  1  1
sll $s1, $s1, 2 # $s1  1  2  4
beq $s0, $s1, target # $s0  $s1, so branch is taken
addi $s1, $s1, 1 # not executed
sub $s1, $s1, $s0 # not executed
target:
add $s1, $s1, $s0 # $s1  4  4  8
Code Example 6.12 CONDITIONAL BRANCHING USING beq
MIPS Assembly Code
addi $s0, $0, 4 # $s0  0  4  4
addi $s1, $0, 1 # $s1  0  1  1
s11 $s1, $s1, 2 # $s1  1  2  4
bne $s0, $s1, target # $s0  $s1, so branch is not taken
addi $s1, $s1, 1 # $s1  4  1  5
sub $s1, $s1, $s0 # $s1  5  4  1
target:
add $s1, $s1, $s0 # $s1  1  4  5
Code Example 6.13 CONDITIONAL BRANCHING USING bne
labels are translated into instruction addresses (see Section 6.5). MIPS
assembly labels are followed by a (:) and cannot use reserved words,
such as instruction mnemonics. Most programmers indent their instructions but not the labels, to help make labels stand out.
Code Example 6.13 shows an example using the branch if not equal
instruction (bne). In this case, the branch is not taken because $s0 is
equal to $s1, and the code continues to execute directly after the bne
instruction. All instructions in this code snippet are executed.
Jump
A program can unconditionally branch, or jump, using the three types of
jump instructions: jump (j), jump and link (jal), and jump register (jr).
Jump (j) jumps directly to the instruction at the specified label. Jump
and link (jal) is similar to j but is used by procedures to save a return
address, as will be discussed in Section 6.4.6. Jump register (jr) jumps to
the address held in a register. Code Example 6.14 shows the use of the
jump instruction (j).
After the j target instruction, the program in Code Example 6.14 unconditionally continues executing the add instruction at the label target.
All of the instructions between the jump and the label are skipped.
j and jal are J-type instructions. jr is an R-type instruction that uses only the rs
operand.
Chapter 06.qxd 1/31/07 8:21 PM 
310 CHAPTER SIX Architecture
MIPS Assembly Code
addi $s0, $0, 4 # $s0  4
addi $s1, $0, 1 # $s1  1
j target # jump to target
addi $s1, $s1, 1 # not executed
sub $s1, $s1, $s0 # not executed
target:
add $s1, $s1, $s0 # $s1  1  4  5
Code Example 6.14 UNCONDITIONAL BRANCHING USING j
MIPS Assembly Code
0x00002000 addi $s0, $0, 0x2010 # $s0  0x2010
0x00002004 jr $s0 # jump to 0x00002010
0x00002008 addi $s1, $0, 1 # not executed
0x0000200c sra $s1, $s1, 2 # not executed
0x00002010 lw $s3, 44 ($s1) # executed after jr instruction
Code Example 6.15 UNCONDITIONAL BRANCHING USING jr
Code Example 6.15 shows the use of the jump register instruction
(jr). Instruction addresses are given to the left of each instruction. jr
$s0 jumps to the address held in $s0, 0x00002010.
6.4.3 Conditional Statements
if statements, if/else statements, and case statements are conditional
statements commonly used by high-level languages. They each conditionally execute a block of code consisting of one or more instructions.
This section shows how to translate these high-level constructs into
MIPS assembly language.
If Statements
An if statement executes a block of code, the if block, only when a
condition is met. Code Example 6.16 shows how to translate an if
statement into MIPS assembly code.
High-Level Code
if (i  j)
f  g  h;
f  f  i;
MIPS Assembly Code
# $s0  f, $s1  g, $s2  h, $s3  i, $s4  j
bne $s3, $s4, L1 # if i !  j, skip if block
add $s0, $s1, $s2 # if block: f  g  h
L1:
sub $s0, $s0, $s3 # f  f  i
Code Example 6.16 if STATEMENT
Chapter 06.qxd 1/31/
The assembly code for the if statement tests the opposite condition of the one in the high-level code. In Code Example 6.16, the
high-level code tests for i  j, and the assembly code tests for
i ! j. The bne instruction branches (skips the if block) when i !
j. Otherwise, i  j, the branch is not taken, and the if block is
executed as desired.
If/Else Statements
if/else statements execute one of two blocks of code depending on a
condition. When the condition in the if statement is met, the if block is
executed. Otherwise, the else block is executed. Code Example 6.17
shows an example if/else statement.
Like if statements, if/else assembly code tests the opposite condition of the one in the high-level code. For example, in Code Example 6.17,
the high-level code tests for i  j. The assembly code tests for the opposite condition (i ! j). If that opposite condition is TRUE, bne skips the
if block and executes the else block. Otherwise, the if block executes
and finishes with a jump instruction (j) to jump past the else block.
6.4 Programming 311
High-Level Code
if (i  j)
f  g  h;
else
f  f  i;
MIPS Assembly Code
# $s0  f, $s1  g, $s2  h, $s3  i, $s4  j
bne $s3, $s4, else # if i !  j, branch to else
add $s0, $s1, $s2 # if block: f  g  h
j L2 # skip past the else block
else:
sub $s0, $s0, $s3 # else block: f  f  i
L2:
Code Example 6.17 if/else STATEMENT
Switch/Case Statements*
switch/case statements execute one of several blocks of code depending
on the conditions. If no conditions are met, the default block is executed.
A case statement is equivalent to a series of nested if/else statements.
Code Example 6.18 shows two high-level code snippets with the same
functionality: they calculate the fee for an ATM (automatic teller machine)
withdrawal of $20, $50, or $100, as defined by amount. The MIPS assembly implementation is the same for both high-level code snippets.
6.4.4 Getting Loopy
Loops repeatedly execute a block of code depending on a condition.
for loops and while loops are common loop constructs used by highlevel languages. This section shows how to translate them into MIPS
assembly language.
Chapter 06.qxd 1/31/07 
312 CHAPTER SIX Architecture
High-Level Code
switch (amount) {
case 20: fee  2; break;
case 50: fee  3; break;
case 100: fee  5; break;
default: fee  0;
}
// equivalent function using if/else statements
if (amount  20) fee  2;
else if (amount  50) fee  3;
else if (amount  100) fee  5;
else fee  0;
MIPS Assembly Code
# $s0  amount, $s1  fee
case20:
addi $t0, $0, 20 # $t0  20
bne $s0, $t0, case50 # i  20? if not,
# skip to case50
addi $s1, $0, 2 # if so, fee  2
j done # and break out of case
case50:
addi $t0, $0, 50 # $t0  50
bne $s0, $t0, case100 # i  50? if not,
# skip to case100
addi $s1, $0, 3 # if so, fee  3
j done # and break out of case
case100:
addi $t0, $0, 100 # $t0  100
bne $s0, $t0, default # i  100? if not,
# skip to default
addi $s1, $0, 5 # if so, fee  5
j done # and break out of case
default:
add $s1, $0, $0 # charge  0
done:
Code Example 6.18 switch/case STATEMENT
While Loops
while loops repeatedly execute a block of code until a condition is not
met. The while loop in Code Example 6.19 determines the value of x
such that 2x  128. It executes seven times, until pow  128.
Like if/else statements, the assembly code for while loops tests
the opposite condition of the one given in the high-level code. If that
opposite condition is TRUE, the while loop is finished.
High-Level Code
int pow  1;
int x  0;
while (pow ! 128)
{
pow  pow * 2;
x  x  1;
}
MIPS Assembly Code
# $s0  pow, $s1  x
addi $s0, $0, 1 # pow  1
addi $s1, $0, 0 # x  0
addi $t0, $0, 128 # t0  128 for comparison
while:
beq $s0, $t0, done # if pow  128, exit while
sll $s0, $s0, 1 # pow  pow * 2
addi $s1, $s1, 1 # x  x  1
j while
done:
Code Example 6.19 while LOOP
Chapter 06.qxd 1/31/07 8:21 PM Page 312
In Code Example 6.19, the while loop compares pow to 128 and
exits the loop if it is equal. Otherwise it doubles pow (using a left shift),
increments x, and jumps back to the start of the while loop.
For Loops
for loops, like while loops, repeatedly execute a block of code until a
condition is not met. However, for loops add support for a loop variable, which typically keeps track of the number of loop executions.
A general format of the for loop is
for (initialization; condition; loop operation)
The initialization code executes before the for loop begins. The
condition is tested at the beginning of each loop. If the condition
is not met, the loop exits. The loop operation executes at the end of
each loop.
Code Example 6.20 adds the numbers from 0 to 9. The loop
variable, i, is initialized to 0 and is incremented at the end of each loop
iteration. At the beginning of each iteration, the for loop executes only
when i is not equal to 10. Otherwise, the loop is finished. In this case,
the for loop executes 10 times. for loops can be implemented using
a while loop, but the for loop is often convenient.
Magnitude Comparison
So far, the examples have used beq and bne to perform equality or
inequality comparisons and branches. MIPS provides the set less than
instruction, slt, for magnitude comparison. slt sets rd to 1 when rs 
rt. Otherwise, rd is 0.
6.4 Programming 313
High-Level Code
int sum  0;
for (i  0; i !  10; i  i  1) {
sum  sum  i;
}
// equivalent to the following while loop
int sum  0;
int i  0;
while (i ! 10) {
sum  sum  i;
i  i  1;
}
MIPS Assembly Code
# $s0  i, $s1  sum
add $s1, $0, $0 # sum  0
addi $s0, $0, 0 # i  0
addi $t0, $0, 10 # $t0  10
for:
beq $s0, $t0, done # if i  10, branch to done
add $s1, $s1, $s0 # sum  sum  i
addi $s0, $s0, 1 # increment i
j for
done:
Code Example 6.20 for LOOP
Chapter 06.qxd 1/31/07 
Example 6.6 LOOPS USING slt
The following high-level code adds the powers of 2 from 1 to 100.
Translate it into assembly language.
// high-level code
int sum  0;
for (i  1; i  101; i  i * 2)
sum  sum  i;
Solution: The assembly language code uses the set less than (slt) instruction to
perform the less than comparison in the for loop.
# MIPS assembly code
# $s0  i, $s1  sum
addi $s1, $0, 0 # sum  0
addi $s0, $0, 1 # i  1
addi $t0, $0, 101 # $t0  101
loop:
slt $t1, $s0, $t0 # if (i  101) $t1  1, else $t1  0
beq $t1, $0, done # if $t1  0 (i  101), branch to done
add $s1, $s1, $s0 # sum  sum  i
sll $s0, $s0, 1 # i  i * 2
j loop
done:
Exercise 6.12 explores how to use slt for other magnitude comparisons including greater than, greater than or equal, and less than or equal.
6.4.5 Arrays
Arrays are useful for accessing large amounts of similar data. An array is
organized as sequential data addresses in memory. Each array element is
identified by a number called its index. The number of elements in the
array is called the size of the array. This section shows how to access
array elements in memory.
Array Indexing
Figure 6.20 shows an array of five integers stored in memory. The index
ranges from 0 to 4. In this case, the array is stored in a processor’s main
memory starting at base address 0x10007000. The base address gives
the address of the first array element, array[0].
Code Example 6.21 multiplies the first two elements in array by 8
and stores them back in the array.
The first step in accessing an array element is to load the base address
of the array into a register. Code Example 6.21 loads the base address
314 CHAPTER SIX Architecture
Chapter 06.qxd 1/3
into $s0. Recall that the load upper immediate (lui) and or immediate
(ori) instructions can be used to load a 32-bit constant into a register.
Code Example 6.21 also illustrates why lw takes a base address and an
offset. The base address points to the start of the array. The offset can be
used to access subsequent elements of the array. For example, array[1] is
stored at memory address 0x10007004 (one word or four bytes after
array[0]), so it is accessed at an offset of 4 past the base address.
You might have noticed that the code for manipulating each of the
two array elements in Code Example 6.21 is essentially the same except
for the index. Duplicating the code is not a problem when accessing two
array elements, but it would become terribly inefficient for accessing all
of the elements in a large array. Code Example 6.22 uses a for loop to
multiply by 8 all of the elements of a 1000-element array stored at a
base address of 0x23B8F000.
Figure 6.21 shows the 1000-element array in memory. The index
into the array is now a variable (i) rather than a constant, so we cannot
take advantage of the immediate offset in lw. Instead, we compute the
address of the ith element and store it in $t0. Remember that each array
element is a word but that memory is byte addressed, so the offset from
6.4 Programming 315
array[4]
array[3]
array[2]
array[1]
0x10007000 array[0]
0x10007004
0x10007008
0x1000700C
0x10007010
Main Memory
Address Data
Figure 6.20 Five-entry
array with base address
of 0x10007000
High-Level Code
int array [5];
array[0]  array[0] * 8;
array[1]  array[1] * 8;
MIPS Assembly Code
# $s0  base address of array
lui $s0, 0x1000 # $s0  0x10000000
ori $s0, $s0, 0x7000 # $s0  0x10007000
lw $t1, 0($s0) # $t1  array[0]
sll $t1, $t1, 3 # $t1  $t1  3  $t1 * 8
sw $t1, 0($s0) # array[0]  $t1
lw $t1, 4($s0) # $t1  array[1]
sll $t1, $t1, 3 # $t1  $t1  3  $t1 * 8
sw $t1, 4($s0) # array[1]  $t1
Code Example 6.21 ACCESSING ARRAYS
Chapter 06.qx
the base address is i * 4. Shifting left by 2 is a convenient way to multiply by 4 in MIPS assembly language. This example readily extends to an
array of any size.
Bytes and Characters
Numbers in the range [128, 127] can be stored in a single byte rather
than an entire word. Because there are much fewer than 256 characters on
an English language keyboard, English characters are often represented by
bytes. The C language uses the type char to represent a byte or character.
Early computers lacked a standard mapping between bytes and
English characters, so exchanging text between computers was difficult.
In 1963, the American Standards Association published the American
Standard Code for Information Interchange (ASCII), which assigns each
text character a unique byte value. Table 6.2 shows these character
encodings for printable characters. The ASCII values are given in hexadecimal. Lower-case and upper-case letters differ by 0x20 (32).
316 CHAPTER SIX Architecture
High-Level Code
int i;
int array[1000];
for (i0; i  1000; i  i  1) {
array[i]  array[i] * 8;
}
MIPS Assembly Code
# $s0  array base address, $s1  i
# initialization code
lui $s0, 0x23B8 # $s0  0x23B80000
ori $s0, $s0, 0xF000 # $s0  0x23B8F000
addi $s1, $0 # i  0
addi $t2, $0, 1000 # $t2  1000
loop:
slt $t0, $s1, $t2 # i  1000?
beq $t0, $0, done # if not then done
sll $t0, $s1, 2 # $t0  i * 4 (byte offset)
add $t0, $t0, $s0 # address of array[i]
lw $t1, 0($t0) # $t1  array[i]
sll $t1, $t1, 3 # $t1  array[i] * 8
sw $t1, 0($t0) # array[i]  array[i] * 8
addi $s1, $s1, 1 # i  i  1
j loop # repeat
done:
Code Example 6.22 ACCESSING ARRAYS USING A for LOOP
Figure 6.21 Memory holding
array[1000] starting at base
address 0x23B8F000
Other program languages,
such as Java, use different
character encodings, most
notably Unicode. Unicode
uses 16 bits to represent each
character, so it supports
accents, umlauts, and Asian
languages. For more information, see www.unicode.org.
23B8FF9C array[999]
23B8FF98
23B8F004
23B8F000
array[998]
array[1]
array[0]
Main Memory
Address Data
Chapter 06.qxd 1
MIPS provides load byte and store byte instructions to manipulate
bytes or characters of data: load byte unsigned (lbu), load byte (lb), and
store byte (sb). All three are illustrated in Figure 6.22.
6.4 Programming 317
ASCII codes developed from
earlier forms of character
encoding. Beginning in 1838,
telegraph machines used
Morse code, a series of dots (.)
and dashes (), to represent
characters. For example, the
letters A, B, C, and D were
represented as .,  ...,
.., and .., respectively.
The number of dots and
dashes varied with each letter.
For efficiency, common letters
used shorter codes.
In 1874, Jean-MauriceEmile Baudot invented a 5-bit
code called the Baudot code.
For example, A, B, C, and D,
were represented as 00011,
11001, 01110, and 01001.
However, the 32 possible
encodings of this 5-bit code
were not sufficient for all the
English characters. But 8-bit
encoding was. Thus, as electronic communication became
prevalent, 8-bit ASCII encoding emerged as the standard.
Table 6.2 ASCII encodings
# Char # Char # Char # Char # Char # Char
20 space 30 0 40 @ 50 P 60 ′ 70 p
21 ! 31 1 41 A 51 Q 61 a 71 q
22 ″ 32 2 42 B 52 R 62 b 72 r
23 # 33 3 43 C 53 S 63 c 73 s
24 $ 34 4 44 D 54 T 64 d 74 t
25 % 35 5 45 E 55 U 65 e 75 u
26 & 36 6 46 F 56 V 66 f 76 v
27 ′ 37 7 47 G 57 W 67 g 77 w
28 ( 38 8 48 H 58 X 68 h 78 x
29 ) 39 9 49 I 59 Y 69 i 79 y
2A * 3A : 4A J 5A Z 6A j 7A z
2B  3B ; 4B K 5B [ 6B k 7B {
2C , 3C  4C L 5C \ 6C l 7C |
2D – 3D  4D M 5D ] 6D m 7D }
2E . 3E  4E N 5E  6E n 7E ~
2F / 3F ? 4F O 5F _ 6F o
Byte Address
Data 428CF7 03
3210
$s1 00 8C lbu $s1, 2($0)
Little-Endian Memory
0000
Registers
$s2 FFFF FF 8C lb $s2, 2($0)
$s3 XX XX XX 9B sb $s3, 3($0)
Figure 6.22 Instructions for
loading and storing bytes
Cha
Load byte unsigned (lbu) zero-extends the byte, and load byte (lb)
sign-extends the byte to fill the entire 32-bit register. Store byte (sb)
stores the least significant byte of the 32-bit register into the specified
byte address in memory. In Figure 6.22, lbu loads the byte at memory
address 2 into the least significant byte of $s1 and fills the remaining
register bits with 0. lb loads the sign-extended byte at memory address 2
into $s2. sb stores the least significant byte of $s3 into memory byte 3;
it replaces 0xF7 with 0x9B. The more significant bytes of $s3 are
ignored.
Example 6.7 USING lb AND sb TO ACCESS A CHARACTER ARRAY
The following high-level code converts a ten-entry array of characters from
lower-case to upper-case by subtracting 32 from each array entry. Translate it
into MIPS assembly language. Remember that the address difference between
array elements is now 1 byte, not 4 bytes. Assume that $s0 already holds the
base address of chararray.
// high-level code
char chararray[10];
int i;
for (i  0; i ! 10; i  i  1)
chararray[i]  chararray[i]  32;
Solution:
# MIPS assembly code
# $s0  base address of chararray, $s1  i
addi $s1, $0, 0 # i  0
addi $t0, $0, 10 # $t0  10
loop: beq $t0, $s1, done # if i  10, exit loop
add $t1, $s1, $s0 # $t1  address of chararray[i]
lb $t2, 0($t1) # $t2  array[i]
addi $t2, $t2, 32 # convert to upper case: $t1  $t1  32
sb $t2, 0($t1) # store new value in array:
# chararray[i]  $t1
addi $s1, $s1, 1 # i  i  1
j loop # repeat
done:
A series of characters is called a string. Strings have a variable
length, so programming languages must provide a way to determine the
length or end of the string. In C, the null character (0x00) signifies the
end of a string. For example, Figure 6.23 shows the string “Hello!”
(0x48 65 6C 6C 6F 21 00) stored in memory. The string is seven bytes
long and extends from address 0x1522FFF0 to 0x1522FFF6. The first
character of the string (H  0x48) is stored at the lowest byte address
(0x1522FFF0).
318 CHAPTER SIX Architecture
Figure 6.23 The string “Hello!”
stored in memory
Word
Address
1522FFF4
1522FFF0
Data
6C6C 65 48
2100 6F
Little-Endian Memory
Byte 3 Byte 0
Chapter 06.qxd 1/3
6.4.6 Procedure Calls
High-level languages often use procedures (also called functions) to reuse
frequently accessed code and to make a program more readable.
Procedures have inputs, called arguments, and an output, called the
return value. Procedures should calculate the return value and cause no
other unintended side effects.
When one procedure calls another, the calling procedure, the caller,
and the called procedure, the callee, must agree on where to put the
arguments and the return value. In MIPS, the caller conventionally
places up to four arguments in registers $a0–$a3 before making the procedure call, and the callee places the return value in registers $v0–$v1
before finishing. By following this convention, both procedures know
where to find the arguments and return value, even if the caller and
callee were written by different people.
The callee must not interfere with the function of the caller. Briefly,
this means that the callee must know where to return to after it completes
and it must not trample on any registers or memory needed by the caller.
The caller stores the return address in $ra at the same time it jumps to the
callee using the jump and link instruction (jal). The callee must not overwrite any architectural state or memory that the caller is depending on.
Specifically, the callee must leave the saved registers, $s0–$s7, $ra, and
the stack, a portion of memory used for temporary variables, unmodified.
This section shows how to call and return from a procedure. It
shows how procedures access input arguments and the return value and
how they use the stack to store temporary variables.
Procedure Calls and Returns
MIPS uses the jump and link instruction (jal) to call a procedure and
the jump register instruction (jr) to return from a procedure. Code
Example 6.23 shows the main procedure calling the simple procedure.
main is the caller, and simple is the callee. The simple procedure is
called with no input arguments and generates no return value; it simply
returns to the caller. In Code Example 6.23, instruction addresses are
given to the left of each MIPS instruction in hexadecimal.
6.4 Programming 319
High-Level Code MIPS Assembly Code
int main() {
simple(); 0x00400200 main: jal simple # call procedure
... 0x00400204 ...
}
// void means the function returns no value
void simple() {
return; 0x00401020 simple: jr $ra # return
}
Code Example 6.23 simple PROCEDURE CALL

Jump and link (jal) and jump register (jr $ra) are the two essential
instructions needed for a procedure call. jal performs two functions: it
stores the address of the next instruction (the instruction after jal) in
the return address register ($ra), and it jumps to the target instruction.
In Code Example 6.23, the main procedure calls the simple procedure by executing the jump and link (jal) instruction. jal jumps to the
simple label and stores 0x00400204 in $ra. The simple procedure
returns immediately by executing the instruction jr $ra, jumping to the
instruction address held in $ra. The main procedure then continues executing at this address, 0x00400204.
Input Arguments and Return Values
The simple procedure in Code Example 6.23 is not very useful, because
it receives no input from the calling procedure (main) and returns no output. By MIPS convention, procedures use $a0–$a3 for input arguments
and $v0–$v1 for the return value. In Code Example 6.24, the procedure
diffofsums is called with four arguments and returns one result.
According to MIPS convention, the calling procedure, main, places
the procedure arguments, from left to right, into the input registers,
$a0–$a3. The called procedure, diffofsums, stores the return value in
the return register, $v0.
A procedure that returns a 64-bit value, such as a double-precision
floating point number, uses both return registers, $v0 and $v1. When a
procedure with more than four arguments is called, the additional input
arguments are placed on the stack, which we discuss next.
320 CHAPTER SIX Architecture
High-Level Code MIPS Assembly Code
# $s0  y
int main () main:
{ ...
int y; addi $a0, $0, 2 # argument 0  2
addi $a1, $0, 3 # argument 1  3
... addi $a2, $0, 4 # argument 2  4
addi $a3, $0, 5 # argument 3  5
y  diffofsums (2, 3, 4, 5); jal diffofsums # call procedure
add $s0, $v0, $0 # y  returned value
... ...
}
# $s0  result
int diffofsums (int f, int g, int h, int i) diffofsums:
{ add $t0, $a0, $a1 # $t0  f  g
int result; add $t1, $a2, $a3 # $t1  h  i
sub $s0, $t0, $t1 # result  (f  g)  (h  i)
result  (f  g)  (h  i); add $v0, $s0, $0 # put return value in $v0
return result; jr $ra # return to caller
}
Code Example 6.24 PROCEDURE CALL WITH ARGUMENTS AND RETURN VALUES
Code Example 6.24 has some
subtle errors. Code Examples
6.25 and 6.26 on page 323
show improved versions of
the program.
Chapter 06.qxd 1/3
6.4 Programming 321
The Stack
The stack is memory that is used to save local variables within a procedure.
The stack expands (uses more memory) as the processor needs more
scratch space and contracts (uses less memory) when the processor no
longer needs the variables stored there. Before explaining how procedures
use the stack to store temporary variables, we explain how the stack works.
The stack is a last-in-first-out (LIFO) queue. Like a stack of dishes, the
last item pushed onto the stack (the top dish) is the first one that can be
pulled (popped) off. Each procedure may allocate stack space to store local
variables but must deallocate it before returning. The top of the stack, is
the most recently allocated space. Whereas a stack of dishes grows up in
space, the MIPS stack grows down in memory. The stack expands to lower
memory addresses when a program needs more scratch space.
Figure 6.24 shows a picture of the stack. The stack pointer, $sp, is a
special MIPS register that points to the top of the stack. A pointer is a
fancy name for a memory address. It points to (gives the address of)
data. For example, in Figure 6.24(a) the stack pointer, $sp, holds the
address value 0x7FFFFFFC and points to the data value 0x12345678.
$sp points to the top of the stack, the lowest accessible memory address
on the stack. Thus, in Figure 6.24(a), the stack cannot access memory
below memory word 0x7FFFFFFC.
The stack pointer ($sp) starts at a high memory address and decrements to expand as needed. Figure 6.24(b) shows the stack expanding to
allow two more data words of temporary storage. To do so, $sp decrements by 8 to become 0x7FFFFFF4. Two additional data words,
0xAABBCCDD and 0x11223344, are temporarily stored on the stack.
One of the important uses of the stack is to save and restore registers
that are used by a procedure. Recall that a procedure should calculate a
return value but have no other unintended side effects. In particular, it
should not modify any registers besides the one containing the return
value, $v0. The diffofsums procedure in Code Example 6.24 violates
this rule because it modifies $t0, $t1, and $s0. If main had been using
$t0, $t1, or $s0 before the call to diffofsums, the contents of these
registers would have been corrupted by the procedure call.
To solve this problem, a procedure saves registers on the stack
before it modifies them, then restores them from the stack before it
returns. Specifically, it performs the following steps.
1. Makes space on the stack to store the values of one or more registers.
2. Stores the values of the registers on the stack.
3. Executes the procedure using the registers.
4. Restores the original values of the registers from the stack.
5. Deallocates space on the stack.
Data
7FFFFFFC 12345678
7FFFFFF8
7FFFFFF4
7FFFFFF0
Address
$sp
(a)
7FFFFFFC
7FFFFFF8
7FFFFFF4
7FFFFFF0
Address
(b)
Data
12345678
$sp
AABBCCDD
11223344
Figure 6.24 The stack

Code Example 6.25 shows an improved version of diffofsums that
saves and restores $t0, $t1, and $s0. The new lines are indicated in
blue. Figure 6.25 shows the stack before, during, and after a call to the
diffofsums procedure from Code Example 6.25. diffofsums makes
room for three words on the stack by decrementing the stack pointer
($sp) by 12. It then stores the current values of $s0, $t0, and $t1 in
the newly allocated space. It executes the rest of the procedure, changing the values in these three registers. At the end of the procedure,
diffofsums restores the values of $s0, $t0, and $t1 from the stack,
deallocates its stack space, and returns. When the procedure returns,
$v0 holds the result, but there are no other side effects: $s0, $t0, $t1,
and $sp have the same values as they did before the procedure call.
The stack space that a procedure allocates for itself is called its stack
frame. diffofsums’s stack frame is three words deep. The principle of
modularity tells us that each procedure should access only its own stack
frame, not the frames belonging to other procedures.
Preserved Registers
Code Example 6.25 assumes that temporary registers $t0 and $t1 must
be saved and restored. If the calling procedure does not use those registers, the effort to save and restore them is wasted. To avoid this waste,
MIPS divides registers into preserved and nonpreserved categories. The
preserved registers include $s0–$s7 (hence their name, saved). The
nonpreserved registers include $t0–$t9 (hence their name, temporary).
A procedure must save and restore any of the preserved registers that it
wishes to use, but it can change the nonpreserved registers freely.
Code Example 6.26 shows a further improved version of diffofsums
that saves only $s0 on the stack. $t0 and $t1 are nonpreserved registers,
so they need not be saved.
Remember that when one procedure calls another, the former is the
caller and the latter is the callee. The callee must save and restore any
preserved registers that it wishes to use. The callee may change any of
the nonpreserved registers. Hence, if the caller is holding active data in a
322 CHAPTER SIX Architecture
Data
FC
F8
F4
F0
Address
$sp
(a)
?
Data
$sp
(c)
FC
F8
F4
F0
Address
?
Data
FC
F8
F4
F0
Address
$sp
(b)
$s0
$t0
?
stack frame
$t1
Figure 6.25 The stack
(a) before, (b) during, and
(c) after diffofsums procedure
call

nonpreserved register, the caller needs to save that nonpreserved register
before making the procedure call and then needs to restore it afterward.
For these reasons, preserved registers are also called callee-save, and
nonpreserved registers are called caller-save.
Table 6.3 summarizes which registers are preserved. $s0–$s7 are
generally used to hold local variables within a procedure, so they must
be saved. $ra must also be saved, so that the procedure knows where to
return. $t0–$t9 are used to hold temporary results before they are
assigned to local variables. These calculations typically complete before
a procedure call is made, so they are not preserved, and it is rare that
the caller needs to save them. $a0–$a3 are often overwritten in the
process of calling a procedure. Hence, they must be saved by the caller
if the caller depends on any of its own arguments after a called procedure returns. $v0–$v1 certainly should not be preserved, because the
callee returns its result in these registers.
6.4 Programming 323
MIPS Assembly Code
# $s0  result
diffofsums:
addi $sp, $sp, 12 # make space on stack to store three registers
sw $s0, 8($sp) # save $s0 on stack
sw $t0, 4($sp) # save $t0 on stack
sw $t1, 0($sp) # save $t1 on stack
add $t0, $a0, $a1 # $t0  f  g
add $t1, $a2, $a3 # $t1  h  i
sub $s0, $t0, $t1 # result  (f  g)  (h  i)
add $v0, $s0, $0 # put return value in $v0
lw $t1, 0($sp) # restore $t1 from stack
lw $t0, 4($sp) # restore $t0 from stack
lw $s0, 8($sp) # restore $s0 from stack
addi $sp, $sp, 12 # deallocate stack space
jr $ra # return to caller
Code Example 6.25 PROCEDURE SAVING REGISTERS ON THE STACK
MIPS Assembly Code
# $s0  result
diffofsums:
addi $sp, $sp, 4 # make space on stack to store one register
sw $s0, 0($sp) # save $s0 on stack
add $t0, $a0, $a1 # $t0  f  g
add $t1, $a2, $a3 # $t1  h  i
sub $s0, $t0, $t1 # result  (f  g)  (h  i)
add $v0, $s0, $0 # put return value in $v0
lw $s0, 0($sp) # restore $s0 from stack
addi $sp, $sp, 4 # deallocate stack space
jr $ra # return to caller
Code Example 6.26 PROCEDURE SAVING PRESERVED REGISTERS
ON THE STACK
Chapter 06.qxd 1
324 CHAPTER SIX Architecture
The stack above the stack pointer is automatically preserved as long
as the callee does not write to memory addresses above $sp. In this way,
it does not modify the stack frame of any other procedures. The stack
pointer itself is preserved, because the callee deallocates its stack frame
before returning by adding back the same amount that it subtracted
from $sp at the beginning of the procedure.
Recursive Procedure Calls
A procedure that does not call others is called a leaf procedure; an
example is diffofsums. A procedure that does call others is called a
nonleaf procedure. As mentioned earlier, nonleaf procedures are somewhat more complicated because they may need to save nonpreserved
registers on the stack before they call another procedure, and then
restore those registers afterward. Specifically, the caller saves any nonpreserved registers ($t0–$t9 and $a0–$a3) that are needed after the
call. The callee saves any of the preserved registers ($s0–$s7 and $ra)
that it intends to modify.
A recursive procedure is a nonleaf procedure that calls itself. The factorial function can be written as a recursive procedure call. Recall that
factorial(n)  n  (n  1)  (n  2)  ...  2  1. The factorial function can be rewritten recursively as factorial(n)  n  factorial(n  1).
The factorial of 1 is simply 1. Code Example 6.27 shows the factorial
function written as a recursive procedure. To conveniently refer to program addresses, we assume that the program starts at address 0x90.
The factorial procedure might modify $a0 and $ra, so it saves
them on the stack. It then checks whether n  2. If so, it puts the return
value of 1 in $v0, restores the stack pointer, and returns to the caller. It
does not have to reload $ra and $a0 in this case, because they were never
modified. If n  1, the procedure recursively calls factorial(n1). It
then restores the value of n ($a0) and the return address ($ra) from the
stack, performs the multiplication, and returns this result. The multiply
instruction (mul $v0, $a0, $v0) multiplies $a0 and $v0 and places the
result in $v0. It is discussed further in Section 6.7.1.
Table 6.3 Preserved and nonpreserved registers
Preserved Nonpreserved
Saved registers: $s0–$s7 Temporary registers: $t0–$t9
Return address: $ra Argument registers: $a0–$a3
Stack pointer: $sp Return value registers: $v0–$v1
Stack above the stack pointer Stack below the stack pointer
Ch
6.4 Programming 325
High-Level Code
int factorial (int n) {
if (n  1)
return 1;
else
return (n * factorial (n1));
}
MIPS Assembly Code
0x90 factorial: addi $sp, $sp, 8 # make room on stack
0x94 sw $a0, 4($sp) # store $a0
0x98 sw $ra, 0($sp) # store $ra
0x9C addi $t0, $0, 2 # $t0  2
0xA0 slt $t0, $a0, $t0 # a  1 ?
0xA4 beq $t0, $0, else # no: goto else
0xA8 addi $v0, $0, 1 # yes: return 1
0xAC addi $sp, $sp, 8 # restore $sp
0xB0 jr $ra # return
0xB4 else: addi $a0, $a0, 1 # n  n  1
0xB8 jal factorial # recursive call
0xBC lw $ra, 0($sp) # restore $ra
0xC0 lw $a0, 4($sp) # restore $a0
0xC4 addi $sp, $sp, 8 # restore $sp
0xC8 mul $v0, $a0, $v0 # n * factorial (n1)
0xCC jr $ra # return
Code Example 6.27 factorial RECURSIVE PROCEDURE CALL
Figure 6.26 shows the stack when executing factorial(3). We
assume that $sp initially points to 0xFC, as shown in Figure 6.26(a).
The procedure creates a two-word stack frame to hold $a0 and $ra. On
the first invocation, factorial saves $a0 (holding n  3) at 0xF8 and
$ra at 0xF4, as shown in Figure 6.26(b). The procedure then changes
$a0 to n  2 and recursively calls factorial(2), making $ra hold
0xBC. On the second invocation, it saves $a0 (holding n  2) at 0xF0
and $ra at 0xEC. This time, we know that $ra contains 0xBC. The procedure then changes $a0 to n  1 and recursively calls factorial(1).
On the third invocation, it saves $a0 (holding n  1) at 0xE8 and $ra at
0xE4. This time, $ra again contains 0xBC. The third invocation of
Figure 6.26 Stack during
factorial procedure call when
n  3: (a) before call, (b) after
last recursive call, (c) after
return
$sp
(a)
FC
F8
F4
F0
EC
E8
E4
E0
DC
Address Data
FC
F8
F4
F0
(b)
$ra
EC
E8
E4
E0
DC
$sp
$sp
$sp
$sp
Address Data
$a0 (0x3)
$ra (0xBC)
$a0 (0x2)
$ra (0xBC)
$a0 (0x1)
FC
F8
F4
F0
(c)
EC
E8
E4
E0
DC
$sp
$sp
$sp
$sp
$a0 = 1
$v0 = 1 x 1
$a0 = 2
$v0 = 2 x 1
$a0 = 3
$v0 = 3 x 2
$v0 = 6
Address Data
$ra
$a0 (0x3)
$ra (0xBC)
$a0 (0x2)
$ra (0xBC)
$a0 (0x1)
Chapter 06
factorial returns the value 1 in $v0 and deallocates the stack frame
before returning to the second invocation. The second invocation
restores n to 2, restores $ra to 0xBC (it happened to already have this
value), deallocates the stack frame, and returns $v0  2  1  2 to the
first invocation. The first invocation restores n to 3, restores $ra to the
return address of the caller, deallocates the stack frame, and returns
$v0  3  2  6. Figure 6.26(c) shows the stack as the recursively
called procedures return. When factorial returns to the caller, the
stack pointer is in its original position (0xFC), none of the contents of
the stack above the pointer have changed, and all of the preserved registers hold their original values. $v0 holds the return value, 6.
Additional Arguments and Local Variables*
Procedures may have more than four input arguments and local variables. The stack is used to store these temporary values. By MIPS convention, if a procedure has more than four arguments, the first four are
passed in the argument registers as usual. Additional arguments are
passed on the stack, just above $sp. The caller must expand its stack to
make room for the additional arguments. Figure 6.27(a) shows the
caller’s stack for calling a procedure with more than four arguments.
A procedure can also declare local variables or arrays. Local variables are declared within a procedure and can be accessed only within
that procedure. Local variables are stored in $s0–$s7; if there are too
many local variables, they can also be stored in the procedure’s stack
frame. In particular, local arrays are stored on the stack.
Figure 6.27(b) shows the organization of a callee’s stack frame. The
frame holds the procedure’s own arguments (if it calls other procedures),
the return address, and any of the saved registers that the procedure will
326 CHAPTER SIX Architecture
$sp
$ra (if needed)
additional arguments
$a0–$a3
(if needed)
$s0–$s7
(if needed)
local variables or
arrays
$spstack
frame
additional arguments
Figure 6.27 Stack usage:
(left) before call,
(right) after call
Chap
modify. It also holds local arrays and any excess local variables. If the
callee has more than four arguments, it finds them in the caller’s stack
frame. Accessing additional input arguments is the one exception in
which a procedure can access stack data not in its own stack frame.
6.5 ADDRESSING MODES
MIPS uses five addressing modes: register-only, immediate, base, PCrelative, and pseudo-direct. The first three modes (register-only, immediate, and base addressing) define modes of reading and writing
operands. The last two (PC-relative and pseudo-direct addressing)
define modes of writing the program counter, PC.
Register-Only Addressing
Register-only addressing uses registers for all source and destination
operands. All R-type instructions use register-only addressing.
Immediate Addressing
Immediate addressing uses the 16-bit immediate along with registers as
operands. Some I-type instructions, such as add immediate (addi) and
load upper immediate (lui), use immediate addressing.
Base Addressing
Memory access instructions, such as load word (lw) and store word (sw),
use base addressing. The effective address of the memory operand is
found by adding the base address in register rs to the sign-extended
16-bit offset found in the immediate field.
PC-relative Addressing
Conditional branch instructions use PC-relative addressing to specify the
new value of the PC if the branch is taken. The signed offset in the immediate field is added to the PC to obtain the new PC; hence, the branch
destination address is said to be relative to the current PC.
Code Example 6.28 shows part of the factorial procedure
from Code Example 6.27. Figure 6.28 shows the machine code for the
beq instruction. The branch target address (BTA) is the address of
the next instruction to execute if the branch is taken. The beq instruction in Figure 6.28 has a BTA of 0xB4, the instruction address of the
else label.
The 16-bit immediate field gives the number of instructions between
the BTA and the instruction after the branch instruction (the instruction
at PC4). In this case, the value in the immediate field of beq is 3
because the BTA (0xB4) is 3 instructions past PC4 (0xA8).
The processor calculates the BTA from the instruction by signextending the 16-bit immediate, multiplying it by 4 (to convert words to
bytes), and adding it to PC4.
6.5 Addressing Modes 327
Cha
Example 6.8 CALCULATING THE IMMEDIATE FIELD
FOR PC-RELATIVE ADDRESSING
Calculate the immediate field and show the machine code for the branch not
equal (bne) instruction in the following program.
# MIPS assembly code
0x40 loop: add $t1, $a0, $s0
0x44 lb $t1, 0($t1)
0x48 add $t2, $a1, $s0
0x4C sb $t1, 0($t2)
0x50 addi $s0, $s0, 1
0x54 bne $t1, $0, loop
0x58 lw $s0, 0($sp)
Solution: Figure 6.29 shows the machine code for the bne instruction. Its branch
target address, 0x40, is 6 instructions behind PC4 (0x58), so the immediate
field is 6.
328 CHAPTER SIX Architecture
op rs imm
beq $t0, $0, else
Machine Code Assembly Code
6 bits 5 bits 5 bits 16 bits
(0x11000003)
6 bits
Field Values
op rs rt imm
4 80 3
5 bits 5 bits 16 bits
000100 01000 00000 0000 0000 0000 0011
rt
Figure 6.28 Machine code for beq
MIPS Assembly Code
0xA4 beq $t0, $0, else
0xA8 addi $v0, $0, 1
0xAC addi $sp, $sp, 8
0xB0 jr $ra
0xB4 else: addi $a0, $a0, 1
0xB8 jal factorial
Code Example 6.28 CALCULATING THE BRANCH TARGET ADDRESS
bne $t1, $0, loop 000101 01001 00000
Assembly Code Machine Code
5 9 0 -6
6 bits 5 bits 5 bits 16 bits
(0x1520FFFA)
6 bits 5 bits 5 bits 16 bits
op rs rt imm op rs rt imm
1111 1111 1111 1010
Field Values
Figure 6.29 bne machine code
Pseudo-Direct Addressing
In direct addressing, an address is specified in the instruction. The jump
instructions, j and jal, ideally would use direct addressing to specify a
C
32-bit jump target address (JTA) to indicate the instruction address to
execute next.
Unfortunately, the J-type instruction encoding does not have enough
bits to specify a full 32-bit JTA. Six bits of the instruction are used for
the opcode, so only 26 bits are left to encode the JTA. Fortunately, the
two least significant bits, JTA1:0, should always be 0, because instructions are word aligned. The next 26 bits, JTA27:2, are taken from the
addr field of the instruction. The four most significant bits, JTA31:28, are
obtained from the four most significant bits of PC4. This addressing
mode is called pseudo-direct.
Code Example 6.29 illustrates a jal instruction using pseudo-direct
addressing. The JTA of the jal instruction is 0x004000A0. Figure 6.30
shows the machine code for this jal instruction. The top four bits and
bottom two bits of the JTA are discarded. The remaining bits are stored
in the 26-bit address field (addr).
The processor calculates the JTA from the J-type instruction by
appending two 0’s and prepending the four most significant bits of
PC4 to the 26-bit address field (addr).
Because the four most significant bits of the JTA are taken from
PC4, the jump range is limited. The range limits of branch and jump
instructions are explored in Exercises 6.23 to 6.26. All J-type instructions, j and jal, use pseudo-direct addressing.
Note that the jump register instruction, jr, is not a J-type instruction. It is an R-type instruction that jumps to the 32-bit value held in
register rs.
6.5 Addressing Modes 329
MIPS Assembly Code
0x0040005C jal sum
...
0x004000A0 sum: add $v0, $a0, $a1
Code Example 6.29 CALCULATING THE JUMP TARGET ADDRESS
op
jal sum
Assembly Code Machine Code
3 (0x0C100028)
op
6 bits
JTA 0000 0000 0100 0000 0000 0000 1010 0000
26-bit addr (0x0100028)
(0x004000A0)
0000 0000
0 0 0 2 8
addr
0x0100028
26 bits 6 bits 26 bits
000011 00 0001 0000 0000 0000 0010 1000
addr
0000 0100 0000 0000 0000 1010
1 0
Field Values
Figure 6.30 jal machine code
Cha
330 CHAPTER SIX Architecture
6.6 LIGHTS, CAMERA, ACTION: COMPILING,
ASSEMBLING, AND LOADING
Up until now, we have shown how to translate short high-level code
snippets into assembly and machine code. This section describes how to
compile and assemble a complete high-level program and how to load
the program into memory for execution.
We begin by introducing the MIPS memory map, which defines
where code, data, and stack memory are located. We then show the steps
of code execution for a sample program.
6.6.1 The Memory Map
With 32-bit addresses, the MIPS address space spans 232 bytes  4 gigabytes (GB). Word addresses are divisible by 4 and range from 0 to
0xFFFFFFFC. Figure 6.31 shows the MIPS memory map. The MIPS
architecture divides the address space into four parts or segments: the
text segment, global data segment, dynamic data segment, and reserved
segments. The following sections describes each segment.
The Text Segment
The text segment stores the machine language program. It is large
enough to accommodate almost 256 MB of code. Note that the four
most significant bits of the address in the text space are all 0, so the j
instruction can directly jump to any address in the program.
The Global Data Segment
The global data segment stores global variables that, in contrast to local
variables, can be seen by all procedures in a program. Global variables
Address Segment
$sp = 0x7FFFFFFC
0xFFFFFFFC
0x80000000
0x7FFFFFFC
0x10010000
0x1000FFFC
0x10000000
0x0FFFFFFC
0x00400000
0x003FFFFC
0x00000000
Reserved
Stack
Heap
Global Data
Text
Reserved
$gp = 0x10008000
PC = 0x00400000
Figure 6.31 MIPS memory map Dynamic Data
C
are defined at start-up, before the program begins executing. These variables are declared outside the main procedure in a C program and can
be accessed by any procedure. The global data segment is large enough
to store 64 KB of global variables.
Global variables are accessed using the global pointer ($gp), which is
initialized to 0x100080000. Unlike the stack pointer ($sp), $gp does not
change during program execution. Any global variable can be accessed
with a 16-bit positive or negative offset from $gp. The offset is known at
assembly time, so the variables can be efficiently accessed using base
addressing mode with constant offsets.
The Dynamic Data Segment
The dynamic data segment holds the stack and the heap. The data in this
segment are not known at start-up but are dynamically allocated and deallocated throughout the execution of the program. This is the largest
segment of memory used by a program, spanning almost 2 GB of the
address space.
As discussed in Section 6.4.6, the stack is used to save and restore
registers used by procedures and to hold local variables such as arrays.
The stack grows downward from the top of the dynamic data segment
(0x7FFFFFFC) and is accessed in last-in-first-out (LIFO) order.
The heap stores data that is allocated by the program during runtime. In C, memory allocations are made by the malloc function; in
C and Java, new is used to allocate memory. Like a heap of clothes
on a dorm room floor, heap data can be used and discarded in any order.
The heap grows upward from the bottom of the dynamic data segment.
If the stack and heap ever grow into each other, the program’s data
can become corrupted. The memory allocator tries to ensure that this
never happens by returning an out-of-memory error if there is insufficient space to allocate more dynamic data.
The Reserved Segments
The reserved segments are used by the operating system and cannot
directly be used by the program. Part of the reserved memory is used for
interrupts (see Section 7.7) and for memory-mapped I/O (see Section 8.5).
6.6.2 Translating and Starting a Program
Figure 6.32 shows the steps required to translate a program from a highlevel language into machine language and to start executing that program.
First, the high-level code is compiled into assembly code. The assembly
code is assembled into machine code in an object file. The linker combines
the machine code with object code from libraries and other files to produce an entire executable program. In practice, most compilers perform all
three steps of compiling, assembling, and linking. Finally, the loader loads
6.6 Lights, Camera, Action: Compiling, Assembling, and Loading 331
Grace Hopper, 1906–1992.
Graduated from Yale
University with a Ph.D. in
mathematics. Developed the
first compiler while working
for the Remington Rand
Corporation and was instrumental in developing the
COBOL programming language. As a naval officer, she
received many awards, including a World War II Victory
Medal and the National
Defence Service Medal.
Ch
the program into memory and starts execution. The remainder of this section walks through these steps for a simple program.
Step 1: Compilation
A compiler translates high-level code into assembly language. Code
Example 6.30 shows a simple high-level program with three global
332 CHAPTER SIX Architecture
Assembly Code
High Level Code
Compiler
Object File
Assembler
Executable
Linker
Memory
Loader
Object Files
Library Files
Figure 6.32 Steps for
translating and starting
a program
High-Level Code
int f, g, y; // global variables
int main (void)
{
f  2;
g  3;
y  sum (f, g);
return y;
}
int sum (int a, int b) {
return (a  b);
}
MIPS Assembly Code
.data
f:
g:
y:
.text
main:
addi $sp, $sp, 4 # make stack frame
sw $ra, 0($sp) # store $ra on stack
addi $a0, $0, 2 # $a0  2
sw $a0, f # f  2
addi $a1, $0, 3 # $a1  3
sw $a1, g # g  3
jal sum # call sum procedure
sw $v0, y # y  sum (f, g)
lw $ra, 0($sp) # restore $ra from stack
addi $sp, $sp, 4 # restore stack pointer
jr $ra # return to operating system
sum:
add $v0, $a0, $a1 # $v0  a  b
jr $ra # return to caller
Code Example 6.30 COMPILING A HIGH-LEVEL PROGRAM
Chapter 06.
variables and two procedures, along with the assembly code produced by
a typical compiler. The .data and .text keywords are assembler directives
that indicate where the text and data segments begin. Labels are used for
global variables f, g, and y. Their storage location will be determined by
the assembler; for now, they are left as symbols in the code.
Step 2: Assembling
The assembler turns the assembly language code into an object file containing machine language code. The assembler makes two passes
through the assembly code. On the first pass, the assembler assigns
instruction addresses and finds all the symbols, such as labels and global
variable names. The code after the first assembler pass is shown here.
0x00400000 main: addi $sp, $sp, 4
0x00400004 sw $ra, 0($sp)
0x00400008 addi $a0, $0, 2
0x0040000C sw $a0, f
0x00400010 addi $a1, $0, 3
0x00400014 sw $a1, g
0x00400018 jal sum
0x0040001C sw $v0, y
0x00400020 lw $ra, 0($sp)
0x00400024 addi $sp, $sp, 4
0x00400028 jr $ra
0x0040002C sum: add $v0, $a0, $a1
0x00400030 jr $ra
The names and addresses of the symbols are kept in a symbol table,
as shown in Table 6.4 for this code. The symbol addresses are filled
in after the first pass, when the addresses of labels are known. Global
variables are assigned storage locations in the global data segment of
memory, starting at memory address 0x10000000.
On the second pass through the code, the assembler produces the
machine language code. Addresses for the global variables and labels are
taken from the symbol table. The machine language code and symbol
table are stored in the object file.
6.6 Lights, Camera, Action: Compiling, Assembling, and Loading 333
Table 6.4 Symbol table
Symbol Address
f 0x10000000
g 0x10000004
y 0x10000008
main 0x00400000
sum 0x0040002C

Executable file header Text Size Data Size
Text segment
Data segment
Address
Address
0x00400000
0x00400004
0x00400008
0x0040000C
0x00400010
0x00400014
0x00400018
0x0040001C
0x00400020
0x00400024
0x00400028
0x0040002C
0x00400030
addi $sp, $sp, –4
sw $ra, 0 ($sp)
addi $a0, $0, 2
sw $a0, 0x8000 ($gp)
addi $a1, $0, 3
sw $a1, 0x8004 ($gp)
jal 0x0040002C
sw $v0, 0x8008 ($gp)
lw $ra, 0 ($sp)
addi $sp, $sp, –4
jr $ra
add $v0, $a0, $a1
jr $ra
0x10000000
0x10000004
0x10000008
f
g
y
0x34 (52 bytes) 0xC (12 bytes)
0x23BDFFFC
0xAFBF0000
0x20040002
0xAF848000
0x20050003
0xAF858004
0x0C10000B
0xAF828008
0x8FBF0000
0x23BD0004
0x03E00008
0x00851020
0x03E0008
Instruction
Data
334 CHAPTER SIX Architecture
Step 3: Linking
Most large programs contain more than one file. If the programmer
changes only one of the files, it would be wasteful to recompile and
reassemble the other files. In particular, programs often call procedures
in library files; these library files almost never change. If a file of highlevel code is not changed, the associated object file need not be updated.
The job of the linker is to combine all of the object files into one
machine language file called the executable. The linker relocates the data
and instructions in the object files so that they are not all on top of each
other. It uses the information in the symbol tables to adjust the addresses
of global variables and of labels that are relocated.
In our example, there is only one object file, so no relocation is
necessary. Figure 6.33 shows the executable file. It has three sections: the
executable file header, the text segment, and the data segment. The executable file header reports the text size (code size) and data size (amount
of globally declared data). Both are given in units of bytes. The text
segment gives the instructions and the addresses where they are to be
stored.
The figure shows the instructions in human-readable format next to
the machine code for ease of interpretation, but the executable file
includes only machine instructions. The data segment gives the address
of each global variable. The global variables are addressed with respect
to the base address given by the global pointer, $gp. For example, the
Figure 6.33 Executable

y
g
f
0x03E00008
0x00851020
0x03E00008
0x23BD0004
0x8FBF0000
0xAF828008
0x0C10000B
0xAF858004
0x20050003
0xAF848000
0x20040002
0xAFBF0000
0x23BDFFFC
Address Memory
0x7FFFFFFC $sp = 0x7FFFFFFC
0x10010000
0x00400000
Stack
Heap
$gp = 0x10008000
PC = 0x00400000
0x10000000
Reserved
Reserved
first store instruction, sw $a0, 0x8000($gp), stores the value 2 to the
global variable f, which is located at memory address 0x10000000.
Remember that the offset, 0x8000, is a 16-bit signed number that is
sign-extended and added to the base address, $gp. So, $gp  0x8000
0x10008000  0xFFFF8000  0x10000000, the memory address of
variable f.
Step 4: Loading
The operating system loads a program by reading the text segment of
the executable file from a storage device (usually the hard disk) into the
text segment of memory. The operating system sets $gp to 0x10008000
(the middle of the global data segment) and $sp to 0x7FFFFFFC (the
top of the dynamic data segment), then performs a jal 0x00400000 to
jump to the beginning of the program. Figure 6.34 shows the memory
map at the beginning of program execution.
6.6 Lights, Camera, Action: Compiling, Assembling, and Loading 335
Figure 6.34 Executable loaded
in memory
Chap
6.7 ODDS AND ENDS*
This section covers a few optional topics that do not fit naturally elsewhere
in the chapter. These topics include pseudoinstructions, exceptions, signed
and unsigned arithmetic instructions, and floating-point instructions.
6.7.1 Pseudoinstructions
If an instruction is not available in the MIPS instruction set, it is probably because the same operation can be performed using one or more
existing MIPS instructions. Remember that MIPS is a reduced instruction set computer (RISC), so the instruction size and hardware complexity are minimized by keeping the number of instructions small.
However, MIPS defines pseudoinstructions that are not actually
part of the instruction set but are commonly used by programmers and
compilers. When converted to machine code, pseudoinstructions are
translated into one or more MIPS instructions.
Table 6.5 gives examples of pseudoinstructions and the MIPS
instructions used to implement them. For example, the load immediate
pseudoinstruction (li) loads a 32-bit constant using a combination of
lui and ori instructions. The multiply pseudoinstruction (mul) provides a three-operand multiply, multiplying two registers and putting the
32 least significant bits of the result into a third register. The no operation pseudoinstruction (nop, pronounced “no op”) performs no operation. The PC is incremented by 4 upon its execution. No other registers
or memory values are altered. The machine code for the nop instruction
is 0x00000000.
Some pseudoinstructions require a temporary register for intermediate
calculations. For example, the pseudoinstruction beq $t2, imm15:0, Loop
compares $t2 to a 16-bit immediate, imm15:0. This pseudoinstruction
336 CHAPTER SIX Architecture
Table 6.5 Pseudoinstructions
Corresponding
Pseudoinstruction MIPS Instructions
li $s0, 0x1234AA77 lui $s0, 0x1234
ori $s0, 0xAA77
mul $s0, $s1, $s2 mult $s1, $s2
mflo $s0
clear $t0 add $t0, $0, $0
move $s1, $s2 add $s2, $s1, $0
nop sll $0, $0, 0

requires a temporary register in which to store the 16-bit immediate.
Assemblers use the assembler register, $at, for such purposes. Table 6.6
shows how the assembler uses $at in converting a pseudoinstruction to
real MIPS instructions. We leave it as Exercise 6.31 to implement other
pseudoinstructions such as rotate left (rol) and rotate right (ror).
6.7.2 Exceptions
An exception is like an unscheduled procedure call that jumps to a new
address. Exceptions may be caused by hardware or software. For example,
the processor may receive notification that the user pressed a key on a keyboard. The processor may stop what it is doing, determine which key was
pressed, save it for future reference, then resume the program that was running. Such a hardware exception triggered by an input/output (I/O) device
such as a keyboard is often called an interrupt. Alternatively, the program
may encounter an error condition such as an undefined instruction. The
program then jumps to code in the operating system (OS), which may
choose to terminate the offending program. Software exceptions are sometimes called traps. Other causes of exceptions include division by zero,
attempts to read nonexistent memory, hardware malfunctions, debugger
breakpoints, and arithmetic overflow (see Section 6.7.3).
The processor records the cause of an exception and the value of the PC
at the time the exception occurs. It then jumps to the exception handler procedure. The exception handler is code (usually in the OS) that examines the
cause of the exception and responds appropriately (by reading the keyboard
on a hardware interrupt, for example). It then returns to the program that
was executing before the exception took place. In MIPS, the exception handler is always located at 0x80000180. When an exception occurs, the
processor always jumps to this instruction address, regardless of the cause.
The MIPS architecture uses a special-purpose register, called the
Cause register, to record the cause of the exception. Different codes
are used to record different exception causes, as given in Table 6.7. The
exception handler code reads the Cause register to determine how to
handle the exception. Some other architectures jump to a different exception handler for each different cause instead of using a Cause register.
MIPS uses another special-purpose register called the Exception
Program Counter (EPC) to store the value of the PC at the time an exception
6.7 Odds and Ends 337
Table 6.6 Pseudoinstruction using $at
Corresponding
Pseudoinstruction MIPS Instructions
beq $t2, imm15:0, Loop addi $at, $0, imm15:0
beq $t2, $at, Loop

takes place. The processor returns to the address in EPC after handling the
exception. This is analogous to using $ra to store the old value of the PC
during a jal instruction.
The EPC and Cause registers are not part of the MIPS register file.
The mfc0 (move from coprocessor 0) instruction copies these and other
special-purpose registers into one of the general purpose registers.
Coprocessor 0 is called the MIPS processor control; it handles interrupts
and processor diagnostics. For example, mfc0 $t0, Cause copies the
Cause register into $t0.
The syscall and break instructions cause traps to perform system
calls or debugger breakpoints. The exception handler uses the EPC to
look up the instruction and determine the nature of the system call or
breakpoint by looking at the fields of the instruction.
In summary, an exception causes the processor to jump to the
exception handler. The exception handler saves registers on the stack,
then uses mfc0 to look at the cause and respond accordingly. When the
handler is finished, it restores the registers from the stack, copies the
return address from EPC to $k0 using mfc0, and returns using jr $k0.
6.7.3 Signed and Unsigned Instructions
Recall that a binary number may be signed or unsigned. The MIPS architecture uses two’s complement representation of signed numbers. MIPS
has certain instructions that come in signed and unsigned flavors, including addition and subtraction, multiplication and division, set less than,
and partial word loads.
Addition and Subtraction
Addition and subtraction are performed identically whether the number
is signed or unsigned. However, the interpretation of the results is different.
As mentioned in Section 1.4.6, if two large signed numbers are added
together, the result may incorrectly produce the opposite sign. For example, adding the following two huge positive numbers gives a negative
338 CHAPTER SIX Architecture
Table 6.7 Exception cause codes
Exception Cause
hardware interrupt 0x00000000
system call 0x00000020
breakpoint/divide by 0 0x00000024
undefined instruction 0x00000028
arithmetic overflow 0x00000030
$k0 and $k1 are included in the
MIPS register set. They are
reserved by the OS for exception handling. They do not
need to be saved and restored
during exceptions.

result: 0x7FFFFFFF  0x7FFFFFFF  0xFFFFFFFE  2. Similarly,
adding two huge negative numbers gives a positive result, 0x80000001
0x80000001  0x00000002. This is called arithmetic overflow.
The C language ignores arithmetic overflows, but other languages,
such as Fortran, require that the program be notified. As mentioned in
Section 6.7.2, the MIPS processor takes an exception on arithmetic overflow. The program can decide what to do about the overflow (for example, it might repeat the calculation with greater precision to avoid the
overflow), then return to where it left off.
MIPS provides signed and unsigned versions of addition and subtraction. The signed versions are add, addi, and sub. The unsigned versions are addu, addiu, and subu. The two versions are identical except
that signed versions trigger an exception on overflow, whereas unsigned
versions do not. Because C ignores exceptions, C programs technically
use the unsigned versions of these instructions.
Multiplication and Division
Multiplication and division behave differently for signed and unsigned
numbers. For example, as an unsigned number, 0xFFFFFFFF represents
a large number, but as a signed number it represents 1. Hence,
0xFFFFFFFF  0xFFFFFFFF would equal 0xFFFFFFFE00000001 if the
numbers were unsigned but 0x0000000000000001 if the numbers were
signed.
Therefore, multiplication and division come in both signed and
unsigned flavors. mult and div treat the operands as signed numbers.
multu and divu treat the operands as unsigned numbers.
Set Less Than
Set less than instructions can compare either two registers (slt) or a register and an immediate (slti). Set less than also comes in signed (slt
and slti) and unsigned (sltu and sltiu) versions. In a signed comparison, 0x80000000 is less than any other number, because it is the most
negative two’s complement number. In an unsigned comparison,
0x80000000 is greater than 0x7FFFFFFF but less than 0x80000001,
because all numbers are positive.
Beware that sltiu sign-extends the immediate before treating it as
an unsigned number. For example, sltiu $s0, $s1, 0x8042 compares
$s1 to 0xFFFF8042, treating the immediate as a large positive number.
Loads
As described in Section 6.4.5, byte loads come in signed (lb) and
unsigned (lbu) versions. lb sign-extends the byte, and lbu zero-extends
the byte to fill the entire 32-bit register. Similarly, MIPS provides signed
and unsigned half-word loads (lh and lhu), which load two bytes into
the lower half and sign- or zero-extend the upper half of the word.
6.7 Odds and Ends 339
Chapt
cop ft fs fd funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
F-type
op
6.7.4 Floating-Point Instructions
The MIPS architecture defines an optional floating-point coprocessor,
known as coprocessor 1. In early MIPS implementations, the floatingpoint coprocessor was a separate chip that users could purchase if
they needed fast floating-point math. In most recent MIPS implementations, the floating-point coprocessor is built in alongside the main
processor.
MIPS defines 32 32-bit floating-point registers, $f0–$f31. These are
separate from the ordinary registers used so far. MIPS supports both
single- and double-precision IEEE floating point arithmetic. Doubleprecision (64-bit) numbers are stored in pairs of 32-bit registers, so only
the 16 even-numbered registers ($f0, $f2, $f4, . . . , $f30) are used
to specify double-precision operations. By convention, certain registers
are reserved for certain purposes, as given in Table 6.8.
Floating-point instructions all have an opcode of 17 (100012). They
require both a funct field and a cop (coprocessor) field to indicate the
type of instruction. Hence, MIPS defines the F-type instruction format
for floating-point instructions, shown in Figure 6.35. Floating-point
instructions come in both single- and double-precision flavors. cop  16
(100002) for single-precision instructions or 17 (100012) for doubleprecision instructions. Like R-type instructions, F-type instructions have
two source operands, fs and ft, and one destination, fd.
Instruction precision is indicated by .s and .d in the mnemonic.
Floating-point arithmetic instructions include addition (add.s, add.d),
subtraction (sub.sz, sub.d), multiplication (mul.s, mul.d), and division (div.s, div.d) as well as negation (neg.s, neg.d) and absolute
value (abs.s, abs.d).
340 CHAPTER SIX Architecture
Table 6.8 MIPS floating-point register set
Name Number Use
$fv0–$fv1 0, 2 procedure return values
$ft0–$ft3 4, 6, 8, 10 temporary variables
$fa0–$fa1 12, 14 procedure arguments
$ft4–$ft5 16, 18 temporary variables
$fs0–$fs5 20, 22, 24, 26, 28, 30 saved variables
Figure 6.35 F-type machine
instruction format
C
Floating-point branches have two parts. First, a compare instruction
is used to set or clear the floating-point condition flag
(fpcond). Then, a
conditional branch checks the value of the flag. The compare instructions include equality (c.seq.s/c.seq.d), less than (c.lt.s/c.lt.d),
and less than or equal to (c.le.s/c.le.d). The conditional branch
instructions are bc1f and bc1t that branch if fpcond is FALSE or
TRUE, respectively. Inequality, greater than or equal to, and greater than
comparisons are performed with seq
, lt, and le, followed by bc1f
.
Floating-point registers are loaded and stored from memory using
lwc1 and swc1. These instructions move 32 bits, so two are necessary to
handle a double-precision number.
6.8 REAL-WORLD PERSPECTIVE: IA-32 ARCHITECTURE*
Almost all personal computers today use IA-32 architecture microprocessors. IA-32 is a 32-bit architecture originally developed by Intel.
AMD also sells IA-32 compatible microprocessors.
The IA-32 architecture has a long and convoluted history dating
back to 1978, when Intel announced the 16-bit 8086 microprocessor.
IBM selected the 8086 and its cousin, the 8088, for IBM’s first personal
computers. In 1985, Intel introduced the 32-bit 80386 microprocessor,
which was backward compatible with the 8086, so it could run software
developed for earlier PCs. Processor architectures compatible with the
80386 are called IA-32 or x86 processors. The Pentium, Core, and
Athlon processors are well known IA-32 processors. Section 7.9
describes the evolution of IA-32 microprocessors in more detail.
Various groups at Intel and AMD over many years have shoehorned
more instructions and capabilities into the antiquated architecture. The
result is far less elegant than MIPS. As Patterson and Hennessy explain,
“this checkered ancestry has led to an architecture that is difficult to
explain and impossible to love.” However, software compatibility is far
more important than technical elegance, so IA-32 has been the de facto
PC standard for more than two decades. More than 100 million IA-32
processors are sold every year. This huge market justifies more than
$5 billion of research and development annually to continue improving
the processors.
IA-32 is an example of a Complex Instruction Set Computer
(CISC
)
architecture. In contrast to RISC architectures such as MIPS, each CISC
instruction can do more work. Programs for CISC architectures usually
require fewer instructions. The instruction encodings were selected to be
more compact, so as to save memory, when RAM was far more expensive than it is today; instructions are of variable length and are often less
than 32 bits. The trade-off is that complicated instructions are more difficult to decode and tend to execute more slowly.
6.8 Real-World Perspective: IA-32 Architecture 341

This section introduces the IA-32 architecture. The goal is not to
make you into an IA-32 assembly language programmer, but rather to
illustrate some of the similarities and differences between IA-32 and MIPS.
We think it is interesting to see how IA-32 works. However, none of the
material in this section is needed to understand the rest of the book.
Major differences between IA-32 and MIPS are summarized in Table 6.9.
6.8.1 IA-32 Registers
The 8086 microprocessor provided eight 16-bit registers. It could separately access the upper and lower eight bits of some of these registers.
When the 32-bit 80386 was introduced, the registers were extended to
32 bits. These registers are called EAX, ECX, EDX, EBX, ESP, EBP,
ESI, and EDI. For backward compatibility, the bottom 16 bits and some
of the bottom 8-bit portions are also usable, as shown in Figure 6.36.
The eight registers are almost, but not quite, general purpose.
Certain instructions cannot use certain registers. Other instructions
always put their results in certain registers. Like $sp in MIPS, ESP is normally reserved for the stack pointer.
The IA-32 program counter is called EIP (the extended instruction
pointer). Like the MIPS PC, it advances from one instruction to the next
or can be changed with branch, jump, and subroutine call instructions.
6.8.2 IA-32 Operands
MIPS instructions always act on registers or immediates. Explicit load
and store instructions are needed to move data between memory and
the registers. In contrast, IA-32 instructions may operate on registers,
immediates, or memory. This partially compensates for the small set of
registers.
342 CHAPTER SIX Architecture
Table 6.9 Major differences between MIPS and IA-32
Feature MIPS IA-32
# of registers 32 general purpose 8, some restrictions on purpose
# of operands 3 (2 source, 1 destination) 2 (1 source, 1 source/destination)
operand location registers or immediates registers, immediates, or memory
operand size 32 bits 8, 16, or 32 bits
condition codes no yes
instruction types simple simple and complicated
instruction encoding fixed, 4 bytes variable, 1–15 bytes
Figure 6.36 IA-32 registers
EAX
0
AH
AX
ECX
1
CH
CX
EDX
2
Byte 3
Byte 2
Byte 1
Byte 0
DH
DX
EBX
3
BH
BX
ESP
4 SP
EBP
5 BP
ESI
6 SI
EDI
7 DI
AL
CL
DL
BL

MIPS instructions generally specify three operands: two sources
and one destination. IA-32 instructions specify only two operands.
The first is a source. The second is both a source and the destination.
Hence, IA-32 instructions always overwrite one of their sources
with the result. Table 6.10 lists the combinations of operand locations
in IA-32. All of the combinations are possible except memory to
memory.
Like MIPS, IA-32 has a 32-bit memory space that is byte-addressable.
However, IA-32 also supports a much wider variety of memory addressing
modes. The memory location is specified with any combination of a base
register, displacement, and a scaled index register. Table 6.11 illustrates
these combinations. The displacement can be an 8-, 16-, or 32-bit value.
The scale multiplying the index register can be 1, 2, 4, or 8. The base
displacement mode is equivalent to the MIPS base addressing mode for
loads and stores. The scaled index provides an easy way to access arrays
or structures of 2-, 4-, or 8-byte elements without having to issue a
sequence of instructions to generate the address.
6.8 Real-World Perspective: IA-32 Architecture 343
Table 6.10 Operand locations
Source/
Destination Source Example Meaning
register register add EAX, EBX EAX – EAX  EBX
register immediate add EAX, 42 EAX – EAX  42
register memory add EAX, [20] EAX – EAX  Mem[20]
memory register add [20], EAX Mem[20] – Mem[20]  EAX
memory immediate add [20], 42 Mem[20] – Mem[20]  42
Table 6.11 Memory addressing modes
Example Meaning Comment
add EAX, [20] EAX – EAX  Mem[20] displacement
add EAX, [ESP] EAX – EAX  Mem[ESP] base addressing
add EAX, [EDX40] EAX – EAX  Mem[EDX40] base  displacement
add EAX, [60EDI*4] EAX – EAX  Mem[60EDI*4] displacement  scaled index
add EAX, [EDX80EDI*2] EAX – EAX  Mem[EDX80EDI*2] base  displacement
scaled index
Chapter 06.qxd 1/31/07 
While MIPS always acts on 32-bit words, IA-32 instructions can
operate on 8-, 16-, or 32-bit data. Table 6.12 illustrates these variations.
6.8.3 Status Flags
IA-32, like many CISC architectures, uses status flags (also called condition codes) to make decisions about branches and to keep track of carries and arithmetic overflow. IA-32 uses a 32-bit register, called EFLAGS,
that stores the status flags. Some of the bits of the EFLAGS register are
given in Table 6.13. Other bits are used by the operating system.
The architectural state of an IA-32 processor includes EFLAGS as
well as the eight registers and the EIP.
6.8.4 IA-32 Instructions
IA-32 has a larger set of instructions than MIPS. Table 6.14 describes some
of the general purpose instructions. IA-32 also has instructions for floatingpoint arithmetic and for arithmetic on multiple short data elements packed
into a longer word. D indicates the destination (a register or memory location), and S indicates the source (a register, memory location, or immediate).
Note that some instructions always act on specific registers. For
example, 32  32-bit multiplication always takes one of the sources
from EAX and always puts the 64-bit result in EDX and EAX. LOOP always
344 CHAPTER SIX Architecture
Table 6.12 Instructions acting on 8-, 16-, or 32-bit data
Example Meaning Data Size
add AH, BL AH – AH  BL 8-bit
add AX, 1 AX – AX  0xFFFF 16-bit
add EAX, EDX EAX – EAX  EDX 32-bit
Table 6.13 Selected EFLAGS
Name Meaning
CF (Carry Flag) Carry out generated by last arithmetic operation.
Indicates overflow in unsigned arithmetic.
Also used for propagating the carry between
words in multiple-precision arithmetic.
ZF (Zero Flag) Result of last operation was zero.
SF (Sign Flag) Result of last operation was negative (msb  1).
OF (Overflow Flag) Overflow of two’s complement arithmetic.
Chap
6.8 Real World Perspective: IA-32 Architecture 345
Table 6.14 Selected IA-32 instructions
Instruction Meaning Function
ADD/SUB add/subtract D  D  S / D  D  S
ADDC add with carry D  D  S  CF
INC/DEC increment/decrement D  D  1 / D  D  1
CMP compare Set flags based on D  S
NEG negate D  D
AND/OR/XOR logical AND/OR/XOR D  D op S
NOT logical NOT D  D

IMUL/MUL signed/unsigned multiply EDX:EAX  EAX  D
IDIV/DIV signed/unsigned divide EDX:EAX/D
EAX  Quotient; EDX  Remainder
SAR/SHR arithmetic/logical shift right D  D  S / D  D  S
SAL/SHL left shift D  D  S
ROR/ROL rotate right/left Rotate D by S
RCR/RCL rotate right/left with carry Rotate CF and D by S
BT bit test CF  D[S] (the Sth bit of D)
BTR/BTS bit test and reset/set CF  D[S]; D[S]  0 / 1
TEST set flags based on masked bits Set flags based on D AND S
MOV move D  S
PUSH push onto stack ESP  ESP  4; Mem[ESP]  S
POP pop off stack D  MEM[ESP]; ESP  ESP  4
CLC, STC clear/set carry flag CF  0 / 1
JMP unconditional jump relative jump: EIP  EIP  S
absolute jump: EIP  S
Jcc conditional jump if (flag) EIP  EIP  S
LOOP loop ECX  ECX  1
if ECX  0 EIP  EIP  imm
CALL procedure call ESP  ESP  4;
MEM[ESP]  EIP; EIP  S
RET procedure return EIP  MEM[ESP]; ESP  ESP  4
Chapter 06.qxd 1/31/07 8:21 PM Page 345
346 CHAPTER SIX Architecture
2 It is possible to construct 17-byte instructions if all the optional fields are used.
However, IA-32 places a 15-byte limit on the length of legal instructions.
Table 6.15 Selected branch conditions
Instruction Meaning Function After cmp d, s
JZ/JE jump if ZF  1 jump if D  S
JNZ/JNE jump if ZF  0 jump if D  S
JGE jump if SF  OF jump if D  S
JG jump if SF  OF and ZF  0 jump if D  S
JLE jump if SF  OF or ZF  1 jump if D  S
JL jump if SF  OF jump if D  S
JC/JB jump if CF  1
JNC jump if CF  0
JO jump if OF  1
JNO jump if OF  0
JS jump if SF  1
JNS jump if SF  0
stores the loop counter in ECX. PUSH, POP, CALL, and RET use the stack
pointer, ESP.
Conditional jumps check the flags and branch if the appropriate
condition is met. They come in many flavors. For example, JZ jumps if
the zero flag (ZF) is 1. JNZ jumps if the zero flag is 0. The jumps usually
follow an instruction, such as the compare instruction (CMP), that sets
the flags. Table 6.15 lists some of the conditional jumps and how they
depend on the flags set by a prior compare operation.
6.8.5 IA-32 Instruction Encoding
The IA-32 instruction encodings are truly messy, a legacy of decades of
piecemeal changes. Unlike MIPS, whose instructions are uniformly 32
bits, IA-32 instructions vary from 1 to 15 bytes, as shown in Figure
6.37.2 The opcode may be 1, 2, or 3 bytes. It is followed by four
optional fields: ModR/M, SIB, Displacement, and Immediate. ModR/M
specifies an addressing mode. SIB specifies the scale, index, and base
Chapter 06.qx
6.8 Real World Perspective: IA-32 Architecture 347
registers in certain addressing modes. Displacement indicates a 1-, 2-,
or 4-byte displacement in certain addressing modes. And Immediate is a
1-, 2-, or 4-byte constant for instructions using an immediate as the
source operand. Moreover, an instruction can be preceded by up to four
optional byte-long prefixes that modify its behavior.
The ModR/M byte uses the 2-bit Mod and 3-bit R/M field to specify the
addressing mode for one of the operands. The operand can come from
one of the eight registers, or from one of 24 memory addressing modes.
Due to artifacts in the encodings, the ESP and EBP registers are not available for use as the base or index register in certain addressing modes.
The Reg field specifies the register used as the other operand. For certain
instructions that do not require a second operand, the Reg field is used
to specify three more bits of the opcode.
In addressing modes using a scaled index register, the SIB byte specifies the index register and the scale (1, 2, 4, or 8). If both a base and
index are used, the SIB byte also specifies the base register.
MIPS fully specifies the instruction in the opcode and funct fields
of the instruction. IA-32 uses a variable number of bits to specify different instructions. It uses fewer bits to specify more common instructions, decreasing the average length of the instructions. Some
instructions even have multiple opcodes. For example, add AL, imm8
performs an 8-bit add of an immediate to AL. It is represented with the
1-byte opcode, 0x04, followed by a 1-byte immediate. The A register
(AL, AX, or EAX) is called the accumulator. On the other hand, add D,
imm8 performs an 8-bit add of an immediate to an arbitrary destination, D (memory or a register). It is represented with the 1-byte opcode,
0x80, followed by one or more bytes specifying D, followed by a 1-byte
immediate. Many instructions have shortened encodings when the destination is the accumulator.
In the original 8086, the opcode specified whether the instruction
acted on 8- or 16-bit operands. When the 80386 introduced 32-bit
operands, no new opcodes were available to specify the 32-bit form.
Prefixes ModR/M SIB Displacement Immediate
Up to 4 optional
prefixes
of 1 byte each
1-, 2-, or 3-byte
opcode
1 byte
(for certain
addressing
modes)
1 byte
(for certain
addressing
modes)
1, 2, or 4 bytes
for addressing
modes with
displacement
1, 2, or 4 bytes
for addressing
modes with
immediate
Mod R/M Scale Index Base Reg/
Opcode
Opcode
2 bits 3 bits 3 bits 2 bits 3 bits 3 bits
Figure 6.37 IA-32 instruction encodings

Instead, the same opcode was used for both 16- and 32-bit forms.
An additional bit in the code segment descriptor used by the OS specifies
which form the processor should choose. The bit is set to 0 for backward compatibility with 8086 programs, defaulting the opcode to 16-bit
operands. It is set to 1 for programs to default to 32-bit operands.
Moreover, the programmer can specify prefixes to change the form for a
particular instruction. If the prefix 0x66 appears before the opcode, the
alternative size operand is used (16 bits in 32-bit mode, or 32 bits in
16-bit mode).
6.8.6 Other IA-32 Peculiarities
The 80286 introduced segmentation to divide memory into segments
of up to 64 KB in length. When the OS enables segmentation,
addresses are computed relative to the beginning of the segment. The
processor checks for addresses that go beyond the end of the segment
and indicates an error, thus preventing programs from accessing memory outside their own segment. Segmentation proved to be a hassle for
programmers and is not used in modern versions of the Windows operating system.
IA-32 contains string instructions that act on entire strings of bytes
or words. The operations include moving, comparing, or scanning for a
specific value. In modern processors, these instructions are usually
slower than performing the equivalent operation with a series of simpler
instructions, so they are best avoided.
As mentioned earlier, the 0x66 prefix is used to choose between
16- and 32-bit operand sizes. Other prefixes include ones used to lock
the bus (to control access to shared variables in a multiprocessor system), to predict whether a branch will be taken or not, and to repeat the
instruction during a string move.
The bane of any architecture is to run out of memory capacity. With
32-bit addresses, IA-32 can access 4 GB of memory. This was far more
than the largest computers had in 1985, but by the early 2000s it had
become limiting. In 2003, AMD extended the address space and register
sizes to 64 bits, calling the enhanced architecture AMD64. AMD64 has
a compatibility mode that allows it to run 32-bit programs unmodified
while the OS takes advantage of the bigger address space. In 2004, Intel
gave in and adopted the 64-bit extensions, renaming them Extended
Memory 64 Technology (EM64T). With 64-bit addresses, computers can
access 16 exabytes (16 billion GB) of memory.
For those curious about more details of the IA-32 architecture, the
IA-32 Intel Architecture Software Developer’s Manual, is freely available
on Intel’s Web site.
348 CHAPTER SIX Architecture
Intel and Hewlett-Packard
jointly developed a new 64-bit
architecture called IA-64 in
the mid 1990’s. It was
designed from a clean slate,
bypassing the convoluted
history of IA-32, taking
advantage of 20 years of new
research in computer architecture, and providing a 64-bit
address space. However,
IA-64 has yet to become a
market success. Most computers needing the large address
space now use the 64-bit
extensions of IA-32.

6.8.7 The Big Picture
This section has given a taste of some of the differences between the MIPS
RISC architecture and the IA-32 CISC architecture. IA-32 tends to have
shorter programs, because a complex instruction is equivalent to a series
of simple MIPS instructions and because the instructions are encoded to
minimize memory use. However, the IA-32 architecture is a hodgepodge
of features accumulated over the years, some of which are no longer useful
but must be kept for compatibility with old programs. It has too few registers, and the instructions are difficult to decode. Merely explaining the
instruction set is difficult. Despite all these failings, IA-32 is firmly
entrenched as the dominant computer architecture for PCs, because the
value of software compatibility is so great and because the huge market
justifies the effort required to build fast IA-32 microprocessors.
6.9 SUMMARY
To command a computer, you must speak its language. A computer
architecture defines how to command a processor. Many different computer architectures are in widespread commercial use today, but once
you understand one, learning others is much easier. The key questions to
ask when approaching a new architecture are
 What is the data word length?
 What are the registers?
 How is memory organized?
 What are the instructions?
MIPS is a 32-bit architecture because it operates on 32-bit data. The
MIPS architecture has 32 general-purpose registers. In principle, almost
any register can be used for any purpose. However, by convention, certain
registers are reserved for certain purposes, for ease of programming and so
that procedures written by different programmers can communicate easily.
For example, register 0, $0, always holds the constant 0, $ra holds
the return address after a jal instruction, and $a0–$a3 and $v0 – $v1
hold the arguments and return value of a procedure. MIPS has a byteaddressable memory system with 32-bit addresses. The memory map was
described in Section 6.6.1. Instructions are 32 bits long and must be
word aligned. This chapter discussed the most commonly used MIPS
instructions.
The power of defining a computer architecture is that a program
written for any given architecture can run on many different implementations of that architecture. For example, programs written for the Intel
6.9 Summary 349
Chap
Pentium processor in 1993 will generally still run (and run much faster)
on the Intel Core 2 Duo or AMD Athlon processors in 2006.
In the first part of this book, we learned about the circuit and
logic levels of abstraction. In this chapter, we jumped up to the architecture level. In the next chapter, we study microarchitecture, the
arrangement of digital building blocks that implement a processor
architecture. Microarchitecture is the link between hardware and software engineering. And, we believe it is one of the most exciting topics
in all of engineering: you will learn to build your own microprocessor!
350 CHAPTER SIX Architecture

Exercises
Exercise 6.1 Give three examples from the MIPS architecture of each of the
architecture design principles: (1) simplicity favors regularity; (2) make the common case fast; (3) smaller is faster; and (4) good design demands good compromises. Explain how each of your examples exhibits the design principle.
Exercise 6.2 The MIPS architecture has a register set that consists of 32 32-bit
registers. Is it possible to design a computer architecture without a register
set? If so, briefly describe the architecture, including the instruction set.
What are advantages and disadvantages of this architecture over the MIPS
architecture?
Exercise 6.3 Consider memory storage of a 32-bit word stored at memory word
42 in a byte addressable memory.
(a) What is the byte address of memory word 42?
(b) What are the byte addresses that memory word 42 spans?
(c) Draw the number 0xFF223344 stored at word 42 in both big-endian and
little-endian machines. Your drawing should be similar to Figure 6.4.
Clearly label the byte address corresponding to each data byte value.
Exercise 6.4 Explain how the following program can be used to determine
whether a computer is big-endian or little-endian:
li $t0, 0xABCD9876
sw $t0, 100($0)
lb $s5, 101($0)
Exercise 6.5 Write the following strings using ASCII encoding. Write your final
answers in hexadecimal.
(a) SOS
(b) Cool!
(c) (your own name)
Exercise 6.6 Show how the strings in Exercise 6.5 are stored in a byte-addressable
memory on (a) a big-endian machine and (b) a little-endian machine starting at
memory address 0x1000100C. Use a memory diagram similar to Figure 6.4.
Clearly indicate the memory address of each byte on each machine.
Exercises 351

Exercise 6.7 Convert the following MIPS assembly code into machine language.
Write the instructions in hexadecimal.
add $t0, $s0, $s1
lw $t0, 0x20($t7)
addi $s0, $0, 10
Exercise 6.8 Repeat Exercise 6.7 for the following MIPS assembly code:
addi $s0, $0, 73
sw $t1, 7($t2)
sub $t1, $s7, $s2
Exercise 6.9 Consider I-type instructions.
(a) Which instructions from Exercise 6.8 are I-type instructions?
(b) Sign-extend the 16-bit immediate of each instruction from part (a) so that it
becomes a 32-bit number.
Exercise 6.10 Convert the following program from machine language into
MIPS assembly language. The numbers on the left are the instruction address
in memory, and the numbers on the right give the instruction at that address.
Then reverse engineer a high-level program that would compile into this
assembly language routine and write it. Explain in words what the program
does. $a0 is the input, and it initially contains a positive number, n. $v0 is
the output.
0x00400000 0x20080000
0x00400004 0x20090001
0x00400008 0x0089502a
0x0040000c 0x15400003
0x00400010 0x01094020
0x00400014 0x21290002
0x00400018 0x08100002
0x0040001c 0x01001020
Exercise 6.11 The nori instruction is not part of the MIPS instruction set,
because the same functionality can be implemented using existing instructions.
Write a short assembly code snippet that has the following functionality:
$t0  $t1 NOR 0xF234. Use as few instructions as possible.
352 CHAPTER SIX Architecture
C
Exercise 6.12 Implement the following high-level code segments using the slt
instruction. Assume the integer variables g and h are in registers $s0 and $s1,
respectively.
(a) if (g  h)
g  g  h;
else
g  g  h;
(b) if (g  h)
g  g  1;
else
h  h  1;
(c) if (g  h)
g  0;
else
h  0;
Exercise 6.13 Write a procedure in a high-level language for int find42(int
array[], int size). size specifies the number of elements in the array. array
specifies the base address of the array. The procedure should return the index
number of the first array entry that holds the value 42. If no array entry is 42, it
should return the value 1.
Exercise 6.14 The high-level procedure strcpy copies the character string x to
the character string y.
// high-level code
void strcpy(char x[], char y[]) {
int i  0;
while (x[i] ! 0) {
y[i]  x[i];
i  i  1;
}
}
(a) Implement the strcpy procedure in MIPS assembly code. Use $s0 for i.
(b) Draw a picture of the stack before, during, and after the strcpy procedure
call. Assume $sp  0x7FFFFF00 just before strcpy is called.
Exercise 6.15 Convert the high-level procedure from Exercise 6.13 into MIPS
assembly code.
Exercises 353
This simple string copy program has a serious flaw: it has
no way of knowing that y has
enough space to receive x. If a
malicious programmer were
able to execute strcpy with
a long string x, the programmer might be able to write
bytes all over memory, possibly even modifying code
stored in subsequent memory
locations. With some cleverness, the modified code might
take over the machine. This
is called a buffer overflow
attack; it is employed by
several nasty programs,
including the infamous Blaster
worm, which caused an estimated $525 million in damages in 2003.
Chapter 06.qxd 1
Exercise 6.16 Each number in the Fibonacci series is the sum of the previous two
numbers. Table 6.16 lists the first few numbers in the series, fib(n).
(a) What is fib(n) for n  0 and n  1?
(b) Write a procedure called fib in a high-level language that returns the
Fibonacci number for any nonnegative value of n. Hint: You probably will
want to use a loop. Clearly comment your code.
(c) Convert the high-level procedure of part (b) into MIPS assembly code. Add
comments after every line of code that explain clearly what it does. Use the
SPIM simulator to test your code on fib(9).
Exercise 6.17 Consider the MIPS assembly code below. proc1, proc2, and
proc3 are non-leaf procedures. proc4 is a leaf procedure. The code is not shown
for each procedure, but the comments indicate which registers are used within
each procedure.
0x00401000 proc1: . . . # proc1 uses $s0 and $s1
0x00401020 jal proc2
...
0x00401100 proc2: . . . # proc2 uses $s2 – $s7
0x0040117C jal proc3
...
0x00401400 proc3: . . . # proc3 uses $s1 – $s3
0x00401704 jal proc4
...
0x00403008 proc4: . . . # proc4 uses no preserved
# registers
0x00403118 jr $ra
(a) How many words are the stack frames of each procedure?
(b) Sketch the stack after proc4 is called. Clearly indicate which registers are
stored where on the stack. Give values where possible.
354 CHAPTER SIX Architecture
Table 6.16 Fibonacci series
n 1 2 3 4 5 6 7 8 9 10 11 . . .
fib(n) 1 1 2 3 5 8 13 21 34 55 89 . . .
Ch
Exercise 6.18 Ben Bitdiddle is trying to compute the function f(a, b)  2a  3b
for nonnegative b. He goes overboard in the use of procedure calls and recursion
and produces the following high-level code for procedures f and f2.
// high-level code for procedures f and f2
int f(int a, int b) {
int j;
j  a;
return j  a  f2(b);
}
int f2(int x)
{
int k;
k  3;
if (x  0) return 0;
else return k  f2(x1);
}
Ben then translates the two procedures into assembly language as follows.
He also writes a procedure, test, that calls the procedure f(5, 3).
# MIPS assembly code
# f: $a0  a, $a1  b, $s0  j f2: $a0  x, $s0  k
0x00400000 test: addi $a0, $0, 5 # $a0  5 (a  5)
0x00400004 addi $a1, $0, 3 # $a1  3 (b  3)
0x00400008 jal f # call f(5,3)
0x0040000c loop: j loop # and loop forever
0x00400010 f: addi $sp, $sp, 16 # make room on the stack
# for $s0, $a0, $a1, and $ra
0x00400014 sw $a1, 12($sp) # save $a1 (b)
0x00400018 sw $a0, 8($sp) # save $a0 (a)
0x0040001c sw $ra, 4($sp) # save $ra
0x00400020 sw $s0, 0($sp) # save $s0
0x00400024 add $s0, $a0, $0 # $s0  $a0 (j  a)
0x00400028 add $a0, $a1, $0 # place b as argument for f2
0x0040002c jal f2 # call f2(b)
0x00400030 lw $a0, 8($sp) # restore $a0 (a) after call
0x00400034 lw $a1, 12($sp) # restore $a1 (b) after call
0x00400038 add $v0, $v0, $s0 # $v0  f2(b)  j
0x0040003c add $v0, $v0, $a0 # $v0  (f2(b)  j)  a
0x00400040 lw $s0, 0($sp) # restore $s0
0x00400044 lw $ra, 4($sp) # restore $ra
0x00400048 addi $sp, $sp, 16 # restore $sp (stack pointer)
0x0040004c jr $ra # return to point of call
0x00400050 f2: addi $sp, $sp, 12 # make room on the stack for
# $s0, $a0, and $ra
0x00400054 sw $a0, 8($sp) # save $a0 (x)
0x00400058 sw $ra, 4($sp) # save return address
0x0040005c sw $s0, 0($sp) # save $s0
0x00400060 addi $s0, $0, 3 # k  3
Exercises 355
Chapter 06.qxd 1/31/07 8:2
0x00400064 bne $a0, $0, else # x  0?
0x00400068 addi $v0, $0, 0 # yes: return value should be 0
0x0040006c j done # and clean up
0x00400070 else: addi $a0, $a0, 1 # no: $a0  $a0  1 (x  x  1)
0x00400074 jal f2 # call f2(x  1)
0x00400078 lw $a0, 8($sp) # restore $a0 (x)
0x0040007c add $v0, $v0, $s0 # $v0  f2(x  1)  k
0x00400080 done: lw $s0, 0($sp) # restore $s0
0x00400084 lw $ra, 4($sp) # restore $ra
0x00400088 addi $sp, $sp, 12 # restore $sp
0x0040008c jr $ra # return to point of call
You will probably find it useful to make drawings of the stack similar to the
one in Figure 6.26 to help you answer the following questions.
(a) If the code runs starting at test, what value is in $v0 when the program
gets to loop? Does his program correctly compute 2a  3b?
(b) Suppose Ben deletes the instructions at addresses 0x0040001C and
0x00400040 that save and restore $ra. Will the program (1) enter an infinite loop but not crash; (2) crash (cause the stack to grow beyond the
dynamic data segment or the PC to jump to a location outside the program);
(3) produce an incorrect value in $v0 when the program returns to loop (if
so, what value?), or (4) run correctly despite the deleted lines?
(c) Repeat part (b) when the instructions at the following instruction addresses
are deleted:
(i) 0x00400018 and 0x00400030 (instructions that save and restore $a0)
(ii) 0x00400014 and 0x00400034 (instructions that save and restore $a1)
(iii) 0x00400020 and 0x00400040 (instructions that save and restore $s0)
(iv) 0x00400050 and 0x00400088 (instructions that save and restore $sp)
(v) 0x0040005C and 0x00400080 (instructions that save and restore $s0)
(vi) 0x00400058 and 0x00400084 (instructions that save and restore $ra)
(vii) 0x00400054 and 0x00400078 (instructions that save and restore $a0)
Exercise 6.19 Convert the following beq, j, and jal assembly instructions into
machine code. Instruction addresses are given to the left of each instruction.
(a)
0x00401000 beq $t0, $s1, Loop
0x00401004 . . .
0x00401008 . . .
0x0040100C Loop: . . .
356 CHAPTER SIX Architecture
Chapte
(b)
0x00401000 beq $t7, $s4, done
... ...
0x00402040 done: . . .
(c)
0x0040310C back: . . .
... ...
0x00405000 beq $t9, $s7, back
(d)
0x00403000 jal proc
... ...
0x0041147C proc: . . .
(e)
0x00403004 back: . . .
... ...
0x0040400C j back
Exercise 6.20 Consider the following MIPS assembly language snippet. The
numbers to the left of each instruction indicate the instruction address.
0x00400028 add $a0, $a1, $0
0x0040002c jal f2
0x00400030 f1: jr $ra
0x00400034 f2: sw $s0, 0($s2)
0x00400038 bne $a0, $0, else
0x0040003c j f1
0x00400040 else: addi $a0, $a0, 1
0x00400044 j f2
(a) Translate the instruction sequence into machine code. Write the machine
code instructions in hexadecimal.
(b) List the addressing mode used at each line of code.
Exercise 6.21 Consider the following C code snippet.
// C code
void set_array(int num) {
int i;
int array[10];
for (i  0; i  10; i  i  1) {
array[i]  compare(num, i);
}
}
int compare(int a, int b) {
if (sub(a, b) > 0)
return 1;
else
Exercises 357
Chapt
return 0;
}
int sub (int a, int b) {
return a  b;
}
(a) Implement the C code snippet in MIPS assembly language. Use $s0 to hold
the variable i. Be sure to handle the stack pointer appropriately. The array
is stored on the stack of the set_array procedure (see Section 6.4.6).
(b) Assume set_array is the first procedure called. Draw the status of the
stack before calling set_array and during each procedure call. Indicate the
names of registers and variables stored on the stack and mark the location
of $sp.
(c) How would your code function if you failed to store $ra on the stack?
Exercise 6.22 Consider the following high-level procedure.
// high-level code
int f(int n, int k) {
int b;
b  k  2;
if (n  0) b  10;
else b  b  (n * n)  f(n  1, k  1);
return b * k;
}
(a) Translate the high-level procedure f into MIPS assembly language. Pay particular attention to properly saving and restoring registers across procedure calls
and using the MIPS preserved register conventions. Clearly comment your
code. You can use the MIPS mult, mfhi, and mflo instructions. The procedure starts at instruction address 0x00400100. Keep local variable b in $s0.
(b) Step through your program from part (a) by hand for the case of f(2, 4).
Draw a picture of the stack similar to the one in Figure 6.26(c). Write the
register name and data value stored at each location in the stack and keep
track of the stack pointer value ($sp). You might also find it useful to keep
track of the values in $a0, $a1, $v0, and $s0 throughout execution. Assume
that when f is called, $s0  0xABCD and $ra  0x400004. What is the
final value of $v0?
Exercise 6.23 What is the range of instruction addresses to which conditional
branches, such as beq and bne, can branch in MIPS? Give your answer in number of instructions relative to the conditional branch instruction.
358 CHAPTER SIX Architecture
Chapter 06.
Exercise 6.24 The following questions examine the limitations of the jump instruction, j. Give your answer in number of instructions relative to the jump instruction.
(a) In the worst case, how far can the jump instruction (j) jump forward
(i.e., to higher addresses)? (The worst case is when the jump instruction
cannot jump far.) Explain using words and examples, as needed.
(b) In the best case, how far can the jump instruction (j) jump forward? (The
best case is when the jump instruction can jump the farthest.) Explain.
(c) In the worst case, how far can the jump instruction (j) jump backward (to
lower addresses)? Explain.
(d) In the best case, how far can the jump instruction (j) jump backward? Explain.
Exercise 6.25 Explain why it is advantageous to have a large address field,
addr, in the machine format for the jump instructions, j and jal.
Exercise 6.26 Write assembly code that jumps to the instruction 64
Minstructions from the first instruction. Recall that 1 Minstruction  220
instructions  1,048,576 instructions. Assume that your code begins at address
0x00400000. Use a minimum number of instructions.
Exercise 6.27 Write a procedure in high-level code that takes a ten-entry array
of 32-bit integers stored in little-endian format and converts it to big-endian format. After writing the high-level code, convert it to MIPS assembly code.
Comment all your code and use a minimum number of instructions.
Exercise 6.28 Consider two strings: string1 and string2.
(a) Write high-level code for a procedure called concat that concatenates (joins
together) the two strings: void concat(char[] string1, char[] string2,
char[] stringconcat). The procedure does not return a value. It concatenates string1 and string2 and places the resulting string in stringconcat.
You may assume that the character array stringconcat is large enough to
accommodate the concatenated string.
(b) Convert the procedure from part (a) into MIPS assembly language.
Exercise 6.29 Write a MIPS assembly program that adds two positive singleprecision floating point numbers held in $s0 and $s1. Do not use any of the
MIPS floating-point instructions. You need not worry about any of the encodings that are reserved for special purposes (e.g., 0, NANs, INF) or numbers that
overflow or underflow. Use the SPIM simulator to test your code. You will need
to manually set the values of $s0 and $s1 to test your code. Demonstrate that
your code functions reliably.
Exercises 359
Ch
Exercise 6.30 Show how the following MIPS program would be loaded into
memory and executed.
# MIPS assembly code
main:
lw $a0, x
lw $a1, y
jal diff
jr $ra
diff:
sub $v0, $a0, $a1
jr $ra
(a) First show the instruction address next to each assembly instruction.
(b) Draw the symbol table showing the labels and their addresses.
(c) Convert all instructions into machine code.
(d) How big (how many bytes) are the data and text segments?
(e) Sketch a memory map showing where data and instructions are stored.
Exercise 6.31 Show the MIPS instructions that implement the following
pseudoinstructions. You may use the assembler register, $at, but you may not
corrupt (overwrite) any other registers.
(a) beq $t1, imm31:0, L
(b) ble $t3, $t5, L
(c) bgt $t3, $t5, L
(d) bge $t3, $t5, L
(e) addi $t0, $2, imm31:0
(f) lw $t5, imm31:0($s0)
(g) rol $t0, $t1, 5 (rotate $t1 left by 5 and put the result in $t0)
(h) ror $s4, $t6, 31 (rotate $t6 right by 31 and put the result in $s4)
360 CHAPTER SIX Architecture

Interview Questions
The following exercises present questions that have been asked at
interviews for digital design jobs (but are usually open to any assembly
language).
Question 6.1 Write MIPS assembly code for swapping the contents of two
registers, $t0 and $t1. You may not use any other registers.
Question 6.2 Suppose you are given an array of both positive and negative
integers. Write MIPS assembly code that finds the subset of the array with the
largest sum. Assume that the array’s base address and the number of array
elements are in $a0 and $a1, respectively. Your code should place the resulting
subset of the array starting at base address $a2. Write code that runs as fast as
possible.
Question 6.3 You are given an array that holds a C string. The string forms a
sentence. Design an algorithm for reversing the words in the sentence and storing
the new sentence back in the array. Implement your algorithm using MIPS
assembly code.
Question 6.4 Design an algorithm for counting the number of 1’s in a 32-bit
number. Implement your algorithm using MIPS assembly code.
Question 6.5 Write MIPS assembly code to reverse the bits in a register. Use as
few instructions as possible. Assume the register of interest is $t3.
Question 6.6 Write MIPS assembly code to test whether overflow occurs when
$t2 and $t3 are added. Use a minimum number of instructions.
Question 6.7 Design an algorithm for testing whether a given string is a palindrome. (Recall, that a palindrome is a word that is the same forward and backward. For example, the words “wow” and “racecar” are palindromes.)
Implement your algorithm using MIPS assembly code.
Interview Questions 361


7
7.1 Introduction
7.2 Performance Analysis
7.3 Single-Cycle Processor
7.4 Multicycle Processor
7.5 Pipelined Processor
7.6 HDL Representation*
7.7 Exceptions*
7.8 Advanced
Microarchitecture*
7.9 Real-World Perspective:
IA-32 Microarchitecture*
7.10 Summary
Exercises
Interview Questions
Microarchitecture
7.1 INTRODUCTION
In this chapter, you will learn how to piece together a MIPS microprocessor. Indeed, you will puzzle out three different versions, each with
different trade-offs between performance, cost, and complexity.
To the uninitiated, building a microprocessor may seem like black
magic. But it is actually relatively straightforward, and by this point you
have learned everything you need to know. Specifically, you have learned
to design combinational and sequential logic given functional and timing
specifications. You are familiar with circuits for arithmetic and memory.
And you have learned about the MIPS architecture, which specifies the
programmer’s view of the MIPS processor in terms of registers, instructions, and memory.
This chapter covers microarchitecture, which is the connection
between logic and architecture. Microarchitecture is the specific arrangement of registers, ALUs, finite state machines (FSMs), memories, and
other logic building blocks needed to implement an architecture. A particular architecture, such as MIPS, may have many different microarchitectures, each with different trade-offs of performance, cost, and
complexity. They all run the same programs, but their internal designs
vary widely. We will design three different microarchitectures in this
chapter to illustrate the trade-offs.
This chapter draws heavily on David Patterson and John Hennessy’s
classic MIPS designs in their text Computer Organization and Design.
They have generously shared their elegant designs, which have the virtue
of illustrating a real commercial architecture while being relatively simple and easy to understand.
7.1.1 Architectural State and Instruction Set
Recall that a computer architecture is defined by its instruction set and
architectural state. The architectural state for the MIPS processor consists
363

of the program counter and the 32 registers. Any MIPS microarchitecture
must contain all of this state. Based on the current architectural state, the
processor executes a particular instruction with a particular set of data to
produce a new architectural state. Some microarchitectures contain
additional nonarchitectural state to either simplify the logic or improve
performance; we will point this out as it arises.
To keep the microarchitectures easy to understand, we consider only
a subset of the MIPS instruction set. Specifically, we handle the following
instructions:
 R-type arithmetic/logic instructions: add, sub, and, or, slt
 Memory instructions: lw, sw
 Branches: beq
After building the microarchitectures with these instructions, we extend
them to handle addi and j. These particular instructions were chosen
because they are sufficient to write many interesting programs. Once you
understand how to implement these instructions, you can expand the
hardware to handle others.
7.1.2 Design Process
We will divide our microarchitectures into two interacting parts: the
datapath and the control. The datapath operates on words of data. It
contains structures such as memories, registers, ALUs, and multiplexers.
MIPS is a 32-bit architecture, so we will use a 32-bit datapath. The control unit receives the current instruction from the datapath and tells the
datapath how to execute that instruction. Specifically, the control unit
produces multiplexer select, register enable, and memory write signals to
control the operation of the datapath.
A good way to design a complex system is to start with hardware
containing the state elements. These elements include the memories and
the architectural state (the program counter and registers). Then, add
blocks of combinational logic between the state elements to compute the
new state based on the current state. The instruction is read from part of
memory; load and store instructions then read or write data from
another part of memory. Hence, it is often convenient to partition the
overall memory into two smaller memories, one containing instructions
and the other containing data. Figure 7.1 shows a block diagram with
the four state elements: the program counter, register file, and instruction
and data memories.
In Figure 7.1, heavy lines are used to indicate 32-bit data busses.
Medium lines are used to indicate narrower busses, such as the 5-bit
address busses on the register file. Narrow blue lines are used to indicate
364 CHAPTER SEVEN Microarchitecture
David Patterson was the first
in his family to graduate from
college (UCLA, 1969). He has
been a professor of computer
science at UC Berkeley since
1977, where he coinvented
RISC, the Reduced Instruction
Set Computer. In 1984, he
developed the SPARC architecture used by Sun Microsystems. He is also the father
of RAID (Redundant Array of
Inexpensive Disks) and NOW
(Network of Workstations).
John Hennessy is president
of Stanford University and
has been a professor of electrical engineering and computer science there since
1977. He coinvented RISC.
He developed the MIPS architecture at Stanford in 1984
and cofounded MIPS Computer Systems. As of 2004, more
than 300 million MIPS microprocessors have been sold.
In their copious free time,
these two modern paragons
write textbooks for recreation
and relaxation.
Cha
control signals, such as the register file write enable. We will use this
convention throughout the chapter to avoid cluttering diagrams with bus
widths. Also, state elements usually have a reset input to put them into a
known state at start-up. Again, to save clutter, this reset is not shown.
The program counter is an ordinary 32-bit register. Its output, PC,
points to the current instruction. Its input, PC, indicates the address of
the next instruction.
The instruction memory has a single read port.1 It takes a 32-bit
instruction address input, A, and reads the 32-bit data (i.e., instruction)
from that address onto the read data output, RD.
The 32-element  32-bit register file has two read ports and one write
port. The read ports take 5-bit address inputs, A1 and A2, each specifying
one of 25  32 registers as source operands. They read the 32-bit register
values onto read data outputs RD1 and RD2, respectively. The write port
takes a 5-bit address input, A3; a 32-bit write data input, WD; a write
enable input, WE3; and a clock. If the write enable is 1, the register file
writes the data into the specified register on the rising edge of the clock.
The data memory has a single read/write port. If the write enable,
WE, is 1, it writes data WD into address A on the rising edge of the
clock. If the write enable is 0, it reads address A onto RD.
The instruction memory, register file, and data memory are all read
combinationally. In other words, if the address changes, the new data
appears at RD after some propagation delay; no clock is involved. They
are written only on the rising edge of the clock. In this fashion, the state
of the system is changed only at the clock edge. The address, data, and
write enable must setup sometime before the clock edge and must
remain stable until a hold time after the clock edge.
Because the state elements change their state only on the rising edge of
the clock, they are synchronous sequential circuits. The microprocessor is
7.1 Introduction 365
Resetting the PC
At the very least, the program
counter must have a reset
signal to initialize its value
when the processor turns on.
MIPS processors initialize the
PC to 0xBFC00000 on reset and
begin executing code to start
up the operating system (OS).
The OS then loads an application program at 0x00400000
and begins executing it.
For simplicity in this chapter,
we will reset the PC to
0x00000000 and place our
programs there instead.
1 This is an oversimplification used to treat the instruction memory as a ROM; in most
real processors, the instruction memory must be writable so that the OS can load a new
program into memory. The multicycle microarchitecture described in Section 7.4 is more
realistic in that it uses a combined memory for instructions and data that can be both read
and written.
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1
WE3
A2
CLK
Register
File
A RD
Data
Memory
WD
PC' PC WE
CLK
32 32
32 32
32
32
32 32
32
32
5
5
5
Figure 7.1 State elements of MIPS processor
Ch
built of clocked state elements and combinational logic, so it too is a
synchronous sequential circuit. Indeed, the processor can be viewed as a
giant finite state machine, or as a collection of simpler interacting state
machines.
7.1.3 MIPS Microarchitectures
In this chapter, we develop three microarchitectures for the MIPS processor architecture: single-cycle, multicycle, and pipelined. They differ in
the way that the state elements are connected together and in the
amount of nonarchitectural state.
The single-cycle microarchitecture executes an entire instruction
in one cycle. It is easy to explain and has a simple control unit.
Because it completes the operation in one cycle, it does not require
any nonarchitectural state. However, the cycle time is limited by the
slowest instruction.
The multicycle microarchitecture executes instructions in a series of
shorter cycles. Simpler instructions execute in fewer cycles than complicated ones. Moreover, the multicycle microarchitecture reduces the hardware cost by reusing expensive hardware blocks such as adders and
memories. For example, the adder may be used on several different
cycles for several purposes while carrying out a single instruction. The
multicycle microprocessor accomplishes this by adding several nonarchitectural registers to hold intermediate results. The multicycle processor
executes only one instruction at a time, but each instruction takes multiple clock cycles.
The pipelined microarchitecture applies pipelining to the single-cycle
microarchitecture. It therefore can execute several instructions simultaneously, improving the throughput significantly. Pipelining must add
logic to handle dependencies between simultaneously executing instructions. It also requires nonarchitectural pipeline registers. The added logic
and registers are worthwhile; all commercial high-performance processors use pipelining today.
We explore the details and trade-offs of these three microarchitectures in the subsequent sections. At the end of the chapter, we briefly
mention additional techniques that are used to get even more speed in
modern high-performance microprocessors.
7.2 PERFORMANCE ANALYSIS
As we mentioned, a particular processor architecture can have many
microarchitectures with different cost and performance trade-offs. The
cost depends on the amount of hardware required and the implementation technology. Each year, CMOS processes can pack more transistors
on a chip for the same amount of money, and processors take advantage
366 CHAPTER SEVEN Microarchitecture

of these additional transistors to deliver more performance. Precise cost
calculations require detailed knowledge of the implementation technology, but in general, more gates and more memory mean more dollars.
This section lays the foundation for analyzing performance.
There are many ways to measure the performance of a computer
system, and marketing departments are infamous for choosing the
method that makes their computer look fastest, regardless of whether
the measurement has any correlation to real world performance. For
example, Intel and Advanced Micro Devices (AMD) both sell compatible microprocessors conforming to the IA-32 architecture. Intel
Pentium III and Pentium 4 microprocessors were largely advertised
according to clock frequency in the late 1990s and early 2000s,
because Intel offered higher clock frequencies than its competitors.
However, Intel’s main competitor, AMD, sold Athlon microprocessors
that executed programs faster than Intel’s chips at the same clock
frequency. What is a consumer to do?
The only gimmick-free way to measure performance is by measuring
the execution time of a program of interest to you. The computer that executes your program fastest has the highest performance. The next best
choice is to measure the total execution time of a collection of programs
that are similar to those you plan to run; this may be necessary if you
haven’t written your program yet or if somebody else who doesn’t have
your program is making the measurements. Such collections of programs
are called benchmarks, and the execution times of these programs are commonly published to give some indication of how a processor performs.
The execution time of a program, measured in seconds, is given by
Equation 7.1.
(7.1)
The number of instructions in a program depends on the processor architecture. Some architectures have complicated instructions that do more
work per instruction, thus reducing the number of instructions in a program. However, these complicated instructions are often slower to execute
in hardware. The number of instructions also depends enormously on the
cleverness of the programmer. For the purposes of this chapter, we will
assume that we are executing known programs on a MIPS processor, so
the number of instructions for each program is constant, independent of
the microarchitecture.
The number of cycles per instruction, often called CPI, is the number of clock cycles required to execute an average instruction. It is the
reciprocal of the throughput (instructions per cycle, or IPC). Different
microarchitectures have different CPIs. In this chapter, we will assume
Execution Time

# instructions cycles
instructionseconds
cycle

7.2 Performance Analysis 367
Chapte
we have an ideal memory system that does not affect the CPI. In
Chapter 8, we examine how the processor sometimes has to wait for the
memory, which increases the CPI.
The number of seconds per cycle is the clock period, Tc. The clock
period is determined by the critical path through the logic on the processor. Different microarchitectures have different clock periods. Logic and
circuit designs also significantly affect the clock period. For example, a
carry-lookahead adder is faster than a ripple-carry adder. Manufacturing
advances have historically doubled transistor speeds every 4–6 years, so
a microprocessor built today will be much faster than one from last
decade, even if the microarchitecture and logic are unchanged.
The challenge of the microarchitect is to choose the design that
minimizes the execution time while satisfying constraints on cost and/or
power consumption. Because microarchitectural decisions affect both
CPI and Tc and are influenced by logic and circuit designs, determining
the best choice requires careful analysis.
There are many other factors that affect overall computer performance. For example, the hard disk, the memory, the graphics system, and
the network connection may be limiting factors that make processor
performance irrelevant. The fastest microprocessor in the world doesn’t
help surfing the Internet on a dial-up connection. But these other factors
are beyond the scope of this book.
7.3 SINGLE-CYCLE PROCESSOR
We first design a MIPS microarchitecture that executes instructions in a
single cycle. We begin constructing the datapath by connecting the state
elements from Figure 7.1 with combinational logic that can execute the
various instructions. Control signals determine which specific instruction
is carried out by the datapath at any given time. The controller contains
combinational logic that generates the appropriate control signals based
on the current instruction. We conclude by analyzing the performance of
the single-cycle processor.
7.3.1 Single-Cycle Datapath
This section gradually develops the single-cycle datapath, adding one
piece at a time to the state elements from Figure 7.1. The new connections are emphasized in black (or blue, for new control signals), while
the hardware that has already been studied is shown in gray.
The program counter (PC) register contains the address of the
instruction to execute. The first step is to read this instruction from
instruction memory. Figure 7.2 shows that the PC is simply connected to
the address input of the instruction memory. The instruction memory
reads out, or fetches, the 32-bit instruction, labeled Instr.
368 CHAPTER SEVEN Microarchitecture

The processor’s actions depend on the specific instruction that was
fetched. First we will work out the datapath connections for the lw
instruction. Then we will consider how to generalize the datapath to
handle the other instructions.
For a lw instruction, the next step is to read the source register
containing the base address. This register is specified in the rs field of
the instruction, Instr25:21. These bits of the instruction are connected
to the address input of one of the register file read ports, A1, as shown in
Figure 7.3. The register file reads the register value onto RD1.
The lw instruction also requires an offset. The offset is stored in the
immediate field of the instruction, Instr15:0. Because the 16-bit immediate might be either positive or negative, it must be sign-extended to
32 bits, as shown in Figure 7.4. The 32-bit sign-extended value is
called SignImm. Recall from Section 1.4.6 that sign extension simply
copies the sign bit (most significant bit) of a short input into all of the
upper bits of the longer output. Specifically, SignImm15:0  Instr15:0
and SignImm31:16  Instr15.
7.3 Single-Cycle Processor 369
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC PC' Instr
A RD
Data
Memory
WD
WE
CLK
Figure 7.2 Fetch instruction
from memory
Instr
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC' PC
25:21
A RD
Data
Memory
WD
WE
CLK
Figure 7.3 Read source operand
from register file
SignImm
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
A RD
Data
Memory
WD
WE
CLK
Figure 7.4 Sign-extend the
immediate

The processor must add the base address to the offset to find the
address to read from memory. Figure 7.5 introduces an ALU to perform
this addition. The ALU receives two operands, SrcA and SrcB. SrcA
comes from the register file, and SrcB comes from the sign-extended
immediate. The ALU can perform many operations, as was described in
Section 5.2.4. The 3-bit ALUControl signal specifies the operation. The
ALU generates a 32-bit ALUResult and a Zero flag, that indicates
whether ALUResult  0. For a lw instruction, the ALUControl signal
should be set to 010 to add the base address and offset. ALUResult is
sent to the data memory as the address for the load instruction, as
shown in Figure 7.5.
The data is read from the data memory onto the ReadData bus,
then written back to the destination register in the register file at the
end of the cycle, as shown in Figure 7.6. Port 3 of the register file is the
370 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
15:0
SrcB
ALUResult
SrcA Zero
CLK
ALUControl2:0
010ALU
Figure 7.5 Compute memory address
A1
A3
WD3
RD2
RD1 WE3
A2
SignImm
CLK
A RD
Instruction
Memory
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
15:0
20:16 SrcB
ALUResult ReadData
SrcA
RegWrite
Zero
CLK
ALUControl2:0
1 010ALU
Figure 7.6 Write data back to register file

write port. The destination register for the lw instruction is specified in
the rt field, Instr20:16, which is connected to the port 3 address input,
A3, of the register file. The ReadData bus is connected to the port 3
write data input, WD3, of the register file. A control signal called
RegWrite is connected to the port 3 write enable input, WE3, and is
asserted during a lw instruction so that the data value is written into the
register file. The write takes place on the rising edge of the clock at the
end of the cycle.
While the instruction is being executed, the processor must
compute the address of the next instruction, PC. Because instructions
are 32 bits  4 bytes, the next instruction is at PC  4. Figure 7.7 uses
another adder to increment the PC by 4. The new address is written into
the program counter on the next rising edge of the clock. This completes
the datapath for the lw instruction.
Next, let us extend the datapath to also handle the sw instruction.
Like the lw instruction, the sw instruction reads a base address from port
1 of the register and sign-extends an immediate. The ALU adds the base
address to the immediate to find the memory address. All of these functions are already supported by the datapath.
The sw instruction also reads a second register from the register file
and writes it to the data memory. Figure 7.8 shows the new connections for
this function. The register is specified in the rt field, Instr20:16. These bits
of the instruction are connected to the second register file read port, A2.
The register value is read onto the RD2 port. It is connected to the write
data port of the data memory. The write enable port of the data memory,
WE, is controlled by MemWrite. For a sw instruction, MemWrite  1, to
write the data to memory; ALUControl  010, to add the base address
7.3 Single-Cycle Processor 371
CLK
SignImm
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
15:0
SrcB 20:16
ALUResult ReadData
SrcA
PCPlus4
Result
RegWrite
Zero
CLK
ALUControl2:0
1 010ALU
+
Figure 7.7 Determine address of next instruction for PC
C
and offset; and RegWrite  0, because nothing should be written to the
register file. Note that data is still read from the address given to the data
memory, but that this ReadData is ignored because RegWrite  0.
Next, consider extending the datapath to handle the R-type instructions add, sub, and, or, and slt. All of these instructions read two registers from the register file, perform some ALU operation on them, and
write the result back to a third register file. They differ only in the specific ALU operation. Hence, they can all be handled with the same hardware, using different ALUControl signals.
Figure 7.9 shows the enhanced datapath handling R-type instructions. The register file reads two registers. The ALU performs an operation on these two registers. In Figure 7.8, the ALU always received its
SrcB operand from the sign-extended immediate (SignImm). Now, we
add a multiplexer to choose SrcB from either the register file RD2 port
or SignImm.
The multiplexer is controlled by a new signal, ALUSrc. ALUSrc is 0
for R-type instructions to choose SrcB from the register file; it is 1 for lw
and sw to choose SignImm. This principle of enhancing the datapath’s
capabilities by adding a multiplexer to choose inputs from several possibilities is extremely useful. Indeed, we will apply it twice more to complete the handling of R-type instructions.
In Figure 7.8, the register file always got its write data from
the data memory. However, R-type instructions write the ALUResult to
the register file. Therefore, we add another multiplexer to choose
between ReadData and ALUResult. We call its output Result. This multiplexer is controlled by another new signal, MemtoReg. MemtoReg is 0
372 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
20:16
15:0
SrcB 20:16
ALUResult ReadData
WriteData
SrcA
PCPlus4
Result
RegWrite MemWrite
Zero
CLK
ALUControl2:0
0 010 1 ALU
+
Figure 7.8 Write data to memory for sw instruction

for R-type instructions to choose Result from the ALUResult; it is 1 for
lw to choose ReadData. We don’t care about the value of MemtoReg for
sw, because sw does not write to the register file.
Similarly, in Figure 7.8, the register to write was specified by the rt
field of the instruction, Instr20:16. However, for R-type instructions, the
register is specified by the rd field, Instr15:11. Thus, we add a third multiplexer to choose WriteReg from the appropriate field of the instruction. The multiplexer is controlled by RegDst. RegDst is 1 for R-type
instructions to choose WriteReg from the rd field, Instr15:11; it is 0 for
lw to choose the rt field, Instr20:16. We don’t care about the value of
RegDst for sw, because sw does not write to the register file.
Finally, let us extend the datapath to handle beq. beq compares
two registers. If they are equal, it takes the branch by adding the branch
offset to the program counter. Recall that the offset is a positive or negative number, stored in the imm field of the instruction, Instr31:26. The offset indicates the number of instructions to branch past. Hence, the
immediate must be sign-extended and multiplied by 4 to get the new
program counter value: PC  PC  4  SignImm  4.
Figure 7.10 shows the datapath modifications. The next PC value
for a taken branch, PCBranch, is computed by shifting SignImm left by
2 bits, then adding it to PCPlus4. The left shift by 2 is an easy way to
multiply by 4, because a shift by a constant amount involves just wires.
The two registers are compared by computing SrcA  SrcB using the
ALU. If ALUResult is 0, as indicated by the Zero flag from the ALU, the
registers are equal. We add a multiplexer to choose PC from either
PCPlus4 or PCBranch. PCBranch is selected if the instruction is
7.3 Single-Cycle Processor 373
SignImm
CLK
A RD
Instruction
Memory
4
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
PC' PC Instr 25:21
20:16
15:0
SrcB
20:16
15:11
ALUResult ReadData
WriteData
SrcA
PCPlus4 WriteReg4:0
Result
RegWrite RegDst ALUSrc MemWrite MemtoReg
Zero
CLK
ALUControl2:0
0 1 1 0 varies 0 ALU
Figure 7.9 Datapath enhancements for R-type instruction
Cha
a branch and the Zero flag is asserted. Hence, Branch is 1 for beq and
0 for other instructions. For beq, ALUControl  110, so the ALU
performs a subtraction. ALUSrc  0 to choose SrcB from the register
file. RegWrite and MemWrite are 0, because a branch does not write to
the register file or memory. We don’t care about the values of RegDst
and MemtoReg, because the register file is not written.
This completes the design of the single-cycle MIPS processor datapath. We have illustrated not only the design itself, but also the design
process in which the state elements are identified and the combinational
logic connecting the state elements is systematically added. In the next
section, we consider how to compute the control signals that direct the
operation of our datapath.
7.3.2 Single-Cycle Control
The control unit computes the control signals based on the opcode and
funct fields of the instruction, Instr31:26 and Instr5:0. Figure 7.11 shows
the entire single-cycle MIPS processor with the control unit attached to
the datapath.
Most of the control information comes from the opcode, but R-type
instructions also use the funct field to determine the ALU operation.
Thus, we will simplify our design by factoring the control unit into two
blocks of combinational logic, as shown in Figure 7.12. The main
decoder computes most of the outputs from the opcode. It also determines a 2-bit ALUOp signal. The ALU decoder uses this ALUOp signal
in conjunction with the funct field to compute ALUControl. The meaning of the ALUOp signal is given in Table 7.1.
374 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch
WriteReg4:0
Result
RegWrite RegDst ALUSrc Branch MemWrite MemtoReg
Zero
PCSrc
CLK
ALUControl2:0
0 0 x 0 110 1 x ALU
Figure 7.10 Datapath enhancements for beq instruction
RegDst
Branch
MemWrite
MemtoReg
ALUSrc Opcode5:0
Control
Unit
Funct5:0 ALUControl2:0
Main
Decoder
ALUOp1:0
ALU
Decoder
RegWrite
Figure 7.12 Control unit
internal structure

7.3 Single-Cycle Processor 375
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch +
+
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
ALU
Figure 7.11 Complete single-cycle MIPS processor
Table 7.1 ALUOp encoding
ALUOp Meaning
00 add
01 subtract
10 look at funct field
11 n/a
Table 7.2 is a truth table for the ALU decoder. Recall that the meanings of the three ALUControl signals were given in Table 5.1. Because
ALUOp is never 11, the truth table can use don’t care’s X1 and 1X
instead of 01 and 10 to simplify the logic. When ALUOp is 00 or 01, the
ALU should add or subtract, respectively. When ALUOp is 10, the
decoder examines the funct field to determine the ALUControl. Note
that, for the R-type instructions we implement, the first two bits of the
funct field are always 10, so we may ignore them to simplify the decoder.
The control signals for each instruction were described as we built
the datapath. Table 7.3 is a truth table for the main decoder that summarizes the control signals as a function of the opcode. All R-type
instructions use the same main decoder values; they differ only in the

ALU decoder output. Recall that, for instructions that do not write to
the register file (e.g., sw and beq), the RegDst and MemtoReg control
signals are don’t cares (X); the address and data to the register write
port do not matter because RegWrite is not asserted. The logic for the
decoder can be designed using your favorite techniques for combinational logic design.
Example 7.1 SINGLE-CYCLE PROCESSOR OPERATION
Determine the values of the control signals and the portions of the datapath that
are used when executing an or instruction.
Solution: Figure 7.13 illustrates the control signals and flow of data during
execution of the or instruction. The PC points to the memory location holding
the instruction, and the instruction memory fetches this instruction.
The main flow of data through the register file and ALU is represented with a
dashed blue line. The register file reads the two source operands specified by
Instr25:21 and Instr20:16. SrcB should come from the second port of the register
376 CHAPTER SEVEN Microarchitecture
Table 7.2 ALU decoder truth table
ALUOp Funct ALUControl
00 X 010 (add)
X1 X 110 (subtract)
1X 100000 (add) 010 (add)
1X 100010 (sub) 110 (subtract)
1X 100100 (and) 000 (and)
1X 100101 (or) 001 (or)
1X 101010 (slt) 111 (set less than)
Table 7.3 Main decoder truth table
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp
R-type 000000 1 1 0 0 0 0 10
lw 100011 1 0 1 0 0 1 00
sw 101011 0 X 1 0 1 X 00
beq 000100 0 X 0 1 0 X 01

file (not SignImm), so ALUSrc must be 0. or is an R-type instruction, so ALUOp
is 10, indicating that ALUControl should be determined from the funct field
to be 001. Result is taken from the ALU, so MemtoReg is 0. The result is written
to the register file, so RegWrite is 1. The instruction does not write memory, so
MemWrite  0.
The selection of the destination register is also shown with a dashed blue
line. The destination register is specified in the rd field, Instr15:11, so RegDst  1.
The updating of the PC is shown with the dashed gray line. The instruction is
not a branch, so Branch  0 and, hence, PCSrc is also 0. The PC gets its next
value from PCPlus4.
Note that data certainly does flow through the nonhighlighted paths, but that
the value of that data is unimportant for this instruction. For example, the
immediate is sign-extended and data is read from memory, but these values do
not influence the next state of the system.
7.3.3 More Instructions
We have considered a limited subset of the full MIPS instruction set.
Adding support for the addi and j instructions illustrates the principle
of how to handle new instructions and also gives us a sufficiently rich
instruction set to write many interesting programs. We will see that
7.3 Single-Cycle Processor 377
SignImm
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
PC
1
Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
001 0
0
1 0
0
1
0
ALU
0 PC'
Figure 7.13 Control signals and data flow while executing or instruction

supporting some instructions simply requires enhancing the main
decoder, whereas supporting others also requires more hardware in the
datapath.
Example 7.2 addi INSTRUCTION
The add immediate instruction, addi, adds the value in a register to the immediate and writes the result to another register. The datapath already is capable of
this task. Determine the necessary changes to the controller to support addi.
Solution: All we need to do is add a new row to the main decoder truth table
showing the control signal values for addi, as given in Table 7.4. The result
should be written to the register file, so RegWrite  1. The destination register is
specified in the rt field of the instruction, so RegDst  0. SrcB comes from the
immediate, so ALUSrc  1. The instruction is not a branch, nor does it write
memory, so Branch  MemWrite  0. The result comes from the ALU, not
memory, so MemtoReg  0. Finally, the ALU should add, so ALUOp  00.
378 CHAPTER SEVEN Microarchitecture
Table 7.4 Main decoder truth table enhanced to support addi
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp
R-type 000000 1 1 0 0 0 0 10
lw 100011 1 0 1 0 0 1 00
sw 101011 0 X 1 0 1 X 00
beq 000100 0 X 0 1 0 X 01
addi 001000 1 0 1 0 0 0 00
Example 7.3 j INSTRUCTION
The jump instruction, j, writes a new value into the PC. The two least significant bits of the PC are always 0, because the PC is word aligned (i.e., always a
multiple of 4). The next 26 bits are taken from the jump address field in
Instr25:0. The upper four bits are taken from the old value of the PC.
The existing datapath lacks hardware to compute PC in this fashion. Determine
the necessary changes to both the datapath and controller to handle j.
Solution: First, we must add hardware to compute the next PC value, PC, in the
case of a j instruction and a multiplexer to select this next PC, as shown in
Figure 7.14. The new multiplexer uses the new Jump control signal.
Ch
Now we must add a row to the main decoder truth table for the j instruction
and a column for the Jump signal, as shown in Table 7.5. The Jump control
signal is 1 for the j instruction and 0 for all others. j does not write the register
file or memory, so RegWrite  MemWrite  0. Hence, we don’t care about
the computation done in the datapath, and RegDst  ALUSrc  Branch 
MemtoReg  ALUOp  X.
7.3 Single-Cycle Processor 379
SignImm
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
0
1
25:0 <<2
27:0 31:28
PCJump
Jump
ALU
Figure 7.14 Single-cycle MIPS datapath enhanced to support the j instruction
Table 7.5 Main decoder truth table enhanced to support j
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp Jump
R-type 000000 1 1 0 0 0 0 10 0
lw 100011 1 0 1 0 0 1 00 0
sw 101011 0 X 1 0 1 X 00 0
beq 000100 0 X 0 1 0 X 01 0
addi 001000 1 0 1 0 0 0 00 0
j 000010 0 X X X 0 X XX 1

7.3.4 Performance Analysis
Each instruction in the single-cycle processor takes one clock cycle, so the
CPI is 1. The critical path for the lw instruction is shown in Figure 7.15
with a heavy dashed blue line. It starts with the PC loading a new address
on the rising edge of the clock. The instruction memory reads the next
instruction. The register file reads SrcA. While the register file is reading,
the immediate field is sign-extended and selected at the ALUSrc multiplexer to determine SrcB. The ALU adds SrcA and SrcB to find the effective address. The data memory reads from this address. The MemtoReg
multiplexer selects ReadData. Finally, Result must setup at the register file
before the next rising clock edge, so that it can be properly written. Hence,
the cycle time is
(7.2)
In most implementation technologies, the ALU, memory, and register file
accesses are substantially slower than other operations. Therefore, the
cycle time simplifies to
(7.3)
The numerical values of these times will depend on the specific implementation technology.
Tc  tpcqPC 2tmem  tRFread 2tmux  tALU  tRFsetup
 tALU  tmem  tmux  tRFsetup
Tc  tpcqPC  tmem  max[tRFread, tsext]  tmux
380 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD1 WE3
CLK
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch +
+
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
1
010 0
1
0
1
0 0
A2 RD2
ALU
Sign Extend
Figure 7.15 Critical path for lw instruction

Other instructions have shorter critical paths. For example, R-type
instructions do not need to access data memory. However, we are
disciplining ourselves to synchronous sequential design, so the clock
period is constant and must be long enough to accommodate the
slowest instruction.
Example 7.4 SINGLE-CYCLE PROCESSOR PERFORMANCE
Ben Bitdiddle is contemplating building the single-cycle MIPS processor in a 65 nm
CMOS manufacturing process. He has determined that the logic elements have the
delays given in Table 7.6. Help him compare the execution time for a program
with 100 billion instructions.
Solution: According to Equation 7.3, the cycle time of the single-cycle processor is
Tc1  30  2(250)  150  2(25)  200  20  950 ps. We use the subscript
“1” to distinguish it from subsequent processor designs. According to Equation
7.1, the total execution time is T1  (100  109 instructions)(1 cycle/instruction)
(950  1012 s/cycle)  95 seconds.
7.4 Multicycle Processor 381
Table 7.6 Delays of circuit elements
Element Parameter Delay (ps)
register clk-to-Q tpcq 30
register setup tsetup 20
multiplexer tmux 25
ALU tALU 200
memory read tmem 250
register file read tRFread 150
register file setup tRFsetup 20
7.4 MULTICYCLE PROCESSOR
The single-cycle processor has three primary weaknesses. First, it
requires a clock cycle long enough to support the slowest instruction
(lw), even though most instructions are faster. Second, it requires three
adders (one in the ALU and two for the PC logic); adders are relatively
expensive circuits, especially if they must be fast. And third, it has separate instruction and data memories, which may not be realistic. Most
computers have a single large memory that holds both instructions and
data and that can be read and written.
Ch
The multicycle processor addresses these weaknesses by breaking an
instruction into multiple shorter steps. In each short step, the processor
can read or write the memory or register file or use the ALU. Different
instructions use different numbers of steps, so simpler instructions can
complete faster than more complex ones. The processor needs only one
adder; this adder is reused for different purposes on various steps. And
the processor uses a combined memory for instructions and data. The
instruction is fetched from memory on the first step, and data may be
read or written on later steps.
We design a multicycle processor following the same procedure we
used for the single-cycle processor. First, we construct a datapath by connecting the architectural state elements and memories with combinational
logic. But, this time, we also add nonarchitectural state elements to hold
intermediate results between the steps. Then we design the controller. The
controller produces different signals on different steps during execution
of a single instruction, so it is now a finite state machine rather than combinational logic. We again examine how to add new instructions to the
processor. Finally, we analyze the performance of the multicycle processor
and compare it to the single-cycle processor.
7.4.1 Multicycle Datapath
Again, we begin our design with the memory and architectural state of the
MIPS processor, shown in Figure 7.16. In the single-cycle design, we used
separate instruction and data memories because we needed to read the
instruction memory and read or write the data memory all in one cycle.
Now, we choose to use a combined memory for both instructions and data.
This is more realistic, and it is feasible because we can read the instruction
in one cycle, then read or write the data in a separate cycle. The PC and
register file remain unchanged. We gradually build the datapath by adding
components to handle each step of each instruction. The new connections
are emphasized in black (or blue, for new control signals), whereas the
hardware that has already been studied is shown in gray.
The PC contains the address of the instruction to execute. The first
step is to read this instruction from instruction memory. Figure 7.17
shows that the PC is simply connected to the address input of the instruction memory. The instruction is read and stored in a new nonarchitectural
382 CHAPTER SEVEN Microarchitecture
CLK
A RD
Instr/Data
Memory
PC' PC
WD
WE
CLK
EN
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
Figure 7.16 State elements with
unified instruction/data memory

Instruction Register so that it is available for future cycles. The Instruction
Register receives an enable signal, called IRWrite, that is asserted when it
should be updated with a new instruction.
As we did with the single-cycle processor, we will work out the datapath connections for the lw instruction. Then we will enhance the datapath to handle the other instructions. For a lw instruction, the next step
is to read the source register containing the base address. This register is
specified in the rs field of the instruction, Instr25:21. These bits of the
instruction are connected to one of the address inputs, A1, of the register
file, as shown in Figure 7.18. The register file reads the register onto
RD1. This value is stored in another nonarchitectural register, A.
The lw instruction also requires an offset. The offset is stored in
the immediate field of the instruction, Instr15:0 and must be signextended to 32 bits, as shown in Figure 7.19. The 32-bit sign-extended
value is called SignImm. To be consistent, we might store SignImm in
another nonarchitectural register. However, SignImm is a combinational function of Instr and will not change while the current instruction is being processed, so there is no need to dedicate a register to
hold the constant value.
The address of the load is the sum of the base address and offset. We
use an ALU to compute this sum, as shown in Figure 7.20. ALUControl
should be set to 010 to perform an addition. ALUResult is stored in a
nonarchitectural register called ALUOut.
The next step is to load the data from the calculated address in the
memory. We add a multiplexer in front of the memory to choose the
7.4 Multicycle Processor 383
PC Instr
CLK
EN
IRWrite
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC'
WD
WE
CLK
Figure 7.17 Fetch instruction
from memory
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC' PC Instr 25:21
CLK
WD
WE
CLK CLK
A
EN
IRWrite
Figure 7.18 Read source operand
from register file

384 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
CLK
WD
WE
CLK CLK
A
EN
IRWrite
Figure 7.19 Sign-extend the immediate
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl2:0
WD
WE
CLK CLK
A CLK
EN
IRWrite
ALU
Figure 7.20 Add base address to offset
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A CLK
EN
IorD IRWrite
0
1
ALU
Figure 7.21 Load data from memory

memory address, Adr, from either the PC or ALUOut, as shown in
Figure 7.21. The multiplexer select signal is called IorD, to indicate
either an instruction or data address. The data read from the memory is
stored in another nonarchitectural register, called Data. Notice that the
address multiplexer permits us to reuse the memory during the lw
instruction. On the first step, the address is taken from the PC to fetch
the instruction. On a later step, the address is taken from ALUOut to
load the data. Hence, IorD must have different values on different steps.
In Section 7.4.2, we develop the FSM controller that generates these
sequences of control signals.
Finally, the data is written back to the register file, as shown in
Figure 7.22. The destination register is specified by the rt field of the
instruction, Instr20:16.
While all this is happening, the processor must update the program
counter by adding 4 to the old PC. In the single-cycle processor, a separate adder was needed. In the multicycle processor, we can use the existing ALU on one of the steps when it is not busy. To do so, we must
insert source multiplexers to choose the PC and the constant 4 as ALU
inputs, as shown in Figure 7.23. A two-input multiplexer controlled by
ALUSrcA chooses either the PC or register A as SrcA. A four-input multiplexer controlled by ALUSrcB chooses either 4 or SignImm as SrcB.
We use the other two multiplexer inputs later when we extend the datapath to handle other instructions. (The numbering of inputs to the multiplexer is arbitrary.) To update the PC, the ALU adds SrcA (PC) to
SrcB (4), and the result is written into the program counter register. The
PCWrite control signal enables the PC register to be written only on
certain cycles.
7.4 Multicycle Processor 385
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
SrcB 20:16
ALUResult
SrcA
ALUOut
RegWrite
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A CLK
EN
IorD IRWrite
0
1
ALU
Figure 7.22 Write data back to register file

This completes the datapath for the lw instruction. Next, let us
extend the datapath to also handle the sw instruction. Like the lw
instruction, the sw instruction reads a base address from port 1 of the
register file and sign-extends the immediate. The ALU adds the base
address to the immediate to find the memory address. All of these functions are already supported by existing hardware in the datapath.
The only new feature of sw is that we must read a second register
from the register file and write it into the memory, as shown in
Figure 7.24. The register is specified in the rt field of the instruction,
Instr20:16, which is connected to the second port of the register file. When
the register is read, it is stored in a nonarchitectural register, B. On the
next step, it is sent to the write data port (WD) of the data memory to be
written. The memory receives an additional MemWrite control signal to
indicate that the write should occur.
386 CHAPTER SEVEN Microarchitecture
PCWrite
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
PC' PC Instr 1 25:21
15:0
SrcB
20:16
ALUResult
SrcA
ALUOut
RegWrite ALUSrcA
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A
00
01
10
11
4
CLK
EN EN
ALUSrcB1:0 IorD IRWrite
0
1
ALU
Figure 7.23 Increment PC by 4
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
SrcB 20:16
ALUResult
SrcA
ALUOut
MemWrite RegWrite ALUSrcA
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A
00
01
10
11
4
CLK
EN EN
PCWrite IorD IRWrite ALUSrcB1:0
B
ALU
Figure 7.24 Enhanced datapath for sw instruction

7.4 Multicycle Processor 387
0
1
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
SrcB 20:16
15:11
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
PCWrite IorD IRWrite ALUSrcB1:0
ALU
Figure 7.25 Enhanced datapath for R-type instructions
For R-type instructions, the instruction is again fetched, and the
two source registers are read from the register file. Another input of the
SrcB multiplexer is used to choose register B as the second source register for the ALU, as shown in Figure 7.25. The ALU performs the appropriate operation and stores the result in ALUOut. On the next step,
ALUOut is written back to the register specified by the rd field of the
instruction, Instr15:11. This requires two new multiplexers. The
MemtoReg multiplexer selects whether WD3 comes from ALUOut (for
R-type instructions) or from Data (for lw). The RegDst instruction
selects whether the destination register is specified in the rt or rd field
of the instruction.
For the beq instruction, the instruction is again fetched, and the two
source registers are read from the register file. To determine whether the
registers are equal, the ALU subtracts the registers and examines the Zero
flag. Meanwhile, the datapath must compute the next value of the PC if
the branch is taken: PC  PC  4  SignImm  4. In the single-cycle
processor, yet another adder was needed to compute the branch address.
In the multicycle processor, the ALU can be reused again to save hardware. On one step, the ALU computes PC  4 and writes it back to the
program counter, as was done for other instructions. On another step, the
ALU uses this updated PC value to compute PC  SignImm  4.
SignImm is left-shifted by 2 to multiply it by 4, as shown in Figure 7.26.
The SrcB multiplexer chooses this value and adds it to the PC. This sum
represents the destination of the branch and is stored in ALUOut. A new
multiplexer, controlled by PCSrc, chooses what signal should be sent to
PC. The program counter should be written either when PCWrite is
asserted or when a branch is taken. A new control signal, Branch, indicates that the beq instruction is being executed. The branch is taken if
Zero is also asserted. Hence, the datapath computes a new PC write
Chap
enable, called PCEn, which is TRUE either when PCWrite is asserted or
when both Branch and Zero are asserted.
This completes the design of the multicycle MIPS processor datapath. The design process is much like that of the single-cycle processor in
that hardware is systematically connected between the state elements to
handle each instruction. The main difference is that the instruction is
executed in several steps. Nonarchitectural registers are inserted to hold
the results of each step. In this way, the ALU can be reused several times,
saving the cost of extra adders. Similarly, the instructions and data can
be stored in one shared memory. In the next section, we develop an FSM
controller to deliver the appropriate sequence of control signals to the
datapath on each step of each instruction.
7.4.2 Multicycle Control
As in the single-cycle processor, the control unit computes the control
signals based on the opcode and funct fields of the instruction,
Instr31:26 and Instr5:0. Figure 7.27 shows the entire multicycle MIPS
processor with the control unit attached to the datapath. The datapath is
shown in black, and the control unit is shown in blue.
As in the single-cycle processor, the control unit is partitioned into a
main controller and an ALU decoder, as shown in Figure 7.28. The
ALU decoder is unchanged and follows the truth table of Table 7.2.
Now, however, the main controller is an FSM that applies the proper
control signals on the proper cycles or steps. The sequence of control
signals depends on the instruction being executed. In the remainder of
this section, we will develop the FSM state transition diagram for the
main controller.
388 CHAPTER SEVEN Microarchitecture
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
IorD IRWrite ALUSrcB1:0 PCWrite
PCEn
ALU
Figure 7.26 Enhanced datapath for beq instruction

The main controller produces multiplexer select and register enable
signals for the datapath. The select signals are MemtoReg, RegDst,
IorD, PCSrc, ALUSrcB, and ALUSrcA. The enable signals are IRWrite,
MemWrite, PCWrite, Branch, and RegWrite.
To keep the following state transition diagrams readable, only the
relevant control signals are listed. Select signals are listed only when
their value matters; otherwise, they are don’t cares. Enable signals are
listed only when they are asserted; otherwise, they are 0.
7.4 Multicycle Processor 389
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File 0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
tsDgeR
Branch
MemWrite
geRotmeM ALUSrcA
RegWrite Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
ULA
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
IRWrite ALUSrcB1:0
IorD
PCWrite
PCEn
Figure 7.27 Complete multicycle MIPS processor
ALUSrcA
PCSrc
Branch
ALUSrcB1:0
Opcode5:0
Control
Unit
ALUControl2:0 Funct5:0
Main
Controller
(FSM)
ALUOp1:0
ALU
Decoder
RegWrite
PCWrite
IorD
MemWrite
IRWrite
RegDst
MemtoReg
Register
Enables
Multiplexer
Selects
Figure 7.28 Control unit internal
structure

The first step for any instruction is to fetch the instruction from
memory at the address held in the PC. The FSM enters this state on
reset. To read memory, IorD  0, so the address is taken from the PC.
IRWrite is asserted to write the instruction into the instruction register,
IR. Meanwhile, the PC should be incremented by 4 to point to the next
instruction. Because the ALU is not being used for anything else, the
processor can use it to compute PC  4 at the same time that it fetches
the instruction. ALUSrcA  0, so SrcA comes from the PC. ALUSrcB 
01, so SrcB is the constant 4. ALUOp  00, so the ALU decoder
produces ALUControl  010 to make the ALU add. To update the
PC with this new value, PCSrc  0, and PCWrite is asserted. These
control signals are shown in Figure 7.29. The data flow on this step is
shown in Figure 7.30, with the instruction fetch shown using the dashed
blue line and the PC increment shown using the dashed gray line.
The next step is to read the register file and decode the instruction.
The register file always reads the two sources specified by the rs and rt
fields of the instruction. Meanwhile, the immediate is sign-extended.
Decoding involves examining the opcode of the instruction to determine
what to do next. No control signals are necessary to decode the instruction, but the FSM must wait 1 cycle for the reading and decoding to
complete, as shown in Figure 7.31. The new state is highlighted in blue.
The data flow is shown in Figure 7.32.
390 CHAPTER SEVEN Microarchitecture
Reset
S0: Fetch
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
Figure 7.29 Fetch
ALU
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
RegDst
MemtoReg
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
Branch
MemWrite
ALUSrcA
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB IRWrite 1:0
IorD
PCWrite
PCEn
0
1 1
0
X
X
0 0
01
010 0
1
0
Figure 7.30 Data flow during the fetch step

Now the FSM proceeds to one of several possible states, depending
on the opcode. If the instruction is a memory load or store (lw or sw),
the multicycle processor computes the address by adding the base
address to the sign-extended immediate. This requires ALUSrcA  1 to
select register A and ALUSrcB  10 to select SignImm. ALUOp  00,
so the ALU adds. The effective address is stored in the ALUOut register
for use on the next step. This FSM step is shown in Figure 7.33, and the
data flow is shown in Figure 7.34.
If the instruction is lw, the multicycle processor must next read data
from memory and write it to the register file. These two steps are shown
in Figure 7.35. To read from memory, IorD  1 to select the memory
address that was just computed and saved in ALUOut. This address in
memory is read and saved in the Data register during step S3. On the
next step, S4, Data is written to the register file. MemtoReg  1 to select
7.4 Multicycle Processor 391
Reset
S0: Fetch S1: Decode
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
Figure 7.31 Decode
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1
WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB IRWrite 1:0
IorD
PCWrite
PCEn
X
0 0
0
X
X
0 X
XX
XXX X
0
0
ALU
Figure 7.32 Data flow during the decode step

Data, and RegDst  0 to pull the destination register from the rt field
of the instruction. RegWrite is asserted to perform the write, completing
the lw instruction. Finally, the FSM returns to the initial state, S0, to
fetch the next instruction. For these and subsequent steps, try to visualize the data flow on your own.
From state S2, if the instruction is sw, the data read from the second
port of the register file is simply written to memory. IorD  1 to select
the address computed in S2 and saved in ALUOut. MemWrite is
asserted to write the memory. Again, the FSM returns to S0 to fetch the
next instruction. The added step is shown in Figure 7.36.
392 CHAPTER SEVEN Microarchitecture
ALUSrcA = 1
Reset
S0: Fetch
S2: MemAdr
S1: Decode
Op = LW
or
Op = SW
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
ALUOp = 00
ALUSrcB = 10
Figure 7.33 Memory address
computation
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB IRWrite 1:0
IorD
PCWrite
PCEn
X
0 0
0
X
X
0 1
10
010 X
0
0
ALU
Figure 7.34 Data flow during memory address computation

If the opcode indicates an R-type instruction, the multicycle processor must calculate the result using the ALU and write that result to the
register file. Figure 7.37 shows these two steps. In S6, the instruction is
executed by selecting the A and B registers (ALUSrcA  1, ALUSrcB 
00) and performing the ALU operation indicated by the funct field
of the instruction. ALUOp  10 for all R-type instructions. The
ALUResult is stored in ALUOut. In S7, ALUOut is written to the register file, RegDst  1, because the destination register is specified in the
rd field of the instruction. MemtoReg  0 because the write data, WD3,
comes from ALUOut. RegWrite is asserted to write the register file.
For a beq instruction, the processor must calculate the destination
address and compare the two source registers to determine whether the
branch should be taken. This requires two uses of the ALU and hence
might seem to demand two new states. Notice, however, that the ALU was
not used during S1 when the registers were being read. The processor
might as well use the ALU at that time to compute the destination address
by adding the incremented PC, PC  4, to SignImm  4, as shown in
7.4 Multicycle Processor 393
IorD = 1
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
Op = LW
or
Op = SW
Op = LW
S4: Mem
Writeback
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
MemtoReg = 1
RegWrite
Figure 7.35 Memory read
C
Figure 7.38 (see page 396). ALUSrcA  0 to select the incremented PC,
ALUSrcB  11 to select SignImm  4, and ALUOp  00 to add. The
destination address is stored in ALUOut. If the instruction is not beq, the
computed address will not be used in subsequent cycles, but its computation was harmless. In S8, the processor compares the two registers by subtracting them and checking to determine whether the result is 0. If it is, the
processor branches to the address that was just computed. ALUSrcA  1
to select register A; ALUSrcB  00 to select register B; ALUOp  01 to
subtract; PCSrc  1 to take the destination address from ALUOut, and
Branch  1 to update the PC with this address if the ALU result is 0.2
Putting these steps together, Figure 7.39 shows the complete main
controller state transition diagram for the multicycle processor (see page
397). Converting it to hardware is a straightforward but tedious task
using the techniques of Chapter 3. Better yet, the FSM can be coded in
an HDL and synthesized using the techniques of Chapter 4.
394 CHAPTER SEVEN Microarchitecture
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
Op = LW
or
Op = SW
Op = LW
Op = SW
S4: Mem
Writeback
IRWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
PCWrite
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
Figure 7.36 Memory write
2 Now we see why the PCSrc multiplexer is necessary to choose PC from either ALUResult
(in S0) or ALUOut (in S8).
Ch
7.4.3 More Instructions
As we did in Section 7.3.3 for the single-cycle processor, let us now
extend the multicycle processor to support the addi and j instructions.
The next two examples illustrate the general design process to support
new instructions.
Example 7.5 addi INSTRUCTION
Modify the multicycle processor to support addi.
Solution: The datapath is already capable of adding registers to immediates, so all
we need to do is add new states to the main controller FSM for addi, as shown in
Figure 7.40 (see page 398). The states are similar to those for R-type instructions.
In S9, register A is added to SignImm (ALUSrcA  1, ALUSrcB  10, ALUOp 
00) and the result, ALUResult, is stored in ALUOut. In S10, ALUOut is written
7.4 Multicycle Processor 395
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
Op = LW
or
Op = SW
Op = LW
S4: Mem
Writeback
S5: MemWrite
S6: Execute
S7: ALU
Writeback
Op = R-type
Op = SW
PCWrite
IRWrite
IorD = 0
PCSrc = 0
ALUOp = 00
ALUSrcB = 01
AluSrcA = 0
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
RegDst = 1
RegWrite
MemtoReg = 0
ALUSrcA = 1
ALUOp = 10
ALUSrcB = 00
Figure 7.37 Execute R-type
operation

to the register specified by the rt field of the instruction (RegDst  0, MemtoReg
 0, RegWrite asserted). The astute reader may notice that S2 and S9 are identical
and could be merged into a single state.
Example 7.6 j INSTRUCTION
Modify the multicycle processor to support j.
Solution: First, we must modify the datapath to compute the next PC value in
the case of a j instruction. Then we add a state to the main controller to handle
the instruction.
Figure 7.41 shows the enhanced datapath (see page 399). The jump destination
address is formed by left-shifting the 26-bit addr field of the instruction by two
bits, then prepending the four most significant bits of the already incremented
PC. The PCSrc multiplexer is extended to take this address as a third input.
396 CHAPTER SEVEN Microarchitecture
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
PCWrite
ALUSrcA = 0
ALUSrcB = 11
ALUOp = 00
ALUSrcA = 1
ALUSrcB = 10
ALUOp = 00
IorD = 1
RegDst = 1
MemtoReg = 0
RegWrite
IorD = 1
MemWrite
ALUSrcA = 1
ALUSrcB = 00
ALUOp = 10
ALUSrcA = 1
ALUSrcB = 00
ALUOp = 01
PCSrc = 1
Branch
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
RegDst = 0
MemtoReg = 1
RegWrite
S4: Mem
Writeback
Figure 7.38 Branch

Figure 7.42 shows the enhanced main controller (see page 400). The new state,
S11, simply selects PC as the PCJump value (PCSrc  10) and writes the PC.
Note that the PCSrc select signal is extended to two bits in S0 and S8 as well.
7.4.4 Performance Analysis
The execution time of an instruction depends on both the number of cycles
it uses and the cycle time. Whereas the single-cycle processor performed all
instructions in one cycle, the multicycle processor uses varying numbers of
cycles for the various instructions. However, the multicycle processor does
less work in a single cycle and, thus, has a shorter cycle time.
The multicycle processor requires three cycles for beq and j instructions, four cycles for sw, addi, and R-type instructions, and five cycles
for lw instructions. The CPI depends on the relative likelihood that each
instruction is used.
7.4 Multicycle Processor 397
IorD = 0
ALUSrcA = 0
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
S4: Mem
Writeback
PCWrite
IRWrite
PCSrc = 0
ALUOp = 00
ALUSrcB = 01
AluSrcA = 0
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
RegDst = 1
RegWrite
MemtoReg = 0
ALUOp = 00
ALUSrcB = 11
ALUSrcA = 1
ALUOp = 10
ALUSrcB = 00
ALUSrcA = 1
Branch
PCSrc = 1
ALUOp = 01
ALUSrcB = 00
Figure 7.39 Complete multicycle
control FSM
C
Example 7.7 MULTICYCLE PROCESSOR CPI
The SPECINT2000 benchmark consists of approximately 25% loads, 10%
stores, 11% branches, 2% jumps, and 52% R-type instructions.3 Determine the
average CPI for this benchmark.
Solution: The average CPI is the sum over each instruction of the CPI for that
instruction multiplied by the fraction of the time that instruction is used. For this
benchmark, Average CPI  (0.11  0.02)(3)  (0.52  0.10)(4)  (0.25)(5) 
4.12. This is better than the worst-case CPI of 5, which would be required if all
instructions took the same time.
398 CHAPTER SEVEN Microarchitecture
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
S4: Mem
Writeback
Op = ADDI
S9: ADDI
Execute
S10: ADDI
Writeback
IorD = 0
PCWrite
IRWrite
PCSrc = 0
ALUOp = 00
ALUSrcB = 01
AluSrcA = 0
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
RegDst = 1
RegWrite
MemtoReg = 0
RegDst = 1
RegWrite
MemtoReg = 0
ALUSrcA = 1
ALUOp = 10
ALUSrcB = 00
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
ALUSrcA = 0
ALUOp = 00
ALUSrcB = 11
ALUSrcA = 1
Branch
PCSrc = 1
ALUOp = 01
ALUSrcB = 00
Figure 7.40 Main controller states for addi
3 Data from Patterson and Hennessy, Computer Organization and Design, 3rd Edition,
Morgan Kaufmann, 2005.

Recall that we designed the multicycle processor so that each cycle
involved one ALU operation, memory access, or register file access. Let
us assume that the register file is faster than the memory and that writing memory is faster than reading memory. Examining the datapath
reveals two possible critical paths that would limit the cycle time:
(7.4)
The numerical values of these times will depend on the specific implementation technology.
Example 7.8 PROCESSOR PERFORMANCE COMPARISON
Ben Bitdiddle is wondering whether he would be better off building the multicycle processor instead of the single-cycle processor. For both designs, he plans on
using a 65 nm CMOS manufacturing process with the delays given in Table 7.6.
Help him compare each processor’s execution time for 100 billion instructions
from the SPECINT2000 benchmark (see Example 7.7).
Solution: According to Equation 7.4, the cycle time of the multicycle processor is
Tc2  30  25  250  20  325 ps. Using the CPI of 4.12 from Example 7.7,
the total execution time is T2  (100  109 instructions)(4.12 cycles/instruction)
(325  1012 s/cycle)  133.9 seconds. According to Example 7.4, the singlecycle processor had a cycle time of Tc1  950 ps, a CPI of 1, and a total execution time of 95 seconds.
One of the original motivations for building a multicycle processor was to
avoid making all instructions take as long as the slowest one. Unfortunately,
this example shows that the multicycle processor is slower than the single-cycle
Tc  tpcq  tmux  max(tALU  tmux, tmem)  tsetup
7.4 Multicycle Processor 399
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ULA
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc1:0
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
IorD IRWrite ALUSrcB1:0 PCWrite
PCEn
00
01
10
<<2
25:0 (addr)
31:28
27:0
PCJump
Figure 7.41 Multicycle MIPS datapath enhanced to support the j instruction
Ch
processor given the assumptions of CPI and circuit element delays. The fundamental problem is that even though the slowest instruction, lw, was broken into
five steps, the multicycle processor cycle time was not nearly improved fivefold. This is partly because not all of the steps are exactly the same length, and
partly because the 50-ps sequencing overhead of the register clk-to-Q and setup
time must now be paid on every step, not just once for the entire instruction. In
general, engineers have learned that it is difficult to exploit the fact that some
computations are faster than others unless the differences are large.
Compared with the single-cycle processor, the multicycle processor is likely to be
less expensive because it eliminates two adders and combines the instruction and
data memories into a single unit. It does, however, require five nonarchitectural
registers and additional multiplexers.
400 CHAPTER SEVEN Microarchitecture
IorD = 0
AluSrcA= 0
ALUSrcB= 01
ALUOp = 00
PCSrc = 00
IRWrite
PCWrite
ALUSrcA= 0
ALUSrcB= 11
ALUOp = 00
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
IorD = 1
RegDst = 1
MemtoReg = 0
RegWrite
IorD = 1
MemWrite
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 10
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 01
PCSrc = 01
Branch
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
RegDst = 0
MemtoReg = 1
RegWrite
S4: Mem
Writeback
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
RegDst = 0
MemtoReg = 0
RegWrite
Op = ADDI
S9: ADDI
Execute
S10: ADDI
Writeback
PCSrc = 10
PCWrite
Op = J
S11: Jump
Figure 7.42 Main controller state for j

7.5 PIPELINED PROCESSOR
Pipelining, introduced in Section 3.6, is a powerful way to improve the
throughput of a digital system. We design a pipelined processor by subdividing the single-cycle processor into five pipeline stages. Thus,
five instructions can execute simultaneously, one in each stage. Because
each stage has only one-fifth of the entire logic, the clock frequency
is almost five times faster. Hence, the latency of each instruction is
ideally unchanged, but the throughput is ideally five times better.
Microprocessors execute millions or billions of instructions per second,
so throughput is more important than latency. Pipelining introduces
some overhead, so the throughput will not be quite as high as we
might ideally desire, but pipelining nevertheless gives such great advantage for so little cost that all modern high-performance microprocessors are pipelined.
Reading and writing the memory and register file and using the ALU
typically constitute the biggest delays in the processor. We choose five
pipeline stages so that each stage involves exactly one of these slow
steps. Specifically, we call the five stages Fetch, Decode, Execute,
Memory, and Writeback. They are similar to the five steps that the
multicycle processor used to perform lw. In the Fetch stage, the processor reads the instruction from instruction memory. In the Decode stage,
the processor reads the source operands from the register file and
decodes the instruction to produce the control signals. In the Execute
stage, the processor performs a computation with the ALU. In the
Memory stage, the processor reads or writes data memory. Finally, in the
Writeback stage, the processor writes the result to the register file, when
applicable.
Figure 7.43 shows a timing diagram comparing the single-cycle and
pipelined processors. Time is on the horizontal axis, and instructions are
on the vertical axis. The diagram assumes the logic element delays from
Table 7.6 but ignores the delays of multiplexers and registers. In the single-cycle processor, Figure 7.43(a), the first instruction is read from
memory at time 0; next the operands are read from the register file; and
then the ALU executes the necessary computation. Finally, the data
memory may be accessed, and the result is written back to the register
file by 950 ps. The second instruction begins when the first completes.
Hence, in this diagram, the single-cycle processor has an instruction
latency of 250
 150
 200
 250
 100
 950 ps and a throughput
of 1 instruction per 950 ps (1.05 billion instructions per second).
In the pipelined processor, Figure 7.43(b), the length of a pipeline
stage is set at 250 ps by the slowest stage, the memory access (in the
Fetch or Memory stage). At time 0, the first instruction is fetched from
memory. At 250 ps, the first instruction enters the Decode stage, and
7.5 Pipelined Processor 401

402
Time (ps)
Instr
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg 1
2
0 100 200 300 400 500 600 700 800 900 1100 1200 1300 1400 1500 1600 1700 1800 1900 1000
(a)
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Instr
1
2
(b)
3
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Fetch
Instruction
Figure 7.43 Timing diagrams: (a) single-cycle processor, (b) pipelined processor

a second instruction is fetched. At 500 ps, the first instruction executes,
the second instruction enters the Decode stage, and a third instruction is
fetched. And so forth, until all the instructions complete. The instruction latency is 5  250  1250 ps. The throughput is 1 instruction per
250 ps (4 billion instructions per second). Because the stages are not
perfectly balanced with equal amounts of logic, the latency is slightly
longer for the pipelined than for the single-cycle processor. Similarly,
the throughput is not quite five times as great for a five-stage pipeline as
for the single-cycle processor. Nevertheless, the throughput advantage is
substantial.
Figure 7.44 shows an abstracted view of the pipeline in operation in
which each stage is represented pictorially. Each pipeline stage is represented with its major component—instruction memory (IM), register file
(RF) read, ALU execution, data memory (DM), and register file writeback—to illustrate the flow of instructions through the pipeline. Reading
across a row shows the clock cycles in which a particular instruction is
in each stage. For example, the sub instruction is fetched in cycle 3 and
executed in cycle 5. Reading down a column shows what the various
pipeline stages are doing on a particular cycle. For example, in cycle 6,
the or instruction is being fetched from instruction memory, while $s1 is
being read from the register file, the ALU is computing $t5 AND $t6,
the data memory is idle, and the register file is writing a sum to $s3.
Stages are shaded to indicate when they are used. For example, the data
memory is used by lw in cycle 4 and by sw in cycle 8. The instruction
memory and ALU are used in every cycle. The register file is written by
7.5 Pipelined Processor 403
Time (cycles)
lw $s2, 40($0) RF 40
$0
RF $s2 + DM
RF $t2
$t1
RF $s3 + DM
RF $s5
$s1
RF $s4 - DM
RF $t6
$t5
RF $s5 & DM
RF 20
$s1
RF $s6 + DM
RF $t4
$t3
RF $s7
| DM
add $s3, $t1, $t2
sub $s4, $s1, $s5
and $s5, $t5, $t6
sw $s6, 20($s1)
or $s7, $t3, $t4
1 2 3 4 5 6 7 8 9 10
add
IM
IM
IM
IM
IM
IM lw
sub
and
sw
or
Figure 7.44 Abstract view of pipeline in operation
C
every instruction except sw. We assume that in the pipelined processor,
the register file is written in the first part of a cycle and read in the second part, as suggested by the shading. This way, data can be written and
read back within a single cycle.
A central challenge in pipelined systems is handling hazards that
occur when the results of one instruction are needed by a subsequent
instruction before the former instruction has completed. For example, if
the add in Figure 7.44 used $s2 rather than $t2, a hazard would occur
because the $s2 register has not been written by the lw by the time it is
read by the add. This section explores forwarding, stalls, and flushes as
methods to resolve hazards. Finally, this section revisits performance
analysis considering sequencing overhead and the impact of hazards.
7.5.1 Pipelined Datapath
The pipelined datapath is formed by chopping the single-cycle datapath
into five stages separated by pipeline registers. Figure 7.45(a) shows the
single-cycle datapath stretched out to leave room for the pipeline registers. Figure 7.45(b) shows the pipelined datapath formed by inserting
four pipeline registers to separate the datapath into five stages. The
stages and their boundaries are indicated in blue. Signals are given a suffix (F, D, E, M, or W) to indicate the stage in which they reside.
The register file is peculiar because it is read in the Decode stage and
written in the Writeback stage. It is drawn in the Decode stage, but the
write address and data come from the Writeback stage. This feedback
will lead to pipeline hazards, which are discussed in Section 7.5.3.
One of the subtle but critical issues in pipelining is that all signals
associated with a particular instruction must advance through the
pipeline in unison. Figure 7.45(b) has an error related to this issue. Can
you find it?
The error is in the register file write logic, which should operate in
the Writeback stage. The data value comes from ResultW, a Writeback
stage signal. But the address comes from WriteRegE, an Execute stage
signal. In the pipeline diagram of Figure 7.44, during cycle 5, the result
of the lw instruction would be incorrectly written to register $s4 rather
than $s2.
Figure 7.46 shows a corrected datapath. The WriteReg signal is now
pipelined along through the Memory and Writeback stages, so it remains
in sync with the rest of the instruction. WriteRegW and ResultW are fed
back together to the register file in the Writeback stage.
The astute reader may notice that the PC logic is also problematic, because it might be updated with a Fetch or a Memory stage
signal (PCPlus4F or PCBranchM). This control hazard will be fixed in
Section 7.5.3.
404 CHAPTER SEVEN Microarchitecture
C
7.5.2 Pipelined Control
The pipelined processor takes the same control signals as the single-cycle
processor and therefore uses the same control unit. The control unit
examines the opcode and funct fields of the instruction in the Decode
stage to produce the control signals, as was described in Section 7.3.2.
These control signals must be pipelined along with the data so that they
remain synchronized with the instruction.
The entire pipelined processor with control is shown in Figure 7.47.
RegWrite must be pipelined into the Writeback stage before it feeds back
to the register file, just as WriteReg was pipelined in Figure 7.46.
7.5 Pipelined Processor 405
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
SrcB
20:16
15:11
<<2
ALUResult
ALU
ReadData
WriteData
SrcA
PCPlus4 +
+ PCBranch
WriteReg4:0
Result
Zero
CLK
(a)
SignImmE
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PCF
1
PC' InstrD 25:21
20:16
15:0
SrcBE
20:16
15:11
RtE
RdE
<<2
ALUOutM
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchM
ResultW
PCPlus4F PCPlus4E
ZeroM
CLK CLK
WriteRegE4:0
CLK
CLK
CLK
ALU
+
+
(b)
Fetch Decode Execute Memory Writeback
Figure 7.45 Single-cycle and pipelined datapaths

7.5.3 Hazards
In a pipelined system, multiple instructions are handled concurrently.
When one instruction is dependent on the results of another that has not
yet completed, a hazard occurs.
406 CHAPTER SEVEN Microarchitecture
SignImmE
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PCF
1
PC' InstrD 25:21
20:16
15:0
5:0
SrcBE
20:16
15:11
RtE
RdE
<<2
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchM
WriteRegM4:0
ResultW
PCPlus4F PCPlus4E
31:26
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
ZeroM
PCSrcM
CLK CLK CLK
CLK CLK
WriteRegW4:0
ALUControlE2:0
RegWriteE RegWriteM RegWriteW
MemtoRegE MemtoRegM MemtoRegW
MemWriteE MemWriteM
BranchE BranchM
RegDstE
ALUSrcE
WriteRegE4:0
Figure 7.47 Pipelined processor with control
SignImmE
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PCF
1
PC' InstrD 25:21
20:16
15:0
SrcBE
20:16
15:11
RtE
RdE
<<2
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchM
WriteRegM4:0
ResultW
PCPlus4F PCPlus4E
ZeroM
CLK CLK
WriteRegE4:0 WriteRegW4:0
CLK
CLK
CLK
Fetch Decode Execute Memory Writeback
Figure 7.46 Corrected pipelined datapath

The register file can be read and written in the same cycle. Let us
assume that the write takes place during the first half of the cycle and the
read takes place during the second half of the cycle, so that a register can
be written and read back in the same cycle without introducing a hazard.
Figure 7.48 illustrates hazards that occur when one instruction
writes a register ($s0) and subsequent instructions read this register. This
is called a read after write (RAW) hazard. The add instruction writes a
result into $s0 in the first half of cycle 5. However, the and instruction
reads $s0 on cycle 3, obtaining the wrong value. The or instruction
reads $s0 on cycle 4, again obtaining the wrong value. The sub instruction reads $s0 in the second half of cycle 5, obtaining the correct value,
which was written in the first half of cycle 5. Subsequent instructions
also read the correct value of $s0. The diagram shows that hazards may
occur in this pipeline when an instruction writes a register and either of
the two subsequent instructions read that register. Without special treatment, the pipeline will compute the wrong result.
On closer inspection, however, observe that the sum from the add
instruction is computed by the ALU in cycle 3 and is not strictly needed
by the and instruction until the ALU uses it in cycle 4. In principle, we
should be able to forward the result from one instruction to the next to
resolve the RAW hazard without slowing down the pipeline. In other
situations explored later in this section, we may have to stall the pipeline
to give time for a result to be computed before the subsequent instruction uses the result. In any event, something must be done to solve
hazards so that the program executes correctly despite the pipelining.
Hazards are classified as data hazards or control hazards. A data
hazard occurs when an instruction tries to read a register that has not
yet been written back by a previous instruction. A control hazard occurs
when the decision of what instruction to fetch next has not been made
by the time the fetch takes place. In the remainder of this section, we will
7.5 Pipelined Processor 407
Time (cycles)
add $s0, $s2, $s3 RF $s3
$s2
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM add
or
sub
Figure 7.48 Abstract pipeline diagram illustrating hazards

enhance the pipelined processor with a hazard unit that detects hazards
and handles them appropriately, so that the processor executes the
program correctly.
Solving Data Hazards with Forwarding
Some data hazards can be solved by forwarding (also called bypassing) a
result from the Memory or Writeback stage to a dependent instruction in
the Execute stage. This requires adding multiplexers in front of the ALU
to select the operand from either the register file or the Memory or
Writeback stage. Figure 7.49 illustrates this principle. In cycle 4, $s0 is
forwarded from the Memory stage of the add instruction to the Execute
stage of the dependent and instruction. In cycle 5, $s0 is forwarded from
the Writeback stage of the add instruction to the Execute stage of the
dependent or instruction.
Forwarding is necessary when an instruction in the Execute stage
has a source register matching the destination register of an instruction
in the Memory or Writeback stage. Figure 7.50 modifies the pipelined
processor to support forwarding. It adds a hazard detection unit and
two forwarding multiplexers. The hazard detection unit receives the two
source registers from the instruction in the Execute stage and the destination registers from the instructions in the Memory and Writeback
stages. It also receives the RegWrite signals from the Memory and
Writeback stages to know whether the destination register will actually
be written (for example, the sw and beq instructions do not write results
to the register file and hence do not need to have their results forwarded). Note that the RegWrite signals are connected by name. In
other words, rather than cluttering up the diagram with long wires running from the control signals at the top to the hazard unit at the bottom,
the connections are indicated by a short stub of wire labeled with the
control signal name to which it is connected.
408 CHAPTER SEVEN Microarchitecture
Time (cycles)
add $s0, $s2, $s3 RF $s3
$s2
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM add
or
sub
Figure 7.49 Abstract pipeline diagram illustrating forwarding

409
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
4
+
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
25:21
20:16
15:0
5:0
EBcrS
25:21
15:11
EsR
EdR
2<<
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
MhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
31:26
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
DmmIngiS
20:16 EtR
DsR
DdR
DtR
tinU
ForwardAE
ForwardBE
RegWriteM
RegWriteW
 drazaH
E4sulPCP
EhcnarB MhcnarB
MoreZ
ALU
Figure 7.50 Pipelined processor with forwarding to solve hazards

The hazard detection unit computes control signals for the forwarding multiplexers to choose operands from the register file or from the
results in the Memory or Writeback stage. It should forward from a
stage if that stage will write a destination register and the destination
register matches the source register. However, $0 is hardwired to 0 and
should never be forwarded. If both the Memory and Writeback stages
contain matching destination registers, the Memory stage should have
priority, because it contains the more recently executed instruction. In
summary, the function of the forwarding logic for SrcA is given below.
The forwarding logic for SrcB (ForwardBE) is identical except that it
checks rt rather than rs.
if ((rsE ! 0) AND (rsE  WriteRegM) AND RegWriteM) then
ForwardAE  10
else if ((rsE ! 0) AND (rsE  WriteRegW) AND RegWriteW) then
ForwardAE  01
else ForwardAE  00
Solving Data Hazards with Stalls
Forwarding is sufficient to solve RAW data hazards when the result is
computed in the Execute stage of an instruction, because its result can
then be forwarded to the Execute stage of the next instruction.
Unfortunately, the lw instruction does not finish reading data until the
end of the Memory stage, so its result cannot be forwarded to the
Execute stage of the next instruction. We say that the lw instruction has
a two-cycle latency, because a dependent instruction cannot use its result
until two cycles later. Figure 7.51 shows this problem. The lw instruction
receives data from memory at the end of cycle 4. But the and instruction
needs that data as a source operand at the beginning of cycle 4. There is
no way to solve this hazard with forwarding.
410 CHAPTER SEVEN Microarchitecture
Time (cycles)
lw $s0, 40($0) RF 40
$0
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM lw
or
sub
Trouble!
Figure 7.51 Abstract pipeline diagram illustrating trouble forwarding from lw

The alternative solution is to stall the pipeline, holding up operation
until the data is available. Figure 7.52 shows stalling the dependent
instruction (and) in the Decode stage. and enters the Decode stage in
cycle 3 and stalls there through cycle 4. The subsequent instruction (or)
must remain in the Fetch stage during both cycles as well, because the
Decode stage is full.
In cycle 5, the result can be forwarded from the Writeback stage of
lw to the Execute stage of and. In cycle 6, source $s0 of the or instruction is read directly from the register file, with no need for forwarding.
Notice that the Execute stage is unused in cycle 4. Likewise,
Memory is unused in Cycle 5 and Writeback is unused in cycle 6. This
unused stage propagating through the pipeline is called a bubble, and it
behaves like a nop instruction. The bubble is introduced by zeroing out
the Execute stage control signals during a Decode stall so that the bubble
performs no action and changes no architectural state.
In summary, stalling a stage is performed by disabling the pipeline
register, so that the contents do not change. When a stage is stalled, all
previous stages must also be stalled, so that no subsequent instructions
are lost. The pipeline register directly after the stalled stage must be
cleared to prevent bogus information from propagating forward. Stalls
degrade performance, so they should only be used when necessary.
Figure 7.53 modifies the pipelined processor to add stalls for lw
data dependencies. The hazard unit examines the instruction in the
Execute stage. If it is lw and its destination register (rtE) matches either
source operand of the instruction in the Decode stage (rsD or rtD), that
instruction must be stalled in the Decode stage until the source operand
is ready.
Stalls are supported by adding enable inputs (EN) to the Fetch and
Decode pipeline registers and a synchronous reset/clear (CLR) input to
the Execute pipeline register. When a lw stall occurs, StallD and StallF
7.5 Pipelined Processor 411
Time (cycles)
lw $s0, 40($0) RF 40
$0
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM lw
or
sub
9
RF $s1
$s0
IM or
Stall
Figure 7.52 Abstract pipeline diagram illustrating stall to solve hazards

412
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
4
+
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
12:52
61:02
0:51
0:5
EBcrS
12:52
11:51
EsR
EdR
2<<
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
MhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
62:13
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
DmmIngiS
61:02 EtR
DsR
DdR
DtR
tinU drazaH
E4sulPCP
EhcnarB MhcnarB
MoreZ
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallD EN
EN
ALU
CLR
StallF
RegWriteW
Figure 7.53 Pipelined processor with stalls to solve lw data hazard

are asserted to force the Decode and Fetch stage pipeline registers to
hold their old values. FlushE is also asserted to clear the contents of the
Execute stage pipeline register, introducing a bubble.4
The MemtoReg signal is asserted for the lw instruction. Hence, the
logic to compute the stalls and flushes is
lwstall  ((rsD  rtE) OR (rtD  rtE)) AND MemtoRegE
StallF  StallD  FlushE  lwstall
Solving Control Hazards
The beq instruction presents a control hazard: the pipelined processor
does not know what instruction to fetch next, because the branch decision has not been made by the time the next instruction is fetched.
One mechanism for dealing with the control hazard is to stall the
pipeline until the branch decision is made (i.e., PCSrc is computed).
Because the decision is made in the Memory stage, the pipeline would
have to be stalled for three cycles at every branch. This would severely
degrade the system performance.
An alternative is to predict whether the branch will be taken and
begin executing instructions based on the prediction. Once the branch
decision is available, the processor can throw out the instructions if the
prediction was wrong. In particular, suppose that we predict that
branches are not taken and simply continue executing the program in
order. If the branch should have been taken, the three instructions following the branch must be flushed (discarded) by clearing the pipeline
registers for those instructions. These wasted instruction cycles are called
the branch misprediction penalty.
Figure 7.54 shows such a scheme, in which a branch from address 20
to address 64 is taken. The branch decision is not made until cycle 4, by
which point the and, or, and sub instructions at addresses 24, 28, and 2C
have already been fetched. These instructions must be flushed, and the
slt instruction is fetched from address 64 in cycle 5. This is somewhat of
an improvement, but flushing so many instructions when the branch is
taken still degrades performance.
We could reduce the branch misprediction penalty if the branch
decision could be made earlier. Making the decision simply requires
comparing the values of two registers. Using a dedicated equality comparator is much faster than performing a subtraction and zero detection.
If the comparator is fast enough, it could be moved back into the
Decode stage, so that the operands are read from the register file and
compared to determine the next PC by the end of the Decode stage.
7.5 Pipelined Processor 413
4 Strictly speaking, only the register designations (RsE, RtE, and RdE) and the control signals that might update memory or architectural state (RegWrite, MemWrite, and Branch)
need to be cleared; as long as these signals are cleared, the bubble can contain random data
that has no effect.

Figure 7.55 shows the pipeline operation with the early branch decision being made in cycle 2. In cycle 3, the and instruction is flushed and
the slt instruction is fetched. Now the branch misprediction penalty is
reduced to only one instruction rather than three.
Figure 7.56 modifies the pipelined processor to move the branch
decision earlier and handle control hazards. An equality comparator is
added to the Decode stage and the PCSrc AND gate is moved earlier, so
414 CHAPTER SEVEN Microarchitecture
Time (cycles)
beq $t1, $t2, 40 RF $t2
$t1
- RF DM
RF $s1
$s0
RF DM
RF $s0
$s4
| RF DM
RF $s5
$s0
- RF DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM lw
or
sub
20
24
28
2C
30
...
...
9
Flush
these
instructions
64 slt $t3, $s2, $s3 RF $s3
$s2
RF DM $t3 IM slt slt
&
Figure 7.54 Abstract pipeline diagram illustrating flushing when a branch is taken
Time (cycles)
beq $t1, $t2, 40 RF $t2
$t1
- RF DM
RF $s1
$s0
& RF DM and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and IM
IM lw 20
24
28
2C
30
...
...
9
Flush
this
instruction
64 slt $t3, $s2, $s3 RF $s3
$s2
RF DM $t3 IM slt slt
Figure 7.55 Abstract pipeline diagram illustrating earlier branch decision

415
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
4
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
12:52
61:02
0:51
0:5
EBcrS
12:52
11:51
EsR
EdR
2<<
+
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
DhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
62:13
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
DcrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
=
DmmIngiS
61:02 EtR
DsR
DdR
DtR
tinU drazaH
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallF
StallD
RegWriteW
CLR
EN
EN
ALU
CLR
Figure 7.56 Pipelined processor handling branch control hazard

that PCSrc can be determined in the Decoder stage rather than the
Memory stage. The PCBranch adder must also be moved into the
Decode stage so that the destination address can be computed in time.
The synchronous clear input (CLR) connected to PCSrcD is added to the
Decode stage pipeline register so that the incorrectly fetched instruction
can be flushed when a branch is taken.
Unfortunately, the early branch decision hardware introduces a new
RAW data hazard. Specifically, if one of the source operands for the branch
was computed by a previous instruction and has not yet been written into
the register file, the branch will read the wrong operand value from the register file. As before, we can solve the data hazard by forwarding the correct
value if it is available or by stalling the pipeline until the data is ready.
Figure 7.57 shows the modifications to the pipelined processor needed
to handle the Decode stage data dependency. If a result is in the Writeback
stage, it will be written in the first half of the cycle and read during
the second half, so no hazard exists. If the result of an ALU instruction is
in the Memory stage, it can be forwarded to the equality comparator
through two new multiplexers. If the result of an ALU instruction is in the
Execute stage or the result of a lw instruction is in the Memory stage, the
pipeline must be stalled at the Decode stage until the result is ready.
The function of the Decode stage forwarding logic is given below.
ForwardAD  (rsD ! 0) AND (rsD  WriteRegM) AND RegWriteM
ForwardBD  (rtD ! 0) AND (rtD  WriteRegM) AND RegWriteM
The function of the stall detection logic for a branch is given below.
The processor must make a branch decision in the Decode stage. If either
of the sources of the branch depends on an ALU instruction in the
Execute stage or on a lw instruction in the Memory stage, the processor
must stall until the sources are ready.
branchstall 
BranchD AND RegWriteE AND (WriteRegE  rsD OR WriteRegE  rtD)
OR
BranchD AND MemtoRegM AND (WriteRegM  rsD OR WriteRegM  rtD)
Now the processor might stall due to either a load or a branch hazard:
StallF  StallD  FlushE  lwstall OR branchstall
Hazard Summary
In summary, RAW data hazards occur when an instruction depends on
the result of another instruction that has not yet been written into the
register file. The data hazards can be resolved by forwarding if the result
is computed soon enough; otherwise, they require stalling the pipeline
until the result is available. Control hazards occur when the decision of
what instruction to fetch has not been made by the time the next instruction must be fetched. Control hazards are solved by predicting which
416 CHAPTER SEVEN Microarchitecture

417
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
omeM yr
4
+
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
12:52
61:02
0:51
0:5
EBcrS
12:52
11:51
EsR
EdR
2<<
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
DhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
62:13
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
Control
Unit
DcrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
0
1
0
1
=
DmmIngiS
61:02 EtR
DsR
DdR
DtR
tinU drazaH
ForwardAD
ForwardAE
ForwardBE
BranchD
ForwardBD
RegWriteE
RegWriteM
MemtoRegE
MemtoRegM
FlushE
CLR
CLR
EN
EN
ALU
StallF
StallD
RegWriteW
Figure 7.57 Pipelined processor handling data dependencies for branch instructions

instruction should be fetched and flushing the pipeline if the prediction is
later determined to be wrong. Moving the decision as early as possible
minimizes the number of instructions that are flushed on a misprediction.
You may have observed by now that one of the challenges of designing a
pipelined processor is to understand all the possible interactions between
instructions and to discover all the hazards that may exist. Figure 7.58
shows the complete pipelined processor handling all of the hazards.
7.5.4 More Instructions
Supporting new instructions in the pipelined processor is much like supporting them in the single-cycle processor. However, new instructions
may introduce hazards that must be detected and solved.
In particular, supporting addi and j instructions on the pipelined
processor requires enhancing the controller, exactly as was described in
Section 7.3.3, and adding a jump multiplexer to the datapath after the
branch multiplexer. Like a branch, the jump takes place in the Decode
stage, so the subsequent instruction in the Fetch stage must be flushed.
Designing this flush logic is left as Exercise 7.29.
7.5.5 Performance Analysis
The pipelined processor ideally would have a CPI of 1, because a new
instruction is issued every cycle. However, a stall or a flush wastes a
cycle, so the CPI is slightly higher and depends on the specific program
being executed.
Example 7.9 PIPELINED PROCESSOR CPI
The SPECINT2000 benchmark considered in Example 7.7 consists of approximately 25% loads, 10% stores, 11% branches, 2% jumps, and 52% R-type
instructions. Assume that 40% of the loads are immediately followed by an
instruction that uses the result, requiring a stall, and that one quarter of the
branches are mispredicted, requiring a flush. Assume that jumps always flush
the subsequent instruction. Ignore other hazards. Compute the average CPI of
the pipelined processor.
Solution: The average CPI is the sum over each instruction of the CPI for that
instruction multiplied by the fraction of time that instruction is used. Loads take
one clock cycle when there is no dependency and two cycles when the processor
must stall for a dependency, so they have a CPI of (0.6)(1)  (0.4)(2)  1.4.
Branches take one clock cycle when they are predicted properly and two when they
are not, so they have a CPI of (0.75)(1)  (0.25)(2)  1.25. Jumps always have a
CPI of 2. All other instructions have a CPI of 1. Hence, for this benchmark, Average
CPI  (0.25)(1.4)  (0.1)(1)  (0.11)(1.25)  (0.02)(2)  (0.52)(1)  1.15.
418 CHAPTER SEVEN Microarchitecture

419
EqualD
SignImmE
CLK
A RD
Instruction
Memory
+
4
A1
A3
WD3
RD2
RD1
WE3
A2
CLK
Sign
Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
1
0
0 PCF
1
PC' InstrD
25:21
20:16
15:0
5:0
SrcBE
25:21
15:11
RsE
RdE
<<2
+
ALUOutM
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchD
WriteRegM4:0
ResultW
PCPlus4F
31:26
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD2:0
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
PCSrcD
CLK CLK CLK
CLK CLK
WriteRegW4:0
ALUControlE2:0
ALU
RegWriteE RegWriteM RegWriteW
MemtoRegE MemtoRegM MemtoRegW
MemWriteE MemWriteM
RegDstE
ALUSrcE
WriteRegE4:0
00
01
10
00
01
10
0
1
0
1
=
SignImmD
20:16 RtE
RsD
RdD
RtD
Hazard Unit
StallF
StallD
ForwardAD
ForwardBD
ForwardAE
ForwardBE
MemtoRegE
RegWriteE
RegWriteM
RegWriteW
BranchD
FlushE
EN
EN CLR
CLR
Figure 7.58 Pipelined processor with full hazard handling

We can determine the cycle time by considering the critical path in
each of the five pipeline stages shown in Figure 7.58. Recall that the register file is written in the first half of the Writeback cycle and read in the second half of the Decode cycle. Therefore, the cycle time of the Decode and
Writeback stages is twice the time necessary to do the half-cycle of work.
(7.5)
Example 7.10 PROCESSOR PERFORMANCE COMPARISON
Ben Bitdiddle needs to compare the pipelined processor performance to that of
the single-cycle and multicycle processors considered in Example 7.8. Most of
the logic delays were given in Table 7.6. The other element delays are 40 ps for
an equality comparator, 15 ps for an AND gate, 100 ps for a register file write,
and 220 ps for a memory write. Help Ben compare the execution time of 100
billion instructions from the SPECINT2000 benchmark for each processor.
Solution: According to Equation 7.5, the cycle time of the pipelined processor is
Tc3  max[30  250  20, 2(150  25  40  15  25  20), 30  25  25 
200  20, 30  220  20, 2(30  25  100)]  550 ps. According to Equation
7.1, the total execution time is T3  (100  109 instructions)(1.15 cycles/ instruction)(550  1012 s/cycle)  63.3 seconds. This compares to 95 seconds for the
single-cycle processor and 133.9 seconds for the multicycle processor.
The pipelined processor is substantially faster than the others. However, its advantage over the single-cycle processor is nowhere near the five-fold speedup one might
hope to get from a five-stage pipeline. The pipeline hazards introduce a small CPI
penalty. More significantly, the sequencing overhead (clk-to-Q and setup times) of
the registers applies to every pipeline stage, not just once to the overall datapath.
Sequencing overhead limits the benefits one can hope to achieve from pipelining.
The careful reader might observe that the Decode stage is substantially slower than
the others, because the register file write, read, and branch comparison must all
happen in half a cycle. Perhaps moving the branch comparison to the Decode stage
was not such a good idea. If branches were resolved in the Execute stage instead,
the CPI would increase slightly, because a mispredict would flush two instructions,
but the cycle time would decrease substantially, giving an overall speedup.
The pipelined processor is similar in hardware requirements to the
single-cycle processor, but it adds a substantial number of pipeline registers, along with multiplexers and control logic to resolve hazards.
Tc  max tpcq  tmem  tsetup
2(tRFread tmux  teq  tAND  tmux tsetup)
tpcq  tmux  tmux  tALU  tsetup
tpcq  tmemwrite  tsetup
2(tpcq  tmux  tRFwrite)

420 CHAPTER SEVEN Microarchitecture
Fetch
Decode
Execute
Memory
Writeback
Chap
7.6 HDL REPRESENTATION*
This section presents HDL code for the single-cycle MIPS processor
supporting all of the instructions discussed in this chapter, including
addi and j. The code illustrates good coding practices for a moderately
complex system. HDL code for the multicycle processor and pipelined
processor are left to Exercises 7.22 and 7.33.
In this section, the instruction and data memories are separated from
the main processor and connected by address and data busses. This is
more realistic, because most real processors have external memory. It also
illustrates how the processor can communicate with the outside world.
The processor is composed of a datapath and a controller. The
controller, in turn, is composed of the main decoder and the ALU
decoder. Figure 7.59 shows a block diagram of the single-cycle MIPS
processor interfaced to external memories.
The HDL code is partitioned into several sections. Section 7.6.1
provides HDL for the single-cycle processor datapath and controller.
Section 7.6.2 presents the generic building blocks, such as registers and
multiplexers, that are used by any microarchitecture. Section 7.6.3
7.6 HDL Representation 421
Opcode5:0
Controller Funct5:0
Datapath
A RD
Instruction
Memory
PC Instr
A RD
Data
Memory
WD
WE
CLK
ALUOut
WriteData
ReadData
ALUControl
Branch
MemtoReg
ALUSrc
RegDst
MemWrite
2:0
Main
Decoder
ALUOp1:0 ALU
Decoder
RegWrite
5:0 31:26
CLK
Reset
MIPS Processor External Memory PCSrc Zero
Figure 7.59 MIPS single-cycle processor interfaced to external memory

introduces the testbench and external memories. The HDL is available
in electronic form on the this book’s Web site (see the preface).
7.6.1 Single-Cycle Processor
The main modules of the single-cycle MIPS processor module are given
in the following HDL examples.
422 CHAPTER SEVEN Microarchitecture
Verilog
module mips (input clk, reset,
output [31:0] pc,
input [31:0] instr,
output memwrite,
output [31:0] aluout, writedata,
input [31:0] readdata);
wire memtoreg, branch,
alusrc, regdst, regwrite, jump;
wire [2:0] alucontrol;
controller c(instr[31:26], instr[5:0], zero,
memtoreg, memwrite, pcsrc,
alusrc, regdst, regwrite, jump,
alucontrol);
datapath dp(clk, reset, memtoreg, pcsrc,
alusrc, regdst, regwrite, jump,
alucontrol,
zero, pc, instr,
aluout, writedata, readdata);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mips is — — single cycle MIPS processor
port (clk, reset: in STD_LOGIC;
pc: out STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR (31 downto 0);
memwrite: out STD_LOGIC;
aluout, writedata: out STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR (31 downto 0));
end;
architecture struct of mips is
component controller
port (op, funct: in STD_LOGIC_VECTOR (5 downto 0);
zero: in STD_LOGIC;
memtoreg, memwrite: out STD_LOGIC;
pcsrc, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end component;
component datapath
port (clk, reset: in STD_LOGIC;
memtoreg, pcsrc: in STD_LOGIC;
alusrc, regdst: in STD_LOGIC;
regwrite, jump: in STD_LOGIC;
alucontrol: in STD_LOGIC_VECTOR (2 downto 0);
zero: out STD_LOGIC;
pc: buffer STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR (31 downto 0);
aluout, writedata: buffer STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR (31 downto 0));
end component;
signal memtoreg, alusrc, regdst, regwrite, jump, pcsrc:
STD_LOGIC;
signal zero: STD_LOGIC;
signal alucontrol: STD_LOGIC_VECTOR (2 downto 0);
begin
cont: controller port map (instr (31 downto 26), instr
(5 downto 0), zero, memtoreg,
memwrite, pcsrc, alusrc, regdst,
regwrite, jump, alucontrol);
dp: datapath port map (clk, reset, memtoreg, pcsrc, alusrc,
regdst, regwrite, jump, alucontrol,
zero, pc, instr, aluout, writedata,
readdata);
end;
HDL Example 7.1 SINGLE-CYCLE MIPS PROCESSOR

7.6 HDL Representation 423
Verilog
module controller (input [5:0] op, funct,
input zero,
output memtoreg, memwrite,
output pcsrc, alusrc,
output regdst, regwrite,
output jump,
output [2:0] alucontrol);
wire [1:0] aluop;
wire branch;
maindec md (op, memtoreg, memwrite, branch,
alusrc, regdst, regwrite, jump,
aluop);
aludec ad (funct, aluop, alucontrol);
assign pcsrc  branch & zero;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity controller is — — single cycle control decoder
port (op, funct: in STD_LOGIC_VECTOR (5 downto 0);
zero: in STD_LOGIC;
memtoreg, memwrite: out STD_LOGIC;
pcsrc, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end;
architecture struct of controller is
component maindec
port (op: in STD_LOGIC_VECTOR (5 downto 0);
memtoreg, memwrite: out STD_LOGIC;
branch, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
aluop: out STD_LOGIC_VECTOR (1 downto 0));
end component;
component aludec
port (funct: in STD_LOGIC_VECTOR (5 downto 0);
aluop: in STD_LOGIC_VECTOR (1 downto 0);
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end component;
signal aluop: STD_LOGIC_VECTOR (1 downto 0);
signal branch: STD_LOGIC;
begin
md: maindec port map (op, memtoreg, memwrite, branch,
alusrc, regdst, regwrite, jump, aluop);
ad: aludec port map (funct, aluop, alucontrol);
pcsrc  branch and zero;
end;
HDL Example 7.2 CONTROLLER

Verilog
module maindec(input [5:0] op,
output memtoreg, memwrite,
output branch, alusrc,
output regdst, regwrite,
output jump,
output [1:0] aluop);
reg [8:0] controls;
assign {regwrite, regdst, alusrc,
branch, memwrite,
memtoreg, jump, aluop}  controls;
always @ (*)
case(op)
6b000000: controls  9b110000010; //Rtyp
6b100011: controls  9b101001000; //LW
6b101011: controls  9b001010000; //SW
6b000100: controls  9b000100001; //BEQ
6b001000: controls  9b101000000; //ADDI
6b000010: controls  9b000000100; //J
default: controls  9bxxxxxxxxx; //???
endcase
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity maindec is — — main control decoder
port (op: in STD_LOGIC_VECTOR (5 downto 0);
memtoreg, memwrite: out STD_LOGIC;
branch, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
aluop: out STD_LOGIC_VECTOR (1 downto 0));
end;
architecture behave of maindec is
signal controls: STD_LOGIC_VECTOR(8 downto 0);
begin
process(op) begin
case op is
when "000000"  controls  "110000010"; — — Rtyp
when "100011"  controls  "101001000"; — — LW
when "101011"  controls  "001010000"; — — SW
when "000100"  controls  "000100001"; — — BEQ
when "001000"  controls  "101000000"; — — ADDI
when "000010"  controls  "000000100"; — — J
when others  controls  "---------"; — — illegal op
end case;
end process;
regwrite  controls(8);
regdst  controls(7);
alusrc  controls(6);
branch  controls(5);
memwrite  controls(4);
memtoreg  controls(3);
jump  controls(2);
aluop  controls(1 downto 0);
end;
HDL Example 7.3 MAIN DECODER
Verilog
module aludec (input [5:0] funct,
input [1:0] aluop,
output reg [2:0] alucontrol);
always @ (*)
case (aluop)
2b00: alucontrol  3b010; // add
2b01: alucontrol  3b110; // sub
default: case(funct) // RTYPE
6b100000: alucontrol  3b010; // ADD
6b100010: alucontrol  3b110; // SUB
6b100100: alucontrol  3b000; // AND
6b100101: alucontrol  3b001; // OR
6b101010: alucontrol  3b111; // SLT
default: alucontrol  3bxxx; // ???
endcase
endcase
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity aludec is — — ALU control decoder
port (funct: in STD_LOGIC_VECTOR (5 downto 0);
aluop: in STD_LOGIC_VECTOR (1 downto 0);
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end;
architecture behave of aludec is
begin
process (aluop, funct) begin
case aluop is
when "00"  alucontrol  "010"; — — add (for 1b/sb/addi)
when "01"  alucontrol  "110"; — — sub (for beq)
when others  case funct is — — R-type instructions
when "100000"  alucontrol 
"010"; — — add
when "100010"  alucontrol 
"110"; — — sub
when "100100"  alucontrol 
"000"; — — and
when "100101"  alucontrol 
"001"; — — or
when "101010"  alucontrol 
"111"; — — slt
when others  alucontrol 
"— — —"; — — ???
end case;
end case;
end process;
end;
HDL Example 7.4 ALU DECODER
Chapter 07.qxd 2/1/07 9:33 P
Verilog
module datapath (input clk, reset,
input memtoreg, pcsrc,
input alusrc, regdst,
input regwrite, jump,
input [2:0] alucontrol,
output zero,
output [31:0] pc,
input [31:0] instr,
output [31:0] aluout, writedata,
input [31:0] readdata);
wire [4:0] writereg;
wire [31:0] pcnext, pcnextbr, pcplus4, pcbranch;
wire [31:0] signimm, signimmsh;
wire [31:0] srca, srcb;
wire [31:0] result;
// next PC logic
flopr #(32) pcreg(clk, reset, pcnext, pc);
adder pcadd1 (pc, 32b100, pcplus4);
sl2 immsh(signimm, signimmsh);
adder pcadd2(pcplus4, signimmsh, pcbranch);
mux2 #(32) pcbrmux(pcplus4, pcbranch, pcsrc,
pcnextbr);
mux2 #(32) pcmux(pcnextbr, {pcplus4[31:28],
instr[25:0], 2b00},
jump, pcnext);
// register file logic
regfile rf(clk, regwrite, instr[25:21],
instr[20:16], writereg,
result, srca, writedata);
mux2 #(5) wrmux(instr[20:16], instr[15:11],
regdst, writereg);
mux2 #(32) resmux(aluout, readdata,
memtoreg, result);
signext se(instr[15:0], signimm);
// ALU logic
mux2 #(32) srcbmux(writedata, signimm, alusrc,
srcb);
alu alu(srca, srcb, alucontrol,
aluout, zero);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all; use
IEEE.STD_LOGIC_ARITH.all;
entity datapath is — — MIPS datapath
port(clk, reset: in STD_LOGIC;
memtoreg, pcsrc: in STD_LOGIC;
alusrc, regdst: in STD_LOGIC;
regwrite, jump: in STD_LOGIC;
alucontrol: in STD_LOGIC_VECTOR (2 downto 0);
zero: out STD_LOGIC;
pc: buffer STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR(31 downto 0);
aluout, writedata: buffer STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR(31 downto 0));
end;
architecture struct of datapath is
component alu
port(a, b: in STD_LOGIC_VECTOR(31 downto 0);
alucontrol: in STD_LOGIC_VECTOR(2 downto 0);
result: buffer STD_LOGIC_VECTOR(31 downto 0);
zero: out STD_LOGIC);
end component;
component regfile
port(clk: in STD_LOGIC;
we3: in STD_LOGIC;
ra1, ra2, wa3: in STD_LOGIC_VECTOR (4 downto 0);
wd3: in STD_LOGIC_VECTOR (31 downto 0);
rd1, rd2: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component adder
port(a, b: in STD_LOGIC_VECTOR (31 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component sl2
port(a: in STD_LOGIC_VECTOR (31 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component signext
port(a: in STD_LOGIC_VECTOR (15 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component flopr generic (width: integer);
port(clk, reset: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (width-1 downto 0);
q: out STD_LOGIC_VECTOR (width-1 downto 0));
end component;
component mux2 generic (width: integer);
port(d0, d1: in STD_LOGIC_VECTOR (width-1 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (width-1 downto 0));
end component;
signal writereg: STD_LOGIC_VECTOR (4 downto 0);
signal pcjump, pcnext, pcnextbr,
pcplus4, pcbranch: STD_LOGIC_VECTOR (31 downto 0);
signal signimm, signimmsh: STD_LOGIC_VECTOR (31 downto 0);
signal srca, srcb, result: STD_LOGIC_VECTOR (31 downto 0);
begin
— — next PC logic
pcjump  pcplus4 (31 downto 28) & instr (25 downto 0) & "00";
pcreg: flopr generic map(32) port map(clk, reset, pcnext, pc);
pcadd1: adder port map(pc, X"00000004", pcplus4);
immsh: sl2 port map(signimm, signimmsh);
pcadd2: adder port map(pcplus4, signimmsh, pcbranch);
pcbrmux: mux2 generic map(32) port map(pcplus4, pcbranch,
pcsrc, pcnextbr);
pcmux: mux2 generic map(32) port map(pcnextbr, pcjump, jump,
pcnext);
— — register file logic
rf: regfile port map(clk, regwrite, instr(25 downto 21),
instr(20 downto 16), writereg, result, srca,
writedata);
wrmux: mux2 generic map(5) port map(instr(20 downto 16),
instr(15 downto 11), regdst, writereg);
resmux: mux2 generic map(32) port map(aluout, readdata,
memtoreg, result);
se: signext port map(instr(15 downto 0), signimm);
— — ALU logic
srcbmux: mux2 generic map (32) port map(writedata, signimm,
alusrc, srcb);
mainalu: alu port map(srca, srcb, alucontrol, aluout, zero);
end;
HDL Example 7.5 DATAPATH
425
Ch
Verilog
module regfile (input clk,
input we3,
input [4:0] ra1, ra2, wa3,
input [31:0] wd3,
output [31:0] rd1, rd2);
reg [31:0] rf[31:0];
// three ported register file
// read two ports combinationally
// write third port on rising edge of clock
// register 0 hardwired to 0
always @ (posedge clk)
if (we3) rf[wa3]  wd3;
assign rd1  (ra1 !  0) ? rf[ra1] : 0;
assign rd2  (ra2 !  0) ? rf[ra2] : 0;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
entity regfile is — — three-port register file
port(clk: in STD_LOGIC;
we3: in STD_LOGIC;
ra1, ra2, wa3:in STD_LOGIC_VECTOR(4 downto 0);
wd3: in STD_LOGIC_VECTOR(31 downto 0);
rd1, rd2: out STD_LOGIC_VECTOR(31 downto 0));
end;
architecture behave of regfile is
type ramtype is array (31 downto 0) of STD_LOGIC_VECTOR (31
downto 0);
signal mem: ramtype;
begin
— — three-ported register file
— — read two ports combinationally
— — write third port on rising edge of clock
process(clk) begin
if clk'event and clk  '1' then
if we3  '1' then mem(CONV_INTEGER(wa3))  wd3;
end if;
end if;
end process;
process (ra1, ra2) begin
if (conv_integer (ra1)  0) then rd1  X"00000000";
— — register 0 holds 0
else rd1  mem(CONV_INTEGER (ra1));
end if;
if (conv_integer(ra2)  0) then rd2  X"00000000";
else rd2  mem(CONV_INTEGER(ra2));
end if;
end process;
end;
HDL Example 7.6 REGISTER FILE
Verilog
module adder (input [31:0] a, b,
output [31:0] y);
assign y  a  b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
entity adder is — adder
port (a, b: in STD_LOGIC_VECTOR(31 downto 0);
y: out STD_LOGIC_VECTOR(31 downto 0));
end;
architecture behave of adder is
begin
y  a  b;
end;
HDL Example 7.7 ADDER
7.6.2 Generic Building Blocks
This section contains generic building blocks that may be useful in any
MIPS microarchitecture, including a register file, adder, left shift unit,
sign-extension unit, resettable flip-flop, and multiplexer. The HDL for
the ALU is left to Exercise 5.9.

Verilog
module sl2 (input [31:0] a,
output [31:0] y);
// shift left by 2
assign y  {a[29:01], 2'b00};
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity sl2 is — — shift left by 2
port (a: in STD_LOGIC_VECTOR (31 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of sl2 is
begin
y  a(29 downto 0) & "00";
end;
HDL Example 7.8 LEFT SHIFT (MULTIPLY BY 4)
Verilog
module signext (input [15:0] a,
output [31:0] y);
assign y  {{16{a[15]}}, a};
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity signext is — — sign extender
port(a: in STD_LOGIC_VECTOR (15 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of signext is
begin
y  X"0000" & a when a (15)  '0' else X"ffff" & a;
end;
HDL Example 7.9 SIGN EXTENSION
Verilog
module flopr # (parameter WIDTH  8)
(input clk, reset,
input [WIDTH-1:0] d,
output reg [WIDTH-1:0] q);
always @ (posedge clk, posedge reset)
if (reset) q  0;
else q  d;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all; use
IEEE.STD_LOGIC_ARITH.all;
entity flopr is — — flip-flop with synchronous reset
generic (width: integer);
port (clk, reset: in STD_LOGIC;
d: in STD_LOGIC_VECTOR(width-1 downto 0);
q: out STD_LOGIC_VECTOR(width-1 downto 0));
end;
architecture asynchronous of flopr is
begin
process (clk, reset) begin
if reset  '1' then q  CONV_STD_LOGIC_VECTOR(0, width);
elsif clk’event and clk  '1' then
q  d;
end if;
end process;
end;
HDL Example 7.10 RESETTABLE FLIP-FLOP
7.6 HDL Representation 427

428 CHAPTER SEVEN Microarchitecture
Verilog
module mux2 # (parameter WIDTH  8)
(input [WIDTH-1:0] d0, d1,
input s,
output [WIDTH-1:0] y);
assign y  s ? d1 : d0;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is — — two-input multiplexer
generic (width: integer);
port (d0, d1: in STD_LOGIC_VECTOR(width-1 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR(width-1 downto 0));
end;
architecture behave of mux2 is
begin
y  d0 when s  '0' else d1;
end;
HDL Example 7.11 2:1 MULTIPLEXER
# mipstest.asm
# David_Harris@hmc.edu 9 November 2005
#
# Test the MIPS processor.
# add, sub, and, or, slt, addi, lw, sw, beq, j
# If successful, it should write the value 7 to address 84
# Assembly Description Address Machine
main: addi $2, $0, 5 # initialize $2  5 0 20020005
addi $3, $0, 12 # initialize $3  12 4 2003000c
addi $7, $3, 9 # initialize $7  3 8 2067fff7
or $4, $7, $2 # $4  3 or 5  7 c 00e22025
and $5, $3, $4 # $5  12 and 7  4 10 00642824
add $5, $5, $4 # $5  4  7  11 14 00a42820
beq $5, $7, end # shouldn’t be taken 18 10a7000a
slt $4, $3, $4 # $4  12  7  0 1c 0064202a
beq $4, $0, around # should be taken 20 10800001
addi $5, $0, 0 # shouldn’t happen 24 20050000
around: slt $4, $7, $2 # $4  3  5  1 28 00e2202a
add $7, $4, $5 # $7  1  11  12 2c 00853820
sub $7, $7, $2 # $7  12  5  7 30 00e23822
sw $7, 68($3) # [80]  7 34 ac670044
lw $2, 80($0) # $2  [80]  7 38 8c020050
j end # should be taken 3c 08000011
addi $2, $0, 1 # shouldn’t happen 40 20020001
end: sw $2, 84($0) # write adr 84  7 44 ac020054
Figure 7.60 Assembly and machine code for MIPS test program
20020005
2003000c
2067fff7
00e22025
00642824
00a42820
10a7000a
0064202a
10800001
20050000
00e2202a
00853820
00e23822
ac670044
8c020050
08000011
20020001
ac020054
Figure 7.61 Contents of
memfile.dat
7.6.3 Testbench
The MIPS testbench loads a program into the memories. The program in
Figure 7.60 exercises all of the instructions by performing a computation
that should produce the correct answer only if all of the instructions are
functioning properly. Specifically, the program will write the value 7 to
address 84 if it runs correctly, and is unlikely to do so if the hardware is
buggy. This is an example of ad hoc testing.

7.6 HDL Representation 429
The machine code is stored in a hexadecimal file called memfile.dat
(see Figure 7.61), which is loaded by the testbench during simulation.
The file consists of the machine code for the instructions, one instruction
per line.
The testbench, top-level MIPS module, and external memory HDL
code are given in the following examples. The memories in this example
hold 64 words each.
Verilog
module testbench();
reg clk;
reg reset;
wire [31:0] writedata, dataadr;
wire memwrite;
// instantiate device to be tested
top dut (clk, reset, writedata, dataadr, memwrite);
// initialize test
initial
begin
reset  1; # 22; reset  0;
end
// generate clock to sequence tests
always
begin
clk  1; # 5; clk  0; # 5;
end
// check results
always @ (negedge clk)
begin
if (memwrite) begin
if (dataadr  84 & writedata  7) begin
$display ("Simulation succeeded");
$stop;
end else if (dataadr ! 80) begin
$display ("Simulation failed");
$stop;
end
end
end
endmodule
VHDL
library IEEE;
use IEEE.STD_LOGIC_1164.all; use IEEE.STD_LOGIC_UNSIGNED.all;
entity testbench is
end;
architecture test of testbench is
component top
port(clk, reset: in STD_LOGIC;
writedata, dataadr: out STD_LOGIC_VECTOR(31 downto 0);
memwrite: out STD_LOGIC);
end component;
signal writedata, dataadr: STD_LOGIC_VECTOR(31 downto 0);
signal clk, reset, memwrite: STD_LOGIC;
begin
— — instantiate device to be tested
dut: top port map (clk, reset, writedata, dataadr, memwrite);
— — Generate clock with 10 ns period
process begin
clk  '1';
wait for 5 ns;
clk  '0';
wait for 5 ns;
end process;
— — Generate reset for first two clock cycles
process begin
reset  '1';
wait for 22 ns;
reset  '0';
wait;
end process;
— — check that 7 gets written to address 84
— — at end of program
process (clk) begin
if (clk'event and clk  '0' and memwrite  '1') then
if (conv_integer(dataadr)  84 and conv_integer
(writedata)  7) then
report "Simulation succeeded";
elsif (dataadr / 80) then
report "Simulation failed";
end if;
end if;
end process;
end;
HDL Example 7.12 MIPS TESTBENCH

Verilog
module top (input clk, reset,
output [31:0] writedata, dataadr,
output memwrite);
wire [31:0] pc, instr, readdata;
// instantiate processor and memories
mips mips (clk, reset, pc, instr, memwrite, dataadr,
writedata, readdata);
imem imem (pc[7:2], instr);
dmem dmem (clk, memwrite, dataadr, writedata,
readdata);
endmodule
VHDL
library IEEE;
use IEEE.STD_LOGIC_1164.all; use IEEE.STD_LOGIC_UNSIGNED.all;
entity top is — — top-level design for testing
port (clk, reset: in STD_LOGIC;
writedata, dataadr: buffer STD_LOGIC_VECTOR (31 downto
0);
memwrite: buffer STD_LOGIC);
end;
architecture test of top is
component mips
port (clk, reset: in STD_LOGIC;
pc: out STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR (31 downto 0);
memwrite: out STD_LOGIC;
aluout, writedata: out STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR (31 downto 0));
end component;
component imem
port (a: in STD_LOGIC_VECTOR (5 downto 0)
rd: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component dmem
port (clk, we: in STD_LOGIC;
a, wd: in STD_LOGIC_VECTOR (31 downto 0);
rd: out STD_LOGIC_VECTOR (31 downto 0));
end component;
signal pc, instr,
readdata: STD_LOGIC_VECTOR (31 downto 0);
begin
— — instantiate processor and memories
mips1: mips port map (clk, reset, pc, instr, memwrite,
dataadr, writedata, readdata);
imem1: imem port map (pc (7 downto 2), instr);
dmem1: dmem port map (clk, memwrite, dataadr, writedata,
readdata);
end;
HDL Example 7.13 MIPS TOP-LEVEL MODULE
module dmem (input clk, we,
input [31:0] a, wd,
output [31:0] rd);
reg [31:0] RAM[63:0];
assign rd  RAM[a[31:2]]; // word aligned
always @ (posedge clk)
if (we)
RAM[a[31:2]]  wd;
endmodule
library IEEE;
use IEEE.STD_LOGIC_1164.all; use STD.TEXTIO.all;
use IEEE.STD_LOGIC_UNSIGNED.all; use IEEE.STD_LOGIC_ARITH.all;
entity dmem is — — data memory
port (clk, we: in STD_LOGIC;
a, wd: in STD_LOGIC_VECTOR (31 downto 0);
rd: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of dmem is
begin
process is
type ramtype is array (63 downto 0) of STD_LOGIC_VECTOR
(31 downto 0);
variable mem: ramtype;
begin
— — read or write memory
loop
if clk'event and clk  '1' then
if (we  '1') then mem (CONV_INTEGER (a(7 downto
2))):  wd;
end if;
end if;
rd  mem (CONV_INTEGER (a (7 downto 2)));
wait on clk, a;
end loop;
end process;
end;
HDL Example 7.14 MIPS DATA MEMORY

7.7 EXCEPTIONS*
Section 6.7.2 introduced exceptions, which cause unplanned changes in
the flow of a program. In this section, we enhance the multicycle
processor to support two types of exceptions: undefined instructions
Verilog
module imem (input [5:0] a,
output [31:0] rd);
reg [31:0] RAM[63:0];
initial
begin
$readmemh ("memfile.dat",RAM);
end
assign rd  RAM[a]; // word aligned
endmodule
VHDL
library IEEE;
use IEEE.STD_LOGIC_1164.all; use STD.TEXTIO.all;
use IEEE.STD_LOGIC_UNSIGNED.all; use IEEE.STD_LOGIC_ARITH.all;
entity imem is — — instruction memory
port (a: in STD_LOGIC_VECTOR (5 downto 0);
rd: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of imem is
begin
process is
file mem_file: TEXT;
variable L: line;
variable ch: character;
variable index, result: integer;
type ramtype is array (63 downto 0) of STD_LOGIC_VECTOR
(31 downto 0);
variable mem: ramtype;
begin
— — initialize memory from file
for i in 0 to 63 loop — set all contents low
mem (conv_integer(i)) : CONV_STD_LOGIC_VECTOR (0, 32);
end loop;
index : 0;
FILE_OPEN (mem_file, "C:/mips/memfile.dat", READ_MODE);
while not endfile(mem_file) loop
readline (mem_file, L);
result : 0;
for i in 1 to 8 loop
read (L, ch);
if '0'  ch and ch  '9' then
result : result*16  character'pos(ch) 
character'pos('0');
elsif 'a'  ch and ch  'f' then
result : result*16  character'pos(ch) 
character'pos('a')  10;
else report "Format error on line" & integer'image
(index) severity error;
end if;
end loop;
mem (index) : CONV_STD_LOGIC_VECTOR (result, 32);
index : index  1;
end loop;
— — read memory
loop
rd  mem(CONV_INTEGER(a));
wait on a;
end loop;
end process;
end;
HDL Example 7.15 MIPS INSTRUCTION MEMORY

and arithmetic overflow. Supporting exceptions in other microarchitectures follows similar principles.
As described in Section 6.7.2, when an exception takes place, the
processor copies the PC to the EPC register and stores a code in the
Cause register indicating the source of the exception. Exception causes
include 0x28 for undefined instructions and 0x30 for overflow (see Table
6.7). The processor then jumps to the exception handler at memory
address 0x80000180. The exception handler is code that responds to the
exception. It is part of the operating system.
Also as discussed in Section 6.7.2, the exception registers are part of
Coprocessor 0, a portion of the MIPS processor that is used for system
functions. Coprocessor 0 defines up to 32 special-purpose registers,
including Cause and EPC. The exception handler may use the mfc0
(move from coprocessor 0) instruction to copy these special-purpose
registers into a general-purpose register in the register file; the Cause register is Coprocessor 0 register 13, and EPC is register 14.
To handle exceptions, we must add EPC and Cause registers to the
datapath and extend the PCSrc multiplexer to accept the exception handler address, as shown in Figure 7.62. The two new registers have write
enables, EPCWrite and CauseWrite, to store the PC and exception cause
when an exception takes place. The cause is generated by a multiplexer
432 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc1:0
CLK
ALUControl2:0
ALU
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB1:0 IorD IRWrite PCWrite
PCEn
<<2
25:0 (jump)
31:28
27:0
PCJump
00
01
10
11
0x80000180
Overflow
CLK
EN
EPCWrite
CLK
EN
CauseWrite
0
1
IntCause
0x30
0x28 EPC
Cause
Figure 7.62 Datapath supporting overflow and undefined instruction exceptions

7.7 Exceptions 433
that selects the appropriate code for the exception. The ALU must also
generate an overflow signal, as was discussed in Section 5.2.4.5
To support the mfc0 instruction, we also add a way to select the
Coprocessor 0 registers and write them to the register file, as shown in
Figure 7.63. The mfc0 instruction specifies the Coprocessor 0 register by
Instr15:11; in this diagram, only the Cause and EPC registers are supported. We add another input to the MemtoReg multiplexer to select the
value from Coprocessor 0.
The modified controller is shown in Figure 7.64. The controller
receives the overflow flag from the ALU. It generates three new control
signals: one to write the EPC, a second to write the Cause register, and a
third to select the Cause. It also includes two new states to support the
two exceptions and another state to handle mfc0.
If the controller receives an undefined instruction (one that it does
not know how to handle), it proceeds to S12, saves the PC in EPC,
writes 0x28 to the Cause register, and jumps to the exception handler.
Similarly, if the controller detects arithmetic overflow on an add or
sub instruction, it proceeds to S13, saves the PC in EPC, writes 0x30
5 Strictly speaking, the ALU should assert overflow only for add and sub, not for other
ALU instructions.
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc1:0
CLK
ALUControl2:0
ALU
WD
WE
CLK
Adr
00
01 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB1:0 IorD IRWrite PCWrite
PCEn
<<2
25:0 (jump)
31:28
27:0
PCJump
00
01
10
11
0x8000 0180
CLK
EN
EPCWrite
CLK
EN
CauseWrite
0
1
IntCause
0x30
0x28 EPC
Cause
Overflow
...
01101
01110
... 15:11
10
C0
Figure 7.63 Datapath supporting mfc0

434 CHAPTER SEVEN Microarchitecture
IorD = 0
AluSrcA= 0
ALUSrcB= 01
ALUOp = 00
PCSrc = 00
IRWrite
ALUSrcA= 0
ALUSrcB= 11
ALUOp = 00
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
IorD = 1
RegDst = 1
MemtoReg = 00
RegWrite
IorD = 1
MemWrite
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 10
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 01
PCSrc = 01
Branch
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
RegDst = 0
MemtoReg = 01
RegWrite
S4: Mem
Writeback
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
RegDst = 0
MemtoReg = 00
RegWrite
Op = ADDI
S9: ADDI
Execute
S10: ADDI
Writeback
PCSrc = 10
PCWrite
Op = J
S11: Jump
Overflow Overflow
S13:
Overflow
PCSrc = 11
PCWrite
IntCause = 0
CauseWrite
EPCWrite
Op = others
 PCSrc = 11
PCWrite
IntCause = 1
CauseWrite
EPCWrite
S12: Undefined
Memtoreg = 10
RegWrite
Op=MFC0
S14: MFC0
PCWrite
RegDst = 0
Figure 7.64 Controller supporting exceptions and mfc0
in the Cause register, and jumps to the exception handler. Note that,
when an exception occurs, the instruction is discarded and the register
file is not written. When a mfc0 instruction is decoded, the processor
goes to S14 and writes the appropriate Coprocessor 0 register to the
main register file.

7.8 ADVANCED MICROARCHITECTURE*
High-performance microprocessors use a wide variety of techniques to
run programs faster. Recall that the time required to run a program is
proportional to the period of the clock and to the number of clock cycles
per instruction (CPI). Thus, to increase performance we would like to
speed up the clock and/or reduce the CPI. This section surveys some
existing speedup techniques. The implementation details become quite
complex, so we will focus on the concepts. Hennessy & Patterson’s
Computer Architecture text is a definitive reference if you want to fully
understand the details.
Every 2 to 3 years, advances in CMOS manufacturing reduce transistor dimensions by 30% in each direction, doubling the number of
transistors that can fit on a chip. A manufacturing process is characterized by its feature size, which indicates the smallest transistor that can
be reliably built. Smaller transistors are faster and generally consume
less power. Thus, even if the microarchitecture does not change, the
clock frequency can increase because all the gates are faster. Moreover,
smaller transistors enable placing more transistors on a chip.
Microarchitects use the additional transistors to build more complicated processors or to put more processors on a chip. Unfortunately,
power consumption increases with the number of transistors and the
speed at which they operate (see Section 1.8). Power consumption is
now an essential concern. Microprocessor designers have a challenging
task juggling the trade-offs among speed, power, and cost for chips
with billions of transistors in some of the most complex systems that
humans have ever built.
7.8.1 Deep Pipelines
Aside from advances in manufacturing, the easiest way to speed up the
clock is to chop the pipeline into more stages. Each stage contains less
logic, so it can run faster. This chapter has considered a classic five-stage
pipeline, but 10 to 20 stages are now commonly used.
The maximum number of pipeline stages is limited by pipeline hazards, sequencing overhead, and cost. Longer pipelines introduce more
dependencies. Some of the dependencies can be solved by forwarding,
but others require stalls, which increase the CPI. The pipeline registers
between each stage have sequencing overhead from their setup time and
clk-to-Q delay (as well as clock skew). This sequencing overhead makes
adding more pipeline stages give diminishing returns. Finally, adding
more stages increases the cost because of the extra pipeline registers and
hardware required to handle hazards.
7.8 Advanced Microarchitecture 435

Example 7.11 DEEP PIPELINES
Consider building a pipelined processor by chopping up the single-cycle processor into N stages (N 	 5). The single-cycle processor has a propagation delay of
900 ps through the combinational logic. The sequencing overhead of a register is
50 ps. Assume that the combinational delay can be arbitrarily divided into any
number of stages and that pipeline hazard logic does not increase the delay. The
five-stage pipeline in Example 7.9 has a CPI of 1.15. Assume that each additional stage increases the CPI by 0.1 because of branch mispredictions and other
pipeline hazards. How many pipeline stages should be used to make the processor execute programs as fast as possible?
Solution: If the 900-ps combinational logic delay is divided into N stages and
each stage also pays 50 ps of sequencing overhead for its pipeline register, the
cycle time is Tc  900/N  50. The CPI is 1.15  0.1(N  5). The time per
instruction, or instruction time, is the product of the cycle time and the CPI.
Figure 7.65 plots the cycle time and instruction time versus the number of stages.
The instruction time has a minimum of 231 ps at N  11 stages. This minimum
is only slightly better than the 250 ps per instruction achieved with a six-stage
pipeline.
In the late 1990s and early 2000s, microprocessors were marketed largely based
on clock frequency (1/Tc). This pushed microprocessors to use very deep
pipelines (20 to 31 stages on the Pentium 4) to maximize the clock frequency,
even if the benefits for overall performance were questionable. Power is proportional to clock frequency and also increases with the number of pipeline registers, so now that power consumption is so important, pipeline depths are
decreasing.
436 CHAPTER SEVEN Microarchitecture
0
50
100
150
200
250
300
5 6 7 8 9 10 11 12 13 14 15
Tc Instruction Time
Number of pipeline stages
Time (ps)
Figure 7.65 Cycle time and
instruction time versus the
number of pipeline stages

7.8.2 Branch Prediction
An ideal pipelined processor would have a CPI of 1. The branch misprediction penalty is a major reason for increased CPI. As pipelines get
deeper, branches are resolved later in the pipeline. Thus, the branch misprediction penalty gets larger, because all the instructions issued after the
mispredicted branch must be flushed. To address this problem, most
pipelined processors use a branch predictor to guess whether the branch
should be taken. Recall that our pipeline from Section 7.5.3 simply predicted that branches are never taken.
Some branches occur when a program reaches the end of a loop
(e.g., a for or while statement) and branches back to repeat the loop.
Loops tend to be executed many times, so these backward branches are
usually taken. The simplest form of branch prediction checks the direction of the branch and predicts that backward branches should be taken.
This is called static branch prediction, because it does not depend on the
history of the program.
Forward branches are difficult to predict without knowing more
about the specific program. Therefore, most processors use dynamic
branch predictors, which use the history of program execution to guess
whether a branch should be taken. Dynamic branch predictors maintain
a table of the last several hundred (or thousand) branch instructions that
the processor has executed. The table, sometimes called a branch target
buffer, includes the destination of the branch and a history of whether
the branch was taken.
To see the operation of dynamic branch predictors, consider the following loop code from Code Example 6.20. The loop repeats 10 times,
and the beq out of the loop is taken only on the last time.
add $s1, $0, $0 # sum  0
add $s0, $0, $0 # i  0
addi $t0, $0, 10 # $t0  10
for:
beq $s0, $t0, done # if i  10, branch to done
add $s1, $s1, $s0 # sum  sum  i
addi $s0, $s0, 1 # increment i
j for
done:
A one-bit dynamic branch predictor remembers whether the branch
was taken the last time and predicts that it will do the same thing the
next time. While the loop is repeating, it remembers that the beq was
not taken last time and predicts that it should not be taken next time.
This is a correct prediction until the last branch of the loop, when the
branch does get taken. Unfortunately, if the loop is run again, the
branch predictor remembers that the last branch was taken. Therefore,
7.8 Advanced Microarchitecture 437

it incorrectly predicts that the branch should be taken when the loop is
first run again. In summary, a 1-bit branch predictor mispredicts the
first and last branches of a loop.
A 2-bit dynamic branch predictor solves this problem by having four
states: strongly taken, weakly taken, weakly not taken, and strongly not
taken, as shown in Figure 7.66. When the loop is repeating, it enters the
“strongly not taken” state and predicts that the branch should not be
taken next time. This is correct until the last branch of the loop, which is
taken and moves the predictor to the “weakly not taken” state. When
the loop is first run again, the branch predictor correctly predicts that
the branch should not be taken and reenters the “strongly not taken”
state. In summary, a 2-bit branch predictor mispredicts only the last
branch of a loop.
As one can imagine, branch predictors may be used to track even
more history of the program to increase the accuracy of predictions.
Good branch predictors achieve better than 90% accuracy on typical
programs.
The branch predictor operates in the Fetch stage of the pipeline so
that it can determine which instruction to execute on the next cycle.
When it predicts that the branch should be taken, the processor fetches
the next instruction from the branch destination stored in the branch
target buffer. By keeping track of both branch and jump destinations in
the branch target buffer, the processor can also avoid flushing the
pipeline during jump instructions.
7.8.3 Superscalar Processor
A superscalar processor contains multiple copies of the datapath hardware to execute multiple instructions simultaneously. Figure 7.67 shows
a block diagram of a two-way superscalar processor that fetches and
executes two instructions per cycle. The datapath fetches two instructions at a time from the instruction memory. It has a six-ported register
file to read four source operands and write two results back in each
cycle. It also contains two ALUs and a two-ported data memory to
execute the two instructions at the same time.
438 CHAPTER SEVEN Microarchitecture
A scalar processor acts on one
piece of data at a time. A vector processor acts on several
pieces of data with a single
instruction. A superscalar
processor issues several
instructions at a time, each of
which operates on one piece
of data.
Our MIPS pipelined
processor is a scalar processor. Vector processors were
popular for supercomputers
in the 1980s and 1990s
because they efficiently handled the long vectors of data
common in scientific computations. Modern highperformance microprocessors
are superscalar, because
issuing several independent
instructions is more flexible
than processing vectors.
However, modern processors also include hardware to
handle short vectors of data
that are common in multimedia and graphics applications. These are called single
instruction multiple data
(SIMD) units.
strongly
taken
predict
taken
weakly
taken
predict
taken
weakly
not taken
predict
not taken
strongly
not taken
predict
not taken
taken taken taken
taken taken taken
taken
taken
Figure 7.66 2-bit branch predictor state transition diagram

Figure 7.68 shows a pipeline diagram illustrating the two-way
superscalar processor executing two instructions on each cycle. For this
program, the processor has a CPI of 0.5. Designers commonly refer to
the reciprocal of the CPI as the instructions per cycle, or IPC. This
processor has an IPC of 2 on this program.
Executing many instructions simultaneously is difficult because of
dependencies. For example, Figure 7.69 shows a pipeline diagram running a program with data dependencies. The dependencies in the code
are shown in blue. The add instruction is dependent on $t0, which is
produced by the lw instruction, so it cannot be issued at the same time
as lw. Indeed, the add instruction stalls for yet another cycle so that lw
can forward $t0 to add in cycle 5. The other dependencies (between
7.8 Advanced Microarchitecture 439
CLK CLK CLK CLK
A RD A1
A2
A3 RD1
WD3
WD6
A4
A5
A6
RD4
RD2
RD5
Instruction
Memory
Register
File Data
Memory
PC
CLK
A1
A2
WD1
ALUs
WD2
RD1
RD2
Figure 7.67 Superscalar datapath
Time (cycles)
12345678
RF
40
$s0
RF
$t0 +
DM IM
lw
add
lw $t0, 40($s0)
add $t1, $s1, $s2
sub $t2, $s1, $s3
and $t3, $s3, $s4
or $t4, $s1, $s5
sw $s5, 80($s0)
$t1 $s2
$s1
+
RF
$s3
$s1
RF
$t2 -
DM IM
sub
and $t3 $s4
$s3
&
RF
+
$s5
$s1
RF
$t4
|
DM IM
or
sw 80
$s0 $s5
Figure 7.68 Abstract view of a superscalar pipeline in operation

440 CHAPTER SEVEN Microarchitecture
Stall
Time (cycles)
12345678
RF
40
$s0
RF
$t0 + DM IM
lw lw $t0, 40($s0)
add $t1, $t0, $s1
sub $t0, $s2, $s3
and $t2, $s4, $t0
sw $s7, 80($t3)
$s1
$t0 add
RF
$s1
$t0
RF
$t1 +
DM
RF
$t0
$s4
RF
$t2 &
DM IM
and
IM or
and
sub
$s6 |
$s5 $t3
RF
80
$t3
RF
+
DM
sw
IM
$s7
9
$s3
$s2
$s3
$s2
- $t0
or $t3, $s5, $s6
IM
or
RF
sub and and based on $t0, and between or and sw based on $t3) are
handled by forwarding results produced in one cycle to be consumed in
the next. This program, also given below, requires five cycles to issue
six instructions, for an IPC of 1.17.
lw $t0, 40($s0)
add $t1, $t0, $s1
sub $t0, $s2, $s3
and $t2, $s4, $t0
or $t3, $s5, $s6
sw $s7, 80($t3)
Recall that parallelism comes in temporal and spatial forms.
Pipelining is a case of temporal parallelism. Multiple execution units is a
case of spatial parallelism. Superscalar processors exploit both forms of
parallelism to squeeze out performance far exceeding that of our singlecycle and multicycle processors.
Commercial processors may be three-, four-, or even six-way superscalar. They must handle control hazards such as branches as well as
data hazards. Unfortunately, real programs have many dependencies, so
wide superscalar processors rarely fully utilize all of the execution units.
Moreover, the large number of execution units and complex forwarding
networks consume vast amounts of circuitry and power.
Figure 7.69 Program with data dependencies

7.8.4 Out-of-Order Processor
To cope with the problem of dependencies, an out-of-order processor looks
ahead across many instructions to issue, or begin executing, independent
instructions as rapidly as possible. The instructions can be issued in a different order than that written by the programmer, as long as dependencies
are honored so that the program produces the intended result.
Consider running the same program from Figure 7.69 on a two-way
superscalar out-of-order processor. The processor can issue up to two
instructions per cycle from anywhere in the program, as long as dependencies are observed. Figure 7.70 shows the data dependencies and the
operation of the processor. The classifications of dependencies as RAW
and WAR will be discussed shortly. The constraints on issuing instructions are described below.
 Cycle 1
– The lw instruction issues.
– The add, sub, and and instructions are dependent on lw by way
of $t0, so they cannot issue yet. However, the or instruction is
independent, so it also issues.
7.8 Advanced Microarchitecture 441
Time (cycles)
12345678
RF
40
$s0
RF
$t0 +
DM IM
lw lw $t0, 40($s0)
add $t1, $t0, $s1
sub $t0, $s2, $s3
and $t2, $s4, $t0
sw $s7, 80($t3)
or $s6 |
$s5 $t3
RF
80
$t3
RF
+
DM
sw $s7
or $t3, $s5, $s6
IM
RF
$s1
$t0
RF
$t1 +
DM IM
add
sub $s3 -
$s2 $t0
Two-cycle latency
between lW
and use of $t0
RAW
WAR
RAW
RF
$t0
$s4
RF
&
DM
and
IM
$t2
RAW
Figure 7.70 Out-of-order execution of a program with dependencies
C
 Cycle 2
– Remember that there is a two-cycle latency between when a lw
instruction issues and when a dependent instruction can use its
result, so add cannot issue yet because of the $t0 dependence.
sub writes $t0, so it cannot issue before add, lest add receive the
wrong value of $t0. and is dependent on sub.
– Only the sw instruction issues.
 Cycle 3
– On cycle 3, $t0 is available, so add issues. sub issues simultaneously, because it will not write $t0 until after add consumes
$t0.
 Cycle 4
– The and instruction issues. $t0 is forwarded from sub to and.
The out-of-order processor issues the six instructions in four cycles, for
an IPC of 1.5.
The dependence of add on lw by way of $t0 is a read after write
(RAW) hazard. add must not read $t0 until after lw has written it. This
is the type of dependency we are accustomed to handling in the pipelined
processor. It inherently limits the speed at which the program can run,
even if infinitely many execution units are available. Similarly, the
dependence of sw on or by way of $t3 and of and on sub by way of
$t0 are RAW dependencies.
The dependence between sub and add by way of $t0 is called a write
after read (WAR) hazard or an antidependence. sub must not write $t0
before add reads $t0, so that add receives the correct value according to
the original order of the program. WAR hazards could not occur in the
simple MIPS pipeline, but they may happen in an out-of-order processor
if the dependent instruction (in this case, sub) is moved too early.
A WAR hazard is not essential to the operation of the program. It is
merely an artifact of the programmer’s choice to use the same register
for two unrelated instructions. If the sub instruction had written $t4
instead of $t0, the dependency would disappear and sub could be issued
before add. The MIPS architecture only has 32 registers, so sometimes
the programmer is forced to reuse a register and introduce a hazard just
because all the other registers are in use.
A third type of hazard, not shown in the program, is called write
after write (WAW) or an output dependence. A WAW hazard occurs if
an instruction attempts to write a register after a subsequent instruction
has already written it. The hazard would result in the wrong value being
442 CHAPTER SEVEN Microarchitecture
Ch
written to the register. For example, in the following program, add and
sub both write $t0. The final value in $t0 should come from sub
according to the order of the program. If an out-of-order processor
attempted to execute sub first, the WAW hazard would occur.
add $t0, $s1, $s2
sub $t0, $s3, $s4
WAW hazards are not essential either; again, they are artifacts
caused by the programmer’s using the same register for two unrelated
instructions. If the sub instruction were issued first, the program could
eliminate the WAW hazard by discarding the result of the add instead of
writing it to $t0. This is called squashing the add.
6
Out-of-order processors use a table to keep track of instructions
waiting to issue. The table, sometimes called a scoreboard, contains
information about the dependencies. The size of the table determines
how many instructions can be considered for issue. On each cycle, the
processor examines the table and issues as many instructions as it can,
limited by the dependencies and by the number of execution units (e.g.,
ALUs, memory ports) that are available.
The instruction level parallelism (ILP) is the number of instructions
that can be executed simultaneously for a particular program and
microarchitecture. Theoretical studies have shown that the ILP can be
quite large for out-of-order microarchitectures with perfect branch predictors and enormous numbers of execution units. However, practical
processors seldom achieve an ILP greater than 2 or 3, even with six-way
superscalar datapaths with out-of-order execution.
7.8.5 Register Renaming
Out-of-order processors use a technique called register renaming to eliminate WAR hazards. Register renaming adds some nonarchitectural
renaming registers to the processor. For example, a MIPS processor
might add 20 renaming registers, called $r0–$r19. The programmer
cannot use these registers directly, because they are not part of the architecture. However, the processor is free to use them to eliminate hazards.
For example, in the previous section, a WAR hazard occurred
between the sub and add instructions based on reusing $t0. The out-oforder processor could rename $t0 to $r0 for the sub instruction. Then
7.8 Advanced Microarchitecture 443
6 You might wonder why the add needs to be issued at all. The reason is that out-of-order
processors must guarantee that all of the same exceptions occur that would have occurred
if the program had been executed in its original order. The add potentially may produce an
overflow exception, so it must be issued to check for the exception, even though the result
can be discarded.

sub could be executed sooner, because $r0 has no dependency on the
add instruction. The processor keeps a table of which registers were
renamed so that it can consistently rename registers in subsequent
dependent instructions. In this example, $t0 must also be renamed to
$r0 in the and instruction, because it refers to the result of sub.
Figure 7.71 shows the same program from Figure 7.70 executing on
an out-of-order processor with register renaming. $t0 is renamed to $r0
in sub and and to eliminate the WAR hazard. The constraints on issuing
instructions are described below.
 Cycle 1
– The lw instruction issues.
– The add instruction is dependent on lw by way of $t0, so it cannot issue yet. However, the sub instruction is independent now
that its destination has been renamed to $r0, so sub also issues.
 Cycle 2
– Remember that there is a two-cycle latency between when a lw
issues and when a dependent instruction can use its result, so
add cannot issue yet because of the $t0 dependence.
– The and instruction is dependent on sub, so it can issue. $r0 is
forwarded from sub to and.
– The or instruction is independent, so it also issues.
444 CHAPTER SEVEN Microarchitecture
Time (cycles)
1234567
RF
40
$s0
RF
$t0 +
DM IM
lw lw $t0, 40($s0)
add $t1, $t0, $s1
sub $r0, $s2, $s3
and $t2, $s4, $r0
sw $s7, 80($t3)
sub $s3 -
$s2 $r0
RF
$r0
$s4
RF
&
DM
and
$s7
or $t3, $s5, $s6
IM
RF
$s1
$t0
RF
$t1 +
DM IM
add
sw 80 +
$t3
RAW
$s6
$s5
| or
RAW
RAW
$t2
$t3
Figure 7.71 Out-of-order execution of a program using register renaming
Ch
 Cycle 3
– On cycle 3, $t0 is available, so add issues. $t3 is also available,
so sw issues.
The out-of-order processor with register renaming issues the six
instructions in three cycles, for an IPC of 2.
7.8.6 Single Instruction Multiple Data
The term SIMD (pronounced “sim-dee”) stands for single instruction
multiple data, in which a single instruction acts on multiple pieces of
data in parallel. A common application of SIMD is to perform many
short arithmetic operations at once, especially for graphics processing.
This is also called packed arithmetic.
For example, a 32-bit microprocessor might pack four 8-bit data
elements into one 32-bit word. Packed add and subtract instructions
operate on all four data elements within the word in parallel.
Figure 7.72 shows a packed 8-bit addition summing four pairs of 8-bit
numbers to produce four results. The word could also be divided
into two 16-bit elements. Performing packed arithmetic requires
modifying the ALU to eliminate carries between the smaller data
elements. For example, a carry out of a0  b0 should not affect the
result of a1  b1.
Short data elements often appear in graphics processing. For example, a pixel in a digital photo may use 8 bits to store each of the red,
green, and blue color components. Using an entire 32-bit word to
process one of these components wastes the upper 24 bits. When the
components from four adjacent pixels are packed into a 32-bit word, the
processing can be performed four times faster.
SIMD instructions are even more helpful for 64-bit architectures,
which can pack eight 8-bit elements, four 16-bit elements, or two 32-bit
elements into a single 64-bit word. SIMD instructions are also used for
floating-point computations; for example, four 32-bit single-precision
floating-point values can be packed into a single 128-bit word.
7.8 Advanced Microarchitecture 445
padd8 $s2, $s0, $s1
a0
32 24 23 16 15 78 0
$s0
Bit position
a3 a2 a1
b3 b2 b1 b0 $s1
a3 + b3 a2 + b2 a1 + b1 a0 + b0 $s2
+
Figure 7.72 Packed arithmetic:
four simultaneous 8-bit
additions

7.8.7 Multithreading
Because the ILP of real programs tends to be fairly low, adding more
execution units to a superscalar or out-of-order processor gives diminishing returns. Another problem, discussed in Chapter 8, is that memory
is much slower than the processor. Most loads and stores access a
smaller and faster memory, called a cache. However, when the instructions or data are not available in the cache, the processor may stall for
100 or more cycles while retrieving the information from the main memory. Multithreading is a technique that helps keep a processor with many
execution units busy even if the ILP of a program is low or the program
is stalled waiting for memory.
To explain multithreading, we need to define a few new terms. A program running on a computer is called a process. Computers can run multiple processes simultaneously; for example, you can play music on a PC
while surfing the web and running a virus checker. Each process consists
of one or more threads that also run simultaneously. For example, a word
processor may have one thread handling the user typing, a second thread
spell-checking the document while the user works, and a third thread
printing the document. In this way, the user does not have to wait, for
example, for a document to finish printing before being able to type again.
In a conventional processor, the threads only give the illusion of running simultaneously. The threads actually take turns being executed on the
processor under control of the OS. When one thread’s turn ends, the OS
saves its architectural state, loads the architectural state of the next thread,
and starts executing that next thread. This procedure is called context
switching. As long as the processor switches through all the threads fast
enough, the user perceives all of the threads as running at the same time.
A multithreaded processor contains more than one copy of its architectural state, so that more than one thread can be active at a time. For
example, if we extended a MIPS processor to have four program counters and 128 registers, four threads could be available at one time. If one
thread stalls while waiting for data from main memory, the processor
could context switch to another thread without any delay, because the
program counter and registers are already available. Moreover, if one
thread lacks sufficient parallelism to keep all the execution units busy,
another thread could issue instructions to the idle units.
Multithreading does not improve the performance of an individual
thread, because it does not increase the ILP. However, it does improve
the overall throughput of the processor, because multiple threads can use
processor resources that would have been idle when executing a single
thread. Multithreading is also relatively inexpensive to implement,
because it replicates only the PC and register file, not the execution units
and memories.
446 CHAPTER SEVEN Microarchitecture

7.8.8 Multiprocessors
A multiprocessor system consists of multiple processors and a method
for communication between the processors. A common form of multiprocessing in computer systems is symmetric multiprocessing (SMP), in
which two or more identical processors share a single main memory.
The multiple processors may be separate chips or multiple cores on
the same chip. Modern processors have enormous numbers of transistors
available. Using them to increase the pipeline depth or to add more execution units to a superscalar processor gives little performance benefit
and is wasteful of power. Around the year 2005, computer architects
made a major shift to build multiple copies of the processor on the same
chip; these copies are called cores.
Multiprocessors can be used to run more threads simultaneously or
to run a particular thread faster. Running more threads simultaneously
is easy; the threads are simply divided up among the processors.
Unfortunately typical PC users need to run only a small number of
threads at any given time. Running a particular thread faster is much
more challenging. The programmer must divide the thread into pieces
to perform on each processor. This becomes tricky when the processors
need to communicate with each other. One of the major challenges for
computer designers and programmers is to effectively use large numbers
of processor cores.
Other forms of multiprocessing include asymmetric multiprocessing
and clusters. Asymmetric multiprocessors use separate specialized microprocessors for separate tasks. For example, a cell phone contains a digital signal processor (DSP) with specialized instructions to decipher the
wireless data in real time and a separate conventional processor to interact with the user, manage the phone book, and play games. In clustered
multiprocessing, each processor has its own local memory system.
Clustering can also refer to a group of PCs connected together on the
network running software to jointly solve a large problem.
7.9 REAL-WORLD PERSPECTIVE: IA-32 MICROARCHITECTURE*
Section 6.8 introduced the IA-32 architecture used in almost all PCs.
This section tracks the evolution of IA-32 processors through progressively faster and more complicated microarchitectures. The same
principles we have applied to the MIPS microarchitectures are used in
IA-32.
Intel invented the first single-chip microprocessor, the 4-bit 4004,
in 1971 as a flexible controller for a line of calculators. It contained
2300 transistors manufactured on a 12-mm2 sliver of silicon in a
process with a 10-
m feature size and operated at 750 KHz. A photograph of the chip taken under a microscope is shown in Figure 7.73.
7.9 Real-World Perspective: IA-32 Microarchitecture 447
Scientists searching for signs
of extraterrestrial intelligence
use the world’s largest clustered multiprocessors to analyze radio telescope data for
patterns that might be signs of
life in other solar systems.
The cluster consists of personal computers owned by
more than 3.8 million volunteers around the world.
When a computer in the
cluster is idle, it fetches
a piece of the data from a
centralized server, analyzes
the data, and sends the
results back to the server.
You can volunteer your
computer’s idle time
for the cluster by visiting
setiathome.berkeley.edu.

In places, columns of four similar-looking structures are visible, as one
would expect in a 4-bit microprocessor. Around the periphery are
bond wires, which are used to connect the chip to its package and the
circuit board.
The 4004 inspired the 8-bit 8008, then the 8080, which eventually
evolved into the 16-bit 8086 in 1978 and the 80286 in 1982. In 1985,
Intel introduced the 80386, which extended the 8086 architecture to
32 bits and defined the IA-32 architecture. Table 7.7 summarizes
major Intel IA-32 microprocessors. In the 35 years since the 4004,
transistor feature size has shrunk 160-fold, the number of transistors
448 CHAPTER SEVEN Microarchitecture
Figure 7.73 4004
microprocessor chip
Table 7.7 Evolution of Intel IA-32 microprocessors
Processor Year Feature Size (
m) Transistors Frequency (MHz) Microarchitecture
80386 1985 1.5–1.0 275k 16–25 multicycle
80486 1989 1.0–0.6 1.2M 25–100 pipelined
Pentium 1993 0.8–0.35 3.2–4.5M 60–300 superscalar
Pentium II 1997 0.35–0.25 7.5M 233–450 out of order
Pentium III 1999 0.25–0.18 9.5M–28M 450–1400 out of order
Pentium 4 2001 0.18–0.09 42–178M 1400–3730 out of order
Pentium M 2003 0.13–0.09 77–140M 900–2130 out of order
Core Duo 2005 0.065 152M 1500–2160 dual core

on a chip has increased by five orders of magnitude, and the operating
frequency has increased by almost four orders of magnitude. No other
field of engineering has made such astonishing progress in such a
short time.
The 80386 is a multicycle processor. The major components are
labeled on the chip photograph in Figure 7.74. The 32-bit datapath is
clearly visible on the left. Each of the columns processes one bit of data.
Some of the control signals are generated using a microcode PLA that
steps through the various states of the control FSM. The memory management unit in the upper right controls access to the external memory.
The 80486, shown in Figure 7.75, dramatically improved performance using pipelining. The datapath is again clearly visible, along with
the control logic and microcode PLA. The 80486 added an on-chip
floating-point unit; previous Intel processors either sent floating-point
instructions to a separate coprocessor or emulated them in software. The
80486 was too fast for external memory to keep up, so it incorporated
an 8-KB cache onto the chip to hold the most commonly used instructions and data. Chapter 8 describes caches in more detail and revisits the
cache systems on Intel IA-32 processors.
7.9 Real-World Perspective: IA-32 Microarchitecture 449
Microcode
PLA
32-bit
Datapath
Memory
Management
Unit
Controller
Figure 7.74 80386
microprocessor chip

The Pentium processor, shown in Figure 7.76, is a superscalar
processor capable of executing two instructions simultaneously. Intel
switched to the name Pentium instead of 80586 because AMD was
becoming a serious competitor selling interchangeable 80486 chips, and
part numbers cannot be trademarked. The Pentium uses separate
instruction and data caches. It also uses a branch predictor to reduce the
performance penalty for branches.
The Pentium Pro, Pentium II, and Pentium III processors all share a
common out-of-order microarchitecture, code named P6. The complex
IA-32 instructions are broken down into one or more micro-ops similar
450 CHAPTER SEVEN Microarchitecture
32-bit
Data path
Controller
8 KB
Cache
Floating
Point
Unit
Microcode
PLA
Figure 7.75 80486
microprocessor chip

7.9 Real-World Perspective: IA-32 Microarchitecture 451
to MIPS instructions. The micro-ops are then executed on a fast out-oforder execution core with an 11-stage pipeline. Figure 7.77 shows the
Pentium III. The 32-bit datapath is called the Integer Execution Unit
(IEU). The floating-point datapath is called the Floating Point Unit
8 KB
Data
Cache Floating
Point
Unit
8 KB
Instruction
Cache
Multiprocessor Logic
Instruction
Fetch & Decode
Complex
Instruction
Support Bus Interface Unit 32-bit Datapath Superscalar Controller
Branch
Prediction
Figure 7.76 Pentium
microprocessor chip
Instruction Fetch
&
16 KB Cache
16 KB
Data Cache
Data TLB
Branch
Target
Buffer
FPU
IEU
SIMD
Register
Renaming
Microcode
PLA
256 KB
Level 2 Cache
Out of
Order
Issue
Logic
Bus Logic
Figure 7.77 Pentium III
microprocessor chip

452 CHAPTER SEVEN Microarchitecture
IEU
8 KB
Data Cache
Trace
Cache
256 KB
Level 2
Cache
FPU
&
SIMD
Bus
Logic
Figure 7.78 Pentium 4
microprocessor chip
(FPU). The processor also has a SIMD unit to perform packed operations on short integer and floating-point data. A larger portion of the
chip is dedicated to issuing instructions out-of-order than to actually
executing the instructions. The instruction and data caches have grown
to 16 KB each. The Pentium III also has a larger but slower 256-KB
second-level cache on the same chip.
By the late 1990s, processors were marketed largely on clock speed.
The Pentium 4 is another out-of-order processor with a very deep
pipeline to achieve extremely high clock frequencies. It started with 20
stages, and later versions adopted 31 stages to achieve frequencies
greater than 3 GHz. The chip, shown in Figure 7.78, packs in 42 to 178
million transistors (depending on the cache size), so even the major execution units are difficult to see on the photograph. Decoding three IA-32
instructions per cycle is impossible at such high clock frequencies
because the instruction encodings are so complex and irregular. Instead,
the processor predecodes the instructions into simpler micro-ops, then
stores the micro-ops in a memory called a trace cache. Later versions of
the Pentium 4 also perform multithreading to increase the throughput of
multiple threads.
The Pentium 4’s reliance on deep pipelines and high clock speed
led to extremely high power consumption, sometimes more than 100 W.
This is unacceptable in laptops and makes cooling of desktops expensive.

Intel discovered that the older P6 architecture could achieve comparable
performance at much lower clock speed and power. The Pentium M uses
an enhanced version of the P6 out-of-order microarchitecture with 32-KB
instruction and data caches and a 1- to 2-MB second-level cache. The
Core Duo is a multicore processor based on two Pentium M cores connected to a shared 2-MB second-level cache. The individual functional
units in Figure 7.79 are difficult to see, but the two cores and the large
cache are clearly visible.
7.10 SUMMARY
This chapter has described three ways to build MIPS processors, each
with different performance and cost trade-offs. We find this topic
almost magical: how can such a seemingly complicated device as a
microprocessor actually be simple enough to fit in a half-page
schematic? Moreover, the inner workings, so mysterious to the uninitiated, are actually reasonably straightforward.
The MIPS microarchitectures have drawn together almost every
topic covered in the text so far. Piecing together the microarchitecture
puzzle illustrates the principles introduced in previous chapters, including the design of combinational and sequential circuits, covered in
Chapters 2 and 3; the application of many of the building blocks
described in Chapter 5; and the implementation of the MIPS architecture, introduced in Chapter 6. The MIPS microarchitectures can be
described in a few pages of HDL, using the techniques from Chapter 4.
Building the microarchitectures has also heavily used our techniques for managing complexity. The microarchitectural abstraction
forms the link between the logic and architecture abstractions, forming
7.10 Summary 453
Core 1 Core 2
2 MB
Shared
Level 2
Cache
Figure 7.79 Core Duo
microprocessor chip

the crux of this book on digital design and computer architecture. We
also use the abstractions of block diagrams and HDL to succinctly
describe the arrangement of components. The microarchitectures
exploit regularity and modularity, reusing a library of common building
blocks such as ALUs, memories, multiplexers, and registers. Hierarchy
is used in numerous ways. The microarchitectures are partitioned into
the datapath and control units. Each of these units is built from logic
blocks, which can be built from gates, which in turn can be built from
transistors using the techniques developed in the first five chapters.
This chapter has compared single-cycle, multicycle, and pipelined
microarchitectures for the MIPS processor. All three microarchitectures
implement the same subset of the MIPS instruction set and have the
same architectural state. The single-cycle processor is the most straightforward and has a CPI of 1.
The multicycle processor uses a variable number of shorter steps to
execute instructions. It thus can reuse the ALU, rather than requiring
several adders. However, it does require several nonarchitectural registers to store results between steps. The multicycle design in principle
could be faster, because not all instructions must be equally long. In
practice, it is generally slower, because it is limited by the slowest steps
and by the sequencing overhead in each step.
The pipelined processor divides the single-cycle processor into five
relatively fast pipeline stages. It adds pipeline registers between the
stages to separate the five instructions that are simultaneously executing.
It nominally has a CPI of 1, but hazards force stalls or flushes that
increase the CPI slightly. Hazard resolution also costs some extra hardware and design complexity. The clock period ideally could be five times
shorter than that of the single-cycle processor. In practice, it is not that
short, because it is limited by the slowest stage and by the sequencing
overhead in each stage. Nevertheless, pipelining provides substantial performance benefits. All modern high-performance microprocessors use
pipelining today.
Although the microarchitectures in this chapter implement only a
subset of the MIPS architecture, we have seen that supporting more
instructions involves straightforward enhancements of the datapath and
controller. Supporting exceptions also requires simple modifications.
A major limitation of this chapter is that we have assumed an ideal
memory system that is fast and large enough to store the entire program
and data. In reality, large fast memories are prohibitively expensive. The
next chapter shows how to get most of the benefits of a large fast memory with a small fast memory that holds the most commonly used information and one or more larger but slower memories that hold the rest of
the information.
454 CHAPTER SEVEN Microarchitecture

Exercises 455
Exercises
Exercise 7.1 Suppose that one of the following control signals in the single-cycle
MIPS processor has a stuck-at-0 fault, meaning that the signal is always 0,
regardless of its intended value. What instructions would malfunction? Why?
(a) RegWrite
(b) ALUOp1
(c) MemWrite
Exercise 7.2 Repeat Exercise 7.1, assuming that the signal has a stuck-at-1 fault.
Exercise 7.3 Modify the single-cycle MIPS processor to implement one of the
following instructions. See Appendix B for a definition of the instructions. Mark
up a copy of Figure 7.11 to indicate the changes to the datapath. Name any new
control signals. Mark up a copy of Table 7.8 to show the changes to the main
decoder. Describe any other changes that are required.
(a) sll
(b) lui
(c) slti
(d) blez
(e) jal
(f) lh
Exercise 7.4 Many processor architectures have a load with postincrement
instruction, which updates the index register to point to the next memory word
after completing the load. lwinc $rt, imm($rs) is equivalent to the following
two instructions:
lw $rt, imm($rs)
addi $rs, $rs, 4
Table 7.8 Main decoder truth table to mark up with changes
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp
R-type 000000 1 1 0 0 0 0 10
lw 100011 1 0 1 0 0 1 00
sw 101011 0 X 1 0 1 X 00
beq 000100 0 X 0 1 0 X 01

Repeat Exercise 7.3 for the lwinc instruction. Is it possible to add the
instruction without modifying the register file?
Exercise 7.5 Add a single-precision floating-point unit to the single-cycle MIPS
processor to handle add.s, sub.s, and mul.s. Assume that you have single-precision floating-point adder and multiplier units available. Explain what changes
must be made to the datapath and the controller.
Exercise 7.6 Your friend is a crack circuit designer. She has offered to redesign
one of the units in the single-cycle MIPS processor to have half the delay. Using
the delays from Table 7.6, which unit should she work on to obtain the greatest
speedup of the overall processor, and what would the cycle time of the improved
machine be?
Exercise 7.7 Consider the delays given in Table 7.6. Ben Bitdiddle builds a prefix
adder that reduces the ALU delay by 20 ps. If the other element delays stay the
same, find the new cycle time of the single-cycle MIPS processor and determine
how long it takes to execute a benchmark with 100 billion instructions.
Exercise 7.8 Suppose one of the following control signals in the multicycle MIPS
processor has a stuck-at-0 fault, meaning that the signal is always 0, regardless
of its intended value. What instructions would malfunction? Why?
(a) MemtoReg
(b) ALUOp0
(c) PCSrc
Exercise 7.9 Repeat Exercise 7.8, assuming that the signal has a stuck-at-1 fault.
Exercise 7.10 Modify the HDL code for the single-cycle MIPS processor, given in
Section 7.6.1, to handle one of the new instructions from Exercise 7.3. Enhance
the testbench, given in Section 7.6.3 to test the new instruction.
Exercise 7.11 Modify the multicycle MIPS processor to implement one of the following instructions. See Appendix B for a definition of the instructions. Mark up
a copy of Figure 7.27 to indicate the changes to the datapath. Name any new
control signals. Mark up a copy of Figure 7.39 to show the changes to the controller FSM. Describe any other changes that are required.
(a) srlv
(b) ori
(c) xori
(d) jr
(e) bne
(f) lbu
456 CHAPTER SEVEN Microarchitecture

Exercise 7.12 Repeat Exercise 7.4 for the multicycle MIPS processor. Show the
changes to the multicycle datapath and control FSM. Is it possible to add the
instruction without modifying the register file?
Exercise 7.13 Repeat Exercise 7.5 for the multicycle MIPS processor.
Exercise 7.14 Suppose that the floating-point adder and multiplier from
Exercise 7.13 each take two cycles to operate. In other words, the inputs are
applied at the beginning of one cycle, and the output is available in the second
cycle. How does your answer to Exercise 7.13 change?
Exercise 7.15 Your friend, the crack circuit designer, has offered to redesign
one of the units in the multicycle MIPS processor to be much faster. Using the
delays from Table 7.6, which unit should she work on to obtain the greatest
speedup of the overall processor? How fast should it be? (Making it faster than
necessary is a waste of your friend’s effort.) What is the cycle time of the
improved processor?
Exercise 7.16 Repeat Exercise 7.7 for the multicycle processor.
Exercise 7.17 Suppose the multicycle MIPS processor has the component delays
given in Table 7.6. Alyssa P. Hacker designs a new register file that has 40% less
power but twice as much delay. Should she switch to the slower but lower power
register file for her multicycle processor design?
Exercise 7.18 Goliath Corp claims to have a patent on a three-ported register
file. Rather than fighting Goliath in court, Ben Bitdiddle designs a new register
file that has only a single read/write port (like the combined instruction and
data memory). Redesign the MIPS multicycle datapath and controller to use his
new register file.
Exercise 7.19 What is the CPI of the redesigned multicycle MIPS processor from
Exercise 7.18? Use the instruction mix from Example 7.7.
Exercise 7.20 How many cycles are required to run the following program on
the multicycle MIPS processor? What is the CPI of this program?
addi $s0, $0, 5 # sum  5
while:
beq $s0, $0, done# if result  0, execute the while block
addi $s0, $s0, 1 # while block: result  result  1
j while
done:
Exercises 457

Exercise 7.21 Repeat Exercise 7.20 for the following program.
add $s0, $0, $0 # i  0
add $s1, $0, $0 # sum  0
addi $t0, $0, 10 # $t0  10
loop:
slt $t1, $s0, $t0 # if (i  10), $t1  1, else $t1  0
beq $t1, $0, done # if $t1  0 (i  10), branch to done
add $s1, $s1, $s0 # sum  sum  i
addi $s0, $s0, 1 # increment i
j loop
done:
Exercise 7.22 Write HDL code for the multicycle MIPS processor. The processor
should be compatible with the following top-level module. The mem module is
used to hold both instructions and data. Test your processor using the testbench
from Section 7.6.3.
module top(input clk, reset,
output [31:0] writedata, adr,
output memwrite);
wire [31:0] readdata;
// instantiate processor and memories
mips mips(clk, reset, adr, writedata, memwrite, readdata);
mem mem(clk, memwrite, adr, writedata, readdata);
endmodule
module mem(input clk, we,
input [31:0] a, wd,
output [31:0] rd);
reg [31:0] RAM[63:0];
initial
begin
$readmemh(“memfile.dat”,RAM);
end
assign rd  RAM[a[31:2]]; // word aligned
always @ (posedge clk)
if (we)
RAM[a[31:2]]  wd;
endmodule
Exercise 7.23 Extend your HDL code for the multicycle MIPS processor from
Exercise 7.22 to handle one of the new instructions from Exercise 7.11. Enhance
the testbench to test the new instruction.
458 CHAPTER SEVEN Microarchitecture

Exercise 7.24 The pipelined MIPS processor is running the following program.
Which registers are being written, and which are being read on the fifth cycle?
add $s0, $t0, $t1
sub $s1, $t2, $t3
and $s2, $s0, $s1
or $s3, $t4, $t5
slt $s4, $s2, $s3
Exercise 7.25 Using a diagram similar to Figure 7.52, show the forwarding and
stalls needed to execute the following instructions on the pipelined MIPS processor.
add $t0, $s0, $s1
sub $t0, $t0, $s2
lw $t1, 60($t0)
and $t2, $t1, $t0
Exercise 7.26 Repeat Exercise 7.25 for the following instructions.
add $t0, $s0, $s1
lw $t1, 60($s2)
sub $t2, $t0, $s3
and $t3, $t1, $t0
Exercise 7.27 How many cycles are required for the pipelined MIPS processor to
issue all of the instructions for the program in Exercise 7.21? What is the CPI of
the processor on this program?
Exercise 7.28 Explain how to extend the pipelined MIPS processor to handle the
addi instruction.
Exercise 7.29 Explain how to extend the pipelined processor to handle the j
instruction. Give particular attention to how the pipeline is flushed when a jump
takes place.
Exercise 7.30 Examples 7.9 and 7.10 point out that the pipelined MIPS processor performance might be better if branches take place during the Execute stage
rather than the Decode stage. Show how to modify the pipelined processor from
Figure 7.58 to branch in the Execute stage. How do the stall and flush signals
change? Redo Examples 7.9 and 7.10 to find the new CPI, cycle time, and
overall time to execute the program.
Exercise 7.31 Your friend, the crack circuit designer, has offered to redesign one
of the units in the pipelined MIPS processor to be much faster. Using the delays
from Table 7.6 and Example 7.10, which unit should she work on to obtain the
greatest speedup of the overall processor? How fast should it be? (Making it
faster than necessary is a waste of your friend’s effort.) What is the cycle time
of the improved processor?
Exercises 459

Exercise 7.32 Consider the delays from Table 7.6 and Example 7.10. Now
suppose that the ALU were 20% faster. Would the cycle time of the pipelined
MIPS processor change? What if the ALU were 20% slower?
Exercise 7.33 Write HDL code for the pipelined MIPS processor. The processor
should be compatible with the top-level module from HDL Example 7.13. It
should support all of the instructions described in this chapter, including addi
and j (see Exercises 7.28 and 7.29). Test your design using the testbench from
HDL Example 7.12.
Exercise 7.34 Design the hazard unit shown in Figure 7.58 for the pipelined
MIPS processor. Use an HDL to implement your design. Sketch the hardware
that a synthesis tool might generate from your HDL.
Exercise 7.35 A nonmaskable interrupt (NMI) is triggered by an input pin to the
processor. When the pin is asserted, the current instruction should finish, then
the processor should set the Cause register to 0 and take an exception. Show
how to modify the multicycle processor in Figures 7.63 and 7.64 to handle
nonmaskable interrupts.
460 CHAPTER SEVEN Microarchitecture

Interview Questions
The following exercises present questions that have been asked at
interviews for digital design jobs.
Question 7.1 Explain the advantages of pipelined microprocessors.
Question 7.2 If additional pipeline stages allow a processor to go faster, why
don’t processors have 100 pipeline stages?
Question 7.3 Describe what a hazard is in a microprocessor and explain ways in
which it can be resolved. What are the pros and cons of each way?
Question 7.4 Describe the concept of a superscalar processor and its pros
and cons.
Interview Questions 461


8
8.1 Introduction
8.2 Memory System
Performance Analysis
8.3 Caches
8.4 Virtual Memory
8.5 Memory-Mapped I/O*
8.6 Real-World Perspective:
IA-32 Memory and I/O
Systems*
8.7 Summary
Exercises
Interview Questions
Memory Systems
8.1 INTRODUCTION
Computer system performance depends on the memory system as well
as the processor microarchitecture. Chapter 7 assumed an ideal memory system that could be accessed in a single clock cycle. However, this
would be true only for a very small memory—or a very slow processor!
Early processors were relatively slow, so memory was able to keep up.
But processor speed has increased at a faster rate than memory speeds.
DRAM memories are currently 10 to 100 times slower than processors. The increasing gap between processor and DRAM memory
speeds demands increasingly ingenious memory systems to try to
approximate a memory that is as fast as the processor. This chapter
investigates practical memory systems and considers trade-offs of
speed, capacity, and cost.
The processor communicates with the memory system over a memory interface. Figure 8.1 shows the simple memory interface used in our
multicycle MIPS processor. The processor sends an address over the
Address bus to the memory system. For a read, MemWrite is 0 and the
memory returns the data on the ReadData bus. For a write, MemWrite
is 1 and the processor sends data to memory on the WriteData bus.
The major issues in memory system design can be broadly explained
using a metaphor of books in a library. A library contains many books
on the shelves. If you were writing a term paper on the meaning of
dreams, you might go to the library1 and pull Freud’s The Interpretation
of Dreams off the shelf and bring it to your cubicle. After skimming it,
you might put it back and pull out Jung’s The Psychology of the
463
1 We realize that library usage is plummeting among college students because of the Internet.
But we also believe that libraries contain vast troves of hard-won human knowledge that are
not electronically available. We hope that Web searching does not completely displace the art
of library research.

Unconscious. You might then go back for another quote from
Interpretation of Dreams, followed by yet another trip to the stacks for
Freud’s The Ego and the Id. Pretty soon you would get tired of walking
from your cubicle to the stacks. If you are clever, you would save time by
keeping the books in your cubicle rather than schlepping them back and
forth. Furthermore, when you pull a book by Freud, you could also pull
several of his other books from the same shelf.
This metaphor emphasizes the principle, introduced in Section 6.2.1,
of making the common case fast. By keeping books that you have
recently used or might likely use in the future at your cubicle, you reduce
the number of time-consuming trips to the stacks. In particular, you use
the principles of temporal and spatial locality. Temporal locality means
that if you have used a book recently, you are likely to use it again soon.
Spatial locality means that when you use one particular book, you are
likely to be interested in other books on the same shelf.
The library itself makes the common case fast by using these principles of locality. The library has neither the shelf space nor the budget to
accommodate all of the books in the world. Instead, it keeps some of the
lesser-used books in deep storage in the basement. Also, it may have an
interlibrary loan agreement with nearby libraries so that it can offer
more books than it physically carries.
In summary, you obtain the benefits of both a large collection and
quick access to the most commonly used books through a hierarchy of
storage. The most commonly used books are in your cubicle. A larger
collection is on the shelves. And an even larger collection is available,
with advanced notice, from the basement and other libraries. Similarly,
memory systems use a hierarchy of storage to quickly access the most
commonly used data while still having the capacity to store large
amounts of data.
Memory subsystems used to build this hierarchy were introduced in
Section 5.5. Computer memories are primarily built from dynamic RAM
(DRAM) and static RAM (SRAM). Ideally, the computer memory system is fast, large, and cheap. In practice, a single memory only has two
of these three attributes; it is either slow, small, or expensive. But computer systems can approximate the ideal by combining a fast small cheap
memory and a slow large cheap memory. The fast memory stores the
most commonly used data and instructions, so on average the memory
464 CHAPTER EIGHT Memory Systems
Processor Memory Address
MemWrite
WriteData
ReadData
WE
CLK
Figure 8.1 The memory
interface

system appears fast. The large memory stores the remainder of the data
and instructions, so the overall capacity is large. The combination of two
cheap memories is much less expensive than a single large fast memory.
These principles extend to using an entire hierarchy of memories of
increasing capacity and decreasing speed.
Computer memory is generally built from DRAM chips. In 2006, a
typical PC had a main memory consisting of 256 MB to 1 GB of
DRAM, and DRAM cost about $100 per gigabyte (GB). DRAM prices
have declined at about 30% per year for the last three decades, and
memory capacity has grown at the same rate, so the total cost of the
memory in a PC has remained roughly constant. Unfortunately, DRAM
speed has improved by only about 7% per year, whereas processor
performance has improved at a rate of 30 to 50% per year, as shown in
Figure 8.2. The plot shows memory and processor speeds with the 1980
speeds as a baseline. In about 1980, processor and memory speeds were
the same. But performance has diverged since then, with memories
badly lagging.
DRAM could keep up with processors in the 1970s and early
1980’s, but it is now woefully too slow. The DRAM access time is one to
two orders of magnitude longer than the processor cycle time (tens of
nanoseconds, compared to less than one nanosecond).
To counteract this trend, computers store the most commonly used
instructions and data in a faster but smaller memory, called a cache. The
cache is usually built out of SRAM on the same chip as the processor.
The cache speed is comparable to the processor speed, because SRAM is
inherently faster than DRAM, and because the on-chip memory eliminates lengthy delays caused by traveling to and from a separate chip. In
2006, on-chip SRAM costs were on the order of $10,000/GB, but the
cache is relatively small (kilobytes to a few megabytes), so the overall
8.1 Introduction 465
Year
CPU
100,000
10,000
100
1000
Performance
10
1
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
Memory
Figure 8.2 Diverging processor
and memory performance
Adapted with permission from
Hennessy and Patterson, Computer
Architecture: A Quantitative Approach,
3rd ed., Morgan Kaufmann, 2003.

cost is low. Caches can store both instructions and data, but we will
refer to their contents generically as “data.”
If the processor requests data that is available in the cache, it is
returned quickly. This is called a cache hit. Otherwise, the processor
retrieves the data from main memory (DRAM). This is called a cache
miss. If the cache hits most of the time, then the processor seldom has to
wait for the slow main memory, and the average access time is low.
The third level in the memory hierarchy is the hard disk, or hard
drive. In the same way that a library uses the basement to store books
that do not fit in the stacks, computer systems use the hard disk to store
data that does not fit in main memory. In 2006, a hard disk cost less than
$1/GB and had an access time of about 10 ms. Hard disk costs have
decreased at 60%/year but access times scarcely improved. The hard disk
provides an illusion of more capacity than actually exists in the main
memory. It is thus called virtual memory. Like books in the basement,
data in virtual memory takes a long time to access. Main memory, also
called physical memory, holds a subset of the virtual memory. Hence, the
main memory can be viewed as a cache for the most commonly used data
from the hard disk.
Figure 8.3 summarizes the memory hierarchy of the computer system discussed in the rest of this chapter. The processor first seeks data in
a small but fast cache that is usually located on the same chip. If the data
is not available in the cache, the processor then looks in main memory. If
the data is not there either, the processor fetches the data from virtual
memory on the large but slow hard disk. Figure 8.4 illustrates this
capacity and speed trade-off in the memory hierarchy and lists typical
costs and access times in 2006 technology. As access time decreases,
speed increases.
Section 8.2 introduces memory system performance analysis.
Section 8.3 explores several cache organizations, and Section 8.4 delves
into virtual memory systems. To conclude, this chapter explores how
processors can access input and output devices, such as keyboards and
monitors, in much the same way as they access memory. Section 8.5
investigates such memory-mapped I/O.
466 CHAPTER EIGHT Memory Systems
CPU Cache
Main
Memory
Processor Chip CLK
Hard
Disk
Figure 8.3 A typical memory
hierarchy

8.2 MEMORY SYSTEM PERFORMANCE ANALYSIS
Designers (and computer buyers) need quantitative ways to measure the
performance of memory systems to evaluate the cost-benefit trade-offs
of various alternatives. Memory system performance metrics are miss
rate or hit rate and average memory access time. Miss and hit rates are
calculated as:
Number of misses Miss Rate  ------------------------------------------------------------------------------------- 1  Hit Rate
(8.1)
Number of total memory accesses
Number of hits Hit Rate  ------------------------------------------------------------------------------------- 1  Miss Rate
Number of total memory accesses
Example 8.1 CALCULATING CACHE PERFORMANCE
Suppose a program has 2000 data access instructions (loads or stores), and 1250
of these requested data values are found in the cache. The other 750 data values
are supplied to the processor by main memory or disk memory. What are the
miss and hit rates for the cache?
Solution: The miss rate is 750/2000  0.375  37.5%. The hit rate is
1250/2000  0.625  1  0.375  62.5%.
Average memory access time (AMAT) is the average time a processor
must wait for memory per load or store instruction. In the typical
computer system from Figure 8.3, the processor first looks for the data
in the cache. If the cache misses, the processor then looks in main
memory. If the main memory misses, the processor accesses virtual
memory on the hard disk. Thus, AMAT is calculated as:
AMAT  tcache  MRcache(tMM  MRMMtVM) (8.2)
8.2 Memory System Performance Analysis 467
Cache
Main Memory
Speed
Virtual Memory
Capacity
Technology Cost/GB Access Time
SRAM ~ $10,000 ~ 1 ns
DRAM ~ $100 ~ 100 ns
Hard Disk ~ $1 ~ 10,000,000 ns
Figure 8.4 Memory hierarchy
components, with typical
characteristics in 2006
Chapter 08.qx
where tcache, tMM, and tVM are the access times of the cache, main
memory, and virtual memory, and MRcache and MRMM are the cache and
main memory miss rates, respectively.
Example 8.2 CALCULATING AVERAGE MEMORY ACCESS TIME
Suppose a computer system has a memory organization with only two levels of
hierarchy, a cache and main memory. What is the average memory access time
given the access times and miss rates given in Table 8.1?
Solution: The average memory access time is 1  0.1(100)  11 cycles.
468 CHAPTER EIGHT Memory Systems
Table 8.1 Access times and miss rates
Memory Access Time Miss
Level (Cycles) Rate
Cache 1 10%
Main Memory 100 0%
Gene Amdahl, 1922–. Most
famous for Amdahl’s Law, an
observation he made in 1965.
While in graduate school, he
began designing computers in
his free time. This side work
earned him his Ph.D. in theoretical physics in 1952. He
joined IBM immediately after
graduation, and later went on
to found three companies,
including one called Amdahl
Corporation in 1970.
Example 8.3 IMPROVING ACCESS TIME
An 11-cycle average memory access time means that the processor spends ten
cycles waiting for data for every one cycle actually using that data. What cache
miss rate is needed to reduce the average memory access time to 1.5 cycles given
the access times in Table 8.1?
Solution: If the miss rate is m, the average access time is 1  100m. Setting this
time to 1.5 and solving for m requires a cache miss rate of 0.5%.
As a word of caution, performance improvements might not always
be as good as they sound. For example, making the memory system ten
times faster will not necessarily make a computer program run ten times
as fast. If 50% of a program’s instructions are loads and stores, a tenfold memory system improvement only means a 1.82-fold improvement
in program performance. This general principle is called Amdahl’s Law,
which says that the effort spent on increasing the performance of a subsystem is worthwhile only if the subsystem affects a large percentage of
the overall performance.
8.3 CACHES
A cache holds commonly used memory data. The number of data
words that it can hold is called the capacity, C. Because the capacity
C
of the cache is smaller than that of main memory, the computer system designer must choose what subset of the main memory is kept in
the cache.
When the processor attempts to access data, it first checks the cache
for the data. If the cache hits, the data is available immediately. If the
cache misses, the processor fetches the data from main memory and
places it in the cache for future use. To accommodate the new data, the
cache must replace old data. This section investigates these issues in
cache design by answering the following questions: (1) What data is held
in the cache? (2) How is the data found? and (3) What data is replaced
to make room for new data when the cache is full?
When reading the next sections, keep in mind that the driving force
in answering these questions is the inherent spatial and temporal locality
of data accesses in most applications. Caches use spatial and temporal
locality to predict what data will be needed next. If a program accesses
data in a random order, it would not benefit from a cache.
As we explain in the following sections, caches are specified by their
capacity (C), number of sets (S), block size (b), number of blocks (B),
and degree of associativity (N).
Although we focus on data cache loads, the same principles apply
for fetches from an instruction cache. Data cache store operations are
similar and are discussed further in Section 8.3.4.
8.3.1 What Data Is Held in the Cache?
An ideal cache would anticipate all of the data needed by the processor
and fetch it from main memory ahead of time so that the cache has a
zero miss rate. Because it is impossible to predict the future with perfect
accuracy, the cache must guess what data will be needed based on the
past pattern of memory accesses. In particular, the cache exploits temporal and spatial locality to achieve a low miss rate.
Recall that temporal locality means that the processor is likely to
access a piece of data again soon if it has accessed that data recently.
Therefore, when the processor loads or stores data that is not in the
cache, the data is copied from main memory into the cache. Subsequent
requests for that data hit in the cache.
Recall that spatial locality means that, when the processor accesses a
piece of data, it is also likely to access data in nearby memory locations.
Therefore, when the cache fetches one word from memory, it may also
fetch several adjacent words. This group of words is called a cache
block. The number of words in the cache block, b, is called the block
size. A cache of capacity C contains B  C/b blocks.
The principles of temporal and spatial locality have been experimentally verified in real programs. If a variable is used in a program, the
8.3 Caches 469
Cache: a hiding place
especially for concealing and
preserving provisions or
implements.
– Merriam Webster Online
Dictionary. 2006. http://
www.merriam-webster.com
C
same variable is likely to be used again, creating temporal locality. If an
element in an array is used, other elements in the same array are also
likely to be used, creating spatial locality.
8.3.2 How Is the Data Found?
A cache is organized into S sets, each of which holds one or more blocks
of data. The relationship between the address of data in main memory
and the location of that data in the cache is called the mapping. Each
memory address maps to exactly one set in the cache. Some of the
address bits are used to determine which cache set contains the data. If
the set contains more than one block, the data may be kept in any of the
blocks in the set.
Caches are categorized based on the number of blocks in a set. In a
direct mapped cache, each set contains exactly one block, so the cache
has S  B sets. Thus, a particular main memory address maps to a
unique block in the cache. In an N-way set associative cache, each set
contains N blocks. The address still maps to a unique set, with S  B/N
sets. But the data from that address can go in any of the N blocks in that
set. A fully associative cache has only S  1 set. Data can go in any of
the B blocks in the set. Hence, a fully associative cache is another name
for a B-way set associative cache.
To illustrate these cache organizations, we will consider a MIPS
memory system with 32-bit addresses and 32-bit words. The memory is
byte-addressable, and each word is four bytes, so the memory consists of
230 words aligned on word boundaries. We analyze caches with an eightword capacity (C) for the sake of simplicity. We begin with a one-word
block size (b), then generalize later to larger blocks.
Direct Mapped Cache
A direct mapped cache has one block in each set, so it is organized into
S  B sets. To understand the mapping of memory addresses onto cache
blocks, imagine main memory as being mapped into b-word blocks, just
as the cache is. An address in block 0 of main memory maps to set 0 of
the cache. An address in block 1 of main memory maps to set 1 of the
cache, and so forth until an address in block B  1 of main memory
maps to block B  1 of the cache. There are no more blocks of the
cache, so the mapping wraps around, such that block B of main memory
maps to block 0 of the cache.
This mapping is illustrated in Figure 8.5 for a direct mapped cache
with a capacity of eight words and a block size of one word. The cache
has eight sets, each of which contains a one-word block. The bottom
two bits of the address are always 00, because they are word aligned.
The next log28  3 bits indicate the set onto which the memory address
maps. Thus, the data at addresses 0x00000004, 0x00000024, . . . ,
470 CHAPTER EIGHT Memory Systems
Chapter
0xFFFFFFE4 all map to set 1, as shown in blue. Likewise, data at
addresses 0x00000010, . . . , 0xFFFFFFF0 all map to set 4, and so
forth. Each main memory address maps to exactly one set in the cache.
Example 8.4 CACHE FIELDS
To what cache set in Figure 8.5 does the word at address 0x00000014 map?
Name another address that maps to the same set.
Solution: The two least significant bits of the address are 00, because the address
is word aligned. The next three bits are 101, so the word maps to set 5. Words at
addresses 0x34, 0x54, 0x74, . . . , 0xFFFFFFF4 all map to this same set.
Because many addresses map to a single set, the cache must also
keep track of the address of the data actually contained in each set.
The least significant bits of the address specify which set holds the
data. The remaining most significant bits are called the tag and indicate which of the many possible addresses is held in that set.
In our previous example, the two least significant bits of the 32-bit
address are called the byte offset, because they indicate the byte within
the word. The next three bits are called the set bits, because they indicate
the set to which the address maps. (In general, the number of set bits is
log2S.) The remaining 27 tag bits indicate the memory address of the
data stored in a given cache set. Figure 8.6 shows the cache fields for
address 0xFFFFFFE4. It maps to set 1 and its tag is all 1’s.
8.3 Caches 471
00...00010000
230-Word Main Memory
mem[0x00000000]
mem[0x00000004]
mem[0x00000008]
mem[0x0000000C]
mem[0x00000010]
mem[0x00000014]
mem[0x00000018]
mem[0x0000001C]
mem[0x00000020]
mem[0x00000024]
mem[0xFFFFFFE0]
mem[0xFFFFFFE4]
mem[0xFFFFFFE8]
mem[0xFFFFFFEC]
mem[0xFFFFFFF0]
mem[0xFFFFFFF4]
mem[0xFFFFFFF8]
mem[0xFFFFFFFC]
23-Word Cache
Address
00...00000000
00...00000100
00...00001000
00...00001100
00...00010100
00...00011000
00...00011100
00...00100000
00...00100100
11...11110000
11...11100000
11...11100100
11...11101000
11...11101100
11...11110100
11...11111000
11...11111100
Set 7 (111)
Set 6 (110)
Set 5 (101)
Set 4 (100)
Set 3 (011)
Set 2 (010)
Set 1 (001)
Set 0 (000)
Data
Figure 8.5 Mapping of main
memory to a direct mapped
cache

Example 8.5 CACHE FIELDS
Find the number of set and tag bits for a direct mapped cache with 1024 (210)
sets and a one-word block size. The address size is 32 bits.
Solution: A cache with 210 sets requires log2(210)  10 set bits. The two least significant bits of the address are the byte offset, and the remaining 32  10  2  20
bits form the tag.
Sometimes, such as when the computer first starts up, the cache sets
contain no data at all. The cache uses a valid bit for each set to indicate
whether the set holds meaningful data. If the valid bit is 0, the contents
are meaningless.
Figure 8.7 shows the hardware for the direct mapped cache of
Figure 8.5. The cache is constructed as an eight-entry SRAM. Each
entry, or set, contains one line consisting of 32 bits of data, 27 bits of
tag, and 1 valid bit. The cache is accessed using the 32-bit address. The
two least significant bits, the byte offset bits, are ignored for word
accesses. The next three bits, the set bits, specify the entry or set in the
cache. A load instruction reads the specified entry from the cache and
checks the tag and valid bits. If the tag matches the most significant
472 CHAPTER EIGHT Memory Systems
00
Tag Set
Byte
Offset Memory
Address 111 ... 111 001
FFFFFF E 4
Figure 8.6 Cache fields for
address 0xFFFFFFE4 when
mapping to the cache in
Figure 8.5
Tag Data
00
Tag Set
Byte
Offset Memory
Address
Hit Data
V
=
27 3
27 32
8-entry x
(1+27+32)-bit
SRAM
Set 7
Set 6
Set 5
Set 4
Set 3
Set 2
Set 1
Set 0
Figure 8.7 Direct mapped cache
with 8 sets
Chap
27 bits of the address and the valid bit is 1, the cache hits and the data is
returned to the processor. Otherwise, the cache misses and the memory
system must fetch the data from main memory.
Example 8.6 TEMPORAL LOCALITY WITH A DIRECT MAPPED CACHE
Loops are a common source of temporal and spatial locality in applications.
Using the eight-entry cache of Figure 8.7, show the contents of the cache after
executing the following silly loop in MIPS assembly code. Assume that the cache
is initially empty. What is the miss rate?
addi $t0, $0, 5
loop: beq $t0, $0, done
lw $t1, 0x4($0)
lw $t2, 0xC($0)
lw $t3, 0x8($0)
addi $t0, $t0, 1
j loop
done:
Solution: The program contains a loop that repeats for five iterations. Each
iteration involves three memory accesses (loads), resulting in 15 total memory
accesses. The first time the loop executes, the cache is empty and the data must
be fetched from main memory locations 0x4, 0xC, and 0x8 into cache sets 1,
3, and 2, respectively. However, the next four times the loop executes, the data
is found in the cache. Figure 8.8 shows the contents of the cache during the
last request to memory address 0x4. The tags are all 0 because the upper 27
bits of the addresses are 0. The miss rate is 3/15  20%.
8.3 Caches 473
V Tag Data
1 00...00 mem[0x00...04]
0
0
0
0
0
00
Tag Set
Byte
Offset Memory
Address
V
3
00...00 001
1
00...00
00...00
1
mem[0x00...0C]
mem[0x00...08]
Set 7 (111)
Set 6 (110)
Set 5 (101)
Set 4 (100)
Set 3 (011)
Set 2 (010)
Set 1 (001)
Set 0 (000)
Figure 8.8 Direct mapped cache
contents
When two recently accessed addresses map to the same cache block,
a conflict occurs, and the most recently accessed address evicts the previous one from the block. Direct mapped caches have only one block in
each set, so two addresses that map to the same set always cause a conflict. The example on the next page illustrates conflicts.
Ch
Example 8.7 CACHE BLOCK CONFLICT
What is the miss rate when the following loop is executed on the eight-word
direct mapped cache from Figure 8.7? Assume that the cache is initially empty.
addi $t0, $0, 5
loop: beq $t0, $0, done
lw $t1, 0x4($0)
lw $t2, 0x24($0)
addi $t0, $t0, 1
j loop
done:
Solution: Memory addresses 0x4 and 0x24 both map to set 1. During the initial
execution of the loop, data at address 0x4 is loaded into set 1 of the cache. Then
data at address 0x24 is loaded into set 1, evicting the data from address 0x4.
Upon the second execution of the loop, the pattern repeats and the cache must
refetch data at address 0x4, evicting data from address 0x24. The two addresses
conflict, and the miss rate is 100%.
Multi-way Set Associative Cache
An N-way set associative cache reduces conflicts by providing N blocks
in each set where data mapping to that set might be found. Each memory address still maps to a specific set, but it can map to any one of the
N blocks in the set. Hence, a direct mapped cache is another name for a
one-way set associative cache. N is also called the degree of associativity
of the cache.
Figure 8.9 shows the hardware for a C  8-word, N  2-way
set associative cache. The cache now has only S  4 sets rather than 8.
474 CHAPTER EIGHT Memory Systems
Tag Data
Tag Set
Byte
Offset Memory
Address
Data
1
0
Hit1
V
=
00
32 32
32
V Tag Data
=
Hit Hit1 0
Hit
28 2
28 28
Way 1 Way 0
Set 3
Set 2
Set 1
Set 0
Figure 8.9 Two-way set
associative cache
Chap
Thus, only log24  2 set bits rather than 3 are used to select the set.
The tag increases from 27 to 28 bits. Each set contains two ways or
degrees of associativity. Each way consists of a data block and the
valid and tag bits. The cache reads blocks from both ways in the
selected set and checks the tags and valid bits for a hit. If a hit occurs
in one of the ways, a multiplexer selects data from that way.
Set associative caches generally have lower miss rates than direct
mapped caches of the same capacity, because they have fewer conflicts.
However, set associative caches are usually slower and somewhat more
expensive to build because of the output multiplexer and additional
comparators. They also raise the question of which way to replace when
both ways are full; this is addressed further in Section 8.3.3. Most
commercial systems use set associative caches.
Example 8.8 SET ASSOCIATIVE CACHE MISS RATE
Repeat Example 8.7 using the eight-word two-way set associative cache from
Figure 8.9.
Solution: Both memory accesses, to addresses 0x4 and 0x24, map to set 1.
However, the cache has two ways, so it can accommodate data from both
addresses. During the first loop iteration, the empty cache misses both
addresses and loads both words of data into the two ways of set 1, as shown in
Figure 8.10. On the next four iterations, the cache hits. Hence, the miss rate is
2/10  20%. Recall that the direct mapped cache of the same size from
Example 8.7 had a miss rate of 100%.
8.3 Caches 475
V Tag Data V Tag Data
00...001 mem[0x00...24] 00...101 mem[0x00...04]
0
0
0
0
0
0
Way 1 Way 0
Set 3
Set 2
Set 1
Set 0
Figure 8.10 Two-way set
associative cache contents
Fully Associative Cache
A fully associative cache contains a single set with B ways, where B is
the number of blocks. A memory address can map to a block in any of
these ways. A fully associative cache is another name for a B-way set
associative cache with one set.
Figure 8.11 shows the SRAM array of a fully associative cache with
eight blocks. Upon a data request, eight tag comparisons (not shown) must
be made, because the data could be in any block. Similarly, an 8:1 multiplexer chooses the proper data if a hit occurs. Fully associative caches tend
Ch
to have the fewest conflict misses for a given cache capacity, but they
require more hardware for additional tag comparisons. They are best suited
to relatively small caches because of the large number of comparators.
Block Size
The previous examples were able to take advantage only of temporal
locality, because the block size was one word. To exploit spatial locality,
a cache uses larger blocks to hold several consecutive words.
The advantage of a block size greater than one is that when a miss
occurs and the word is fetched into the cache, the adjacent words in the
block are also fetched. Therefore, subsequent accesses are more likely to
hit because of spatial locality. However, a large block size means that a
fixed-size cache will have fewer blocks. This may lead to more conflicts,
increasing the miss rate. Moreover, it takes more time to fetch the missing
cache block after a miss, because more than one data word is fetched
from main memory. The time required to load the missing block into the
cache is called the miss penalty. If the adjacent words in the block are not
accessed later, the effort of fetching them is wasted. Nevertheless, most
real programs benefit from larger block sizes.
Figure 8.12 shows the hardware for a C  8-word direct mapped
cache with a b  4-word block size. The cache now has only B  C/b  2
blocks. A direct mapped cache has one block in each set, so this cache is
476 CHAPTER EIGHT Memory Systems
V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data
Way 7 Way 6 Way 5 Way 4 Way 3 Way 2 Way 1 Way 0
Tag Data
00
Tag
Byte
Offset Memory
Address
Data
V
Block
Offset
32 32 32 32
32
Hit
=
Set
27
27 2
Set 1
Set 0
11
10
01
00
Figure 8.11 Eight-block fully associative cache
Figure 8.12 Direct mapped cache with two sets and a four-word block size
Chap
organized as two sets. Thus, only log22  1 bit is used to select the set.
A multiplexer is now needed to select the word within the block. The
multiplexer is controlled by the log24  2 block offset bits of the address.
The most significant 27 address bits form the tag. Only one tag is needed
for the entire block, because the words in the block are at consecutive
addresses.
Figure 8.13 shows the cache fields for address 0x8000009C when
it maps to the direct mapped cache of Figure 8.12. The byte offset
bits are always 0 for word accesses. The next log2b  2 block offset
bits indicate the word within the block. And the next bit indicates the
set. The remaining 27 bits are the tag. Therefore, word 0x8000009C
maps to set 1, word 3 in the cache. The principle of using larger
block sizes to exploit spatial locality also applies to associative
caches.
Example 8.9 SPATIAL LOCALITY WITH A DIRECT MAPPED CACHE
Repeat Example 8.6 for the eight-word direct mapped cache with a four-word
block size.
Solution: Figure 8.14 shows the contents of the cache after the first memory
access. On the first loop iteration, the cache misses on the access to memory
address 0x4. This access loads data at addresses 0x0 through 0xC into the cache
block. All subsequent accesses (as shown for address 0xC) hit in the cache.
Hence, the miss rate is 1/15  6.67%.
8.3 Caches 477
00
Tag
Byte
Offset Memory
Address 100...100 11
Block
Offset
1
800000 9 C
Set
Figure 8.13 Cache fields for
address 0x8000009C when
mapping to the cache of
Figure 8.12
Set 1
V Tag Data
1 00...00 mem[0x00...0C] Set 0
0
mem[0x00...08] mem[0x00...04] mem[0x00...00]
00
Tag
Byte
Offset Memory
Address
V
Block
Set Offset
00...00 0 11
Figure 8.14 Cache contents with a block size (b) of four words
Putting It All Together
Caches are organized as two-dimensional arrays. The rows are called
sets, and the columns are called ways. Each entry in the array consists of
Chap
a data block and its associated valid and tag bits. Caches are
characterized by
 capacity C
 block size b (and number of blocks, B  C/b)
 number of blocks in a set (N)
Table 8.2 summarizes the various cache organizations. Each address in
memory maps to only one set but can be stored in any of the ways.
Cache capacity, associativity, set size, and block size are typically
powers of 2. This makes the cache fields (tag, set, and block offset bits)
subsets of the address bits.
Increasing the associativity, N, usually reduces the miss rate caused
by conflicts. But higher associativity requires more tag comparators.
Increasing the block size, b, takes advantage of spatial locality to reduce
the miss rate. However, it decreases the number of sets in a fixed sized
cache and therefore could lead to more conflicts. It also increases the
miss penalty.
8.3.3 What Data Is Replaced?
In a direct mapped cache, each address maps to a unique block and set.
If a set is full when new data must be loaded, the block in that set is
replaced with the new data. In set associative and fully associative
caches, the cache must choose which block to evict when a cache set is
full. The principle of temporal locality suggests that the best choice is to
evict the least recently used block, because it is least likely to be used
again soon. Hence, most associative caches have a least recently used
(LRU) replacement policy.
In a two-way set associative cache, a use bit, U, indicates which way
within a set was least recently used. Each time one of the ways is used, U
is adjusted to indicate the other way. For set associative caches with
more than two ways, tracking the least recently used way becomes complicated. To simplify the problem, the ways are often divided into two
groups and U indicates which group of ways was least recently used.
478 CHAPTER EIGHT Memory Systems
Table 8.2 Cache organizations
Number of Ways Number of Sets
Organization (N) (S)
direct mapped 1 B
set associative 1  N  B B/N
fully associative B 1
Chap
Upon replacement, the new block replaces a random block within the
least recently used group. Such a policy is called pseudo-LRU and is
good enough in practice.
Example 8.10 LRU REPLACEMENT
Show the contents of an eight-word two-way set associative cache after executing the following code. Assume LRU replacement, a block size of one word, and
an initially empty cache.
lw $t0, 0x04($0)
lw $t1, 0x24($0)
lw $t2, 0x54($0)
Solution: The first two instructions load data from memory addresses 0x4 and
0x24 into set 1 of the cache, shown in Figure 8.15(a). U  0 indicates that data
in way 0 was the least recently used. The next memory access, to address 0x54,
also maps to set 1 and replaces the least recently used data in way 0, as shown in
Figure 8.15(b), The use bit, U, is set to 1 to indicate that data in way 1 was the
least recently used.
8.3 Caches 479
V Tag Data
0
V Tag Data
0
0
0
0
0
U
1 00...010 mem[0x00...24] 1 00...000 mem[0x00...04]
0
0
0
0
(a)
Way 1 Way 0
Set 3 (11)
Set 2 (10)
Set 1 (01)
Set 0 (00)
V Tag Data
0
V Tag Data
0
0
0
0
0
U
1 00...010 mem[0x00...24] 1 00...101 mem[0x00...54]
0
0
0
1
(b)
Way 1 Way 0
Set 3 (11)
Set 2 (10)
Set 1 (01)
Set 0 (00)
Figure 8.15 Two-way associative
cache with LRU replacement
8.3.4 Advanced Cache Design*
Modern systems use multiple levels of caches to decrease memory access
time. This section explores the performance of a two-level caching system and examines how block size, associativity, and cache capacity
affect miss rate. The section also describes how caches handle stores, or
writes, by using a write-through or write-back policy.
C
Multiple-Level Caches
Large caches are beneficial because they are more likely to hold data of
interest and therefore have lower miss rates. However, large caches tend
to be slower than small ones. Modern systems often use two levels of
caches, as shown in Figure 8.16. The first-level (L1) cache is small
enough to provide a one- or two-cycle access time. The second-level
(L2) cache is also built from SRAM but is larger, and therefore slower,
than the L1 cache. The processor first looks for the data in the L1
cache. If the L1 cache misses, the processor looks in the L2 cache. If the
L2 cache misses, the processor fetches the data from main memory.
Some modern systems add even more levels of cache to the memory
hierarchy, because accessing main memory is so slow.
Example 8.11 SYSTEM WITH AN L2 CACHE
Use the system of Figure 8.16 with access times of 1, 10, and 100 cycles for the
L1 cache, L2 cache, and main memory, respectively. Assume that the L1 and L2
caches have miss rates of 5% and 20%, respectively. Specifically, of the 5% of
accesses that miss the L1 cache, 20% of those also miss the L2 cache. What is
the average memory access time (AMAT)?
Solution: Each memory access checks the L1 cache. When the L1 cache misses
(5% of the time), the processor checks the L2 cache. When the L2 cache
misses (20% of the time), the processor fetches the data from main memory.
Using Equation 8.2, we calculate the average memory access time as follows:
1 cycle  0.05[10 cycles  0.2(100 cycles)]  2.5 cycles
The L2 miss rate is high because it receives only the “hard” memory accesses,
those that miss in the L1 cache. If all accesses went directly to the L2 cache, the
L2 miss rate would be about 1%.
480 CHAPTER EIGHT Memory Systems
L1
Cache
L2 Cache
Main Memory
Capacity
Virtual Memory
Speed
Figure 8.16 Memory hierarchy
with two levels of cache
C
Reducing Miss Rate
Cache misses can be reduced by changing capacity, block size, and/or
associativity. The first step to reducing the miss rate is to understand
the causes of the misses. The misses can be classified as compulsory,
capacity, and conflict. The first request to a cache block is called a
compulsory miss, because the block must be read from memory regardless of the cache design. Capacity misses occur when the cache is too
small to hold all concurrently used data. Conflict misses are caused
when several addresses map to the same set and evict blocks that are
still needed.
Changing cache parameters can affect one or more type of cache
miss. For example, increasing cache capacity can reduce conflict and
capacity misses, but it does not affect compulsory misses. On the other
hand, increasing block size could reduce compulsory misses (due to spatial locality) but might actually increase conflict misses (because more
addresses would map to the same set and could conflict).
Memory systems are complicated enough that the best way to evaluate their performance is by running benchmarks while varying cache
parameters. Figure 8.17 plots miss rate versus cache size and degree of
associativity for the SPEC2000 benchmark. This benchmark has a
small number of compulsory misses, shown by the dark region near the
x-axis. As expected, when cache size increases, capacity misses
decrease. Increased associativity, especially for small caches, decreases
the number of conflict misses shown along the top of the curve.
8.3 Caches 481
0.10
1-way
2-way
4-way
8-way
Capacity Compulsory
Cache Size (KB)
0.09
0.08
0.07
0.06
0.05 Miss Rate
per Type
0.04
0.03
0.02
0.01
0.00
84 16 32 64 128 256 512 1024
Figure 8.17 Miss rate versus
cache size and associativity on
SPEC2000 benchmark
Adapted with permission from
Hennessy and Patterson, Computer
Architecture: A Quantitative Approach,
3rd ed., Morgan Kaufmann, 2003.

Increasing associativity beyond four or eight ways provides only small
decreases in miss rate.
As mentioned, miss rate can also be decreased by using larger
block sizes that take advantage of spatial locality. But as block size
increases, the number of sets in a fixed size cache decreases, increasing
the probability of conflicts. Figure 8.18 plots miss rate versus block
size (in number of bytes) for caches of varying capacity. For small
caches, such as the 4-KB cache, increasing the block size beyond
64 bytes increases the miss rate because of conflicts. For larger caches,
increasing the block size does not change the miss rate. However, large
block sizes might still increase execution time because of the larger
miss penalty, the time required to fetch the missing cache block from
main memory on a miss.
Write Policy
The previous sections focused on memory loads. Memory stores, or
writes, follow a similar procedure as loads. Upon a memory store, the
processor checks the cache. If the cache misses, the cache block is
fetched from main memory into the cache, and then the appropriate
word in the cache block is written. If the cache hits, the word is simply
written to the cache block.
Caches are classified as either write-through or write-back. In a
write-through cache, the data written to a cache block is simultaneously
written to main memory. In a write-back cache, a dirty bit (D) is associated with each cache block. D is 1 when the cache block has been written and 0 otherwise. Dirty cache blocks are written back to main
memory only when they are evicted from the cache. A write-through
482 CHAPTER EIGHT Memory Systems
10%
Miss
Rate 5%
0%
16 32 64
Block Size
128 256
256 K
64 K
16 K
4 K
Figure 8.18 Miss rate versus block size and cache size on SPEC92 benchmark Adapted with permission from
Hennessy and Patterson, Computer Architecture: A Quantitative Approach, 3rd ed., Morgan Kaufmann, 2003.

cache requires no dirty bit but usually requires more main memory
writes than a write-back cache. Modern caches are usually write-back,
because main memory access time is so large.
Example 8.12 WRITE-THROUGH VERSUS WRITE-BACK
Suppose a cache has a block size of four words. How many main memory
accesses are required by the following code when using each write policy: writethrough or write-back?
sw $t0, 0x0($0)
sw $t0, 0xC($0)
sw $t0, 0x8($0)
sw $t0, 0x4($0)
Solution: All four store instructions write to the same cache block. With a writethrough cache, each store instruction writes a word to main memory, requiring
four main memory writes. A write-back policy requires only one main memory
access, when the dirty cache block is evicted.
8.3.5 The Evolution of MIPS Caches*
Table 8.3 traces the evolution of cache organizations used by the MIPS
processor from 1985 to 2004. The major trends are the introduction of
multiple levels of cache, larger cache capacity, and increased associativity. These trends are driven by the growing disparity between CPU
frequency and main memory speed and the decreasing cost of transistors. The growing difference between CPU and memory speeds necessitates a lower miss rate to avoid the main memory bottleneck, and the
decreasing cost of transistors allows larger cache sizes.
8.3 Caches 483
Table 8.3 MIPS cache evolution*
Year CPU MHz L1 Cache L2 Cache
1985 R2000 16.7 none none
1990 R3000 33 32 KB direct mapped none
1991 R4000 100 8 KB direct mapped 1 MB direct mapped
1995 R10000 250 32 KB two-way 4 MB two-way
2001 R14000 600 32 KB two-way 16 MB two-way
2004 R16000A 800 64 KB two-way 16 MB two-way
* Adapted from D. Sweetman, See MIPS Run, Morgan Kaufmann, 1999.

8.4 VIRTUAL MEMORY
Most modern computer systems use a hard disk (also called a hard
drive) as the lowest level in the memory hierarchy (see Figure 8.4).
Compared with the ideal large, fast, cheap memory, a hard disk is large
and cheap but terribly slow. The disk provides a much larger capacity
than is possible with a cost-effective main memory (DRAM). However, if
a significant fraction of memory accesses involve the disk, performance
is dismal. You may have encountered this on a PC when running too
many programs at once.
Figure 8.19 shows a hard disk with the lid of its case removed. As the
name implies, the hard disk contains one or more rigid disks or platters,
each of which has a read/write head on the end of a long triangular arm.
The head moves to the correct location on the disk and reads or writes
data magnetically as the disk rotates beneath it. The head takes several
milliseconds to seek the correct location on the disk, which is fast from a
human perspective but millions of times slower than the processor.
484 CHAPTER EIGHT Memory Systems
Figure 8.19 Hard disk

The objective of adding a hard disk to the memory hierarchy is to
inexpensively give the illusion of a very large memory while still providing
the speed of faster memory for most accesses. A computer with only 128
MB of DRAM, for example, could effectively provide 2 GB of memory
using the hard disk. This larger 2-GB memory is called virtual memory,
and the smaller 128-MB main memory is called physical memory. We will
use the term physical memory to refer to main memory throughout this
section.
Programs can access data anywhere in virtual memory, so they must
use virtual addresses that specify the location in virtual memory. The
physical memory holds a subset of most recently accessed virtual memory. In this way, physical memory acts as a cache for virtual memory.
Thus, most accesses hit in physical memory at the speed of DRAM, yet
the program enjoys the capacity of the larger virtual memory.
Virtual memory systems use different terminologies for the same
caching principles discussed in Section 8.3. Table 8.4 summarizes the
analogous terms. Virtual memory is divided into virtual pages, typically
4 KB in size. Physical memory is likewise divided into physical pages of
the same size. A virtual page may be located in physical memory
(DRAM) or on the disk. For example, Figure 8.20 shows a virtual memory that is larger than physical memory. The rectangles indicate pages.
Some virtual pages are present in physical memory, and some are located
on the disk. The process of determining the physical address from
the virtual address is called address translation. If the processor attempts
to access a virtual address that is not in physical memory, a page fault
occurs, and the operating system loads the page from the hard disk into
physical memory.
To avoid page faults caused by conflicts, any virtual page can map
to any physical page. In other words, physical memory behaves as a fully
associative cache for virtual memory. In a conventional fully associative
cache, every cache block has a comparator that checks the most significant address bits against a tag to determine whether the request hits in
8.4 Virtual Memory 485
A computer with 32-bit
addresses can access a maximum of 232 bytes  4 GB of
memory. This is one of the
motivations for moving to
64-bit computers, which can
access far more memory.
Table 8.4 Analogous cache and virtual memory terms
Cache Virtual Memory
Block Page
Block size Page size
Block offset Page offset
Miss Page fault
Tag Virtual page number
C
the block. In an analogous virtual memory system, each physical page
would need a comparator to check the most significant virtual address
bits against a tag to determine whether the virtual page maps to that
physical page.
A realistic virtual memory system has so many physical pages that
providing a comparator for each page would be excessively expensive.
Instead, the virtual memory system uses a page table to perform address
translation. A page table contains an entry for each virtual page, indicating its location in physical memory or that it is on the disk. Each load or
store instruction requires a page table access followed by a physical
memory access. The page table access translates the virtual address used
by the program to a physical address. The physical address is then used
to actually read or write the data.
The page table is usually so large that it is located in physical
memory. Hence, each load or store involves two physical memory
accesses: a page table access, and a data access. To speed up address
translation, a translation lookaside buffer (TLB) caches the most commonly used page table entries.
The remainder of this section elaborates on address translation, page
tables, and TLBs.
8.4.1 Address Translation
In a system with virtual memory, programs use virtual addresses so that
they can access a large memory. The computer must translate these virtual addresses to either find the address in physical memory or take a
page fault and fetch the data from the hard disk.
Recall that virtual memory and physical memory are divided into
pages. The most significant bits of the virtual or physical address specify
the virtual or physical page number. The least significant bits specify the
word within the page and are called the page offset.
486 CHAPTER EIGHT Memory Systems
Physical Memory
Physical Addresses
Virtual Addresses
Hard Disk
Address Translation
Figure 8.20 Virtual and physical
pages

Figure 8.21 illustrates the page organization of a virtual memory
system with 2 GB of virtual memory and 128 MB of physical memory
divided into 4-KB pages. MIPS accommodates 32-bit addresses. With
a 2-GB  231-byte virtual memory, only the least significant 31 virtual
address bits are used; the 32nd bit is always 0. Similarly, with a
128-MB  227-byte physical memory, only the least significant
27 physical address bits are used; the upper 5 bits are always 0.
Because the page size is 4 KB  212 bytes, there are 231/212  219
virtual pages and 227/212  215 physical pages. Thus, the virtual and
physical page numbers are 19 and 15 bits, respectively. Physical memory
can only hold up to 1/16th of the virtual pages at any given time. The
rest of the virtual pages are kept on disk.
Figure 8.21 shows virtual page 5 mapping to physical page 1, virtual
page 0x7FFFC mapping to physical page 0x7FFE, and so forth. For
example, virtual address 0x53F8 (an offset of 0x3F8 within virtual
page 5) maps to physical address 0x13F8 (an offset of 0x3F8 within
physical page 1). The least significant 12 bits of the virtual and physical
addresses are the same (0x3F8) and specify the page offset within the
virtual and physical pages. Only the page number needs to be translated
to obtain the physical address from the virtual address.
Figure 8.22 illustrates the translation of a virtual address to a physical address. The least significant 12 bits indicate the page offset and
require no translation. The upper 19 bits of the virtual address specify
the virtual page number (VPN) and are translated to a 15-bit physical
page number (PPN). The next two sections describe how page tables and
TLBs are used to perform this address translation.
8.4 Virtual Memory 487
Physical Memory
Physical
Page
Number Physical Addresses
Virtual Memory
Virtual
Page
Number Virtual Addresses
7FFF 0x7FFF000 - 0x7FFFFFF
0x7FFE000 - 0x7FFEFFF
0x0000000 - 0x0000FFF
0x0001000 - 0x0001FFF
7FFE
0001
0000
7FFFA
7FFF9
00006
00005
7FFFC
7FFFB
7FFFE
7FFFD
7FFFF
00001
00000
00003
00002
00004
0x7FFFF000 - 0x7FFFFFFF
0x7FFFE000 - 0x7FFFEFFF
0x7FFFD000 - 0x7FFFDFFF
0x7FFFC000 - 0x7FFFCFFF
0x7FFFB000 - 0x7FFFBFFF
0x7FFFA000 - 0x7FFFAFFF
0x00005000 - 0x00005FFF
0x00003000 - 0x00003FFF
0x00001000 - 0x00001FFF
0x7FFF9000 - 0x7FFF9FFF
0x00006000 - 0x00006FFF
0x00004000 - 0x00004FFF
0x00002000 - 0x00002FFF
0x00000000 - 0x00000FFF
Figure 8.21 Physical and virtual
pages
Chapt
Example 8.13 VIRTUAL ADDRESS TO PHYSICAL ADDRESS
TRANSLATION
Find the physical address of virtual address 0x247C using the virtual memory
system shown in Figure 8.21.
Solution: The 12-bit page offset (0x47C) requires no translation. The remaining
19 bits of the virtual address give the virtual page number, so virtual address
0x247C is found in virtual page 0x2. In Figure 8.21, virtual page 0x2 maps to
physical page 0x7FFF. Thus, virtual address 0x247C maps to physical address
0x7FFF47C.
8.4.2 The Page Table
The processor uses a page table to translate virtual addresses to physical
addresses. Recall that the page table contains an entry for each virtual
page. This entry contains a physical page number and a valid bit. If the
valid bit is 1, the virtual page maps to the physical page specified in the
entry. Otherwise, the virtual page is found on disk.
Because the page table is so large, it is stored in physical memory.
Let us assume for now that it is stored as a contiguous array, as shown
in Figure 8.23. This page table contains the mapping of the memory
system of Figure 8.21. The page table is indexed with the virtual page
number (VPN). For example, entry 5 specifies that virtual page 5 maps
to physical page 1. Entry 6 is invalid (V  0), so virtual page 6 is
located on disk.
Example 8.14 USING THE PAGE TABLE TO PERFORM ADDRESS
TRANSLATION
Find the physical address of virtual address 0x247C using the page table shown
in Figure 8.23.
Solution: Figure 8.24 shows the virtual address to physical address translation
for virtual address 0x247C. The 12-bit page offset requires no translation. The
remaining 19 bits of the virtual address are the virtual page number, 0x2, and
488 CHAPTER EIGHT Memory Systems
PPN Page Offset
11 10 9 ... 2 1 0
VPN Page Offset
Virtual Address
Physical Address
30 29 28 ... 14 13 12
26 25 24 ... 13 12 11 10 9 ... 2 1 0
19
15
12 Translation
Figure 8.22 Translation from
virtual address to physical
address
Page Table
Virtual
Page
Number
7FFFA
00006
00005
7FFFC
7FFFB
7FFFE
7FFFD
7FFFF
00001
00000
00003
00002
00004
V
00007
Physical
Page
Number
0
0
1 0x0000
1 0x7FFE
0
0
0
0
1 0x0001
0
0
1 0x7FFF
0
0
Figure 8.23 The page table for
Figure 8.21
C
The page table can be stored anywhere in physical memory, at the
discretion of the OS. The processor typically uses a dedicated register,
called the page table register, to store the base address of the page table
in physical memory.
To perform a load or store, the processor must first translate the
virtual address to a physical address and then access the data at that
physical address. The processor extracts the virtual page number from
the virtual address and adds it to the page table register to find the
physical address of the page table entry. The processor then reads this
page table entry from physical memory to obtain the physical page
number. If the entry is valid, it merges this physical page number with
the page offset to create the physical address. Finally, it reads or
writes data at this physical address. Because the page table is stored in
physical memory, each load or store involves two physical memory
accesses.
8.4 Virtual Memory 489
0
0
1 0x0000
1 0x7FFE
0
0
0
0
1 0x0001
Page Table
0
0
1 0x7FFF
0
0
V
Virtual
Address 0x00002 47C
Hit
Physical
Page Number
19 12
15 12
Virtual
Page Number
Page
Offset
Physical
Address 0x7FFF 47C
Figure 8.24 Address translation
using the page table
give the index into the page table. The page table maps virtual page 0x2 to
physical page 0x7FFF. So, virtual address 0x247C maps to physical address
0x7FFF47C. The least significant 12 bits are the same in both the physical and
the virtual address.

8.4.3 The Translation Lookaside Buffer
Virtual memory would have a severe performance impact if it required
a page table read on every load or store, doubling the delay of loads
and stores. Fortunately, page table accesses have great temporal locality. The temporal and spatial locality of data accesses and the large
page size mean that many consecutive loads or stores are likely to reference the same page. Therefore, if the processor remembers the last
page table entry that it read, it can probably reuse this translation
without rereading the page table. In general, the processor can keep
the last several page table entries in a small cache called a translation
lookaside buffer (TLB). The processor “looks aside” to find the translation in the TLB before having to access the page table in physical
memory. In real programs, the vast majority of accesses hit in the
TLB, avoiding the time-consuming page table reads from physical
memory.
A TLB is organized as a fully associative cache and typically holds
16 to 512 entries. Each TLB entry holds a virtual page number and its
corresponding physical page number. The TLB is accessed using the
virtual page number. If the TLB hits, it returns the corresponding
physical page number. Otherwise, the processor must read the page
table in physical memory. The TLB is designed to be small enough
that it can be accessed in less than one cycle. Even so, TLBs typically
have a hit rate of greater than 99%. The TLB decreases the number of
memory accesses required for most load or store instructions from
two to one.
Example 8.15 USING THE TLB TO PERFORM ADDRESS TRANSLATION
Consider the virtual memory system of Figure 8.21. Use a two-entry TLB or
explain why a page table access is necessary to translate virtual addresses
0x247C and 0x5FB0 to physical addresses. Suppose the TLB currently holds
valid translations of virtual pages 0x2 and 0x7FFFD.
Solution: Figure 8.25 shows the two-entry TLB with the request for virtual
address 0x247C. The TLB receives the virtual page number of the incoming
address, 0x2, and compares it to the virtual page number of each entry. Entry
0 matches and is valid, so the request hits. The translated physical address is
the physical page number of the matching entry, 0x7FFF, concatenated with
the page offset of the virtual address. As always, the page offset requires no
translation.
The request for virtual address 0x5FB0 misses in the TLB. So, the request is forwarded to the page table for translation.
490 CHAPTER EIGHT Memory Systems

8.4.4 Memory Protection
So far this section has focused on using virtual memory to provide a
fast, inexpensive, large memory. An equally important reason to use virtual memory is to provide protection between concurrently running
programs.
As you probably know, modern computers typically run several programs or processes at the same time. All of the programs are simultaneously present in physical memory. In a well-designed computer system,
the programs should be protected from each other so that no program
can crash or hijack another program. Specifically, no program should be
able to access another program’s memory without permission. This is
called memory protection.
Virtual memory systems provide memory protection by giving each
program its own virtual address space. Each program can use as much
memory as it wants in that virtual address space, but only a portion of the
virtual address space is in physical memory at any given time. Each program can use its entire virtual address space without having to worry
about where other programs are physically located. However, a program
can access only those physical pages that are mapped in its page table. In
this way, a program cannot accidentally or maliciously access another program’s physical pages, because they are not mapped in its page table. In
some cases, multiple programs access common instructions or data. The
operating system adds control bits to each page table entry to determine
which programs, if any, can write to the shared physical pages.
8.4 Virtual Memory 491
Hit1
V
=
15 15
15
=
Hit
1
0
Hit 1 0
Hit
19 19
19
Virtual
Page Number
Physical
Page Number
Entry 1
1 0x7FFFD 0x0000 1 0x00002 0x7FFF
Virtual
Address 0x00002 47C
19 12
Virtual
Page Number
Page
Offset
V
Virtual
Page Number
Physical
Page Number
Entry 0
12 Physical
Address 0x7FFF 47C
TLB
Figure 8.25 Address translation
using a two-entry TLB

8.4.5 Replacement Policies*
Virtual memory systems use write-back and an approximate least recently
used (LRU) replacement policy. A write-through policy, where each write
to physical memory initiates a write to disk, would be impractical. Store
instructions would operate at the speed of the disk instead of the speed of
the processor (milliseconds instead of nanoseconds). Under the writeback policy, the physical page is written back to disk only when it is
evicted from physical memory. Writing the physical page back to disk and
reloading it with a different virtual page is called swapping, so the disk in
a virtual memory system is sometimes called swap space. The processor
swaps out one of the least recently used physical pages when a page fault
occurs, then replaces that page with the missing virtual page. To support
these replacement policies, each page table entry contains two additional
status bits: a dirty bit, D, and a use bit, U.
The dirty bit is 1 if any store instructions have changed the physical
page since it was read from disk. When a physical page is swapped out,
it needs to be written back to disk only if its dirty bit is 1; otherwise, the
disk already holds an exact copy of the page.
The use bit is 1 if the physical page has been accessed recently. As in
a cache system, exact LRU replacement would be impractically complicated. Instead, the OS approximates LRU replacement by periodically
resetting all the use bits in the page table. When a page is accessed, its
use bit is set to 1. Upon a page fault, the OS finds a page with U  0 to
swap out of physical memory. Thus, it does not necessarily replace the
least recently used page, just one of the least recently used pages.
8.4.6 Multilevel Page Tables*
Page tables can occupy a large amount of physical memory. For example, the page table from the previous sections for a 2 GB virtual memory
with 4 KB pages would need 219 entries. If each entry is 4 bytes, the page
table is 219  22 bytes  221 bytes  2 MB.
To conserve physical memory, page tables can be broken up into multiple (usually two) levels. The first-level page table is always kept in physical memory. It indicates where small second-level page tables are stored
in virtual memory. The second-level page tables each contain the actual
translations for a range of virtual pages. If a particular range of translations is not actively used, the corresponding second-level page table can
be swapped out to the hard disk so it does not waste physical memory.
In a two-level page table, the virtual page number is split into two
parts: the page table number and the page table offset, as shown in
Figure 8.26. The page table number indexes the first-level page table,
which must reside in physical memory. The first-level page table entry
gives the base address of the second-level page table or indicates that
492 CHAPTER EIGHT Memory Systems
Cha
it must be fetched from disk when V is 0. The page table offset indexes
the second-level page table. The remaining 12 bits of the virtual address
are the page offset, as before, for a page size of 212  4 KB.
In Figure 8.26 the 19-bit virtual page number is broken into 9 and 10
bits, to indicate the page table number and the page table offset, respectively. Thus, the first-level page table has 29  512 entries. Each of these
512 second-level page tables has 210  1 K entries. If each of the first- and
second-level page table entries is 32 bits (4 bytes) and only two secondlevel page tables are present in physical memory at once, the hierarchical
page table uses only (512  4 bytes)  2  (1 K  4 bytes)  10 KB of
physical memory. The two-level page table requires a fraction of the physical memory needed to store the entire page table (2 MB). The drawback
of a two-level page table is that it adds yet another memory access for
translation when the TLB misses.
Example 8.16 USING A MULTILEVEL PAGE TABLE FOR ADDRESS
TRANSLATION
Figure 8.27 shows the possible contents of the two-level page table from Figure
8.26. The contents of only one second-level page table are shown. Using this
two-level page table, describe what happens on an access to virtual address
0x003FEFB0.
8.4 Virtual Memory 493
First-Level
Page Table
Page Table
Address
210 = 1K entries
29 = 512 entries
Page Table
Number
Page Table
Offset Virtual
Address
V
9
Physical Page
V Number
10
Second-Level
Page Tables
Page
Offset
Figure 8.26 Hierarchical page
tables
Chap
8.5 MEMORY-MAPPED I/O*
Processors also use the memory interface to communicate with
input/output (I/O) devices such as keyboards, monitors, and printers.
A processor accesses an I/O device using the address and data busses in
the same way that it accesses memory.
A portion of the address space is dedicated to I/O devices rather
than memory. For example, suppose that addresses in the range
494 CHAPTER EIGHT Memory Systems
Page Table
Address
Page Table
Number
Page Table
Offset Virtual
Address
V
9
Physical Page
V Number
10
Page
Offset
0
0
0
1 0x40000
Valid1
1 0x2375000
First-Level Page Table
Second-Level Page Tables
0x0 3FE FB0
Valid2 15 12
Physical
Address 0x23F1 FB0
12
0
1 0x7FFE
0
0
0
0
1 0x0073
0
0
1 0x72FC
0
0
0
1 0x00C1
1 0x1003
1 0x23F1
Figure 8.27 Address translation
using a two-level page table
Solution: As always, only the virtual page number requires translation. The most
significant nine bits of the virtual address, 0x0, give the page table number, the
index into the first-level page table. The first-level page table at entry 0x0 indicates that the second-level page table is resident in memory (V  1) and its physical address is 0x2375000.
The next ten bits of the virtual address, 0x3FE, are the page table offset, which
gives the index into the second-level page table. Entry 0 is at the bottom of the
second-level page table, and entry 0x3FF is at the top. Entry 0x3FE in the secondlevel page table indicates that the virtual page is resident in physical memory
(V  1) and that the physical page number is 0x23F1. The physical page number
is concatenated with the page offset to form the physical address, 0x23F1FB0.
Ch
0xFFFF0000 to 0xFFFFFFFF are used for I/O. Recall from Section 6.6.1
that these addresses are in a reserved portion of the memory map. Each
I/O device is assigned one or more memory addresses in this range. A
store to the specified address sends data to the device. A load receives
data from the device. This method of communicating with I/O devices is
called memory-mapped I/O.
In a system with memory-mapped I/O, a load or store may access
either memory or an I/O device. Figure 8.28 shows the hardware needed to
support two memory-mapped I/O devices. An address decoder determines
which device communicates with the processor. It uses the Address and
MemWrite signals to generate control signals for the rest of the hardware.
The ReadData multiplexer selects between memory and the various I/O
devices. Write-enabled registers hold the values written to the I/O devices.
Example 8.17 COMMUNICATING WITH I/O DEVICES
Suppose I/O Device 1 in Figure 8.28 is assigned the memory address 0xFFFFFFF4.
Show the MIPS assembly code for writing the value 7 to I/O Device 1 and for
reading the output value from I/O Device 1.
Solution: The following MIPS assembly code writes the value 7 to I/O Device 1.2
addi $t0, $0, 7
sw $t0, 0xFFF4($0)
The address decoder asserts WE1 because the address is 0xFFFFFFF4 and
MemWrite is TRUE. The value on the WriteData bus, 7, is written into the register connected to the input pins of I/O Device 1.
8.5 Memory-Mapped I/O 495
Processor Memory Address
MemWrite
WriteData
I/O ReadData
Device 1
I/O
Device 2
CLK
EN
EN
Address Decoder WE1 WEM
RDsel1:0
WE2
WE
CLK
00
01
10
CLK
Figure 8.28 Support hardware
for memory-mapped I/O
Some architectures, notably
IA-32, use specialized instructions instead of memorymapped I/O to communicate
with I/O devices. These
instructions are of the following form, where devicel and
device2 are the unique ID of
the peripheral device:
lwio $t0, device1
swio $t0, device2
This type of communication
with I/O devices is called programmed I/O.
2 Recall that the 16-bit immediate 0xFFF4 is sign-extended to the 32-bit value
0xFFFFFFF4.

To read from I/O Device 1, the processor performs the following MIPS assembly
code.
lw $t1, 0xFFF4($0)
The address decoder sets RDsel1:0 to 01, because it detects the address
0xFFFFFFF4 and MemWrite is FALSE. The output of I/O Device 1 passes
through the multiplexer onto the ReadData bus and is loaded into $t1 in the
processor.
Software that communicates with an I/O device is called a device
driver. You have probably downloaded or installed device drivers for
your printer or other I/O device. Writing a device driver requires detailed
knowledge about the I/O device hardware. Other programs call functions in the device driver to access the device without having to understand the low-level device hardware.
To illustrate memory-mapped I/O hardware and software, the rest of
this section describes interfacing a commercial speech synthesizer chip to
a MIPS processor.
Speech Synthesizer Hardware
The Radio Shack SP0256 speech synthesizer chip generates robot-like
speech. Words are composed of one or more allophones, the fundamental
units of sound. For example, the word “hello” uses five allophones represented by the following symbols in the SP0256 speech chip: HH1 EH LL
AX OW. The speech synthesizer uses 6-bit codes to represent 64 different
allophones that appear in the English language. For example, the five
allophones for the word “hello” correspond to the hexadecimal values
0x1B, 0x07, 0x2D, 0x0F, 0x20, respectively. The processor sends a series
of allophones to the speech synthesizer, which drives a speaker to blabber
the sounds.
Figure 8.29 shows the pinout of the SP0256 speech chip. The I/O
pins highlighted in blue are used to interface with the MIPS processor to
produce speech. Pins A6:1 receive the 6-bit allophone encoding from the
processor. The allophone sound is produced on the Digital Out pin. The
Digital Out signal is first amplified and then sent to a speaker. The other
two highlighted pins, SBY and
_____
ALD, are status and control pins. When
the SBY output is 1, the speech chip is standing by and is ready to
receive a new allophone. On the falling edge of the address load input _____
ALD, the speech chip reads the allophone specified by A6:1. Other pins,
such as power and ground (VDD and VSS) and the clock (OSC1), must
be connected as shown but are not driven by the processor.
Figure 8.30 shows the speech synthesizer interfaced to the MIPS
processor. The processor uses three memory-mapped I/O addresses to
communicate with the speech synthesizer. We arbitrarily have chosen
496 CHAPTER EIGHT Memory Systems
See www.speechchips.com
for more information about
the SP0256 and the allophone
encodings.

that the A6:1 port is mapped to address 0xFFFFFF00, _____
ALD to
0xFFFFFF04, and SBY to 0xFFFFFF08. Although the WriteData bus is
32 bits, only the least significant 6 bits are used for A6:1, and the least
significant bit is used for
_____
ALD; the other bits are ignored. Similarly,
SBY is read on the least significant bit of the ReadData bus; the other
bits are 0.
8.5 Memory-Mapped I/O 497
1
2
3
4
5
6
7
8
9
10
11
12
13
14
28
27
26
25
24
23
22
21
20
19
18
17
16
15
OSC 2
OSC 1
ROM Clock
SBY Reset
Digital Out
VD1
Test
Ser In
ALD
SE
A1
A2
A3
A4
VSS
Reset
ROM Disable
C1
C2
C3
VDD
SBY To
Processor From
Processor
From
Processor
From
Processor
LRQ
A8
A7
Ser Out
A6
A5
SPO256
3.12 MHz
A
Amplifier Speaker
Processor Memory Address
MemWrite
WriteData
ReadData
SP0256
CLK
EN
EN
Address Decoder
WE
CLK CLK
5:0
0
A6:1
ALD
SBY
0
1
31:1
WE2
WE1
WEM
RDsel
Figure 8.29 SPO256 speech synthesizer chip pinout
Figure 8.30 Hardware for
driving the SPO256 speech
synthesizer

Speech Synthesizer Device Driver
The device driver controls the speech synthesizer by sending an appropriate series of allophones over the memory-mapped I/O interface. It follows
the protocol expected by the SPO256 chip, given below:
 Set
_____
ALD to 1
 Wait until the chip asserts SBY to indicate that it is finished speaking
the previous allophone and is ready for the next
 Write a 6-bit allophone to A6:1
 Reset
_____
ALD to 0 to initiate speech
This sequence can be repeated for any number of allophones. The
MIPS assembly in Code Example 8.1 writes five allophones to the speech
chip. The allophone encodings are stored as 32-bit values in a five-entry
array starting at memory address 0x10000000.
The assembly code in Code Example 8.1 polls, or repeatedly checks,
the SBY signal to determine when the speech chip is ready to receive a
new allophone. The code functions correctly but wastes valuable processor cycles that could be used to perform useful work. Instead of polling,
the processor could use an interrupt connected to SBY. When SBY rises,
the processor stops what it is doing and jumps to code that handles the
interrupt. In the case of the speech synthesizer, the interrupt handler
498 CHAPTER EIGHT Memory Systems
init:
addi $t1, $0, 1 # $t1  1 (value to write to ALD
___
)
addi $t2, $0, 20 # $t2  array size * 4
lui $t3, 0x1000 # $t3  array base address
addi $t4, $0, 0 # $t4  0 (array index)
start:
sw $t1, 0xFF04($0) # ALD
___
=1
loop:
lw $t5, 0xFF08($0) # $t5  SBY
beq $0, $t5, loop # loop until SBY  1
add $t5, $t3, $t4 # $t5  address of allophone
lw $t5, 0($t5) # $t5  allophone
sw $t5, 0xFF00($0) # A6:1  allophone
sw $0, 0xFF04($0) # ALD
___
= 0 to initiate speech
addi $t4, $t4, 4 # increment array index
beq $t4, $t2, done # last allophone in array?
j start # repeat
done:
Code Example 8.1 SPEECH CHIP DEVICE DRIVER
Chapter 08.qxd
would send the next allophone, then let the processor resume what it was
doing before the interrupt. As described in Section 6.7.2, the processor
handles interrupts like any other exception.
8.6 REAL-WORLD PERSPECTIVE: IA-32 MEMORY
AND I/O SYSTEMS*
As processors get faster, they need ever more elaborate memory hierarchies
to keep a steady supply of data and instructions flowing. This section
describes the memory systems of IA-32 processors to illustrate the progression. Section 7.9 contained photographs of the processors, highlighting the
on-chip caches. IA-32 also has an unusual programmed I/O system that
differs from the more common memory-mapped I/O.
8.6.1 IA-32 Cache Systems
The 80386, initially produced in 1985, operated at 16 MHz. It lacked a
cache, so it directly accessed main memory for all instructions and data.
Depending on the speed of the memory, the processor might get an
immediate response, or it might have to pause for one or more cycles for
the memory to react. These cycles are called wait states, and they
increase the CPI of the processor. Microprocessor clock frequencies have
increased by at least 25% per year since then, whereas memory latency
has scarcely diminished. The delay from when the processor sends an
address to main memory until the memory returns the data can now
exceed 100 processor clock cycles. Therefore, caches with a low miss
rate are essential to good performance. Table 8.5 summarizes the evolution of cache systems on Intel IA-32 processors.
The 80486 introduced a unified write-through cache to hold both
instructions and data. Most high-performance computer systems also provided a larger second-level cache on the motherboard using commercially
available SRAM chips that were substantially faster than main memory.
The Pentium processor introduced separate instruction and data
caches to avoid contention during simultaneous requests for data and
instructions. The caches used a write-back policy, reducing the communication with main memory. Again, a larger second-level cache (typically
256–512 KB) was usually offered on the motherboard.
The P6 series of processors (Pentium Pro, Pentium II, and Pentium
III) were designed for much higher clock frequencies. The second-level
cache on the motherboard could not keep up, so it was moved closer to
the processor to improve its latency and throughput. The Pentium Pro
was packaged in a multichip module (MCM) containing both the processor chip and a second-level cache chip, as shown in Figure 8.31. Like the
Pentium, the processor had separate 8-KB level 1 instruction and data
8.6 Real-World Perspective: IA-32 Memory and I/O Systems 499

caches. However, these caches were nonblocking, so that the out-oforder processor could continue executing subsequent cache accesses even
if the cache missed a particular access and had to fetch data from main
memory. The second-level cache was 256 KB, 512 KB, or 1 MB in size
and could operate at the same speed as the processor. Unfortunately, the
MCM packaging proved too expensive for high-volume manufacturing.
Therefore, the Pentium II was sold in a lower-cost cartridge containing
the processor and the second-level cache. The level 1 caches were doubled in size to compensate for the fact that the second-level cache operated at half the processor’s speed. The Pentium III integrated a full-speed
second-level cache directly onto the same chip as the processor. A cache
on the same chip can operate at better latency and throughput, so it is
substantially more effective than an off-chip cache of the same size.
The Pentium 4 offered a nonblocking level 1 data cache. It switched
to a trace cache to store instructions after they had been decoded into
micro-ops, avoiding the delay of redecoding each time instructions were
fetched from the cache.
The Pentium M design was adapted from the Pentium III. It further
increased the level 1 caches to 32 KB each and featured a 1- to 2-MB level
2 cache. The Core Duo contains two modified Pentium M processors and
500 CHAPTER EIGHT Memory Systems
Table 8.5 Evolution of Intel IA-32 microprocessor memory systems
Frequency Level 1 Level 1
Processor Year (MHz) Data Cache Instruction Cache Level 2 Cache
80386 1985 16–25 none none none
80486 1989 25–100 8 KB unified none on chip
Pentium 1993 60–300 8 KB 8 KB none on chip
Pentium Pro 1995 150–200 8 KB 8 KB 256 KB–1 MB
on MCM
Pentium II 1997 233–450 16 KB 16 KB 256–512 KB
on cartridge
Pentium III 1999 450–1400 16 KB 16 KB 256–512 KB
on chip
Pentium 4 2001 1400–3730 8–16 KB 12 K op 256 KB–2 MB
trace cache on chip
Pentium M 2003 900–2130 32 KB 32 KB 1–2 MB
on chip
Core Duo 2005 1500–2160 32 KB/core 32 KB/core 2 MB shared
on chip

a shared 2-MB cache on one chip. The shared cache is used for communication between the processors: one can write data to the cache, and the
other can read it.
8.6.2 IA-32 Virtual Memory
IA-32 processors operate in either real mode or protected mode. Real
mode is backward compatible with the original 8086. It only uses
20 bits of addresses, limiting memory to 1 MB, and it does not allow
virtual memory.
Protected mode was introduced with the 80286 and extended to
32-bit addresses with the 80386. It supports virtual memory with 4-KB
pages. It also provides memory protection so that one program cannot
access the pages belonging to other programs. Hence, a buggy or malicious program cannot crash or corrupt other programs. All modern
operating systems now use protected mode.
A 32-bit address permits up to 4 GB of memory. Processors since
the Pentium Pro have bumped the memory capacity to 64 GB using a
8.6 Real-World Perspective: IA-32 Memory and I/O Systems 501
Figure 8.31 Pentium Pro
multichip module with processor
(left) and 256-KB cache (right)
in a pin grid array (PGA)
package (Courtesy Intel.)
Although memory protection
became available in the hardware in the early 1980s,
Microsoft Windows took
almost 15 years to take
advantage of the feature and
prevent bad programs from
crashing the entire computer.
Until the release of Windows
2000, consumer versions of
Windows were notoriously
unstable. The lag between
hardware features and software support can be
extremely long.

technique called physical address extension. Each process uses 32-bit
addresses. The virtual memory system maps these addresses onto a
larger 36-bit virtual memory space. It uses different page tables for
each process, so that each process can have its own address space of up
to 4 GB.
8.6.3 IA-32 Programmed I/O
Most architectures use memory-mapped I/O, described in Section 8.5,
in which programs access I/O devices by reading and writing memory
locations. IA-32 uses programmed I/O, in which special IN and OUT
instructions are used to read and write I/O devices. IA-32 defines 216
I/O ports. The IN instruction reads one, two, or four bytes from the
port specified by DX into AL, AX, or EAX. OUT is similar, but writes
the port.
Connecting a peripheral device to a programmed I/O system is similar to connecting it to a memory-mapped system. When accessing an I/O
port, the processor sends the port number rather than the memory
address on the 16 least significant bits of the address bus. The device
reads or writes data from the data bus. The major difference is that the
processor also produces an M/IO
___ signal. When M/IO
___  1, the processor
is accessing memory. When it is 0, the process is accessing one of the I/O
devices. The address decoder must also look at M/IO
___ to generate the
appropriate enables for main memory and for the I/O devices. I/O
devices can also send interrupts to the processor to indicate that they are
ready to communicate.
8.7 SUMMARY
Memory system organization is a major factor in determining computer
performance. Different memory technologies, such as DRAM, SRAM,
and hard disks, offer trade-offs in capacity, speed, and cost. This chapter
introduced cache and virtual memory organizations that use a hierarchy
of memories to approximate an ideal large, fast, inexpensive memory.
Main memory is typically built from DRAM, which is significantly
slower than the processor. A cache reduces access time by keeping commonly used data in fast SRAM. Virtual memory increases the memory
capacity by using a hard disk to store data that does not fit in the main
memory. Caches and virtual memory add complexity and hardware to a
computer system, but the benefits usually outweigh the costs. All modern personal computers use caches and virtual memory. Most processors
also use the memory interface to communicate with I/O devices. This is
called memory-mapped I/O. Programs use load and store operations to
access the I/O devices.
502 CHAPTER EIGHT Memory Systems
C
EPILOGUE
This chapter brings us to the end of our journey together into the realm
of digital systems. We hope this book has conveyed the beauty and thrill
of the art as well as the engineering knowledge. You have learned to
design combinational and sequential logic using schematics and hardware description languages. You are familiar with larger building blocks
such as multiplexers, ALUs, and memories. Computers are one of the
most fascinating applications of digital systems. You have learned how
to program a MIPS processor in its native assembly language and how to
build the processor and memory system using digital building blocks.
Throughout, you have seen the application of abstraction, discipline,
hierarchy, modularity, and regularity. With these techniques, we have
pieced together the puzzle of a microprocessor’s inner workings. From
cell phones to digital television to Mars rovers to medical imaging systems, our world is an increasingly digital place.
Imagine what Faustian bargain Charles Babbage would have made
to take a similar journey a century and a half ago. He merely aspired
to calculate mathematical tables with mechanical precision. Today’s
digital systems are yesterday’s science fiction. Might Dick Tracy have
listened to iTunes on his cell phone? Would Jules Verne have launched
a constellation of global positioning satellites into space? Could
Hippocrates have cured illness using high-resolution digital images of
the brain? But at the same time, George Orwell’s nightmare of ubiquitous government surveillance becomes closer to reality each day. And
rogue states develop nuclear weapons using laptop computers more
powerful than the room-sized supercomputers that simulated Cold War
bombs. The microprocessor revolution continues to accelerate. The
changes in the coming decades will surpass those of the past. You now
have the tools to design and build these new systems that will shape
our future. With your newfound power comes profound responsibility.
We hope that you will use it, not just for fun and riches, but also for
the benefit of humanity.
Epilogue 503

Exercises
Exercise 8.1 In less than one page, describe four everyday activities that exhibit
temporal or spatial locality. List two activities for each type of locality, and be
specific.
Exercise 8.2 In one paragraph, describe two short computer applications that
exhibit temporal and/or spatial locality. Describe how. Be specific.
Exercise 8.3 Come up with a sequence of addresses for which a direct mapped
cache with a size (capacity) of 16 words and block size of 4 words outperforms
a fully associative cache with least recently used (LRU) replacement that has the
same capacity and block size.
Exercise 8.4 Repeat Exercise 8.3 for the case when the fully associative cache
outperforms the direct mapped cache.
Exercise 8.5 Describe the trade-offs of increasing each of the following cache
parameters while keeping the others the same:
(a) block size
(b) associativity
(c) cache size
Exercise 8.6 Is the miss rate of a two-way set associative cache always, usually,
occasionally, or never better than that of a direct mapped cache of the same
capacity and block size? Explain.
Exercise 8.7 Each of the following statements pertains to the miss rate of caches.
Mark each statement as true or false. Briefly explain your reasoning; present a
counterexample if the statement is false.
(a) A two-way set associative cache always has a lower miss rate than a direct
mapped cache with the same block size and total capacity.
(b) A 16-KB direct mapped cache always has a lower miss rate than an 8-KB
direct mapped cache with the same block size.
(c) An instruction cache with a 32-byte block size usually has a lower miss rate
than an instruction cache with an 8-byte block size, given the same degree
of associativity and total capacity.
504 CHAPTER EIGHT Memory Systems

Exercise 8.8 A cache has the following parameters: b, block size given in
numbers of words; S, number of sets; N, number of ways; and A, number of
address bits.
(a) In terms of the parameters described, what is the cache capacity, C?
(b) In terms of the parameters described, what is the total number of bits
required to store the tags?
(c) What are S and N for a fully associative cache of capacity C words with
block size b?
(d) What is S for a direct mapped cache of size C words and block size b?
Exercise 8.9 A 16-word cache has the parameters given in Exercise 8.8. Consider
the following repeating sequence of lw addresses (given in hexadecimal):
40 44 48 4C 70 74 78 7C 80 84 88 8C 90 94 98 9C 0 4 8 C 10 14 18 1C 20
Assuming least recently used (LRU) replacement for associative caches,
determine the effective miss rate if the sequence is input to the following
caches, ignoring startup effects (i.e., compulsory misses).
(a) direct mapped cache, S  16, b  1 word
(b) fully associative cache, N  16, b  1 word
(c) two-way set associative cache, S  8, b  1 word
(d) direct mapped cache, S  8, b  2 words
Exercise 8.10 Suppose you are running a program with the following data access
pattern. The pattern is executed only once.
0x0, 0x8, 0x10, 0x18, 0x20, 0x28
(a) If you use a direct mapped cache with a cache size of 1 KB and a block size
of 8 bytes (2 words), how many sets are in the cache?
(b) With the same cache and block size as in part (a), what is the miss rate of
the direct mapped cache for the given memory access pattern?
(c) For the given memory access pattern, which of the following would decrease
the miss rate the most? (Cache capacity is kept constant.) Circle one.
(i) Increasing the degree of associativity to 2.
(ii) Increasing the block size to 16 bytes.
Exercises 505
Chapter 
(iii) Either (i) or (ii).
(iv) Neither (i) nor (ii).
Exercise 8.11 You are building an instruction cache for a MIPS processor. It has
a total capacity of 4C  2c2 bytes. It is N  2n-way set associative (N ≥ 8),
with a block size of b  2b bytes (b ≥ 8). Give your answers to the following
questions in terms of these parameters.
(a) Which bits of the address are used to select a word within a block?
(b) Which bits of the address are used to select the set within the cache?
(c) How many bits are in each tag?
(d) How many tag bits are in the entire cache?
Exercise 8.12 Consider a cache with the following parameters:
N (associativity)  2, b (block size)  2 words, W (word size)  32 bits,
C (cache size)  32 K words, A (address size)  32 bits. You need consider only
word addresses.
(a) Show the tag, set, block offset, and byte offset bits of the address. State how
many bits are needed for each field.
(b) What is the size of all the cache tags in bits?
(c) Suppose each cache block also has a valid bit (V) and a dirty bit (D). What
is the size of each cache set, including data, tag, and status bits?
(d) Design the cache using the building blocks in Figure 8.32 and a small
number of two-input logic gates. The cache design must include tag storage,
data storage, address comparison, data output selection, and any other parts
you feel are relevant. Note that the multiplexer and comparator blocks may
be any size (n or p bits wide, respectively), but the SRAM blocks must be
16 K  4 bits. Be sure to include a neatly labeled block diagram.
506 CHAPTER EIGHT Memory Systems
16K × 4
SRAM
14
4
=
p p
0
1
n
n
n
Figure 8.32 Building blocks
Chapter 
Exercise 8.13 You’ve joined a hot new Internet startup to build wrist watches
with a built-in pager and Web browser. It uses an embedded processor with a
multilevel cache scheme depicted in Figure 8.33. The processor includes a small
on-chip cache in addition to a large off-chip second-level cache. (Yes, the watch
weighs 3 pounds, but you should see it surf!)
Assume that the processor uses 32-bit physical addresses but accesses data only
on word boundaries. The caches have the characteristics given in Table 8.6. The
DRAM has an access time of tm and a size of 512 MB.
(a) For a given word in memory, what is the total number of locations in which
it might be found in the on-chip cache and in the second-level cache?
(b) What is the size, in bits, of each tag for the on-chip cache and the secondlevel cache?
(c) Give an expression for the average memory read access time. The caches are
accessed in sequence.
(d) Measurements show that, for a particular problem of interest, the on-chip
cache hit rate is 85% and the second-level cache hit rate is 90%. However,
when the on-chip cache is disabled, the second-level cache hit rate shoots up
to 98.5%. Give a brief explanation of this behavior.
Exercises 507
Figure 8.33 Computer system
CPU Level 1
Cache
Level 2
Cache
Main
Memory
Processor Chip
Table 8.6 Memory characteristics
Characteristic On-chip Cache Off-chip Cache
organization four-way set associative direct mapped
hit rate A B
access time ta tb
block size 16 bytes 16 bytes
number of blocks 512 256K

Exercise 8.14 This chapter described the least recently used (LRU) replacement
policy for multiway associative caches. Other, less common, replacement policies
include first-in-first-out (FIFO) and random policies. FIFO replacement evicts the
block that has been there the longest, regardless of how recently it was accessed.
Random replacement randomly picks a block to evict.
(a) Discuss the advantages and disadvantages of each of these replacement
policies.
(b) Describe a data access pattern for which FIFO would perform better than
LRU.
Exercise 8.15 You are building a computer with a hierarchical memory system
that consists of separate instruction and data caches followed by main memory.
You are using the MIPS multicycle processor from Figure 7.41 running at
1 GHz.
(a) Suppose the instruction cache is perfect (i.e., always hits) but the data cache
has a 5% miss rate. On a cache miss, the processor stalls for 60 ns to access
main memory, then resumes normal operation. Taking cache misses into
account, what is the average memory access time?
(b) How many clock cycles per instruction (CPI) on average are required for
load and store word instructions considering the non-ideal memory system?
(c) Consider the benchmark application of Example 7.7 that has 25% loads,
10% stores, 11% branches, 2% jumps, and 52% R-type instructions.3
Taking the non-ideal memory system into account, what is the average CPI
for this benchmark?
(d) Now suppose that the instruction cache is also non-ideal and has a 7% miss
rate. What is the average CPI for the benchmark in part (c)? Take into
account both instruction and data cache misses.
Exercise 8.16 If a computer uses 64-bit virtual addresses, how much virtual
memory can it access? Note that 240 bytes  1 terabyte, 250 bytes  1 petabyte,
and 260 bytes  1 exabyte.
Exercise 8.17 A supercomputer designer chooses to spend $1 million on DRAM
and the same amount on hard disks for virtual memory. Using the prices from
Figure 8.4, how much physical and virtual memory will the computer have?
How many bits of physical and virtual addresses are necessary to access this
memory?
508 CHAPTER EIGHT Memory Systems
3 Data from Patterson and Hennessy, Computer Organization and Design, 3rd Edition,
Morgan Kaufmann, 2005. Used with permission.
Cha
Exercise 8.18 Consider a virtual memory system that can address a total of 232
bytes. You have unlimited hard disk space, but are limited to only 8 MB of
semiconductor (physical) memory. Assume that virtual and physical pages are
each 4 KB in size.
(a) How many bits is the physical address?
(b) What is the maximum number of virtual pages in the system?
(c) How many physical pages are in the system?
(d) How many bits are the virtual and physical page numbers?
(e) Suppose that you come up with a direct mapped scheme that maps virtual
pages to physical pages. The mapping uses the least significant bits of the
virtual page number to determine the physical page number. How many
virtual pages are mapped to each physical page? Why is this “direct
mapping” a bad plan?
(f) Clearly, a more flexible and dynamic scheme for translating virtual
addresses into physical addresses is required than the one described in part
(d). Suppose you use a page table to store mappings (translations from
virtual page number to physical page number). How many page table
entries will the page table contain?
(g) Assume that, in addition to the physical page number, each page table entry
also contains some status information in the form of a valid bit (V) and a
dirty bit (D). How many bytes long is each page table entry? (Round up to
an integer number of bytes.)
(h) Sketch the layout of the page table. What is the total size of the page table
in bytes?
Exercise 8.19 You decide to speed up the virtual memory system of Exercise
8.18 by using a translation lookaside buffer (TLB). Suppose your memory
system has the characteristics shown in Table 8.7. The TLB and cache miss
rates indicate how often the requested entry is not found. The main memory
miss rate indicates how often page faults occur.
Exercises 509
Table 8.7 Memory characteristics
Memory Unit Access Time (Cycles) Miss Rate
TLB 1 0.05%
cache 1 2%
main memory 100 0.0003%
disk 1,000,000 0%

(a) What is the average memory access time of the virtual memory system
before and after adding the TLB? Assume that the page table is always
resident in physical memory and is never held in the data cache.
(b) If the TLB has 64 entries, how big (in bits) is the TLB? Give numbers for
data (physical page number), tag (virtual page number), and valid bits of
each entry. Show your work clearly.
(c) Sketch the TLB. Clearly label all fields and dimensions.
(d) What size SRAM would you need to build the TLB described in part (c)?
Give your answer in terms of depth  width.
Exercise 8.20 Suppose the MIPS multicycle processor described in Section 7.4
uses a virtual memory system.
(a) Sketch the location of the TLB in the multicycle processor schematic.
(b) Describe how adding a TLB affects processor performance.
Exercise 8.21 The virtual memory system you are designing uses a single-level
page table built from dedicated hardware (SRAM and associated logic).
It supports 25-bit virtual addresses, 22-bit physical addresses, and 216-byte
(64 KB) pages. Each page table entry contains a physical page number, a valid
bit (V) and a dirty bit (D).
(a) What is the total size of the page table, in bits?
(b) The operating system team proposes reducing the page size from 64 to
16 KB, but the hardware engineers on your team object on the grounds of
added hardware cost. Explain their objection.
(c) The page table is to be integrated on the processor chip, along with the
on-chip cache. The on-chip cache deals only with physical (not virtual)
addresses. Is it possible to access the appropriate set of the on-chip cache
concurrently with the page table access for a given memory access? Explain
briefly the relationship that is necessary for concurrent access to the cache
set and page table entry.
(d) Is it possible to perform the tag comparison in the on-chip cache
concurrently with the page table access for a given memory access?
Explain briefly.
Exercise 8.22 Describe a scenario in which the virtual memory system might
affect how an application is written. Be sure to include a discussion of how
the page size and physical memory size affect the performance of the
application.
510 CHAPTER EIGHT Memory Systems

Exercise 8.23 Suppose you own a personal computer (PC) that uses 32-bit
virtual addresses.
(a) What is the maximum amount of virtual memory space each program
can use?
(b) How does the size of your PC’s hard disk affect performance?
(c) How does the size of your PC’s physical memory affect performance?
Exercise 8.24 Use MIPS memory-mapped I/O to interact with a user. Each time
the user presses a button, a pattern of your choice displays on five light-emitting
diodes (LEDs). Suppose the input button is mapped to address 0xFFFFFF10 and
the LEDs are mapped to address 0xFFFFFF14. When the button is pushed, its
output is 1; otherwise it is 0.
(a) Write MIPS code to implement this functionality.
(b) Draw a schematic similar to Figure 8.30 for this memory-mapped I/O
system.
(c) Write HDL code to implement the address decoder for your memorymapped I/O system.
Exercise 8.25 Finite state machines (FSMs), like the ones you built in Chapter 3,
can also be implemented in software.
(a) Implement the traffic light FSM from Figure 3.25 using MIPS assembly
code. The inputs (TA and TB) are memory-mapped to bit 1 and bit 0,
respectively, of address 0xFFFFF000. The two 3-bit outputs (LA and LB)
are mapped to bits 0–2 and bits 3–5, respectively, of address 0xFFFFF004.
Assume one-hot output encodings for each light, LA and LB; red is 100,
yellow is 010, and green is 001.
(b) Draw a schematic similar to Figure 8.30 for this memory-mapped I/O
system.
(c) Write HDL code to implement the address decoder for your memorymapped I/O system.
Exercises 511

Interview Questions
The following exercises present questions that have been asked on
interviews.
Question 8.1 Explain the difference between direct mapped, set associative, and
fully associative caches. For each cache type, describe an application for which
that cache type will perform better than the other two.
Question 8.2 Explain how virtual memory systems work.
Question 8.3 Explain the advantages and disadvantages of using a virtual
memory system.
Question 8.4 Explain how cache performance might be affected by the virtual
page size of a memory system.
Question 8.5 Can addresses used for memory-mapped I/O be cached? Explain
why or why not.
512 CHAPTER EIGHT Memory Systems



A
A.1 Introduction
A.2 74xx Logic
A.3 Programmable Logic
A.4 Application-Specific
Integrated Circuits
A.5 Data Sheets
A.6 Logic Families
A.7 Packaging and Assembly
A.8 Transmission Lines
A.9 Economics
Digital System Implementation
A.1 INTRODUCTION
This appendix introduces practical issues in the design of digital systems. The material in this appendix is not necessary for understanding
the rest of the book. However, it seeks to demystify the process of
building real digital systems. Moreover, we believe that the best way to
understand digital systems is to build and debug them yourself in the
laboratory.
Digital systems are usually built using one or more chips. One strategy is to connect together chips containing individual logic gates or
larger elements such as arithmetic/logical units (ALUs) or memories.
Another is to use programmable logic, which contains generic arrays of
circuitry that can be programmed to perform specific logic functions. Yet
a third is to design a custom integrated circuit containing the specific
logic necessary for the system. These three strategies offer trade-offs in
cost, speed, power consumption, and design time that are explored in
the following sections. This appendix also examines the physical packaging and assembly of circuits, the transmission lines that connect the
chips, and the economics of digital systems.
A.2 74XX LOGIC
In the 1970s and 1980s, many digital systems were built from simple
chips, each containing a handful of logic gates. For example, the 7404
chip contains six NOT gates, the 7408 contains four AND gates, and the
7474 contains two flip-flops. These chips are collectively referred to as
74xx-series logic. They were sold by many manufacturers, typically for
10 to 25 cents per chip. These chips are now largely obsolete, but they
are still handy for simple digital systems or class projects, because they
are so inexpensive and easy to use. 74xx-series chips are commonly sold
in 14-pin dual inline packages (DIPs).
515
74LS04 inverter chip in a
14-pin dual inline package.
The part number is on the
first line. LS indicates the
logic family (see Section A.6).
The N suffix indicates a DIP
package. The large S is the
logo of the manufacturer,
Signetics. The bottom two
lines of gibberish are codes
indicating the batch in which
the chip was manufactured.

A.2.1 Logic Gates
Figure A.1 shows the pinout diagrams for a variety of popular 74xx-series
chips containing basic logic gates. These are sometimes called small-scale
integration (SSI) chips, because they are built from a few transistors. The
14-pin packages typically have a notch at the top or a dot on the top left
to indicate orientation. Pins are numbered starting with 1 in the upper left
and going counterclockwise around the package. The chips need to receive
power (VDD  5 V) and ground (GND  0 V) at pins 14 and 7, respectively. The number of logic gates on the chip is determined by the number
of pins. Note that pins 3 and 11 of the 7421 chip are not connected (NC)
to anything. The 7474 flip-flop has the usual D, CLK, and Q terminals. It
also has a complementary output, . Moreover, it receives asynchronous
set (also called preset, or PRE) and reset (also called clear, or CLR) signals. These are active low; in other words, the flop sets when ,
resets when , and operates normally when .
A.2.2 Other Functions
The 74xx series also includes somewhat more complex logic functions,
including those shown in Figures A.2 and A.3. These are called mediumscale integration (MSI) chips. Most use larger packages to accommodate
more inputs and outputs. Power and ground are still provided at the
upper right and lower left, respectively, of each chip. A general functional description is provided for each chip. See the manufacturer’s data
sheets for complete descriptions.
A.3 PROGRAMMABLE LOGIC
Programmable logic consists of arrays of circuitry that can be configured
to perform specific logic functions. We have already introduced three
forms of programmable logic: programmable read only memories
(PROMs), programmable logic arrays (PLAs), and field programmable
gate arrays (FPGAs). This section shows chip implementations for each of
these. Configuration of these chips may be performed by blowing on-chip
fuses to connect or disconnect circuit elements. This is called one-time
programmable (OTP) logic because, once a fuse is blown, it cannot be
restored. Alternatively, the configuration may be stored in a memory that
can be reprogrammed at will. Reprogrammable logic is convenient in the
laboratory, because the same chip can be reused during development.
A.3.1 PROMs
As discussed in Section 5.5.7, PROMs can be used as lookup tables. A
2N-word  M-bit PROM can be programmed to perform any combinational function of N inputs and M outputs. Design changes simply
CLR  0 PRE  CLR  1
PRE  0
Q
516 APPENDIX A Digital System Implementation
Appendi
A.3 Programmable Logic 517
Q D
Q
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7400 NAND
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1Y VDD
1A
1B
2Y
2A
2B
4Y
4B
4A
3Y
3B
3A
7402 NOR
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1Y
2A
2Y
3A
3Y
6A
6Y
5A
5Y
4A
4Y
7404 NOT
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
2A
2B
2C
2Y
1C
1Y
3C
3B
3A
3Y
7411 AND3
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7408 AND
1
2
3
4
5
6
7
14
13
12
11
10
9
8
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7486 XOR
GND
1
2
3
4
5
6
7
14
13
12
11
10
9
8
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7432 OR
GND
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
1C
1D
1Y
2D
2C
NC
2B
2A
2Y
7421 AND4
NC
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
VDD
1D
1Q
7474 FLOP
1CLK
1CLR
1PRE
1Q
2CLR
2D
2PRE
2Q
2Q
2CLK
reset
set
D Q
Q reset
set
Figure A.1 Common 74xx-series logic gates

518 APPENDIX A Digital System Implementation
CLR
CLK
D0
D1
D2
D3
ENP
GND
74161/163 Counter
VDD
Q0
RCO
LOAD
ENT
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
Q1
Q2
Q3
always @(posedge CLK) // 74163
 if (~CLRb) Q <= 4'b0000;
 else if (~LOADb) Q <= D;
 else if (ENP & ENT) Q <= Q+1;
assign RCO = (Q == 4'b1111) & ENT;
4-bit Counter
CLK: clock
Q3:0: counter output
D3:0: parallel input
CLRb: async reset (161)
sync reset (163)
LOADb: load Q from D
ENP, ENT: enables
RCO: ripple carry out
1G
S1
1D3
1D2
1D1
1D0
1Y
GND
74153 4 :1 Mux
VDD
2G
S0
2D3
2D2
2D1
2D0
2Y
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
always @(1Gb, S, 1D)
 if (1Gb) 1Y = 0;
 else 1Y = 1D[S];
always @(2Gb, S, 2D)
 if (2Gb) 2Y = 0;
 else 2Y = 2D[S];
Two 4:1 Multiplexers
D3:0: data
S1:0: select
Y: output
Gb: enable
A0
A1
A2
G2A
G2B
G1
Y7
GND
74138 3:8 Decoder
VDD
Y0
Y1
Y2
Y3
Y4
Y5
Y6
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
3:8 Decoder
A2:0: address
output
G1: active high enable
G2: active low enables
G1 G2A G2B A2:0 Y7:0
0 x x xxx 11111111
1 1 x xxx 11111111
1 0 1 xxx 11111111
1 0 0
1Y
S
1D0
1D1
2D0
2D1
2Y
GND
74157 2 :1 Mux
VDD
G
4D0
4D1
4Y
3D0
3D1
3Y
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
always @(*)
 if (~Gb) 1Y = 0;
 else 1Y = S ? 1D[1] : 1D[0];
 if (~Gb) 2Y = 0;
 else 2Y = S ? 2D[1] : 2D[0];
 if (~Gb) 3Y = 0;
 else 3Y = S ? 3D[1] : 3D[0];
 if (~Gb) 4Y = 0;
 else 4Y = S ? 4D[1] : 4D[0];
Four 2:1 Multiplexers
D1:0: data
S: select
Y: output
Gb: enable
1EN
1A0
2Y3
1A1
2Y2
GND
74244 Tristate Buffer
VDD
2EN
1Y0
2A3
1Y1
2A2
1Y2
2A1
1
2
3
4
5
6
7
8
20
19
18
17
16
15
14
13
9
10
12
11
1A2
2Y1
1A3
1Y0 1Y3
2A0
assign 1Y =
 1ENb ? 4'bzzzz : 1A;
assign 2Y =
2ENB ? 4'bzzzz : 2A;
8-bit Tristate Buffer
A3:0: input
Y3:0: output
ENb: enable
always @(posedge clk)
 if (~ENb) Q <= D;
EN
Q0
D0
D1
Q1
GND
74377 Register
VDD
Q7
D7
D6
Q6
Q5
D5
D4
1
2
3
4
5
6
7
8
20
19
18
17
16
15
14
13
9
10
12
11
Q2
D2
D3
Q3 Q4
CLK
8-bit Enableable Register
CLK: clock
D7:0: data
Q7:0: output
ENb: enable
Yb7:0:
000 11111110
1 0 0 001 11111101
1 0 0 010 11111011
1 0 0 011 11110111
1 0 0 100 11101111
1 0 0 101 11011111
1 0 0 110 10111111
1 0 0 111 01111111
Figure A.2 Medium-scale integration chips
Note: Verilog variable names cannot start with numbers, but the names in the example code in Figure A.2
are chosen to match the manufacturer’s data sheet.

A.3 Programmable Logic 519
4-bit ALU
A3:0, B3:0 :
Y3:0 : output
F3:0 : function select
M : mode select
Cbn : carry in
Cbnplus4 : carry out
AeqB : equality
(in some modes)
X,Y : carry lookahead
adder outputs
B0
A0
S3
S2
S1
S0
Cn
M
F0
F1
F2
GND
74181 ALU
VDD
A1
B1
A2
B2
A3
B3
Y
Cn+4
X
A=B
F3
1
2
3
4
5
6
7
8
9
10
11
12
24
23
22
21
20
19
18
17
16
15
14
13
D1
D2
LT
RBO
RBI
D3
D0
GND
7447 7-Segment
Decoder
VDD
f
g
a
b
c
d
e
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
7-segment Display Decoder
D3:0 : data
a...f : segments
(low = ON)
LTb: light test
RBIb: ripple blanking in
RBOb: ripple blanking out
RBO LT RBI D3:0 a b c d e f g
0 x x x 1 1 1 1 1 1 1
1 0 x x 0 0 0 0 0 0 0
x 1 0 0000 1 1 1 1 1 1 1
1 1 1 0000 0 0 0 0 0 0 1
1 1 1 0001 1 0 0 1 1 1 1
1 1 1 0010 0 0 1 0 0 1 0
1 1 1 0011 0 0 0 0 1 1 0
1 1 1 0100 1 0 0 1 1 0 0
1 1 1 0101 0 1 0 0 1 0 0
1 1 1 0110 1 1 0 0 0 0 0
1 1 1 0111 0 0 0 1 1 1 1
1 1 1 1000 0 0 0 0 0 0 0
1 1 1 1001 0 0 0 1 1 0 0
1 1 1 1010 1 1 1 0 0 1 0
1 1 1 1011 1 1 0 0 1 1 0
1 1 1 1100 1 0 1 1 1 0 0
1 1 1 1101 0 1 1 0 1 0 0
1 1 1 1110 0 0 0 1 1 1 1
1 1 1 1111 0 0 0 0 0 0 0
a
b
c d e
f g
B3
AltBin
AeqBin
AgtBin
AgtBout
AeqBout
AltBout
GND
7485 Comparator
VDD
A3
B2
A2
A1
B1
A0
B0
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
always @(*)
 if (A > B | (A == B & AgtBin)) begin
 AgtBout = 1; AeqBout = 0; AltBout = 0;
 end
 else if (A < B | (A == B & AltBin) begin
 AgtBout = 0; AeqBout = 0; AltBout = 1;
 end else begin
 AgtBout = 0; AeqBout = 1; AltBout = 0;
 end
4-bit Comparator
A3:0, B3:0 : data
relin : input relation
relout : output relation
always @(*)
 case (F)
 0000: Y = M ? ~A : A + ~Cbn;
 0001: Y = M ? ~(A | B) : A + B + ~Cbn;
 0010: Y = M ? (~A) & B : A + ~B + ~Cbn;
 0011: Y = M ? 4'b0000 : 4'b1111 + ~Cbn;
 0100: Y = M ? ~(A & B) : A + (A & ~B) + ~Cbn;
 0101: Y = M ? ~B : (A | B) + (A & ~B) + ~Cbn;
 0110: Y = M ? A ^ B : A - B - Cbn;
 0111: Y = M ? A & ~B : (A & ~B) - Cbn;
 1000: Y = M ? ~A + B : A + (A & B) + ~Cbn;
 1001: Y = M ? ~(A ^ B) : A + B + ~Cbn;
 1010: Y = M ? B : (A | ~B) + (A & B) + ~Cbn;
 1011: Y = M ? A & B : (A & B) + ~Cbn;
 1100: Y = M ? 1 : A + A + ~Cbn;
 1101: Y = M ? A | ~B : (A | B) + A + ~Cbn;
 1110: Y = M ? A | B : (A | ~B) + A + ~Cbn;
 1111: Y = M ? A : A - Cbn;
 endcase
inputs
Figure A.3 More medium-scale integration (MSI) chips

involve replacing the contents of the PROM rather than rewiring connections between chips. Lookup tables are useful for small functions but
become prohibitively expensive as the number of inputs grows.
For example, the classic 2764 8-KB (64-Kb) erasable PROM
(EPROM) is shown in Figure A.4. The EPROM has 13 address lines to
specify one of the 8K words and 8 data lines to read the byte of data at
that word. The chip enable and output enable must both be asserted for
data to be read. The maximum propagation delay is 200 ps. In normal
operation, and VPP is not used. The EPROM is usually programmed on a special programmer that sets , applies 13 V to
VPP, and uses a special sequence of inputs to configure the memory.
Modern PROMs are similar in concept but have much larger capacities and more pins. Flash memory is the cheapest type of PROM, selling
for about $30 per gigabyte in 2006. Prices have historically declined by
30 to 40% per year.
A.3.2 PLAs
As discussed in Section 5.6.1, PLAs contain AND and OR planes to
compute any combinational function written in sum-of-products form.
The AND and OR planes can be programmed using the same techniques
for PROMs. A PLA has two columns for each input and one column for
each output. It has one row for each minterm. This organization is more
efficient than a PROM for many functions, but the array still grows
excessively large for functions with numerous I/Os and minterms.
Many different manufacturers have extended the basic PLA concept
to build programmable logic devices (PLDs) that include registers.
PGM  0
PGM  1
520 APPENDIX A Digital System Implementation
A6
VPP
A12
A7
A5
A4
A3
A2
VDD
PGM
NC
A8
A9
A11
OE
A10
1
2
3
4
5
6
7
8
28
27
26
25
24
23
22
21
9
10
11
12
13
14
20
19
18
17
16
15
A0
A1
D0
D1
D2
GND
CE
D7
D6
D5
D4
D3
assign D = (~CEb & ~OEb) ? ROM[A]
 : 8'bZ;
8 KB EPROM
A12:0: address input
D7:0 : data output
CEb : chip enable
OEb : output enable
PGMb : program
VPP : program voltage
NC : no connection
Figure A.4 2764 8KB EPROM
Ap
The 22V10 is one of the most popular classic PLDs. It has 12 dedicated
input pins and 10 outputs. The outputs can come directly from the PLA
or from clocked registers on the chip. The outputs can also be fed back
into the PLA. Thus, the 22V10 can directly implement FSMs with up to
12 inputs, 10 outputs, and 10 bits of state. The 22V10 costs about $2 in
quantities of 100. PLDs have been rendered mostly obsolete by the rapid
improvements in capacity and cost of FPGAs.
A.3.3 FPGAs
As discussed in Section 5.6.2, FPGAs consist of arrays of configurable
logic blocks (CLBs) connected together with programmable wires. The
CLBs contain small lookup tables and flip-flops. FPGAs scale gracefully
to extremely large capacities, with thousands of lookup tables. Xilinx
and Altera are two of the leading FPGA manufacturers.
Lookup tables and programmable wires are flexible enough to
implement any logic function. However, they are an order of magnitude
less efficient in speed and cost (chip area) than hard-wired versions of
the same functions. Thus, FPGAs often include specialized blocks, such
as memories, multipliers, and even entire microprocessors.
Figure A.5 shows the design process for a digital system on an
FPGA. The design is usually specified with a hardware description language (HDL), although some FPGA tools also support schematics. The
design is then simulated. Inputs are applied and compared against
expected outputs to verify that the logic is correct. Usually some debugging is required. Next, logic synthesis converts the HDL into Boolean
functions. Good synthesis tools produce a schematic of the functions,
and the prudent designer examines these schematics, as well as any
warnings produced during synthesis, to ensure that the desired logic
was produced. Sometimes sloppy coding leads to circuits that are much
larger than intended or to circuits with asynchronous logic. When the
synthesis results are good, the FPGA tool maps the functions onto the
CLBs of a specific chip. The place and route tool determines which
functions go in which lookup tables and how they are wired together.
Wire delay increases with length, so critical circuits should be placed
close together. If the design is too big to fit on the chip, it must be
reengineered. Timing analysis compares the timing constraints (e.g., an
intended clock speed of 100 MHz) against the actual circuit delays and
reports any errors. If the logic is too slow, it may have to be redesigned
or pipelined differently. When the design is correct, a file is generated
specifying the contents of all the CLBs and the programming of all
the wires on the FPGA. Many FPGAs store this configuration information in static RAM that must be reloaded each time the FPGA is turned
on. The FPGA can download this information from a computer in the
A.3 Programmable Logic 521
Design Entry
Synthesis
Logic Verification
Mapping
Timing Analysis
Configure FPGA
Place and Route
Debug
Debug
Too big
Too slow
Figure A.5 FPGA design flow

laboratory, or can read it from a nonvolatile ROM when power is first
applied.
Example A.1 FPGA TIMING ANALYSIS
Alyssa P. Hacker is using an FPGA to implement an M&M sorter with a color
sensor and motors to put red candy in one jar and green candy in another. Her
design is implemented as an FSM, and she is using a Spartan XC3S200 FPGA, a
chip from the Spartan 3 series family. According to the data sheet, the FPGA has
the timing characteristics shown in Table A.1. Assume that the design is small
enough that wire delay is negligible.
Alyssa would like her FSM to run at 100 MHz. What is the maximum number
of CLBs on the critical path? What is the fastest speed at which her FSM could
possibly run?
SOLUTION: At 100 MHz, the cycle time, Tc, is 10 ns. Alyssa uses Equation 3.13 figure to out the minimum combinational propagation delay, tpd, at this cycle time:
tpd  10 ns  (0.72 ns  0.53 ns)  8.75 ns (A.1)
Alyssa’s FSM can use at most 14 consecutive CLBs (8.75/0.61) to implement the
next-state logic.
The fastest speed at which an FSM will run on a Spartan 3 FPGA is when it
is using a single CLB for the next state logic. The minimum cycle time is
Tc  0.61 ns  0.72 ns  0.53 ns  1.86 ns (A.2)
Therefore, the maximum frequency is 538 MHz.
522 APPENDIX A Digital System Implementation
Table A.1 Spartan 3 XC3S200 timing
name value (ns)
tpcq 0.72
tsetup 0.53
thold 0
tpd (per CLB) 0.61
tskew 0
Xilinx advertises the XC3S100E FPGA with 1728 lookup tables and
flip-flops for $2 in quantities of 500,000 in 2006. In more modest quantities, medium-sized FPGAs typically cost about $10, and the largest
Ap
FPGAs cost hundreds or even thousands of dollars. The cost has
declined at approximately 30% per year, so FPGAs are becoming
extremely popular.
A.4 APPLICATION-SPECIFIC INTEGRATED CIRCUITS
Application-specific integrated circuits (ASICs) are chips designed for a
particular purpose. Graphics accelerators, network interface chips, and
cell phone chips are common examples of ASICS. The ASIC designer
places transistors to form logic gates and wires the gates together.
Because the ASIC is hardwired for a specific function, it is typically several times faster than an FPGA and occupies an order of magnitude less
chip area (and hence cost) than an FPGA with the same function.
However, the masks specifying where transistors and wires are located
on the chip cost hundreds of thousands of dollars to produce. The fabrication process usually requires 6 to 12 weeks to manufacture, package,
and test the ASICs. If errors are discovered after the ASIC is manufactured, the designer must correct the problem, generate new masks, and
wait for another batch of chips to be fabricated. Hence, ASICs are suitable only for products that will be produced in large quantities and
whose function is well defined in advance.
Figure A.6 shows the ASIC design process, which is similar to the
FPGA design process of Figure A.5. Logic verification is especially
important because correction of errors after the masks are produced is
expensive. Synthesis produces a netlist consisting of logic gates and connections between the gates; the gates in this netlist are placed, and the
wires are routed between gates. When the design is satisfactory, masks
are generated and used to fabricate the ASIC. A single speck of dust can
ruin an ASIC, so the chips must be tested after fabrication. The fraction
of manufactured chips that work is called the yield; it is typically 50 to
90%, depending on the size of the chip and the maturity of the
manufacturing process. Finally, the working chips are placed in packages, as will be discussed in Section A.7.
A.5 DATA SHEETS
Integrated circuit manufacturers publish data sheets that describe the
functions and performance of their chips. It is essential to read and
understand the data sheets. One of the leading sources of errors in digital systems comes from misunderstanding the operation of a chip.
Data sheets are usually available from the manufacturer’s Web
site. If you cannot locate the data sheet for a part and do not have clear
documentation from another source, don’t use the part. Some of the
entries in the data sheet may be cryptic. Often the manufacturer publishes data books containing data sheets for many related parts. The
A.5 Data Sheets 523
Design Entry
Synthesis
Logic Verification
Timing Analysis
Generate Masks
Place and Route
Debug
Debug
Too big
Too slow
Fabricate ASIC
Test ASIC
Package ASIC
Defective
Figure A.6 ASIC design flow

beginning of the data book has additional explanatory information. This
information can usually be found on the Web with a careful search.
This section dissects the Texas Instruments (TI) data sheet for a
74HC04 inverter chip. The data sheet is relatively simple but illustrates
many of the major elements. TI still manufacturers a wide variety of
74xx-series chips. In the past, many other companies built these chips
too, but the market is consolidating as the sales decline.
Figure A.7 shows the first page of the data sheet. Some of the key sections are highlighted in blue. The title is SN54HC04, SN74HC04 HEX
INVERTERS. HEX INVERTERS means that the chip contains six inverters.
SN indicates that TI is the manufacturer. Other manufacture codes include
MC for Motorola and DM for National Semiconductor. You can generally
ignore these codes, because all of the manufacturers build compatible 74xxseries logic. HC is the logic family (high speed CMOS). The logic family
determines the speed and power consumption of the chip, but not the function. For example, the 7404, 74HC04, and 74LS04 chips all contain six
inverters, but they differ in performance and cost. Other logic families are
discussed in Section A.6. The 74xx chips operate across the commercial or
industrial temperature range (0 to 70 C or 40 to 85 C, respectively),
whereas the 54xx chips operate across the military temperature range
(55 to 125 C) and sell for a higher price but are otherwise compatible.
The 7404 is available in many different packages, and it is important to order the one you intended when you make a purchase. The
packages are distinguished by a suffix on the part number. N indicates a
plastic dual inline package (PDIP), which fits in a breadboard or can be
soldered in through-holes in a printed circuit board. Other packages are
discussed in Section A.7.
The function table shows that each gate inverts its input. If A is
HIGH (H), Y is LOW (L) and vice versa. The table is trivial in this case
but is more interesting for more complex chips.
Figure A.8 shows the second page of the data sheet. The logic diagram indicates that the chip contains inverters. The absolute maximum
section indicates conditions beyond which the chip could be destroyed.
In particular, the power supply voltage (VCC, also called VDD in this
book) should not exceed 7 V. The continuous output current should not
exceed 25 mA. The thermal resistance or impedance, 	JA, is used to calculate the temperature rise caused by the chip’s dissipating power. If the
ambient temperature in the vicinity of the chip is TA and the chip dissipates Pchip, then the temperature on the chip itself at its junction with
the package is
TJ  TA  Pchip 	JA (A.3)
For example, if a 7404 chip in a plastic DIP package is operating in a
hot box at 50 C and consumes 20 mW, the junction temperature will
524 APPENDIX A Digital System Implementation
A
A.5 Data Sheets 525
SCLS078D – DECEMBER 1982 – REVISED JULY 2003
POST OFFICE BOX 655303 • DALLAS, TEXAS 75265
Wide Operating Voltage Range of 2 V to 6 V
Outputs Can Drive Up To 10 LSTTL Loads
Low Power Consumption, 20-µA Max ICC
Typical tpd = 8 ns
±4-mA Output Drive at 5V
Low Input Current of 1 µA Max
1
2
3
4
5
6
7
14
13
12
11
10
9
8
1A
1Y
2A
2Y
3A
3Y
GND
VCC
6A
6Y
5A
5Y
4A
4Y
SN54HC04 . . . J OR W PACKAGE
SN74HC04 . . . D, N, NS, OR PW PACKAGE
(TOPVIEW)
3 2 1 20 19
9 10 11 12 13
4
5
6
7
8
18
17
16
15
14
6Y
NC
5A
NC
5Y
2A
NC
2Y
NC
3A
Y1
A1
CN
Y4
A4
VCC
A6
Y3
DNG
CN
SN54HC04 . . . FK PACKAGE
(TOPVIEW)
NC – No internal connection
description/ordering information
ORDERING INFORMATION
TA PACKAGE† ORDERABLE
PARTNUMBER
TOP-SIDE
MARKING
PDIP – N Tube of 25 SN74HC04N SN74HC04N
Tube of 50 SN74HC04D
SOIC – D
Reel of 2500 SN74HC04DR HC04
Reel of 250 SN74HC04DT –40°C to 85°C SOP – NS Reel of 2000 SN74HC04NSR HC04
Tube of 90 SN74HC04PW
TSSOP – PW Reel of 2000 SN74HC04PWR HC04
Reel of 250 SN74HC04PWT
CDIP – J Tube of 25 SNJ54HC04J SNJ54HC04J
–55°C to 125°C CFP – W Tube of 150 SNJ54HC04W SNJ54HC04W
LCCC – FK Tube of 55 SNJ54HC04FK SNJ54HC04FK
† Package drawings, standard packing quantities, thermal data, symbolization, and PCB design guidelines are
available at www.ti.com/sc/package.
FUNCTION TABLE
(each inverter)
INPUT
A
OUTPUT
Y
H L
L H
Please be aware that an important notice concerning availability, standard warranty, and use in critical applications of
Texas Instruments semiconductor products and disclaimers there to appears at the end of this data sheet.
PRODUCTION DATA information is current as of publication date. Copyright (c)2003, Texas Instruments Incorporated
Products conform to specifications per the terms of Texas Instruments
standard warranty. Production processing does not necessarily include
testing of all parameters.
On products compliant to MIL-PRF-38535, all parameters are tested
unless otherwise noted. On all other products, production
processing does not necessarily include testing of all parameters.
SN54HC04, SN74HC04
HEX INVERTERS
The ’HC04 devices contain six independent inverters. They perform the Boolean function Y = A
in positive logic.
Figure A.7 7404 data sheet page 1

526 APPENDIX A Digital System Implementation
SN54HC04, SN74HC04
HEX INVERTERS
SCLS078D – DECEMBER 1982 – REVISED JULY 2003
POST OFFICE BOX 655303 • DALLAS, TEXAS 75265
logic diagram (positive logic)
A Y
absolute maximum ratings over operating free-air temperature range (unless otherwise noted)†
Supply voltage range, VCC .......................................................... –0.5 V to 7 V
Input clamp current, IIK (VI
 < 0 or VI > VCC) (see Note 1) .................................... ±20 mA
Output clamp current, IOK (VO < 0 or VO > VCC) (see Note 1) ................................ ±20 mA
Continuous output current, IO (VO = 0 to VCC) .............................................. ±25 mA
Continuous current through VCC or GND ................................................... ±50 mA
Package thermal impedance, θJA ( egakcapD:)2etoNees ................................... 86° C/W
N ................................... egakcap 80° C/W
N egakcapS ................................. 76° C/W
P egakcapW ................................ 131° C/W
Storage temperature range, Tstg ................................................... –65° C to 150° C
† Stresses beyond those listed under “absolute maximum ratings” may cause permanent damage to the device. These are stress ratings only, and
functional operation of the device at these or any other conditions beyond those indicated under “recommended operating conditions” is not
implied. Exposure to absolute-maximum-rated conditions for extended periods may affect device reliability.
NOTES: 1. The input and output voltage ratings may be exceeded if the input and output current ratings are observed.
2. The package thermal impedance is calculated in accordance with JESD 51-7.
recommended operating conditions (see Note 3)
SN54HC04 SN74HC04 UNIT MIN NOM MAX MIN NOM MAX UNIT
VCC Supply voltage 2 5 6 2 5 6 V
VCC =2V 1.5 1.5
VIH High-level input voltage VCC = 4.5 V 3.15 3.15 V
VCC =6V 4.2 4.2
VCC =2V 0.5 0.5
VIL Low-level input voltage VCC = 4.5 V 1.35 1.35 V
VCC =6V 1.8 1.8
VI Input voltage 0 VCC 0 VCC V
VO Output voltage 0 VCC 0 VCC V
VCC =2V 1000 1000
∆t/∆v Input transition rise/fall time VCC = 4.5 V 500 500 ns
VCC =6V 400 400
TA Operating free-air temperature –55 125 –40 85 °C
NOTE 3: All unused inputs of the device must be held at VCC or GND to ensure proper device operation. Refer to the TI application report,
Implications of Slow or Floating CMOS Inputs, literature number SCBA004.
TEXAS
INSTRUMENTS
Figure A.8 7404 datasheet page 2

climb to 50

C
 0.02 W
 80
C/W
 51.6
C. Internal power dissipation is seldom important for 74xx-series chips, but it becomes important
for modern chips that dissipate tens of watts or more.
The recommended operating conditions define the environment in
which the chip should be used. Within these conditions, the chip
should meet specifications. These conditions are more stringent than
the absolute maximums. For example, the power supply voltage
should be between 2 and 6 V. The input logic levels for the HC logic
family depend on
VDD. Use the 4.5 V entries when
VDD
 5 V,
to allow for a 10% droop in the power supply caused by noise in the
system.
Figure A.9 shows the third page of the data sheet. The electrical
characteristics describe how the device performs when used within the
recommended operating conditions if the inputs are held constant.
For example, if
VCC
 5 V (and droops to 4.5 V) and the output current, IOH/IOL does not exceed 20 
A, VOH  4.4 V and VOL  0.1 V
in the worst case. If the output current increases, the output voltages
become less ideal, because the transistors on the chip struggle to provide the current. The HC logic family uses CMOS transistors that
draw very little current. The current into each input is guaranteed
to be less than 1000 nA and is typically only 0.1 nA at room temperature. The quiescent power supply current (IDD) drawn while the
chip is idle is less than 20

A. Each input has less than 10 pF of
capacitance.
The switching characteristics define how the device performs when
used within the recommended operating conditions if the inputs change.
The propagation delay, tpd, is measured from when the input passes
through 0.5
VCC to when the output passes through 0.5
VCC. If
VCC is
nominally 5 V and the chip drives a capacitance of less than 50 pF, the
propagation delay will not exceed 24 ns (and typically will be much
faster). Recall that each input may present 10 pF, so the chip cannot
drive more than five identical chips at full speed. Indeed, stray capacitance from the wires connecting chips cuts further into the useful load.
The transition time, also called the rise/fall time, is measured as the output transitions between 0.1 VCC and 0.9 VCC.
Recall from Section 1.8 that chips consume both static and
dynamic power. Static power is low for HC circuits. At 85
C, the
maximum quiescent supply current is 20

A. At 5 V, this gives a static
power consumption of 0.1 mW. The dynamic power depends on the
capacitance being driven and the switching frequency. The 7404 has an
internal power dissipation capacitance of 20 pF per inverter. If all six
inverters on the 7404 switch at 10 MHz and drive external loads of
25 pF, then the dynamic power given by Equation 1.4 is (6)(20 pF

25 pF)(5
2)(10 MHz)
 33.75 mW and the maximum total power is
33.85 mW.
12
A.5 Data Sheets 527
Appendi
528 APPENDIX A Digital System Implementation
SN54HC04, SN74HC04
HEX INVERTERS
SCLS078D – DECEMBER 1982 – REVISED JULY 2003
POST OFFICE BOX 655303 • DALLAS, TEXAS 75265
TEXAS
INSTRUMENTS
electrical characteristics over recommended operating free-air temperature range (unless
otherwise noted)
PARAMETER TEST CONDITIONS VCC
TA=25°C SN54HC04 SN74HC04 UNIT MIN TYP MAX MIN MAX MIN MAX UNIT
2 V 1.9 1.998 1.9 1.9
I
OH = –20 µA
I
OL = 20 µA
4.5V 4.4 4.499 4.4 4.4
VOH
VOL
VI =VIH or VIL
VI =VIH or VIL
VI =VCC or 0
VI =VCC or 0,
6 V 5.9 5.999 5.9 5.9 V
I
OH = –4 mA
I
OL = 4 mA
4.5V 3.98 4.3 3.7 3.84
I
OH = –5.2 mA
I
OL = 5.2 mA
6 V 5.48 5.8 5.2 5.34
2 V 0.002 0.1 0.1 0.1
4.5V 0.001 0.1 0.1 0.1
6 V 0.001 0.1 0.1 0.1 V
4.5V 0.17 0.26 0.4 0.33
6 V 0.15 0.26 0.4 0.33
I
I 6 V ±0.1 ±100 ±1000 ±1000 nA
I
CC I
O = 0 6 V 2 40 20 µA
Ci 2 Vto 6 V 3 10 10 10 pF
switching characteristics over recommended operating free-air temperature range, CL = 50 pF
(unless otherwise noted) (see Figure 1)
FROM TO VCC
TA = 25° C SN54HC04 SN74HC04 PARAMETER UNIT (INPUT) (OUTPUT) MIN TYP MAX MIN MAX MIN MAX UNIT
2 V 45 95 145 120
t
pd A Y 4.5V 9 19 29 24 ns
6 V 8 16 25 20
2 V 38 75 110 95
t
t Y 4.5V 8 15 22 19 ns
6 V 6 13 19 16
operating characteristics, TA = 25° C
PARAMETER TEST CONDITIONS TYP UNIT
Cpd Power dissipation capacitance per inverter No load 20 pF
Figure A.9 7404 datasheet page 3

A.6 LOGIC FAMILIES
The 74xx-series logic chips have been manufactured using many different
technologies, called logic families, that offer different speed, power, and
logic level trade-offs. Other chips are usually designed to be compatible
with some of these logic families. The original chips, such as the 7404,
were built using bipolar transistors in a technology called TransistorTransistor Logic (TTL). Newer technologies add one or more letters after
the 74 to indicate the logic family, such as 74LS04, 74HC04, or
74AHCT04. Table A.2 summarizes the most common 5-V logic families.
Advances in bipolar circuits and process technology led to the
Schottky (S) and Low-Power Schottky (LS) families. Both are faster than
TTL. Schottky draws more power, whereas Low-power Schottky draws
less. Advanced Schottky (AS) and Advanced Low-Power Schottky (ALS)
have improved speed and power compared to S and LS. Fast (F) logic is
faster and draws less power than AS. All of these families provide more
current for LOW outputs than for HIGH outputs and hence have asymmetric logic levels. They conform to the “TTL” logic levels: VIH  2 V,
VIL  0.8 V, VOH  2.4 V, and VOL  0.5 V.
A.6 Logic Families 529
Table A.2 Typical specifications for 5-V logic families
CMOS / TTL
Bipolar / TTL CMOS Compatible
Characteristic TTL S LS AS ALS F HC AHC HCT AHCT
tpd (ns) 22 9 12 7.5 10 6 21 7.5 30 7.7
VIH (V) 2 2 2 2 2 2 3.15 3.15 2 2
VIL (V) 0.8 0.8 0.8 0.8 0.8 0.8 1.35 1.35 0.8 0.8
VOH (V) 2.4 2.7 2.7 2.5 2.5 2.5 3.84 3.8 3.84 3.8
VOL (V) 0.4 0.5 0.5 0.5 0.5 0.5 0.33 0.44 0.33 0.44
IOH (mA) 0.4 1 0.4 2 0.4 1 4 8 4 8
IOL (mA) 16 20 8 20 8 20 4 8 4 8
IIL (mA) 1.6 2 0.4 0.5 0.1 0.6 0.001 0.001 0.001 0.001
IIH (mA) 0.04 0.05 0.02 0.02 0.02 0.02 0.001 0.001 0.001 0.001
IDD (mA) 33 54 6.6 26 4.2 15 0.02 0.02 0.02 0.02
Cpd (pF) n/a 20 12 20 14
cost* (US $) obsolete 0.57 0.29 0.53 0.33 0.20 0.15 0.15 0.15 0.15
* Per unit in quantities of 1000 for the 7408 from Texas Instruments in 2006
Ap
As CMOS circuits matured in the 1980s and 1990s, they became
popular because they draw very little power supply or input current.
The High Speed CMOS (HC) and Advanced High Speed CMOS
(AHC) families draw almost no static power. They also deliver
the same current for HIGH and LOW outputs. They conform to
the “CMOS” logic levels: VIH  3.15 V, VIL  1.35 V, VOH  3.8 V,
and VOL  0.44 V. Unfortunately, these levels are incompatible with
TTL circuits, because a TTL HIGH output of 2.4 V may not be recognized as a legal CMOS HIGH input. This motivates the use of High
Speed TTL-compatible CMOS (HCT) and Advanced High Speed TTLcompatible CMOS (AHCT), which accept TTL input logic levels and
generate valid CMOS output logic levels. These families are slightly
slower than their pure CMOS counterparts. All CMOS chips are sensitive to electrostatic discharge (ESD) caused by static electricity.
Ground yourself by touching a large metal object before handling
CMOS chips, lest you zap them.
The 74xx-series logic is inexpensive. The newer logic families are
often cheaper than the obsolete ones. The LS family is widely available
and robust and is a popular choice for laboratory or hobby projects that
have no special performance requirements.
The 5-V standard collapsed in the mid-1990s, when transistors
became too small to withstand the voltage. Moreover, lower voltage
offers lower power consumption. Now 3.3, 2.5, 1.8, 1.2, and even lower
voltages are commonly used. The plethora of voltages raises challenges in
communicating between chips with different power supplies. Table A.3
lists some of the low-voltage logic families. Not all 74xx parts are available in all of these logic families.
All of the low-voltage logic families use CMOS transistors, the
workhorse of modern integrated circuits. They operate over a wide
range of VDD, but the speed degrades at lower voltage. Low-Voltage
CMOS (LVC) logic and Advanced Low-Voltage CMOS (ALVC) logic
are commonly used at 3.3, 2.5, or 1.8 V. LVC withstands inputs up
to 5.5 V, so it can receive inputs from 5-V CMOS or TTL circuits.
Advanced Ultra-Low-Voltage CMOS (AUC) is commonly used at 2.5,
1.8, or 1.2 V and is exceptionally fast. Both ALVC and AUC withstand inputs up to 3.6 V, so they can receive inputs from 3.3-V
circuits.
FPGAs often offer separate voltage supplies for the internal logic,
called the core, and for the input/output (I/O) pins. As FPGAs have
advanced, the core voltage has dropped from 5 to 3.3, 2.5, 1.8, and 1.2 V
to save power and avoid damaging the very small transistors. FPGAs
have configurable I/Os that can operate at many different voltages, so as
to be compatible with the rest of the system.
530 APPENDIX A Digital System Implementation
Ap
A.7 PACKAGING AND ASSEMBLY
Integrated circuits are typically placed in packages made of plastic or
ceramic. The packages serve a number of functions, including connecting
the tiny metal I/O pads of the chip to larger pins in the package for ease
of connection, protecting the chip from physical damage, and spreading
the heat generated by the chip over a larger area to help with cooling.
The packages are placed on a breadboard or printed circuit board and
wired together to assemble the system.
Packages
Figure A.10 shows a variety of integrated circuit packages. Packages can
be generally categorized as through-hole or surface mount (SMT).
Through-hole packages, as their name implies, have pins that can be
inserted through holes in a printed circuit board or into a socket. Dual
inline packages (DIPs) have two rows of pins with 0.1-inch spacing
between pins. Pin grid arrays (PGAs) support more pins in a smaller
package by placing the pins under the package. SMT packages are
soldered directly to the surface of a printed circuit board without using
holes. Pins on SMT parts are called leads. The thin small outline package
(TSOP) has two rows of closely spaced leads (typically 0.02-inch spacing). Plastic leaded chip carriers (PLCCs) have J-shaped leads on all four
A.7 Packaging and Assembly 531
Table A.3 Typical specifications for low-voltage logic families
LVC ALVC AUC
Vdd (V) 3.3 2.5 1.8 3.3 2.5 1.8 2.5 1.8 1.2
tpd (ns) 4.1 6.9 9.8 2.8 3 ?1 1.8 2.3 3.4
VIH (V) 2 1.7 1.17 2 1.7 1.17 1.7 1.17 0.78
VIL (V) 0.8 0.7 0.63 0.8 0.7 0.63 0.7 0.63 0.42
VOH (V) 2.2 1.7 1.2 2 1.7 1.2 1.8 1.2 0.8
VOL (V) 0.55 0.7 0.45 0.55 0.7 0.45 0.6 0.45 0.3
IO (mA) 24 8 4 24 12 12 9 8 3
II (mA) 0.02 0.005 0.005
IDD (mA) 0.01 0.01 0.01
Cpd (pF) 10 9.8 7 27.5 23 ?* 17 14 14
cost (US $) 0.17 0.20 not available
* Delay and capacitance not available at the time of writing

sides, with 0.05-inch spacing. They can be soldered directly to a board
or placed in special sockets. Quad flat packs (QFPs) accommodate a
large number of pins using closely spaced legs on all four sides. Ball grid
arrays (BGAs) eliminate the legs altogether. Instead, they have hundreds
of tiny solder balls on the underside of the package. They are carefully
placed over matching pads on a printed circuit board, then heated so
that the solder melts and joins the package to the underlying board.
Breadboards
DIPs are easy to use for prototyping, because they can be placed in a
breadboard. A breadboard is a plastic board containing rows of sockets,
as shown in Figure A.11. All five holes in a row are connected together.
Each pin of the package is placed in a hole in a separate row. Wires can
be placed in adjacent holes in the same row to make connections to the
pin. Breadboards often provide separate columns of connected holes
running the height of the board to distribute power and ground.
Figure A.11 shows a breadboard containing a majority gate built
with a 74LS08 AND chip and a 74LS32 OR chip. The schematic of the
circuit is shown in Figure A.12. Each gate in the schematic is labeled
with the chip (08 or 32) and the pin numbers of the inputs and outputs
(see Figure A.1). Observe that the same connections are made on the
breadboard. The inputs are connected to pins 1, 2, and 5 of the 08 chip,
and the output is measured at pin 6 of the 32 chip. Power and ground
are connected to pins 14 and 7, respectively, of each chip, from the vertical power and ground columns that are attached to the banana plug
receptacles, Vb and Va. Labeling the schematic in this way and checking
off connections as they are made is a good way to reduce the number of
mistakes made during breadboarding.
Unfortunately, it is easy to accidentally plug a wire in the wrong
hole or have a wire fall out, so breadboarding requires a great deal of
care (and usually some debugging in the laboratory). Breadboards are
suited only to prototyping, not production.
532 APPENDIX A Digital System Implementation
Figure A.10 Integrated circuit
packages

Printed Circuit Boards
Instead of breadboarding, chip packages may be soldered to a printed
circuit board (PCB). The PCB is formed of alternating layers of conducting copper and insulating epoxy. The copper is etched to form wires
called traces. Holes called vias are drilled through the board and plated
with metal to connect between layers. PCBs are usually designed with
computer-aided design (CAD) tools. You can etch and drill your own
simple boards in the laboratory, or you can send the board design to a
specialized factory for inexpensive mass production. Factories have turnaround times of days (or weeks, for cheap mass production runs) and
typically charge a few hundred dollars in setup fees and a few dollars per
board for moderately complex boards built in large quantities.
A.7 Packaging and Assembly 533
08
08
08
32
32
1
BC
Y
2 3
4
5 6
8 9
10
1
2
3
4
5 6
A
Figure A.12 Majority gate
schematic with chips and pins
identified
GND
Rows of 5 pins are
internally connected
VDD Figure A.11 Majority circuit on
breadboard

534 APPENDIX A Digital System Implementation
Signal Layer
Signal Layer
Power Plane
Ground Plane
Copper Trace Insulator
Figure A.13 Printed circuit
board cross-section
PCB traces are normally made of copper because of its low resistance.
The traces are embedded in an insulating material, usually a green, fireresistant plastic called FR4. A PCB also typically has copper power and
ground layers, called planes, between signal layers. Figure A.13 shows a
cross-section of a PCB. The signal layers are on the top and bottom, and
the power and ground planes are embedded in the center of the board.
The power and ground planes have low resistance, so they distribute stable power to components on the board. They also make the capacitance
and inductance of the traces uniform and predictable.
Figure A.14 shows a PCB for a 1970s vintage Apple II computer. At
the top is a Motorola 6502 microprocessor. Beneath are six 16-Kb ROM
chips forming 12 KB of ROM containing the operating system. Three rows
of eight 16-Kb DRAM chips provide 48 KB of RAM. On the right are
several rows of 74xx-series logic for memory address decoding and other
functions. The lines between chips are traces that wire the chips together.
The dots at the ends of some of the traces are vias filled with metal.
Putting It All Together
Most modern chips with large numbers of inputs and outputs use SMT
packages, especially QFPs and BGAs. These packages require a printed
circuit board rather than a breadboard. Working with BGAs is especially
challenging because they require specialized assembly equipment.
Moreover, the balls cannot be probed with a voltmeter or oscilloscope
during debugging in the laboratory, because they are hidden under the
package.
In summary, the designer needs to consider packaging early on to
determine whether a breadboard can be used during prototyping and
whether BGA parts will be required. Professional engineers rarely use
breadboards when they are confident of connecting chips together correctly without experimentation.
A.8 TRANSMISSION LINES
We have assumed so far that wires are equipotential connections that have
a single voltage along their entire length. Signals actually propagate along
wires at the speed of light in the form of electromagnetic waves. If the wires
are short enough or the signals change slowly, the equipotential assumption

is good enough. When the wire is long or the signal is very fast, the transmission time along the wire becomes important to accurately determine the
circuit delay. We must model such wires as transmission lines, in which a
wave of voltage and current propagates at the speed of light. When the
wave reaches the end of the line, it may reflect back along the line. The
reflection may cause noise and odd behaviors unless steps are taken to limit
it. Hence, the digital designer must consider transmission line behavior to
accurately account for the delay and noise effects in long wires.
Electromagnetic waves travel at the speed of light in a given medium,
which is fast but not instantaneous. The speed of light depends on the
permittivity, ε, and permeability, µ, of the medium2: . v  1
s
  1
sLC
A.8 Transmission Lines 535
Figure A.14 Apple II circuit board
2 The capacitance, C, and inductance, L, of a wire are related to the permittivity and
permeability of the physical medium in which the wire is located.
Ap
The speed of light in free space is   c  3  108 m/s. Signals in a PCB
travel at about half this speed, because the FR4 insulator has four times
the permittivity of air. Thus, PCB signals travel at about 1.5  108 m/s,
or 15 cm/ns. The time delay for a signal to travel along a transmission
line of length l is
td  l/v (A.4)
The characteristic impedance of a transmission line, Z0 (pronounced
“Z-naught”), is the ratio of voltage to current in a wave traveling along
the line: Z0  V/I. It is not the resistance of the wire (a good transmission line in a digital system typically has negligible resistance).
Z0 depends on the inductance and capacitance of the line (see the
derivation in Section A.8.7) and typically has a value of 50 to 75 .
(A.5)
Figure A.15 shows the symbol for a transmission line. The symbol
resembles a coaxial cable with an inner signal conductor and an outer
grounded conductor like that used in television cable wiring.
The key to understanding the behavior of transmission lines is to
visualize the wave of voltage propagating along the line at the speed
of light. When the wave reaches the end of the line, it may be
absorbed or reflected, depending on the termination or load at the
end. Reflections travel back along the line, adding to the voltage
already on the line. Terminations are classified as matched, open,
short, or mismatched. The following subsections explore how a wave
propagates along the line and what happens to the wave when it
reaches the termination.
A.8.1 Matched Termination
Figure A.16 shows a transmission line of length l with a matched termination, which means that the load impedance, ZL, is equal to the
characteristic impedance, Z0. The transmission line has a characteristic impedance of 50 . One end of the line is connected to a voltage
Z0  v L
C
536 APPENDIX A Digital System Implementation
I
V
–
+
Z0 =
C
L
Figure A.15 Transmission line
symbol
Appendi
A.8 Transmission Lines 537
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZL = 50 Ω
Figure A.16 Transmission line
with matched termination
Figure A.17 Voltage waveforms
for Figure A.16 at points A, B,
and C
VS ZL = 50 Ω
A, B, C
Figure A.18 Equivalent circuit of
Figure A.16 at steady state
source through a switch that closes at time t  0. The other end is
connected to the 50  matched load. This section analyzes the voltages and currents at points A, B, and C—at the beginning of the line,
one-third of the length along the line, and at the end of the line,
respectively.
Figure A.17 shows the voltages at points A, B, and C over time.
Initially, there is no voltage or current flowing in the transmission
line, because the switch is open. At time t  0, the switch closes, and
the voltage source launches a wave with voltage V  VS along the
line. Because the characteristic impedance is Z0, the wave has current
I  VS /Z0. The voltage reaches the beginning of the line (point A)
immediately, as shown in Figure A.17(a). The wave propagates along
the line at the speed of light. At time td/3, the wave reaches point B.
The voltage at this point abruptly rises from 0 to VS, as shown in
Figure A.17(b). At time td, the incident wave reaches point C at the
end of the line, and the voltage rises there too. All of the current, I,
flows into the resistor, ZL, producing a voltage across the resistor of
ZLI  ZL(VS /Z0)  VS because ZL  Z0. This voltage is consistent
with the wave flowing along the transmission line. Thus, the wave is
absorbed by the load impedance, and the transmission line reaches its
steady state.
In steady state, the transmission line behaves like an ideal equipotential wire because it is, after all, just a wire. The voltage at all points
along the line must be identical. Figure A.18 shows the steady-state
equivalent model of the circuit in Figure A.16. The voltage is VS everywhere along the wire.
Example A.2 TRANSMISSION LINE WITH MATCHED SOURCE AND
LOAD TERMINATIONS
Figure A.19 shows a transmission line with matched source and load impedances
ZS and ZL. Plot the voltage at nodes A, B, and C versus time. When does the
system reach steady-state, and what is the equivalent circuit at steady-state?
SOLUTION: When the voltage source has a source impedance, ZS, in series with
the transmission line, part of the voltage drops across ZS, and the remainder
(a)
t
t
d
VA
VS
(c)
t
VS
t
d
VC
(b)
t
VS
t
d/3 t
d
VB
Appendi
propagates down the transmission line. At first, the transmission line behaves
as an impedance Z0, because the load at the end of the line cannot possibly
influence the behavior of the line until a speed of light delay has elapsed.
Hence, by the voltage divider equation, the incident voltage flowing down the
line is
(A.6)
Thus, at t  0, a wave of voltage, , is sent down the line from point A.
Again, the signal reaches point B at time td/3 and point C at td, as shown in
Figure A.20. All of the current is absorbed by the load impedance ZL, so the
circuit enters steady-state at t  td. In steady-state, the entire line is at VS /2, just
as the steady-state equivalent circuit in Figure A.21 would predict.
A.8.2 Open Termination
When the load impedance is not equal to Z0, the termination cannot
absorb all of the current, and some of the wave must be reflected.
Figure A.22 shows a transmission line with an open load termination.
No current can flow through an open termination, so the current at
point C must always be 0.
The voltage on the line is initially zero. At t  0, the switch closes
and a wave of voltage, , begins propagating down the
line. Notice that this initial wave is the same as that of Example A.2 and
is independent of the termination, because the load at the end of the line
cannot influence the behavior at the beginning until at least td has
elapsed. This wave reaches point B at td/3 and point C at td as shown in
Figure A.23.
When the incident wave reaches point C, it cannot continue forward
because the wire is open. It must instead reflect back toward the source.
The reflected wave also has voltage , because the open termination reflects the entire wave.
The voltage at any point is the sum of the incident and reflected
waves. At time t  td, the voltage at point C is . The
reflected wave reaches point B at 5td/3 and point A at 2td. When
it reaches point A, the wave is absorbed by the source termination
V  VS
2  VS
2  VS
V  VS
2
VVS
Z0
Z0  ZS  VS
2
V  VS
2
V  VS Z0
Z0  ZS   VS
2
538 APPENDIX A Digital System Implementation
t
VA
(a)
VS/2
t
d
t
(b)
t
d /3
VS/2
VB
t
(c)
t
d
VC
VS/2
t
d
Figure A.20 Voltage waveforms
for Figure A.19 at points A, B,
and C
ZL = 50 Ω
ZS = 50 Ω
A, B, C
VS
Figure A.21 Equivalent circuit
of Figure A.19 at steady state
VS
t = 0
Z0 = 50 Ω
ZL = 50 Ω
ZS = 50 Ω
length = l
td = l/v
A B C
l/3
Figure A.19 Transmission line
with matched source and load
impedances
Appendix A.qxd
A.8 Transmission Lines 539
impedance that matches the characteristic impedance of the line. Thus,
the system reaches steady state at time t  2td, and the transmission
line becomes equivalent to an equipotential wire with voltage VS and
current I  0.
A.8.3 Short Termination
Figure A.24 shows a transmission line terminated with a short circuit to
ground. Thus, the voltage at point C must always be 0.
As in the previous examples, the voltages on the line are initially 0.
When the switch closes, a wave of voltage, , begins propagating
down the line (Figure A.25). When it reaches the end of the line, it must
reflect with opposite polarity. The reflected wave, with voltage ,
adds to the incident wave, ensuring that the voltage at point C
remains 0. The reflected wave reaches the source at time t  2td and is
absorbed by the source impedance. At this point, the system reaches
steady state, and the transmission line is equivalent to an equipotential
wire with voltage V  0.
V  VS
2
V  VS
2
t
VS/2
VS
5t
d t /3 d/3 t
d
VB
t
VS
t
d
VC
(c)
(b)
(a)
t
t
d
VA
VS/2
VS
2t
d
Figure A.23 Voltage waveforms
for Figure A.22 at points
A, B, and C
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZS = 50 Ω Figure A.22 Transmission line
with open load termination
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZS = 50 Ω
Figure A.24 Transmission line with short termination
A.8.4 Mismatched Termination
The termination impedance is said to be mismatched when it does not
equal the characteristic impedance of the line. In general, when an incident
wave reaches a mismatched termination, part of the wave is absorbed and
part is reflected. The reflection coefficient, kr, indicates the fraction of the
incident wave (Vi
) that is reflected: Vr  krVi
.
Section A.8.8 derives the reflection coefficient using conservation of
current arguments. It shows that, when an incident wave flowing along a
Appendi
transmission line of characteristic impedance Z0 reaches a termination
impedance, ZT, at the end of the line, the reflection coefficient is
(A.7)
Note a few special cases. If the termination is an open circuit (ZT  ),
kr  1, because the incident wave is entirely reflected (so the current out the
end of the line remains zero). If the termination is a short circuit (ZT  0),
kr  1, because the incident wave is reflected with negative polarity (so
the voltage at the end of the line remains zero). If the termination is a
matched load (ZT  Z0), kr  0, because the incident wave is absorbed.
Figure A.26 illustrates reflections in a transmission line with a mismatched load termination of 75 . ZT  ZL  75 , and Z0  50 , so
kr  1/5. As in previous examples, the voltage on the line is initially 0.
When the switch closes, a wave of voltage, , propagates down the
line, reaching the end at t  td. When the incident wave reaches the
termination at the end of the line, one fifth of the wave is reflected, and
the remaining four fifths flows into the load impedance. Thus, the
reflected wave has a voltage . The total voltage at point
C is the sum of the incoming and reflected voltages, .
At t  2td, the reflected wave reaches point A, where it is absorbed by the
matched 50  termination, ZS. Figure A.27 plots the voltages and currents
along the line. Again, note that, in steady state (in this case at time t 
2td), the transmission line is equivalent to an equipotential wire, as shown
in Figure A.28. At steady-state, the system acts like a voltage divider, so
Reflections can occur at both ends of the transmission line. Figure A.29
shows a transmission line with a source impedance, ZS, of 450  and an
open termination at the load. The reflection coefficients at the load and
source, krL and krS, are 4/5 and 1, respectively. In this case, waves reflect off
both ends of the transmission line until a steady state is reached.
The bounce diagram, shown in Figure A.30, helps visualize reflections
off both ends of the transmission line. The horizontal axis represents
VA  VB  VC  VS ZL
ZL  ZS   VS 75
75  50    3VS
5
VC VS
2  VS
10  3VS
5
V VS
2  1
5  VS
10
V  VS
2
kr  ZT Z0
ZT  Z0
540 APPENDIX A Digital System Implementation
t
t
d
VA
VS/2
2t
d
(a)
t
VS/2
5t
d t
d/3 t /3 d
VB
(b)
(c)
t
VS
t
d
VC
Figure A.25 Voltage waveforms
for Figure A.24 at points A, B,
and C
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZL = 75 Ω
ZS = 50 Ω
+
–
Figure A.26 Transmission line with mismatched termination
Appendix A.qxd 1/30/07 2:57 
distance along the transmission line, and the vertical axis represents time,
increasing downward. The two sides of the bounce diagram represent the
source and load ends of the transmission line, points A and C. The incoming and reflected signal waves are drawn as diagonal lines between points
A and C. At time t  0, the source impedance and transmission line
behave as a voltage divider, launching a voltage wave of from
point A toward point C. At time t  td, the signal reaches point C and is
completely reflected (krL  1). At time t  2td, the reflected wave of
reaches point A and is reflected with a reflection coefficient, krS  4/5,
to produce a wave of traveling toward point C, and so forth.
The voltage at a given time at any point on the transmission line is the
sum of all the incident and reflected waves. Thus, at time t  1.1td, the
voltage at point C is . At time t  3.1td, the voltage at point
C is , and so forth. Figure A.31 plots the voltages
against time. As t approaches infinity, the voltages approach steady-state
with VA  VB  VC  VS.
VS
10  VS
10  2VS
25  2VS
25  9VS
25
VS
10  VS
10  VS
5
2VS
25
VS
10
VS
10
A.8 Transmission Lines 541
Figure A.27 Voltage waveforms for Figure A.26 at points A, B, and C
VS
t = 0
length = l
td = l/v
A C B
l/3
ZS = 450 Ω
+
Z0 = 50 Ω
–
Figure A.29 Transmission line with mismatched source and load terminations
A, B, C
VS
ZS = 50 Ω
+ ZL = 75 Ω –
Figure A.28 Equivalent circuit of Figure A.26 at steady-state
Voltage
A C
10
VS
10
25
25
125
125
t = td
krS = 4/5
B
VS
2VS
2VS
8VS
8VS
t = 3td
t = 5td
t = 6td
t = 4td
t = 2td
t = 0
td krL = 1
Figure A.30 Bounce diagram for
Figure A.29
t t
d
VA
VS/2
3VS/5
2t
d
(a)
t
VB
3VS/5
VS/2
t
d 5t
d t /3 d /3
(b)
t t
d
VC
(c)
3VS/5
Appendix A.q
A.8.5 When to Use Transmission Line Models
Transmission line models for wires are needed whenever the wire delay,
td, is longer than a fraction (e.g., 20%) of the edge rates (rise or fall
times) of a signal. If the wire delay is shorter, it has an insignificant effect
on the propagation delay of the signal, and the reflections dissipate while
the signal is transitioning. If the wire delay is longer, it must be considered in order to accurately predict the propagation delay and waveform
of the signal. In particular, reflections may distort the digital characteristic of a waveform, resulting in incorrect logic operations.
Recall that signals travel on a PCB at about 15 cm/ns. For TTL
logic, with edge rates of 10 ns, wires must be modeled as transmission
lines only if they are longer than 30 cm (10 ns  15 cm/ns  20%). PCB
traces are usually less than 30 cm, so most traces can be modeled as
ideal equipotential wires. In contrast, many modern chips have edge
rates of 2 ns or less, so traces longer than about 6 cm (about 2.5 inches)
must be modeled as transmission lines. Clearly, use of edge rates that are
crisper than necessary just causes difficulties for the designer.
Breadboards lack a ground plane, so the electromagnetic fields of each
signal are nonuniform and difficult to model. Moreover, the fields interact
with other signals. This can cause strange reflections and crosstalk
between signals. Thus, breadboards are unreliable above a few megahertz.
In contrast, PCBs have good transmission lines with consistent
characteristic impedance and velocity along the entire line. As long as
they are terminated with a source or load impedance that is matched
to the impedance of the line, PCB traces do not suffer from reflections.
A.8.6 Proper Transmission Line Terminations
There are two common ways to properly terminate a transmission line,
shown in Figure A.32. In parallel termination (Figure A.32(a)), the driver
has a low impedance (ZS ≈ 0). A load resistor (ZL) with impedance Z0 is
placed in parallel with the load (between the input of the receiver gate and
542 APPENDIX A Digital System Implementation
Figure A.31 Voltage and current waveforms for Figure A.29
t
VS
10
25
25
VA
2td
7VS
16VS
4td 6td
(a)
t
3 3 3 3 3 3
10
25
25
5
25
VB
VS
VS
7VS
9VS
16VS
td 5td 7td 11td 13td 17td
(b)
t
5
25
125
VC
(c)
td 3td 5td
9VS
61VS
VS
Ap
ground). When the driver switches from 0 to VDD, it sends a wave with
voltage VDD down the line. The wave is absorbed by the matched load termination, and no reflections take place. In series termination (Figure
A.32(b)), a source resistor (Z5) is placed in series with the driver to raise
the source impedance to Z0. The load has a high impedance (ZL ≈ ).
When the driver switches, it sends a wave with voltage VDD/2 down
the line. The wave reflects at the open circuit load and returns, bringing the
voltage on the line up to VDD. The wave is absorbed at the source termination. Both schemes are similar in that the voltage at the receiver transitions
from 0 to VDD at t  td, just as one would desire. They differ in power
consumption and in the waveforms that appear elsewhere along the line.
Parallel termination dissipates power continuously through the load resistor when the line is at a high voltage. Series termination dissipates no DC
power, because the load is an open circuit. However, in series terminated
lines, points near the middle of the transmission line initially see a voltage
of VDD/2, until the reflection returns. If other gates are attached to the
A.8 Transmission Lines 543
driver A C B
gate
(b)
receiver
gate
series
termination
resistor
ZL ≈ ∞
t
t
td
t
VDD
VA
VB
VC
t
t
t
V DD
VDD
VB
VC
A C B
(a)
driver
gate
receiver
gate
parallel
termination
resistor
ZS ≈ 0
Z0
ZL = Z0
ZS = Z0
Z0
VA
VDD /2
VDD
td/3 td
VDD
td
td 2td
5td td /3 td /3
td
VDD
VDD /2
Figure A.32 Termination schemes: (a) parallel, (b) series
A
middle of the line, they will momentarily see an illegal logic level.
Therefore, series termination works best for point-to-point communication
with a single driver and a single receiver. Parallel termination is better for a
bus with multiple receivers, because receivers at the middle of the line never
see an illegal logic level.
A.8.7 Derivation of Z0*
Z0 is the ratio of voltage to current in a wave propagating along a transmission line. This section derives Z0; it assumes some previous knowledge
of resistor-inductor-capacitor (RLC) circuit analysis.
Imagine applying a step voltage to the input of a semi-infinite transmission line (so that there are no reflections). Figure A.33 shows the
semi-infinite line and a model of a segment of the line of length dx. R, L,
and C, are the values of resistance, inductance, and capacitance per unit
length. Figure A.33(b) shows the transmission line model with a resistive
component, R. This is called a lossy transmission line model, because
energy is dissipated, or lost, in the resistance of the wire. However, this loss
is often negligible, and we can simplify analysis by ignoring the resistive
component and treating the transmission line as an ideal transmission line,
as shown in Figure A.33(c).
Voltage and current are functions of time and space throughout the
transmission line, as given by Equations A.8 and A.9.
(A.8)
(A.9)
Taking the space derivative of Equation A.8 and the time derivative of
Equation A.9 and substituting gives Equation A.10, the wave equation.
(A.10)
Z0 is the ratio of voltage to current in the transmission line, as illustrated in Figure A.34(a). Z0 must be independent of the length of the
line, because the behavior of the wave cannot depend on things at a distance. Because it is independent of length, the impedance must still equal
Z0 after the addition of a small amount of transmission line, dx, as
shown in Figure A.34(b).
∂2
∂x2V(x, t)  LC ∂2
∂t
2 I(x, t)
∂
∂x I(x, t)C ∂
∂t V(x, t)
∂
∂xV(x, t) L∂
∂t I(x, t)
544 APPENDIX A Digital System Implementation
dx x
(a)
Cdx
Rdx
dx
Ldx
(b)
Ldx
Cdx
dx
(c)
Figure A.33 Transmission line
models: (a) semi-infinite cable,
(b) lossy, (c) ideal
App
Using the impedances of an inductor and a capacitor, we rewrite the
relationship of Figure A.34 in equation form:
Z0  jLdx  [Z0||(1/(jCdx))] (A.11)
Rearranging, we get
Z2
0(jC)jL 2Z0LCdx0 (A.12)
Taking the limit as dx approaches 0, the last term vanishes and we find that
(A.13)
A.8.8 Derivation of the Reflection Coefficient*
The reflection coefficient, kr, is derived using conservation of current.
Figure A.35 shows a transmission line with characteristic impedance, Z0,
and load impedance, ZL. Imagine an incident wave of voltage Vi and current Ii. When the wave reaches the termination, some current, IL, flows
through the load impedance, causing a voltage drop, VL. The remainder
of the current reflects back down the line in a wave of voltage, Vr, and
current, Ir. Z0 is the ratio of voltage to current in waves propagating
along the line, so .
The voltage on the line is the sum of the voltages of the incident
and reflected waves. The current flowing in the positive direction on the
line is the difference between the currents of the incident and reflected
waves.
VL  Vi  Vr (A.14)
IL  Ii  Ir (A.15)
Vi
Ii  Vr
Ir  Z0
Z0  v L
C
A.8 Transmission Lines 545
jωLdx
dx
jωCdx
1
V Z0
–
+
I
V
–
+
I
(a)
Z0
(b)
Figure A.34 Transmission line
model: (a) for entire line and
(b) with additional length, dx
Figure A.35 Transmission line
showing incoming, reflected, and
load voltages and currents
Z0
ZL
Ii ,Vi
+
–
IL
Ir ,Vr
VL
Appendi
546 APPENDIX A Digital System Implementation
Using Ohm’s law and substituting for IL, Ii, and Ir in Equation A.15,
we get
(A.16)
Rearranging, we solve for the reflection coefficient, kr:
(A.17)
A.8.9 Putting It All Together
Transmission lines model the fact that signals take time to propagate
down long wires because the speed of light is finite. An ideal transmission line has uniform inductance, L, and capacitance, C, per unit length
and zero resistance. The transmission line is characterized by its characteristic impedance, Z0, and delay, td, which can be derived from the
inductance, capacitance, and wire length. The transmission line has
significant delay and noise effects on signals whose rise/fall times are less
than about 5td. This means that, for systems with 2 ns rise/fall times,
PCB traces longer than about 6 cm must be analyzed as transmission
lines to accurately understand their behavior.
A digital system consisting of a gate driving a long wire attached to
the input of a second gate can be modeled with a transmission line as
shown in Figure A.36. The voltage source, source impedance (ZS), and
switch model the first gate switching from 0 to 1 at time 0. The driver
gate cannot supply infinite current; this is modeled by ZS. ZS is usually
small for a logic gate, but a designer may choose to add a resistor in
series with the gate to raise ZS and match the impedance of the line. The
input to the second gate is modeled as ZL. CMOS circuits usually have
little input current, so ZL may be close to infinity. The designer may also
choose to add a resistor in parallel with the second gate, between the
gate input and ground, so that ZL matches the impedance of the line.
Vr
Vi
 ZLZ0
ZL  Z0
 kr
Vi  Vr
ZL
 Vi
Z0
 Vr
Z0
(a)
long wire
driver
gate
receiver
gate
VS td , Z0
Z
–
S
(b)
receiver
gate long wire driver gate
ZL
t = 0
+
Figure A.36 Digital system
modeled with transmission line
App
When the first gate switches, a wave of voltage is driven onto the
transmission line. The source impedance and transmission line form a
voltage divider, so the voltage of the incident wave is
(A.18)
At time td, the wave reaches the end of the line. Part is absorbed by
the load impedance, and part is reflected. The reflection coefficient, kr,
indicates the portion that is reflected: kr  Vr/Vi, where Vr is the voltage
of the reflected wave and Vi is the voltage of the incident wave.
(A.19)
The reflected wave adds to the voltage already on the line. It reaches
the source at time 2td, where part is absorbed and part is again reflected.
The reflections continue back and forth, and the voltage on the line
eventually approaches the value that would be expected if the line were a
simple equipotential wire.
A.9 ECONOMICS
Although digital design is so much fun that some of us would do it for
free, most designers and companies intend to make money. Therefore,
economic considerations are a major factor in design decisions.
The cost of a digital system can be divided into nonrecurring
engineering costs (NRE), and recurring costs. NRE accounts for the cost
of designing the system. It includes the salaries of the design team, computer and software costs, and the costs of producing the first working
unit. The fully loaded cost of a designer in the United States in 2006
(including salary, health insurance, retirement plan, and a computer with
design tools) is roughly $200,000 per year, so design costs can be significant. Recurring costs are the cost of each additional unit; this includes
components, manufacturing, marketing, technical support, and shipping.
The sales price must cover not only the cost of the system but also
other costs such as office rental, taxes, and salaries of staff who do not
directly contribute to the design (such as the janitor and the CEO). After
all of these expenses, the company should still make a profit.
Example A.3 BEN TRIES TO MAKE SOME MONEY
Ben Bitdiddle has designed a crafty circuit for counting raindrops. He decides to
sell the device and try to make some money, but he needs help deciding what
implementation to use. He decides to use either an FPGA or an ASIC. The
kr  ZLZ0
ZL  Z0
Vi  VS
Z0
Z0  ZS
A.9 Economics 547
App
development kit to design and test the FPGA costs $1500. Each FPGA costs
$17. The ASIC costs $600,000 for a mask set and $4 per chip.
Regardless of what chip implementation he chooses, Ben needs to mount the
packaged chip on a printed circuit board (PCB), which will cost him $1.50 per
board. He thinks he can sell 1000 devices per month. Ben has coerced a team
of bright undergraduates into designing the chip for their senior project, so it
doesn’t cost him anything to design.
If the sales price has to be twice the cost (100% profit margin), and the product
life is 2 years, which implementation is the better choice?
SOLUTION: Ben figures out the total cost for each implementation over 2 years, as
shown in Table A.4. Over 2 years, Ben plans on selling 24,000 devices, and the
total cost is given in Table A.4 for each option. If the product life is only two
years, the FPGA option is clearly superior. The per-unit cost is $445,500/24,000
$18.56, and the sales price is $37.13 per unit to give a 100% profit margin. The
ASIC option would have cost $732,000/24,000  $30.50 and would have sold for
$61 per unit.
Example A.4 BEN GETS GREEDY
After seeing the marketing ads for his product, Ben thinks he can sell even more
chips per month than originally expected. If he were to choose the ASIC option,
how many devices per month would he have to sell to make the ASIC option
more profitable than the FPGA option?
SOLUTION: Ben solves for the minimum number of units, N, that he would need
to sell in 2 years:
$600,000  (N  $5.50)  $1500  (N  $18.50)
548 APPENDIX A Digital System Implementation
Table A.4 ASIC vs FPGA costs
Cost ASIC FPGA
NRE $600,000 $1500
chip $4 $17
PCB $1.50 $1.50
TOTAL $600,000  (24,000  $5.50) $1500  (24,000  $18.50)
 $732,000  $445,500
per unit $30.50 $18.56
Appendix 
A.9 Economics 549
Solving the equation gives N  46,039 units, or 1919 units per month. He would
need to almost double his monthly sales to benefit from the ASIC solution.
Example A.5 BEN GETS LESS GREEDY
Ben realizes that his eyes have gotten too big for his stomach, and he doesn’t
think he can sell more than 1000 devices per month. But he does think the product life can be longer than 2 years. At a sales volume of 1000 devices per month,
how long would the product life have to be to make the ASIC option worthwhile?
SOLUTION: If Ben sells more than 46,039 units in total, the ASIC option is the
best choice. So, Ben would need to sell at a volume of 1000 per month for at
least 47 months (rounding up), which is almost 4 years. By then, his product is
likely to be obsolete.
Chips are usually purchased from a distributor rather than directly
from the manufacturer (unless you are ordering tens of thousands of
units). Digikey (www.digikey.com) is a leading distributor that sells a
wide variety of electronics. Jameco (www.jameco.com) and All
Electronics (www.allelectronics.com) have eclectic catalogs that are
competitively priced and well suited to hobbyists.
A

MIPS Instructions B
This appendix summarizes MIPS instructions used in this book. Tables
B.1–B.3 define the opcode and funct fields for each instruction, along
with a short description of what the instruction does. The following notations are used:
 [reg]: contents of the register
 imm: 16-bit immediate field of the I-type instruction
 addr: 26-bit address field of the J-type instruction
 SignImm: sign-extended immediate
 {{16{imm[15]}}, imm}
 ZeroImm: zero-extended immediate
 {16b0, imm}
 Address: [rs]  SignImm
 [Address]: contents of memory location Address
 BTA: branch target address1
 PC  4  (SignImm  2)
 JTA: jump target address
 {(PC  4)[31:28], addr, 2b0}
551
1 The SPIM simulator has no branch delay slot, so BTA is PC  (SignImm  2). Thus, if you use the SPIM
assembler to create machine code for a real MIPS processor, you must decrement the immediate field by 1 to
compensate.
Appendix B.qxd 
Table B.1 Instructions, sorted by opcode
552 APPENDIX B MIPS Instructions
Opcode Name Description Operation
000000 (0) R-type all R-type instructions see Table B.2
000001 (1) bltz/bgez branch less than zero/ if ([rs]  0) PC  BTA/
(rt  0/1) branch greater than or if ([rs]  0) PC  BTA
equal to zero
000010 (2) j jump PC  JTA
000011 (3) jal jump and link $ra  PC4, PC  JTA
000100 (4) beq branch if equal if ([rs][rt]) PC  BTA
000101 (5) bne branch if not equal if ([rs]![rt]) PC  BTA
000110 (6) blez branch if less than or equal to zero if ([rs]  0) PC  BTA
000111 (7) bgtz branch if greater than zero if ([rs]  0) PC  BTA
001000 (8) addi add immediate [rt]  [rs]  SignImm
001001 (9) addiu add immediate unsigned [rt]  [rs]  SignImm
001010 (10) slti set less than immediate [rs]  SignImm ? [rt]1 : [rt]0
001011 (11) sltiu set less than immediate unsigned [rs]  SignImm ? [rt]1 : [rt]0
001100 (12) andi and immediate [rt]  [rs] & ZeroImm
001101 (13) ori or immediate [rt]  [rs] | ZeroImm
001110 (14) xori xor immediate [rt]  [rs]  ZeroImm
001111 (15) lui load upper immediate [rt]  {Imm, 16b0}
010000 (16) mfc0, move from/to coprocessor 0 [rt]  [rd]/[rd]  [rt]
(rs  0/4) mtc0 (rd is in coprocessor 0)
010001 (17) F-type fop  16/17: F-type instructions see Table B.3
010001 (17) bc1f/bc1t fop  8: branch if fpcond is if (fpcond  0) PC  BTA/
(rt  0/1) FALSE/TRUE if (fpcond  1) PC  BTA
100000 (32) lb load byte [rt]  SignExt ([Address]7:0)
100001 (33) lh load halfword [rt]  SignExt ([Address]15:0)
100011 (35) lw load word [rt]  [Address]
100100 (36) lbu load byte unsigned [rt]  ZeroExt ([Address]7:0)
100101 (37) lhu load halfword unsigned [rt]  ZeroExt ([Address]15:0)
101000 (40) sb store byte [Address]7:0  [rt]7:0
(continued)
Appendix B.qxd 2/1/07 9:36 PM Page 552
Table B.1 Instructions, sorted by opcode—Cont’d
Table B.2 R-type instructions, sorted by funct field
Appendix B 553
Funct Name Description Operation
000000 (0) sll shift left logical [rd]  [rt]  shamt
000010 (2) srl shift right logical [rd]  [rt]  shamt
000011 (3) sra shift right arithmetic [rd]  [rt]  shamt
000100 (4) sllv shift left logical [rd]  [rt]  [rs]4:0
variable assembly: sllv rd, rt, rs
000110 (6) srlv shift right logical [rd]  [rt]  [rs]4:0
variable assembly: srlv rd, rt, rs
000111 (7) srav shift right arithmetic [rd]  [rt]  [rs]4:0
variable assembly: srav rd, rt, rs
001000 (8) jr jump register PC  [rs]
001001 (9) jalr jump and link register $ra  PC  4, PC  [rs]
001100 (12) syscall system call system call exception
001101 (13) break break break exception
010000 (16) mfhi move from hi [rd]  [hi]
010001 (17) mthi move to hi [hi]  [rs]
010010 (18) mflo move from lo [rd]  [lo]
010011 (19) mtlo move to lo [lo]  [rs]
011000 (24) mult multiply {[hi], [lo]}  [rs] 	 [rt]
011001 (25) multu multiply unsigned {[hi], [lo]}  [rs] 	 [rt]
011010 (26) div divide [lo]  [rs]/[rt],
[hi]  [rs]%[rt]
101001 (41) sh store halfword [Address]15:0  [rt]15:0
101011 (43) sw store word [Address]  [rt]
110001 (49) lwc1 load word to FP coprocessor 1 [ft]  [Address]
111001 (56) swc1 store word to FP coprocessor 1 [Address]  [ft]
(continued)
Opcode Name Description Operation
Appendix B.qxd 2/1/07
554 APPENDIX B MIPS Instructions
Table B.2 R-type instructions, sorted by funct field—Cont’d
Table B.3 F-type instructions (fop  16/17)
Funct Name Description Operation
011011 (27) divu divide unsigned [lo]  [rs]/[rt],
[hi]  [rs]%[rt]
100000 (32) add add [rd]  [rs]  [rt]
100001 (33) addu add unsigned [rd]  [rs]  [rt]
100010 (34) sub subtract [rd]  [rs] 
 [rt]
100011 (35) subu subtract unsigned [rd]  [rs] 
 [rt]
100100 (36) and and [rd]  [rs] & [rt]
100101 (37) or or [rd]  [rs] | [rt]
100110 (38) xor xor [rd]  [rs]  [rt]
100111 (39) nor nor [rd]  ~([rs] | [rt])
101010 (42) slt set less than [rs]  [rt] ? [rd]  1 : [rd]  0
101011 (43) sltu set less than unsigned [rs]  [rt] ? [rd]  1 : [rd]  0
Funct Name Description Operation
000000 (0) add.s/add.d FP add [fd]  [fs]  [ft]
000001 (1) sub.s/sub.d FP subtract [fd]  [fs] 
 [ft]
000010 (2) mul.s/mul.d FP multiply [fd]  [fs] * [ft]
000011 (3) div.s/div.d FP divide [fd]  [fs]/[ft]
000101 (5) abs.s/abs.d FP absolute value [fd]  ([fs]  0) ? [
fs]
: [fs]
000111 (7) neg.s/neg.d FP negation [fd]  [
fs]
111010 (58) c.seq.s/c.seq.d FP equality comparison fpcond  ([fs]  [ft])
111100 (60) c.lt.s/c.lt.d FP less than comparison fpcond  ([fs]  [ft])
111110 (62) c.le.s/c.le.d FP less than or equal comparison fpcond  ([fs]  [ft])
Appendix B.qxd 2/1/07 9:36 
555
Further Reading
Berlin L., The Man Behind the Microchip: Robert Noyce and the
Invention of Silicon Valley, Oxford University Press, 2005.
The fascinating biography of Robert Noyce, an inventor of the microchip
and founder of Fairchild and Intel. For anyone thinking of working in
Silicon Valley, this book gives insights into the culture of the region, a culture influenced more heavily by Noyce than any other individual.
Colwell R., The Pentium Chronicles: The People, Passion, and Politics
Behind Intel’s Landmark Chips, Wiley, 2005.
An insider’s tale of the development of several generations of Intel’s
Pentium chips, told by one of the leaders of the project. For those considering a career in the field, this book offers views into the managment
of huge design projects and a behind-the-scenes look at one of the most
significant commercial microprocessor lines.
Ercegovac M., and Lang T., Digital Arithmetic, Morgan Kaufmann, 2003.
The most complete text on computer arithmetic systems. An excellent
resource for building high-quality arithmetic units for computers.
Hennessy J., and Patterson D., Computer Architecture: A Quantitative
Approach, 4th ed., Morgan Kaufmann, 2006.
The authoritative text on advanced computer architecture. If you are
intrigued about the inner workings of cutting-edge microprocessors, this
is the book for you.
Kidder T., The Soul of a New Machine, Back Bay Books, 1981.
A classic story of the design of a computer system. Three decades later,
the story is still a page-turner and the insights on project managment
and technology still ring true.
Pedroni V., Circuit Design with VHDL, MIT Press, 2004.
A reference showing how to design circuits with VHDL.
Thomas D., and Moorby P., The Verilog Hardware Description
Language, 5th ed., Kluwer Academic Publishers, 2002.
Ciletti M., Advanced Digital Design with the Verilog HDL, Prentice
Hall, 2003.
Both excellent references covering Verilog in more detail.
555

556 Further Reading
Verilog IEEE Standard (IEEE STD 1364).
The IEEE standard for the Verilog Hardware Description Language;
last updated in 2001. Available at ieeexplore.ieee.org.
VHDL IEEE Standard (IEEE STD 1076).
The IEEE standard for VHDL; last updated in 2004. Available from
IEEE. Available at ieeexplore.ieee.org.
Wakerly J., Digital Design: Principles and Practices, 4th ed., Prentice
Hall, 2006.
A comprehensive and readable text on digital design, and an excellent reference book.
Weste N., and Harris D., CMOS VLSI Design, 3rd ed., Addison-Wesley,
2005.
Very Large Scale Integration (VLSI) Design is the art and science of
building chips containing oodles of transistors. This book, coauthored
by one of our favorite writers, spans the field from the beginning
through the most advanced techniques used in commercial products.

557
Index
0, 23. See also LOW, OFF
1, 23. See also HIGH, ON
74xx series logic, 515–516
parts,
2:1 Mux (74157), 518
3:8 Decoder (74138), 518
4:1 Mux (74153), 518
AND (7408), 517
AND3 (7411), 517
AND4 (7421), 517
Counter (74161, 74163), 518
FLOP (7474), 515–517
NAND (7400), 517
NOR (7402), 517
NOT (7404), 515
OR (7432), 517
XOR (7486), 517
Register (74377), 518
Tristate buffer (74244), 518
schematics of, 517–518
A
add, 291
Adders, 233–240
carry-lookahead. See Carrylookahead adder
carry-propagate (CPA). See Carrypropagate adder
prefix. See Prefix adder
ripple-carry. See Ripple-carry adder
add immediate (addi), 327, 378
add immediate unsigned (addiu), 552
add unsigned (addu), 554
Addition, 14–15, 233–240, 291
floating-point, 252–255
overflow. See Overflow
two’s complement, 15, 240
underflow. See Underflow
unsigned binary, 15
Address, 485–490
physical, 485
translation, 486–489
virtual, 485–490
word alignment, 298
Addressing modes, 327–329
base, 327
immediate, 327
MIPS, 327–329
PC-relative, 327–328
pseudo-direct, 328–329
register-only, 327
Advanced microarchitecture, 435–447
branch prediction. See Branch
prediction
deep pipelines. See Deep pipelines
multiprocessors. See Multiprocessors
multithreading. See Multithreading
out-of-order processor.
See Out-of-order processor
register renaming. See Register
renaming
single instruction multiple data. See
Single instruction multiple
data (SIMD) units
superscalar processor. See
Superscalar processor
vector processor. See Single instruction multiple data (SIMD)
units
Alignment. See Word alignment
ALU. See Arithmetic/logical unit
ALU decoder, 374–376
ALU decoder truth table, 376
ALUControl, 370
ALUOp, 374–375
ALUOut, 383–385
ALUResult, 370–371
AMAT. See Average memory access time
Amdahl, Gene, 468
Amdahl’s Law, 468
Anodes, 27–28
and immediate (andi), 306, 346–347
AND gate, 20–22, 32–33
Application-specific integrated circuits
(ASICs), 523
Architectural state, 363–364
Architecture. See Instruction Set
Architecture
Arithmetic, 233–249, 305–308, 515.
See also Adders, Addition,
Comparator, Divider,
Multiplier
adders. See Adders
addition. See Addition
ALU. See Arithmetic/logical unit
circuits, 233–248
comparators. See Comparators
divider. See Divider
division. See Division
fixed-point, 249
floating-point, 252–255
logical instructions, 305–308
multiplier. See Multiplier
packed, 445
rotators. See Rotators
shifters. See Shifters
signed and unsigned, 338–339
subtraction. See Subtraction
557

Arithmetic (Continued)
subtractor. See Subtractor
underflow. See Addition, Underflow
Arithmetic/logical unit (ALU),
242–244, 515
32-bit, 244
adder. See Adders
ALUOp, 374–376
ALUResult, 370–371
ALUOut, 383–385
comparator. See Comparators
control, 242
subtractor. See Subtractor
Arrays, 314–318
accessing, 315–317
bytes and characters, 316–318
FPGA, See Field programmable gate
array
logic. See Logic arrays
memory. See Memory
RAM, 265
ROM, 266
ASCII (American Standard Code
for Information Interchange)
codes, 317
table of, 317
Assembler directives, 333
ASICs. See Application-specific integrated circuits
Assembly language, MIPS. See MIPS
assembly language
Associativity, 58–59
Average memory access time (AMAT),
467–468
Asynchronous circuits, 116–117
Asynchronous inputs, 144
Asynchronous resettable registers
HDL for, 192
Axioms. See Boolean axioms
B
Babbage, Charles, 7–8, 26
Base address, 295–296
Base register, 302, 343, 347
Base 2 number representations.
See Binary numbers
Base 8 number representations.
See Octal numbers
Base 16 number representations.
See Hexadecimal numbers
Block,
digital building. See Digital building
blocks
in code,
else block, 311
if block, 310–311
Base addressing, 327
Baudot, Jean-Maurice-Emile, 317
Behavioral modeling, 171–185
Benchmarks, 367
SPEC2000, 398–399
Biased numbers, 41. See also Floating
point
Big-Endian, 172, 296–297
Binary numbers, 9–11. See also
Arithmetic
ASCII, 317
binary coded decimal, 252
conversion. See Number
conversion
fixed point, 249–250
floating-point. See Floating-point
numbers
signed, 15–19
sign/magnitude. See Sign/
magnitude numbers
two’s complement. See Two’s
complement numbers
unsigned, 9–15
Binary to decimal conversion,
10–11
Binary to hexadecimal conversion, 12
Bit, 9–11
dirty, 482–483
least significant, 13–14
most significant, 13–14
sign, 16
use, 478–479
valid, 472
Bit cells, 258
Bit swizzling, 182
Bitwise operators, 171–174
Boole, George, 8
Boolean algebra, 56–62
axioms, 57
equation simplification, 61–62
theorems, 57–60
Boolean axioms, 57
Boolean equations, 54–56
product-of-sums (POS) canonical
form, 56
sum-of-products (SOP) canonical
form, 54–55
terminology, 54
Boolean theorems, 57–60
DeMorgan’s, 59
complement, 58, 59
consensus, 60
covering, 58
combining, 58
Branching, 308–310
calculating address, 309
conditional, 308
prediction, 437–438
target address (BTA), 327–328
unconditional (jump), 309
Branch equal (beq), 308–309
Branch not equal (bne), 308–309
Branch/control hazards, 407, 413–416.
See also Hazards
Branch prediction, 437–438
Breadboards, 532–533
Bubble, 59
pushing, 67–69
Buffer, 20
tristate, 70–71
“Bugs,” 169
Bus, 52
tristate, 71
Bypassing, 408
Bytes, 13–14
Byte-addressable memory, 295–297
Byte order, 296–297
Big-Endian, 296–297
Little-Endian, 296–297
C
C programming language, 290
overflows, 339
strings, 318
Caches, 468–484. See also Memory
accessing, 472–473, 476–477, 479
advanced design, 479–483
associativity, 469, 474–475,
478–479
block size, 476–477
blocks, 469–470, 476
capacity, 468–470
data placement, 469–470
data replacement, 478–479
558 Index

definition, 468
direct mapped, 470–474
dirty bit, 482–483
entry fields, 472–473
evolution of MIPS, 483
fully associative, 475–476
hit, 466
hit rate, 467
IA-32 systems, 499–500
level 2 (L2), 480
mapping, 470–478
miss, 466
capacity, 481
compulsory, 481
conflict, 481
miss rate, 467
miss rate versus cache parameters,
481–482
miss penalty, 476
multiway set associative, 474–475
nonblocking, 500
organizations, 478
performance, 467
set associative, 470, 474–475
tag, 471–472
use bit, 478–479
valid bit, 472–473
write policy, 482–483
CAD. See Computer-aided design
Canonical form, 53
Capacitors, 28
Capacity miss, 481
Carry-lookahead adder, 235–237
Carry propagate adder (CPA), 274
Cathodes, 27–28
Cause register, 337–338
Chips, 28, 449
74xx series logic. See 74xx series
logic
Circuits, 82–83, 87–88
74xx series logic. See 74xx series
logic
application-specific integrated
(ASIC), 523
arithmetic. See Arithmetic
asynchronous, 116–117
bistable device, 147
combinational, 53
definition, 51
delays, 73–75, 84–87
calculating, 86–87
dynamic, 273
multiple-output, 64
pipelining, 152
priority, 65, 202, 203
synchronous sequential, 114–116
synthesized, 186–190, 193–195,
199, 200
timing, 84–91
timing analysis, 138
types, 51–54
without glitch, 91
CISC. See Complex instruction set
computers
CLBs. See Configurable logic blocks
Clock cycle. See Clock period
Clock period, 135–139
Clock rate. See Clock period
Clock cycles per instruction (CPI),
367–368
Clustered computers, 447
Clock skew, 140–143
CMOS. See Complementary MetalOxide-Semiconductor Logic
Code, 303
Code size, 334
Combinational composition, 52–53
Combinational logic design, 51–100
Boolean algebra, 56–62
Boolean equations, 54–56
building blocks, 79–84
delays, 85–87
don’t cares, 65
HDLs and. See Hardware description languages
Karnaugh maps, 71–79
logic, 62–65
multilevel, 65–69
overview, 51–54
precedence, 54
timing, 84–91
two-level, 65–66
X’s (contention). See Contention
X’s (don’t cares). See Don’t cares
Z’s (floating). See Floating
Comparators, 240–241
equality, 241
magnitude, 241, 242
Compiler, 331–333
Complementary Metal-OxideSemiconductor Logic (CMOS), 25
bubble pushing, 69
logic gates, 31–33
NAND gate, 32
NOR gate, 32–33
NOT gate, 31
transistors, 26–34
Complex instruction set computers
(CISC), 292, 341
Compulsory miss, 481
Computer-aided design (CAD), 167
Computer Organization and Design
(Patterson and Hennessy),
290, 363
Complexity management, 4–6
abstraction, 4–5
discipline, 5–6
hierarchy, 6
modularity, 6
regularity, 6
Conditional assignment, 175–176
Conditional branches, 308
Condition codes, 344
Conflict misses, 481
Conditional statements, 310–311
Constants, 298. See also Immediates
Contamination delay, 84–88
Contention (X), 69–70
Context switch, 446
Control signals, 79, 242–243
Control unit, 364, 366, 374–406
multicycle MIPS processor FSM,
381–395
pipelined MIPS processor, 405–406
single-cycle MIPS processor,
374–377
Configurable logic blocks (CLBs),
268–272
Control hazards. See Branch/control
hazards, Pipelining
Coprocessor 0, 338
Counters, 254
Covalent bond, 27
CPA. See Carry propagate adder
CPI. See Clock cycles per instruction
Critical path, 85–89
Cyclic paths, 114
Cycle time. See Clock Period
D
Data hazards. See Hazards
Data memory, 365
Data sheets, 523–528
Datapath, 364. See also MIPS microprocessors
Index 559

Datapath (Continued)
elements, 364
multicycle MIPS processor, 382–388
pipelined MIPS processor, 404
single-cycle MIPS processor,
368–374
Data segments. See Memory map
Data types. See Hardware description
languages
DC. See Direct current
Decimal numbers, 9
conversion to binary and hexadecimal. See Number conversion
scientific notation, 249–250
Decimal to Binary conversion, 11
Decimal to hexadecimal conversion, 13
Decoders
implementation, 83
logic, 83
parameterized, 212
Deep pipelines, 435–436
Delay, 182
DeMorgan, Augustus, 56
DeMorgan’s theorem, 59, 60
Dennard, Robert, 260
Destination register, 301, 305–306,
370–371, 377–378
Device under test (DUT), 214–218.
See also Unit under test
Device driver, 496, 498
Dice, 28
Digital abstraction, 4–5, 8–9, 22–26
DC transfer characteristics, 23–24
logic levels, 22–23
noise margins, 23
supply voltage, 22
Digital design
abstraction, 7–9
discipline, 5–6
hierarchy, 6
modularity, 6
regularity, 6
Digital system implementation, 515–548
74xx series logic. See 74xx series
logic
application-specific integrated
circuits (ASICs), 523
data sheets, 523–528
economics, 546–548
logic families, 529–531
overview, 515
packaging and assembly,
531–534
breadboards, 532
packages, 531–532
printed circuit boards, 533–534
programmable logic, 516–523
transmission lines. See Transmission
lines
Diodes, 27–28
DIP. See Dual-inline package
Direct current (DC), 23, 24
transfer characteristics, 23–24, 25
Direct mapped cache, 470–474
Dirty bit, 482–483
Discipline, 5–6
Disk. See Hard disk
divide (div), 308
divide unsigned (divu), 339
Divider, 247–248
Division, 247–248
floating-point, 253
instructions, 308
Divisor, 247
Don’t care (X), 65
Dopant atoms, 27
Double. See Double-precision floatingpoint numbers
Double-precision floating point numbers, 251–252
DRAM. See Dynamic random access
memory
Driver. See Device driver
Dual-inline package (DIP), 28, 531
DUT. See Device under test
Dynamic discipline, 134
Dynamic data segment, 331
Dynamic random access memory
(DRAM), 257, 260, 463
E
Edison, Thomas, 169
Edge-triggered digital systems,
108, 112
Equations
simplification, 61–62
Electrically erasable programmable read
only memory (EEPROM), 263
Enabled registers, 193
HDL for, 193
EPC. See Exception Program Counter
register
Erasable programmable read only
memory (EPROM), 263
Exceptions, 337–339
Exception handler, 337–338
Exception program counter (EPC),
337–338
Exclusive or. See XOR
Executable file, 334
Execution time, 367
Exponent, 250–253
F
Failures, 144–146
FDIV bug, 253
FET. See Field effect transistors
Field programmable gate array (FPGA),
268–272, 521–523
Field effect transistors, 26
FIFO. See First-in-first-out queue. See
also Queue
Finite state machines (FSMs),
117–133
design example, 117–123
divide-by-3, 207–208
factoring, 129–132
HDLs and, 206–213
Mealy machines. See Mealy machines
Moore machines. See Moore
machines
Moore versus Mealy machines,
126–129
state encodings, 123–126
state transition diagram, 118–119
First-in-first-out (FIFO) queue, 508
Fixed-point numbers, 249–250
Flash memory, 263–264
Flip-flops, 103–112, 257. See also
Registers
comparison with latches, 106, 112
D, 108
enabled, 109–110
register, 108–109
resettable, 110, 427
asynchronous, 192
synchronous, 192
transistor count, 108
transistor-level, 110–111
Floating (Z), 69–71
Floating-point numbers, 250–253
560 Index

addition, 252–255. See also
Addition
converting binary or decimal to. See
Number conversions
division, 253. See also Division
double-precision, 251–252
FDIV bug. See FDIV bug
floating-point unit (FPU), 253
instructions, 340–341
rounding, 252
single-precision, 251–252
special cases
infinity, 251
not a number (NaN), 251
Forwarding, 408–409
Fractional numbers, 274
FPGA. See Field programmable gate
array
Frequency, 135. See also Clock period
FSM. See Finite state machines
Full adder, 52, 178. See also Adder,
Addition
HDL using always/process, 197
using nonblocking assignments,
204
Fully associative cache, 475–476
Funct field, 299–300
Functional specification, 51
Functions. See Procedure calls
Fuse, 263
G
Gates, 19–22
AND, 20–22, 32–33
NAND, 21, 32
NOR, 21, 32–33
OR, 21
transistor-level implementation, 262
XOR, 21
XNOR, 21
Gedanken-Experiments on Sequential
Machines (Moore), 111
Generate signal, 235, 237
Glitches, 75, 88–91
Global pointer ($gp), 294, 331. See
also Static data segment
Gray, Frank, 65
Gray codes, 65, 72
Gulliver’s Travels (Swift), 297
H
Half word, 339
Half adder, 233–234
Hard disk, 484
Hard drive. See Hard disk
Hardware reduction, 66–67
Hardware description languages
(HDLs), 167–230
assignment statements, 177
behavioral modeling, 171–184
combinational logic, 171–185,
195–206
bit swizzling, 182
bitwise operators, 171–174
blocking and nonblocking
assignments, 201–206
case statements, 198–199
conditional assignment, 175–176
delays, 182–183
if statements, 199–201
internal variables, 176–178
numbers, 179
precedence, 178–179
reduction operators, 174
synthesis tools. See Synthesis Tools
Verilog, 201
VHDL libraries and types,
183–185
Z’s and X’s, 179–182
finite state machines, 206–213
generate statement, 213
generic building blocks, 426
invalid logic level, 181
language origins, 168–169
modules, 167–168
origins of, 168–169
overview, 167
parameterized modules, 211–213
representation, 421–431
sequential logic, 190–195, 205
enabled registers, 193
latches, 195
multiple registers, 194–195
registers, 190–191. See also
Registers
resettable registers, 191–193
simulation and synthesis, 169–171
single-cycle processor, 422
structural modeling, 185–189
testbenches, 214–218, 428–431
Hazards, 75, 406–418. See also
Glitches
control hazards, 413–416
data hazards 408–413
solving, 408–416
forwarding, 408–410
stalls, 410–413
WAR. See Write after read
WAW. See Write after write
Hazard unit, 408, 411, 419
Heap, 331
HDLs. See Hardware description
languages
Hennessy, John, 290, 364
Hexadecimal numbers, 11–13
to binary conversion table, 12
Hierarchy, 6, 189
HIGH, 23. See also 1, ON
High-level programming languages,
290–294
translating into assembly, 290–291
compiling, linking, and launching,
330–331
Hit, 466
High impedance. See Floating, Z
High Z. See Floating, High
impedance, Z
Hold time, 133–154
I
I-type instruction, 301–302
IA-32 microprocessor, 290, 341–349,
447–453
branch conditions, 346
cache systems, 499–500
encoding, 346–348
evolution, 448, 500
instructions, 344, 345
memory and input/output (I/O)
systems, 499–502
operands, 342–344
programmed I/O, 502
registers, 342
status flags, 344
virtual memory, 501
IEEE 754 floating-point standard, 251
Idempotency, 58
Idioms, 171
IEEE, 169
Index 561

If statements, 199–201, 310
If/else statements, 311
ILP. See Instruction-level parallelism
Immediates, 298
Immediate addressing, 327
Information, amount of, 8
IorD, 385
I/O (input/output), 337, 494–502
communicating with, 494–495
device driver, 496, 498
devices, 494–496. See also
Peripheral devices
memory interface, 494, 502
memory-mapped I/O, 494–499
Inputs, asynchronous, 144–145
Instruction encoding. See Machine
Language
Instruction register (IR), 383, 390
Instruction set architecture (ISA),
289–361. See also MIPS instruction set architecture
Input/output blocks (IOBs), 268
Input terminals, 51
Institute of Electrical and Electronics
Engineers, 250
Instruction decode, 401–402
Instruction encoding. See Instruction
format
Instruction format
F-type, 340
I-type, 301–302
J-type, 302
R-type, 299–300
Instruction-level parallelism (ILP), 443,
446
Instruction memory, 365
Instruction set. See Instruction set
architecture
Instructions. See also Language
arithmetic/logical, 304–308
floating-point, 340–341
IA-32, 344–346
I-type, 301–302
J-type, 302
loads. See Loads
multiplication and division, 308
pseudoinstructions, 336–337
R-type, 299–300
set less than, 339
shift, 306–307
signed and unsigned, 338–339
Intel, 30, 111, 290, 348, 367
Inverter. See NOT gate
Integrated circuits (ICs), 26, 137,
515, 532
costs, 137, 169
manufacturing process, 515
Intel. See IA-32 microprocessors
Interrupts. See Exceptions
An Investigation of the Laws of
Thought (Boole), 8
Involution, 58
IOBs. See Input/output blocks
I-type instructions, 301–302
J
Java, 316. See also Language
JTA. See Jump target address
J-type instructions, 302
Jump, 309–310. See also Branch,
unconditional, Programming
Jump target address (JTA), 329
K
K-maps. See Karnaugh maps
Karnaugh, Maurice, 64, 71
Karnaugh maps (K-maps), 71–79
logic minimization using, 73–76
prime implicants, 61, 74
seven-segment display decoder,
75–77
with “don’t cares,” 78
without glitches, 91
Kilby, Jack, 26
Kilobyte, 14
K-maps. See Karnaugh maps
L
Labels, 308–309
Language. See also Instructions
assembly, 290–299
high-level, 290–294
machine, 299–304
mnemonic, 291
translating assembly to machine, 300
Last-in-first-out (LIFO) queue, 321. See
also Stack, Queue
Latches, 103–112
comparison with flip-flops, 106, 112
D, 107
SR, 105–107
transistor-level, 110–111
Latency, 149
Lattice, 27
Leaf procedures, 324–325
Least recently used (LRU) replacement,
478–479
Least significant bit (lsb), 13
Least significant byte (LSB), 296
LIFO. See Last-in-first-out queue
Literal, 54, 61, 167
Little-Endian, 296–297
Load, 255
byte (lb), 317
byte unsigned (lbu), 317
half (lh), 339
immediate (li), 336
upper immediate (lui), 308
word (lw), 295–296
Loading, 335
Locality, 464
Local variables, 326–327
Logic, 62–65. See also Multilevel combinational logic; Sequential logic
design
bubble pushing. See Bubble pushing
combinational. See Combinational
logic
families, 529–531
gates. See Logic gates
hardware reduction. See Hardware
reduction, Equation simplification
multilevel. See Multilevel combinational logic
programmable, 516–523
sequential. See Sequential logic
synthesis, 170–171
two-level, 65–66
using memory arrays, 264. See also
Logic arrays
Logic arrays, 266–274
field programmable gate array,
268–272
programmable logic array, 266–268
transistor-level implementations,
273–274
Logic families, 25, 529–531
562 Index

compatibility, 26
specifications, 529, 531
Logic gates, 19–22, 173
buffer, 20
delays, 183
multiple-input gates, 21–22
two-input gates, 21
types
AND. See AND gate
AOI (and-or-invert).
See And-or-invert gate
NAND. See NAND gate
NOR. See NOR gate
NOT. See NOT gate
OAI (or-and-invert).
See Or-and-invert gate
OR. See OR gate
XOR. See XOR gate
XNOR. See XNOR gate
Logic levels, 22–23
Logical operations, 304–308
Lookup tables (LUTs), 268
Loops, 311–314
for, 313
while, 312–313
LOW, 23. See also 0, OFF
Low Voltage CMOS Logic
(LVCMOS), 25
Low Voltage TTL Logic (LVTTL), 25
LRU. See Least recently used replacement
LSB. See Least significant byte
LUTs. See Lookup tables
LVCMOS. See Low Voltage CMOS
Logic
LVTTL. See Low Voltage TTL Logic
M
Machine language, 299–304
function fields, 299
interpreting code, 302–303
I-type instructions, 301–302
J-type instructions, 302
opcodes, 299–303
R-type instructions, 299–300
stored programs, 303–304
translating from assembly language,
300–302
translating into assembly
language, 303
Main decoder, 374–379
Main memory, 466–469
Mapping, 470
Mantissa, 250, 252–253. See also
Floating-point numbers
Masuoka, Fujio, 263
MCM. See Multichip module
Mealy, George H., 111
Mealy machines, 126–129, 130, 210
combined state transition and
output table, 127
state transition diagram, 118–119
timing diagram, 131
Mean time between failures (MTBF),
146
Memory, 51, 295–298
access, 298
average memory access time
(AMAT), 467
cache. See Caches
DRAM. See Dynamic random
access memory
hierarchy, 466
interface, 464
main, 466–469
map, 330–331
dynamic data segment, 331
global data segment, 330–331
reserved segment, 331
text segment, 330
nonvolatile, 259–260
performance, 465
physical, 466, 485–486
protection, 491. See also Virtual
memory
RAM, 259
ROM, 259
separate data and instruction,
430–431
shared, 71
stack. See Stack
types
flip-flops, 105–112
latches, 105–112
DRAM, 257
registers, 108–109
register file, 261–262
SRAM, 257
virtual, 466. See also Virtual
memory
volatile, 259–261
word-addressable, 29. See Wordaddressable memory
Memory arrays, 257–266
area, 261
bit cells, 258
delay, 261
DRAM. See Dynamic random
access memory
HDL code for, 264–266
logic implementation using, 264.
See also Logic arrays
organization, 258
overview, 257–260
ports, 259
register files built using, 261–262
types, 259–260
DRAM. See Dynamic random
access memory
ROM. See Read only memory
SRAM. See Static random access
memory
Memory-mapped I/O (input/output),
494–498
address decoder, 495–496
communicating with I/O devices,
495–496
hardware, 495
speech synthesizer device driver,
498
speech synthesizer hardware,
496–497
SP0256, 496
Memory protection, 491
Memory systems, 463–512
caches. See Caches
IA-32, 499–502
MIPS, 470–478
overview, 463–467
performance analysis, 467–468
virtual memory. See Virtual
memory
Mercedes Benz, 268
Metal-oxide-semiconductor field effect
transistors (MOSFETs), 26–31.
See also CMOS, nMOS, pMOS,
transistors
Metastability, 143–144
MTBF. See Mean time between
failures
metastable state, 143
probability of failure, 145
resolution time, 144
synchronizers, 144–146
A Method of Synthesizing Sequential
Circuits (Mealy), 111
Index 563

Microarchitecture, 290, 363–461
advanced. See Advanced
microarchitecture
architectural state. See Architectural
State. See also Architecture
design process, 364–366
exception handling, 431–434
HDL representation, 421–431.
IA-32. See IA-32 microprocessor
instruction set. See Instruction set
MIPS. See MIPS microprocessor
overview, 363–366
performance analysis, 366–368
types,
advanced. See Advanced
microarchitecture
multicycle. See Multicycle MIPS
processor
pipelined. See Pipelined MIPS
processor
single-cycle. See Single-cycle
MIPS processor
Microprocessors, 3, 13
advanced. See Advanced
microarchitecture
chips. See Chips
clock frequencies, 124
IA-32. See IA-32 microprocessor
instructions. See MIPS instructions,
IA-32 instructions
MIPS. See MIPS microprocessor
Microsoft Windows, 501
Minterms, 54
Maxterms, 54
MIPS (Millions of instructions per
second). See Millions of
instructions per second
MIPS architecture. See MIPS
instruction set architecture (ISA)
MIPS assembly language. See also
MIPS instruction set
architecture
addressing modes, 327–329
assembler directives, 333
instructions, 290–292
logical instructions, 304–308
mnemonic, 291
operands, 292–298
procedure calls, 319–327
table of instructions, 336
translating machine language to, 303
translating to machine language, 300
MIPS instruction set architecture
(ISA)
addressing modes, 327–329
assembly language, 290–299
compiling, assembling, and loading,
330–335
exceptions, 337–338
floating-point instructions, 340–341
IA-32 instructions, 344–346
machine language, 299–304
MIPS instructions, 290–292
overview, 289–290
programming, 304–327
pseudoinstructions, 336–337
signed and unsigned instructions,
338–339
SPARC, 364
translating and starting a program.
See Translating and starting
a program
MIPS instructions, 551–554
formats
F-type, 340
I-type, 301–302
J-type, 302
R-type, 299–300
tables of, 552–554
opcodes, 552
R-type funct fields, 553–554
types,
arithmetic, 304–308
branching. See Branching
division, 308
floating-point, 340–341
logical, 304–308
multiplication, 308
pseudoinstructions, 336–337
MIPS microprocessor, 364
ALU, 242–244
multicycle. See Multicycle MIPS
processor
pipelined. See Pipelined MIPS
processor
single-cycle. See Single-cycle MIPS
processor
MIPS processor. See MIPS microprocessor
MIPS registers, 293–294, 308
nonpreserved, 322–324
preserved, 322–324
table of, 294
MIPS single-cycle HDL implementation, 421–431
building blocks, 426–428
controller, 423
datapath, 425
testbench, 429
top-level module, 430
Misses, 466, 481
AMAT. See Average memory access
time
cache, 466
capacity, 481
compulsory, 481
conflict, 481
page fault, 485
Miss penalty, 476
Miss rate, 467
Mnemonic, 291
Modeling, structural. See Structural
modeling
Modeling, behavioral. See Behavioral
modeling
Modularity, 6, 168
Modules, in HDL 167–168. See also
Hardware description
languages
behavioral, 168, 171
parameterized, 211–213
structural, 168
Moore, Edward F., 111
Moore, Gordon, 30
Moore machine, 126–129, 130,
208, 209
output table, 128
state transition diagram, 127
state transition table, 128
timing diagram, 131
Moore’s law, 30
MOSFETs. See Metal-oxide-semiconductor field effect transistors
Most significant bit (msb), 13
Most significant byte (MSB), 296
Move from hi (mfhi), 308
Move from lo (mflo), 308
Move from coprocessor 0 (mfc0), 338
msb. See Most significant bit
MSB. See Most significant byte
MTBF. See Mean time before failure
Multichip module (MCM), 499
Multicycle MIPS processor, 366,
381–400
control, 388–394
control FSM, 394–395
datapath, 382–388
performance analysis, 397–400
Multilevel combinational logic, 65–69.
See also Logic
Multilevel page table, 493
Multiplexers, 79–82, 175, 176, 428
564 Index

instance, 188
logic, 80–82
parameterized, 211. See also
Hardware description
languages
symbol and truth table, 79
timing, 87–88
with type conversion, 185
wide, 80
Multiplicand, 246
Multiplication, 246–247. See also
Arithmetic, Multiplier
architecture, 339
instructions, 308
Multiplier, 246–247
multiply (mult), 308, 339
multiply unsigned (multu), 339
Multiprocessors, 447
chip, 448
Multithreading, 446
Mux. See Multiplexers
N
NaN. See Not a number
Negation, 340. See also Taking the
two’s complement
Not a number (NaN), 251
NAND gate, 21, 32
nor, 179
NOR gate, 21, 32–33
NOT gate, 24, 172. See also Inverter
in HDL using always/process, 196
Nested procedure calls, 324–326
Netlist, 170
Next state, 115
Nibbles, 13–14
nMOS, 28–31
nMOS transistors, 28–31
No operation. See nop
Noise margins, 23, 24
nop, 336
Noyce, Robert, 26
Number conversion, 9–19 See also
Number systems, Binary numbers
binary to decimal, 10–11
binary to hexadecimal, 12
decimal to binary, 11
decimal to hexadecimal, 13
taking the two’s complement, 15, 240
Number systems, 9–19, 249–253
addition. See Addition
binary numbers. See Binary numbers
comparison of, 18–19
conversion of. See Number
conversions
decimal numbers. See Decimal
numbers
estimating powers of two, 14
fixed-point, 249–250. See Fixedpoint numbers
floating-point, 250–253. See
Floating-point numbers
in hardware description languages,
179
hexadecimal numbers. See
Hexadecimal numbers
negative and positive, 15–19
rounding, 252. See also Floatingpoint numbers
sign bit, 16
signed, 15–19. See also Signed
binary numbers
sign/magnitude numbers. See
Sign/magnitude numbers
two’s complement numbers. See
Two’s complement
numbers
unsigned,
O
OAI gate. See Or-and-invert gate
Object files, 331–334
Octal numbers, 180
OFF, 23. See also 0, LOW
Offset, 295–296
ON, 23. See also 1, HIGH, Asserted
One-cold, 124
One-hot, 82
Opcode, 299
Operands, 292–298
Operators
bitwise, 171–174
or immediate (ori), 308
OR gate, 21
Or-and-invert (OAI) gate, 43
precedence table of, 179
reduction, 174
Out-of-order execution, 443
Out-of-order processor, 441–443
Output devices, 466
Output terminals, 51
Overflow, 15
detecting, 15
with addition, 15
P
Page, 485
size, 485
Page fault, 485
Page offset, 486–488
Page table, 486
Parity, 22
Parallelism, 149–153
pipelining. See Pipelining, Pipelined
MIPS processor
SIMD. See Single instruction
multiple data unit
spatial and temporal, 151
vector processor. See Vector processor
Patterson, David, 290, 364
PCBs. See Printed circuit boards
PC-relative addressing, 327–328.
See also Addressing modes
PCSrc, 281, 387–390
PCWrite, 385
Pentium processors, 449–452. See also
Intel, IA-32
Pentium 4, 452
Pentium II, 450, 499
Pentium III, 450, 451, 499
Pentium M, 453
Pentium Pro, 450, 499, 501
Perfect induction, 60
Performance, 366–368
Peripheral devices. See I/O devices
Perl programming language, 20
Physical memory, 466, 485–486
Physical pages, 486–494
Physical page number, 486
Pipelined MIPS processor, 151, 366,
401–421. See also MIPS,
Architecture, Microarchitecture
control, 405–406
datapath, 404
forwarding, 408–410
hazards. See Hazards
performance analysis, 418–421
processor performance
comparison, 420
stalls, 410–413. See also Hazards
timing, 402
Index 565

Pipelining hazards. See Hazards
PLAs. See Programmable logic arrays
Plastic leaded chip carriers (PLCCs),
531–532
PLCCs. See Plastic leaded chip carriers
PLDs. See Programmable logic devices
pMOS, 29–30
pMOS transistors, 28–31
Pointer, 321
global, 331
stack, 321
Pop, 345–346. See also Stack
Ports, 259
POS. See Product of sums form
Power consumption, 34–35
Prediction. See Branch prediction
Prefix adder, 237–239
Preserved registers, 322–324
Prime implicants, 61, 74
Printed circuit boards (PCBs),
533–534
Procedure calls, 319–327
arguments and variables, 326–327
nested, 324–326
preserved versus nonpreserved
registers, 323–324
returns, 319–320
return values, 320
stack frame, 322
stack usage, 321–322
Processors. See Microprocessors
Product-of-sums (POS) canonical
form, 56
Program counter (PC), 365
Programmable logic arrays (PLAs), 63,
266–268, 520–521
Programmable logic devices
(PLDs), 268
Programmable read only memories
(PROMs), 516–520. See also
Read only memories
Programming, 304–327
arithmetic/logical instructions. See
Arithmetic, 304–308
arrays. See Arrays
branching. See Branching
conditional statements. See
Conditional statements
constants, 307–308
immediates, 298
loops. See Loops
procedure calls. See Procedure calls
shift instructions, 306–307
translating and starting a program,
331–335
Programming languages, 290–294
Propagation delay, 84
Protection, memory. See Memory
protection
Proving Boolean theorems. See Perfect
induction
PROMs. See Programmable read only
memories
Pseudo-direct addressing, 328–329.
See also Addressing modes
Pseudo-nMOS logic, 33–34. See also
Transistors
Pseudoinstructions, 336–337
Push, 67–69. See also Stack
Q
Queue, 321
FIFO. See First-in-first-out queue
LIFO. See Last-in-first-out queue
Q output. See Sequential logic design
R
R-type instructions, 299–300
RAM. See Random access memory
Random access memory (RAM), 257,
262–264. See also Memory
arrays
synthesized, 265
Read only memory, 199, 257, 262–264.
See also Memory arrays
EPROM. See Erasable programmable read only memory
EEPROM. See Electrically erasable
programmable read only
memory
flash memory. See Flash memory
PROM. See Programmable read
only memory
transistor-level implementation, 273
Read/write head, 484
Recursive procedure calls, 324–325.
See also Procedure calls
Reduced instruction set computer
(RISC), 292, 364
Reduction operators, 174. See also
Hardware description languages,
Verilog
RegDst, 373–374
Register-only addressing, 327. See also
Addressing modes
Register renaming, 443–445. See also
Advanced microarchitecture
Register(s), 190–191, 261–262,
292–294. See also Flip-flops,
Register file
arguments ($a0-$a3)
assembler temporary ($at)
enabled. See Enabled registers
file, 261
global pointer ($gp), 331
multiple, 194–195
program counter (PC), 365
preserved and nonpreserved,
322–324
renaming, 443–445
resettable. See Resettable registers
asynchronous, 192
synchronous, 192
return address ($ra), 319–320
Register set, 294. See also Register file,
MIPS registers, IA-32 registers
Regularity, 6, 188
RegWrite, 371
Replacement policies, 492. See also
Caches, Virtual memory
Resettable registers,191–193
asynchronous. See Asynchronous
resettable registers
synchronous. See Synchronous
resettable registers
Return address ($ra), 319–320
Ripple-carry adder, 234
RISC. See Reduced instruction set
computer
ROM, See Read Only Memory
Rotators, 244–246
Rounding, 252
S
Scalar processor, 438
Scan chains, 255. See also Shift registers
Schematic, 62–65
Scientific notation, 249–250
566 Index

Seek time, 484
Segments, memory, 330–331
Semiconductor industry, sales, 3
Semiconductors, 27. See also
Transistors
CMOS. See Complementary metal
oxide silicon
diodes. See Diodes
transistors. See Transistors
MOSFET. See Metal oxide silicon
field effect transistors
nMOS. See nMOS transistors
pMOS. See pMOS transistors
pseudo nMOS. See Pseudo nMOS
Sensitivity list, 190, 191, 196
Sequential building blocks.
See Sequential logic
Sequential logic, 103–165, 190–195,
254–257
enabled registers, 193
latches, 103–112, 195
multiple registers, 194–195
overview, 103
registers, 190–191
shift registers, 255–257
synchronous, 113–117
timing of, 133–149. See also Timing
set if less than (slt), 313
set if less than immediate (slti), 339
set if less than immediate unsigned
(sltiu), 339
set if less than unsigned (sltu), 339
Setup time, 133, 135–136
Seven-segment display decoder, 75–77
with “don’t cares,” 78
Shared memory, 71
Shift amount (shamt), 245
shift left logical (sll), 306
shift left logical variable (sllv), 306
shift right arithmetic (sra), 306
shift right arithmetic variable (srav), 306
shift right logical (srl), 306
shift right logical variable (srlv), 306
Shifters, 244–246
arithmetic, 244
logical, 244
Shift instructions, 306–307
Shift registers, 255–257
Short path, 86
Sign/magnitude numbers, 15–16
Sign bit, 16
Sign extension, 18
Significand. See Mantissa
Silicon (Si), 27
Silicon dioxide (SO2), 28
Silicon Valley, 26
Simplicity, 291–292
SIMD. See Single instruction multiple
data units
Single-cycle MIPS processor, 366,
368–381. See also MIPS microprocessor, MIPS architecture,
MIPS microarchitecture
control, 374–377
ALU decoder truth table. See
ALU decoder
Main decoder truth table. See
Main decoder
datapath, 368–374
HDL representation, 422
operation, 376–377 performance
analysis, 380–381
timing, 402
Single instruction multiple data (SIMD)
units, 438, 445
Slash notation, 53
SPARC architecture, 364
SRAM. See Static random access
memory
SP0256, 496
Spatial locality, 464
Speech synthesis, 496–498
device driver, 498
SP0256, 496
Stack, 321–322. See also Memory map,
Procedure calls, Queue
dynamic data segment, 331
frame, 322
LIFO. See Last-in-first-out queue
pointer, 321
Stalls, 410–413. See also Hazards
Static discipline, 24–26
Static random access memory (SRAM),
257, 260
Status flags, 344
Stored program concept, 303–304
Stores
store byte (sb), 318
store half (sh), 553
store word (sw), 296
Strings, 318
Structural modeling, 185–189
subtract (sub), 291
subtract unsigned (subu), 339
Subtraction, 17–18, 240, 291
Subtractor, 240
Sum-of-products (SOP) canonical form,
54–55
Sun Microsystems, 364
Superscalar processor, 438–440
Supply voltage, 22
Swap space, 492
Swift, Jonathan, 297
Switch/case statements, 311
Symbol table, 333
Synchronizers, 144–146
asynchronous inputs, 146
MTBF. See Mean time before
failure
probability of failure. See
Probability of failure
Synchronous resettable registers, 192
HDL for, 192
Synchronous sequential logic,
113–117
problematic circuits, 113–114
Synthesis, 78–79, 186, 187, 188, 189,
190, 193, 194, 195, 199, 200
Synthesis Tools, 195
T
Taking the two’s complement, 16
Temporal locality, 464
Testbenches, 171, 214–218, 428–431
self-checking, 215
with test vector file, 216
Text segment, 330
Theorems. See Boolean Theorems
Thin small outline package (TSOP), 531
Threshold voltage, 29
Throughput, 149
Timing
analysis, 137–138
delay, 86–87
glitches, 88–91
of sequential logic, 133–149
clock skew, 140–143
dynamic discipline, 134
hold time. See Hold time
hold time constraint, 136–137.
See Hold time constraint,
Hold time violation
hold time violation. See Hold
time violation, Hold time
constraint, 139
Index 567

Timing (Continued)
metastability, 143–144
setup time. See Setup time
setup time constraint,
135–136
setup time violations
resolution time, 146–149. See
also Metastability
synchronizers, 144–146
system timing, 135–140
specification, 51
TLB. See Translation lookaside
buffer
Token, 149
Transistors, 23, 28–31, 34
CMOS. See Complement metal
oxide silicon
nMOS. See nMOS
pMOS. See pMOS
Transistor-Transistor Logic (TTL), 25
Translation lookaside buffer (TLB),
490
Translating and starting a program,
331–336
Transmission gates, 33. See also
Transistors
Transmission lines, 534–546
reflection coefficient, 544–545
Z0, 543–544
matched termination,
536–538
mismatched termination,
539–541
open termination, 538–539
proper terminations, 542
short termination, 539
when to use, 41–542
Tristate buffer, 70–71
Truth tables, 55, 56, 60, 61, 177
ALU decoder, 376
“don’t care,” 77
main decoder, 376, 379
multiplexer, 79
seven-segment display decoder, 76
SR latch, 106
with undefined and floating inputs,
181
TSOP. See Thin small outline
package
TTL. See Transistor-Transistor Logic
Two-level logic, 65–66
Two’s complement numbers, 16–18.
See also Binary numbers
U
Unconditional branches, 308. See also
Jumps
Unicode, 316. See also ASCII
Unit under test (UUT), 201
Unity gain points, 24
Unsigned numbers, 18
Use bit, 478–479
UUT. See Unit under test
V
Valid bit, 472. See also Caches, Virtual
memory
Vanity Fair (Carroll), 65
VCC, 23
VDD, 23
Vector processors, 438. See also
Advanced microarchitecture
Verilog, 167, 169, 172, 173, 174, 175,
176, 178, 180, 181, 201, 203,
205
3:8 decoder, 199
accessing parts of busses, 189
adder, 426
ALU decoder, 424
AND, 168
architecture body, 168
assign statement, 168, 197, 178
asynchronous reset, 192
bad synchronizer with blocking
assignments, 206
bit swizzling, 182
blocking assignment, 202
case sensitivity, 174
casez, 201
combinational logic, 168, 202
comments, 174
comparators, 242
continuous assignment statement,
173
controller, 423
counter, 254
datapath, 425
default, 198
divide-by-3 finite state machine,
207, 208
D latch, 195
eight-input AND, 174
entity declaration, 168
full adder, 178
using always/process, 197
using nonblocking assignments,
204
IEEE_STD_LOGIC_1164, 168, 183
IEEE_STD_LOGIC_SIGNED, 183
IEEE_STD_LOGIC_UNSIGNED,
183
inverters, 172
using always/process, 196
left shift, 427
library use clause, 168
logic gates, 173
with delays, 183
main decoder, 424
MIPS testbench, 429
MIPS top-level module, 430
multiplexers, 175, 176, 428
multiplier, 247
nonblocking assignment, 191,
202
NOT, 168
numbers, 180
operator precedence, 179
OR, 168
parameterized
N:2N decoder, 212
N-bit multiplexer, 211
N-input AND gate, 213
pattern recognizer
Mealy FSM, 210
Moore FSM, 209
priority circuit, 201
RAM, 265
reg, 191, 194
register, 191
register file, 42
resettable flip-flop, 427
resettable enabled register, 193
resettable register, 192
ROM, 266
self-checking testbench, 215
seven-segment display decoder,
198
shift register, 256
sign extension, 427
single-cycle MIPS processor,
422
STD_LOGIC, 168, 183
statement, 173
structural models
568 Index

2:1 multiplexer, 188
4:1 multiplexer, 187
subtractor, 241
synchronizer, 194
synchronous reset, 192
testbench, 214
with test vector file, 216
tristate buffer, 180
truth tables with undefined and
floating inputs, 181
type declaration, 168
wires, 178
VHDL libraries and types, 167, 169,
172, 173, 174, 175, 176, 178,
180, 181, 183–185, 203, 205
3:8 decoder, 199
accessing parts of busses, 189
adder, 426
architecture, 187
asynchronous reset, 192
bad synchronizer with blocking
assignments, 206
bit swizzling, 182
boolean, 183
case sensitivity, 174
clk’event, 191
combinational logic, 168
comments, 174
comparators, 242
concurrent signal assignment, 173,
178
controller, 423
CONV_STD_LOGIC_VECTOR,
212
counter, 254
datapath, 425
decoder, 424
divide-by-3 finite state machine,
207, 208
D latch, 195
eight-input AND, 174
expression, 173
full adder, 178
using always/process, 197
using nonblocking assignments,
204
generic statement, 211
inverters, 172
using always/process, 196
left shift, 427
logic gates, 173
with delays, 183
main decoder, 424
MIPS testbench, 429
MIPS top-level module, 430
multiplexers, 175, 176, 428
multiplier, 247
numbers, 180
operand, 173
operator precedence, 179
others, 198
parameterized
N-bit multiplexer, 211
N-input AND gate, 213
N:2N decoder, 212
pattern recognizer
Mealy FSM, 210
Moore FSM, 209
priority circuit, 201
process, 191
RAM, 265
register, 191
register file, 426
resettable enabled register, 193
resettable flip-flop, 427
resettable register, 192
RISING_EDGE, 191
ROM, 266
selected signal assignment statements, 176
self-checking testbench, 215
seven-segment display decoder, 198
shift register, 256
sign extension, 427
signals, 178, 194
simulation waveforms with delays,
170, 183
single-cycle MIPS processor, 422
STD_LOGIC_ARITH, 212
structural models
2:1 multiplexer, 188
4:1 multiplexer, 187
subtractor, 241
synchronizer, 194
synchronous reset, 192
testbench, 214
with test vector file, 216
tristate buffer, 180
truth tables with undefined and
floating inputs, 181
VHSIC, 169
Virtual address, 485–490
Virtual memory, 484–494
address translation, 486–488
IA-32, 501
memory protection, 491
pages, 485
page faults, 485
page offset, 486–488
page table, 488–489
multilevel page tables, 492–494
replacement policies, 492
translation lookaside buffer (TLB),
490. See Translation lookaside buffer
write policies, 482–483
Virtual pages, 485
Virtual page number, 487
Volatile memory, 259. See also DRAM,
SRAM, Flip-flops
Voltage, threshold, 29
VSS, 23
W
Wafers, 28
Wall, Larry, 20
WAR. See Write after read
WAW. See Write after write
While loop, 312–313
White space, 174
Whitmore, Georgiana, 7
Wire, 63
Word-addressable memory, 295
Write policies, 482–483
Write after read (WAR) hazard, 442.
See also Hazards
Write after write (WAW) hazard,
442–443. See also Hazards
X
XOR gate, 21
XNOR gate, 21
Xilinx FPGA, 268
X. See Contention, Don’t care.,
Z
Z,. See Floating
Zero extension, 302
Index 569
In Praise of Digital Design
and Computer Architecture
Harris and Harris have taken the popular pedagogy from Computer
Organization and Design to the next level of refinement, showing in
detail how to build a MIPS microprocessor in both Verilog and VHDL.
Given the exciting opportunity that students have to run large digital
designs on modern FGPAs, the approach the authors take in this book is
both informative and enlightening.
David A. Patterson University of California, Berkeley
Digital Design and Computer Architecture brings a fresh perspective to
an old discipline. Many textbooks tend to resemble overgrown shrubs,
but Harris and Harris have managed to prune away the deadwood while
preserving the fundamentals and presenting them in a contemporary context. In doing so, they offer a text that will benefit students interested in
designing solutions for tomorrow’s challenges.
Jim Frenzel University of Idaho
Harris and Harris have a pleasant and informative writing style.
Their treatment of the material is at a good level for introducing students
to computer engineering with plenty of helpful diagrams. Combinational
circuits, microarchitecture, and memory systems are handled particularly well.
James Pinter-Lucke Claremont McKenna College
Harris and Harris have written a book that is very clear and easy to
understand. The exercises are well-designed and the real-world examples
are a nice touch. The lengthy and confusing explanations often found in
similar textbooks are not seen here. It’s obvious that the authors have
devoted a great deal of time and effort to create an accessible text.
I strongly recommend Digital Design and Computer Architecture.
Peiyi Zhao Chapman University

Harris and Harris have created the first book that successfully combines
digital system design with computer architecture. Digital Design and
Computer Architecture is a much-welcomed text that extensively explores
digital systems designs and explains the MIPS architecture in fantastic
detail. I highly recommend this book.
James E. Stine, Jr., Oklahoma State University
Digital Design and Computer Architecture is a brilliant book. Harris and
Harris seamlessly tie together all the important elements in microprocessor design—transistors, circuits, logic gates, finite state machines, memories, arithmetic units—and conclude with computer architecture. This
text is an excellent guide for understanding how complex systems can be
flawlessly designed.
Jaeha Kim Rambus, Inc.
Digital Design and Computer Architecture is a very well-written book
that will appeal to both young engineers who are learning these subjects
for the first time and also to the experienced engineers who want to use
this book as a reference. I highly recommend it.
A. Utku Diril Nvidia Corporation

Digital Design and
Computer Architecture

About the Authors
David Money Harris is an associate professor of engineering at Harvey
Mudd College. He received his Ph.D. in electrical engineering from
Stanford University and his M.Eng. in electrical engineering and computer science from MIT. Before attending Stanford, he worked at Intel as
a logic and circuit designer on the Itanium and Pentium II processors.
Since then, he has consulted at Sun Microsystems, Hewlett-Packard,
Evans & Sutherland, and other design companies.
David’s passions include teaching, building chips, and exploring the
outdoors. When he is not at work, he can usually be found hiking,
mountaineering, or rock climbing. He particularly enjoys hiking with his
son, Abraham, who was born at the start of this book project. David
holds about a dozen patents and is the author of three other textbooks
on chip design, as well as two guidebooks to the Southern California
mountains.
Sarah L. Harris is an assistant professor of engineering at Harvey Mudd
College. She received her Ph.D. and M.S. in electrical engineering from
Stanford University. Before attending Stanford, she received a B.S. in electrical and computer engineering from Brigham Young University. Sarah
has also worked with Hewlett-Packard, the San Diego Supercomputer
Center, Nvidia, and Microsoft Research in Beijing.
Sarah loves teaching, exploring and developing new technologies,
traveling, wind surfing, rock climbing, and playing the guitar. Her recent
exploits include researching sketching interfaces for digital circuit design,
acting as a science correspondent for a National Public Radio affiliate,
and learning how to kite surf. She speaks four languages and looks forward to adding a few more to the list in the near future.

Digital Design and
Computer Architecture
David Money Harris
Sarah L. Harris
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Morgan Kaufmann Publishers is an imprint of Elsevier

Publisher: Denise E. M. Penrose
Senior Developmental Editor: Nate McFadden
Publishing Services Manager: George Morrison
Project Manager: Marilyn E Rash
Assistant Editor: Mary E. James
Editorial Assistant: Kimberlee Honjo
Cover and Editorial Illustrations: Duane Bibby
Interior Design: Frances Baca Design
Composition: Integra
Technical Illustrations: Harris and Harris/Integra
Production Services: Graphic World Inc.
Interior Printer: Courier-Westford
Cover Printer: Phoenix Color Corp.
Morgan Kaufmann Publishers is an imprint of Elsevier.
500 Sansome Street, Suite 400, San Francisco, CA 94111
This book is printed on acid-free paper.
© 2007 by Elsevier Inc. All rights reserved.
Designations used by companies to distinguish their products are often claimed as trademarks or
registered trademarks. In all instances in which Morgan Kaufmann Publishers is aware of a claim,
the product names appear in initial capital or all capital letters. Readers, however, should contact
the appropriate companies for more complete information regarding trademarks and registration.
Permissions may be sought directly from Elsevier’s Science & Technology Rights Department
in Oxford, UK: phone: (44) 1865 843830, fax: (44) 1865 853333, e-mail: permissions
@elsevier.co.uk. You may also complete your request on-line via the Elsevier homepage
(http://elsevier.com) by selecting “Customer Support” and then “Obtaining Permissions.”
Library of Congress Cataloging-in-Publication Data
Harris, David Money.
Digital design and computer architecture / David Money Harris and
Sarah L. Harris.—1st ed.
p. cm.
Includes bibliographical references and index.
ISBN 13: 978-0-12-370497-9 (alk. paper)
ISBN 10: 0-12-370497-9
1. Digital electronics. 2. Logic design. 3. Computer architecture. I. Harris, Sarah L. II. Title.
TK7868.D5H298 2007
621.381—dc22
2006030554
For information on all Morgan Kaufmann publications,
visit our Web site at www.mkp.com
Printed in the United States of America
07 08 09 10 11 5 4 3 2 1
Pr
To my family, Jennifer and Abraham
– DMH
To my sisters, Lara and Jenny
– SLH


Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviii
Online Supplements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
How to Use the Software Tools in a Course . . . . . . . . . . . . . . xix
Labs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xx
Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
Chapter 1 From Zero to One . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1 The Game Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 The Art of Managing Complexity . . . . . . . . . . . . . . . . . . . . . 4
1.2.1 Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.2 Discipline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.3 The Three -Y’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 The Digital Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 Number Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.1 Decimal Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.2 Binary Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4.3 Hexadecimal Numbers . . . . . . . . . . . . . . . . . . . . . . 11
1.4.4 Bytes, Nibbles, and All That Jazz . . . . . . . . . . . . . . 13
1.4.5 Binary Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.4.6 Signed Binary Numbers . . . . . . . . . . . . . . . . . . . . . . 15
1.5 Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.5.1 NOT Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5.2 Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5.3 AND Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.5.4 OR Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.5.5 Other Two-Input Gates . . . . . . . . . . . . . . . . . . . . . . 21
1.5.6 Multiple-Input Gates . . . . . . . . . . . . . . . . . . . . . . . . 21
1.6 Beneath the Digital Abstraction . . . . . . . . . . . . . . . . . . . . . . . 22
1.6.1 Supply Voltage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.6.2 Logic Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.6.3 Noise Margins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.6.4 DC Transfer Characteristics . . . . . . . . . . . . . . . . . . . 23
1.6.5 The Static Discipline . . . . . . . . . . . . . . . . . . . . . . . . 24
ix

1.7 CMOS Transistors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
1.7.1 Semiconductors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.7.2 Diodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.7.3 Capacitors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.7.4 nMOS and pMOS Transistors . . . . . . . . . . . . . . . . . 28
1.7.5 CMOS NOT Gate . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.7.6 Other CMOS Logic Gates . . . . . . . . . . . . . . . . . . . . 31
1.7.7 Transmission Gates . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.7.8 Pseudo-nMOS Logic . . . . . . . . . . . . . . . . . . . . . . . . 33
1.8 Power Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.9 Summary and a Look Ahead . . . . . . . . . . . . . . . . . . . . . . . . . 35
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Chapter 2 Combinational Logic Design . . . . . . . . . . . . . . . . . . . . 51
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.2 Boolean Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.1 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.2 Sum-of-Products Form . . . . . . . . . . . . . . . . . . . . . . . 54
2.2.3 Product-of-Sums Form . . . . . . . . . . . . . . . . . . . . . . . 56
2.3 Boolean Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.3.1 Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.3.2 Theorems of One Variable . . . . . . . . . . . . . . . . . . . . 57
2.3.3 Theorems of Several Variables . . . . . . . . . . . . . . . . . 58
2.3.4 The Truth Behind It All . . . . . . . . . . . . . . . . . . . . . . 60
2.3.5 Simplifying Equations . . . . . . . . . . . . . . . . . . . . . . . 61
2.4 From Logic to Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
2.5 Multilevel Combinational Logic . . . . . . . . . . . . . . . . . . . . . . . 65
2.5.1 Hardware Reduction . . . . . . . . . . . . . . . . . . . . . . . . 66
2.5.2 Bubble Pushing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
2.6 X’s and Z’s, Oh My . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.6.1 Illegal Value: X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.6.2 Floating Value: Z . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
2.7 Karnaugh Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
2.7.1 Circular Thinking . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.7.2 Logic Minimization with K-Maps . . . . . . . . . . . . . . . 73
2.7.3 Don’t Cares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
2.7.4 The Big Picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
2.8 Combinational Building Blocks . . . . . . . . . . . . . . . . . . . . . . . 79
2.8.1 Multiplexers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
2.8.2 Decoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
2.9 Timing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
2.9.1 Propagation and Contamination Delay . . . . . . . . . . 84
2.9.2 Glitches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
x CONTENTS

2.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
Chapter 3 Sequential Logic Design . . . . . . . . . . . . . . . . . . . . . . 103
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.2 Latches and Flip-Flops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.2.1 SR Latch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.2.2 D Latch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.2.3 D Flip-Flop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.2.4 Register . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.2.5 Enabled Flip-Flop . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.2.6 Resettable Flip-Flop . . . . . . . . . . . . . . . . . . . . . . . . 110
3.2.7 Transistor-Level Latch and Flip-Flop Designs . . . . 110
3.2.8 Putting It All Together . . . . . . . . . . . . . . . . . . . . . . 112
3.3 Synchronous Logic Design . . . . . . . . . . . . . . . . . . . . . . . . . . 113
3.3.1 Some Problematic Circuits . . . . . . . . . . . . . . . . . . . 113
3.3.2 Synchronous Sequential Circuits . . . . . . . . . . . . . . . 114
3.3.3 Synchronous and Asynchronous Circuits . . . . . . . . 116
3.4 Finite State Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.4.1 FSM Design Example . . . . . . . . . . . . . . . . . . . . . . . 117
3.4.2 State Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.4.3 Moore and Mealy Machines . . . . . . . . . . . . . . . . . . 126
3.4.4 Factoring State Machines . . . . . . . . . . . . . . . . . . . . 129
3.4.5 FSM Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
3.5 Timing of Sequential Logic. . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.5.1 The Dynamic Discipline . . . . . . . . . . . . . . . . . . . . . 134
3.5.2 System Timing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
3.5.3 Clock Skew . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
3.5.4 Metastability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
3.5.5 Synchronizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
3.5.6 Derivation of Resolution Time . . . . . . . . . . . . . . . . 146
3.6 Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
Chapter 4 Hardware Description Languages . . . . . . . . . . . . . . 167
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
4.1.1 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
4.1.2 Language Origins . . . . . . . . . . . . . . . . . . . . . . . . . . 168
4.1.3 Simulation and Synthesis . . . . . . . . . . . . . . . . . . . . 169
CONTENTS xi

4.2 Combinational Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.2.1 Bitwise Operators . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.2.2 Comments and White Space . . . . . . . . . . . . . . . . . . 174
4.2.3 Reduction Operators . . . . . . . . . . . . . . . . . . . . . . . 174
4.2.4 Conditional Assignment . . . . . . . . . . . . . . . . . . . . . 175
4.2.5 Internal Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 176
4.2.6 Precedence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
4.2.7 Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.2.8 Z’s and X’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.2.9 Bit Swizzling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.2.10 Delays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.2.11 VHDL Libraries and Types . . . . . . . . . . . . . . . . . . 183
4.3 Structural Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
4.4 Sequential Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.4.1 Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.4.2 Resettable Registers . . . . . . . . . . . . . . . . . . . . . . . . 191
4.4.3 Enabled Registers . . . . . . . . . . . . . . . . . . . . . . . . . . 193
4.4.4 Multiple Registers . . . . . . . . . . . . . . . . . . . . . . . . . . 194
4.4.5 Latches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
4.5 More Combinational Logic . . . . . . . . . . . . . . . . . . . . . . . . . 195
4.5.1 Case Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
4.5.2 If Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.5.3 Verilog casez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
4.5.4 Blocking and Nonblocking Assignments . . . . . . . . 201
4.6 Finite State Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
4.7 Parameterized Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
4.8 Testbenches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
4.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Chapter 5 Digital Building Blocks . . . . . . . . . . . . . . . . . . . . . . . 233
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.2 Arithmetic Circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.2.1 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.2.2 Subtraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
5.2.3 Comparators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
5.2.4 ALU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
5.2.5 Shifters and Rotators . . . . . . . . . . . . . . . . . . . . . . . 244
5.2.6 Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
5.2.7 Division . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
5.2.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
xii CONTENTS

5.3 Number Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
5.3.1 Fixed-Point Number Systems . . . . . . . . . . . . . . . . . 249
5.3.2 Floating-Point Number Systems . . . . . . . . . . . . . . . 250
5.4 Sequential Building Blocks. . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.4.1 Counters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.4.2 Shift Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
5.5 Memory Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
5.5.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
5.5.2 Dynamic Random Access Memory . . . . . . . . . . . . . 260
5.5.3 Static Random Access Memory . . . . . . . . . . . . . . . 260
5.5.4 Area and Delay . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
5.5.5 Register Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
5.5.6 Read Only Memory . . . . . . . . . . . . . . . . . . . . . . . . 262
5.5.7 Logic Using Memory Arrays . . . . . . . . . . . . . . . . . 264
5.5.8 Memory HDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
5.6 Logic Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
5.6.1 Programmable Logic Array . . . . . . . . . . . . . . . . . . 266
5.6.2 Field Programmable Gate Array . . . . . . . . . . . . . . 268
5.6.3 Array Implementations . . . . . . . . . . . . . . . . . . . . . . 273
5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
Chapter 6 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
6.2 Assembly Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
6.2.1 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
6.2.2 Operands: Registers, Memory, and
Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
6.3 Machine Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.3.1 R-type Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.3.2 I-type Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 301
6.3.3 J-type Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 302
6.3.4 Interpreting Machine Language Code . . . . . . . . . . 302
6.3.5 The Power of the Stored Program . . . . . . . . . . . . . 303
6.4 Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
6.4.1 Arithmetic/Logical Instructions . . . . . . . . . . . . . . . 304
6.4.2 Branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
6.4.3 Conditional Statements . . . . . . . . . . . . . . . . . . . . . 310
6.4.4 Getting Loopy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
6.4.5 Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
6.4.6 Procedure Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
6.5 Addressing Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
CONTENTS xiii

6.6 Lights, Camera, Action: Compiling, Assembling,
and Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
6.6.1 The Memory Map . . . . . . . . . . . . . . . . . . . . . . . . . 330
6.6.2 Translating and Starting a Program . . . . . . . . . . . . 331
6.7 Odds and Ends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
6.7.1 Pseudoinstructions . . . . . . . . . . . . . . . . . . . . . . . . . 336
6.7.2 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
6.7.3 Signed and Unsigned Instructions . . . . . . . . . . . . . . 338
6.7.4 Floating-Point Instructions . . . . . . . . . . . . . . . . . . . 340
6.8 Real-World Perspective: IA-32 Architecture . . . . . . . . . . . . . 341
6.8.1 IA-32 Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
6.8.2 IA-32 Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
6.8.3 Status Flags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
6.8.4 IA-32 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 344
6.8.5 IA-32 Instruction Encoding . . . . . . . . . . . . . . . . . . 346
6.8.6 Other IA-32 Peculiarities . . . . . . . . . . . . . . . . . . . . 348
6.8.7 The Big Picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
6.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
Chapter 7 Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . 363
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
7.1.1 Architectural State and Instruction Set . . . . . . . . . . 363
7.1.2 Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
7.1.3 MIPS Microarchitectures . . . . . . . . . . . . . . . . . . . . 366
7.2 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
7.3 Single-Cycle Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
7.3.1 Single-Cycle Datapath . . . . . . . . . . . . . . . . . . . . . . 368
7.3.2 Single-Cycle Control . . . . . . . . . . . . . . . . . . . . . . . . 374
7.3.3 More Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 377
7.3.4 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . 380
7.4 Multicycle Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
7.4.1 Multicycle Datapath . . . . . . . . . . . . . . . . . . . . . . . . 382
7.4.2 Multicycle Control . . . . . . . . . . . . . . . . . . . . . . . . . 388
7.4.3 More Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 395
7.4.4 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . 397
7.5 Pipelined Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
7.5.1 Pipelined Datapath . . . . . . . . . . . . . . . . . . . . . . . . . 404
7.5.2 Pipelined Control . . . . . . . . . . . . . . . . . . . . . . . . . . 405
7.5.3 Hazards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
7.5.4 More Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 418
7.5.5 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . 418
xiv CONTENTS

7.6 HDL Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
7.6.1 Single-Cycle Processor . . . . . . . . . . . . . . . . . . . . . . 422
7.6.2 Generic Building Blocks . . . . . . . . . . . . . . . . . . . . . 426
7.6.3 Testbench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
7.7 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
7.8 Advanced Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . 435
7.8.1 Deep Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
7.8.2 Branch Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 437
7.8.3 Superscalar Processor . . . . . . . . . . . . . . . . . . . . . . . 438
7.8.4 Out-of-Order Processor . . . . . . . . . . . . . . . . . . . . . 441
7.8.5 Register Renaming . . . . . . . . . . . . . . . . . . . . . . . . . 443
7.8.6 Single Instruction Multiple Data . . . . . . . . . . . . . . 445
7.8.7 Multithreading . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
7.8.8 Multiprocessors . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
7.9 Real-World Perspective: IA-32 Microarchitecture . . . . . . . . . 447
7.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
Chapter 8 Memory Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
8.2 Memory System Performance Analysis . . . . . . . . . . . . . . . . . 467
8.3 Caches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
8.3.1 What Data Is Held in the Cache? . . . . . . . . . . . . . . 469
8.3.2 How Is the Data Found? . . . . . . . . . . . . . . . . . . . . 470
8.3.3 What Data Is Replaced? . . . . . . . . . . . . . . . . . . . . . 478
8.3.4 Advanced Cache Design . . . . . . . . . . . . . . . . . . . . . 479
8.3.5 The Evolution of MIPS Caches . . . . . . . . . . . . . . . 483
8.4 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
8.4.1 Address Translation . . . . . . . . . . . . . . . . . . . . . . . . 486
8.4.2 The Page Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
8.4.3 The Translation Lookaside Buffer . . . . . . . . . . . . . 490
8.4.4 Memory Protection . . . . . . . . . . . . . . . . . . . . . . . . 491
8.4.5 Replacement Policies . . . . . . . . . . . . . . . . . . . . . . . 492
8.4.6 Multilevel Page Tables . . . . . . . . . . . . . . . . . . . . . . 492
8.5 Memory-Mapped I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
8.6 Real-World Perspective: IA-32 Memory and I/O Systems . . . 499
8.6.1 IA-32 Cache Systems . . . . . . . . . . . . . . . . . . . . . . . 499
8.6.2 IA-32 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . 501
8.6.3 IA-32 Programmed I/O . . . . . . . . . . . . . . . . . . . . . 502
8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
Interview Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
CONTENTS xv

Appendix A Digital System Implementation . . . . . . . . . . . . . . . 515
A.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
A.2 74xx Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
A.2.1 Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.2.2 Other Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.3 Programmable Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.3.1 PROMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
A.3.2 PLAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
A.3.3 FPGAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
A.4 Application-Specific Integrated Circuits . . . . . . . . . . . . . . . . 523
A.5 Data Sheets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
A.6 Logic Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529
A.7 Packaging and Assembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531
A.8 Transmission lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
A.8.1 Matched Termination . . . . . . . . . . . . . . . . . . . . . . . 536
A.8.2 Open Termination . . . . . . . . . . . . . . . . . . . . . . . . . 538
A.8.3 Short Termination . . . . . . . . . . . . . . . . . . . . . . . . . 539
A.8.4 Mismatched Termination . . . . . . . . . . . . . . . . . . . . 539
A.8.5 When to Use Transmission Line Models . . . . . . . . . 542
A.8.6 Proper Transmission Line Terminations . . . . . . . . . 542
A.8.7 Derivation of Z0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
A.8.8 Derivation of the Reflection Coefficient . . . . . . . . . 545
A.8.9 Putting It All Together . . . . . . . . . . . . . . . . . . . . . . 546
A.9 Economics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
Appendix B MIPS Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . 551
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
xvi CONTENTS

xvii
Preface
Why publish yet another book on digital design and computer architecture? There are dozens of good books in print on digital design. There
are also several good books about computer architecture, especially the
classic texts of Patterson and Hennessy. This book is unique in its treatment in that it presents digital logic design from the perspective of computer architecture, starting at the beginning with 1’s and 0’s, and leading
students through the design of a MIPS microprocessor.
We have used several editions of Patterson and Hennessy’s
Computer Organization and Design (COD) for many years at Harvey
Mudd College. We particularly like their coverage of the MIPS architecture and microarchitecture because MIPS is a commercially successful
microprocessor architecture, yet it is simple enough to clearly explain
and build in an introductory class. Because our class has no prerequisites, the first half of the semester is dedicated to digital design, which is
not covered by COD. Other universities have indicated a need for a
book that combines digital design and computer architecture. We have
undertaken to prepare such a book.
We believe that building a microprocessor is a special rite of passage
for engineering and computer science students. The inner workings of a
processor seem almost magical to the uninitiated, yet prove to be
straightforward when carefully explained. Digital design in itself is a
powerful and exciting subject. Assembly language programming unveils
the inner language spoken by the processor. Microarchitecture is the link
that brings it all together.
This book is suitable for a rapid-paced, single-semester introduction to digital design and computer architecture or for a two-quarter or
two-semester sequence giving more time to digest the material and
experiment in the lab. The only prerequisite is basic familiarity with a
high-level programming language such as C, C++, or Java. The material is usually taught at the sophomore- or junior-year level, but may
also be accessible to bright freshmen who have some programming
experience.

FEATURES
This book offers a number of special features.
Side-by-Side Coverage of Verilog and VHDL
Hardware description languages (HDLs) are at the center of modern digital design practices. Unfortunately, designers are evenly split between
the two dominant languages, Verilog and VHDL. This book introduces
HDLs in Chapter 4 as soon as combinational and sequential logic design
has been covered. HDLs are then used in Chapters 5 and 7 to design
larger building blocks and entire processors. Nevertheless, Chapter 4 can
be skipped and the later chapters are still accessible for courses that
choose not to cover HDLs.
This book is unique in its side-by-side presentation of Verilog and
VHDL, enabling the reader to quickly compare and contrast the two
languages. Chapter 4 describes principles applying to both HDLs, then
provides language-specific syntax and examples in adjacent columns.
This side-by-side treatment makes it easy for an instructor to choose
either HDL, and for the reader to transition from one to the other, either
in a class or in professional practice.
Classic MIPS Architecture and Microarchitecture
Chapters 6 and 7 focus on the MIPS architecture adapted from the treatment of Patterson and Hennessy. MIPS is an ideal architecture because it is a
real architecture shipped in millions of products yearly, yet it is streamlined
and easy to learn. Moreover, hundreds of universities around the world
have developed pedagogy, labs, and tools around the MIPS architecture.
Real-World Perspectives
Chapters 6, 7, and 8 illustrate the architecture, microarchitecture, and
memory hierarchy of Intel IA-32 processors. These real-world perspective chapters show how the concepts in the chapter relate to the chips
found in most PCs.
Accessible Overview of Advanced Microarchitecture
Chapter 7 includes an overview of modern high-performance microarchitectural features including branch prediction, superscalar and out-oforder operation, multithreading, and multicore processors. The
treatment is accessible to a student in a first course and shows how the
microarchitectures in the book can be extended to modern processors.
End-of-Chapter Exercises and Interview Questions
The best way to learn digital design is to do it. Each chapter ends with
numerous exercises to practice the material. The exercises are followed
by a set of interview questions that our industrial colleagues have asked
students applying for work in the field. These questions provide a helpful
xviii PREFACE

glimpse into the types of problems job applicants will typically encounter
during the interview process. (Exercise solutions are available via the
book’s companion and instructor Web pages. For more details, see the
next section, Online Supplements.)
ONLINE SUPPLEMENTS
Supplementary materials are available online at textbooks.elsevier.com/
9780123704979. This companion site (accessible to all readers) includes:
 Solutions to odd-numbered exercises
 Links to professional-strength computer-aided design (CAD) tools
from Xilinx® and Synplicity®
 Link to PCSPIM, a Windows-based MIPS simulator
 Hardware description language (HDL) code for the MIPS processor
 Xilinx Project Navigator helpful hints
 Lecture slides in PowerPoint (PPT) format
 Sample course and lab materials
 List of errata
The instructor site (linked to the companion site and accessible to
adopters who register at textbooks.elsevier.com) includes:
 Solutions to even-numbered exercises
 Links to professional-strength computer-aided design (CAD) tools
from Xilinx® and Synplicity®. (Instructors from qualified universities can access free Synplicity tools for use in their classroom and
laboratories. More details are available at the instructor site.)
 Figures from the text in JPG and PPT formats
Additional details on using the Xilinx, Synplicity, and PCSPIM tools in
your course are provided in the next section. Details on the sample lab
materials are also provided here.
HOW TO USE THE SOFTWARE TOOLS IN A COURSE
Xilinx ISE WebPACK
Xilinx ISE WebPACK is a free version of the professional-strength Xilinx
ISE Foundation FPGA design tools. It allows students to enter their digital designs in schematic or using either the Verilog or VHDL hardware
description language (HDL). After entering the design, students can
PREFACE xix
Prelims.qxd
simulate their circuits using ModelSim MXE III Starter, which is
included in the Xilinx WebPACK. Xilinx WebPACK also includes XST, a
logic synthesis tool supporting both Verilog and VHDL.
The difference between WebPACK and Foundation is that WebPACK
supports a subset of the most common Xilinx FPGAs. The difference
between ModelSim MXE III Starter and ModelSim commercial versions
is that Starter degrades performance for simulations with more than
10,000 lines of HDL.
Synplify Pro
Synplify Pro® is a high-performance, sophisticated logic synthesis engine
for FPGA and CPLD designs. Synplify Pro also contains HDL Analyst, a
graphical interface tool that generates schematic views of the HDL
source code. We have found that this is immensely useful in the learning
and debugging process.
Synplicity has generously agreed to donate Synplify Pro to qualified universities and will provide as many licenses as needed to fill
university labs. Instructors should visit the instructor Web page for
this text for more information on how to request Synplify Pro licenses.
For additional information on Synplicity and its other software, visit
www.synplicity.com/university.
PCSPIM
PCSPIM, also called simply SPIM, is a Windows-based MIPS simulator
that runs MIPS assembly code. Students enter their MIPS assembly code
into a text file and run it using PCSPIM. PCSPIM displays the instructions, memory, and register values. Links to the user’s manual and an
example file are available at the companion site (textbooks.elsevier.com/
9780123704979).
LABS
The companion site includes links to a series of labs that cover topics
from digital design through computer architecture. The labs teach students how to use the Xilinx WebPACK or Foundation tools to enter,
simulate, synthesize, and implement their designs. The labs also include
topics on assembly language programming using the PCSPIM simulator.
After synthesis, students can implement their designs using the Digilent
Spartan 3 Starter Board or the XUP-Virtex 2 Pro (V2Pro) Board. Both of
these powerful and competitively priced boards are available from
www.digilentinc.com. The boards contain FPGAs that can be programmed
to implement student designs. We provide labs that describe how to implement a selection of designs using Digilent’s Spartan 3 Board using
xx PREFACE

PREFACE xxi
WebPACK. Unfortunately, Xilinx WebPACK does not support the huge
FPGA on the V2Pro board. Qualified universities may contact the Xilinx
University Program to request a donation of the full Foundation tools.
To run the labs, students will need to download and install the
Xilinx WebPACK, PCSPIM, and possibly Synplify Pro. Instructors may
also choose to install the tools on lab machines. The labs include instructions on how to implement the projects on the Digilent’s Spartan 3
Starter Board. The implementation step may be skipped, but we have
found it of great value. The labs will also work with the XST synthesis
tool, but we recommend using Synplify Pro because the schematics it
produces give students invaluable feedback.
We have tested the labs on Windows, but the tools are also available
for Linux.
BUGS
As all experienced programmers know, any program of significant complexity undoubtedly contains bugs. So too do books. We have taken
great care to find and squash the bugs in this book. However, some
errors undoubtedly do remain. We will maintain a list of errata on the
book’s Web page.
Please send your bug reports to ddcabugs@onehotlogic.com. The first
person to report a substantive bug with a fix that we use in a future printing will be rewarded with a $1 bounty! (Be sure to include your mailing
address.)
ACKNOWLEDGMENTS
First and foremost, we thank David Patterson and John Hennessy for
their pioneering MIPS microarchitectures described in their Computer
Organization and Design textbook. We have taught from various editions
of their book for many years. We appreciate their gracious support of this
book and their permission to build on their microarchitectures.
Duane Bibby, our favorite cartoonist, labored long and hard to illustrate the fun and adventure of digital design. We also appreciate the enthusiasm of Denise Penrose, Nate McFadden, and the rest of the team at
Morgan Kaufmann who made this book happen. Jeff Somers at Graphic
World Publishing Services has ably guided the book through production.
Numerous reviewers have substantially improved the book. They
include John Barr (Ithaca College), Jack V. Briner (Charleston Southern
University), Andrew C. Brown (SK Communications), Carl Baumgaertner
(Harvey Mudd College), A. Utku Diril (Nvidia Corporation), Jim Frenzel
(University of Idaho), Jaeha Kim (Rambus, Inc.), Phillip King

xxii PREFACE
(ShotSpotter, Inc.), James Pinter-Lucke (Claremont McKenna College),
Amir Roth, Z. Jerry Shi (University of Connecticut), James E. Stine
(Oklahoma State University), Luke Teyssier, Peiyi Zhao (Chapman
University), and an anonymous reviewer. Simon Moore was a wonderful
host during David’s sabbatical visit to Cambridge University, where
major sections of this book were written.
We also appreciate the students in our course at Harvey Mudd
College who have given us helpful feedback on drafts of this textbook.
Of special note are Casey Schilling, Alice Clifton, Chris Acon, and
Stephen Brawner.
I, David, particularly thank my wife, Jennifer, who gave birth to our
son Abraham at the beginning of the project. I appreciate her patience and
loving support through yet another project at a busy time in our lives.



1
1.1 The Game Plan
1.2 The Art of Managing
Complexity
1.3 The Digital Abstraction
1.4 Number Systems
1.5 Logic Gates
1.6 Beneath the Digital
Abstraction
1.7 CMOS Transistors*
1.8 Power Consumption*
1.9 Summary and a Look
Ahead
Exercises
Interview Questions
From Zero to One
1.1 THE GAME PLAN
Microprocessors have revolutionized our world during the past three
decades. A laptop computer today has far more capability than a
room-sized mainframe of yesteryear. A luxury automobile contains
about 50 microprocessors. Advances in microprocessors have made
cell phones and the Internet possible, have vastly improved medicine,
and have transformed how war is waged. Worldwide semiconductor
industry sales have grown from US $21 billion in 1985 to $227 billion
in 2005, and microprocessors are a major segment of these sales.
We believe that microprocessors are not only technically, economically, and socially important, but are also an intrinsically fascinating
human invention. By the time you finish reading this book, you will
know how to design and build your own microprocessor. The skills
you learn along the way will prepare you to design many other digital
systems.
We assume that you have a basic familiarity with electricity, some
prior programming experience, and a genuine interest in understanding
what goes on under the hood of a computer. This book focuses on the
design of digital systems, which operate on 1’s and 0’s. We begin with
digital logic gates that accept 1’s and 0’s as inputs and produce 1’s and
0’s as outputs. We then explore how to combine logic gates into more
complicated modules such as adders and memories. Then we shift gears
to programming in assembly language, the native tongue of the microprocessor. Finally, we put gates together to build a microprocessor that
runs these assembly language programs.
A great advantage of digital systems is that the building blocks are
quite simple: just 1’s and 0’s. They do not require grungy mathematics or
a profound knowledge of physics. Instead, the designer’s challenge is to
combine these simple blocks into complicated systems. A microprocessor
may be the first system that you build that is too complex to fit in your
3

head all at once. One of the major themes weaved through this book is
how to manage complexity.
1.2 THE ART OF MANAGING COMPLEXITY
One of the characteristics that separates an engineer or computer scientist
from a layperson is a systematic approach to managing complexity.
Modern digital systems are built from millions or billions of transistors.
No human being could understand these systems by writing equations
describing the movement of electrons in each transistor and solving all of
the equations simultaneously. You will need to learn to manage complexity to understand how to build a microprocessor without getting mired in
a morass of detail.
1.2.1 Abstraction
The critical technique for managing complexity is abstraction: hiding
details when they are not important. A system can be viewed from many
different levels of abstraction. For example, American politicians
abstract the world into cities, counties, states, and countries. A county
contains multiple cities and a state contains many counties. When a
politician is running for president, the politician is mostly interested in
how the state as a whole will vote, rather than how each county votes,
so the state is the most useful level of abstraction. On the other hand,
the Census Bureau measures the population of every city, so the agency
must consider the details of a lower level of abstraction.
Figure 1.1 illustrates levels of abstraction for an electronic computer
system along with typical building blocks at each level. At the lowest
level of abstraction is the physics, the motion of electrons. The behavior
of electrons is described by quantum mechanics and Maxwell’s equations. Our system is constructed from electronic devices such as transistors (or vacuum tubes, once upon a time). These devices have
well-defined connection points called terminals and can be modeled by
the relationship between voltage and current as measured at each terminal. By abstracting to this device level, we can ignore the individual electrons. The next level of abstraction is analog circuits, in which devices
are assembled to create components such as amplifiers. Analog circuits
input and output a continuous range of voltages. Digital circuits such as
logic gates restrict the voltages to discrete ranges, which we will use to
indicate 0 and 1. In logic design, we build more complex structures, such
as adders or memories, from digital circuits.
Microarchitecture links the logic and architecture levels of abstraction.
The architecture level of abstraction describes a computer from the programmer’s perspective. For example, the Intel IA-32 architecture used by
microprocessors in most personal computers (PCs) is defined by a set of
4 CHAPTER ONE From Zero to One
Physics
Devices
Analog
Circuits
Digital
Circuits
Logic
Microarchitecture
Architecture
Operating
Systems
Application
Software
Electrons
Transistors
Diodes
Amplifiers
Filters
AND gates
NOT gates
Adders
Memories
Datapaths
Controllers
Instructions
Registers
Device
Drivers
Programs
Figure 1.1 Levels of abstraction
for electronic computing system

instructions and registers (memory for temporarily storing variables) that
the programmer is allowed to use. Microarchitecture involves combining
logic elements to execute the instructions defined by the architecture.
A particular architecture can be implemented by one of many different
microarchitectures with different price/performance/power trade-offs. For
example, the Intel Core 2 Duo, the Intel 80486, and the AMD Athlon all
implement the IA-32 architecture with different microarchitectures.
Moving into the software realm, the operating system handles lowlevel details such as accessing a hard drive or managing memory. Finally,
the application software uses these facilities provided by the operating system to solve a problem for the user. Thanks to the power of abstraction,
your grandmother can surf the Web without any regard for the quantum
vibrations of electrons or the organization of the memory in her computer.
This book focuses on the levels of abstraction from digital circuits
through computer architecture. When you are working at one level of
abstraction, it is good to know something about the levels of abstraction
immediately above and below where you are working. For example, a
computer scientist cannot fully optimize code without understanding the
architecture for which the program is being written. A device engineer
cannot make wise trade-offs in transistor design without understanding
the circuits in which the transistors will be used. We hope that by the
time you finish reading this book, you can pick the level of abstraction
appropriate to solving your problem and evaluate the impact of your
design choices on other levels of abstraction.
1.2.2 Discipline
Discipline is the act of intentionally restricting your design choices so
that you can work more productively at a higher level of abstraction.
Using interchangeable parts is a familiar application of discipline. One of
the first examples of interchangeable parts was in flintlock rifle manufacturing. Until the early 19th century, rifles were individually crafted by
hand. Components purchased from many different craftsmen were carefully filed and fit together by a highly skilled gunmaker. The discipline of
interchangeable parts revolutionized the industry. By limiting the components to a standardized set with well-defined tolerances, rifles could be
assembled and repaired much faster and with less skill. The gunmaker
no longer concerned himself with lower levels of abstraction such as the
specific shape of an individual barrel or gunstock.
In the context of this book, the digital discipline will be very important. Digital circuits use discrete voltages, whereas analog circuits use continuous voltages. Therefore, digital circuits are a subset of analog circuits
and in some sense must be capable of less than the broader class of analog
circuits. However, digital circuits are much simpler to design. By limiting
1.2 The Art of Managing Complexity
5

ourselves to digital circuits, we can easily combine components into
sophisticated systems that ultimately outperform those built from analog
components in many applications. For example, digital televisions, compact disks (CDs), and cell phones are replacing their analog predecessors.
1.2.3 The Three -Y’s
In addition to abstraction and discipline, designers use the three “-y’s” to
manage complexity: hierarchy, modularity, and regularity. These principles apply to both software and hardware systems.
 Hierarchy involves dividing a system into modules, then further subdividing each of these modules until the pieces are easy to understand.
 Modularity states that the modules have well-defined functions and
interfaces, so that they connect together easily without unanticipated
side effects.
 Regularity seeks uniformity among the modules. Common modules
are reused many times, reducing the number of distinct modules that
must be designed.
To illustrate these “-y’s” we return to the example of rifle manufacturing. A flintlock rifle was one of the most intricate objects in common
use in the early 19th century. Using the principle of hierarchy, we can
break it into components shown in Figure 1.2: the lock, stock, and barrel.
The barrel is the long metal tube through which the bullet is fired.
The lock is the firing mechanism. And the stock is the wooden body that
holds the parts together and provides a secure grip for the user. In turn,
the lock contains the trigger, hammer, flint, frizzen, and pan. Each of
these components could be hierarchically described in further detail.
Modularity teaches that each component should have a well-defined
function and interface. A function of the stock is to mount the barrel
and lock. Its interface consists of its length and the location of its mounting pins. In a modular rifle design, stocks from many different manufacturers can be used with a particular barrel as long as the stock and barrel
are of the correct length and have the proper mounting mechanism. A
function of the barrel is to impart spin to the bullet so that it travels
more accurately. Modularity dictates that there should be no side effects:
the design of the stock should not impede the function of the barrel.
Regularity teaches that interchangeable parts are a good idea. With
regularity, a damaged barrel can be replaced by an identical part. The
barrels can be efficiently built on an assembly line, instead of being
painstakingly hand-crafted.
We will return to these principles of hierarchy, modularity, and regularity throughout the book.
6 CHAPTER ONE From Zero to One
Captain Meriwether Lewis of
the Lewis and Clark
Expedition was one of the
early advocates of interchangeable parts for rifles. In
1806, he explained:
The guns of Drewyer and
Sergt. Pryor were both out
of order. The first was
repared with a new lock, the
old one having become unfit
for use; the second had the
cock screw broken which
was replaced by a duplicate
which had been prepared for
the lock at Harpers Ferry
where she was manufactured. But for the precaution
taken in bringing on those
extra locks, and parts of
locks, in addition to the
ingenuity of John Shields,
most of our guns would at
this moment been entirely
unfit for use; but fortunately
for us I have it in my power
here to record that they are
all in good order.
See Elliott Coues, ed., The
History of the Lewis and
Clark Expedition... (4 vols),
New York: Harper, 1893;
reprint, 3 vols, New York:
Dover, 3:817.
Cha
1.3 THE DIGITAL ABSTRACTION
Most physical variables are continuous. For example, the voltage on a
wire, the frequency of an oscillation, or the position of a mass are all
continuous quantities. Digital systems, on the other hand, represent
information with discrete-valued variables—that is, variables with a
finite number of distinct values.
An early digital system using variables with ten discrete values was
Charles Babbage’s Analytical Engine. Babbage labored from 1834 to
1871,1 designing and attempting to build this mechanical computer. The
Analytical Engine used gears with ten positions labeled 0 through 9,
much like a mechanical odometer in a car. Figure 1.3 shows a prototype
1.3 The Digital Abstraction 7
Barrel
Stoc
Lock
Expanded view of Lock
k
Flint
Cock
Pan
Spring
String
Figure 1.2 Flintlock rifle with
a close-up view of the lock
(Image by Euroams Italia.
www.euroarms.net © 2006).
1 And we thought graduate school was long!
Charles Babbage, 1791–1871.
Attended Cambridge University and married Georgiana
Whitmore in 1814. Invented
the Analytical Engine, the
world’s first mechanical
computer. Also invented the
cowcatcher and the universal
postage rate. Interested in
lock-picking, but abhorred
street musicians (image
courtesy of Fourmilab
Switzerland, www.
fourmilab.ch).

of the Analytical Engine, in which each row processes one digit. Babbage
chose 25 rows of gears, so the machine has 25-digit precision.
Unlike Babbage’s machine, most electronic computers use a binary
(two-valued) representation in which a high voltage indicates a ‘1’ and a
low voltage indicates a ‘0,’ because it is easier to distinguish between
two voltages than ten.
The amount of information D in a discrete valued variable with N
distinct states is measured in units of bits as
D  log2N bits (1.1)
A binary variable conveys log22  1 bit of information. Indeed, the
word bit is short for binary digit. Each of Babbage’s gears carried log210
3.322 bits of information because it could be in one of 23.322  10 unique
positions. A continuous signal theoretically contains an infinite amount of
information because it can take on an infinite number of values. In practice, noise and measurement error limit the information to only 10 to 16
bits for most continuous signals. If the measurement must be made rapidly,
the information content is lower (e.g., 8 bits).
This book focuses on digital circuits using binary variables: 1’s and
0’s. George Boole developed a system of logic operating on binary variables that is now known as Boolean logic. Each of Boole’s variables
could be TRUE or FALSE. Electronic computers commonly use a positive voltage to represent ‘1’ and zero volts to represent ‘0’. In this book,
we will use the terms ‘1,’ TRUE, and HIGH synonymously. Similarly, we
will use ‘0,’ FALSE, and LOW interchangeably.
The beauty of the digital abstraction is that digital designers can
focus on 1’s and 0’s, ignoring whether the Boolean variables are physically represented with specific voltages, rotating gears, or even hydraulic
8 CHAPTER ONE From Zero to One
Figure 1.3 Babbage’s Analytical
Engine, under construction at
the time of his death in 1871
(image courtesy of Science
Museum/Science and Society
Picture Library).
George Boole, 1815–1864. Born
to working-class parents and
unable to afford a formal education, Boole taught himself
mathematics and joined the
faculty of Queen’s College in
Ireland. He wrote An
Investigation of the Laws of
Thought (1854), which introduced binary variables and the
three fundamental logic operations: AND, OR, and NOT
(image courtesy of xxx).
Chap
fluid levels. A computer programmer can work without needing to know
the intimate details of the computer hardware. On the other hand,
understanding the details of the hardware allows the programmer to
optimize the software better for that specific computer.
An individual bit doesn’t carry much information. In the next section, we examine how groups of bits can be used to represent numbers.
In later chapters, we will also use groups of bits to represent letters and
programs.
1.4 NUMBER SYSTEMS
You are accustomed to working with decimal numbers. In digital systems consisting of 1’s and 0’s, binary or hexadecimal numbers are often
more convenient. This section introduces the various number systems
that will be used throughout the rest of the book.
1.4.1 Decimal Numbers
In elementary school, you learned to count and do arithmetic in decimal.
Just as you (probably) have ten fingers, there are ten decimal digits, 0, 1,
2, ..., 9. Decimal digits are joined together to form longer decimal numbers. Each column of a decimal number has ten times the weight of the
previous column. From right to left, the column weights are 1, 10, 100,
1000, and so on. Decimal numbers are referred to as base 10. The base
is indicated by a subscript after the number to prevent confusion when
working in more than one base. For example, Figure 1.4 shows how the
decimal number 974210 is written as the sum of each of its digits multiplied by the weight of the corresponding column.
An N-digit decimal number represents one of 10N possibilities: 0, 1,
2, 3, ..., 10N1. This is called the range of the number. For example, a
three-digit decimal number represents one of 1000 possibilities in the
range of 0 to 999.
1.4.2 Binary Numbers
Bits represent one of two values, 0 or 1, and are joined together to form
binary numbers. Each column of a binary number has twice the weight
1.4 Number Systems 9
974210 = 9 × 103 + 7 × 102 + 4 × 101 + 2 × 100
nine
thousands
1000's column
100's column
10's column
seven
hundreds
four
tens
two
ones
1's column
Figure 1.4 Representation
of a decimal number
C
of the previous column, so binary numbers are base 2. In binary, the column weights (again from right to left) are 1, 2, 4, 8, 16, 32, 64, 128,
256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, and so on. If
you work with binary numbers often, you’ll save time if you remember
these powers of two up to 216.
An N-bit binary number represents one of 2N possibilities: 0, 1, 2,
3, ..., 2N1. Table 1.1 shows 1, 2, 3, and 4-bit binary numbers and their
decimal equivalents.
Example 1.1 BINARY TO DECIMAL CONVERSION
Convert the binary number 101102 to decimal.
Solution: Figure 1.5 shows the conversion.
10 CHAPTER ONE From Zero to One
1-Bit 2-Bit 3-Bit 4-Bit
Binary Binnary Binary Binary Decimal
Numbers Numbers Numbers Numbers Equivalents
0 00 000 0000 0
1 01 001 0001 1
10 010 0010 2
11 011 0011 3
100 0100 4
101 0101 5
110 0110 6
111 0111 7
1000 8
1001 9
1010 10
1011 11
1100 12
1101 13
1110 14
1111 15
Table 1.1 Binary numbers and their decimal equivalent
C
Example 1.2 DECIMAL TO BINARY CONVERSION
Convert the decimal number 8410 to binary.
Solution: Determine whether each column of the binary result has a 1 or a 0. We
can do this starting at either the left or the right column.
Working from the left, start with the largest power of 2 less than the number (in
this case, 64). 84  64, so there is a 1 in the 64’s column, leaving 84  64  20.
20  32, so there is a 0 in the 32’s column. 20  16, so there is a 1 in the 16’s
column, leaving 20  16  4. 4  8, so there is a 0 in the 8’s column. 4  4, so
there is a 1 in the 4’s column, leaving 4  4  0. Thus there must be 0’s in the
2’s and 1’s column. Putting this all together, 8410  10101002.
Working from the right, repeatedly divide the number by 2. The remainder goes
in each column. 84/2  42, so 0 goes in the 1’s column. 42/2  21, so 0 goes in
the 2’s column. 21/2  10 with a remainder of 1 going in the 4’s column. 10/2
5, so 0 goes in the 8’s column. 5/2  2 with a remainder of 1 going in the 16’s
column. 2/2  1, so 0 goes in the 32’s column. Finally 1/2  0 with a remainder
of 1 going in the 64’s column. Again, 8410  10101002
1.4.3 Hexadecimal Numbers
Writing long binary numbers becomes tedious and prone to error.
A group of four bits represents one of 24  16 possibilities. Hence, it is
sometimes more convenient to work in base 16, called hexadecimal.
Hexadecimal numbers use the digits 0 to 9 along with the letters A to F,
as shown in Table 1.2. Columns in base 16 have weights of 1, 16, 162
(or 256), 163 (or 4096), and so on.
Example 1.3 HEXADECIMAL TO BINARY AND DECIMAL CONVERSION
Convert the hexadecimal number 2ED16 to binary and to decimal.
Solution: Conversion between hexadecimal and binary is easy because each hexadecimal digit directly corresponds to four binary digits. 216  00102, E16  11102
and D16  11012, so 2ED16  0010111011012. Conversion to decimal requires
the arithmetic shown in Figure 1.6.
1.4 Number Systems 11
101102 = 1 × 24 + 0 × 23 + 1 × 22 + 1 × 21
+ 0 × 20 = 2210
one
sixteen
1's column
no
eight one
four one
two no
one
16's column
8's column
4's column
2's column
Figure 1.5 Conversion of a
binary number to decimal
“Hexadecimal,” a term
coined by IBM in 1963,
derives from the Greek hexi
(six) and Latin decem (ten). A
more proper term would use
the Latin sexa (six), but sexidecimal sounded too risqué.
Chapter 01.qxd 1/27/
Example 1.4 BINARY TO HEXADECIMAL CONVERSION
Convert the binary number 11110102 to hexadecimal.
Solution: Again, conversion is easy. Start reading from the right. The four least
significant bits are 10102  A16. The next bits are 1112  716. Hence 11110102
 7A16.
12 CHAPTER ONE From Zero to One
Table 1.2 Hexadecimal number system
Hexadecimal Digit Decimal Equivalent Binary Equivalent
0 0 0000
1 1 0001
2 2 0010
3 3 0011
4 4 0100
5 5 0101
6 6 0110
7 7 0111
8 8 1000
9 9 1001
A 10 1010
B 11 1011
C 12 1100
D 13 1101
E 14 1110
F 15 1111
2ED16 = 2 × 162 + E × 161 + D × 160 = 74910
two
two hundred
fifty six's
1's column
fourteen
sixteens
thirteen
ones
256's column
16's column
Figure 1.6 Conversion of
hexadecimal number to decimal
Cha
Example 1.5 DECIMAL TO HEXADECIMAL AND BINARY CONVERSION
Convert the decimal number 33310 to hexadecimal and binary.
Solution: Like decimal to binary conversion, decimal to hexadecimal conversion
can be done from the left or the right.
Working from the left, start with the largest power of 16 less than the number (in
this case, 256). 256 goes into 333 once, so there is a 1 in the 256’s column, leaving
333  256  77. 16 goes into 77 four times, so there is a 4 in the 16’s column,
leaving 77  16  4  13. 1310  D16, so there is a D in the 1’s column. In summary, 33310  14D16. Now it is easy to convert from hexadecimal to binary, as in
Example 1.3. 14D16  1010011012.
Working from the right, repeatedly divide the number by 16. The remainder
goes in each column. 333/16  20 with a remainder of 1310  D16 going in
the 1’s column. 20/16  1 with a remainder of 4 going in the 16’s column.
1/16  0 with a remainder of 1 going in the 256’s column. Again, the result
is 14D16.
1.4.4 Bytes, Nibbles, and All That Jazz
A group of eight bits is called a byte. It represents one of 28  256 possibilities. The size of objects stored in computer memories is customarily
measured in bytes rather than bits.
A group of four bits, or half a byte, is called a nibble. It represents
one of 24  16 possibilities. One hexadecimal digit stores one nibble and
two hexadecimal digits store one full byte. Nibbles are no longer a commonly used unit, but the term is cute.
Microprocessors handle data in chunks called words. The size of a
word depends on the architecture of the microprocessor. When this
chapter was written in 2006, most computers had 32-bit processors,
indicating that they operate on 32-bit words. At the time, computers
handling 64-bit words were on the verge of becoming widely available.
Simpler microprocessors, especially those used in gadgets such as toasters, use 8- or 16-bit words.
Within a group of bits, the bit in the 1’s column is called the least
significant bit (lsb), and the bit at the other end is called the most
significant bit (msb), as shown in Figure 1.7(a) for a 6-bit binary
number. Similarly, within a word, the bytes are identified as least
significant byte (LSB) through most significant byte (MSB), as shown
in Figure 1.7(b) for a four-byte number written with eight hexadecimal digits.
1.4 Number Systems 13
A microprocessor is a processor built on a single chip.
Until the 1970’s, processors
were too complicated to fit on
one chip, so mainframe
processors were built from
boards containing many chips.
Intel introduced the first 4-bit
microprocessor, called the
4004, in 1971. Now, even the
most sophisticated supercomputers are built using microprocessors. We will use the
terms microprocessor and
processor interchangeably
throughout this book.
Chapter 01.qx
By handy coincidence, 210  1024  103. Hence, the term kilo
(Greek for thousand) indicates 210. For example, 210 bytes is one kilobyte (1 KB). Similarly, mega (million) indicates 220  106, and giga
(billion) indicates 230  109. If you know 210  1 thousand, 220  1
million, 230  1 billion, and remember the powers of two up to 29, it is
easy to estimate any power of two in your head.
Example 1.6 ESTIMATING POWERS OF TWO
Find the approximate value of 224 without using a calculator.
Solution: Split the exponent into a multiple of ten and the remainder. 224  220
 24. 220  1 million. 24  16. So 224  16 million. Technically, 224
16,777,216, but 16 million is close enough for marketing purposes.
1024 bytes is called a kilobyte (KB). 1024 bits is called a kilobit (Kb
or Kbit). Similarly, MB, Mb, GB, and Gb are used for millions and billions of bytes and bits. Memory capacity is usually measured in bytes.
Communication speed is usually measured in bits/sec. For example, the
maximum speed of a dial-up modem is usually 56 Kbits/sec.
1.4.5 Binary Addition
Binary addition is much like decimal addition, but easier, as shown in
Figure 1.8. As in decimal addition, if the sum of two numbers is greater
than what fits in a single digit, we carry a 1 into the next column. Figure
1.8 compares addition of decimal and binary numbers. In the right-most
column of Figure 1.8(a), 7  9  16, which cannot fit in a single digit
because it is greater than 9. So we record the 1’s digit, 6, and carry the
10’s digit, 1, over to the next column. Likewise, in binary, if the sum of
two numbers is greater than 1, we carry the 2’s digit over to the next column. For example, in the right-most column of Figure 1.8(b), the sum
14 CHAPTER ONE From Zero to One
101100
least
significant
bit
most
significant
bit
(a) (b)
DEAFDAD8
least
significant
byte
most
significant
byte
1011
+ 0011
1110
carries 11
4277
+ 5499
9776
11
(a) (b)
Figure 1.7 Least and most
significant bits and bytes
Figure 1.8 Addition examples
showing carries: (a) decimal
(b) binary
Chapter 01.qx
1  1  210  102 cannot fit in a single binary digit. So we record the
1’s digit (0) and carry the 2’s digit (1) of the result to the next column. In
the second column, the sum is 1  1  1  310  112. Again, we record
the 1’s digit (1) and carry the 2’s digit (1) to the next column. For obvious reasons, the bit that is carried over to the neighboring column is
called the carry bit.
Example 1.7 BINARY ADDITION
Compute 01112  01012.
Solution: Figure 1.9 shows that the sum is 11002. The carries are indicated in
blue. We can check our work by repeating the computation in decimal. 01112
710. 01012  510. The sum is 1210  11002.
Digital systems usually operate on a fixed number of digits.
Addition is said to overflow if the result is too big to fit in the available
digits. A 4-bit number, for example, has the range [0, 15]. 4-bit binary
addition overflows if the result exceeds 15. The fifth bit is discarded,
producing an incorrect result in the remaining four bits. Overflow can be
detected by checking for a carry out of the most significant column.
Example 1.8 ADDITION WITH OVERFLOW
Compute 11012  01012. Does overflow occur?
Solution: Figure 1.10 shows the sum is 100102. This result overflows the range
of a 4-bit binary number. If it must be stored as four bits, the most significant bit
is discarded, leaving the incorrect result of 00102. If the computation had been
done using numbers with five or more bits, the result 100102 would have been
correct.
1.4.6 Signed Binary Numbers
So far, we have considered only unsigned binary numbers that represent
positive quantities. We will often want to represent both positive and
negative numbers, requiring a different binary number system. Several
schemes exist to represent signed binary numbers; the two most widely
employed are called sign/magnitude and two’s complement.
Sign/Magnitude Numbers
Sign/magnitude numbers are intuitively appealing because they match our
custom of writing negative numbers with a minus sign followed by the
magnitude. An N-bit sign/magnitude number uses the most significant bit
1.4 Number Systems 15
0111
+ 0101
1100
111
1101
+ 0101
10010
11 1
Figure 1.9 Binary addition
example
Figure 1.10 Binary addition
example with overflow
Chapter
as the sign and the remaining N  1 bits as the magnitude (absolute
value). A sign bit of 0 indicates positive and a sign bit of 1 indicates
negative.
Example 1.9 SIGN/MAGNITUDE NUMBERS
Write 5 and 5 as 4-bit sign/magnitude numbers
Solution: Both numbers have a magnitude of 510  1012. Thus, 510  01012 and
510  11012.
Unfortunately, ordinary binary addition does not work for sign/magnitude numbers. For example, using ordinary addition on 510  510
gives 11012  01012  100102, which is nonsense.
An N-bit sign/magnitude number spans the range [2N1  1, 2N1
 1]. Sign/magnitude numbers are slightly odd in that both 0 and 0
exist. Both indicate zero. As you may expect, it can be troublesome to
have two different representations for the same number.
Two’s Complement Numbers
Two’s complement numbers are identical to unsigned binary numbers
except that the most significant bit position has a weight of 2N1
instead of 2N1. They overcome the shortcomings of sign/magnitude
numbers: zero has a single representation, and ordinary addition works.
In two’s complement representation, zero is written as all zeros:
00...0002. The most positive number has a 0 in the most significant position and 1’s elsewhere: 01...1112  2N1  1. The most negative number
has a 1 in the most significant position and 0’s elsewhere: 10...0002
2N1. And 1 is written as all ones: 11...1112.
Notice that positive numbers have a 0 in the most significant position and negative numbers have a 1 in this position, so the most significant bit can be viewed as the sign bit. However, the remaining bits are
interpreted differently for two’s complement numbers than for sign/
magnitude numbers.
The sign of a two’s complement number is reversed in a process
called taking the two’s complement. The process consists of inverting all
of the bits in the number, then adding 1 to the least significant bit
position. This is useful to find the representation of a negative number
or to determine the magnitude of a negative number.
Example 1.10 TWO’S COMPLEMENT REPRESENTATION
OF A NEGATIVE NUMBER
Find the representation of 210 as a 4-bit two’s complement number.
16 CHAPTER ONE From Zero to One
The $7 billion Ariane 5
rocket, launched on June 4,
1996, veered off course 40
seconds after launch, broke
up, and exploded. The failure
was caused when the computer controlling the rocket
overflowed its 16-bit range
and crashed.
The code had been extensively tested on the Ariane 4
rocket. However, the Ariane 5
had a faster engine that produced larger values for the
control computer, leading to
the overflow.
(Photograph courtesy
ESA/CNES/ ARIANESPACEService Optique CS6.)
Chapter 01.qxd 1/27/07 1
Solution: Start with  210  00102. To get 210, invert the bits and add 1.
Inverting 00102 produces 11012. 11012  1  11102. So 210 is 11102.
Example 1.11 VALUE OF NEGATIVE TWO’S COMPLEMENT NUMBERS
Find the decimal value of the two’s complement number 10012.
Solution: 10012 has a leading 1, so it must be negative. To find its magnitude,
invert the bits and add 1. Inverting 10012  01102. 01102  1  01112  710.
Hence, 10012  710.
Two’s complement numbers have the compelling advantage that
addition works properly for both positive and negative numbers. Recall
that when adding N-bit numbers, the carry out of the Nth bit (i.e., the
N  1th result bit), is discarded.
Example 1.12 ADDING TWO’S COMPLEMENT NUMBERS
Compute (a) 210  110 and (b) 710  710 using two’s complement numbers.
Solution: (a) 210  110  11102  00012  11112  110. (b) 710  710
 10012  01112  100002. The fifth bit is discarded, leaving the correct 4-bit
result 00002.
Subtraction is performed by taking the two’s complement of the second number, then adding.
Example 1.13 SUBTRACTING TWO’S COMPLEMENT NUMBERS
Compute (a) 510  310 and (b) 310  510 using 4-bit two’s complement numbers.
Solution: (a) 310  00112. Take its two’s complement to obtain 310  11012.
Now add 510  (310)  01012  11012  00102  210. Note that the carry
out of the most significant position is discarded because the result is stored in
four bits. (b) Take the two’s complement of 510 to obtain 510  1011. Now
add 310  (510)  00112  10112  11102  210.
The two’s complement of 0 is found by inverting all the bits (producing 11...1112) and adding 1, which produces all 0’s, disregarding the
carry out of the most significant bit position. Hence, zero is always
represented with all 0’s. Unlike the sign/magnitude system, the two’s
complement system has no separate 0. Zero is considered positive
because its sign bit is 0.
1.4 Number Systems 17
Chapter 01.qxd 1/27/07 10:20 AM Page
Like unsigned numbers, N-bit two’s complement numbers represent one of 2N possible values. However the values are split between
positive and negative numbers. For example, a 4-bit unsigned number
represents 16 values: 0 to 15. A 4-bit two’s complement number also
represents 16 values: 8 to 7. In general, the range of an N-bit two’s
complement number spans [2N1, 2N1  1]. It should make sense
that there is one more negative number than positive number because
there is no 0. The most negative number 10...0002  2N1 is
sometimes called the weird number. Its two’s complement is found by
inverting the bits (producing 01...1112 and adding 1, which produces
10...0002, the weird number, again). Hence, this negative number has
no positive counterpart.
Adding two N-bit positive numbers or negative numbers may
cause overflow if the result is greater than 2N1  1 or less than
2N1. Adding a positive number to a negative number never causes
overflow. Unlike unsigned numbers, a carry out of the most significant
column does not indicate overflow. Instead, overflow occurs if the two
numbers being added have the same sign bit and the result has the
opposite sign bit.
Example 1.14 ADDING TWO’S COMPLEMENT NUMBERS
WITH OVERFLOW
Compute (a) 410  510 using 4-bit two’s complement numbers. Does the result
overflow?
Solution: (a) 410  510  01002  01012  10012  710. The result overflows
the range of 4-bit positive two’s complement numbers, producing an incorrect
negative result. If the computation had been done using five or more bits, the
result 010012  910 would have been correct.
When a two’s complement number is extended to more bits, the sign
bit must be copied into the most significant bit positions. This process is
called sign extension. For example, the numbers 3 and 3 are written as
4-bit two’s complement numbers 0011 and 1101, respectively. They are
sign-extended to seven bits by copying the sign bit into the three new
upper bits to form 0000011 and 1111101, respectively.
Comparison of Number Systems
The three most commonly used binary number systems are unsigned,
two’s complement, and sign/magnitude. Table 1.3 compares the range of
N-bit numbers in each of these three systems. Two’s complement numbers are convenient because they represent both positive and negative
integers and because ordinary addition works for all numbers.
18 CHAPTER ONE From Zero to One
Chapter 01.qxd 1/27
Subtraction is performed by negating the second number (i.e., taking the
two’s complement), and then adding. Unless stated otherwise, assume
that all signed binary numbers use two’s complement representation.
Figure 1.11 shows a number line indicating the values of 4-bit numbers in each system. Unsigned numbers span the range [0, 15] in regular
binary order. Two’s complement numbers span the range [8, 7]. The
nonnegative numbers [0, 7] share the same encodings as unsigned numbers. The negative numbers [8, 1] are encoded such that a larger
unsigned binary value represents a number closer to 0. Notice that the
weird number, 1000, represents 8 and has no positive counterpart.
Sign/magnitude numbers span the range [7, 7]. The most significant bit
is the sign bit. The positive numbers [1, 7] share the same encodings as
unsigned numbers. The negative numbers are symmetric but have the
sign bit set. 0 is represented by both 0000 and 1000. Thus, N-bit
sign/magnitude numbers represent only 2N  1 integers because of the
two representations for 0.
1.5 LOGIC GATES
Now that we know how to use binary variables to represent information, we explore digital systems that perform operations on these binary
variables. Logic gates are simple digital circuits that take one or more
binary inputs and produce a binary output. Logic gates are drawn with
a symbol showing the input (or inputs) and the output. Inputs are
1.5 Logic Gates 19
–8
1000 1001
–7 –6 –5 –4 –3 –2 –1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
1010 1011 1100 1101 1110 1111 0000 0001 0010 0011 0100 0101 0110 0111 Two's Complement
1000 1111 1110 1101 1100 1011 1010 1001 0000 0001 0010 0011 0100 0101 0110 0111
0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111
Sign/Magnitude
Unsigned
Table 1.3 Range of N-bit numbers
System Range
Unsigned [0, 2N  1]
Sign/Magnitude [2N1  1, 2N1  1]
Two’s Complement [2N1, 2N1  1]
Figure 1.11 Number line and 4-bit binary encodings
Chapter 01.qxd 
usually drawn on the left (or top) and outputs on the right (or bottom).
Digital designers typically use letters near the beginning of the alphabet
for gate inputs and the letter Y for the gate output. The relationship
between the inputs and the output can be described with a truth table or
a Boolean equation. A truth table lists inputs on the left and the corresponding output on the right. It has one row for each possible combination of inputs. A Boolean equation is a mathematical expression using
binary variables.
1.5.1 NOT Gate
A NOT gate has one input, A, and one output, Y, as shown in Figure
1.12. The NOT gate’s output is the inverse of its input. If A is FALSE,
then Y is TRUE. If A is TRUE, then Y is FALSE. This relationship is
summarized by the truth table and Boolean equation in the figure. The
line over A in the Boolean equation is pronounced NOT, so Y  A
_
is
read “Y equals NOT A.” The NOT gate is also called an inverter.
Other texts use a variety of notations for NOT, including Y  A, Y
A, Y  !A or Y  ~A. We will use Y  A
_
exclusively, but don’t be puzzled if you encounter another notation elsewhere.
1.5.2 Buffer
The other one-input logic gate is called a buffer and is shown in Figure
1.13. It simply copies the input to the output.
From the logical point of view, a buffer is no different from a wire,
so it might seem useless. However, from the analog point of view, the
buffer might have desirable characteristics such as the ability to deliver
large amounts of current to a motor or the ability to quickly send its
output to many gates. This is an example of why we need to consider
multiple levels of abstraction to fully understand a system; the digital
abstraction hides the real purpose of a buffer.
The triangle symbol indicates a buffer. A circle on the output is
called a bubble and indicates inversion, as was seen in the NOT gate
symbol of Figure 1.12.
1.5.3 AND Gate
Two-input logic gates are more interesting. The AND gate shown in Figure
1.14 produces a TRUE output, Y, if and only if both A and B are TRUE.
Otherwise, the output is FALSE. By convention, the inputs are listed in the
order 00, 01, 10, 11, as if you were counting in binary. The Boolean equation for an AND gate can be written in several ways: Y  A • B, Y  AB, or
Y  A  B. The  symbol is pronounced “intersection” and is preferred by
logicians. We prefer Y  AB, read “Y equals A and B,” because we are lazy.
20 CHAPTER ONE From Zero to One
NOT
Y = A
A Y
0 1
1 0
A Y
BUF
Y = A
A Y
0 0
1 1
A Y
AND
Y = AB
ABY
000
010
100
111
A
B Y
Figure 1.12 NOT gate
Figure 1.14 AND gate
Figure 1.13 Buffer
According to Larry Wall,
inventor of the Perl programming language, “the three
principal virtues of a programmer are Laziness,
Impatience, and Hubris.”
Chapter 01.q
1.5.4 OR Gate
The OR gate shown in Figure 1.15 produces a TRUE output, Y, if either
A or B (or both) are TRUE. The Boolean equation for an OR gate is
written as Y  A  B or Y  A  B. The  symbol is pronounced
union and is preferred by logicians. Digital designers normally use the 
notation, Y  A  B is pronounced “Y equals A or B”.
1.5.5 Other Two-Input Gates
Figure 1.16 shows other common two-input logic gates. XOR (exclusive
OR, pronounced “ex-OR”) is TRUE if A or B, but not both, are TRUE.
Any gate can be followed by a bubble to invert its operation. The
NAND gate performs NOT AND. Its output is TRUE unless both inputs
are TRUE. The NOR gate performs NOT OR. Its output is TRUE if
neither A nor B is TRUE.
Example 1.15 XNOR GATE
Figure 1.17 shows the symbol and Boolean equation for a two-input XNOR
gate that performs the inverse of an XOR. Complete the truth table.
Solution: Figure 1.18 shows the truth table. The XNOR output is TRUE if both
inputs are FALSE or both inputs are TRUE. The two-input XNOR gate is sometimes called an equality gate because its output is TRUE when the inputs are equal.
1.5.6 Multiple-Input Gates
Many Boolean functions of three or more inputs exist. The most common
are AND, OR, XOR, NAND, NOR, and XNOR. An N-input AND
gate produces a TRUE output when all N inputs are TRUE. An N-input
OR gate produces a TRUE output when at least one input is TRUE.
1.5 Logic Gates 21
OR
Y = A + B
ABY
000
011
101
111
A
B Y
Figure 1.15 OR gate
XOR NAND NOR
Y =A + B Y =AB Y=A+B
ABY
000
011
101
110
ABY
001
011
101
110
ABY
001
010
100
110
A
B Y A
B Y A
B Y
Figure 1.16 More two-input logic gates
A silly way to remember the
OR symbol is that it’s input
side is curved like Pacman’s
mouth, so the gate is hungry
and willing to eat any TRUE
inputs it can find!
Figure 1.17 XNOR gate
XNOR
Y =A + B
ABY
0 0
0 1
1 0
1 1
A
B Y
Chapt
An N-input XOR gate is sometimes called a parity gate and produces a
TRUE output if an odd number of inputs are TRUE. As with two-input
gates, the input combinations in the truth table are listed in counting order.
Example 1.16 THREE-INPUT NOR GATE
Figure 1.19 shows the symbol and Boolean equation for a three-input NOR gate.
Complete the truth table.
Solution: Figure 1.20 shows the truth table. The output is TRUE only if none of
the inputs are TRUE.
Example 1.17 FOUR-INPUT AND GATE
Figure 1.21 shows the symbol and Boolean equation for a four-input AND gate.
Create a truth table.
Solution: Figure 1.22 shows the truth table. The output is TRUE only if all of the
inputs are TRUE.
1.6 BENEATH THE DIGITAL ABSTRACTION
A digital system uses discrete-valued variables. However, the variables are
represented by continuous physical quantities such as the voltage on a wire,
the position of a gear, or the level of fluid in a cylinder. Hence, the designer
must choose a way to relate the continuous value to the discrete value.
For example, consider representing a binary signal A with a voltage on
a wire. Let 0 volts (V) indicate A  0 and 5 V indicate A  1. Any real system must tolerate some noise, so 4.97 V probably ought to be interpreted
as A  1 as well. But what about 4.3 V? Or 2.8 V? Or 2.500000 V?
1.6.1 Supply Voltage
Suppose the lowest voltage in the system is 0 V, also called ground or
GND. The highest voltage in the system comes from the power supply
and is usually called VDD. In 1970’s and 1980’s technology, VDD was
generally 5 V. As chips have progressed to smaller transistors, VDD has
dropped to 3.3 V, 2.5 V, 1.8 V, 1.5 V, 1.2 V, or even lower to save power
and avoid overloading the transistors.
1.6.2 Logic Levels
The mapping of a continuous variable onto a discrete binary variable is
done by defining logic levels, as shown in Figure 1.23. The first gate is
called the driver and the second gate is called the receiver. The output of
22 CHAPTER ONE From Zero to One
Figure 1.19 Three-input NOR
gate
Figure 1.20 Three-input NOR
truth table
Figure 1.21 Four-input AND gate
NOR3
Y = A + B + C
BCY
0 0
0 1
1 0
1 1
A
B Y
C
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
BCY
001
010
100
110
A
0
0
0
0
000
010
100
110
1
1
1
1
AND4
Y = ABCD
A
B
C Y
D
Figure 1.18 XNOR truth table
ABY
0 0
0 1
1 0
1 1
1
0
0
1
Cha
the driver is connected to the input of the receiver. The driver produces a
LOW (0) output in the range of 0 to VOL or a HIGH (1) output in the
range of VOH to VDD. If the receiver gets an input in the range of 0 to
VIL, it will consider the input to be LOW. If the receiver gets an input in
the range of VIH to VDD, it will consider the input to be HIGH. If, for
some reason such as noise or faulty components, the receiver’s input
should fall in the forbidden zone between VIL and VIH, the behavior of
the gate is unpredictable. VOH, VOL, VIH, and VIL are called the output
and input high and low logic levels.
1.6.3 Noise Margins
If the output of the driver is to be correctly interpreted at the input of
the receiver, we must choose VOL  VIL and VOH 	 VIH. Thus, even if
the output of the driver is contaminated by some noise, the input of the
receiver will still detect the correct logic level. The noise margin is the
amount of noise that could be added to a worst-case output such that
the signal can still be interpreted as a valid input. As can be seen in
Figure 1.23, the low and high noise margins are, respectively
NML  VIL  VOL (1.2)
NMH  VOH  VIH (1.3)
Example 1.18
Consider the inverter circuit of Figure 1.24. VO1 is the output voltage of inverter
I1, and VI2 is the input voltage of inverter I2. Both inverters have the following
characteristics: VDD  5 V, VIL  1.35 V, VIH  3.15 V, VOL  0.33 V, and
VOH  3.84 V. What are the inverter low and high noise margins? Can the circuit tolerate 1 V of noise between VO1 and VI2?
Solution: The inverter noise margins are: NML  VIL  VOL  (1.35 V  0.33 V)
 1.02 V, NMH  VOH  VIH  (3.84 V  3.15 V)  0.69 V. The circuit can tolerate 1 V of noise when the output is LOW (NML  1.02 V) but not when the
output is HIGH (NMH  0.69 V). For example, suppose the driver, I1, outputs its
worst-case HIGH value, VO1  VOH  3.84 V. If noise causes the voltage to
droop by 1 V before reaching the input of the receiver, VI2  (3.84 V  1 V)
2.84 V. This is less than the acceptable input HIGH value, VIH  3.15 V, so the
receiver may not sense a proper HIGH input.
1.6.4 DC Transfer Characteristics
To understand the limits of the digital abstraction, we must delve into
the analog behavior of a gate. The DC transfer characteristics of a gate
describe the output voltage as a function of the input voltage when the
1.6 Beneath the Digital Abstraction 23
Figure 1.22 Four-input AND
truth table
BDY
000
010
100
110
C
0
0
0
0
000
010
100
110
1
1
1
1
A
000
010
100
110
0
0
0
0
000
010
100
111
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
VDD stands for the voltage on
the drain of a metal-oxidesemiconductor transistor, used
to build most modern chips.
The power supply voltage is
also sometimes called VCC,
standing for the voltage on
the collector of a bipolar transistor used to build chips in
an older technology. Ground
is sometimes called VSS
because it is the voltage on
the source of a metal-oxidesemiconductor transistor. See
Section 1.7 for more information on transistors.
Chapter 01.qxd 1/27/07 10:2
input is changed slowly enough that the output can keep up. They are
called transfer characteristics because they describe the relationship
between input and output voltages.
An ideal inverter would have an abrupt switching threshold at
VDD/2, as shown in Figure 1.25(a). For V(A)  VDD/2, V(Y)  VDD. For
V(A) 	 VDD/2, V(Y)  0. In such a case, VIH  VIL  VDD/2. VOH
VDD and VOL  0.
A real inverter changes more gradually between the extremes, as
shown in Figure 1.25(b). When the input voltage V(A) is 0, the output
voltage V(Y)  VDD. When V(A)  VDD, V(Y)  0. However, the transition between these endpoints is smooth and may not be centered at
exactly VDD/2. This raises the question of how to define the logic levels.
A reasonable place to choose the logic levels is where the slope of
the transfer characteristic dV(Y)/dV(A) is 1. These two points are
called the unity gain points. Choosing logic levels at the unity gain
points usually maximizes the noise margins. If VIL were reduced, VOH
would only increase by a small amount. But if VIL were increased, VOH
would drop precipitously.
1.6.5 The Static Discipline
To avoid inputs falling into the forbidden zone, digital logic gates are
designed to conform to the static discipline. The static discipline requires
that, given logically valid inputs, every circuit element will produce logically valid outputs.
By conforming to the static discipline, digital designers sacrifice the
freedom of using arbitrary analog circuit elements in return for the
24 CHAPTER ONE From Zero to One
I1 I2
Noise
VO1 VI2 Figure 1.24 Inverter circuit
DC indicates behavior when
an input voltage is held constant or changes slowly
enough for the rest of the system to keep up. The term’s
historical root comes from
direct current, a method of
transmitting power across a
line with a constant voltage.
In contrast, the transient
response of a circuit is the
behavior when an input voltage changes rapidly. Section
2.9 explores transient
response further.
Forbidden
Zone
NML
NMH
Output Characteristics Input Characteristics
VOH
VDD
VOL
GND
VIH
VIL
Logic High
Input Range
Logic Low
Input Range
Logic High
Output Range
Logic Low
Output Range
Driver Receiver
Figure 1.23 Logic levels and
noise margins
Chapter 01
simplicity and robustness of digital circuits. They raise the level of
abstraction from analog to digital, increasing design productivity by
hiding needless detail.
The choice of VDD and logic levels is arbitrary, but all gates that
communicate must have compatible logic levels. Therefore, gates are
grouped into logic families such that all gates in a logic family obey the
static discipline when used with other gates in the family. Logic gates in
the same logic family snap together like Legos in that they use consistent
power supply voltages and logic levels.
Four major logic families that predominated from the 1970’s through
the 1990’s are Transistor-Transistor Logic (TTL), Complementary MetalOxide-Semiconductor Logic (CMOS, pronounced sea-moss), Low Voltage
TTL Logic (LVTTL), and Low Voltage CMOS Logic (LVCMOS). Their
logic levels are compared in Table 1.4. Since then, logic families have
balkanized with a proliferation of even lower power supply voltages.
Appendix A.6 revisits popular logic families in more detail.
1.6 Beneath the Digital Abstraction 25
VDD
V(A)
V(Y)
VOH VDD
VOL
VIL, VIH
0
A Y
VDD
V(A)
V(Y)
VOH
VDD
VOL
VIL VIH
Unity Gain Points
Slope = –1
0
(a) (b)
VDD/ 2
Figure 1.25 DC transfer characteristics and logic levels
Table 1.4 Logic levels of 5 V and 3.3 V logic families
Logic Family VDD VIL VIH VOL VOH
TTL 5 (4.75–5.25) 0.8 2.0 0.4 2.4
CMOS 5 (4.5–6) 1.35 3.15 0.33 3.84
LVTTL 3.3 (3–3.6) 0.8 2.0 0.4 2.4
LVCMOS 3.3 (3–3.6) 0.9 1.8 0.36 2.7

Example 1.19 LOGIC FAMILY COMPATIBILITY
Which of the logic families in Table 1.4 can communicate with each other reliably?
Solution: Table 1.5 lists which logic families have compatible logic levels. Note
that a 5 V logic family such as TTL or CMOS may produce an output voltage as
HIGH as 5 V. If this 5 V signal drives the input of a 3.3 V logic family such as
LVTTL or LVCMOS, it can damage the receiver, unless the receiver is specially
designed to be “5-volt compatible.”
1.7 CMOS TRANSISTORS*
This section and other sections marked with a * are optional and are
not necessary to understand the main flow of the book.
Babbage’s Analytical Engine was built from gears, and early electrical computers used relays or vacuum tubes. Modern computers use transistors because they are cheap, small, and reliable. Transistors are
electrically controlled switches that turn ON or OFF when a voltage or
current is applied to a control terminal. The two main types of transistors are bipolar transistors and metal-oxide-semiconductor field effect
transistors (MOSFETs or MOS transistors, pronounced “moss-fets” or
“M-O-S”, respectively).
In 1958, Jack Kilby at Texas Instruments built the first integrated
circuit containing two transistors. In 1959, Robert Noyce at Fairchild
Semiconductor patented a method of interconnecting multiple transistors
on a single silicon chip. At the time, transistors cost about $10 each.
Thanks to more than three decades of unprecedented manufacturing
advances, engineers can now pack roughly one billion MOSFETs onto a
1 cm2 chip of silicon, and these transistors cost less than 10 microcents
apiece. The capacity and cost continue to improve by an order of magnitude every 8 years or so. MOSFETs are now the building blocks of
26 CHAPTER ONE From Zero to One
Table 1.5 Compatibility of logic families
Receiver
TTL CMOS LVTTL LVCMOS
Driver TTL OK NO: VOH  VIH MAYBEa MAYBEa
CMOS OK OK MAYBEa MAYBEa
LVTTL OK NO: VOH  VIH OK OK
LVCMOS OK NO: VOH  VIH OK OK
a As long as a 5 V HIGH level does not damage the receiver input
Robert Noyce, 1927–1990. Born
in Burlington, Iowa. Received
a B.A. in physics from
Grinnell College and a Ph.D.
in physics from MIT. Nicknamed “Mayor of Silicon
Valley” for his profound
influence on the industry.
Cofounded Fairchild
Semiconductor in 1957 and
Intel in 1968. Coinvented the
integrated circuit. Many engineers from his teams went on
to found other seminal semiconductor companies
(© 2006, Intel Corporation.
Reproduced by permission).

almost all digital systems. In this section, we will peer beneath the digital
abstraction to see how logic gates are built from MOSFETs.
1.7.1 Semiconductors
MOS transistors are built from silicon, the predominant atom in rock
and sand. Silicon (Si) is a group IV atom, so it has four electrons in its
valence shell and forms bonds with four adjacent atoms, resulting in a
crystalline lattice. Figure 1.26(a) shows the lattice in two dimensions for
ease of drawing, but remember that the lattice actually forms a cubic
crystal. In the figure, a line represents a covalent bond. By itself, silicon is
a poor conductor because all the electrons are tied up in covalent bonds.
However, it becomes a better conductor when small amounts of impurities, called dopant atoms, are carefully added. If a group V dopant such
as arsenic (As) is added, the dopant atoms have an extra electron that is
not involved in the bonds. The electron can easily move about the lattice,
leaving an ionized dopant atom (As) behind, as shown in Figure
1.26(b). The electron carries a negative charge, so we call arsenic an
n-type dopant. On the other hand, if a group III dopant such as boron
(B) is added, the dopant atoms are missing an electron, as shown in
Figure 1.26(c). This missing electron is called a hole. An electron from a
neighboring silicon atom may move over to fill the missing bond, forming an ionized dopant atom (B) and leaving a hole at the neighboring
silicon atom. In a similar fashion, the hole can migrate around the lattice.
The hole is a lack of negative charge, so it acts like a positively charged
particle. Hence, we call boron a p-type dopant. Because the conductivity
of silicon changes over many orders of magnitude depending on the
concentration of dopants, silicon is called a semiconductor.
1.7.2 Diodes
The junction between p-type and n-type silicon is called a diode. The
p-type region is called the anode and the n-type region is called the cathode, as illustrated in Figure 1.27. When the voltage on the anode rises
above the voltage on the cathode, the diode is forward biased, and
1.7 CMOS Transistors 27
Si Si Si
Si Si Si
Si Si Si
(a)
Si As Si
Si Si Si
Si Si Si
(b)
-
+
Free electron
Si B Si
Si Si Si
Si Si Si
(c)
+
-
Free hole
Figure 1.26 Silicon lattice and
dopant atoms
C
current flows through the diode from the anode to the cathode. But
when the anode voltage is lower than the voltage on the cathode, the
diode is reverse biased, and no current flows. The diode symbol intuitively shows that current only flows in one direction.
1.7.3 Capacitors
A capacitor consists of two conductors separated by an insulator. When
a voltage V is applied to one of the conductors, the conductor accumulates electric charge Q and the other conductor accumulates the opposite
charge Q. The capacitance C of the capacitor is the ratio of charge to
voltage: C  Q/V. The capacitance is proportional to the size of the conductors and inversely proportional the distance between them. The symbol for a capacitor is shown in Figure 1.28.
Capacitance is important because charging or discharging a conductor takes time and energy. More capacitance means that a circuit will be
slower and require more energy to operate. Speed and energy will be discussed throughout this book.
1.7.4 nMOS and pMOS Transistors
A MOSFET is a sandwich of several layers of conducting and insulating
materials. MOSFETs are built on thin flat wafers of silicon of about
15 to 30 cm in diameter. The manufacturing process begins with a bare
wafer. The process involves a sequence of steps in which dopants are
implanted into the silicon, thin films of silicon dioxide and silicon are
grown, and metal is deposited. Between each step, the wafer is patterned
so that the materials appear only where they are desired. Because transistors are a fraction of a micron2 in length and the entire wafer is
processed at once, it is inexpensive to manufacture billions of transistors
at a time. Once processing is complete, the wafer is cut into rectangles
called chips or dice that contain thousands, millions, or even billions of
transistors. The chip is tested, then placed in a plastic or ceramic package with metal pins to connect it to a circuit board.
The MOSFET sandwich consists of a conducting layer called the
gate on top of an insulating layer of silicon dioxide (SiO2) on top of the
silicon wafer, called the substrate. Historically, the gate was constructed
from metal, hence the name metal-oxide-semiconductor. Modern manufacturing processes use polycrystalline silicon for the gate, because it
does not melt during subsequent high-temperature processing steps.
Silicon dioxide is better known as glass and is often simply called oxide
in the semiconductor industry. The metal-oxide-semiconductor sandwich forms a capacitor, in which a thin layer of insulating oxide called a
dielectric separates the metal and semiconductor plates.
28 CHAPTER ONE From Zero to One
p-type n-type
anode cathode
Figure 1.27 The p-n junction
diode structure and symbol
C
Figure 1.28 Capacitor symbol
Technicians in an Intel clean
room wear Gore-Tex bunny
suits to prevent particulates
from their hair, skin, and
clothing from contaminating
the microscopic transistors on
silicon wafers (© 2006, Intel
Corporation. Reproduced by
permission).
A 40-pin dual-inline package
(DIP) contains a small chip
(scarcely visible) in the center
that is connected to 40 metal
pins, 20 on a side, by gold
wires thinner than a strand of
hair (photograph by Kevin
Mapp. © Harvey Mudd
College). 2 1 
m  1 micron  106 m.
Chapt
There are two flavors of MOSFETs: nMOS and pMOS (pronounced
“n-moss” and “p-moss”). Figure 1.29 shows cross-sections of each type,
made by sawing through a wafer and looking at it from the side. The
n-type transistors, called nMOS, have regions of n-type dopants adjacent
to the gate called the source and the drain and are built on a p-type
semiconductor substrate. The pMOS transistors are just the opposite,
consisting of p-type source and drain regions in an n-type substrate.
A MOSFET behaves as a voltage-controlled switch in which the gate
voltage creates an electric field that turns ON or OFF a connection
between the source and drain. The term field effect transistor comes
from this principle of operation. Let us start by exploring the operation
of an nMOS transistor.
The substrate of an nMOS transistor is normally tied to GND, the
lowest voltage in the system. First, consider the situation when the gate is
also at 0 V, as shown in Figure 1.30(a). The diodes between the source or
drain and the substrate are reverse biased because the source or drain
voltage is nonnegative. Hence, there is no path for current to flow between
the source and drain, so the transistor is OFF. Now, consider when the gate
is raised to VDD, as shown in Figure 1.30(b). When a positive voltage is
applied to the top plate of a capacitor, it establishes an electric field that
attracts positive charge on the top plate and negative charge to the bottom
plate. If the voltage is sufficiently large, so much negative charge is
attracted to the underside of the gate that the region inverts from p-type to
effectively become n-type. This inverted region is called the channel. Now
the transistor has a continuous path from the n-type source through the
n-type channel to the n-type drain, so electrons can flow from source to
drain. The transistor is ON. The gate voltage required to turn on a transistor is called the threshold voltage, Vt, and is typically 0.3 to 0.7 V.
1.7 CMOS Transistors 29
n
p
source gate drain
substrate
SiO2
n
source gate drain
Polysilicon
n p p
gate
source drain
gate
source drain
substrate
(a) nMOS (b) pMOS
Figure 1.29 nMOS and pMOS transistors
The source and drain terminals are physically symmetric.
However, we say that charge
flows from the source to the
drain. In an nMOS transistor,
the charge is carried by electrons, which flow from negative voltage to positive
voltage. In a pMOS transistor,
the charge is carried by holes,
which flow from positive voltage to negative voltage. If we
draw schematics with the
most positive voltage at the
top and the most negative at
the bottom, the source of
(negative) charges in an
nMOS transistor is the bottom
terminal and the source of
(positive) charges in a pMOS
transistor is the top terminal.
A technician holds a 12-inch
wafer containing hundreds
of microprocessor chips
(© 2006, Intel Corporation.
Reproduced by permission).

pMOS transistors work in just the opposite fashion, as might be
guessed from the bubble on their symbol. The substrate is tied to VDD.
When the gate is also at VDD, the pMOS transistor is OFF. When the gate
is at GND, the channel inverts to p-type and the pMOS transistor is ON.
Unfortunately, MOSFETs are not perfect switches. In particular,
nMOS transistors pass 0’s well but pass 1’s poorly. Specifically, when the
gate of an nMOS transistor is at VDD, the drain will only swing between
0 and VDD  Vt. Similarly, pMOS transistors pass 1’s well but 0’s
poorly. However, we will see that it is possible to build logic gates that
use transistors only in their good mode.
nMOS transistors need a p-type substrate, and pMOS transistors
need an n-type substrate. To build both flavors of transistors on the
same chip, manufacturing processes typically start with a p-type wafer,
then implant n-type regions called wells where the pMOS transistors
should go. These processes that provide both flavors of transistors are
called Complementary MOS or CMOS. CMOS processes are used to
build the vast majority of all transistors fabricated today.
In summary, CMOS processes give us two types of electrically
controlled switches, as shown in Figure 1.31. The voltage at the gate (g)
regulates the flow of current between the source (s) and drain (d). nMOS
transistors are OFF when the gate is 0 and ON when the gate is 1.
30 CHAPTER ONE From Zero to One
n
p
gate
source drain
substrate
n
(a)
GND
GND
n
p
source gate drain
substrate
n
(b)
VDD
GND
-------
channel
+++++++
Figure 1.30 nMOS transistor operation
Figure 1.31 Switch models of
MOSFETs
g
s
d
g
d
s
nMOS
pMOS
g = 0
s
d
d
s
OFF
ON
g = 1
s
d
d
s
ON
OFF
Gordon Moore, 1929–. Born in
San Francisco. Received a
B.S. in chemistry from UC
Berkeley and a Ph.D. in chemistry and physics from
Caltech. Cofounded Intel in
1968 with Robert Noyce.
Observed in 1965 that the
number of transistors on a
computer chip doubles every
year. This trend has become
known as Moore’s Law. Since
1975, transistor counts have
doubled every two years.
A corollary of Moore’s
Law is that microprocessor
performance doubles every
18 to 24 months. Semiconductor sales have also
increased exponentially.
Unfortunately, power consumption has increased
exponentially as well
(© 2006, Intel Corporation.
Reproduced by permission).
C
pMOS transistors are just the opposite: ON when the gate is 0 and OFF
when the gate is 1.
1.7.5 CMOS NOT Gate
Figure 1.32 shows a schematic of a NOT gate built with CMOS transistors. The triangle indicates GND, and the flat bar indicates VDD; these
labels will be omitted from future schematics. The nMOS transistor, N1,
is connected between GND and the Y output. The pMOS transistor, P1,
is connected between VDD and the Y output. Both transistor gates are
controlled by the input, A.
If A  0, N1 is OFF and P1 is ON. Hence, Y is connected to VDD
but not to GND, and is pulled up to a logic 1. P1 passes a good 1. If
A  1, N1 is ON and P1 is OFF, and Y is pulled down to a logic 0. N1
passes a good 0. Checking against the truth table in Figure 1.12, we see
that the circuit is indeed a NOT gate.
1.7.6 Other CMOS Logic Gates
Figure 1.33 shows a schematic of a two-input NAND gate. In schematic
diagrams, wires are always joined at three-way junctions. They are
joined at four-way junctions only if a dot is shown. The nMOS transistors N1 and N2 are connected in series; both nMOS transistors must be
ON to pull the output down to GND. The pMOS transistors P1 and P2
are in parallel; only one pMOS transistor must be ON to pull the output
up to VDD. Table 1.6 lists the operation of the pull-down and pull-up
networks and the state of the output, demonstrating that the gate does
function as a NAND. For example, when A  1 and B  0, N1 is ON,
but N2 is OFF, blocking the path from Y to GND. P1 is OFF, but P2 is
ON, creating a path from VDD to Y. Therefore, Y is pulled up to 1.
Figure 1.34 shows the general form used to construct any inverting
logic gate, such as NOT, NAND, or NOR. nMOS transistors are good at
passing 0’s, so a pull-down network of nMOS transistors is placed between
the output and GND to pull the output down to 0. pMOS transistors are
1.7 CMOS Transistors 31
Figure 1.32 NOT gate schematic
Figure 1.33 Two-input NAND
gate schematic
VDD
A Y
GND
N1
P1
A
B
Y
N2
N1
P2 P1
Table 1.6 NAND gate operation
A B Pull-Down Network Pull-Up Network Y
0 0 OFF ON 1
0 1 OFF ON 1
1 0 OFF ON 1
1 1 ON OFF 0
pMOS
pull-up
network
output
inputs
nMOS
pull-down
network
Figure 1.34 General form of an
inverting logic gate
Chap
good at passing 1’s, so a pull-up network of pMOS transistors is placed
between the output and VDD to pull the output up to 1. The networks may
consist of transistors in series or in parallel. When transistors are in parallel, the network is ON if either transistor is ON. When transistors are in
series, the network is ON only if both transistors are ON. The slash across
the input wire indicates that the gate may receive multiple inputs.
If both the pull-up and pull-down networks were ON simultaneously, a short circuit would exist between VDD and GND. The output of
the gate might be in the forbidden zone and the transistors would consume large amounts of power, possibly enough to burn out. On the other
hand, if both the pull-up and pull-down networks were OFF simultaneously, the output would be connected to neither VDD nor GND. We say
that the output floats. Its value is again undefined. Floating outputs are
usually undesirable, but in Section 2.6 we will see how they can occasionally be used to the designer’s advantage.
In a properly functioning logic gate, one of the networks should be
ON and the other OFF at any given time, so that the output is pulled
HIGH or LOW but not shorted or floating. We can guarantee this by
using the rule of conduction complements. When nMOS transistors are
in series, the pMOS transistors must be in parallel. When nMOS transistors are in parallel, the pMOS transistors must be in series.
Example 1.20 THREE-INPUT NAND SCHEMATIC
Draw a schematic for a three-input NAND gate using CMOS transistors.
Solution: The NAND gate should produce a 0 output only when all three inputs
are 1. Hence, the pull-down network should have three nMOS transistors in
series. By the conduction complements rule, the pMOS transistors must be in
parallel. Such a gate is shown in Figure 1.35; you can verify the function by
checking that it has the correct truth table.
Example 1.21 TWO-INPUT NOR SCHEMATIC
Draw a schematic for a two-input NOR gate using CMOS transistors.
Solution: The NOR gate should produce a 0 output if either input is 1. Hence,
the pull-down network should have two nMOS transistors in parallel. By the
conduction complements rule, the pMOS transistors must be in series. Such a
gate is shown in Figure 1.36.
Example 1.22 TWO-INPUT AND SCHEMATIC
Draw a schematic for a two-input AND gate.
32 CHAPTER ONE From Zero to One
A
B
Y
C
A
B
Y
Figure 1.35 Three-input NAND
gate schematic
Figure 1.36 Two-input NOR gate
schematic
Experienced designers claim
that electronic devices operate
because they contain magic
smoke. They confirm this theory with the observation that
if the magic smoke is ever let
out of the device, it ceases to
work.

Solution: It is impossible to build an AND gate with a single CMOS gate.
However, building NAND and NOT gates is easy. Thus, the best way to build an
AND gate using CMOS transistors is to use a NAND followed by a NOT, as
shown in Figure 1.37.
1.7.7 Transmission Gates
At times, designers find it convenient to use an ideal switch that can pass
both 0 and 1 well. Recall that nMOS transistors are good at passing 0
and pMOS transistors are good at passing 1, so the parallel combination
of the two passes both values well. Figure 1.38 shows such a circuit,
called a transmission gate or pass gate. The two sides of the switch are
called A and B because a switch is bidirectional and has no preferred
input or output side. The control signals are called enables, EN and EN
___
.
When EN  0 and EN
___  1, both transistors are OFF. Hence, the transmission gate is OFF or disabled, so A and B are not connected. When EN
 1 and EN
___  0, the transmission gate is ON or enabled, and any logic
value can flow between A and B.
1.7.8 Pseudo-nMOS Logic
An N-input CMOS NOR gate uses N nMOS transistors in parallel and
N pMOS transistors in series. Transistors in series are slower than
transistors in parallel, just as resistors in series have more resistance
than resistors in parallel. Moreover, pMOS transistors are slower than
nMOS transistors because holes cannot move around the silicon lattice
as fast as electrons. Therefore the parallel nMOS transistors are fast
and the series pMOS transistors are slow, especially when many are in
series.
Pseudo-nMOS logic replaces the slow stack of pMOS transistors
with a single weak pMOS transistor that is always ON, as shown in
Figure 1.39. This pMOS transistor is often called a weak pull-up. The
physical dimensions of the pMOS transistor are selected so that the
pMOS transistor will pull the output, Y, HIGH weakly—that is, only if
none of the nMOS transistors are ON. But if any nMOS transistor is
ON, it overpowers the weak pull-up and pulls Y down close enough to
GND to produce a logic 0.
The advantage of pseudo-nMOS logic is that it can be used to build
fast NOR gates with many inputs. For example, Figure 1.40 shows a
pseudo-nMOS four-input NOR. Pseudo-nMOS gates are useful for certain
memory and logic arrays discussed in Chapter 5. The disadvantage is that
a short circuit exists between VDD and GND when the output is LOW; the
weak pMOS and nMOS transistors are both ON. The short circuit draws
continuous power, so pseudo-nMOS logic must be used sparingly.
1.7 CMOS Transistors 33
Figure 1.40 Pseudo-nMOS fourinput NOR gate
Figure 1.37 Two-input AND gate
schematic
Figure 1.38 Transmission gate
Figure 1.39 Generic pseudonMOS gate
A
B Y
A B
EN
EN
Y
inputs nMOS
pull-down
network
weak
A B
Y
weak
C D
Chap
Pseudo-nMOS gates got their name from the 1970’s, when manufacturing processes only had nMOS transistors. A weak nMOS transistor
was used to pull the output HIGH because pMOS transistors were not
available.
1.8 POWER CONSUMPTION*
Power consumption is the amount of energy used per unit time. Power
consumption is of great importance in digital systems. The battery life of
portable systems such as cell phones and laptop computers is limited by
power consumption. Power is also significant for systems that are
plugged in, because electricity costs money and because the system will
overheat if it draws too much power.
Digital systems draw both dynamic and static power. Dynamic
power is the power used to charge capacitance as signals change between
0 and 1. Static power is the power used even when signals do not change
and the system is idle.
Logic gates and the wires that connect them have capacitance. The
energy drawn from the power supply to charge a capacitance C to voltage VDD is CVDD
2. If the voltage on the capacitor switches at frequency
f (i.e., f times per second), it charges the capacitor f/2 times and discharges it f/2 times per second. Discharging does not draw energy from
the power supply, so the dynamic power consumption is
Pdynamic  CV2
DDf (1.4)
Electrical systems draw some current even when they are idle. When
transistors are OFF, they leak a small amount of current. Some circuits,
such as the pseudo-nMOS gate discussed in Section 1.7.8, have a path
from VDD to GND through which current flows continuously. The total
static current, IDD, is also called the leakage current or the quiescent
supply current flowing between VDD and GND. The static power consumption is proportional to this static current:
Pstatic  IDDVDD (1.5)
Example 1.23 POWER CONSUMPTION
A particular cell phone has a 6 watt-hour (W-hr) battery and operates at 1.2 V.
Suppose that, when it is in use, the cell phone operates at 300 MHz and the
average amount of capacitance in the chip switching at any given time is 10 nF
(108 Farads). When in use, it also broadcasts 3 W of power out of its antenna.
When the phone is not in use, the dynamic power drops to almost zero because
the signal processing is turned off. But the phone also draws 40 mA of quiescent
current whether it is in use or not. Determine the battery life of the phone (a) if it
is not being used, and (b) if it is being used continuously.
1
2
34 CHAPTER ONE From Zero to One
Cha
Solution: The static power is Pstatic  (0.040 A)(1.2 V)  48 mW. If the phone is
not being used, this is the only power consumption, so the battery life is (6 Whr)/(0.048 W)  125 hours (about 5 days). If the phone is being used, the
dynamic power is Pdynamic  (0.5)(108 F)(1.2 V)2(3  108 Hz)  2.16 W.
Together with the static and broadcast power, the total active power is 2.16 W 
0.048 W  3 W  5.2 W, so the battery life is 6 W-hr/5.2 W  1.15 hours. This
example somewhat oversimplifies the actual operation of a cell phone, but it
illustrates the key ideas of power consumption.
1.9 SUMMARY AND A LOOK AHEAD
There are 10 kinds of people in this world: those who can count in
binary and those who can’t.
This chapter has introduced principles for understanding and designing
complex systems. Although the real world is analog, digital designers
discipline themselves to use a discrete subset of possible signals. In particular, binary variables have just two states: 0 and 1, also called FALSE
and TRUE or LOW and HIGH. Logic gates compute a binary output
from one or more binary inputs. Some of the common logic gates are:
 NOT: TRUE when input is FALSE
 AND: TRUE when all inputs are TRUE
 OR: TRUE when any inputs are TRUE
 XOR: TRUE when an odd number of inputs are TRUE
Logic gates are commonly built from CMOS transistors, which
behave as electrically controlled switches. nMOS transistors turn ON
when the gate is 1. pMOS transistors turn ON when the gate is 0.
In Chapters 2 through 5, we continue the study of digital logic.
Chapter 2 addresses combinational logic, in which the outputs depend
only on the current inputs. The logic gates introduced already are examples of combinational logic. You will learn to design circuits involving
multiple gates to implement a relationship between inputs and outputs
specified by a truth table or Boolean equation. Chapter 3 addresses
sequential logic, in which the outputs depend on both current and past
inputs. Registers are common sequential elements that remember their
previous input. Finite state machines, built from registers and combinational logic, are a powerful way to build complicated systems in a systematic fashion. We also study timing of digital systems to analyze how
fast the systems can operate. Chapter 4 describes hardware description
languages (HDLs). HDLs are related to conventional programming
languages but are used to simulate and build hardware rather than
software. Most digital systems today are designed with HDLs. Verilog
1.9 Summary and a Look Ahead 35
Chapter 01.q
and VHDL are the two prevalent languages, and they are covered sideby-side in this book. Chapter 5 studies other combinational and sequential building blocks such as adders, multipliers, and memories.
Chapter 6 shifts to computer architecture. It describes the MIPS
processor, an industry-standard microprocessor used in consumer electronics, some Silicon Graphics workstations, and many communications
systems such as televisions, networking hardware and wireless links. The
MIPS architecture is defined by its registers and assembly language
instruction set. You will learn to write programs in assembly language
for the MIPS processor so that you can communicate with the processor
in its native language.
Chapters 7 and 8 bridge the gap between digital logic and computer
architecture. Chapter 7 investigates microarchitecture, the arrangement
of digital building blocks, such as adders and registers, needed to construct a processor. In that chapter, you learn to build your own MIPS
processor. Indeed, you learn three microarchitectures illustrating different
trade-offs of performance and cost. Processor performance has increased
exponentially, requiring ever more sophisticated memory systems to feed
the insatiable demand for data. Chapter 8 delves into memory system
architecture and also describes how computers communicate with peripheral devices such as keyboards and printers.
36 CHAPTER ONE From Zero to One

Exercises 37
Exercises
Exercise 1.1 Explain in one paragraph at least three levels of abstraction that are
used by
a) biologists studying the operation of cells.
b) chemists studying the composition of matter.
Exercise 1.2 Explain in one paragraph how the techniques of hierarchy,
modularity, and regularity may be used by
a) automobile designers.
b) businesses to manage their operations.
Exercise 1.3 Ben Bitdiddle is building a house. Explain how he can use the
principles of hierarchy, modularity, and regularity to save time and money
during construction.
Exercise 1.4 An analog voltage is in the range of 0–5 V. If it can be measured with
an accuracy of  50 mV, at most how many bits of information does it convey?
Exercise 1.5 A classroom has an old clock on the wall whose minute hand
broke off.
a) If you can read the hour hand to the nearest 15 minutes, how many bits
of information does the clock convey about the time?
b) If you know whether it is before or after noon, how many additional bits
of information do you know about the time?
Exercise 1.6 The Babylonians developed the sexagesimal (base 60) number system about 4000 years ago. How many bits of information is conveyed with one
sexagesimal digit? How do you write the number 400010 in sexagesimal?
Exercise 1.7 How many different numbers can be represented with 16 bits?
Exercise 1.8 What is the largest unsigned 32-bit binary number?
Exercise 1.9 What is the largest 16-bit binary number that can be represented
with
a) unsigned numbers?
b) two’s complement numbers?
c) sign/magnitude numbers?

38 CHAPTER ONE From Zero to One
Exercise 1.10 What is the smallest (most negative) 16-bit binary number that
can be represented with
a) unsigned numbers?
b) two’s complement numbers?
c) sign/magnitude numbers?
Exercise 1.11 Convert the following unsigned binary numbers to decimal.
a) 10102
b) 1101102
c) 111100002
d) 00011000101001112
Exercise 1.12 Repeat Exercise 1.11, but convert to hexadecimal.
Exercise 1.13 Convert the following hexadecimal numbers to decimal.
a) A516
b) 3B16
c) FFFF16
d) D000000016
Exercise 1.14 Repeat Exercise 1.13, but convert to unsigned binary.
Exercise 1.15 Convert the following two’s complement binary numbers to decimal.
a) 10102
b) 1101102
c) 011100002
d) 100111112
Exercise 1.16 Repeat Exercise 1.15, assuming the binary numbers are in
sign/magnitude form rather than two’s complement representation.
Exercise 1.17 Convert the following decimal numbers to unsigned binary
numbers.
a) 4210
b) 6310

c) 22910
d) 84510
Exercise 1.18 Repeat Exercise 1.17, but convert to hexadecimal.
Exercise 1.19 Convert the following decimal numbers to 8-bit two’s complement
numbers or indicate that the decimal number would overflow the range.
a) 4210
b) 6310
c) 12410
d) 12810
e) 13310
Exercise 1.20 Repeat Exercise 1.19, but convert to 8-bit sign/magnitude
numbers.
Exercise 1.21 Convert the following 4-bit two’s complement numbers to 8-bit
two’s complement numbers.
a) 01012
b) 10102
Exercise 1.22 Repeat Exercise 1.21 if the numbers are unsigned rather than
two’s complement.
Exercise 1.23 Base 8 is referred to as octal. Convert each of the numbers from
Exercise 1.17 to octal.
Exercise 1.24 Convert each of the following octal numbers to binary,
hexadecimal, and decimal.
a) 428
b) 638
c) 2558
d) 30478
Exercise 1.25 How many 5-bit two’s complement numbers are greater than 0?
How many are less than 0? How would your answers differ for sign/magnitude
numbers?
Exercises 39
Ch
Exercise 1.26 How many bytes are in a 32-bit word? How many nibbles are in
the word?
Exercise 1.27 How many bytes are in a 64-bit word?
Exercise 1.28 A particular DSL modem operates at 768 kbits/sec. How many
bytes can it receive in 1 minute?
Exercise 1.29 Hard disk manufacturers use the term “megabyte” to mean 106
bytes and “gigabyte” to mean 109 bytes. How many real GBs of music can you
store on a 50 GB hard disk?
Exercise 1.30 Estimate the value of 231 without using a calculator.
Exercise 1.31 A memory on the Pentium II microprocessor is organized as a rectangular array of bits with 28 rows and 29 columns. Estimate how many bits it
has without using a calculator.
Exercise 1.32 Draw a number line analogous to Figure 1.11 for 3-bit unsigned,
two’s complement, and sign/magnitude numbers.
Exercise 1.33 Perform the following additions of unsigned binary numbers.
Indicate whether or not the sum overflows a 4-bit result.
a) 10012  01002
b) 11012  10112
Exercise 1.34 Perform the following additions of unsigned binary numbers.
Indicate whether or not the sum overflows an 8-bit result.
a) 100110012  010001002
b) 110100102  101101102
Exercise 1.35 Repeat Exercise 1.34, assuming that the binary numbers are in
two’s complement form.
Exercise 1.36 Convert the following decimal numbers to 6-bit two’s complement
binary numbers and add them. Indicate whether or not the sum overflows a 6-bit
result.
a) 1610  910
b) 2710  3110
c) 410  1910
40 CHAPTER ONE From Zero to One
C
d) 310  3210
e) 1610  910
f) 2710  3110
Exercise 1.37 Perform the following additions of unsigned hexadecimal
numbers. Indicate whether or not the sum overflows an 8-bit (two hex digit)
result.
a) 716  916
b) 1316  2816
c) AB16  3E16
d) 8F16  AD16
Exercise 1.38 Convert the following decimal numbers to 5-bit two’s complement
binary numbers and subtract them. Indicate whether or not the difference overflows a 5-bit result.
a) 910  710
b) 1210  1510
c) 610  1110
d) 410  810
Exercise 1.39 In a biased N-bit binary number system with bias B, positive and
negative numbers are represented as their value plus the bias B. For example, for
5-bit numbers with a bias of 15, the number 0 is represented as 01111, 1 as
10000, and so forth. Biased number systems are sometimes used in floating point
mathematics, which will be discussed in Chapter 5. Consider a biased 8-bit
binary number system with a bias of 12710.
a) What decimal value does the binary number 100000102 represent?
b) What binary number represents the value 0?
c) What is the representation and value of the most negative number?
d) What is the representation and value of the most positive number?
Exercise 1.40 Draw a number line analogous to Figure 1.11 for 3-bit biased
numbers with a bias of 3 (see Exercise 1.39 for a definition of biased numbers).
Exercise 1.41 In a binary coded decimal (BCD) system, 4 bits are used to represent a decimal digit from 0 to 9. For example, 3710 is written as 00110111BCD.
Exercises 41
Chapter 01.
a) Write 28910 in BCD.
b) Convert 100101010001BCD to decimal.
c) Convert 01101001BCD to binary.
d) Explain why BCD might be a useful way to represent numbers.
Exercise 1.42 A flying saucer crashes in a Nebraska cornfield. The FBI investigates the wreckage and finds an engineering manual containing an equation in
the Martian number system: 325  42  411. If this equation is correct, how
many fingers would you expect Martians have?
Exercise 1.43 Ben Bitdiddle and Alyssa P. Hacker are having an argument. Ben
says, “All integers greater than zero and exactly divisible by six have exactly two
1’s in their binary representation.” Alyssa disagrees. She says, “No, but all such
numbers have an even number of 1’s in their representation.” Do you agree with
Ben or Alyssa or both or neither? Explain.
Exercise 1.44 Ben Bitdiddle and Alyssa P. Hacker are having another argument.
Ben says, “I can get the two’s complement of a number by subtracting 1, then
inverting all the bits of the result.” Alyssa says, “No, I can do it by examining
each bit of the number, starting with the least significant bit. When the first 1 is
found, invert each subsequent bit.” Do you agree with Ben or Alyssa or both or
neither? Explain.
Exercise 1.45 Write a program in your favorite language (e.g., C, Java, Perl) to
convert numbers from binary to decimal. The user should type in an unsigned
binary number. The program should print the decimal equivalent.
Exercise 1.46 Repeat Exercise 1.45 but convert from decimal to hexadecimal.
Exercise 1.47 Repeat Exercise 1.45 but convert from an arbitrary base b1 to
another base b2, as specified by the user. Support bases up to 16, using the letters
of the alphabet for digits greater than 9. The user should enter b1, b2, and then
the number to convert in base b1. The program should print the equivalent number in base b2.
Exercise 1.48 Draw the symbol, Boolean equation, and truth table for
a) a three-input OR gate.
b) a three-input exclusive OR (XOR) gate.
c) a four-input XNOR gate
42 CHAPTER ONE From Zero to One
C
Exercise 1.49 A majority gate produces a TRUE output if and only if more than
half of its inputs are TRUE. Complete a truth table for the three-input majority
gate shown in Figure 1.41.
Exercise 1.50 A three-input AND-OR (AO) gate shown in Figure 1.42 produces
a TRUE output if both A and B are TRUE, or if C is TRUE. Complete a truth
table for the gate.
Exercise 1.51 A three-input OR-AND-INVERT (OAI) gate shown in Figure
1.43 produces a FALSE input if C is TRUE and A or B is TRUE. Otherwise it
produces a TRUE output. Complete a truth table for the gate.
Exercise 1.52 There are 16 different truth tables for Boolean functions of two
variables. List each truth table. Give each one a short descriptive name (such as
OR, NAND, and so on).
Exercise 1.53 How many different truth tables exist for Boolean functions of N
variables?
Exercise 1.54 Is it possible to assign logic levels so that a device with the transfer characteristics shown in Figure 1.44 would serve as an inverter? If so, what
are the input and output low and high levels (VIL, VOL, VIH, and VOH) and
noise margins (NML and NMH)? If not, explain why not.
Exercises 43
A
B Y
C
MAJ
A
B Y C
A
B Y C
Figure 1.41 Three-input majority gate
Figure 1.42 Three-input AND-OR gate
Figure 1.43 Three-input OR-AND-INVERT gate

Exercise 1.55 Repeat Exercise 1.54 for the transfer characteristics shown in
Figure 1.45.
Exercise 1.56 Is it possible to assign logic levels so that a device with the transfer characteristics shown in Figure 1.46 would serve as a buffer? If so, what are
the input and output low and high levels (VIL, VOL, VIH, and VOH) and noise
margins (NML and NMH)? If not, explain why not.
44 CHAPTER ONE From Zero to One
Figure 1.45 DC transfer characteristics
Vin
Vout
012345
0
1
2
3
4
5
Figure 1.44 DC transfer characteristics
Vin
Vout
012345
0
1
2
3
4
5

Exercise 1.57 Ben Bitdiddle has invented a circuit with the transfer characteristics shown in Figure 1.47 that he would like to use as a buffer. Will it work?
Why or why not? He would like to advertise that it is compatible with LVCMOS
and LVTTL logic. Can Ben’s buffer correctly receive inputs from those logic
families? Can its output properly drive those logic families? Explain.
Exercise 1.58 While walking down a dark alley, Ben Bitdiddle encounters a twoinput gate with the transfer function shown in Figure 1.48. The inputs are A and
B and the output is Y.
a) What kind of logic gate did he find?
b) What are the approximate high and low logic levels?
Exercises 45
Vin
Vout
012345
0
1
2
3
4
5
Vin
Vout
0
0
0.6
1.2
1.8
2.4
3.0
3.3
0.6 1.2 1.8 2.4 3.0 3.3
Figure 1.46 DC transfer characteristics
Figure 1.47 Ben’s buffer DC transfer characteristics

Exercise 1.59 Repeat Exercise 1.58 for Figure 1.49.
Exercise 1.60 Sketch a transistor-level circuit for the following CMOS gates. Use
a minimum number of transistors.
a) A four-input NAND gate.
b) A three-input OR-AND-INVERT gate (see Exercise 1.51).
c) A three-input AND-OR gate (see Exercise 1.50).
Exercise 1.61 A minority gate produces a TRUE output if and only if fewer
than half of its inputs are TRUE. Otherwise it produces a FALSE output. Sketch
a transistor-level circuit for a CMOS minority gate. Use a minimum number of
transistors.
46 CHAPTER ONE From Zero to One
Figure 1.48 Two-input DC transfer characteristics
Figure 1.49 Two-input DC transfer characteristics
0
1
2
3
0
1
2
3
0
1
2
3
A B
Y
0
1
2
3
0
1
2
3
0
1
2
3
A B
Y

Exercise 1.62 Write a truth table for the function performed by the gate in
Figure 1.50. The truth table should have two inputs, A and B. What is the name
of this function?
Exercise 1.63 Write a truth table for the function performed by the gate in
Figure 1.51. The truth table should have three inputs, A, B, and C.
Exercise 1.64 Implement the following three-input gates using only pseudonMOS logic gates. Your gates receive three inputs, A, B, and C. Use a minimum
number of transistors.
a) three-input NOR gate
b) three-input NAND gate
a) three-input AND gate
Exercise 1.65 Resistor-Transistor Logic (RTL) uses nMOS transistors to pull
the gate output LOW and a weak resistor to pull the output HIGH when none
of the paths to ground are active. A NOT gate built using RTL is shown in
Figure 1.52. Sketch a three-input RTL NOR gate. Use a minimum number of
transistors.
Exercises 47
A
B
A
B
A
A
B
B
Y
A
B
C
C
A B
Y
A
Y
weak
Figure 1.50 Mystery schematic
Figure 1.51 Mystery schematic
Figure 1.52 RTL NOT gate

Interview Questions
These questions have been asked at interviews for digital design jobs.
Question 1.1 Sketch a transistor-level circuit for a CMOS four-input NOR gate.
Question 1.2 The king receives 64 gold coins in taxes but has reason to believe
that one is counterfeit. He summons you to identify the fake coin. You have a
balance that can hold coins on each side. How many times do you need to use
the balance to find the lighter, fake coin?
Question 1.3 The professor, the teaching assistant, the digital design student, and
the freshman track star need to cross a rickety bridge on a dark night. The
bridge is so shakey that only two people can cross at a time. They have only one
flashlight among them and the span is too long to throw the flashlight, so somebody must carry it back to the other people. The freshman track star can cross
the bridge in 1 minute. The digital design student can cross the bridge in 2 minutes. The teaching assistant can cross the bridge in 5 minutes. The professor
always gets distracted and takes 10 minutes to cross the bridge. What is the
fastest time to get everyone across the bridge?
48 CHAPTER ONE From Zero to One



2
2.1 Introduction
2.2 Boolean Equations
2.3 Boolean Algebra
2.4 From Logic to Gates
2.5 Multilevel Combinational
Logic
2.6 X’s and Z’s, Oh My
2.7 Karnaugh Maps
2.8 Combinational Building
Blocks
2.9 Timing
2.10 Summary
Exercises
Interview Questions
Combinational Logic Design
2.1 INTRODUCTION
In digital electronics, a circuit is a network that processes discretevalued variables. A circuit can be viewed as a black box, shown in
Figure 2.1, with
 one or more discrete-valued input terminals
 one or more discrete-valued output terminals
 a functional specification describing the relationship between inputs
and outputs
 a timing specification describing the delay between inputs changing
and outputs responding.
Peering inside the black box, circuits are composed of nodes and elements. An element is itself a circuit with inputs, outputs, and a specification. A node is a wire, whose voltage conveys a discrete-valued variable.
Nodes are classified as input, output, or internal. Inputs receive values
from the external world. Outputs deliver values to the external world.
Wires that are not inputs or outputs are called internal nodes. Figure 2.2
51
inputs outputs
functional spec
timing spec
Figure 2.1 Circuit as a black
box with inputs, outputs, and
specifications
Figure 2.2 Elements and nodes
A E1
E2
B E3
C
n1
Y
Z
Chap
illustrates a circuit with three elements, E1, E2, and E3, and six nodes.
Nodes A, B, and C are inputs. Y and Z are outputs. n1 is an internal
node between E1 and E3.
Digital circuits are classified as combinational or sequential. A combinational circuit’s outputs depend only on the current values of the
inputs; in other words, it combines the current input values to compute
the output. For example, a logic gate is a combinational circuit. A
sequential circuit’s outputs depend on both current and previous values
of the inputs; in other words, it depends on the input sequence. A combinational circuit is memoryless, but a sequential circuit has memory. This
chapter focuses on combinational circuits, and Chapter 3 examines
sequential circuits.
The functional specification of a combinational circuit expresses
the output values in terms of the current input values. The timing
specification of a combinational circuit consists of lower and upper
bounds on the delay from input to output. We will initially concentrate
on the functional specification, then return to the timing specification
later in this chapter.
Figure 2.3 shows a combinational circuit with two inputs and one
output. On the left of the figure are the inputs, A and B, and on the right
is the output, Y. The symbol CL inside the box indicates that it is implemented using only combinational logic. In this example, the function, F,
is specified to be OR: Y  F(A, B)  A  B. In words, we say the
output, Y, is a function of the two inputs, A and B, namely Y  A OR B.
Figure 2.4 shows two possible implementations for the combinational logic circuit in Figure 2.3. As we will see repeatedly throughout
the book, there are often many implementations for a single function.
You choose which to use given the building blocks at your disposal and
your design constraints. These constraints often include area, speed,
power, and design time.
Figure 2.5 shows a combinational circuit with multiple outputs. This
particular combinational circuit is called a full adder and we will revisit
it in Section 5.2.1. The two equations specify the function of the outputs, S and Cout, in terms of the inputs, A, B, and Cin.
To simplify drawings, we often use a single line with a slash through
it and a number next to it to indicate a bus, a bundle of multiple signals.
The number specifies how many signals are in the bus. For example,
Figure 2.6(a) represents a block of combinational logic with three inputs
and two outputs. If the number of bits is unimportant or obvious from
the context, the slash may be shown without a number. Figure 2.6(b)
indicates two blocks of combinational logic with an arbitrary number of
outputs from one block serving as inputs to the second block.
The rules of combinational composition tell us how we can build
a large combinational circuit from smaller combinational circuit
52 CHAPTER TWO Combinational Logic Design
Figure 2.3 Combinational logic
circuit
Figure 2.4 Two OR
implementations
A
B Y
Y = F(A, B) = A + B
CL
A
B Y
(a)
Y
(b)
A
B
Figure 2.5 Multiple-output
combinational circuit
Figure 2.6 Slash notation for
multiple signals
A S
S = A ⊕ B ⊕ Cin
Cout = AB + ACin + BCin
B
Cin
CL Cout
CL 3
(a)
CL CL
(b)
2
Chap
elements. A circuit is combinational if it consists of interconnected
circuit elements such that
 Every circuit element is itself combinational.
 Every node of the circuit is either designated as an input to the
circuit or connects to exactly one output terminal of a circuit
element.
 The circuit contains no cyclic paths: every path through the circuit
visits each circuit node at most once.
Example 2.1 COMBINATIONAL CIRCUITS
Which of the circuits in Figure 2.7 are combinational circuits according to the
rules of combinational composition?
Solution: Circuit (a) is combinational. It is constructed from two combinational
circuit elements (inverters I1 and I2). It has three nodes: n1, n2, and n3. n1 is an
input to the circuit and to I1; n2 is an internal node, which is the output of I1
and the input to I2; n3 is the output of the circuit and of I2. (b) is not combinational, because there is a cyclic path: the output of the XOR feeds back to one of
its inputs. Hence, a cyclic path starting at n4 passes through the XOR to n5,
which returns to n4. (c) is combinational. (d) is not combinational, because node
n6 connects to the output terminals of both I3 and I4. (e) is combinational,
illustrating two combinational circuits connected to form a larger combinational
circuit. (f) does not obey the rules of combinational composition because it has a
cyclic path through the two elements. Depending on the functions of the
elements, it may or may not be a combinational circuit.
Large circuits such as microprocessors can be very complicated, so
we use the principles from Chapter 1 to manage the complexity. Viewing
a circuit as a black box with a well-defined interface and function is an
2.1 Introduction 53
The rules of combinational
composition are sufficient but
not strictly necessary. Certain
circuits that disobey these
rules are still combinational,
so long as the outputs depend
only on the current values of
the inputs. However, determining whether oddball circuits are combinational is
more difficult, so we will usually restrict ourselves to combinational composition as a
way to build combinational
circuits.
Figure 2.7 Example circuits
(a)
n1 n2 n3 I1 I2
(c)
CL
CL
(e)
n4
n5
(b)
n6
I3
I4
(d)
CL
CL
(f)
Cha
application of abstraction and modularity. Building the circuit out of
smaller circuit elements is an application of hierarchy. The rules of
combinational composition are an application of discipline.
The functional specification of a combinational circuit is usually
expressed as a truth table or a Boolean equation. In the next sections, we
describe how to derive a Boolean equation from any truth table and how
to use Boolean algebra and Karnaugh maps to simplify equations. We
show how to implement these equations using logic gates and how to
analyze the speed of these circuits.
2.2 BOOLEAN EQUATIONS
Boolean equations deal with variables that are either TRUE or FALSE,
so they are perfect for describing digital logic. This section defines some
terminology commonly used in Boolean equations, then shows how to
write a Boolean equation for any logic function given its truth table.
2.2.1 Terminology
The complement of a variable, A, is its inverse, . The variable or its
complement is called a literal. For example, A, , B, and are literals.
We call A the true form of the variable and the complementary form;
“true form” does not mean that A is TRUE, but merely that A does not
have a line over it.
The AND of one or more literals is called a product or an implicant.
, , and B are all implicants for a function of three variables.
A minterm is a product involving all of the inputs to the function.
is a minterm for a function of the three variables A, B, and C, but is
not, because it does not involve C. Similarly, the OR of one or more
literals is called a sum. A maxterm is a sum involving all of the inputs
to the function. is a maxterm for a function of the three
variables A, B, and C.
The order of operations is important when interpreting Boolean
equations. Does Y  A  BC mean Y  (A OR B) AND C or Y  A
OR (B AND C)? In Boolean equations, NOT has the highest precedence,
followed by AND, then OR. Just as in ordinary equations, products are
performed before sums. Therefore, the equation is read as Y  A OR
(B AND C). Equation 2.1 gives another example of order of operations.
(2.1)
2.2.2 Sum-of-Products Form
A truth table of N inputs contains 2N rows, one for each possible value
of the inputs. Each row in a truth table is associated with a minterm
AB  BCD  ((A)B)  (BC(D))
A  B  C
AB
ABC
AB ABC
A
A B
A
54 CHAPTER TWO Combinational Logic Design
Chapter 02
that is TRUE for that row. Figure 2.8 shows a truth table of two
inputs, A and B. Each row shows its corresponding minterm. For example, the minterm for the first row is because is TRUE when
A  0, B  0.
We can write a Boolean equation for any truth table by summing
each of the minterms for which the output, Y, is TRUE. For example,
in Figure 2.8, there is only one row (or minterm) for which the
output Y is TRUE, shown circled in blue. Thus, . Figure 2.9
shows a truth table with more than one row in which the output is
TRUE. Taking the sum of each of the circled minterms gives
.
This is called the sum-of-products canonical form of a function
because it is the sum (OR) of products (ANDs forming minterms).
Although there are many ways to write the same function, such as
, we will sort the minterms in the same order that they
appear in the truth table, so that we always write the same Boolean
expression for the same truth table.
Example 2.2 SUM-OF-PRODUCTS FORM
Ben Bitdiddle is having a picnic. He won’t enjoy it if it rains or if there are ants.
Design a circuit that will output TRUE only if Ben enjoys the picnic.
Solution: First define the inputs and outputs. The inputs are A and R, which
indicate if there are ants and if it rains. A is TRUE when there are ants and
FALSE when there are no ants. Likewise, R is TRUE when it rains and FALSE
when the sun smiles on Ben. The output is E, Ben’s enjoyment of the picnic. E is
TRUE if Ben enjoys the picnic and FALSE if he suffers. Figure 2.10 shows the
truth table for Ben’s picnic experience.
Using sum-of-products form, we write the equation as: . We can build the
equation using two inverters and a two-input AND gate, shown in Figure 2.11(a).
You may recognize this truth table as the NOR function from Section 1.5.5: E  A
NOR . Figure 2.11(b) shows the NOR implementation. In
Section 2.3, we show that the two equations, and , are equivalent.
The sum-of-products form provides a Boolean equation for any truth
table with any number of variables. Figure 2.12 shows a random threeinput truth table. The sum-of-products form of the logic function is
(2.2)
Unfortunately, sum-of-products form does not necessarily generate
the simplest equation. In Section 2.3 we show how to write the same
function using fewer terms.
Y  ABC  ABC  ABC
AR A  R
R  A  R
E AR
Y  BA  BA
Y  AB  AB
Y  AB
AB AB
2.2 Boolean Equations 55
Figure 2.8 Truth table and
minterms
Figure 2.9 Truth table with
multiple TRUE minterms
0
ABY
0 0
0 1
1 0
1 1
0
1
0
minterm
A B
A B
A B
A B
ABY
0 0
0 1
1 0
1 1
0
1
0
1
minterm
A B
A B
A B
A B
Canonical form is just a fancy
word for standard form. You
can use the term to impress
your friends and scare your
enemies.
Chapter 02.qxd 
2.2.3 Product-of-Sums Form
An alternative way of expressing Boolean functions is the product-ofsums canonical form. Each row of a truth table corresponds to a maxterm that is FALSE for that row. For example, the maxterm for the
first row of a two-input truth table is (A  B) because (A  B) is
FALSE when A  0, B  0. We can write a Boolean equation for any
circuit directly from the truth table as the AND of each of the maxterms for which the output is FALSE.
Example 2.3 PRODUCT-OF-SUMS FORM
Write an equation in product-of-sums form for the truth table in Figure 2.13.
Solution: The truth table has two rows in which the output is FALSE. Hence, the
function can be written in product-of-sums form as . The
first maxterm, (A  B), guarantees that Y  0 for A  0, B  0, because any
value AND 0 is 0. Likewise, the second maxterm, , guarantees that Y  0
for A  1, B  0. Figure 2.13 is the same truth table as Figure 2.9, showing that
the same function can be written in more than one way.
Similarly, a Boolean equation for Ben’s picnic from Figure 2.10
can be written in product-of-sums form by circling the three rows of
0’s to obtain . This is uglier than the sumof-products equation, , but the two equations are logically
equivalent.
Sum-of-products produces the shortest equations when the output is
TRUE on only a few rows of a truth table; product-of-sums is simpler
when the output is FALSE on only a few rows of a truth table.
2.3 BOOLEAN ALGEBRA
In the previous section, we learned how to write a Boolean expression
given a truth table. However, that expression does not necessarily lead to
the simplest set of logic gates. Just as you use algebra to simplify mathematical equations, you can use Boolean algebra to simplify Boolean
equations. The rules of Boolean algebra are much like those of ordinary
algebra but are in some cases simpler, because variables have only two
possible values: 0 or 1.
Boolean algebra is based on a set of axioms that we assume are correct. Axioms are unprovable in the sense that a definition cannot be
proved. From these axioms, we prove all the theorems of Boolean algebra. These theorems have great practical significance, because they teach
us how to simplify logic to produce smaller and less costly circuits.
E  AR
E  (A  R)(A  R)(A  R)
(A  B)
Y  (A  B)(A  B)
56 CHAPTER TWO Combinational Logic Design
Figure 2.13 Truth table with
multiple FALSE maxterms
A + B
ABY
0 0
0 1
1 0
1 1
0
1
0
1
maxterm
A + B
A + B
A + B
Figure 2.12 Random three-input
truth table
BCY
0 0
0 1
1 0
1 1
1
0
0
0
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
Figure 2.11 Ben’s circuit
A
R
E
(a)
A
R E
(b)
Figure 2.10 Ben’s truth table
ARE
0 0
0 1
1 0
1 1
1
0
0
0
Chapter 02.qxd 1/31/
Axioms and theorems of Boolean algebra obey the principle of duality. If the symbols 0 and 1 and the operators • (AND) and  (OR) are
interchanged, the statement will still be correct. We use the prime ()
symbol to denote the dual of a statement.
2.3.1 Axioms
Table 2.1 states the axioms of Boolean algebra. These five axioms and
their duals define Boolean variables and the meanings of NOT, AND,
and OR. Axiom A1 states that a Boolean variable B is 0 if it is not 1.
The axiom’s dual, A1, states that the variable is 1 if it is not 0. Together,
A1 and A1 tell us that we are working in a Boolean or binary field of
0’s and 1’s. Axioms A2 and A2 define the NOT operation. Axioms A3
to A5 define AND; their duals, A3 to A5 define OR.
2.3.2 Theorems of One Variable
Theorems T1 to T5 in Table 2.2 describe how to simplify equations
involving one variable.
The identity theorem, T1, states that for any Boolean variable B, B
AND 1  B. Its dual states that B OR 0  B. In hardware, as shown in
Figure 2.14, T1 means that if one input of a two-input AND gate is
always 1, we can remove the AND gate and replace it with a wire connected to the variable input (B). Likewise, T1 means that if one input of
a two-input OR gate is always 0, we can replace the OR gate with a wire
connected to B. In general, gates cost money, power, and delay, so
replacing a gate with a wire is beneficial.
The null element theorem, T2, says that B AND 0 is always equal
to 0. Therefore, 0 is called the null element for the AND operation,
because it nullifies the effect of any other input. The dual states that B
OR 1 is always equal to 1. Hence, 1 is the null element for the OR
operation. In hardware, as shown in Figure 2.15, if one input of an
AND gate is 0, we can replace the AND gate with a wire that is tied
2.3 Boolean Algebra 57
Axiom Dual Name
A1 B  0 if B  1 A1 B  1 if B  0 Binary field
A2 A2 NOT
A3 0 • 0  0 A3 1  1  1 AND/OR
A4 1 • 1  1 A4 0  0  0 AND/OR
A5 0 • 1  1 • 0  0 A5 1  0  0  1  1 AND/OR
0  1 1  0
Table 2.1 Axioms of Boolean algebra
The null element theorem
leads to some outlandish statements that are actually true! It
is particularly dangerous when
left in the hands of advertisers:
YOU WILL GET A MILLION
DOLLARS or we’ll send you a
toothbrush in the mail. (You’ll
most likely be receiving a
toothbrush in the mail.)
Figure 2.14 Identity theorem in
hardware: (a) T1, (b) T1
1 =
(a)
B B
= 0
B B
(b)
Chapter 02.qxd 1/31
LOW (to 0). Likewise, if one input of an OR gate is 1, we can replace
the OR gate with a wire that is tied HIGH (to 1).
Idempotency, T3, says that a variable AND itself is equal to just
itself. Likewise, a variable OR itself is equal to itself. The theorem gets
its name from the Latin roots: idem (same) and potent (power). The
operations return the same thing you put into them. Figure 2.16 shows
that idempotency again permits replacing a gate with a wire.
Involution, T4, is a fancy way of saying that complementing a variable twice results in the original variable. In digital electronics, two
wrongs make a right. Two inverters in series logically cancel each other
out and are logically equivalent to a wire, as shown in Figure 2.17. The
dual of T4 is itself.
The complement theorem, T5 (Figure 2.18), states that a variable
AND its complement is 0 (because one of them has to be 0). And, by duality, a variable OR its complement is 1 (because one of them has to be 1).
2.3.3 Theorems of Several Variables
Theorems T6 to T12 in Table 2.3 describe how to simplify equations
involving more than one Boolean variable.
Commutativity and associativity, T6 and T7, work the same as in
traditional algebra. By commutativity, the order of inputs for an AND or
OR function does not affect the value of the output. By associativity, the
specific groupings of inputs do not affect the value of the output.
The distributivity theorem, T8, is the same as in traditional algebra,
but its dual, T8, is not. By T8, AND distributes over OR, and by T8, OR
distributes over AND. In traditional algebra, multiplication distributes
over addition but addition does not distribute over multiplication, so that
(B  C)  (B  D)  B  (C  D).
The covering, combining, and consensus theorems, T9 to T11, permit us to eliminate redundant variables. With some thought, you should
be able to convince yourself that these theorems are correct.
58 CHAPTER TWO Combinational Logic Design
Theorem Dual Name
T1 B • 1  B T1 B  0  B Identity
T2 B • 0  0 T2 B  1  1 Null Element
T3 B • B  B T3 B  B  B Idempotency
T4 B
=  B Involution
T5 B • B  0 T5 B  B  1 Complements
Table 2.2 Boolean theorems of one variable
Figure 2.16 Idempotency
theorem in hardware: (a) T3,
(b) T3
B =
(a)
B B
= B
B B
(b)
Figure 2.17 Involution theorem
in hardware: T4
Figure 2.18 Complement
theorem in hardware: (a) T5,
(b) T5
B = B
B =
(a)
B
0
= B
B
1
(b)
Figure 2.15 Null element
theorem in hardware: (a) T2,
(b) T2
0 =
(a)
B 0
= 1
B 1
(b)
Chapter 02.qxd 1
De Morgan’s Theorem, T12, is a particularly powerful tool in digital
design. The theorem explains that the complement of the product of all
the terms is equal to the sum of the complement of each term. Likewise,
the complement of the sum of all the terms is equal to the product of the
complement of each term.
According to De Morgan’s theorem, a NAND gate is equivalent to
an OR gate with inverted inputs. Similarly, a NOR gate is equivalent to
an AND gate with inverted inputs. Figure 2.19 shows these De Morgan
equivalent gates for NAND and NOR gates. The two symbols shown for
each function are called duals. They are logically equivalent and can be
used interchangeably.
2.3 Boolean Algebra 59
Theorem Dual Name
T6 B • C  C • B T6 B  C  C  B Commutativity
T7 (B • C) • D  B • (C • D) T7 (B  C)  D  B  (C  D) Associativity
T8 (B • C)  (B • D)  B • (C  D) T8 (B  C) • (B  D)  B  (C • D) Distributivity
T9 B • (B  C)  B T9 B  (B • C)  B Covering
T10 T10 Combining
T11 T11 Consensus
T12 T12 De Morgan’s
 (B0  B1  B2 ...)  (B0 • B1 • B2) Theorem
B0 • B1 • B2... B0  B1  B2...
 B • C  B • D  (B  C) • (B  D)
(B • C)  (B • D)  (C • D) (B  C) • (B  D) • (C  D)
(B • C)  (B • C)  B (B  C) • (B  C)  B
Table 2.3 Boolean theorems of several variables
Augustus De Morgan, died 1871.
A British mathematician, born
in India. Blind in one eye. His
father died when he was 10.
Attended Trinity College,
Cambridge, at age 16, and
was appointed Professor of
Mathematics at the newly
founded London University at
age 22. Wrote widely on many
mathematical subjects, including logic, algebra, and paradoxes. De Morgan’s crater on
the moon is named for him.
He proposed a riddle for the
year of his birth: “I was x
years of age in the year x2.”
Figure 2.19 De Morgan equivalent gates
ABY
001
011
101
110
NAND
A
B Y
A
B Y
NOR
A
B Y
A
B Y
ABY
001
010
100
110
Y = AB = A + B Y = A + B = A B
Chapter 02.qxd 1/31/07 9:55 PM Page 59
The inversion circle is called a bubble. Intuitively, you can imagine that
“pushing” a bubble through the gate causes it to come out at the other side
and flips the body of the gate from AND to OR or vice versa. For example,
the NAND gate in Figure 2.19 consists of an AND body with a bubble on
the output. Pushing the bubble to the left results in an OR body with
bubbles on the inputs. The underlying rules for bubble pushing are
 Pushing bubbles backward (from the output) or forward (from the
inputs) changes the body of the gate from AND to OR or vice versa.
 Pushing a bubble from the output back to the inputs puts bubbles
on all gate inputs.
 Pushing bubbles on all gate inputs forward toward the output puts a
bubble on the output.
Section 2.5.2 uses bubble pushing to help analyze circuits.
Example 2.4 DERIVE THE PRODUCT-OF-SUMS FORM
Figure 2.20 shows the truth table for a Boolean function, Y, and its complement,
. Using De Morgan’s Theorem, derive the product-of-sums canonical form of Y
from the sum-of-products form of .
Solution: Figure 2.21 shows the minterms (circled) contained in . The sum-ofproducts canonical form of is
(2.3)
Taking the complement of both sides and applying De Morgan’s Theorem twice,
we get:
(2.4)
2.3.4 The Truth Behind It All
The curious reader might wonder how to prove that a theorem is true. In
Boolean algebra, proofs of theorems with a finite number of variables
are easy: just show that the theorem holds for all possible values of these
variables. This method is called perfect induction and can be done with a
truth table.
Example 2.5 PROVING THE CONSENSUS THEOREM
Prove the consensus theorem, T11, from Table 2.3.
Solution: Check both sides of the equation for all eight combinations of B, C,
and D. The truth table in Figure 2.22 illustrates these combinations. Because
BC  BD  CD  BC  BD for all cases, the theorem is proved.
Y  Y  AB  AB  (AB)(AB)  (A  B)(A  B)
Y  AB  AB
Y
Y
Y
Y
60 CHAPTER TWO Combinational Logic Design
FIGURE 2.20 Truth table
showing Y and Y
–
Figure 2.21 Truth table showing
minterms for Y
–
ABY
0 0
0 1
1 0
1 1
0
0
1
1
Y
1
1
0
0
ABY
0 0
0 1
1 0
1 1
0
0
1
1
Y
1
1
0
0
minterm
A B
A B
A B
A B
Chapter 02.qxd 1
2.3.5 Simplifying Equations
The theorems of Boolean algebra help us simplify Boolean equations.
For example, consider the sum-of-products expression from the truth
table of Figure 2.9: . By Theorem T10, the equation simplifies to . This may have been obvious looking at the truth table.
In general, multiple steps may be necessary to simplify more complex
equations.
The basic principle of simplifying sum-of-products equations is to
combine terms using the relationship , where P may be
any implicant. How far can an equation be simplified? We define an
equation in sum-of-products form to be minimized if it uses the fewest
possible implicants. If there are several equations with the same number
of implicants, the minimal one is the one with the fewest literals.
An implicant is called a prime implicant if it cannot be combined
with any other implicants to form a new implicant with fewer literals.
The implicants in a minimal equation must all be prime implicants.
Otherwise, they could be combined to reduce the number of literals.
Example 2.6 EQUATION MINIMIZATION
Minimize Equation 2.2: .
Solution: We start with the original equation and apply Boolean theorems step
by step, as shown in Table 2.4.
Have we simplified the equation completely at this point? Let’s take a closer
look. From the original equation, the minterms and differ only in the
variable A. So we combined the minterms to form . However, if we look at
the original equation, we note that the last two minterms and also
differ by a single literal (C and ). Thus, using the same method, we could have
combined these two minterms to form the minterm . We say that implicants
and share the minterm .
So, are we stuck with simplifying only one of the minterm pairs, or can we simplify
both? Using the idempotency theorem, we can duplicate terms as many times as we
want: B  B  B  B  B … . Using this principle, we simplify the equation completely to its two prime implicants, , as shown in Table 2.5. BC  AB
BC AB ABC
AB
C
ABC ABC
BC
ABC ABC
ABC  ABC  AB C
PA  PA  P
Y  B
Y  AB  AB
2.3 Boolean Algebra 61
Figure 2.22 Truth table proving
T11
0 0
0 1
1 0
1 1
BCD
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
BC + BD + CD BC + BD
0
1
0
1
0
0
1
1
0
1
0
1
0
0
1
1
Chapter 02.q
Although it is a bit counterintuitive, expanding an implicant (for
example, turning AB into ) is sometimes useful in minimizing equations. By doing this, you can repeat one of the expanded
minterms to be combined (shared) with another minterm.
You may have noticed that completely simplifying a Boolean equation with the theorems of Boolean algebra can take some trial and error.
Section 2.7 describes a methodical technique called Karnaugh maps that
makes the process easier.
Why bother simplifying a Boolean equation if it remains logically
equivalent? Simplifying reduces the number of gates used to physically
implement the function, thus making it smaller, cheaper, and possibly
faster. The next section describes how to implement Boolean equations
with logic gates.
2.4 FROM LOGIC TO GATES
A schematic is a diagram of a digital circuit showing the elements and
the wires that connect them together. For example, the schematic in
Figure 2.23 shows a possible hardware implementation of our favorite
logic function, Equation 2.2:
Y  ABC  ABC  ABC.
ABC  ABC
62 CHAPTER TWO Combinational Logic Design
Step Equation Justification
1 T3: Idempotency
2 T8: Distributivity
3 T5: Complements
4 BC  AB T1: Identity
BC(1)  AB(1)
BC(A  A)  AB(C  C)
ABC  ABC  ABC  ABC
ABC  ABC  ABC
Table 2.5 Improved equation minimization
Step Equation Justification
1 T8: Distributivity
2 T5: Complements
3 BC  ABC T1: Identity
BC (1)  ABC
BC (A  A)  ABC
ABC  ABC  ABC
Table 2.4 Equation minimization
Chapter 02.qxd 1/31/
By drawing schematics in a consistent fashion, we make them easier
to read and debug. We will generally obey the following guidelines:
 Inputs are on the left (or top) side of a schematic.
 Outputs are on the right (or bottom) side of a schematic.
 Whenever possible, gates should flow from left to right.
 Straight wires are better to use than wires with multiple corners
(jagged wires waste mental effort following the wire rather than
thinking of what the circuit does).
 Wires always connect at a T junction.
 A dot where wires cross indicates a connection between the wires.
 Wires crossing without a dot make no connection.
The last three guidelines are illustrated in Figure 2.24.
Any Boolean equation in sum-of-products form can be drawn as a
schematic in a systematic way similar to Figure 2.23. First, draw
columns for the inputs. Place inverters in adjacent columns to provide
the complementary inputs if necessary. Draw rows of AND gates for
each of the minterms. Then, for each output, draw an OR gate
connected to the minterms related to that output. This style is called
a programmable logic array (PLA) because the inverters, AND gates,
and OR gates are arrayed in a systematic fashion. PLAs will be
discussed further in Section 5.6.
Figure 2.25 shows an implementation of the simplified equation we
found using Boolean algebra in Example 2.6. Notice that the simplified
circuit has significantly less hardware than that of Figure 2.23. It may
also be faster, because it uses gates with fewer inputs.
We can reduce the number of gates even further (albeit by a single
inverter) by taking advantage of inverting gates. Observe that is an BC
2.4 From Logic to Gates 63
Figure 2.23 Schematic of
Y A
–
B
–
C
–  AB
–
C
– AB–
C
A C B
Y
minterm: ABC
minterm: ABC
minterm: ABC
ABC
Figure 2.24 Wire connections
wires connect
at a T junction
wires connect
at a dot
wires crossing
without a dot do
not connect
A B C
Y
Figure 2.25 Schematic of
Y B
–
C
– AB
–
Chapter 02.q
1 Black light, twinkies, and beer.
AND with inverted inputs. Figure 2.26 shows a schematic using this
optimization to eliminate the inverter on C. Recall that by De Morgan’s
theorem the AND with inverted inputs is equivalent to a NOR gate.
Depending on the implementation technology, it may be cheaper to use
the fewest gates or to use certain types of gates in preference to others.
For example, NANDs and NORs are preferred over ANDs and ORs in
CMOS implementations.
Many circuits have multiple outputs, each of which computes a separate Boolean function of the inputs. We can write a separate truth table
for each output, but it is often convenient to write all of the outputs on a
single truth table and sketch one schematic with all of the outputs.
Example 2.7 MULTIPLE-OUTPUT CIRCUITS
The dean, the department chair, the teaching assistant, and the dorm social
chair each use the auditorium from time to time. Unfortunately, they occasionally conflict, leading to disasters such as the one that occurred when the dean’s
fundraising meeting with crusty trustees happened at the same time as the
dorm’s BTB1 party. Alyssa P. Hacker has been called in to design a room reservation system.
The system has four inputs, A3, . . . , A0, and four outputs, Y3, . . . , Y0. These
signals can also be written as A3:0 and Y3:0. Each user asserts her input when she
requests the auditorium for the next day. The system asserts at most one output,
granting the auditorium to the highest priority user. The dean, who is paying for
the system, demands highest priority (3). The department chair, teaching assistant, and dorm social chair have decreasing priority.
Write a truth table and Boolean equations for the system. Sketch a circuit that
performs this function.
Solution: This function is called a four-input priority circuit. Its symbol and truth
table are shown in Figure 2.27.
We could write each output in sum-of-products form and reduce the equations
using Boolean algebra. However, the simplified equations are clear by inspection from the functional description (and the truth table): Y3 is TRUE whenever A3 is asserted, so Y3  A3. Y2 is TRUE if A2 is asserted and A3 is not
asserted, so . Y1 is TRUE if A1 is asserted and neither of the higher
priority inputs is asserted: . And Y0 is TRUE whenever A0 and
no other input is asserted: . The schematic is shown in Figure
2.28. An experienced designer can often implement a logic circuit by inspection. Given a clear specification, simply turn the words into equations and the
equations into gates.
Y0  A3A2A1 A0
Y1  A3A2 A1
Y2  A3A2
64 CHAPTER TWO Combinational Logic Design
Y
A B C
Figure 2.26 Schematic using
fewer gates
Chap
Notice that if A3 is asserted in the priority circuit, the outputs don’t
care what the other inputs are. We use the symbol X to describe inputs
that the output doesn’t care about. Figure 2.29 shows that the four-input
priority circuit truth table becomes much smaller with don’t cares. From
this truth table, we can easily read the Boolean equations in sum-ofproducts form by ignoring inputs with X’s. Don’t cares can also appear
in truth table outputs, as we will see in Section 2.7.3.
2.5 MULTILEVEL COMBINATIONAL LOGIC
Logic in sum-of-products form is called two-level logic because it consists of literals connected to a level of AND gates connected to a level of
2.5 Multilevel Combinational Logic 65
A0
A1
Priority
Circuit
A2
A3
0 0
0 1
1 0
1 1
0
0
0
0
0
0
0
0
0
0
1
1
0
1
0
0
Y0
Y1
Y2
Y3
0 0
0 0
0 0
0 0
0 1 000 1 0 0
0 1
1 0
1 1
0 0
0 1
0 1
0 1
1 0
1 0 0 1
1 0
1 1
0 0
0 1
1 0
1 0
1 1
1 1
1 1 1 0
1 1 1 1
0
0
0
1
1
1
1
0
0
0
0
0
0
0
0
0
1 00 0
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
1 00 0
1 00 0
A3 A2 A1 A0 Y3 Y2 Y1 Y0
Figure 2.27 Priority circuit
Figure 2.28 Priority circuit
schematic
A3A2A1A0
Y3
Y2
Y1
Y0
Figure 2.29 Priority circuit truth table
with don’t cares (X’s)
A1 A0
0 0
0 1
1 X
X X
0
0
0
0
0
0
0
1
0
0
1
0
0
1
0
0
A3 A2
0 0
0 0
0 0
0 1
1 X X X 1 000
Y3 Y2 Y1 Y0
X is an overloaded symbol
that means “don’t care” in
truth tables and “contention”
in logic simulation (see
Section 2.6.1). Think about
the context so you don’t mix
up the meanings. Some
authors use D or ? instead
for “don’t care” to avoid
this ambiguity.

OR gates. Designers often build circuits with more than two levels of
logic gates. These multilevel combinational circuits may use less hardware than their two-level counterparts. Bubble pushing is especially
helpful in analyzing and designing multilevel circuits.
2.5.1 Hardware Reduction
Some logic functions require an enormous amount of hardware when
built using two-level logic. A notable example is the XOR function of
multiple variables. For example consider building a three-input XOR
using the two-level techniques we have studied so far.
Recall that an N-input XOR produces a TRUE output when an odd
number of inputs are TRUE. Figure 2.30 shows the truth table for a
three-input XOR with the rows circled that produce TRUE outputs.
From the truth table, we read off a Boolean equation in sum-of-products
form in Equation 2.5. Unfortunately, there is no way to simplify this
equation into fewer implicants.
(2.5)
On the other hand, A  B  C  (A  B)  C (prove this to yourself by perfect induction if you are in doubt). Therefore, the three-input
XOR can be built out of a cascade of two-input XORs, as shown in
Figure 2.31.
Similarly, an eight-input XOR would require 128 eight-input AND
gates and one 128-input OR gate for a two-level sum-of-products implementation. A much better option is to use a tree of two-input XOR
gates, as shown in Figure 2.32.
Y  ABC  ABC  ABC  ABC
66 CHAPTER TWO Combinational Logic Design
Figure 2.30 Three-input XOR:
(a) functional specification
and (b) two-level logic
implementation
B C
0 0
0 1
1 0
1 1
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
1
1
0
1
0
0
1
Y
XOR3
Y = A ⊕ B ⊕ C
A
B Y
C
A B C
Y
(a) (b)
Chapter 0
Selecting the best multilevel implementation of a specific logic function is not a simple process. Moreover, “best” has many meanings: fewest
gates, fastest, shortest design time, least cost, least power consumption. In
Chapter 5, you will see that the “best” circuit in one technology is not
necessarily the best in another. For example, we have been using ANDs
and ORs, but in CMOS, NANDs and NORs are more efficient. With
some experience, you will find that you can create a good multilevel
design by inspection for most circuits. You will develop some of this
experience as you study circuit examples through the rest of this book. As
you are learning, explore various design options and think about the
trade-offs. Computer-aided design (CAD) tools are also available to
search a vast space of possible multilevel designs and seek the one that
best fits your constraints given the available building blocks.
2.5.2 Bubble Pushing
You may recall from Section 1.7.6 that CMOS circuits prefer NANDs
and NORs over ANDs and ORs. But reading the equation by inspection
from a multilevel circuit with NANDs and NORs can get pretty hairy.
Figure 2.33 shows a multilevel circuit whose function is not immediately clear by inspection. Bubble pushing is a helpful way to redraw
these circuits so that the bubbles cancel out and the function can be
more easily determined. Building on the principles from Section 2.3.3,
the guidelines for bubble pushing are as follows:
 Begin at the output of the circuit and work toward the inputs.
 Push any bubbles on the final output back toward the inputs so that
you can read an equation in terms of the output (for example, Y)
instead of the complement of the output .
 Working backward, draw each gate in a form so that bubbles cancel.
If the current gate has an input bubble, draw the preceding gate with
an output bubble. If the current gate does not have an input bubble,
draw the preceding gate without an output bubble.
Figure 2.34 shows how to redraw Figure 2.33 according to the
bubble pushing guidelines. Starting at the output, Y, the NAND gate
has a bubble on the output that we wish to eliminate. We push the
output bubble back to form an OR with inverted inputs, shown in
(Y)
2.5 Multilevel Combinational Logic 67
Figure 2.31 Three-input XOR
using two two-input XORs
A
B
Y C
Figure 2.32 Eight-input XOR
using seven two-input XORs
Figure 2.33 Multilevel circuit
using NANDs and NORs
A
B
C
D
Y
Cha
Figure 2.34(a). Working to the left, the rightmost gate has an input
bubble that cancels with the output bubble of the middle NAND gate,
so no change is necessary, as shown in Figure 2.34(b). The middle gate
has no input bubble, so we transform the leftmost gate to have no
output bubble, as shown in Figure 2.34(c). Now all of the bubbles in
the circuit cancel except at the inputs, so the function can be read by
inspection in terms of ANDs and ORs of true or complementary
inputs: .
For emphasis of this last point, Figure 2.35 shows a circuit logically equivalent to the one in Figure 2.34. The functions of internal
nodes are labeled in blue. Because bubbles in series cancel, we
can ignore the bubble on the output of the middle gate and the input
of the rightmost gate to produce the logically equivalent circuit of
Figure 2.35.
Y  ABC  D
68 CHAPTER TWO Combinational Logic Design
A
B
C Y
D
(a)
no output
bubble
bubble on
A input and output
B
C
D
Y
(b)
A
B
C
D
Y
(c)
Y = ABC + D
no bubble on
input and output
Figure 2.34 Bubble-pushed
circuit
Figure 2.35 Logically equivalent
bubble-pushed circuit
A
B
C
D
Y
AB
ABC
Y = ABC + D
Ch
Example 2.8 BUBBLE PUSHING FOR CMOS LOGIC
Most designers think in terms of AND and OR gates, but suppose you would
like to implement the circuit in Figure 2.36 in CMOS logic, which favors
NAND and NOR gates. Use bubble pushing to convert the circuit to NANDs,
NORs, and inverters.
Solution: A brute force solution is to just replace each AND gate with a NAND
and an inverter, and each OR gate with a NOR and an inverter, as shown in
Figure 2.37. This requires eight gates. Notice that the inverter is drawn with the
bubble on the front rather than back, to emphasize how the bubble can cancel
with the preceding inverting gate.
For a better solution, observe that bubbles can be added to the output of a gate
and the input of the next gate without changing the function, as shown in Figure
2.38(a). The final AND is converted to a NAND and an inverter, as shown in
Figure 2.38(b). This solution requires only five gates.
2.6 X’S AND Z’S, OH MY
Boolean algebra is limited to 0’s and 1’s. However, real circuits can also
have illegal and floating values, represented symbolically by X and Z.
2.6.1 Illegal Value: X
The symbol X indicates that the circuit node has an unknown or illegal
value. This commonly happens if it is being driven to both 0 and 1 at the
same time. Figure 2.39 shows a case where node Y is driven both HIGH
and LOW. This situation, called contention, is considered to be an error
2.6 X’s and Z’s, Oh My 69
Figure 2.36 Circuit using ANDs
and ORs
Figure 2.37 Poor circuit using
NANDs and NORs
(a) (b)
Figure 2.38 Better circuit using
NANDs and NORs
Figure 2.39 Circuit with
contention
A = 1
Y = X
B = 0

and must be avoided. The actual voltage on a node with contention may
be somewhere between 0 and VDD, depending on the relative strengths
of the gates driving HIGH and LOW. It is often, but not always, in the
forbidden zone. Contention also can cause large amounts of power to
flow between the fighting gates, resulting in the circuit getting hot and
possibly damaged.
X values are also sometimes used by circuit simulators to indicate
an uninitialized value. For example, if you forget to specify the
value of an input, the simulator may assume it is an X to warn you of
the problem.
As mentioned in Section 2.4, digital designers also use the symbol
X to indicate “don’t care” values in truth tables. Be sure not to mix up
the two meanings. When X appears in a truth table, it indicates that
the value of the variable in the truth table is unimportant. When
X appears in a circuit, it means that the circuit node has an unknown
or illegal value.
2.6.2 Floating Value: Z
The symbol Z indicates that a node is being driven neither HIGH nor
LOW. The node is said to be floating, high impedance, or high Z. A typical
misconception is that a floating or undriven node is the same as a logic 0.
In reality, a floating node might be 0, might be 1, or might be at some voltage in between, depending on the history of the system. A floating node
does not always mean there is an error in the circuit, so long as some other
circuit element does drive the node to a valid logic level when the value of
the node is relevant to circuit operation.
One common way to produce a floating node is to forget to
connect a voltage to a circuit input, or to assume that an unconnected
input is the same as an input with the value of 0. This mistake may
cause the circuit to behave erratically as the floating input randomly
changes from 0 to 1. Indeed, touching the circuit may be enough to
trigger the change by means of static electricity from the body. We have
seen circuits that operate correctly only as long as the student keeps a
finger pressed on a chip.
The tristate buffer, shown in Figure 2.40, has three possible output states: HIGH (1), LOW (0), and floating (Z). The tristate buffer
has an input, A, an output, Y, and an enable, E. When the enable is
TRUE, the tristate buffer acts as a simple buffer, transferring the input
value to the output. When the enable is FALSE, the output is allowed
to float (Z).
The tristate buffer in Figure 2.40 has an active high enable. That is,
when the enable is HIGH (1), the buffer is enabled. Figure 2.41 shows a
tristate buffer with an active low enable. When the enable is LOW (0),
70 CHAPTER TWO Combinational Logic Design
Figure 2.40 Tristate buffer
Figure 2.41 Tristate buffer with
active low enable
EAY
00Z
01Z
100
111
A
E
Y
EAY
000
011
10Z
11Z
A
E
Y

the buffer is enabled. We show that the signal is active low by putting a
bubble on its input wire. We often indicate an active low input by drawing a bar over its name, , or appending the word “bar” after its name,
Ebar.
Tristate buffers are commonly used on busses that connect multiple chips. For example, a microprocessor, a video controller, and an
Ethernet controller might all need to communicate with the memory
system in a personal computer. Each chip can connect to a shared
memory bus using tristate buffers, as shown in Figure 2.42. Only one
chip at a time is allowed to assert its enable signal to drive a
value onto the bus. The other chips must produce floating outputs
so that they do not cause contention with the chip talking to the
memory. Any chip can read the information from the shared bus at
any time. Such tristate busses were once common. However, in modern computers, higher speeds are possible with point-to-point links, in
which chips are connected to each other directly rather than over
a shared bus.
2.7 KARNAUGH MAPS
After working through several minimizations of Boolean equations using
Boolean algebra, you will realize that, if you’re not careful, you sometimes end up with a completely different equation instead of a simplified
equation. Karnaugh maps (K-maps) are a graphical method for simplifying Boolean equations. They were invented in 1953 by Maurice
Karnaugh, a telecommunications engineer at Bell Labs. K-maps work
well for problems with up to four variables. More important, they give
insight into manipulating Boolean equations.
E
2.7 Karnaugh Maps 71
Figure 2.42 Tristate bus
connecting multiple chips
en1
to bus
from bus
en2
to bus
from bus
en3
to bus
from bus
en4
to bus
from bus
Processor
Video
Ethernet
shared bus
Memory
Maurice Karnaugh, 1924–.
Graduated with a bachelor’s
degree in physics from the
City College of New York in
1948 and earned a Ph.D. in
physics from Yale in 1952.
Worked at Bell Labs and IBM
from 1952 to 1993 and as a
computer science professor at
the Polytechnic University of
New York from 1980 to 1999.

Recall that logic minimization involves combining terms. Two terms
containing an implicant, P, and the true and complementary forms of
some variable, A, are combined to eliminate A: .
Karnaugh maps make these combinable terms easy to see by putting
them next to each other in a grid.
Figure 2.43 shows the truth table and K-map for a three-input
function. The top row of the K-map gives the four possible values
for the A and B inputs. The left column gives the two possible
values for the C input. Each square in the K-map corresponds to a
row in the truth table and contains the value of the output, Y, for that
row. For example, the top left square corresponds to the first row in
the truth table and indicates that the output value Y  1 when ABC
000. Just like each row in a truth table, each square in a K-map
represents a single minterm. For the purpose of explanation,
Figure 2.43(c) shows the minterm corresponding to each square in
the K-map.
Each square, or minterm, differs from an adjacent square by a
change in a single variable. This means that adjacent squares share
all the same literals except one, which appears in true form in one
square and in complementary form in the other. For example,
the squares representing the minterms and are adjacent
and differ only in the variable C. You may have noticed that the
A and B combinations in the top row are in a peculiar order: 00, 01,
11, 10. This order is called a Gray code. It differs from ordinary
binary order (00, 01, 10, 11) in that adjacent entries differ only in
a single variable. For example, 01 : 11 only changes A from 0 to 1,
while 01 : 10 would change A from 1 to 0 and B from 0 to 1.
Hence, writing the combinations in binary order would not have
produced our desired property of adjacent squares differing only in
one variable.
The K-map also “wraps around.” The squares on the far right are
effectively adjacent to the squares on the far left, in that they differ only
in one variable, A. In other words, you could take the map and roll it
into a cylinder, then join the ends of the cylinder to form a torus (i.e., a
donut), and still guarantee that adjacent squares would differ only in one
variable.
2.7.1 Circular Thinking
In the K-map in Figure 2.43, only two minterms are present in the
equation, and , as indicated by the 1’s in the left column.
Reading the minterms from the K-map is exactly equivalent to reading
equations in sum-of-products form directly from the truth table.
ABC ABC
ABC ABC
PA  PA  P
72 CHAPTER TWO Combinational Logic Design
Gray codes were patented
(U.S. Patent 2,632,058) by
Frank Gray, a Bell Labs
researcher, in 1953. They are
especially useful in mechanical encoders because a slight
misalignment causes an error
in only one bit.
Gray codes generalize to
any number of bits. For
example, a 3-bit Gray code
sequence is:
000, 001, 011, 010,
110, 111, 101, 100
Lewis Carroll posed a related
puzzle in Vanity Fair in 1879.
“The rules of the Puzzle are
simple enough. Two words
are proposed, of the same
length; and the puzzle
consists of linking these
together by interposing
other words, each of which
shall differ from the next
word in one letter only. That
is to say, one letter may be
changed in one of the given
words, then one letter in the
word so obtained, and so
on, till we arrive at the
other given word.”
For example, SHIP to DOCK:
SHIP, SLIP, SLOP,
SLOT, SOOT, LOOT,
LOOK, LOCK, DOCK.
Can you find a shorter
sequence?
Chap
As before, we can use Boolean algebra to minimize equations in sum-ofproducts form.
(2.6)
K-maps help us do this simplification graphically by circling 1’s in
adjacent squares, as shown in Figure 2.44. For each circle, we write the
corresponding implicant. Remember from Section 2.2 that an implicant is
the product of one or more literals. Variables whose true and complementary forms are both in the circle are excluded from the implicant. In this
case, the variable C has both its true form (1) and its complementary form
(0) in the circle, so we do not include it in the implicant. In other words, Y
is TRUE when A  B  0, independent of C. So the implicant is . This
K-map gives the same answer we reached using Boolean algebra.
2.7.2 Logic Minimization with K-Maps
K-maps provide an easy visual way to minimize logic. Simply circle all
the rectangular blocks of 1’s in the map, using the fewest possible
number of circles. Each circle should be as large as possible. Then read
off the implicants that were circled.
More formally, recall that a Boolean equation is minimized when it
is written as a sum of the fewest number of prime implicants. Each circle
on the K-map represents an implicant. The largest possible circles are
prime implicants.
AB
Y  ABC  ABC  AB(C  C)  AB
2.7 Karnaugh Maps 73
Figure 2.43 Three-input function: (a) truth table, (b) K-map, (c) K-map showing minterms
B C
0 0
0 1
1 0
1 1
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
0
0
0
0
Y
(a)
C 00 01
0
1
Y
11 10
AB
1
1
0
0
0
0
0
0
(b)
C 00 01
0
1
Y
11 10
AB
ABC
ABC
ABC
ABC
ABC
ABC
ABC
ABC
(c)
C 00 01
0
1
Y
11 10
AB
1
0
0
0
0
0
0
1 Figure 2.44 K-map minimization
Chapter
For example, in the K-map of Figure 2.44, and are
implicants, but not prime implicants. Only is a prime implicant in
that K-map. Rules for finding a minimized equation from a K-map are
as follows:
 Use the fewest circles necessary to cover all the 1’s.
 All the squares in each circle must contain 1’s.
 Each circle must span a rectangular block that is a power of 2
(i.e., 1, 2, or 4) squares in each direction.
 Each circle should be as large as possible.
 A circle may wrap around the edges of the K-map.
 A 1 in a K-map may be circled multiple times if doing so allows
fewer circles to be used.
Example 2.9 MINIMIZATION OF A THREE-VARIABLE FUNCTION
USING A K-MAP
Suppose we have the function Y  F(A, B, C) with the K-map shown in
Figure 2.45. Minimize the equation using the K-map.
Solution: Circle the 1’s in the K-map using as few circles as possible, as shown in
Figure 2.46. Each circle in the K-map represents a prime implicant, and the dimension of each circle is a power of two (2  1 and 2  2). We form the prime implicant for each circle by writing those variables that appear in the circle only in true or
only in complementary form.
For example, in the 2  1 circle, the true and complementary forms of B are
included in the circle, so we do not include B in the prime implicant. However, only
the true form of A(A) and complementary form of C are in this circle, so we
include these variables in the prime implicant . Similarly, the 2  2 circle covers
all squares where B  0, so the prime implicant is .
Notice how the top-right square (minterm) is covered twice to make the prime
implicant circles as large as possible. As we saw with Boolean algebra
techniques, this is equivalent to sharing a minterm to reduce the size of the
B
AC
(C)
AB
ABC ABC
74 CHAPTER TWO Combinational Logic Design
Figure 2.45 K-map for
Example 2.9
00 01
Y
11 10
AB
1
1
0
0
1
0
1
1
0
1
C
Chapter 
implicant. Also notice how the circle covering four squares wraps around
the sides of the K-map.
Example 2.10 SEVEN-SEGMENT DISPLAY DECODER
A seven-segment display decoder takes a 4-bit data input, D3:0, and produces
seven outputs to control light-emitting diodes to display a digit from 0 to 9. The
seven outputs are often called segments a through g, or Sa–Sg, as defined in
Figure 2.47. The digits are shown in Figure 2.48. Write a truth table for the outputs, and use K-maps to find Boolean equations for outputs Sa and Sb. Assume
that illegal input values (10–15) produce a blank readout.
Solution: The truth table is given in Table 2.6. For example, an input of 0000
should turn on all segments except Sg.
Each of the seven outputs is an independent function of four variables. The
K-maps for outputs Sa and Sb are shown in Figure 2.49. Remember that adjacent
squares may differ in only a single variable, so we label the rows and columns in
Gray code order: 00, 01, 11, 10. Be careful to also remember this ordering when
entering the output values into the squares.
Next, circle the prime implicants. Use the fewest number of circles necessary to
cover all the 1’s. A circle can wrap around the edges (vertical and horizontal),
and a 1 may be circled more than once. Figure 2.50 shows the prime implicants
and the simplified Boolean equations.
Note that the minimal set of prime implicants is not unique. For example, the
0000 entry in the Sa K-map was circled along with the 1000 entry to produce
the minterm. The circle could have included the 0010 entry instead,
producing a minterm, as shown with dashed lines in Figure 2.51.
Figure 2.52 illustrates (see page 78) a common error in which a nonprime implicant was chosen to cover the 1 in the upper left corner. This minterm,
, gives a sum-of-products equation that is not minimal. The minterm
could have been combined with either of the adjacent ones to form a larger circle, as was done in the previous two figures.
D3D2D1D0
D3D2D0
D2D1D0
2.7 Karnaugh Maps 75
Figure 2.46 Solution for
Example 2.9
Figure 2.47 Seven-segment
display decoder icon
00 01
Y
11 10
AB
1
1
0
0
1
0
1
1
0
1
C
AC
Y = AC + B
B
4 7
7-segment
display
decoder
a
b
c
d
g
e
f
D S

76 CHAPTER TWO Combinational Logic Design
Figure 2.49 Karnaugh maps for
Sa and Sb
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
D3:2 00
00
10 01 11
1
1
1
0
0
0
1
01 1
1
1
1
0
0
0
0
0
11
10
00
00
D 10 1:0
D3:2
D1:0
Sa Sb
D3:0 Sa Sb Sc Sd Se Sf Sg
0000 1 1 1 1 1 1 0
0001 0 1 1 0 0 0 0
0010 1 1 0 1 1 0 1
0011 1 1 1 1 0 0 1
0100 0 1 1 0 0 1 1
0101 1 0 1 1 0 1 1
0110 1 0 1 1 1 1 1
0111 1 1 1 0 0 0 0
1000 1 1 1 1 1 1 1
1001 1 1 1 0 0 1 1
others 0 0 0 0 0 0 0
Table 2.6 Seven-segment display decoder truth table
Figure 2.48 Seven-segment
display digits
0123456789

2.7.3 Don’t Cares
Recall that “don’t care” entries for truth table inputs were introduced
in Section 2.4 to reduce the number of rows in the table when some
variables do not affect the output. They are indicated by the symbol X,
which means that the entry can be either 0 or 1.
Don’t cares also appear in truth table outputs where the output
value is unimportant or the corresponding input combination can never
happen. Such outputs can be treated as either 0’s or 1’s at the designer’s
discretion.
2.7 Karnaugh Maps 77
Figure 2.50 K-map solution for Example 2.10
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sa = D3D1 + D3D2D0 + D3D2D1 + D2D1D0
D3D1
D3D2D0
D2D1D0
Sa
D3D2D1
01 11
1
1
1
0
0
0
1
01 1
1
1
1
0
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sb = D3D2 + D2D1 + D3D1D0 + D3D1D0
D3D2
D2D1
Sb
D3D1D0
D3D1D0
Figure 2.51 Alternative K-map
for Sa showing different set of
prime implicants
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sa = D3D1 + D3D2D0 + D3D2D1 + D3D2D0
D3D1
D3D2D0 D3D2D1
D3D2D0
Sa

In a K-map, X’s allow for even more logic minimization. They can
be circled if they help cover the 1’s with fewer or larger circles, but they
do not have to be circled if they are not helpful.
Example 2.11 SEVEN-SEGMENT DISPLAY DECODER WITH DON’T CARES
Repeat Example 2.10 if we don’t care about the output values for illegal input
values of 10 to 15.
Solution: The K-map is shown in Figure 2.53 with X entries representing don’t
care. Because don’t cares can be 0 or 1, we circle a don’t care if it allows us to
cover the 1’s with fewer or bigger circles. Circled don’t cares are treated as 1’s,
whereas uncircled don’t cares are 0’s. Observe how a 2  2 square wrapping
around all four corners is circled for segment Sa. Use of don’t cares simplifies the
logic substantially.
2.7.4 The Big Picture
Boolean algebra and Karnaugh maps are two methods for logic simplification. Ultimately, the goal is to find a low-cost method of implementing
a particular logic function.
In modern engineering practice, computer programs called logic
synthesizers produce simplified circuits from a description of the logic
function, as we will see in Chapter 4. For large problems, logic synthesizers are much more efficient than humans. For small problems, a
human with a bit of experience can find a good solution by inspection.
Neither of the authors has ever used a Karnaugh map in real life to
78 CHAPTER TWO Combinational Logic Design
01 11
1
0
0
1
0
0
1
01 1
1
1
1
1
0
0
0
0
11
10
00
00
10
D3:2
D1:0
Sa
D3D1
D3D2D0 D3D2D1
D3D2D1D0
Sa = D3D1 + D3D2D0 + D3D2D1 + D3D2D1D0
Figure 2.52 Alternative K-map
for Sa showing incorrect
nonprime implicant

solve a practical problem. But the insight gained from the principles
underlying Karnaugh maps is valuable. And Karnaugh maps often
appear at job interviews!
2.8 COMBINATIONAL BUILDING BLOCKS
Combinational logic is often grouped into larger building blocks to
build more complex systems. This is an application of the principle of
abstraction, hiding the unnecessary gate-level details to emphasize the
function of the building block. We have already studied three such
building blocks: full adders (from Section 2.1), priority circuits (from
Section 2.4), and seven-segment display decoders (from Section 2.7).
This section introduces two more commonly used building blocks:
multiplexers and decoders. Chapter 5 covers other combinational
building blocks.
2.8.1 Multiplexers
Multiplexers are among the most commonly used combinational circuits. They choose an output from among several possible inputs based
on the value of a select signal. A multiplexer is sometimes affectionately
called a mux.
2:1 Multiplexer
Figure 2.54 shows the schematic and truth table for a 2:1 multiplexer
with two data inputs, D0 and D1, a select input, S, and one output, Y.
The multiplexer chooses between the two data inputs based on the
select: if S  0, Y  D0, and if S  1, Y  D1. S is also called a control
signal because it controls what the multiplexer does.
2.8 Combinational Building Blocks 79
Figure 2.53 K-map solution
with don’t cares
01 11
1
0
0
1
X
X
1
01 1
1
1
1
1
X
X
X
X
11
10
00
00
10 01 11
1
1
1
0
X
X
1
01 1
1
1
1
0
X
X
X
X
11
10
00
00
10
D3:2
D1:0
Sa D3:2
D1:0
Sb
Sa = D1 + D3 + D2D0 + D2D0 Sb = D3 + D3D2 + D1D0 + D1D0
Figure 2.54 2:1 multiplexer
symbol and truth table
Y
0 0
0 1
1 0
1 1
0
1
0
1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
1
1
0
1
S
D0
Y
D1
S D1 D0
Chap
A 2:1 multiplexer can be built from sum-of-products logic as shown
in Figure 2.55. The Boolean equation for the multiplexer may be derived
with a Karnaugh map or read off by inspection (Y is 1 if S  0 AND D0
is 1 OR if S  1 AND D1 is 1).
Alternatively, multiplexers can be built from tristate buffers, as
shown in Figure 2.56. The tristate enables are arranged such that, at
all times, exactly one tristate buffer is active. When S  0, tristate T0
is enabled, allowing D0 to flow to Y. When S  1, tristate T1 is
enabled, allowing D1 to flow to Y.
Wider Multiplexers
A 4:1 multiplexer has four data inputs and one output, as shown
in Figure 2.57. Two select signals are needed to choose among the four
data inputs. The 4:1 multiplexer can be built using sum-of-products
logic, tristates, or multiple 2:1 multiplexers, as shown in Figure 2.58.
The product terms enabling the tristates can be formed using AND
gates and inverters. They can also be formed using a decoder, which we
will introduce in Section 2.8.2.
Wider multiplexers, such as 8:1 and 16:1 multiplexers, can be built
by expanding the methods shown in Figure 2.58. In general, an N:1
multiplexer needs log2N select lines. Again, the best implementation
choice depends on the target technology.
Multiplexer Logic
Multiplexers can be used as lookup tables to perform logic functions.
Figure 2.59 shows a 4:1 multiplexer used to implement a two-input
80 CHAPTER TWO Combinational Logic Design
D1
Y
D0
S
S 00 01
0
1
Y
11 10
D1:0
0
0
1
0
1
1
0
1
Y = D0S + D1S Figure 2.55 2:1 multiplexer
implementation using two-level
logic
Figure 2.56 Multiplexer using
tristate buffers
Figure 2.57 4:1 multiplexer
Y
D0
S
T0
T1
Y = D0S + D1S
D1
00
S1:0
D0
D1 Y 01
10
11
D2
D3
2
Shorting together the outputs
of multiple gates technically
violates the rules for combinational circuits given in Section
2.1. But because exactly one of
the outputs is driven at any
time, this exception is allowed.
Chap
AND gate. The inputs, A and B, serve as select lines. The multiplexer
data inputs are connected to 0 or 1 according to the corresponding row
of the truth table. In general, a 2N-input multiplexer can be programmed to perform any N-input logic function by applying 0’s and 1’s
to the appropriate data inputs. Indeed, by changing the data inputs, the
multiplexer can be reprogrammed to perform a different function.
With a little cleverness, we can cut the multiplexer size in half, using
only a 2N1-input multiplexer to perform any N-input logic function.
The strategy is to provide one of the literals, as well as 0’s and 1’s, to the
multiplexer data inputs.
To illustrate this principle, Figure 2.60 shows two-input AND and
XOR functions implemented with 2:1 multiplexers. We start with an
ordinary truth table, and then combine pairs of rows to eliminate the
rightmost input variable by expressing the output in terms of this
variable. For example, in the case of AND, when A  0, Y  0, regardless of B. When A  1, Y  0 if B  0 and Y  1 if B  1, so Y  B.
We then use the multiplexer as a lookup table according to the new,
smaller truth table.
Example 2.12 LOGIC WITH MULTIPLEXERS
Alyssa P. Hacker needs to implement the function to finish her senior project, but when she looks in her lab kit, the only part she has left
is an 8:1 multiplexer. How does she implement the function?
Solution: Figure 2.61 shows Alyssa’s implementation using a single 8:1 multiplexer. The multiplexer acts as a lookup table where each row in the truth table
corresponds to a multiplexer input.
Y  AB  BC  ABC
2.8 Combinational Building Blocks 81
Figure 2.58 4:1 multiplexer
implementations: (a) twolevel logic, (b) tristates,
(c) hierarchical
(a)
Y
D0
D1
D2
D3
(b) (c)
S0
Y
0
1
0
1
0
1
S1
D0
D1
D2
D3
Y
S1S0
S1S0
S1S0
S1S0
D0
D2
D3
D1
S1 S0
ABY
000
010
100
1 1 1
Y = AB
00
Y 01
10
11
A B
Figure 2.59 4:1 multiplexer
implementation of two-input
AND function
Chapter 02.
Example 2.13 LOGIC WITH MULTIPLEXERS, REPRISED
Alyssa turns on her circuit one more time before the final presentation and
blows up the 8:1 multiplexer. (She accidently powered it with 20 V instead of 5
V after not sleeping all night.) She begs her friends for spare parts and they
give her a 4:1 multiplexer and an inverter. Can she build her circuit with only
these parts?
Solution: Alyssa reduces her truth table to four rows by letting the output
depend on C. (She could also have chosen to rearrange the columns of the truth
table to let the output depend on A or B.) Figure 2.62 shows the new design.
2.8.2 Decoders
A decoder has N inputs and 2N outputs. It asserts exactly one of its
outputs depending on the input combination. Figure 2.63 shows a
2:4 decoder. When A1:0  00, Y0 is 1. When A1:0  01, Y1 is 1. And so
forth. The outputs are called one-hot, because exactly one is “hot”
(HIGH) at a given time.
82 CHAPTER TWO Combinational Logic Design
Figure 2.61 Alyssa’s circuit:
(a) truth table, (b) 8:1
multiplexer implementation
AB Y
0 0 1
010
100
1 1 1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
C
(a)
Y = AB + BC + ABC
A B C
(b)
000
001
010
011
100
101
110
111
Y
Figure 2.60 Multiplexer logic
using variable inputs
ABY
000
011
101
110
A Y
0
1
A
B
Y
1 B
Y = A ⊕ B
0 B B
(b)
ABY
000
010
100
111
Y = AB
A Y
0 0 0
1
A
B
Y
1 B
(a)
Ch
Example 2.14 DECODER IMPLEMENTATION
Implement a 2:4 decoder with AND, OR, and NOT gates.
Solution: Figure 2.64 shows an implementation for the 2:4 decoder using four
AND gates. Each gate depends on either the true or the complementary form of
each input. In general, an N:2N decoder can be constructed from 2N N-input AND
gates that accept the various combinations of true or complementary inputs. Each
output in a decoder represents a single minterm. For example, Y0 represents the
minterm . This fact will be handy when using decoders with other digital
building blocks.
A1A0
Decoder Logic
Decoders can be combined with OR gates to build logic functions.
Figure 2.65 shows the two-input XNOR function using a 2:4 decoder
and a single OR gate. Because each output of a decoder represents a
single minterm, the function is built as the OR of all the minterms in the
function. In Figure 2.65, Y  AB  AB  AB .
2.8 Combinational Building Blocks 83
AB Y
001
010
100
111
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
0
C
(a)
A Y
0 0
0 1
101
110
B
C
C
00
Y 01
10
11
A B
C
(b) (c)
Figure 2.62 Alyssa’s new circuit
Figure 2.63 2:4 decoder
2:4
Decoder
A1
A0
Y3
Y2
Y1
00 Y0
01
10
11
0 0
0 1
1 0
1 1
0
0
0
1
A1 A0 Y3 Y2 Y1 Y0
0
0
1
0
0
1
0
0
1
0
0
0
Figure 2.64 2:4 decoder implementation
A1 A0
Y3
Y2
Y1
Y0
Figure 2.65 Logic function using
decoder
2:4
Decoder
A
B
00
01
10
11
Y = A B
Y
AB
AB
AB
AB
Minterm
⊕
Chap
When using decoders to build logic, it is easiest to express functions
as a truth table or in canonical sum-of-products form. An N-input function with M 1’s in the truth table can be built with an N:2N decoder and
an M-input OR gate attached to all of the minterms containing 1’s in the
truth table. This concept will be applied to the building of Read Only
Memories (ROMs) in Section 5.5.6.
2.9 TIMING
In previous sections, we have been concerned primarily with whether the
circuit works—ideally, using the fewest gates. However, as any seasoned
circuit designer will attest, one of the most challenging issues in circuit
design is timing: making a circuit run fast.
An output takes time to change in response to an input change.
Figure 2.66 shows the delay between an input change and the
subsequent output change for a buffer. The figure is called a timing
diagram; it portrays the transient response of the buffer circuit when
an input changes. The transition from LOW to HIGH is called the
rising edge. Similarly, the transition from HIGH to LOW (not shown
in the figure) is called the falling edge. The blue arrow indicates
that the rising edge of Y is caused by the rising edge of A. We measure
delay from the 50% point of the input signal, A, to the 50% point
of the output signal, Y. The 50% point is the point at which the
signal is half-way (50%) between its LOW and HIGH values as it
transitions.
2.9.1 Propagation and Contamination Delay
Combinational logic is characterized by its propagation delay and
contamination delay. The propagation delay, tpd, is the maximum time
from when an input changes until the output or outputs reach their
final value. The contamination delay, tcd, is the minimum time from
when an input changes until any output starts to change its value.
84 CHAPTER TWO Combinational Logic Design
Figure 2.66 Circuit delay A
Y
Time
delay
A Y
When designers speak of calculating the delay of a circuit,
they generally are referring to
the worst-case value (the
propagation delay), unless it
is clear otherwise from the
context.

Figure 2.67 illustrates a buffer’s propagation delay and contamination delay in blue and gray, respectively. The figure shows that A is
initially either HIGH or LOW and changes to the other state at a particular time; we are interested only in the fact that it changes, not
what value it has. In response, Y changes some time later. The arcs
indicate that Y may start to change tcd after A transitions and that Y
definitely settles to its new value within tpd.
The underlying causes of delay in circuits include the time required
to charge the capacitance in a circuit and the speed of light. tpd and tcd
may be different for many reasons, including
 different rising and falling delays
 multiple inputs and outputs, some of which are faster than others
 circuits slowing down when hot and speeding up when cold
Calculating tpd and tcd requires delving into the lower levels of
abstraction beyond the scope of this book. However, manufacturers normally supply data sheets specifying these delays for each gate.
Along with the factors already listed, propagation and contamination delays are also determined by the path a signal takes from input to
output. Figure 2.68 shows a four-input logic circuit. The critical path,
shown in blue, is the path from input A or B to output Y. It is the
longest, and therefore the slowest, path, because the input travels
2.9 Timing 85
Figure 2.67 Propagation and
contamination delay
A Y
A
Y
Time
tpd
tcd
A
B
C
D Y
Critical Path
Short Path
n1
n2 Figure 2.68 Short path and
critical path
Circuit delays are ordinarily
on the order of picoseconds
(1 ps  1012 seconds) to
nanoseconds (1 ns  109
seconds). Trillions of picoseconds have elapsed in the time
you spent reading this sidebar.
Chapt
through three gates to the output. This path is critical because it limits
the speed at which the circuit operates. The short path through the circuit, shown in gray, is from input D to output Y. This is the shortest, and
therefore the fastest, path through the circuit, because the input travels
through only a single gate to the output.
The propagation delay of a combinational circuit is the sum of the
propagation delays through each element on the critical path. The
contamination delay is the sum of the contamination delays through
each element on the short path. These delays are illustrated in Figure
2.69 and are described by the following equations:
(2.7)
(2.8)
Example 2.15 FINDING DELAYS
Ben Bitdiddle needs to find the propagation delay and contamination delay of
the circuit shown in Figure 2.70. According to his data book, each gate has
a propagation delay of 100 picoseconds (ps) and a contamination delay
of 60 ps.
Solution: Ben begins by finding the critical path and the shortest path through
the circuit. The critical path, highlighted in blue in Figure 2.71, is from input A
tcd  tcdAND
tpd  2tpdAND  tpdOR
86 CHAPTER TWO Combinational Logic Design
A = 1 0
Y = 1 0
D
Y
delay
Time
A
Y
delay
A = 1
B = 1
C = 0
D = 1 0 Y = 1 0
Short Path
Critical Path
Time
n1
n2
n1
n2
n1
n2
B = 1
C = 0
D = 1
Figure 2.69 Critical and short path waveforms
Although we are ignoring wire
delay in this analysis, digital
circuits are now so fast that
the delay of long wires can be
as important as the delay of
the gates. The speed of light
delay in wires is covered in
Appendix A.
Cha
or B through three gates to the output, Y. Hence, tpd is three times the propagation delay of a single gate, or 300 ps.
The shortest path, shown in gray in Figure 2.72, is from input C, D, or E
through two gates to the output, Y. There are only two gates in the shortest path,
so tcd is 120 ps.
Example 2.16 MULTIPLEXER TIMING: CONTROL-CRITICAL
VS. DATA-CRITICAL
Compare the worst-case timing of the three four-input multiplexer designs
shown in Figure 2.58 in Section 2.8.1. Table 2.7 lists the propagation delays for
the components. What is the critical path for each design? Given your timing
analysis, why might you choose one design over the other?
Solution: One of the critical paths for each of the three design options is
highlighted in blue in Figures 2.73 and 2.74. tpd_sy indicates the propagation
delay from input S to output Y; tpd_dy indicates the propagation delay from
input D to output Y; tpd is the worst of the two: max(tpd_sy, tpd_dy).
For both the two-level logic and tristate implementations in Figure 2.73, the
critical path is from one of the control signals, S, to the output, Y: tpd  tpd_sy.
These circuits are control critical, because the critical path is from the control
signals to the output. Any additional delay in the control signals will add directly
to the worst-case delay. The delay from D to Y in Figure 2.73(b) is only 50 ps,
compared with the delay from S to Y of 125 ps.
2.9 Timing 87
A
B
C
D
E
Y Figure 2.70 Ben’s circuit
Figure 2.71 Ben’s critical path
A
B
C
D
E
Y
Figure 2.72 Ben’s shortest path
A
B
C
D
E
Y
C
Figure 2.74 shows the hierarchical implementation of the 4:1 multiplexer using
two stages of 2:1 multiplexers. The critical path is from any of the D inputs to
the output. This circuit is data critical, because the critical path is from the data
input to the output: (tpd  tpd_dy).
If data inputs arrive well before the control inputs, we would prefer the design
with the shortest control-to-output delay (the hierarchical design in Figure 2.74).
Similarly, if the control inputs arrive well before the data inputs, we would
prefer the design with the shortest data-to-output delay (the tristate design in
Figure 2.73(b)).
The best choice depends not only on the critical path through the circuit and
the input arrival times, but also on the power, cost, and availability of parts.
2.9.2 Glitches
So far we have discussed the case where a single input transition causes a
single output transition. However, it is possible that a single input transition can cause multiple output transitions. These are called glitches or
hazards. Although glitches usually don’t cause problems, it is important
to realize that they exist and recognize them when looking at timing diagrams. Figure 2.75 shows a circuit with a glitch and the Karnaugh map
of the circuit.
The Boolean equation is correctly minimized, but let’s look at what
happens when A  0, C  1, and B transitions from 1 to 0. Figure 2.76
(see page 90) illustrates this scenario. The short path (shown in gray)
goes through two gates, the AND and OR gates. The critical path (shown
in blue) goes through an inverter and two gates, the AND and OR gates.
88 CHAPTER TWO Combinational Logic Design
Gate tpd (ps)
NOT 30
2-input AND 60
3-input AND 80
4-input OR 90
tristate (A to Y) 50
tristate (enable to Y) 35
Table 2.7 Timing specifications for
multiplexer circuit elements
Hazards have another meaning related to microarchitecture in Chapter 7, so we will
stick with the term glitches
for multiple output transitions
to avoid confusion.
Cha
2.9 Timing 89
Figure 2.73 4:1 multiplexer
propagation delays:
(a) two-level logic,
(b) tristate
tpd_sy = tpd_INV + tpd_AND3 + tpd_OR4
= 30 ps + 80 ps + 90 ps
= 200 ps
S1
D0
D1
D2
D3
Out
S0
(a)
tpd_dy = tpd_AND3 + tpd_OR4
= 170 ps
D2
D3
Out
S1 S0
tpd_sy = tpd_INV + tpd_AND2 + tpd_TRI_SY
 = 30 ps + 60 ps + 35 ps
 = 125 ps (b)
tpd_dy = tpd_TRI_AY
= 50 ps
D0
D1
S0
D0
D1
D2
D3
S1
Y
tpd_s0y = tpd_TRLSY + tpd_TRI_AY = 85 ns
2:1 mux
2:1 mux
2:1 mux
tpd_dy = 2 tpd_TRI_AY = 100 ns
Figure 2.74 4:1 multiplexer propagation
delays: hierarchical using 2:1 multiplexers
As B transitions from 1 to 0, n2 (on the short path) falls before n1
(on the critical path) can rise. Until n1 rises, the two inputs to the OR
gate are 0, and the output Y drops to 0. When n1 eventually rises, Y
returns to 1. As shown in the timing diagram of Figure 2.76, Y starts at
1 and ends at 1 but momentarily glitches to 0.
A
B
C
Y
00 01
1
Y
11 10
AB
1
1
0
1
0
1
0
0
C
0
Y = AB + BC
Figure 2.75 Circuit with a glitch

90 CHAPTER TWO Combinational Logic Design
Figure 2.77 Input change
crosses implicant boundary
00 01
1
Y
11 10
AB
1
1
0
1
0
1
0
0
C
0
Y = AB + BC
Figure 2.76 Timing of a glitch
A = 0
C = 1
B = 1 0
Y = 1 0 1
Short Path
Critical Path
B
Y
Time
1 0
0 1
glitch
n1
n2
n2
n1
As long as we wait for the propagation delay to elapse before we
depend on the output, glitches are not a problem, because the output
eventually settles to the right answer.
If we choose to, we can avoid this glitch by adding another gate to
the implementation. This is easiest to understand in terms of the K-map.
Figure 2.77 shows how an input transition on B from ABC  001 to
ABC  011 moves from one prime implicant circle to another. The transition across the boundary of two prime implicants in the K-map
indicates a possible glitch.
As we saw from the timing diagram in Figure 2.76, if the circuitry
implementing one of the prime implicants turns off before the
circuitry of the other prime implicant can turn on, there is a glitch.
To fix this, we add another circle that covers that prime implicant
boundary, as shown in Figure 2.78. You might recognize this as the
consensus theorem, where the added term, , is the consensus or
redundant term.
AC
Ch
2.10 Summary 91
00 01
1
Y
11 10
AB
1
1
0
1
0
1
0
0
C
0
AC Y = AB + BC + AC
B = 1 0
Y = 1
A = 0
C = 1
Figure 2.78 K-map without
glitch
Figure 2.79 Circuit without
glitch
Figure 2.79 shows the glitch-proof circuit. The added AND gate is
highlighted in blue. Now a transition on B when A  0 and C  1 does
not cause a glitch on the output, because the blue AND gate outputs 1
throughout the transition.
In general, a glitch can occur when a change in a single variable
crosses the boundary between two prime implicants in a K-map. We can
eliminate the glitch by adding redundant implicants to the K-map
to cover these boundaries. This of course comes at the cost of extra
hardware.
However, simultaneous transitions on multiple variables can also
cause glitches. These glitches cannot be fixed by adding hardware.
Because the vast majority of interesting systems have simultaneous (or
near-simultaneous) transitions on multiple variables, glitches are a fact
of life in most circuits. Although we have shown how to eliminate one
kind of glitch, the point of discussing glitches is not to eliminate them
but to be aware that they exist. This is especially important when looking at timing diagrams on a simulator or oscilloscope.
2.10 SUMMARY
A digital circuit is a module with discrete-valued inputs and outputs and
a specification describing the function and timing of the module. This
chapter has focused on combinational circuits, circuits whose outputs
depend only on the current values of the inputs.
Ch
The function of a combinational circuit can be given by a truth
table or a Boolean equation. The Boolean equation for any truth table
can be obtained systematically using sum-of-products or product-ofsums form. In sum-of-products form, the function is written as the
sum (OR) of one or more implicants. Implicants are the product
(AND) of literals. Literals are the true or complementary forms of the
input variables.
Boolean equations can be simplified using the rules of Boolean algebra. In particular, they can be simplified into minimal sum-of-products
form by combining implicants that differ only in the true and complementary forms of one of the literals: . Karnaugh maps are
a visual tool for minimizing functions of up to four variables. With practice, designers can usually simplify functions of a few variables by
inspection. Computer-aided design tools are used for more complicated
functions; such methods and tools are discussed in Chapter 4.
Logic gates are connected to create combinational circuits that perform the desired function. Any function in sum-of-products form can be
built using two-level logic with the literals as inputs: NOT gates form
the complementary literals, AND gates form the products, and OR gates
form the sum. Depending on the function and the building blocks available, multilevel logic implementations with various types of gates may be
more efficient. For example, CMOS circuits favor NAND and NOR
gates because these gates can be built directly from CMOS transistors
without requiring extra NOT gates. When using NAND and NOR
gates, bubble pushing is helpful to keep track of the inversions.
Logic gates are combined to produce larger circuits such as multiplexers, decoders, and priority circuits. A multiplexer chooses one of the data
inputs based on the select input. A decoder sets one of the outputs HIGH
according to the input. A priority circuit produces an output indicating the
highest priority input. These circuits are all examples of combinational
building blocks. Chapter 5 will introduce more building blocks, including
other arithmetic circuits. These building blocks will be used extensively to
build a microprocessor in Chapter 7.
The timing specification of a combinational circuit consists of the
propagation and contamination delays through the circuit. These indicate the longest and shortest times between an input change and the
consequent output change. Calculating the propagation delay of a circuit
involves identifying the critical path through the circuit, then adding up
the propagation delays of each element along that path. There are many
different ways to implement complicated combinational circuits; these
ways offer trade-offs between speed and cost.
The next chapter will move to sequential circuits, whose outputs
depend on previous as well as current values of the inputs. In other
words, sequential circuits have memory of the past.
PA  PA  P
92 CHAPTER TWO Combinational Logic Design
Ch
Exercises 93
Exercises
Exercise 2.1 Write a Boolean equation in sum-of-products canonical form for
each of the truth tables in Figure 2.80.
Exercise 2.2 Write a Boolean equation in product-of-sums canonical form for
the truth tables in Figure 2.80.
Exercise 2.3 Minimize each of the Boolean equations from Exercise 2.1.
Exercise 2.4 Sketch a reasonably simple combinational circuit implementing
each of the functions from Exercise 2.3. Reasonably simple means that you are
not wasteful of gates, but you don’t waste vast amounts of time checking every
possible implementation of the circuit either.
Exercise 2.5 Repeat Exercise 2.4 using only NOT gates and AND and OR
gates.
Exercise 2.6 Repeat Exercise 2.4 using only NOT gates and NAND and NOR
gates.
Exercise 2.7 Simplify the following Boolean equations using Boolean theorems.
Check for correctness using a truth table or K-map.
(a)
(b)
(c) Y ABCD  ABC  ABCD  ABD ABCD  BCD  A
Y AB  ABC  (A  C)
Y  AC  ABC
BCY
0 0
0 1
1 0
1 1
1
0
1
0
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
1
1
0
1
BCY
0 0
0 1
1 0
1 1
1
0
0
0
A
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
1
ABY
0 0
0 1
1 0
1 1
1
0
1
1
CDY
0 0
0 1
1 0
1 1
1
1
1
1
B
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
A
0 0
0 1
1 0
1 1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
0
1
0
0
0
1
0
CDY
0 0
0 1
1 0
1 1
1
0
0
1
B
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
1
1
0
A
0 0
0 1
1 0
1 1
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
0
1
1
0
1
0
0
1
(a) (b) (c) (d) (e)
Figure 2.80 Truth tables
Chapter 02.qx
94 CHAPTER TWO Combinational Logic Design
Exercise 2.8 Sketch a reasonably simple combinational circuit implementing
each of the functions from Exercise 2.7.
Exercise 2.9 Simplify each of the following Boolean equations. Sketch a reasonably simple combinational circuit implementing the simplified equation.
(a)
(b)
(c) Y  ABC  ABD  ABE  ACD  ACE 

Exercise 2.10 Give an example of a truth table requiring between 3 billion and
5 billion rows that can be constructed using fewer than 40 (but at least 1)
two-input gates.
Exercise 2.11 Give an example of a circuit with a cyclic path that is nevertheless
combinational.
Exercise 2.12 Alyssa P. Hacker says that any Boolean function can be written in
minimal sum-of-products form as the sum of all of the prime implicants of the
function. Ben Bitdiddle says that there are some functions whose minimal equation does not involve all of the prime implicants. Explain why Alyssa is right or
provide a counterexample demonstrating Ben’s point.
Exercise 2.13 Prove that the following theorems are true using perfect induction.
You need not prove their duals.
(a) The idempotency theorem (T3)
(b) The distributivity theorem (T8)
(c) The combining theorem (T10)
Exercise 2.14 Prove De Morgan’s Theorem (T12) for three variables, B2, B1, B0,
using perfect induction.
Exercise 2.15 Write Boolean equations for the circuit in Figure 2.81. You need
not minimize the equations.
BCE BDE CDE
(A  D  E) BCD
Y  A AB AB  A  B
Y  BC  ABC  BC
Chapter 02.qxd 1/31/
Exercises 95
Exercise 2.16 Minimize the Boolean equations from Exercise 2.15 and sketch an
improved circuit with the same function.
Exercise 2.17 Using De Morgan equivalent gates and bubble pushing methods,
redraw the circuit in Figure 2.82 so that you can find the Boolean equation by
inspection. Write the Boolean equation.
Exercise 2.18 Repeat Exercise 2.17 for the circuit in Figure 2.83.
A B C D
Y Z
A
B
C
D
E Y
Figure 2.81 Circuit schematic
Figure 2.82 Circuit schematic

96 CHAPTER TWO Combinational Logic Design
Exercise 2.19 Find a minimal Boolean equation for the function in Figure 2.84.
Remember to take advantage of the don’t care entries.
Exercise 2.20 Sketch a circuit for the function from Exercise 2.19.
Exercise 2.21 Does your circuit from Exercise 2.20 have any potential glitches
when one of the inputs changes? If not, explain why not. If so, show how to
modify the circuit to eliminate the glitches.
Exercise 2.22 Ben Bitdiddle will enjoy his picnic on sunny days that have no
ants. He will also enjoy his picnic any day he sees a hummingbird, as well as on
days where there are ants and ladybugs. Write a Boolean equation for his enjoyment (E) in terms of sun (S), ants (A), hummingbirds (H), and ladybugs (L).
CDY
00X
01X
10X
110
B
0 0
0 1
1 0
1 1
0
X
0
X
0
0
0
0
1
1
1
1
A
0
0
0
0
0
0
0
0
001
010
10X
111
0 0
0 1
1 0
1 1
1
1
X
1
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
Figure 2.84 Truth table
A
B
C
D
E
F
G
Y
Figure 2.83 Circuit schematic

Exercises 97
Exercise 2.23 Complete the design of the seven-segment decoder segments Sc
through Sg (see Example 2.10):
(a) Derive Boolean equations for the outputs Sc through Sg assuming that inputs
greater than 9 must produce blank (0) outputs.
(b) Derive Boolean equations for the outputs Sc through Sg assuming that inputs
greater than 9 are don’t cares.
(c) Sketch a reasonably simple gate-level implementation of part (b). Multiple
outputs can share gates where appropriate.
Exercise 2.24 A circuit has four inputs and two outputs. The inputs, A3:0, represent a number from 0 to 15. Output P should be TRUE if the number is prime
(0 and 1 are not prime, but 2, 3, 5, and so on, are prime). Output D should be
TRUE if the number is divisible by 3. Give simplified Boolean equations for each
output and sketch a circuit.
Exercise 2.25 A priority encoder has 2N inputs. It produces an N-bit binary output indicating the most significant bit of the input that is TRUE, or 0 if none of
the inputs are TRUE. It also produces an output NONE that is TRUE if none of
the input bits are TRUE. Design an eight-input priority encoder with inputs A7:0
and outputs Y2:0 and NONE. For example, if the input is 00100000, the output
Y should be 101 and NONE should be 0. Give a simplified Boolean equation for
each output, and sketch a schematic.
Exercise 2.26 Design a modified priority encoder (see Exercise 2.25) that
receives an 8-bit input, A7:0, and produces two 3-bit outputs, Y2:0 and Z2:0.
Y indicates the most significant bit of the input that is TRUE. Z indicates the
second most significant bit of the input that is TRUE. Y should be 0 if none of
the inputs are TRUE. Z should be 0 if no more than one of the inputs is TRUE.
Give a simplified Boolean equation for each output, and sketch a schematic.
Exercise 2.27 An M-bit thermometer code for the number k consists of k 1’s in
the least significant bit positions and M  k 0’s in all the more significant bit
positions. A binary-to-thermometer code converter has N inputs and 2N1
outputs. It produces a 2N1 bit thermometer code for the number specified by
the input. For example, if the input is 110, the output should be 0111111.
Design a 3:7 binary-to-thermometer code converter. Give a simplified Boolean
equation for each output, and sketch a schematic.
Exercise 2.28 Write a minimized Boolean equation for the function performed
by the circuit in Figure 2.85.

98 CHAPTER TWO Combinational Logic Design
Exercise 2.30 Implement the function from Figure 2.80(b) using
(a) an 8:1 multiplexer
(b) a 4:1 multiplexer and one inverter
(c) a 2:1 multiplexer and two other logic gates
Exercise 2.31 Implement the function from Exercise 2.9(a) using
(a) an 8:1 multiplexer
(b) a 4:1 multiplexer and no other gates
(c) a 2:1 multiplexer, one OR gate, and an inverter
Exercise 2.32 Determine the propagation delay and contamination delay of the
circuit in Figure 2.83. Use the gate delays given in Table 2.8.
Exercise 2.29 Write a minimized Boolean equation for the function performed
by the circuit in Figure 2.86.
Figure 2.85 Multiplexer circuit
0
1
00
C, D
01
10
11
A
Y
Figure 2.86 Multiplexer circuit
00
C, D
01
10
11
Y
00
A, B
01
10
11

Exercises 99
Exercise 2.33 Sketch a schematic for a fast 3:8 decoder. Suppose gate delays are
given in Table 2.8 (and only the gates in that table are available). Design your
decoder to have the shortest possible critical path, and indicate what that path is.
What are its propagation delay and contamination delay?
Exercise 2.34 Redesign the circuit from Exercise 2.24 to be as fast as possible.
Use only the gates from Table 2.8. Sketch the new circuit and indicate the critical
path. What are its propagation delay and contamination delay?
Exercise 2.35 Redesign the priority encoder from Exercise 2.25 to be as fast
as possible. You may use any of the gates from Table 2.8. Sketch the new
circuit and indicate the critical path. What are its propagation delay and
contamination delay?
Exercise 2.36 Design an 8:1 multiplexer with the shortest possible delay from
the data inputs to the output. You may use any of the gates from Table 2.7 on
page 88. Sketch a schematic. Using the gate delays from the table, determine
this delay.
Gate tpd (ps) tcd (ps)
NOT 15 10
2-input NAND 20 15
3-input NAND 30 25
2-input NOR 30 25
3-input NOR 45 35
2-input AND 30 25
3-input AND 40 30
2-input OR 40 30
3-input OR 55 45
2-input XOR 60 40
Table 2.8 Gate delays for Exercises 2.32–2.35

100 CHAPTER TWO Combinational Logic Design
Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 2.1 Sketch a schematic for the two-input XOR function using only
NAND gates. How few can you use?
Question 2.2 Design a circuit that will tell whether a given month has 31 days
in it. The month is specified by a 4-bit input, A3:0. For example, if the inputs
are 0001, the month is January, and if the inputs are 1100, the month is
December. The circuit output, Y, should be HIGH only when the month specified by the inputs has 31 days in it. Write the simplified equation, and draw
the circuit diagram using a minimum number of gates. (Hint: Remember to
take advantage of don’t cares.)
Question 2.3 What is a tristate buffer? How and why is it used?
Question 2.4 A gate or set of gates is universal if it can be used to construct any
Boolean function. For example, the set {AND, OR, NOT} is universal.
(a) Is an AND gate by itself universal? Why or why not?
(b) Is the set {OR, NOT} universal? Why or why not?
(c) Is a NAND gate by itself universal? Why or why not?
Question 2.5 Explain why a circuit’s contamination delay might be less than
(instead of equal to) its propagation delay.



3
3.1 Introduction
3.2 Latches and Flip-Flops
3.3 Synchronous Logic Design
3.4 Finite State Machines
3.5 Timing of Sequential Logic
3.6 Parallelism
3.7 Summary
Exercises
Interview Questions
Sequential Logic Design
3.1 INTRODUCTION
In the last chapter, we showed how to analyze and design combinational
logic. The output of combinational logic depends only on current input
values. Given a specification in the form of a truth table or Boolean
equation, we can create an optimized circuit to meet the specification.
In this chapter, we will analyze and design sequential logic. The outputs of sequential logic depend on both current and prior input values.
Hence, sequential logic has memory. Sequential logic might explicitly
remember certain previous inputs, or it might distill the prior inputs into
a smaller amount of information called the state of the system. The state
of a digital sequential circuit is a set of bits called state variables that
contain all the information about the past necessary to explain the future
behavior of the circuit.
The chapter begins by studying latches and flip-flops, which are
simple sequential circuits that store one bit of state. In general, sequential
circuits are complicated to analyze. To simplify design, we discipline
ourselves to build only synchronous sequential circuits consisting of combinational logic and banks of flip-flops containing the state of the circuit.
The chapter describes finite state machines, which are an easy way to
design sequential circuits. Finally, we analyze the speed of sequential
circuits and discuss parallelism as a way to increase clock speed.
3.2 LATCHES AND FLIP-FLOPS
The fundamental building block of memory is a bistable element, an
element with two stable states. Figure 3.1(a) shows a simple bistable
element consisting of a pair of inverters connected in a loop. Figure
3.1(b) shows the same circuit redrawn to emphasize the symmetry. The
inverters are cross-coupled, meaning that the input of I1 is the output of
I2 and vice versa. The circuit has no inputs, but it does have two outputs,
103

Q and . Analyzing this circuit is different from analyzing a combinational circuit because it is cyclic: Q depends on , and depends on Q.
Consider the two cases, Q is 0 or Q is 1. Working through the
consequences of each case, we have:
 Case I: Q  0
As shown in Figure 3.2(a), I2 receives a FALSE input, Q, so it produces a TRUE output on . I1 receives a TRUE input, , so it
produces a FALSE output on Q. This is consistent with the original
assumption that Q  0, so the case is said to be stable.
 Case II: Q  1
As shown in Figure 3.2(b), I2 receives a TRUE input and produces a
FALSE output on . I1 receives a FALSE input and produces a TRUE
output on Q. This is again stable.
Because the cross-coupled inverters have two stable states, Q  0
and Q  1, the circuit is said to be bistable. A subtle point is that the
circuit has a third possible state with both outputs approximately
halfway between 0 and 1. This is called a metastable state and will be
discussed in Section 3.5.4.
An element with N stable states conveys log2N bits of information,
so a bistable element stores one bit. The state of the cross-coupled
inverters is contained in one binary state variable, Q. The value of Q
tells us everything about the past that is necessary to explain the future
behavior of the circuit. Specifically, if Q  0, it will remain 0 forever,
and if Q  1, it will remain 1 forever. The circuit does have another
node, , but does not contain any additional information because if
Q is known, is also known. On the other hand, is also an acceptable choice for the state variable.
Q Q
Q Q
Q
Q Q
Q Q
Q
104 CHAPTER THREE Sequential Logic Design
(b)
Q
Q
I1
I2
Q Q
(a)
I2 I1
Figure 3.1 Cross-coupled
inverter pair
(b)
Q
Q
I1
I2
1
0
0
1
(a)
I1 Q
I2
0
1 Q
1
0
Figure 3.2 Bistable operation
of cross-coupled inverters
Just as Y is commonly used
for the output of combinational logic, Q is commonly
used for the output of sequential logic.
Chapter 0
When power is first applied to a sequential circuit, the initial state is
unknown and usually unpredictable. It may differ each time the circuit is
turned on.
Although the cross-coupled inverters can store a bit of information,
they are not practical because the user has no inputs to control the state.
However, other bistable elements, such as latches and flip-flops, provide
inputs to control the value of the state variable. The remainder of this
section considers these circuits.
3.2.1 SR Latch
One of the simplest sequential circuits is the SR latch, which is
composed of two cross-coupled NOR gates, as shown in Figure 3.3.
The latch has two inputs, S and R, and two outputs, Q and . The
SR latch is similar to the cross-coupled inverters, but its state can
be controlled through the S and R inputs, which set and reset the
output Q.
A good way to understand an unfamiliar circuit is to work out its
truth table, so that is where we begin. Recall that a NOR gate produces
a FALSE output when either input is TRUE. Consider the four possible
combinations of R and S.
 Case I: R  1, S  0
N1 sees at least one TRUE input, R, so it produces a FALSE output
on Q. N2 sees both Q and S FALSE, so it produces a TRUE
output on .
 Case II: R  0, S  1
N1 receives inputs of 0 and . Because we don’t yet know , we
can’t determine the output Q. N2 receives at least one TRUE input,
S, so it produces a FALSE output on . Now we can revisit N1,
knowing that both inputs are FALSE, so the output Q is TRUE.
 Case III: R  1, S  1
N1 and N2 both see at least one TRUE input (R or S), so each
produces a FALSE output. Hence Q and are both FALSE.
 Case IV: R  0, S  0
N1 receives inputs of 0 and . Because we don’t yet know , we
can’t determine the output. N2 receives inputs of 0 and Q. Because
we don’t yet know Q, we can’t determine the output. Now we are
stuck. This is reminiscent of the cross-coupled inverters. But we
know that Q must either be 0 or 1. So we can solve the problem by
checking what happens in each of these subcases.
Q Q
Q
Q
Q Q
Q
Q
3.2 Latches and Flip-Flops 105
R
S
N1 Q
N2 Q
Figure 3.3 SR latch schematic
Chapter 03.q
 Case IVa: Q  0
Because S and Q are FALSE, N2 produces a TRUE output on , as
shown in Figure 3.4(a). Now N1 receives one TRUE input, , so its
output, Q, is FALSE, just as we had assumed.
 Case IVb: Q  1
Because Q is TRUE, N2 produces a FALSE output on , as shown
in Figure 3.4(b). Now N1 receives two FALSE inputs, R and , so
its output, Q, is TRUE, just as we had assumed.
Putting this all together, suppose Q has some known prior value,
which we will call Qprev, before we enter Case IV. Qprev is either 0 or 1,
and represents the state of the system. When R and S are 0, Q will
remember this old value, Qprev, and will be its complement, .
This circuit has memory.
The truth table in Figure 3.5 summarizes these four cases. The
inputs S and R stand for Set and Reset. To set a bit means to make it
TRUE. To reset a bit means to make it FALSE. The outputs, Q and ,
are normally complementary. When R is asserted, Q is reset to 0 and
does the opposite. When S is asserted, Q is set to 1 and does the
opposite. When neither input is asserted, Q remembers its old value,
Qprev. Asserting both S and R simultaneously doesn’t make much sense
because it means the latch should be set and reset at the same time,
which is impossible. The poor confused circuit responds by making both
outputs 0.
The SR latch is represented by the symbol in Figure 3.6. Using the
symbol is an application of abstraction and modularity. There are various ways to build an SR latch, such as using different logic gates or transistors. Nevertheless, any circuit element with the relationship specified
by the truth table in Figure 3.5 and the symbol in Figure 3.6 is called an
SR latch.
Like the cross-coupled inverters, the SR latch is a bistable element
with one bit of state stored in Q. However, the state can be controlled
through the S and R inputs. When R is asserted, the state is reset to 0.
When S is asserted, the state is set to 1. When neither is asserted, the
state retains its old value. Notice that the entire history of inputs can be
Q
Q
Q
Q Qprev
Q
Q
Q
Q
106 CHAPTER THREE Sequential Logic Design
R
S
N1 Q
N2
0
0
(b)
1
0 1
0
Q
R
S
N1 Q
N2
0
0
(a)
0
1 0
1
Q
Figure 3.4 Bistable states of SR
latch
SRQ
0 0 Qprev
010
101
110
1
0
0
Case
IV
I
II
III
Q
Qprev
Figure 3.5 SR latch truth table
S
R Q
Q
Figure 3.6 SR latch symbol
Cha
accounted for by the single state variable Q. No matter what pattern of
setting and resetting occurred in the past, all that is needed to predict
the future behavior of the SR latch is whether it was most recently set
or reset.
3.2.2 D Latch
The SR latch is awkward because it behaves strangely when both S and
R are simultaneously asserted. Moreover, the S and R inputs conflate
the issues of what and when. Asserting one of the inputs determines
not only what the state should be but also when it should change.
Designing circuits becomes easier when these questions of what and
when are separated. The D latch in Figure 3.7(a) solves these problems.
It has two inputs. The data input, D, controls what the next state
should be. The clock input, CLK, controls when the state should
change.
Again, we analyze the latch by writing the truth table, given in
Figure 3.7(b). For convenience, we first consider the internal nodes , S,
and R. If CLK  0, both S and R are FALSE, regardless of the value
of D. If CLK  1, one AND gate will produce TRUE and the other
FALSE, depending on the value of D. Given S and R, Q and are determined using Figure 3.5. Observe that when CLK  0, Q remembers its
old value, Qprev. When CLK  1, Q  D. In all cases, is the complement of Q, as would seem logical. The D latch avoids the strange case of
simultaneously asserted R and S inputs.
Putting it all together, we see that the clock controls when data
flows through the latch. When CLK  1, the latch is transparent. The
data at D flows through to Q as if the latch were just a buffer. When
CLK  0, the latch is opaque. It blocks the new data from flowing
through to Q, and Q retains the old value. Hence, the D latch is sometimes called a transparent latch or a level-sensitive latch. The D latch
symbol is given in Figure 3.7(c).
The D latch updates its state continuously while CLK  1. We shall
see later in this chapter that it is useful to update the state only at a specific instant in time. The D flip-flop described in the next section does
just that.
Q
Q
D
3.2 Latches and Flip-Flops 107
S
R Q Q
D
CLK D R
S
(a)
Q Q
CLK
D Q
(c)
Q
S R Q
0 0 Qprev
0 1 0
1 0 1
1
0
CLK D
0 X
1 0
1 1
D
X
1
0
(b)
Qprev
Q
Figure 3.7 D latch: (a) schematic, (b) truth table, (c) symbol
Some people call a latch open
or closed rather than transparent or opaque. However,
we think those terms are
ambiguous—does open mean
transparent like an open
door, or opaque, like an open
circuit?
Chapter 
3.2.3 D Flip-Flop
A D flip-flop can be built from two back-to-back D latches controlled by
complementary clocks, as shown in Figure 3.8(a). The first latch, L1, is
called the master. The second latch, L2, is called the slave. The node
between them is named N1. A symbol for the D flip-flop is given in
Figure 3.8(b). When the output is not needed, the symbol is often
condensed as in Figure 3.8(c).
When CLK  0, the master latch is transparent and the slave is
opaque. Therefore, whatever value was at D propagates through to N1.
When CLK  1, the master goes opaque and the slave becomes transparent. The value at N1 propagates through to Q, but N1 is cut off from
D. Hence, whatever value was at D immediately before the clock rises
from 0 to 1 gets copied to Q immediately after the clock rises. At all
other times, Q retains its old value, because there is always an opaque
latch blocking the path between D and Q.
In other words, a D flip-flop copies D to Q on the rising edge of the
clock, and remembers its state at all other times. Reread this definition
until you have it memorized; one of the most common problems for
beginning digital designers is to forget what a flip-flop does. The rising
edge of the clock is often just called the clock edge for brevity. The D
input specifies what the new state will be. The clock edge indicates when
the state should be updated.
A D flip-flop is also known as a master-slave flip-flop, an edge-triggered
flip-flop, or a positive edge-triggered flip-flop. The triangle in the symbols
denotes an edge-triggered clock input. The output is often omitted when
it is not needed.
Example 3.1 FLIP-FLOP TRANSISTOR COUNT
How many transistors are needed to build the D flip-flop described in this section?
Solution: A NAND or NOR gate uses four transistors. A NOT gate uses two
transistors. An AND gate is built from a NAND and a NOT, so it uses six
transistors. The SR latch uses two NOR gates, or eight transistors. The D
latch uses an SR latch, two AND gates, and a NOT gate, or 22 transistors.
The D flip-flop uses two D latches and a NOT gate, or 46 transistors. Section
3.2.7 describes a more efficient CMOS implementation using transmission
gates.
3.2.4 Register
An N-bit register is a bank of N flip-flops that share a common CLK
input, so that all bits of the register are updated at the same time.
Q
Q
108 CHAPTER THREE Sequential Logic Design
(a)
CLK
D Q
CLK
D D Q Q N1
CLK
L1 L2
master slave
(b)
D Q
(c)
Q Q Q
Q
Figure 3.8 D flip-flop:
(a) schematic, (b) symbol,
(c) condensed symbol
The precise distinction
between flip-flops and latches
is somewhat muddled and has
evolved over time. In common
industry usage, a flip-flop is
edge-triggered. In other
words, it is a bistable element
with a clock input. The state
of the flip-flop changes only
in response to a clock edge,
such as when the clock rises
from 0 to 1. Bistable elements
without an edge-triggered
clock are commonly called
latches.
The term flip-flop or latch
by itself usually refers to a D
flip-flop or D latch, respectively, because these are the
types most commonly used in
practice.
Ch
Registers are the key building block of most sequential circuits. Figure
3.9 shows the schematic and symbol for a four-bit register with inputs
D3:0 and outputs Q3:0. D3:0 and Q3:0 are both 4-bit busses.
3.2.5 Enabled Flip-Flop
An enabled flip-flop adds another input called EN or ENABLE to
determine whether data is loaded on the clock edge. When EN is
TRUE, the enabled flip-flop behaves like an ordinary D flip-flop.
When EN is FALSE, the enabled flip-flop ignores the clock and
retains its state. Enabled flip-flops are useful when we wish to load a
new value into a flip-flop only some of the time, rather than on every
clock edge.
Figure 3.10 shows two ways to construct an enabled flip-flop from a
D flip-flop and an extra gate. In Figure 3.10(a), an input multiplexer
chooses whether to pass the value at D, if EN is TRUE, or to recycle the
old state from Q, if EN is FALSE. In Figure 3.10(b), the clock is gated.
3.2 Latches and Flip-Flops 109
CLK
D Q
D Q
D Q
D Q
D3
D2
D1
D0
Q3
Q2
Q1
Q0
(a)
D3:0 Q3:0
4 4
CLK
(b)
Figure 3.9 A 4-bit register:
(a) schematic and (b) symbol
(b)
D Q
CLK EN
D Q D Q
EN
(a) (c)
D Q
EN CLK
D
Q
0
1
Figure 3.10 Enabled flip-flop:
(a, b) schematics, (c) symbol

If EN is TRUE, the CLK input to the flip-flop toggles normally. If EN is
FALSE, the CLK input is also FALSE and the flip-flop retains its old
value. Notice that EN must not change while CLK  1, lest the flip-flop
see a clock glitch (switch at an incorrect time). Generally, performing
logic on the clock is a bad idea. Clock gating delays the clock and can
cause timing errors, as we will see in Section 3.5.3, so do it only if you
are sure you know what you are doing. The symbol for an enabled flipflop is given in Figure 3.10(c).
3.2.6 Resettable Flip-Flop
A resettable flip-flop adds another input called RESET. When RESET
is FALSE, the resettable flip-flop behaves like an ordinary D flip-flop.
When RESET is TRUE, the resettable flip-flop ignores D and resets the
output to 0. Resettable flip-flops are useful when we want to force a
known state (i.e., 0) into all the flip-flops in a system when we first
turn it on.
Such flip-flops may be synchronously or asynchronously resettable.
Synchronously resettable flip-flops reset themselves only on the rising
edge of CLK. Asynchronously resettable flip-flops reset themselves as
soon as RESET becomes TRUE, independent of CLK.
Figure 3.11(a) shows how to construct a synchronously resettable
flip-flop from an ordinary D flip-flop and an AND gate. When
is FALSE, the AND gate forces a 0 into the input of the flip-flop. When
is TRUE, the AND gate passes D to the flip-flop. In this
example, is an active low signal, meaning that the reset signal
performs its function when it is 0, not 1. By adding an inverter, the
circuit could have accepted an active high RESET signal instead.
Figures 3.11(b) and 3.11(c) show symbols for the resettable flip-flop
with active high RESET.
Asynchronously resettable flip-flops require modifying the internal
structure of the flip-flop and are left to you to design in Exercise 3.10;
however, they are frequently available to the designer as a standard
component.
As you might imagine, settable flip-flops are also occasionally
used. They load a 1 into the flip-flop when SET is asserted, and they
too come in synchronous and asynchronous flavors. Resettable and
settable flip-flops may also have an enable input and may be grouped
into N-bit registers.
3.2.7 Transistor-Level Latch and Flip-Flop Designs*
Example 3.1 showed that latches and flip-flops require a large number
of transistors when built from logic gates. But the fundamental role of a
RESET
RESET
RESET
110 CHAPTER THREE Sequential Logic Design
(a)
D Q
CLK
D Q RESET
D Q
RESET
(b) (c)
r
Figure 3.11 Synchronously
resettable flip-flop:
(a) schematic, (b, c) symbols
C
latch is to be transparent or opaque, much like a switch. Recall from
Section 1.7.7 that a transmission gate is an efficient way to build a
CMOS switch, so we might expect that we could take advantage of
transmission gates to reduce the transistor count.
A compact D latch can be constructed from a single transmission gate,
as shown in Figure 3.12(a). When CLK  1 and , the transmission gate is ON, so D flows to Q and the latch is transparent. When CLK
 0 and , the transmission gate is OFF, so Q is isolated from D
and the latch is opaque. This latch suffers from two major limitations:
 Floating output node: When the latch is opaque, Q is not held at its
value by any gates. Thus Q is called a floating or dynamic node. After
some time, noise and charge leakage may disturb the value of Q.
 No buffers: The lack of buffers has caused malfunctions on several commercial chips. A spike of noise that pulls D to a negative
voltage can turn on the nMOS transistor, making the latch transparent, even when CLK  0. Likewise, a spike on D above VDD
can turn on the pMOS transistor even when CLK  0. And the
transmission gate is symmetric, so it could be driven backward
with noise on Q affecting the input D. The general rule is that
neither the input of a transmission gate nor the state node of a
sequential circuit should ever be exposed to the outside world,
where noise is likely.
Figure 3.12(b) shows a more robust 12-transistor D latch used on
modern commercial chips. It is still built around a clocked transmission
gate, but it adds inverters I1 and I2 to buffer the input and output. The
state of the latch is held on node N1. Inverter I3 and the tristate buffer,
T1, provide feedback to turn N1 into a static node. If a small amount of
noise occurs on N1 while CLK  0, T1 will drive N1 back to a valid
logic value.
Figure 3.13 shows a D flip-flop constructed from two static latches
controlled by and CLK. Some redundant internal inverters have
been removed, so the flip-flop requires only 20 transistors.
CLK
CLK1
CLK 0
3.2 Latches and Flip-Flops 111
(a)
CLK
D Q
CLK
CLK
D
Q
N1
(b)
CLK
CLK
CLK
I1
I2
I3
T1
Figure 3.12 D latch schematic
This circuit assumes CLK and
are both available. If
not, two more transistors are
needed for a CLK inverter.
CLK
CLK
D N1
CLK
CLK
CLK
I1 I2
T1
I3
CLK
CLK
T2
CLK
CLK
I4 Q N2
Figure 3.13 D flip-flop
schematic
Chapter 0
3.2.8 Putting It All Together
Latches and flip-flops are the fundamental building blocks of sequential
circuits. Remember that a D latch is level-sensitive, whereas a D flip-flop
is edge-triggered. The D latch is transparent when CLK  1, allowing
the input D to flow through to the output Q. The D flip-flop copies D to
Q on the rising edge of CLK. At all other times, latches and flip-flops
retain their old state. A register is a bank of several D flip-flops that
share a common CLK signal.
Example 3.2 FLIP-FLOP AND LATCH COMPARISON
Ben Bitdiddle applies the D and CLK inputs shown in Figure 3.14 to a D latch
and a D flip-flop. Help him determine the output, Q, of each device.
Solution: Figure 3.15 shows the output waveforms, assuming a small delay for
Q to respond to input changes. The arrows indicate the cause of an output
change. The initial value of Q is unknown and could be 0 or 1, as indicated by
the pair of horizontal lines. First consider the latch. On the first rising edge of
CLK, D  0, so Q definitely becomes 0. Each time D changes while CLK  1,
Q also follows. When D changes while CLK  0, it is ignored. Now consider
the flip-flop. On each rising edge of CLK, D is copied to Q. At all other times,
Q retains its state.
112 CHAPTER THREE Sequential Logic Design
CLK
D
Q (latch)
Q (flop)
Figure 3.14 Example waveforms
CLK
D
Q (latch)
Q (flop)
Figure 3.15 Solution waveforms
Chap
3.3 SYNCHRONOUS LOGIC DESIGN
In general, sequential circuits include all circuits that are not combinational—that is, those whose output cannot be determined simply by looking at the current inputs. Some sequential circuits are just plain kooky. This
section begins by examining some of those curious circuits. It then introduces the notion of synchronous sequential circuits and the dynamic discipline. By disciplining ourselves to synchronous sequential circuits, we can
develop easy, systematic ways to analyze and design sequential systems.
3.3.1 Some Problematic Circuits
Example 3.3 ASTABLE CIRCUITS
Alyssa P. Hacker encounters three misbegotten inverters who have tied themselves in a loop, as shown in Figure 3.16. The output of the third inverter is fed
back to the first inverter. Each inverter has a propagation delay of 1 ns.
Determine what the circuit does.
Solution: Suppose node X is initially 0. Then Y  1, Z  0, and hence X  1,
which is inconsistent with our original assumption. The circuit has no stable
states and is said to be unstable or astable. Figure 3.17 shows the behavior of the
circuit. If X rises at time 0, Y will fall at 1 ns, Z will rise at 2 ns, and X will fall
again at 3 ns. In turn, Y will rise at 4 ns, Z will fall at 5 ns, and X will rise again
at 6 ns, and then the pattern will repeat. Each node oscillates between 0 and 1
with a period (repetition time) of 6 ns. This circuit is called a ring oscillator.
The period of the ring oscillator depends on the propagation delay of each
inverter. This delay depends on how the inverter was manufactured, the power
supply voltage, and even the temperature. Therefore, the ring oscillator period is
difficult to accurately predict. In short, the ring oscillator is a sequential circuit
with zero inputs and one output that changes periodically.
3.3 Synchronous Logic Design 113
X Y Z
Figure 3.16 Three-inverter loop
X
Y
Z
012345678 time (ns)
Figure 3.17 Ring oscillator
waveforms
Example 3.4 RACE CONDITIONS
Ben Bitdiddle designed a new D latch that he claims is better than the one in
Figure 3.17 because it uses fewer gates. He has written the truth table to find
Cha
the output, Q, given the two inputs, D and CLK, and the old state of the latch,
Qprev. Based on this truth table, he has derived Boolean equations. He obtains
Qprev by feeding back the output, Q. His design is shown in Figure 3.18. Does
his latch work correctly, independent of the delays of each gate?
Solution: Figure 3.19 shows that the circuit has a race condition that causes
it to fail when certain gates are slower than others. Suppose CLK  D  1.
The latch is transparent and passes D through to make Q  1. Now, CLK
falls. The latch should remember its old value, keeping Q  1. However, suppose the delay through the inverter from CLK to is rather long compared to the delays of the AND and OR gates. Then nodes N1 and Q may
both fall before rises. In such a case, N2 will never rise, and Q becomes
stuck at 0.
This is an example of asynchronous circuit design in which outputs are directly
fed back to inputs. Asynchronous circuits are infamous for having race conditions where the behavior of the circuit depends on which of two paths through
logic gates is fastest. One circuit may work, while a seemingly identical one built
from gates with slightly different delays may not work. Or the circuit may work
only at certain temperatures or voltages at which the delays are just right. These
mistakes are extremely difficult to track down.
3.3.2 Synchronous Sequential Circuits
The previous two examples contain loops called cyclic paths, in
which outputs are fed directly back to inputs. They are sequential
rather than combinational circuits. Combinational logic has no
cyclic paths and no races. If inputs are applied to combinational
logic, the outputs will always settle to the correct value within a
propagation delay. However, sequential circuits with cyclic paths can
have undesirable races or unstable behavior. Analyzing such circuits
for problems is time-consuming, and many bright people have made
mistakes.
CLK
CLK
114 CHAPTER THREE Sequential Logic Design
CLK
N1
N2
Q
CLK
Figure 3.19 Latch waveforms
illustrating race condition
Q
0
1
0
CLK D
0 0
0 0
0 1
Qprev
0
1
0
1
0
0
0 1
1 0
1 0
1
0
1
1
1
1 1
1 1
0
1
CLK
D
CLK
Qprev
Q
N1 = CLK·D
N2 = CLK·Qprev
Q = CLK·D + CLK·Qprev
Figure 3.18 An improved (?)
D latch
Chap
To avoid these problems, designers break the cyclic paths by inserting registers somewhere in the path. This transforms the circuit into a
collection of combinational logic and registers. The registers contain the
state of the system, which changes only at the clock edge, so we say the
state is synchronized to the clock. If the clock is sufficiently slow, so
that the inputs to all registers settle before the next clock edge, all races
are eliminated. Adopting this discipline of always using registers in the
feedback path leads us to the formal definition of a synchronous
sequential circuit.
Recall that a circuit is defined by its input and output terminals and
its functional and timing specifications. A sequential circuit has a finite
set of discrete states {S0, S1, . . . , Sk  1}. A synchronous sequential circuit
has a clock input, whose rising edges indicate a sequence of times at
which state transitions occur. We often use the terms current state and
next state to distinguish the state of the system at the present from the
state to which it will enter on the next clock edge. The functional specification details the next state and the value of each output for each possible combination of current state and input values. The timing
specification consists of an upper bound, tpcq, and a lower bound, tccq,
on the time from the rising edge of the clock until the output changes,
as well as setup and hold times, tsetup and thold, that indicate when the
inputs must be stable relative to the rising edge of the clock.
The rules of synchronous sequential circuit composition teach us
that a circuit is a synchronous sequential circuit if it consists of interconnected circuit elements such that
 Every circuit element is either a register or a combinational circuit
 At least one circuit element is a register
 All registers receive the same clock signal
 Every cyclic path contains at least one register.
Sequential circuits that are not synchronous are called asynchronous.
A flip-flop is the simplest synchronous sequential circuit. It
has one input, D, one clock, CLK, one output, Q, and two states,
{0, 1}. The functional specification for a flip-flop is that the next
state is D and that the output, Q, is the current state, as shown in
Figure 3.20.
We often call the current state variable S and the next state variable S.
In this case, the prime after S indicates next state, not inversion. The timing of sequential circuits will be analyzed in Section 3.5.
Two other common types of synchronous sequential circuits are
called finite state machines and pipelines. These will be covered later in
this chapter.
3.3 Synchronous Logic Design 115
tpcq stands for the time of
propagation from clock to Q,
where Q indicates the output
of a synchronous sequential
circuit. tccq stands for the time
of contamination from clock to
Q. These are analogous to tpd
and tcd in combinational logic.
D Q
Next
State
Current
State
S ′ S
CLK
Figure 3.20 Flip-flop current
state and next state
This definition of a synchronous sequential circuit is
sufficient, but more restrictive
than necessary. For example,
in high-performance microprocessors, some registers
may receive delayed or gated
clocks to squeeze out the last
bit of performance or power.
Similarly, some microprocessors use latches instead of
registers. However, the
definition is adequate for all
of the synchronous sequential
circuits covered in this book
and for most commercial
digital systems.
Chapt
Example 3.5 SYNCHRONOUS SEQUENTIAL CIRCUITS
Which of the circuits in Figure 3.21 are synchronous sequential circuits?
Solution: Circuit (a) is combinational, not sequential, because it has no registers. (b) is a simple sequential circuit with no feedback. (c) is neither a combinational circuit nor a synchronous sequential circuit, because it has a latch
that is neither a register nor a combinational circuit. (d) and (e) are synchronous sequential logic; they are two forms of finite state machines, which are
discussed in Section 3.4. (f) is neither combinational nor synchronous sequential, because it has a cyclic path from the output of the combinational logic
back to the input of the same logic but no register in the path. (g) is synchronous sequential logic in the form of a pipeline, which we will study in Section
3.6. (h) is not, strictly speaking, a synchronous sequential circuit, because the
second register receives a different clock signal than the first, delayed by two
inverter delays.
3.3.3 Synchronous and Asynchronous Circuits
Asynchronous design in theory is more general than synchronous design,
because the timing of the system is not limited by clocked registers. Just
as analog circuits are more general than digital circuits because analog
circuits can use any voltage, asynchronous circuits are more general
than synchronous circuits because they can use any kind of feedback.
However, synchronous circuits have proved to be easier to design and
use than asynchronous circuits, just as digital are easier than analog circuits. Despite decades of research on asynchronous circuits, virtually all
digital systems are essentially synchronous.
116 CHAPTER THREE Sequential Logic Design
CL
CLK
CL
(a)
CL
(b)
CLK
CL
CLK
(c)
(d)
CL
CLK
CL
(e)
CL
(f)
CLK
(g)
CL
CLK
CL
CLK CLK CLK
(h)
CL
Figure 3.21 Example circuits

Of course, asynchronous circuits are occasionally necessary when communicating between systems with different clocks or when receiving inputs
at arbitrary times, just as analog circuits are necessary when communicating with the real world of continuous voltages. Furthermore, research in
asynchronous circuits continues to generate interesting insights, some of
which can improve synchronous circuits too.
3.4 FINITE STATE MACHINES
Synchronous sequential circuits can be drawn in the forms shown in
Figure 3.22. These forms are called finite state machines (FSMs). They get
their name because a circuit with k registers can be in one of a finite number (2k) of unique states. An FSM has M inputs, N outputs, and k bits of
state. It also receives a clock and, optionally, a reset signal. An FSM consists of two blocks of combinational logic, next state logic and output
logic, and a register that stores the state. On each clock edge, the FSM
advances to the next state, which was computed based on the current state
and inputs. There are two general classes of finite state machines, characterized by their functional specifications. In Moore machines, the outputs
depend only on the current state of the machine. In Mealy machines, the
outputs depend on both the current state and the current inputs. Finite
state machines provide a systematic way to design synchronous sequential
circuits given a functional specification. This method will be explained in
the remainder of this section, starting with an example.
3.4.1 FSM Design Example
To illustrate the design of FSMs, consider the problem of inventing a
controller for a traffic light at a busy intersection on campus.
Engineering students are moseying between their dorms and the labs on
Academic Ave. They are busy reading about FSMs in their favorite
3.4 Finite State Machines 117
CLK
M next k N
state
logic
output
logic
(a)
inputs outputs state
next
state k
(b)
CLK
M next k N
state
logic
output
logic
inputs outputs state
next
state k
Figure 3.22 Finite state
machines: (a) Moore machine,
(b) Mealy machine
Moore and Mealy machines
are named after their promoters, researchers who developed
automata theory, the mathematical underpinnings of state
machines, at Bell Labs.
Edward F. Moore
(1925–2003), not to be
confused with Intel founder
Gordon Moore, published his
seminal article, Gedankenexperiments on Sequential
Machines in 1956. He subsequently became a professor of
mathematics and computer
science at the University of
Wisconsin.
George H. Mealy published
A Method of Synthesizing
Sequential Circuits in 1955.
He subsequently wrote the
first Bell Labs operating
system for the IBM 704
computer. He later joined
Harvard University.

textbook and aren’t looking where they are going. Football players are
hustling between the athletic fields and the dining hall on Bravado
Boulevard. They are tossing the ball back and forth and aren’t looking
where they are going either. Several serious injuries have already
occurred at the intersection of these two roads, and the Dean of Students
asks Ben Bitdiddle to install a traffic light before there are fatalities.
Ben decides to solve the problem with an FSM. He installs two traffic
sensors, TA and TB, on Academic Ave. and Bravado Blvd., respectively.
Each sensor indicates TRUE if students are present and FALSE if the street
is empty. He also installs two traffic lights, LA and LB, to control traffic.
Each light receives digital inputs specifying whether it should be green,
yellow, or red. Hence, his FSM has two inputs, TA and TB, and two outputs, LA and LB. The intersection with lights and sensors is shown in
Figure 3.23. Ben provides a clock with a 5-second period. On each clock
tick (rising edge), the lights may change based on the traffic sensors. He
also provides a reset button so that Physical Plant technicians can put the
controller in a known initial state when they turn it on. Figure 3.24 shows
a black box view of the state machine.
Ben’s next step is to sketch the state transition diagram, shown in
Figure 3.25, to indicate all the possible states of the system and the transitions between these states. When the system is reset, the lights are green
on Academic Ave. and red on Bravado Blvd. Every 5 seconds, the controller examines the traffic pattern and decides what to do next. As long
118 CHAPTER THREE Sequential Logic Design
TA
TB
LA
LB
CLK
Reset
Traffic
Light
Controller
Figure 3.24 Black box view of
finite state machine
TA
LA
TA
LB
TB
TB
LA
LB
Academic Ave. Bravado Blvd.
Dorms
Fields
Athletic
Dining
Hall
Labs
Figure 3.23 Campus map

as traffic is present on Academic Ave., the lights do not change. When
there is no longer traffic on Academic Ave., the light on Academic Ave.
becomes yellow for 5 seconds before it turns red and Bravado Blvd.’s light
turns green. Similarly, the Bravado Blvd. light remains green as long as
traffic is present on the boulevard, then turns yellow and eventually red.
In a state transition diagram, circles represent states and arcs represent transitions between states. The transitions take place on the rising
edge of the clock; we do not bother to show the clock on the diagram,
because it is always present in a synchronous sequential circuit. Moreover,
the clock simply controls when the transitions should occur, whereas the
diagram indicates which transitions occur. The arc labeled Reset pointing
from outer space into state S0 indicates that the system should enter that
state upon reset, regardless of what previous state it was in. If a state has
multiple arcs leaving it, the arcs are labeled to show what input triggers
each transition. For example, when in state S0, the system will remain in
that state if TA is TRUE and move to S1 if TA is FALSE. If a state has a
single arc leaving it, that transition always occurs regardless of the inputs.
For example, when in state S1, the system will always move to S2. The
value that the outputs have while in a particular state are indicated in the
state. For example, while in state S2, LA is red and LB is green.
Ben rewrites the state transition diagram as a state transition table
(Table 3.1), which indicates, for each state and input, what the next
state, S, should be. Note that the table uses don’t care symbols (X)
whenever the next state does not depend on a particular input. Also note
that Reset is omitted from the table. Instead, we use resettable flip-flops
that always go to state S0 on reset, independent of the inputs.
The state transition diagram is abstract in that it uses states labeled
{S0, S1, S2, S3} and outputs labeled {red, yellow, green}. To build a real
circuit, the states and outputs must be assigned binary encodings. Ben
chooses the simple encodings given in Tables 3.2 and 3.3. Each state and
each output is encoded with two bits: S1:0, LA1:0, and LB1:0.
3.4 Finite State Machines 119
S0
LA: green
LB: red
LA: red
LB: yellow
LA: yellow
LB: red
LA: red
LB: green
S1
S3 S2
TA
TA
TB
TB
Reset
Figure 3.25 State transition
diagram

Ben updates the state transition table to use these binary encodings, as
shown in Table 3.4. The revised state transition table is a truth table specifying the next state logic. It defines next state, S, as a function of the current state, S, and the inputs. The revised output table is a truth table
specifying the output logic. It defines the outputs, LA and LB, as functions
of the current state, S.
From this table, it is straightforward to read off the Boolean equations for the next state in sum-of-products form.
(3.1)
The equations can be simplified using Karnaugh maps, but often
doing it by inspection is easier. For example, the TB and terms in the
S
1 equation are clearly redundant. Thus S
1 reduces to an XOR operation. Equation 3.2 gives the next state equations.
TB
S
0S1S0TA S1S0TB
S
1 S1S0 S1S0TB S1S0TB
120 CHAPTER THREE Sequential Logic Design
Table 3.3 Output encoding
Output Encoding L1:0
green 00
yellow 01
red 10
Table 3.4 State transition table with binary encodings
Current State Inputs Next State
S1 S0 TA TB S
1 S0
00 0 X0 1
00 1 X0 0
01 XX1 0
10 X01 1
10 X11 0
11 XX0 0
Table 3.1 State transition table
Current Inputs Next State
State S TA TB S
S0 0 X S1
S0 1 X S0
S1 X X S2
S2 X 0 S3
S2 X 1 S2
S3 X X S0
Table 3.2 State encoding
State Encoding S1:0
S0 00
S1 01
S2 10
S3 11
Ch
(3.2)
Similarly, Ben writes an output table (Table 3.5) indicating, for each
state, what the output should be in that state. Again, it is straightforward to read off and simplify the Boolean equations for the outputs. For
example, observe that LA1 is TRUE only on the rows where S1 is TRUE.
(3.3)
Finally, Ben sketches his Moore FSM in the form of Figure 3.22(a).
First, he draws the 2-bit state register, as shown in Figure 3.26(a). On
each clock edge, the state register copies the next state, S
1:0, to become
the state, S1:0. The state register receives a synchronous or asynchronous
reset to initialize the FSM at startup. Then, he draws the next state logic,
based on Equation 3.2, which computes the next state, based on the current state and inputs, as shown in Figure 3.26(b). Finally, he draws the
output logic, based on Equation 3.3, which computes the outputs based
on the current state, as shown in Figure 3.26(c).
Figure 3.27 shows a timing diagram illustrating the traffic light controller going through a sequence of states. The diagram shows CLK,
Reset, the inputs TA and TB, next state S, state S, and outputs LA and LB.
Arrows indicate causality; for example, changing the state causes the outputs to change, and changing the inputs causes the next state to change.
Dashed lines indicate the rising edge of CLK when the state changes.
The clock has a 5-second period, so the traffic lights change at most
once every 5 seconds. When the finite state machine is first turned on, its
state is unknown, as indicated by the question marks. Therefore, the system should be reset to put it into a known state. In this timing diagram,
LB0S1 S0
LB1S1
LA0S1S0
LA1S1
S
0 S1S0TAS1S0TB
S
1S1 ⊕ S0
3.4 Finite State Machines 121
Table 3.5 Output table
Current State Outputs
S1 S0 LA1 LA0 LB1 LB0
00 0 0 10
01 0 1 10
10 1 0 00
11 1 0 01
Chapte
122 CHAPTER THREE Sequential Logic Design
(a)
S1
S0
S'1
S'0
CLK
state register
Reset
r
S1
S0
S'1
S'0
CLK
next state logic state register
Reset
TA
TB
inputs
(b)
S1 S0
r
S1
S0
S'1
S'0
CLK
next state logic output
logic
state register
Reset
LA1
LB1
LB0
LA0
TA
TB
inputs outputs
(c)
S1 S0
r
Figure 3.26 State machine circuit for traffic light controller
CLK
Reset
TA
TB
S'1:0
S1:0
LA1:0
LB1:0
Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 5 Cycle 6 Cycle 7 Cycle 8 Cycle 9 Cycle 10
S1 (01) S2 (10) S3 (11) S0 (00)
t (sec)
??
??
S0 (00)
S0 (00) S1 (01) S2 (10) S3 (11) S1 (01)
??
??
0 5 10 15 20 25 30 35 40 45
Green (00)
Red (10)
S0 (00)
Yellow (01) Red (10) Green (00)
Green (00) Yellow (01) Red (10)
Figure 3.27 Timing diagram for traffic light controller
This schematic uses some
AND gates with bubbles on
the inputs. They might be
constructed with AND gates
and input inverters, with
NOR gates and inverters for
the non-bubbled inputs, or
with some other combination
of gates. The best choice
depends on the particular
implementation technology.

S immediately resets to S0, indicating that asynchronously resettable flipflops are being used. In state S0, light LA is green and light LB is red.
In this example, traffic arrives immediately on Academic Ave.
Therefore, the controller remains in state S0, keeping LA green even
though traffic arrives on Bravado Blvd. and starts waiting. After 15 seconds, the traffic on Academic Ave. has all passed through and TA falls.
At the following clock edge, the controller moves to state S1, turning LA
yellow. In another 5 seconds, the controller proceeds to state S2 in which
LA turns red and LB turns green. The controller waits in state S2 until all
the traffic on Bravado Blvd. has passed through. It then proceeds to state
S3, turning LB yellow. 5 seconds later, the controller enters state S0,
turning LB red and LA green. The process repeats.
3.4.2 State Encodings
In the previous example, the state and output encodings were selected arbitrarily. A different choice would have resulted in a different circuit. A natural question is how to determine the encoding that produces the circuit with
the fewest logic gates or the shortest propagation delay. Unfortunately, there
is no simple way to find the best encoding except to try all possibilities,
which is infeasible when the number of states is large. However, it is often
possible to choose a good encoding by inspection, so that related states or
outputs share bits. Computer-aided design (CAD) tools are also good at
searching the set of possible encodings and selecting a reasonable one.
One important decision in state encoding is the choice between
binary encoding and one-hot encoding. With binary encoding, as was
used in the traffic light controller example, each state is represented as a
binary number. Because K binary numbers can be represented by log2K
bits, a system with K states only needs log2K bits of state.
In one-hot encoding, a separate bit of state is used for each state. It is
called one-hot because only one bit is “hot” or TRUE at any time. For
example, a one-hot encoded FSM with three states would have state encodings of 001, 010, and 100. Each bit of state is stored in a flip-flop, so onehot encoding requires more flip-flops than binary encoding. However, with
one-hot encoding, the next-state and output logic is often simpler, so fewer
gates are required. The best encoding choice depends on the specific FSM.
Example 3.6 FSM STATE ENCODING
A divide-by-N counter has one output and no inputs. The output Y is HIGH for
one clock cycle out of every N. In other words, the output divides the frequency
of the clock by N. The waveform and state transition diagram for a divide-by-3
counter is shown in Figure 3.28. Sketch circuit designs for such a counter using
binary and one-hot state encodings.
3.4 Finite State Machines 123
Despite Ben’s best efforts, students don’t pay attention to
traffic lights and collisions
continue to occur. The Dean
of Students next asks him to
design a catapult to throw
engineering students directly
from their dorm roofs
through the open windows
of the lab, bypassing the
troublesome intersection all
together. But that is the subject of another textbook.

Solution: Tables 3.6 and 3.7 show the abstract state transition and output tables
before encoding.
Table 3.8 compares binary and one-hot encodings for the three states.
The binary encoding uses two bits of state. Using this encoding, the state transition table is shown in Table 3.9. Note that there are no inputs; the next state
depends only on the current state. The output table is left as an exercise to the
reader. The next-state and output equations are:
(3.4)
(3.5)
The one-hot encoding uses three bits of state. The state transition table for this
encoding is shown in Table 3.10 and the output table is again left as an exercise
to the reader. The next-state and output equations are as follows:
(3.6)
(3.7)
Figure 3.29 shows schematics for each of these designs. Note that the hardware
for the binary encoded design could be optimized to share the same gate for
Y and S
0. Also observe that the one-hot encoding requires both settable (s) and
resettable (r) flip-flops to initialize the machine to S0 on reset. The best implementation choice depends on the relative cost of gates and flip-flops, but the
one-hot design is usually preferable for this specific example.
A related encoding is the one-cold encoding, in which K states are
represented with K bits, exactly one of which is FALSE.
Y S0
 S
0 S2
 S
1 S0
 S
2 S1
Y S1S0
 S
0S1S0
 S
1S1 S0
124 CHAPTER THREE Sequential Logic Design
CLK
Y
(a)
S0
Y: 1
S1
Y: 0
S2
Y: 0
Reset
(b)
Figure 3.28 Divide-by-3 counter
(a) waveform and (b) state
transition diagram
Table 3.6 Divide-by-3 counter
state transition table
Current Next
State State
S0 S1
S1 S2
S2 S0
Table 3.7 Divide-by-3 counter
output table
Current
State Output
S0 1
S1 0
S2 0
Chapter
3.4 Finite State Machines 125
Table 3.8 Binary and one-hot encodings for divide-by-3 counter
State Binary Encoding One-Hot Encoding
S2 S1 S0 S1 S0
S0 0 0 1 0 1
S1 0 1 0 1 0
S2 1 0 0 0 0
Table 3.9 State transition table with binary
encoding
Current State Next State
S1 S0 S1 S0
00 0 1
01 1 0
10 0 0
Table 3.10 State transition table with one-hot encoding
Current State Next State
S2 S1 S0 S
2 S
1 S
0
001 0 1 0
010 1 0 0
100 0 0 1
S1
S0
S'1
S'0
CLK
Reset
Y
next state logic state register output logic output
(a)
Reset
CLK
rrs
S1 S2 S0
Y
(b)
S1 S0
r
Figure 3.29 Divide-by-3 circuits
for (a) binary and (b) one-hot
encodings

3.4.3 Moore and Mealy Machines
So far, we have shown examples of Moore machines, in which the output
depends only on the state of the system. Hence, in state transition diagrams for Moore machines, the outputs are labeled in the circles. Recall
that Mealy machines are much like Moore machines, but the outputs can
depend on inputs as well as the current state. Hence, in state transition
diagrams for Mealy machines, the outputs are labeled on the arcs instead
of in the circles. The block of combinational logic that computes the outputs uses the current state and inputs, as was shown in Figure 3.22(b).
Example 3.7 MOORE VERSUS MEALY MACHINES
Alyssa P. Hacker owns a pet robotic snail with an FSM brain. The snail crawls
from left to right along a paper tape containing a sequence of 1’s and 0’s. On each
clock cycle, the snail crawls to the next bit. The snail smiles when the last four
bits that it has crawled over are, from left to right, 1101. Design the FSM to compute when the snail should smile. The input A is the bit underneath the snail’s
antennae. The output Y is TRUE when the snail smiles. Compare Moore and
Mealy state machine designs. Sketch a timing diagram for each machine showing
the input, states, and output as your snail crawls along the sequence 111011010.
Solution: The Moore machine requires five states, as shown in Figure 3.30(a).
Convince yourself that the state transition diagram is correct. In particular, why
is there an arc from S4 to S2 when the input is 1?
In comparison, the Mealy machine requires only four states, as shown in Figure
3.30(b). Each arc is labeled as A/Y. A is the value of the input that causes that
transition, and Y is the corresponding output.
Tables 3.11 and 3.12 show the state transition and output tables for the Moore
machine. The Moore machine requires at least three bits of state. Consider using a
binary state encoding: S0  000, S1  001, S2  010, S3  011, and S4  100.
Tables 3.13 and 3.14 rewrite the state transition and output tables with these
encodings (These four tables follow on page 128).
From these tables, we find the next state and output equations by inspection.
Note that these equations are simplified using the fact that states 101, 110, and
111 do not exist. Thus, the corresponding next state and output for the nonexistent states are don’t cares (not shown in the tables). We use the don’t cares
to minimize our equations.
(3.8)
S
0S2S1S0 A S1S0A
S
1S1S0A S1S0S2A
S
2S1S0A
126 CHAPTER THREE Sequential Logic Design
An easy way to remember the
difference between the two
types of finite state machines
is that a Moore machine typically has more states than a
Mealy machine for a given
problem.
Chapter 
(3.9)
Table 3.15 shows the combined state transition and output table for the Mealy
machine. The Mealy machine requires at least two bits of state. Consider using a
binary state encoding: S0  00, S1  01, S2  10, and S3  11. Table 3.16
rewrites the state transition and output table with these encodings.
From these tables, we find the next state and output equations by inspection.
(3.10)
(3.11)
The Moore and Mealy machine schematics are shown in Figure 3.31(a) and
3.31(b), respectively.
The timing diagrams for the Moore and Mealy machines are shown in Figure
3.32 (see page 131). The two machines follow a different sequence of states.
Moreover, the Mealy machine’s output rises a cycle sooner because it responds to
the input rather than waiting for the state change. If the Mealy output were
delayed through a flip-flop, it would match the Moore output. When choosing
your FSM design style, consider when you want your outputs to respond.
Y S1S0A
S0
  S1S0A S1 S0AS1S0A
S
1S1S0  S1S0A
Y  S2
3.4 Finite State Machines 127
Reset
(a)
S0
0
S1
0
S2
0
S3
0
S4
1
0
110 1
1
1 0 0
0
Reset
(b)
S0 S1 S2 S3
0/0
1/0 1/0 0/0
1/1
0/0 1/0
0/0
Figure 3.30 FSM state
transition diagrams: (a) Moore
machine, (b) Mealy machine
Chapter 
128 CHAPTER THREE Sequential Logic Design
Table 3.14 Moore output table
with state encodings
Current State Output
S2 S1 S0 Y
000 0
001 0
010 0
011 0
100 1
Table 3.13 Moore state transition table with state encodings
Current State Input Next State
S2 S1 S0 A S
2 S
1 S
0
000 0 000
000 1 001
001 0 000
001 1 010
010 0 011
010 1 010
011 0 000
011 1 100
100 0 000
100 1 010
Table 3.11 Moore state transition table
Current State Input Next State
S AS
S0 0 S0
S0 1 S1
S1 0 S0
S1 1 S2
S2 0 S3
S2 1 S2
S3 0 S0
S3 1 S4
S4 0 S0
S4 1 S2
Table 3.12 Moore output table
Current Output
State
S Y
S0 0
S1 0
S2 0
S3 0
S4 1

3.4.4 Factoring State Machines
Designing complex FSMs is often easier if they can be broken down into
multiple interacting simpler state machines such that the output of some
machines is the input of others. This application of hierarchy and modularity is called factoring of state machines.
3.4 Finite State Machines 129
Table 3.15 Mealy state transition and output table
Current State Input Next State Output
S AS Y
S0 0 S0 0
S0 1 S1 0
S1 0 S0 0
S1 1 S2 0
S2 0 S3 0
S2 1 S2 0
S3 0 S0 0
S3 1 S1 1
Table 3.16 Mealy state transition and output table with
state encodings
Current State Input Next State Output
S1 S0 A S
1 S
0 Y
00 000 0
00 101 0
01 000 0
01 110 0
10 011 0
10 110 0
11 000 0
11 101 1

130 CHAPTER THREE Sequential Logic Design
Example 3.8 UNFACTORED AND FACTORED STATE MACHINES
Modify the traffic light controller from Section 3.4.1 to have a parade mode,
which keeps the Bravado Boulevard light green while spectators and the band
march to football games in scattered groups. The controller receives two more
inputs: P and R. Asserting P for at least one cycle enters parade mode. Asserting
R for at least one cycle leaves parade mode. When in parade mode, the controller proceeds through its usual sequence until LB turns green, then remains in
that state with LB green until parade mode ends.
First, sketch a state transition diagram for a single FSM, as shown in Figure
3.33(a). Then, sketch the state transition diagrams for two interacting FSMs, as
S2
S1
S0
S'2
S'1
S'0
Y
CLK
Reset
A
(a)
S2
S1
S0
(b)
S'1
S'0
CLK
Reset
S1
S0
A
Y
S0
S1
Figure 3.31 FSM schematics for
(a) Moore and (b) Mealy
machines

3.4 Finite State Machines 131
Mealy Machine
Moore Machine
CLK
Reset
A
S
Y
S
Y
Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 5 Cycle 6 Cycle 7 Cycle 8 Cycle 9 Cycle 10
S0
S0
S1
S1
??
??
S2
S2
S3
S3
S4
S1
S0
S0
1 11 11 1 0 00
S2 S3
S3
S4
S1
S2
S2 S2
Figure 3.32 Timing diagrams for Moore and Mealy machines
Controller
TA FSM
(a)
TB
LA
LB
(b)
Mode
FSM
Lights
FSM
P
M
Controller
FSM
LA
LB
TA
TB
R
P
R
Figure 3.33 (a) single and
(b) factored designs for
modified traffic light
controller FSM
shown in Figure 3.33(b). The Mode FSM asserts the output M when it is in
parade mode. The Lights FSM controls the lights based on M and the traffic
sensors, TA and TB.
Solution: Figure 3.34(a) shows the single FSM design. States S0 to S3 handle
normal mode. States S4 to S7 handle parade mode. The two halves of the diagram are almost identical, but in parade mode, the FSM remains in S6 with a
green light on Bravado Blvd. The P and R inputs control movement between
these two halves. The FSM is messy and tedious to design. Figure 3.34(b) shows
the factored FSM design. The mode FSM has two states to track whether the
lights are in normal or parade mode. The Lights FSM is modified to remain in
S2 while M is TRUE.

3.4.5 FSM Review
Finite state machines are a powerful way to systematically design
sequential circuits from a written specification. Use the following procedure to design an FSM:
 Identify the inputs and outputs.
 Sketch a state transition diagram.
132 CHAPTER THREE Sequential Logic Design
S0
LA: green
LB: red LA: green
LB: red
LA: yellow
LB: red LA: yellow
LB: red
LA: red
LB: green LA: red
LB: green
LA: red
LB: yellow LA: red
LB: yellow
S1
S3 S2
TA
TA
TB
TB
Reset
S4 S5
S7 S6
TA
TA
P
P P
P
P
P
R
R
R
R
R
P
R
P
PTA
PTA
P
RTA
RTA
R
RTB
RTB
(a)
LA: green
LB: red
LA: yellow
LB: red
LA: red
LB: green
LA: red
LB: yellow
S0 S1
S3 S2
TA
TA
M + TB
MTB
Reset
Lights FSM
(b)
S0
M: 0
S1
M: 1
P
Reset P
Mode FSM
R
R
Figure 3.34 State transition
diagrams: (a) unfactored,
(b) factored
Ch
 For a Moore machine:
– Write a state transition table.
– Write an output table.
 For a Mealy machine:
– Write a combined state transition and output table.
 Select state encodings—your selection affects the hardware design.
 Write Boolean equations for the next state and output logic.
 Sketch the circuit schematic.
We will repeatedly use FSMs to design complex digital systems throughout this book.
3.5 TIMING OF SEQUENTIAL LOGIC
Recall that a flip-flop copies the input D to the output Q on the rising edge
of the clock. This process is called sampling D on the clock edge. If D is
stable at either 0 or 1 when the clock rises, this behavior is clearly defined.
But what happens if D is changing at the same time the clock rises?
This problem is similar to that faced by a camera when snapping a
picture. Imagine photographing a frog jumping from a lily pad into the
lake. If you take the picture before the jump, you will see a frog on a lily
pad. If you take the picture after the jump, you will see ripples in the
water. But if you take it just as the frog jumps, you may see a blurred
image of the frog stretching from the lily pad into the water. A camera is
characterized by its aperture time, during which the object must remain
still for a sharp image to be captured. Similarly, a sequential element has
an aperture time around the clock edge, during which the input must be
stable for the flip-flop to produce a well-defined output.
The aperture of a sequential element is defined by a setup time and
a hold time, before and after the clock edge, respectively. Just as the
static discipline limited us to using logic levels outside the forbidden
zone, the dynamic discipline limits us to using signals that change outside the aperture time. By taking advantage of the dynamic discipline,
we can think of time in discrete units called clock cycles, just as we
think of signal levels as discrete 1’s and 0’s. A signal may glitch and
oscillate wildly for some bounded amount of time. Under the dynamic
discipline, we are concerned only about its final value at the end of the
clock cycle, after it has settled to a stable value. Hence, we can simply
write A[n], the value of signal A at the end of the nth clock cycle,
where n is an integer, rather than A(t), the value of A at some instant t,
where t is any real number.
The clock period has to be long enough for all signals to settle. This
sets a limit on the speed of the system. In real systems, the clock does not
3.5 Timing of Sequential Logic 133
Chap
reach all flip-flops at precisely the same time. This variation in time,
called clock skew, further increases the necessary clock period.
Sometimes it is impossible to satisfy the dynamic discipline, especially when interfacing with the real world. For example, consider a circuit with an input coming from a button. A monkey might press the
button just as the clock rises. This can result in a phenomenon called
metastability, where the flip-flop captures a value partway between
0 and 1 that can take an unlimited amount of time to resolve into a good
logic value. The solution to such asynchronous inputs is to use a synchronizer, which has a very small (but nonzero) probability of producing
an illegal logic value.
We expand on all of these ideas in the rest of this section.
3.5.1 The Dynamic Discipline
So far, we have focused on the functional specification of sequential
circuits. Recall that a synchronous sequential circuit, such as a flip-flop
or FSM, also has a timing specification, as illustrated in Figure 3.35.
When the clock rises, the output (or outputs) may start to change after
the clock-to-Q contamination delay, tccq, and must definitely settle to the
final value within the clock-to-Q propagation delay, tpcq. These represent the fastest and slowest delays through the circuit, respectively. For
the circuit to sample its input correctly, the input (or inputs) must have
stabilized at least some setup time, tsetup, before the rising edge of the
clock and must remain stable for at least some hold time, thold, after the
rising edge of the clock. The sum of the setup and hold times is called
the aperture time of the circuit, because it is the total time for which the
input must remain stable.
The dynamic discipline states that the inputs of a synchronous
sequential circuit must be stable during the setup and hold aperture time
around the clock edge. By imposing this requirement, we guarantee that
the flip-flops sample signals while they are not changing. Because we
are concerned only about the final values of the inputs at the time
they are sampled, we can treat signals as discrete in time as well as in
logic levels.
134 CHAPTER THREE Sequential Logic Design
CLK
tccq
tpcq
tsetup
output(s)
input(s)
thold
Figure 3.35 Timing specification
for synchronous sequential
circuit

3.5.2 System Timing
The clock period or cycle time, Tc, is the time between rising edges of a
repetitive clock signal. Its reciprocal, fc  1/Tc, is the clock frequency.
All else being the same, increasing the clock frequency increases the
work that a digital system can accomplish per unit time. Frequency is
measured in units of Hertz (Hz), or cycles per second: 1 megahertz
(MHz)  106 Hz, and 1 gigahertz (GHz)  109 Hz.
Figure 3.36(a) illustrates a generic path in a synchronous sequential
circuit whose clock period we wish to calculate. On the rising edge of
the clock, register R1 produces output (or outputs) Q1. These signals
enter a block of combinational logic, producing D2, the input (or inputs)
to register R2. The timing diagram in Figure 3.36(b) shows that each
output signal may start to change a contamination delay after its input
change and settles to the final value within a propagation delay after its
input settles. The gray arrows represent the contamination delay through
R1 and the combinational logic, and the blue arrows represent the propagation delay through R1 and the combinational logic. We analyze the
timing constraints with respect to the setup and hold time of the second
register, R2.
Setup Time Constraint
Figure 3.37 is the timing diagram showing only the maximum delay
through the path, indicated by the blue arrows. To satisfy the setup
time of R2, D2 must settle no later than the setup time before the next
clock edge.
3.5 Timing of Sequential Logic 135
CL
CLK CLK
R1 R2
Q1 D2
(a)
CLK
Q1
D2
(b)
Tc
Figure 3.36 Path between
registers and timing diagram
CLK
Q1
D2
Tc
tpcq tpd tsetup
CL
CLK CLK
Q1 D2
R1 R2
Figure 3.37 Maximum delay
for setup time constraint
In the three decades from
when one of the authors’
families bought an Apple II
computer to the present time
of writing, microprocessor
clock frequencies have
increased from 1 MHz to
several GHz, a factor of
more than 1000. This speedup
partially explains the revolutionary changes computers
have made in society.
Cha
Hence, we find an equation for the minimum clock period:
(3.12)
In commercial designs, the clock period is often dictated by the Director
of Engineering or by the marketing department (to ensure a competitive
product). Moreover, the flip-flop clock-to-Q propagation delay and
setup time, tpcq and tsetup, are specified by the manufacturer. Hence, we
rearrange Equation 3.12 to solve for the maximum propagation delay
through the combinational logic, which is usually the only variable
under the control of the individual designer.
(3.13)
The term in parentheses, tpcq  tsetup, is called the sequencing overhead.
Ideally, the entire cycle time, Tc, would be available for useful computation in the combinational logic, tpd. However, the sequencing overhead
of the flip-flop cuts into this time. Equation 3.13 is called the setup time
constraint or max-delay constraint, because it depends on the setup
time and limits the maximum delay through combinational logic.
If the propagation delay through the combinational logic is too
great, D2 may not have settled to its final value by the time R2 needs it
to be stable and samples it. Hence, R2 may sample an incorrect result or
even an illegal logic level, a level in the forbidden region. In such a case,
the circuit will malfunction. The problem can be solved by increasing the
clock period or by redesigning the combinational logic to have a shorter
propagation delay.
Hold Time Constraint
The register R2 in Figure 3.36(a) also has a hold time constraint. Its
input, D2, must not change until some time, thold, after the rising edge of
the clock. According to Figure 3.38, D2 might change as soon as tccq 
tcd after the rising edge of the clock.
tpd  Tc  (tpcq tsetup)
Tc  tpcq  tpd  tsetup
136 CHAPTER THREE Sequential Logic Design
CLK
Q1
D2
tccq tcd
thold
CL
CLK CLK
Q1 D2
R1 R2
Figure 3.38 Minimum delay for
hold time constraint
C
Hence, we find
(3.14)
Again, tccq and thold are characteristics of the flip-flop that are usually
outside the designer’s control. Rearranging, we can solve for the minimum contamination delay through the combinational logic:
(3.15)
Equation 3.15 is also called the min-delay constraint because it limits the
minimum delay through combinational logic.
We have assumed that any logic elements can be connected to each
other without introducing timing problems. In particular, we would
expect that two flip-flops may be directly cascaded as in Figure 3.39
without causing hold time problems.
In such a case, tcd  0 because there is no combinational logic between
flip-flops. Substituting into Equation 3.15 yields the requirement that
(3.16)
In other words, a reliable flip-flop must have a hold time shorter
than its contamination delay. Often, flip-flops are designed with thold
0, so that Equation 3.16 is always satisfied. Unless noted otherwise, we
will usually make that assumption and ignore the hold time constraint in
this book.
Nevertheless, hold time constraints are critically important. If they
are violated, the only solution is to increase the contamination delay
through the logic, which requires redesigning the circuit. Unlike setup
time constraints, they cannot be fixed by adjusting the clock period.
Redesigning an integrated circuit and manufacturing the corrected
design takes months and millions of dollars in today’s advanced technologies, so hold time violations must be taken extremely seriously.
Putting It All Together
Sequential circuits have setup and hold time constraints that dictate the
maximum and minimum delays of the combinational logic between flipflops. Modern flip-flops are usually designed so that the minimum delay
through the combinational logic is 0—that is, flip-flops can be placed
back-to-back. The maximum delay constraint limits the number of consecutive gates on the critical path of a high-speed circuit, because a high
clock frequency means a short clock period.
Example 3.9 TIMING ANALYSIS
Ben Bitdiddle designed the circuit in Figure 3.40. According to the data sheets
for the components he is using, flip-flops have a clock-to-Q contamination delay
thold  tccq
tcd  thold tccq
tccq  tcd  thold
3.5 Timing of Sequential Logic 137
CLK
Figure 3.39 Back-to-back
flip-flops
Cha
of 30 ps and a propagation delay of 80 ps. They have a setup time of 50 ps and
a hold time of 60 ps. Each logic gate has a propagation delay of 40 ps and a contamination delay of 25 ps. Help Ben determine the maximum clock frequency
and whether any hold time violations could occur. This process is called timing
analysis.
Solution: Figure 3.41(a) shows waveforms illustrating when the signals might
change. The inputs, A to D, are registered, so they change shortly after CLK
rises only.
The critical path occurs when B  1, C  0, D  0, and A rises from 0 to 1,
triggering n1 to rise, X to rise and Y to fall, as shown in Figure 3.41(b). This
path involves three gate delays. For the critical path, we assume that each gate
requires its full propagation delay. Y must setup before the next rising edge of
the CLK. Hence, the minimum cycle time is
(3.17)
The maximum clock frequency is fc  1/Tc  4 GHz.
A short path occurs when A  0 and C rises, causing X to rise, as shown in
Figure 3.41(c). For the short path, we assume that each gate switches after
only a contamination delay. This path involves only one gate delay, so it may
occur after tccq  tcd  30  25  55 ps. But recall that the flip-flop has a
hold time of 60 ps, meaning that X must remain stable for 60 ps after the
rising edge of CLK for the flip-flop to reliably sample its value. In this case,
X  0 at the first rising edge of CLK, so we want the flip-flop to capture
X  0. Because X did not hold stable long enough, the actual value of X is
unpredictable. The circuit has a hold time violation and may behave erratically at any clock frequency.
Tc  tpcq3 tpd  tsetup 80  3 40 50 250 ps
138 CHAPTER THREE Sequential Logic Design
CLK CLK
A
B
C
D
n1
X'
Y'
X
Y
Figure 3.40 Sample circuit for
timing analysis
Chapter 03.q
Example 3.10 FIXING HOLD TIME VIOLATIONS
Alyssa P. Hacker proposes to fix Ben’s circuit by adding buffers to slow down
the short paths, as shown in Figure 3.42. The buffers have the same delays as
other gates. Determine the maximum clock frequency and whether any hold time
problems could occur.
Solution: Figure 3.43 shows waveforms illustrating when the signals might
change. The critical path from A to Y is unaffected, because it does not pass
through any buffers. Therefore, the maximum clock frequency is still 4 GHz.
However, the short paths are slowed by the contamination delay of the buffer.
Now X will not change until tccq  2tcd  30  2  25  80 ps. This is after
the 60 ps hold time has elapsed, so the circuit now operates correctly.
This example had an unusually long hold time to illustrate the point of hold time
problems. Most flip-flops are designed with thold 	 tccq to avoid such problems.
However, several high-performance microprocessors, including the Pentium 4, use
an element called a pulsed latch in place of a flip-flop. The pulsed latch behaves like
a flip-flop but has a short clock-to Q delay and a long hold time. In general, adding
buffers can usually, but not always, solve hold time problems without slowing the
critical path.
3.5 Timing of Sequential Logic 139
A–D
CLK
n1
X'
Y'
0 50 100 150 200 250 t (ps)
(a)
A
n1
X'
Y'
tpcq
tpd
tpd
tpd
tsetup
C
X'
tccq
tcd
thold
(b)
(c)
Figure 3.41 Timing diagram:
(a) general case, (b) critical
path, (c) short path
Ch
3.5.3 Clock Skew*
In the previous analysis, we assumed that the clock reaches all registers
at exactly the same time. In reality, there is some variation in this time.
This variation in clock edges is called clock skew. For example, the wires
from the clock source to different registers may be of different lengths,
resulting in slightly different delays, as shown in Figure 3.44. Noise also
results in different delays. Clock gating, described in Section 3.2.5,
further delays the clock. If some clocks are gated and others are not,
there will be substantial skew between the gated and ungated clocks. In
140 CHAPTER THREE Sequential Logic Design
CLK CLK
A
B
C
D
n1
X'
Y'
X
Y Buffers added to fix
hold time violation
n2
n3
Figure 3.42 Corrected circuit
to fix hold time problem
A–D
CLK
n1–n3
X'
Y'
0 50 100 150 200 250 t (ps)
Figure 3.43 Timing diagram
with buffers to fix hold time
problem
t skew
CLK1
CLK2
CL
CLK1 CLK2
R1 R2
Q1 D2
CLK
delay
CLK
Figure 3.44 Clock skew caused
by wire delay

Figure 3.44, CLK2 is early with respect to CLK1, because the clock wire
between the two registers follows a scenic route. If the clock had been
routed differently, CLK1 might have been early instead. When doing
timing analysis, we consider the worst-case scenario, so that we can
guarantee that the circuit will work under all circumstances.
Figure 3.45 adds skew to the timing diagram from Figure 3.36. The
heavy clock line indicates the latest time at which the clock signal might
reach any register; the hashed lines show that the clock might arrive up
to tskew earlier.
First, consider the setup time constraint shown in Figure 3.46. In the
worst case, R1 receives the latest skewed clock and R2 receives the earliest skewed clock, leaving as little time as possible for data to propagate
between the registers.
The data propagates through the register and combinational logic
and must setup before R2 samples it. Hence, we conclude that
(3.18)
(3.19)
Next, consider the hold time constraint shown in Figure 3.47. In
the worst case, R1 receives an early skewed clock, CLK1, and R2
receives a late skewed clock, CLK2. The data zips through the register
tpd  Tc (tpcq tsetup tskew)
Tc  tpcqtpd tsetuptskew
3.5 Timing of Sequential Logic 141
CL
CLK CLK
R1 R2
Q1 D2
(a)
CLK
Q1
D2
(b)
Tc
tskew
Figure 3.45 Timing diagram
with clock skew
CLK1
Q1
D2
Tc
tpcq tpd tsetuptskew
CL
CLK1 CLK2
R1 R2
Q1 D2
CLK2
Figure 3.46 Setup time
constraint with clock skew
C
and combinational logic but must not arrive until a hold time after the
late clock. Thus, we find that
(3.20)
(3.21)
In summary, clock skew effectively increases both the setup time and
the hold time. It adds to the sequencing overhead, reducing the time
available for useful work in the combinational logic. It also increases
the required minimum delay through the combinational logic. Even if
thold  0, a pair of back-to-back flip-flops will violate Equation 3.21
if tskew 
 tccq. To prevent serious hold time failures, designers must not
permit too much clock skew. Sometimes flip-flops are intentionally
designed to be particularly slow (i.e., large tccq), to prevent hold time
problems even when the clock skew is substantial.
Example 3.11 TIMING ANALYSIS WITH CLOCK SKEW
Revisit Example 3.9 and assume that the system has 50 ps of clock skew.
Solution: The critical path remains the same, but the setup time is effectively
increased by the skew. Hence, the minimum cycle time is
(3.22)
The maximum clock frequency is fc  1/Tc  3.33 GHz.
The short path also remains the same at 55 ps. The hold time is effectively
increased by the skew to 60  50  110 ps, which is much greater than 55 ps.
Hence, the circuit will violate the hold time and malfunction at any frequency.
The circuit violated the hold time constraint even without skew. Skew in the
system just makes the violation worse.
 80  3  40  50  50  300 ps
Tc  tpcq  3tpd  tsetuptskew
tcd  thold  tskew tccq
tccq  tcd  thold  tskew
142 CHAPTER THREE Sequential Logic Design
tcd
thold
Q1
D2
tskew
CL
CLK1 CLK2
R1 R2
Q1 D2
CLK2
CLK1
tccq
Figure 3.47 Hold time
constraint with clock skew
Chapter
Example 3.12 FIXING HOLD TIME VIOLATIONS
Revisit Example 3.10 and assume that the system has 50 ps of clock skew.
Solution: The critical path is unaffected, so the maximum clock frequency
remains 3.33 GHz.
The short path increases to 80 ps. This is still less than thold  tskew  110 ps, so
the circuit still violates its hold time constraint.
To fix the problem, even more buffers could be inserted. Buffers would need to
be added on the critical path as well, reducing the clock frequency. Alternatively,
a better flip-flop with a shorter hold time might be used.
3.5.4 Metastability
As noted earlier, it is not always possible to guarantee that the input to a
sequential circuit is stable during the aperture time, especially when the
input arrives from the external world. Consider a button connected to
the input of a flip-flop, as shown in Figure 3.48. When the button is not
pressed, D  0. When the button is pressed, D  1. A monkey presses
the button at some random time relative to the rising edge of CLK. We
want to know the output Q after the rising edge of CLK. In Case I,
when the button is pressed much before CLK, Q  1.
In Case II, when the button is not pressed until long after CLK, Q  0.
But in Case III, when the button is pressed sometime between tsetup before
CLK and thold after CLK, the input violates the dynamic discipline and the
output is undefined.
Metastable State
In reality, when a flip-flop samples an input that is changing during its
aperture, the output Q may momentarily take on a voltage between 0
and VDD that is in the forbidden zone. This is called a metastable state.
Eventually, the flip-flop will resolve the output to a stable state of either
0 or 1. However, the resolution time required to reach the stable state is
unbounded.
The metastable state of a flip-flop is analogous to a ball on the summit of a hill between two valleys, as shown in Figure 3.49. The two valleys are stable states, because a ball in the valley will remain there as
long as it is not disturbed. The top of the hill is called metastable
because the ball would remain there if it were perfectly balanced. But
because nothing is perfect, the ball will eventually roll to one side or the
other. The time required for this change to occur depends on how nearly
well balanced the ball originally was. Every bistable device has a
metastable state between the two stable states.
3.5 Timing of Sequential Logic 143
D Q
CLK
button
CLK
tsetup thold
aperture
D
Q
D
Q
D
Q
???
Case I Case II Case III
Figure 3.48 Input changing
before, after, or during
aperture
Chapt
Resolution Time
If a flip-flop input changes at a random time during the clock cycle, the
resolution time, tres, required to resolve to a stable state is also a random
variable. If the input changes outside the aperture, then tres  tpcq. But if
the input happens to change within the aperture, tres can be substantially
longer. Theoretical and experimental analyses (see Section 3.5.6) have
shown that the probability that the resolution time, tres, exceeds some
arbitrary time, t, decreases exponentially with t:
(3.23)
where Tc is the clock period, and T0 and  are characteristic of the flipflop. The equation is valid only for t substantially longer than tpcq.
Intuitively, T0/Tc describes the probability that the input changes at
a bad time (i.e., during the aperture time); this probability decreases with
the cycle time, Tc.  is a time constant indicating how fast the flip-flop
moves away from the metastable state; it is related to the delay through
the cross-coupled gates in the flip-flop.
In summary, if the input to a bistable device such as a flip-flop
changes during the aperture time, the output may take on a metastable
value for some time before resolving to a stable 0 or 1. The amount of
time required to resolve is unbounded, because for any finite time, t, the
probability that the flip-flop is still metastable is nonzero. However, this
probability drops off exponentially as t increases. Therefore, if we wait
long enough, much longer than tpcq, we can expect with exceedingly
high probability that the flip-flop will reach a valid logic level.
3.5.5 Synchronizers
Asynchronous inputs to digital systems from the real world are
inevitable. Human input is asynchronous, for example. If handled carelessly, these asynchronous inputs can lead to metastable voltages
within the system, causing erratic system failures that are extremely
difficult to track down and correct. The goal of a digital system
designer should be to ensure that, given asynchronous inputs, the probability of encountering a metastable voltage is sufficiently small.
“Sufficiently” depends on the context. For a digital cell phone, perhaps
one failure in 10 years is acceptable, because the user can always turn
the phone off and back on if it locks up. For a medical device, one failure in the expected life of the universe (1010 years) is a better target. To
guarantee good logic levels, all asynchronous inputs should be passed
through synchronizers.
P(tres 
 t)  T0
Tc
et

144 CHAPTER THREE Sequential Logic Design
metastable
stable stable
Figure 3.49 Stable and
metastable states
Cha
A synchronizer, shown in Figure 3.50, is a device that receives an
asynchronous input, D, and a clock, CLK. It produces an output, Q,
within a bounded amount of time; the output has a valid logic level with
extremely high probability. If D is stable during the aperture, Q should
take on the same value as D. If D changes during the aperture, Q may
take on either a HIGH or LOW value but must not be metastable.
Figure 3.51 shows a simple way to build a synchronizer out of two
flip-flops. F1 samples D on the rising edge of CLK. If D is changing at
that time, the output D2 may be momentarily metastable. If the clock
period is long enough, D2 will, with high probability, resolve to a valid
logic level before the end of the period. F2 then samples D2, which is
now stable, producing a good output Q.
We say that a synchronizer fails if Q, the output of the synchronizer,
becomes metastable. This may happen if D2 has not resolved to a valid
level by the time it must setup at F2—that is, if tres 
 Tctsetup.
According to Equation 3.23, the probability of failure for a single input
change at a random time is
(3.24)
The probability of failure, P(failure), is the probability that the output,
Q, will be metastable upon a single change in D. If D changes once per
second, the probability of failure per second is just P(failure). However,
if D changes N times per second, the probability of failure per second is
N times as great:
P(failure)/sec N (3.25) T0
Tc
e
 Tctsetup

P(failure) T0
Tc
e
Tctsetup

3.5 Timing of Sequential Logic 145
D Q
CLKSYNC
Figure 3.50 Synchronizer
symbol
D
Q
D2 Q
D2
Tc
tsetup tpcq
CLK CLK
CLK
tres
metastable
F1 F2
Figure 3.51 Simple synchronizer
Chapter
System reliability is usually measured in mean time between failures
(MTBF). As the name might suggest, MTBF is the average amount of
time between failures of the system. It is the reciprocal of the probability
that the system will fail in any given second
(3.26)
Equation 3.26 shows that the MTBF improves exponentially as the
synchronizer waits for a longer time, Tc. For most systems, a synchronizer that waits for one clock cycle provides a safe MTBF. In exceptionally high-speed systems, waiting for more cycles may be necessary.
Example 3.13 SYNCHRONIZER FOR FSM INPUT
The traffic light controller FSM from Section 3.4.1 receives asynchronous inputs
from the traffic sensors. Suppose that a synchronizer is used to guarantee stable
inputs to the controller. Traffic arrives on average 0.2 times per second. The flipflops in the synchronizer have the following characteristics:   200 ps, T0
150 ps, and tsetup  500 ps. How long must the synchronizer clock period be for
the MTBF to exceed 1 year?
Solution: 1 year    107 seconds. Solve Equation 3.26.
(3.27)
This equation has no closed form solution. However, it is easy enough to solve by
guess and check. In a spreadsheet, try a few values of Tc and calculate the MTBF
until discovering the value of Tc that gives an MTBF of 1 year: Tc  3.036 ns.
3.5.6 Derivation of Resolution Time*
Equation 3.23 can be derived using a basic knowledge of circuit theory,
differential equations, and probability. This section can be skipped if you
are not interested in the derivation or if you are unfamiliar with the
mathematics.
A flip-flop output will be metastable after some time, t, if the flipflop samples a changing input (causing a metastable condition) and the
output does not resolve to a valid level within that time after the clock
edge. Symbolically, this can be expressed as
P(t (3.28) res 
 t) P(samples changing input) P(unresolved)
107  Tc e
Tc5001012
2001012
(0.2)(1501012)
MTBF  1
P(failure)/sec  Tc e
Tc  tsetup

NT0
146 CHAPTER THREE Sequential Logic Design
Chapter 03.qxd
We consider each probability term individually. The asynchronous
input signal switches between 0 and 1 in some time, tswitch, as shown in
Figure 3.52. The probability that the input changes during the aperture
around the clock edge is
(3.29)
If the flip-flop does enter metastability—that is, with probability
P(samples changing input)—the time to resolve from metastability
depends on the inner workings of the circuit. This resolution time determines P(unresolved), the probability that the flip-flop has not yet
resolved to a valid logic level after a time t. The remainder of this section
analyzes a simple model of a bistable device to estimate this probability.
A bistable device uses storage with positive feedback. Figure 3.53(a)
shows this feedback implemented with a pair of inverters; this circuit’s
behavior is representative of most bistable elements. A pair of inverters
behaves like a buffer. Let us model it as having the symmetric DC transfer
characteristics shown in Figure 3.53(b), with a slope of G. The buffer can
deliver only a finite amount of output current; we can model this as an
output resistance, R. All real circuits also have some capacitance, C, that
must be charged up. Charging the capacitor through the resistor causes
an RC delay, preventing the buffer from switching instantaneously.
Hence, the complete circuit model is shown in Figure 3.53(c), where
vout(t) is the voltage of interest conveying the state of the bistable device.
The metastable point for this circuit is vout(t)  vin(t)  VDD/2; if the
circuit began at exactly that point, it would remain there indefinitely in the
P(samples changing input) tswitch  tsetup  thold
Tc
3.5 Timing of Sequential Logic 147
CLK
D
Tc
tswitch
tsetup thold Figure 3.52 Input timing
(a)
v(t)
vout(t)
R
C
(c)
vin(t) i(t)
G
(b)
vin
vout
slope = G
VDD 0
VDD
VDD/2
VDD/2 ∆V Figure 3.53 Circuit model
of bistable device
Cha
absence of noise. Because voltages are continuous variables, the chance that
the circuit will begin at exactly the metastable point is vanishingly small.
However, the circuit might begin at time 0 near metastability at vout(0)
VDD/2  V for some small offset V. In such a case, the positive feedback
will eventually drive vout(t) to VDD if V 
 0 and to 0 if V 	 0. The time
required to reach VDD or 0 is the resolution time of the bistable device.
The DC transfer characteristic is nonlinear, but it appears linear near
the metastable point, which is the region of interest to us. Specifically, if
vin(t)  VDD/2  V/G, then vout(t)  VDD/2  V for small V. The
current through the resistor is i(t)  (vout(t)  vin(t))/R. The capacitor
charges at a rate dvin(t)/dt  i(t)/C. Putting these facts together, we find
the governing equation for the output voltage.
(3.30)
This is a linear first-order differential equation. Solving it with the initial
condition vout(0)  VDD/2  V gives
(3.31)
Figure 3.54 plots trajectories for vout(t) given various starting points.
vout(t) moves exponentially away from the metastable point VDD/2 until
it saturates at VDD or 0. The output voltage eventually resolves to 1 or 0.
The amount of time this takes depends on the initial voltage offset (V)
from the metastable point (VDD/2).
Solving Equation 3.31 for the resolution time tres, such that vout(tres)
 VDD or 0, gives
(3.32)
t (3.33) res RC
G  1 ln VDD
2|V|
|V|e
(G1)tres
RC  VDD
2
vout (t)  VDD
2  Ve
(G1)t
RC
dvout(t)
dt  (G1)
RC vout(t) VDD
2 
148 CHAPTER THREE Sequential Logic Design
0
VDD/2
VDD
t
vout(t)
Figure 3.54 Resolution
trajectories
Chapter 03.qxd 1/2
In summary, the resolution time increases if the bistable device has high
resistance or capacitance that causes the output to change slowly.
It decreases if the bistable device has high gain, G. The resolution time
also increases logarithmically as the circuit starts closer to the metastable
point (V → 0).
Define  as . Solving Equation 3.33 for V finds the initial
offset, Vres, that gives a particular resolution time, tres:
(3.34)
Suppose that the bistable device samples the input while it is changing. It measures a voltage, vin(0), which we will assume is uniformly distributed between 0 and VDD. The probability that the output has not
resolved to a legal value after time tres depends on the probability that
the initial offset is sufficiently small. Specifically, the initial offset on vout
must be less than Vres, so the initial offset on vin must be less than
Vres/G. Then the probability that the bistable device samples the input
at a time to obtain a sufficiently small initial offset is
(3.35)
Putting this all together, the probability that the resolution time exceeds
some time, t, is given by the following equation:
(3.36)
Observe that Equation 3.36 is in the form of Equation 3.23, where
T0  (tswitch  tsetup  thold)/G and   RC/(G  1). In summary, we
have derived Equation 3.23 and shown how T0 and  depend on physical properties of the bistable device.
3.6 PARALLELISM
The speed of a system is measured in latency and throughput of tokens
moving through a system. We define a token to be a group of inputs that
are processed to produce a group of outputs. The term conjures up the
notion of placing subway tokens on a circuit diagram and moving them
around to visualize data moving through the circuit. The latency of a system is the time required for one token to pass through the system from
start to end. The throughput is the number of tokens that can be produced per unit time.
Ptres 
 t  tswitch  tsetup  thold
GTc
e
 t

P(unresolved) Pvin(0)  VDD
2 	
Vres
G  2Vres
GVDD
Vres  VDD
2 etres/
RC
G1
3.6 Parallelism 149
Chapter 03.
Example 3.14 COOKIE THROUGHPUT AND LATENCY
Ben Bitdiddle is throwing a milk and cookies party to celebrate the installation
of his traffic light controller. It takes him 5 minutes to roll cookies and place
them on his tray. It then takes 15 minutes for the cookies to bake in the oven.
Once the cookies are baked, he starts another tray. What is Ben’s throughput and
latency for a tray of cookies?
Solution: In this example, a tray of cookies is a token. The latency is 1/3 hour
per tray. The throughput is 3 trays/hour.
As you might imagine, the throughput can be improved by processing
several tokens at the same time. This is called parallelism, and it comes in
two forms: spatial and temporal. With spatial parallelism, multiple copies
of the hardware are provided so that multiple tasks can be done at the
same time. With temporal parallelism, a task is broken into stages, like an
assembly line. Multiple tasks can be spread across the stages. Although
each task must pass through all stages, a different task will be in each
stage at any given time so multiple tasks can overlap. Temporal parallelism is commonly called pipelining. Spatial parallelism is sometimes just
called parallelism, but we will avoid that naming convention because it is
ambiguous.
Example 3.15 COOKIE PARALLELISM
Ben Bitdiddle has hundreds of friends coming to his party and needs to bake
cookies faster. He is considering using spatial and/or temporal parallelism.
Spatial Parallelism: Ben asks Alyssa P. Hacker to help out. She has her own
cookie tray and oven.
Temporal Parallelism: Ben gets a second cookie tray. Once he puts one cookie
tray in the oven, he starts rolling cookies on the other tray rather than waiting
for the first tray to bake.
What is the throughput and latency using spatial parallelism? Using temporal
parallelism? Using both?
Solution: The latency is the time required to complete one task from start to
finish. In all cases, the latency is 1/3 hour. If Ben starts with no cookies, the
latency is the time needed for him to produce the first cookie tray.
The throughput is the number of cookie trays per hour. With spatial parallelism, Ben
and Alyssa each complete one tray every 20 minutes. Hence, the throughput doubles,
to 6 trays/hour. With temporal parallelism, Ben puts a new tray in the oven every 15
minutes, for a throughput of 4 trays/hour. These are illustrated in Figure 3.55.
If Ben and Alyssa use both techniques, they can bake 8 trays/hour.
150 CHAPTER THREE Sequential Logic Design

Consider a task with latency L. In a system with no parallelism, the
throughput is 1/L. In a spatially parallel system with N copies of the
hardware, the throughput is N/L. In a temporally parallel system, the
task is ideally broken into N steps, or stages, of equal length. In such a
case, the throughput is also N/L, and only one copy of the hardware is
required. However, as the cookie example showed, finding N steps of
equal length is often impractical. If the longest step has a latency L1, the
pipelined throughput is 1/L1.
Pipelining (temporal parallelism) is particularly attractive because it
speeds up a circuit without duplicating the hardware. Instead, registers
are placed between blocks of combinational logic to divide the logic into
shorter stages that can run with a faster clock. The registers prevent a
token in one pipeline stage from catching up with and corrupting the
token in the next stage.
Figure 3.56 shows an example of a circuit with no pipelining. It contains four blocks of logic between the registers. The critical path passes
through blocks 2, 3, and 4. Assume that the register has a clock-to-Q
propagation delay of 0.3 ns and a setup time of 0.2 ns. Then the cycle
time is Tc  0.3  3  2  4  0.2  9.5 ns. The circuit has a latency
of 9.5 ns and a throughput of 1/9.5 ns  105 MHz.
Figure 3.57 shows the same circuit partitioned into a two-stage
pipeline by adding a register between blocks 3 and 4. The first stage has
a minimum clock period of 0.3  3  2  0.2  5.5 ns. The second
3.6 Parallelism 151
Spatial
Parallelism
Temporal
Parallelism
Ben 1 Ben 1
Roll
Bake
Ben 2 Ben 2
Ben 3 Ben 3
Ben 1 Ben 1
Alyssa 1 Alyssa 1
Ben 2 Ben 2
Alyssa 2 Alyssa 2
Time
0 5 10 15 20 25 30 35 40 45 50
Tray 1
Tray 2
Tray 3
Tray 4
Latency:
time to
first tray
Legend
Tray 1
Tray 2
Tray 3
Figure 3.55 Spatial and temporal parallelism in the cookie kitchen
Chap
stage has a minimum clock period of 0.3  4  0.2  4.5 ns. The clock
must be slow enough for all stages to work. Hence, Tc  5.5 ns. The
latency is two clock cycles, or 11 ns. The throughput is 1/5.5 ns  182
MHz. This example shows that, in a real circuit, pipelining with two
stages almost doubles the throughput and slightly increases the latency.
In comparison, ideal pipelining would exactly double the throughput at
no penalty in latency. The discrepancy comes about because the circuit
cannot be divided into two exactly equal halves and because the registers
introduce more sequencing overhead.
Figure 3.58 shows the same circuit partitioned into a three-stage
pipeline. Note that two more registers are needed to store the results of
blocks 1 and 2 at the end of the first pipeline stage. The cycle time is now
limited by the third stage to 4.5 ns. The latency is three cycles, or 13.5 ns.
The throughput is 1/4.5 ns  222 MHz. Again, adding a pipeline stage
improves throughput at the expense of some latency.
Although these techniques are powerful, they do not apply to all
situations. The bane of parallelism is dependencies. If a current task is
152 CHAPTER THREE Sequential Logic Design
CL CL CL
CL
CLK CLK
tpd1 = 2.4 ns
tpd2 = 3 ns
tpd3 = 2 ns tpd4 = 4 ns
1
2
3 4
Stage 1: 5.5 ns Stage 2: 4.5 ns
CLK
Figure 3.57 Circuit with twostage pipeline
CL CL CL
CL
CLK CLK
tpd1 = 2.4 ns
tpd2 = 3 ns
tpd3 = 2 ns tpd4 = 4 ns
1
2
3 4
Stage 1: 3.5 ns Stage 3: 4.5 ns
CLK CLK
Stage 2: 2.5 ns
Figure 3.58 Circuit with threestage pipeline
CL CL CL
CL
CLK CLK
tpd1 = 2.4 ns
tpd2 = 3 ns
tpd3 = 2 ns tpd4 = 4 ns
1
2
3 4
Tc = 9.5 ns
Figure 3.56 Circuit with no
pipelining
Chap
dependent on the result of a prior task, rather than just prior steps in
the current task, the task cannot start until the prior task has completed. For example, if Ben wants to check that the first tray of cookies
tastes good before he starts preparing the second, he has a dependency
that prevents pipelining or parallel operation. Parallelism is one of the
most important techniques for designing high-performance microprocessors. Chapter 7 discusses pipelining further and shows examples
of handling dependencies.
3.7 SUMMARY
This chapter has described the analysis and design of sequential logic. In
contrast to combinational logic, whose outputs depend only on the current inputs, sequential logic outputs depend on both current and prior
inputs. In other words, sequential logic remembers information about
prior inputs. This memory is called the state of the logic.
Sequential circuits can be difficult to analyze and are easy to design
incorrectly, so we limit ourselves to a small set of carefully designed
building blocks. The most important element for our purposes is the
flip-flop, which receives a clock and an input, D, and produces an output, Q. The flip-flop copies D to Q on the rising edge of the clock and
otherwise remembers the old state of Q. A group of flip-flops sharing a
common clock is called a register. Flip-flops may also receive reset or
enable control signals.
Although many forms of sequential logic exist, we discipline ourselves to use synchronous sequential circuits because they are easy to
design. Synchronous sequential circuits consist of blocks of combinational logic separated by clocked registers. The state of the circuit is
stored in the registers and updated only on clock edges.
Finite state machines are a powerful technique for designing sequential circuits. To design an FSM, first identify the inputs and outputs of
the machine and sketch a state transition diagram, indicating the states
and the transitions between them. Select an encoding for the states, and
rewrite the diagram as a state transition table and output table, indicating the next state and output given the current state and input. From
these tables, design the combinational logic to compute the next state
and output, and sketch the circuit.
Synchronous sequential circuits have a timing specification including
the clock-to-Q propagation and contamination delays, tpcq and tccq, and
the setup and hold times, tsetup and thold. For correct operation, their
inputs must be stable during an aperture time that starts a setup time
before the rising edge of the clock and ends a hold time after the
rising edge of the clock. The minimum cycle time, Tc, of the system is
equal to the propagation delay, tpd, through the combinational logic plus
3.7 Summary 153
Anyone who could invent
logic whose outputs depend
on future inputs would be
fabulously wealthy!

154 CHAPTER THREE Sequential Logic Design
tpcq  tsetup of the register. For correct operation, the contamination
delay through the register and combinational logic must be greater than
thold. Despite the common misconception to the contrary, hold time does
not affect the cycle time.
Overall system performance is measured in latency and throughput.
The latency is the time required for a token to pass from start to end.
The throughput is the number of tokens that the system can process per
unit time. Parallelism improves the system throughput.

Exercises 155
Exercises
Exercise 3.1 Given the input waveforms shown in Figure 3.59, sketch the output, Q, of an SR latch.
S
R
Figure 3.59 Input waveform of SR latch
Exercise 3.2 Given the input waveforms shown in Figure 3.60, sketch the output, Q, of a D latch.
Exercise 3.3 Given the input waveforms shown in Figure 3.60, sketch the output, Q, of a D flip-flop.
Exercise 3.4 Is the circuit in Figure 3.61 combinational logic or sequential logic?
Explain in a simple fashion what the relationship is between the inputs and
outputs. What would you call this circuit?
Exercise 3.5 Is the circuit in Figure 3.62 combinational logic or sequential logic?
Explain in a simple fashion what the relationship is between the inputs and
outputs. What would you call this circuit?
CLK
D
Figure 3.60 Input waveform of D latch or flip-flop
S
R
Q
Q
Figure 3.61 Mystery circuit

156 CHAPTER THREE Sequential Logic Design
Exercise 3.6 The toggle (T) flip-flop has one input, CLK, and one output, Q.
On each rising edge of CLK, Q toggles to the complement of its previous value.
Draw a schematic for a T flip-flop using a D flip-flop and an inverter.
Exercise 3.7 A JK flip-flop receives a clock and two inputs, J and K. On the
rising edge of the clock, it updates the output, Q. If J and K are both 0,
Q retains its old value. If only J is 1, Q becomes 1. If only K is 1, Q becomes
0. If both J and K are 1, Q becomes the opposite of its present state.
(a) Construct a JK flip-flop using a D flip-flop and some combinational logic.
(b) Construct a D flip-flop using a JK flip-flop and some combinational logic.
(c) Construct a T flip-flop (see Exercise 3.6) using a JK flip-flop.
Exercise 3.8 The circuit in Figure 3.63 is called a Muller C-element. Explain in a
simple fashion what the relationship is between the inputs and output.
Q
Q
S
R
S
R
D
R
CLK
Figure 3.62 Mystery circuit
A
B
A
B
C
weak
Figure 3.63 Muller C-element
Exercise 3.9 Design an asynchronously resettable D latch using logic gates.
Exercise 3.10 Design an asynchronously resettable D flip-flop using logic gates.
Exercise 3.11 Design a synchronously settable D flip-flop using logic gates.

Exercises 157
Exercise 3.12 Design an asynchronously settable D flip-flop using logic gates.
Exercise 3.13 Suppose a ring oscillator is built from N inverters connected in a
loop. Each inverter has a minimum delay of tcd and a maximum delay of tpd. If
N is odd, determine the range of frequencies at which the oscillator might
operate.
Exercise 3.14 Why must N be odd in Exercise 3.13?
Exercise 3.15 Which of the circuits in Figure 3.64 are synchronous sequential
circuits? Explain.
Exercise 3.16 You are designing an elevator controller for a building with 25
floors. The controller has two inputs: UP and DOWN. It produces an output
indicating the floor that the elevator is on. There is no floor 13. What is the minimum number of bits of state in the controller?
Exercise 3.17 You are designing an FSM to keep track of the mood of four students working in the digital design lab. Each student’s mood is either HAPPY
(the circuit works), SAD (the circuit blew up), BUSY (working on the circuit),
CLUELESS (confused about the circuit), or ASLEEP (face down on the circuit
board). How many states does the FSM have? What is the minimum number of
bits necessary to represent these states?
Exercise 3.18 How would you factor the FSM from Exercise 3.17 into multiple
simpler machines? How many states does each simpler machine have? What is
the minimum total number of bits necessary in this factored design?
(a)
CL CL
CLK
CL
CL
CL
CL
CLK
CL
(b)
(c) (d)
CL CL
CLK
Figure 3.64 Circuits

Exercise 3.19 Describe in words what the state machine in Figure 3.65 does.
Using binary state encodings, complete a state transition table and output table
for the FSM. Write Boolean equations for the next state and output and sketch
a schematic of the FSM.
Exercise 3.20 Describe in words what the state machine in Figure 3.66 does.
Using binary state encodings, complete a state transition table and output table
for the FSM. Write Boolean equations for the next state and output and sketch
a schematic of the FSM.
Exercise 3.21 Accidents are still occurring at the intersection of Academic
Avenue and Bravado Boulevard. The football team is rushing into the intersection the moment light B turns green. They are colliding with sleep-deprived CS
majors who stagger into the intersection just before light A turns red. Extend the
traffic light controller from Section 3.4.1 so that both lights are red for 5 seconds
before either light turns green again. Sketch your improved Moore machine state
transition diagram, state encodings, state transition table, output table, next
state and output equations, and your FSM schematic.
Exercise 3.22 Alyssa P. Hacker’s snail from Section 3.4.3 has a daughter with a
Mealy machine FSM brain. The daughter snail smiles whenever she slides over
the pattern 1101 or the pattern 1110. Sketch the state transition diagram for this
happy snail using as few states as possible. Choose state encodings and write a
158 CHAPTER THREE Sequential Logic Design
S0
Q: 0
S1
Q: 0
S2
Q: 1
Reset
A B
A
B
Figure 3.65 State transition diagram
S0 S1 S2
Reset
A/0 B/0
A/0
B/0 AB/1
A + B/0
Figure 3.66 State transition diagram

Exercises 159
combined state transition and output table using your encodings. Write the next
state and output equations and sketch your FSM schematic.
Exercise 3.23 You have been enlisted to design a soda machine dispenser for
your department lounge. Sodas are partially subsidized by the student chapter of
the IEEE, so they cost only 25 cents. The machine accepts nickels, dimes, and
quarters. When enough coins have been inserted, it dispenses the soda and
returns any necessary change. Design an FSM controller for the soda machine.
The FSM inputs are Nickel, Dime, and Quarter, indicating which coin was
inserted. Assume that exactly one coin is inserted on each cycle. The outputs are
Dispense, ReturnNickel, ReturnDime, and ReturnTwoDimes. When the FSM
reaches 25 cents, it asserts Dispense and the necessary Return outputs required
to deliver the appropriate change. Then it should be ready to start accepting
coins for another soda.
Exercise 3.24 Gray codes have a useful property in that consecutive numbers
differ in only a single bit position. Table 3.17 lists a 3-bit Gray code representing
the numbers 0 to 7. Design a 3-bit modulo 8 Gray code counter FSM with no
inputs and three outputs. (A modulo N counter counts from 0 to N1, then
repeats. For example, a watch uses a modulo 60 counter for the minutes and seconds that counts from 0 to 59.) When reset, the output should be 000. On each
clock edge, the output should advance to the next Gray code. After reaching
100, it should repeat with 000.
Table 3.17 3-bit Gray code
Number Gray code
0 000
1 001
2 011
3 010
4 110
5 111
6 101
7 100
Exercise 3.25 Extend your modulo 8 Gray code counter from Exercise 3.24 to be
an UP/DOWN counter by adding an UP input. If UP  1, the counter advances
to the next number. If UP  0, the counter retreats to the previous number.
Cha
Exercise 3.26 Your company, Detect-o-rama, would like to design an FSM that
takes two inputs, A and B, and generates one output, Z. The output in cycle n,
Zn, is either the Boolean AND or OR of the corresponding input An and the previous input An1, depending on the other input, Bn:
(a) Sketch the waveform for Z given the inputs shown in Figure 3.67.
(b) Is this FSM a Moore or a Mealy machine?
(c) Design the FSM. Show your state transition diagram, encoded state transition table, next state and output equations, and schematic.
 ZnAn An1 if Bn 1
Zn An An1 if Bn  0
160 CHAPTER THREE Sequential Logic Design
CLK
A
B
Figure 3.67 FSM input waveforms
Exercise 3.27 Design an FSM with one input, A, and two outputs, X and Y. X
should be 1 if A has been 1 for at least three cycles altogether (not necessarily
consecutively). Y should be 1 if A has been 1 for at least two consecutive cycles.
Show your state transition diagram, encoded state transition table, next state
and output equations, and schematic.
Exercise 3.28 Analyze the FSM shown in Figure 3.68. Write the state transition
and output tables and sketch the state transition diagram. Describe in words
what the FSM does.
CLK CLK
X
Q
Figure 3.68 FSM schematic
Chapter
Exercises 161
Exercise 3.29 Repeat Exercise 3.28 for the FSM shown in Figure 3.69. Recall
that the r and s register inputs indicate set and reset, respectively.
Exercise 3.30 Ben Bitdiddle has designed the circuit in Figure 3.70 to compute
a registered four-input XOR function. Each two-input XOR gate has a propagation delay of 100 ps and a contamination delay of 55 ps. Each flip-flop has a
setup time of 60 ps, a hold time of 20 ps, a clock-to-Q maximum delay of
70 ps, and a clock-to-Q minimum delay of 50 ps.
(a) If there is no clock skew, what is the maximum operating frequency of the
circuit?
(b) How much clock skew can the circuit tolerate if it must operate at 2 GHz?
(c) How much clock skew can the circuit tolerate before it might experience
a hold time violation?
(d) Alyssa P. Hacker points out that she can redesign the combinational logic
between the registers to be faster and tolerate more clock skew. Her
improved circuit also uses three two-input XORs, but they are arranged
differently. What is her circuit? What is its maximum frequency if there is
no clock skew? How much clock skew can the circuit tolerate before it
might experience a hold time violation?
CLK
A
CLK
CLK
Q
reset
r
r
s
Figure 3.69 FSM schematic
CLK
CLK
Figure 3.70 Registered four-input XOR circuit
Exercise 3.31 You are designing an adder for the blindingly fast 2-bit
RePentium Processor. The adder is built from two full adders such that the
carry out of the first adder is the carry in to the second adder, as shown in
Figure 3.71. Your adder has input and output registers and must complete the

addition in one clock cycle. Each full adder has the following propagation
delays: 20 ps from Cin to Cout or to Sum (S), 25 ps from A or B to Cout, and 30
ps from A or B to S. The adder has a contamination delay of 15 ps from Cin to
either output and 22 ps from A or B to either output. Each flip-flop has a setup
time of 30 ps, a hold time of 10 ps, a clock-to-Q propagation delay of 35 ps,
and a clock-to-Q contamination delay of 21 ps.
(a) If there is no clock skew, what is the maximum operating frequency of the
circuit?
(b) How much clock skew can the circuit tolerate if it must operate at 8 GHz?
(c) How much clock skew can the circuit tolerate before it might experience a
hold time violation?
Exercise 3.32 A field programmable gate array (FPGA) uses configurable logic
blocks (CLBs) rather than logic gates to implement combinational logic. The
Xilinx Spartan 3 FPGA has propagation and contamination delays of 0.61 and
0.30 ns, respectively for each CLB. It also contains flip-flops with propagation
and contamination delays of 0.72 and 0.50 ns, and setup and hold times of 0.53
and 0 ns, respectively.
(a) If you are building a system that needs to run at 40 MHz, how many
consecutive CLBs can you use between two flip-flops? Assume there is no
clock skew and no delay through wires between CLBs.
(b) Suppose that all paths between flip-flops pass through at least one CLB.
How much clock skew can the FPGA have without violating the hold time?
Exercise 3.33 A synchronizer is built from a pair of flip-flops with tsetup  50 ps,
T0  20 ps, and   30 ps. It samples an asynchronous input that changes 108
times per second. What is the minimum clock period of the synchronizer to
achieve a mean time between failures (MTBF) of 100 years?
162 CHAPTER THREE Sequential Logic Design
A
B
Cin
Cout
S
A
B
Cin
Cout
S
CLK
C
A0
B0
A1
B1
S0
S1
CLK
Figure 3.71 2-bit adder schematic
Cha
Exercises 163
Exercise 3.34 You would like to build a synchronizer that can receive
asynchronous inputs with an MTBF of 50 years. Your system is running at
1 GHz, and you use sampling flip-flops with   100 ps, T0  110 ps, and
tsetup  70 ps. The synchronizer receives a new asynchronous input on
average 0.5 times per second (i.e., once every 2 seconds). What is the
required probability of failure to satisfy this MTBF? How many clock cycles
would you have to wait before reading the sampled input signal to give that
probability of error?
Exercise 3.35 You are walking down the hallway when you run into your
lab partner walking in the other direction. The two of you first step one
way and are still in each other’s way. Then you both step the other way
and are still in each other’s way. Then you both wait a bit, hoping the
other person will step aside. You can model this situation as a metastable
point and apply the same theory that has been applied to synchronizers
and flip-flops. Suppose you create a mathematical model for yourself and
your lab partner. You start the unfortunate encounter in the metastable
state. The probability that you remain in this state after t seconds is .
 indicates your response rate; today, your brain has been blurred by lack of
sleep and has   20 seconds.
(a) How long will it be until you have 99% certainty that you will have
resolved from metastability (i.e., figured out how to pass one another)?
(b) You are not only sleepy, but also ravenously hungry. In fact, you will
starve to death if you don’t get going to the cafeteria within 3 minutes.
What is the probability that your lab partner will have to drag you to
the morgue?
Exercise 3.36 You have built a synchronizer using flip-flops with T0  20 ps
and   30 ps. Your boss tells you that you need to increase the MTBF by a factor of 10. By how much do you need to increase the clock period?
Exercise 3.37 Ben Bitdiddle invents a new and improved synchronizer in
Figure 3.72 that he claims eliminates metastability in a single cycle. He
explains that the circuit in box M is an analog “metastability detector” that
produces a HIGH output if the input voltage is in the forbidden zone
between VIL and VIH. The metastability detector checks to determine
e t

CLK
D r
CLK
Q
M
D2
Figure 3.72 “New and improved” synchronizer
Chapter
164 CHAPTER THREE Sequential Logic Design
whether the first flip-flop has produced a metastable output on D2. If so,
it asynchronously resets the flip-flop to produce a good 0 at D2. The second
flip-flop then samples D2, always producing a valid logic level on Q.
Alyssa P. Hacker tells Ben that there must be a bug in the circuit, because
eliminating metastability is just as impossible as building a perpetual motion
machine. Who is right? Explain, showing Ben’s error or showing why Alyssa
is wrong.

Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 3.1 Draw a state machine that can detect when it has received the serial
input sequence 01010.
Question 3.2 Design a serial (one bit at a time) two’s complementer FSM with
two inputs, Start and A, and one output, Q. A binary number of arbitrary length
is provided to input A, starting with the least significant bit. The corresponding
bit of the output appears at Q on the same cycle. Start is asserted for one cycle
to initialize the FSM before the least significant bit is provided.
Question 3.3 What is the difference between a latch and a flip-flop? Under what
circumstances is each one preferable?
Question 3.4 Design a 5-bit counter finite state machine.
Question 3.5 Design an edge detector circuit. The output should go HIGH for
one cycle after the input makes a 0 → 1 transition.
Question 3.6 Describe the concept of pipelining and why it is used.
Question 3.7 Describe what it means for a flip-flop to have a negative hold time.
Question 3.8 Given signal A, shown in Figure 3.73, design a circuit that
produces signal B.
Interview Questions 165
A
B
Figure 3.73 Signal waveforms
Question 3.9 Consider a block of logic between two registers. Explain the timing
constraints. If you add a buffer on the clock input of the receiver (the second
flip-flop), does the setup time constraint get better or worse?


4
4.1 Introduction
4.2 Combinational Logic
4.3 Structural Modeling
4.4 Sequential Logic
4.5 More Combinational Logic
4.6 Finite State Machines
4.7 Parameterized Modules*
4.8 Testbenches
4.9 Summary
Exercises
Interview Questions
Hardware Description Languages
4.1 INTRODUCTION
Thus far, we have focused on designing combinational and sequential
digital circuits at the schematic level. The process of finding an efficient
set of logic gates to perform a given function is labor intensive and error
prone, requiring manual simplification of truth tables or Boolean equations and manual translation of finite state machines (FSMs) into gates.
In the 1990’s, designers discovered that they were far more productive if
they worked at a higher level of abstraction, specifying just the logical
function and allowing a computer-aided design (CAD) tool to produce
the optimized gates. The specifications are generally given in a hardware
description language (HDL). The two leading hardware description languages are Verilog and VHDL.
Verilog and VHDL are built on similar principles but have different
syntax. Discussion of these languages in this chapter is divided into two
columns for literal side-by-side comparison, with Verilog on the left and
VHDL on the right. When you read the chapter for the first time, focus
on one language or the other. Once you know one, you’ll quickly master
the other if you need it.
Subsequent chapters show hardware in both schematic and HDL
form. If you choose to skip this chapter and not learn one of the HDLs,
you will still be able to master the principles of computer organization
from the schematics. However, the vast majority of commercial systems
are now built using HDLs rather than schematics. If you expect to do
digital design at any point in your professional life, we urge you to learn
one of the HDLs.
4.1.1 Modules
A block of hardware with inputs and outputs is called a module. An AND
gate, a multiplexer, and a priority circuit are all examples of hardware
modules. The two general styles for describing module functionality are
167

behavioral and structural. Behavioral models describe what a module
does. Structural models describe how a module is built from simpler
pieces; it is an application of hierarchy. The Verilog and VHDL code in
HDL Example 4.1 illustrate behavioral descriptions of a module that
computes the Boolean function from Example 2.6,
In both languages, the module is named sillyfunction and has three
inputs, a, b, and c, and one output, y.
y  abcabc  abc.
168 CHAPTER FOUR Hardware Description Languages
Verilog
module sillyfunction (input a, b, c,
output y);
assign y  ~a & ~b & ~c |
a & ~b & ~c |
a & ~b & c;
endmodule
A Verilog module begins with the module name and a listing
of the inputs and outputs. The assign statement describes
combinational logic. ~ indicates NOT, & indicates AND, and
| indicates OR.
Verilog signals such as the inputs and outputs are
Boolean variables (0 or 1). They may also have floating and
undefined values, as discussed in Section 4.2.8.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity sillyfunction is
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of sillyfunction is
begin
y  ((not a) and (not b) and (not c)) or
(a and (not b) and (not c)) or
(a and (not b) and c);
end;
VHDL code has three parts: the library use clause, the entity
declaration, and the architecture body. The library use clause
is required and will be discussed in Section 4.2.11. The entity
declaration lists the module name and its inputs and outputs.
The architecture body defines what the module does.
VHDL signals, such as inputs and outputs, must have a
type declaration. Digital signals should be declared to be
STD_LOGIC type. STD_LOGIC signals can have a value of ‘0’ or
‘1’, as well as floating and undefined values that will be
described in Section 4.2.8. The STD_LOGIC type is defined in
the IEEE.STD_LOGIC_1164 library, which is why the library
must be used.
VHDL lacks a good default order of operations, so
Boolean equations should be parenthesized.
HDL Example 4.1 COMBINATIONAL LOGIC
A module, as you might expect, is a good application of modularity.
It has a well defined interface, consisting of its inputs and outputs, and it
performs a specific function. The particular way in which it is coded is
unimportant to others that might use the module, as long as it performs
its function.
4.1.2 Language Origins
Universities are almost evenly split on which of these languages is taught
in a first course, and industry is similarly split on which language is preferred. Compared to Verilog, VHDL is more verbose and cumbersome,
Chapt
as you might expect of a language developed by committee. U.S. military
contractors, the European Space Agency, and telecommunications companies use VHDL extensively.
Both languages are fully capable of describing any hardware system,
and both have their quirks. The best language to use is the one that is
already being used at your site or the one that your customers demand.
Most CAD tools today allow the two languages to be mixed, so that different modules can be described in different languages.
4.1.3 Simulation and Synthesis
The two major purposes of HDLs are logic simulation and synthesis.
During simulation, inputs are applied to a module, and the outputs are
checked to verify that the module operates correctly. During synthesis,
the textual description of a module is transformed into logic gates.
Simulation
Humans routinely make mistakes. Such errors in hardware designs are
called bugs. Eliminating the bugs from a digital system is obviously important, especially when customers are paying money and lives depend on the
correct operation. Testing a system in the laboratory is time-consuming.
Discovering the cause of errors in the lab can be extremely difficult,
because only signals routed to the chip pins can be observed. There is no
way to directly observe what is happening inside a chip. Correcting errors
after the system is built can be devastatingly expensive. For example,
correcting a mistake in a cutting-edge integrated circuit costs more than
a million dollars and takes several months. Intel’s infamous FDIV (floating
point division) bug in the Pentium processor forced the company to recall
chips after they had shipped, at a total cost of $475 million. Logic simulation is essential to test a system before it is built.
4.1 Introduction 169
The term “bug” predates the
invention of the computer.
Thomas Edison called the “little faults and difficulties” with
his inventions “bugs” in 1878.
The first real computer bug
was a moth, which got caught
between the relays of the
Harvard Mark II electromechanical computer in 1947.
It was found by Grace Hopper,
who logged the incident, along
with the moth itself and the
comment “first actual case of
bug being found.”
Source: Notebook entry courtesy Naval Historical Center,
US Navy; photo No. NII
96566-KN
1 The Institute of Electrical and Electronics Engineers (IEEE) is a professional society
responsible for many computing standards including WiFi (802.11), Ethernet (802.3),
and floating-point numbers (754) (see Chapter 5).
VHDL
VHDL is an acronym for the VHSIC Hardware Description
Language. VHSIC is in turn an acronym for the Very High
Speed Integrated Circuits program of the US Department of
Defense.
VHDL was originally developed in 1981 by the Department of Defense to describe the structure and function of
hardware. Its roots draw from the Ada programming language. The IEEE standardized it in 1987 (IEEE STD 1076)
and has updated the standard several times since. The language was first envisioned for documentation but was
quickly adopted for simulation and synthesis.
Verilog
Verilog was developed by Gateway Design Automation as a
proprietary language for logic simulation in 1984. Gateway
was acquired by Cadence in 1989 and Verilog was made an
open standard in 1990 under the control of Open Verilog
International. The language became an IEEE standard1 in
1995 (IEEE STD 1364) and was updated in 2001.

Figure 4.1 shows waveforms from a simulation2 of the previous
sillyfunction module demonstrating that the module works correctly.
y is TRUE when a, b, and c are 000, 100, or 101, as specified by the
Boolean equation.
Synthesis
Logic synthesis transforms HDL code into a netlist describing the hardware (e.g., the logic gates and the wires connecting them). The logic synthesizer might perform optimizations to reduce the amount of hardware
required. The netlist may be a text file, or it may be drawn as a
schematic to help visualize the circuit. Figure 4.2 shows the results of
synthesizing the sillyfunction module.3 Notice how the three threeinput AND gates are simplified into two two-input AND gates, as we
discovered in Example 2.6 using Boolean algebra.
Circuit descriptions in HDL resemble code in a programming
language. However, you must remember that the code is intended to
represent hardware. Verilog and VHDL are rich languages with many
commands. Not all of these commands can be synthesized into hardware. For example, a command to print results on the screen during simulation does not translate into hardware. Because our primary interest is
170 CHAPTER FOUR Hardware Description Languages
0 ns
0
a 0
Now:
800 ns
b
c
y 0
0
160 320 ns 480 640 ns 800
Figure 4.1 Simulation waveforms
un5_y
un8_y
y
c y
b
a
Figure 4.2 Synthesized circuit
2 The simulation was performed with the Xilinx ISE Simulator, which is part of the Xilinx
ISE 8.2 software. The simulator was selected because it is used commercially, yet is freely
available to universities.
3 Synthesis was performed with Synplify Pro from Synplicity. The tool was selected
because it is the leading commercial tool for synthesizing HDL to field-programmable
gate arrays (see Section 5.6.2) and because it is available inexpensively for universities.

to build hardware, we will emphasize a synthesizable subset of the languages. Specifically, we will divide HDL code into synthesizable modules
and a testbench. The synthesizable modules describe the hardware. The
testbench contains code to apply inputs to a module, check whether the
output results are correct, and print discrepancies between expected and
actual outputs. Testbench code is intended only for simulation and
cannot be synthesized.
One of the most common mistakes for beginners is to think of HDL
as a computer program rather than as a shorthand for describing digital
hardware. If you don’t know approximately what hardware your HDL
should synthesize into, you probably won’t like what you get. You might
create far more hardware than is necessary, or you might write code that
simulates correctly but cannot be implemented in hardware. Instead,
think of your system in terms of blocks of combinational logic, registers,
and finite state machines. Sketch these blocks on paper and show how
they are connected before you start writing code.
In our experience, the best way to learn an HDL is by example.
HDLs have specific ways of describing various classes of logic; these
ways are called idioms. This chapter will teach you how to write
the proper HDL idioms for each type of block and then how to put
the blocks together to produce a working system. When you need to
describe a particular kind of hardware, look for a similar example
and adapt it to your purpose. We do not attempt to rigorously
define all the syntax of the HDLs, because that is deathly boring and
because it tends to encourage thinking of HDLs as programming
languages, not shorthand for hardware. The IEEE Verilog and
VHDL specifications, and numerous dry but exhaustive textbooks,
contain all of the details, should you find yourself needing more information on a particular topic. (See Further Readings section at back of
the book.)
4.2 COMBINATIONAL LOGIC
Recall that we are disciplining ourselves to design synchronous sequential circuits, which consist of combinational logic and registers. The outputs of combinational logic depend only on the current inputs. This
section describes how to write behavioral models of combinational logic
with HDLs.
4.2.1 Bitwise Operators
Bitwise operators act on single-bit signals or on multi-bit busses. For
example, the inv module in HDL Example 4.2 describes four inverters
connected to 4-bit busses.
4.2 Combinational Logic 171

The endianness of a bus is purely arbitrary. (See the sidebar in Section
6.2.2 for the origin of the term.) Indeed, endianness is also irrelevant to
this example, because a bank of inverters doesn’t care what the order of
the bits are. Endianness matters only for operators, such as addition,
where the sum of one column carries over into the next. Either ordering is
acceptable, as long as it is used consistently. We will consistently use the
little-endian order, [N 1:0] in Verilog and (N 1 downto 0) in VHDL,
for an N-bit bus.
After each code example in this chapter is a schematic produced
from the Verilog code by the Synplify Pro synthesis tool. Figure 4.3
shows that the inv module synthesizes to a bank of four inverters,
indicated by the inverter symbol labeled y[3:0]. The bank of inverters
connects to 4-bit input and output busses. Similar hardware is produced
from the synthesized VHDL code.
The gates module in HDL Example 4.3 demonstrates bitwise operations acting on 4-bit busses for other basic logic functions.
172 CHAPTER FOUR Hardware Description Languages
y[3:0]
a[3:0] y[3:0] [3:0] [3:0]
Figure 4.3 inv synthesized circuit
Verilog
module inv (input [3:0] a,
output [3:0] y);
assign y  ~a;
endmodule
a[3:0] represents a 4-bit bus. The bits, from most significant
to least significant, are a[3], a[2], a[1], and a[0]. This is
called little-endian order, because the least significant bit has
the smallest bit number. We could have named the bus
a[4:1], in which case a[4] would have been the most significant. Or we could have used a[0:3], in which case the bits,
from most significant to least significant, would be a[0],
a[1], a[2], and a[3]. This is called big-endian order.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity inv is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of inv is
begin
y  not a;
end;
VHDL uses STD_LOGIC_VECTOR, to indicate busses of
STD_LOGIC. STD_LOGIC_VECTOR (3 downto 0) represents a 4-bit
bus. The bits, from most significant to least significant, are
3, 2, 1, and 0. This is called little-endian order, because the
least significant bit has the smallest bit number. We could
have declared the bus to be STD_LOGIC_VECTOR (4 downto 1),
in which case bit 4 would have been the most significant. Or
we could have written STD_LOGIC_VECTOR (0 to 3), in which
case the bits, from most significant to least significant, would
be 0, 1, 2, and 3. This is called big-endian order.
HDL Example 4.2 INVERTERS
Ch
4.2 Combinational Logic 173
y1[3:0]
y2[3:0]
y3[3:0]
y4[3:0]
y5[3:0]
y5[3:0] [3:0]
y4[3:0] [3:0]
y3[3:0] [3:0]
y2[3:0] [3:0]
y1[3:0] [3:0]
b[3:0] [3:0] a[3:0] [3:0] [3:0] [3:0] [3:0]
[3:0] [3:0] [3:0]
[3:0]
[3:0]
Figure 4.4 gates synthesized circuit
Verilog
module gates (input [3:0] a, b,
output [3:0] y1, y2,
y3, y4, y5);
/* Five different two-input logic
gates acting on 4 bit busses */
assign y1  a & b; // AND
assign y2  a | b; // OR
assign y3  a  b; // XOR
assign y4  ~(a & b); // NAND
assign y5  ~(a | b); // NOR
endmodule
~, , and | are examples of Verilog operators, whereas a, b,
and y1 are operands. A combination of operators and
operands, such as a & b, or ~(a | b), is called an expression.
A complete command such as assign y4  ~(a & b); is
called a statement.
assign out  in1 op in2; is called a continuous assignment statement. Continuous assignment statements end with a
semicolon. Anytime the inputs on the right side of the  in a
continuous assignment statement change, the output on the left
side is recomputed. Thus, continuous assignment statements
describe combinational logic.
HDL Example 4.3 LOGIC GATES
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity gates is
port (a, b: in STD_LOGIC_VECTOR (3 downto 0);
y1, y2, y3, y4,
y5: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of gates is
begin
— — Five different two-input logic gates
— —acting on 4 bit busses
y1  a and b;
y2  a or b;
y3  a xor b;
y4  a nand b;
y5  a nor b;
end;
not, xor, and or are examples of VHDL operators, whereas a,
b, and y1 are operands. A combination of operators and
operands, such as a and b, or a nor b, is called an expression.
A complete command such as y4  a nand b; is called a
statement.
out  in1 op in2; is called a concurrent signal assignment statement. VHDL assignment statements end with a
semicolon. Anytime the inputs on the right side of the  in
a concurrent signal assignment statement change, the output
on the left side is recomputed. Thus, concurrent signal
assignment statements describe combinational logic.
Chapter 04.qxd 1/3
4.2.3 Reduction Operators
Reduction operators imply a multiple-input gate acting on a single bus.
HDL Example 4.4 describes an eight-input AND gate with inputs a7,
a6, . . . , a0.
174 CHAPTER FOUR Hardware Description Languages
Verilog
Verilog comments are just like those in C or Java. Comments
beginning with /* continue, possibly across multiple lines, to
the next */. Comments beginning with // continue to the end
of the line.
Verilog is case-sensitive. y1 and Y1 are different signals
in Verilog.
VHDL
VHDL comments begin with — — and continue to the end of the
line. Comments spanning multiple lines must use — — at the
beginning of each line.
VHDL is not case-sensitive. y1 and Y1 are the same signal in VHDL. However, other tools that may read your file
might be case sensitive, leading to nasty bugs if you blithely
mix upper and lower case.
4.2.2 Comments and White Space
The gates example showed how to format comments. Verilog and
VHDL are not picky about the use of white space (i.e., spaces, tabs, and
line breaks). Nevertheless, proper indenting and use of blank lines is
helpful to make nontrivial designs readable. Be consistent in your use of
capitalization and underscores in signal and module names. Module and
signal names must not begin with a digit.
Verilog
module and8 (input [7:0] a,
output y);
assign y  &a;
// &a is much easier to write than
// assign y  a[7] & a[6] & a[5] & a[4] &
// a[3] & a[2] & a[1] & a[0];
endmodule
As one would expect, |, , ~&, and ~ | reduction operators
are available for OR, XOR, NAND, and NOR as well.
Recall that a multi-input XOR performs parity, returning
TRUE if an odd number of inputs are TRUE.
VHDL
VHDL does not have reduction operators. Instead, it provides the generate command (see Section 4.7). Alternatively,
the operation can be written explicitly, as shown below.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity and8 is
port (a: in STD_LOGIC_VECTOR (7 downto 0);
y: out STD_LOGIC);
end;
architecture synth of and8 is
begin
y  a(7) and a(6) and a(5) and a(4) and
a(3) and a(2) and a(1) and a(0);
end;
HDL Example 4.4 EIGHT-INPUT AND
Chap
4.2.4 Conditional Assignment
Conditional assignments select the output from among alternatives
based on an input called the condition. HDL Example 4.5 illustrates a
2:1 multiplexer using conditional assignment.
4.2 Combinational Logic 175
y
y
a[7:0] [7:0]
[0]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
Figure 4.5 and8 synthesized circuit
Verilog
The conditional operator ?: chooses, based on a first expression, between a second and third expression. The first
expression is called the condition. If the condition is 1, the
operator chooses the second expression. If the condition is 0,
the operator chooses the third expression.
?: is especially useful for describing a multiplexer because,
based on the first input, it selects between two others. The following code demonstrates the idiom for a 2:1 multiplexer with
4-bit inputs and outputs using the conditional operator.
module mux2 (input [3:0] d0, d1,
input s,
output [3:0] y);
assign y  s ? d1 : d0;
endmodule
If s is 1, then y  d1. If s is 0, then y  d0.
?: is also called a ternary operator, because it takes three
inputs. It is used for the same purpose in the C and Java
programming languages.
VHDL
Conditional signal assignments perform different operations
depending on some condition. They are especially useful for
describing a multiplexer. For example, a 2:1 multiplexer can
use conditional signal assignment to select one of two 4-bit
inputs.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is
port (d0, d1: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of mux2 is
begin
y  d0 when s  ‘0’ else d1;
end;
The conditional signal assignment sets y to d0 if s is 0.
Otherwise it sets y to d1.
HDL Example 4.5 2:1 MULTIPLEXER
y[3:0]
0
1
y[3:0]
s
d1[3:0]
d0[3:0] [3:0]
[3:0]
[3:0]
Figure 4.6 mux2 synthesized circuit
Chapt
HDL Example 4.6 shows a 4:1 multiplexer based on the same
principle as the 2:1 multiplexer in HDL Example 4.5.
Figure 4.7 shows the schematic for the 4:1 multiplexer produced by
Synplify Pro. The software uses a different multiplexer symbol than this
text has shown so far. The multiplexer has multiple data (d) and one-hot
enable (e) inputs. When one of the enables is asserted, the associated
data is passed to the output. For example, when s[1]  s[0]  0, the
bottom AND gate, un1_s_5, produces a 1, enabling the bottom input of
the multiplexer and causing it to select d0[3:0].
4.2.5 Internal Variables
Often it is convenient to break a complex function into intermediate
steps. For example, a full adder, which will be described in Section 5.2.1,
176 CHAPTER FOUR Hardware Description Languages
Verilog
A 4:1 multiplexer can select one of four inputs using nested
conditional operators.
module mux4 (input [3:0] d0, d1, d2, d3,
input [1:0] s,
output [3:0] y);
assign y  s[1] ? (s[0] ? d3 : d2)
: (s[0] ? d1 : d0);
endmodule
If s[1] is 1, then the multiplexer chooses the first expression,
(s[0] ? d3 : d2). This expression in turn chooses either d3
or d2 based on s[0] (y  d3 if s[0] is 1 and d2 if s[0] is 0).
If s[1] is 0, then the multiplexer similarly chooses the second
expression, which gives either d1 or d0 based on s[0].
VHDL
A 4:1 multiplexer can select one of four inputs using multiple
else clauses in the conditional signal assignment.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux4 is
port (d0, d1,
d2, d3: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC_VECTOR (1 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synthl of mux4 is
begin
y  d0 when s  “00” else
d1 when s  “01” else
d2 when s  “10” else
d3;
end;
VHDL also supports selected signal assignment statements to
provide a shorthand when selecting from one of several
possibilities. This is analogous to using a case statement in
place of multiple if/else statements in some programming
languages. The 4:1 multiplexer can be rewritten with selected
signal assignment as follows:
architecture synth2 of mux4 is
begin
with a select y 
d0 when “00”,
d1 when “01”,
d2 when “10”,
d3 when others;
end;
HDL Example 4.6 4:1 MULTIPLEXER
Chapter 0
is a circuit with three inputs and two outputs defined by the following
equations:
(4.1)
If we define intermediate signals, P and G,
(4.2)
we can rewrite the full adder as follows:
(4.3)
P and G are called internal variables, because they are neither inputs nor
outputs but are used only internal to the module. They are similar to
local variables in programming languages. HDL Example 4.7 shows
how they are used in HDLs.
HDL assignment statements (assign in Verilog and  in VHDL)
take place concurrently. This is different from conventional programming languages such as C or Java, in which statements are evaluated in
the order in which they are written. In a conventional language, it is
CoutG  PCin
S P⊕ Cin
GAB
PA⊕B
Cout AB  ACin  BCin
S A ⊕ B⊕Cin
4.2 Combinational Logic 177
un1_s_2
un1_s_3
un1_s_4
un1_s_5
y[3:0]
e
d
e
d
e
d
e
d
y[3:0]
s[1:0] [1:0]
d3[3:0]
d2[3:0]
d1[3:0]
d0[3:0]
[0]
[1]
[1]
[0]
[0]
[1]
[0]
[1]
[3:0]
[3:0] [3:0]
[3:0]
[3:0]
Figure 4.7 mux4 synthesized
circuit
Check this by filling out the
truth table to convince
yourself it is correct.
Chapter 04
important that S  P  Cin comes after P  A  B, because statements are executed sequentially. In an HDL, the order does not matter.
Like hardware, HDL assignment statements are evaluated any time the
inputs, signals on the right hand side, change their value, regardless of
the order in which the assignment statements appear in a module.
4.2.6 Precedence
Notice that we parenthesized the cout computation in HDL Example 4.7
to define the order of operations as Cout  G  (P  Cin), rather than Cout
 (G  P)  Cin. If we had not used parentheses, the default operation order
is defined by the language. HDL Example 4.8 specifies operator precedence
from highest to lowest for each language. The tables include arithmetic,
shift, and comparison operators that will be defined in Chapter 5.
178 CHAPTER FOUR Hardware Description Languages
p
g s
un1_cout cout
cout
s
cin
b
a
Figure 4.8 fulladder synthesized circuit
Verilog
In Verilog, wires are used to represent internal variables
whose values are defined by assign statements such as
assign p  a  b; Wires technically have to be declared
only for multibit busses, but it is good practice to include
them for all internal variables; their declaration could have
been omitted in this example.
module fulladder(input a, b, cin,
output s, cout);
wire p, g;
assign p  a  b;
assign g  a & b;
assign s  p  cin;
assign cout  g | (p & cin);
endmodule
VHDL
In VHDL, signals are used to represent internal variables
whose values are defined by concurrent signal assignment
statements such as p  a xor b;
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fulladder is
port(a, b, cin: in STD_LOGIC;
s, cout: out STD_LOGIC);
end;
architecture synth of fulladder is
signal p, g: STD_LOGIC;
begin
p  a xor b;
g  a and b;
s  p xor cin;
cout  g or (p and cin);
end;
HDL Example 4.7 FULL ADDER
Chapter 04.qxd 1/31/0
4.2 Combinational Logic 179
Verilog VHDL
HDL Example 4.8 OPERATOR PRECEDENCE
Table 4.2 VHDL operator precedence
Op Meaning
not NOT
*, /, mod, rem MUL, DIV, MOD, REM
, , PLUS, MINUS,
& CONCATENATE
rol, ror, Rotate,
srl, sll, Shift logical,
sra, sla Shift arithmetic
, /, , Comparison
, , 
and, or, nand, Logical Operations
nor, xor
H
i
g
h
e
s
t
L
o
w
e
s
t
The operator precedence for Verilog is much like you would
expect in other programming languages. In particular, AND
has precedence over OR. We could take advantage of this
precedence to eliminate the parentheses.
assign cout  g | p & cin;
Multiplication has precedence over addition in VHDL, as
you would expect. However, unlike Verilog, all of the logical
operations (and, or, etc.) have equal precedence, unlike what
one might expect in Boolean algebra. Thus, parentheses are
necessary; otherwise cout  g or p and cin would be
interpreted from left to right as cout  (g or p) and cin.
4.2.7 Numbers
Numbers can be specified in a variety of bases. Underscores in numbers
are ignored and can be helpful in breaking long numbers into more readable chunks. HDL Example 4.9 explains how numbers are written in
each language.
4.2.8 Z’s and X’s
HDLs use z to indicate a floating value. z is particularly useful for describing a tristate buffer, whose output floats when the enable is 0. Recall from
Section 2.6 that a bus can be driven by several tristate buffers, exactly one
of which should be enabled. HDL Example 4.10 shows the idiom for a
tristate buffer. If the buffer is enabled, the output is the same as the input.
If the buffer is disabled, the output is assigned a floating value (z).
Table 4.1 Verilog operator precedence
Op Meaning
~ NOT
*, /, % MUL, DIV, MOD
,  PLUS, MINUS
,  Logical Left/Right Shift
,  Arithmetic Left/Right Shift
, , ,  Relative Comparison
, ! Equality Comparison
&, ~& AND, NAND
, ~ XOR, XNOR
|, ~| OR, NOR
?: Conditional
H
i
g
h
e
s
t
L
o
w
e
s
t
Chapter 04.qxd 1
180 CHAPTER FOUR Hardware Description Languages
Verilog
Verilog numbers can specify their base and size (the number
of bits used to represent them). The format for declaring
constants is NBvalue, where N is the size in bits, B is the base,
and value gives the value. For example 9h25 indicates a
9-bit number with a value of 2516  3710  0001001012.
Verilog supports b for binary (base 2), o for octal (base 8),
d for decimal (base 10), and h for hexadecimal (base 16). If
the base is omitted, the base defaults to decimal.
If the size is not given, the number is assumed to have as
many bits as the expression in which it is being used. Zeros
are automatically padded on the front of the number to
bring it up to full size. For example, if w is a 6-bit bus, assign
w  b11 gives w the value 000011. It is better practice to
explicitly give the size.
VHDL
In VHDL, STD_LOGIC numbers are written in binary and
enclosed in single quotes: ‘0’ and ‘1’ indicate logic 0 and 1.
STD_LOGIC_VECTOR numbers are written in binary
or hexadecimal and enclosed in double quotation marks. The
base is binary by default and can be explicitly defined with
the prefix X for hexadecimal or B for binary.
HDL Example 4.9 NUMBERS
Table 4.3 Verilog numbers
Numbers Bits Base Val Stored
3b101 3 2 5 101
b11 ? 2 3 000 ... 0011
8b11 8 2 3 00000011
8b1010_1011 8 2 171 10101011
3d6 3 10 6 110
6o42 6 8 34 100010
8hAB 8 16 171 10101011
42 ? 10 42 00 ... 0101010
Table 4.4 VHDL numbers
Numbers Bits Base Val Stored
“101” 3 2 5 101
B“101” 3 2 5 101
X“AB” 8 16 161 10101011
Verilog
module tristate (input [3:0] a,
input en,
output [3:0] y);
assign y  en ? a : 4bz;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity tristate is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
en: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of tristate is
begin
y  “ZZZZ” when en  ‘0’ else a;
end;
HDL Example 4.10 TRISTATE BUFFER
Chapte
4.2 Combinational Logic 181
y_1[3:0]
y[3:0]
en
a[3:0] [3:0] [3:0]
Figure 4.9 tristate synthesized circuit
Similarly, HDLs use x to indicate an invalid logic level. If a bus is
simultaneously driven to 0 and 1 by two enabled tristate buffers (or other
gates), the result is x, indicating contention. If all the tristate buffers driving a bus are simultaneously OFF, the bus will float, indicated by z.
At the start of simulation, state nodes such as flip-flop outputs are
initialized to an unknown state (x in Verilog and u in VHDL). This is
helpful to track errors caused by forgetting to reset a flip-flop before its
output is used.
If a gate receives a floating input, it may produce an x output when
it can’t determine the correct output value. Similarly, if it receives an
illegal or uninitialized input, it may produce an x output. HDL
Example 4.11 shows how Verilog and VHDL combine these different
signal values in logic gates.
Verilog
Verilog signal values are 0, 1, z, and x. Verilog constants
starting with z or x are padded with leading z’s or x’s (instead
of 0’s) to reach their full length when necessary.
Table 4.5 shows a truth table for an AND gate using all
four possible signal values. Note that the gate can sometimes
determine the output despite some inputs being unknown.
For example 0&z returns 0 because the output of an AND
gate is always 0 if either input is 0. Otherwise, floating or
invalid inputs cause invalid outputs, displayed as x in Verilog.
VHDL
VHDL STD_LOGIC signals are ‘0’, ‘1’, ‘z’, ‘x’, and ‘u’.
Table 4.6 shows a truth table for an AND gate using all
five possible signal values. Notice that the gate can sometimes determine the output despite some inputs being
unknown. For example, ‘0’ and ‘z’ returns ‘0’ because the
output of an AND gate is always ‘0’ if either input is ‘0.’
Otherwise, floating or invalid inputs cause invalid outputs,
displayed as ‘x’ in VHDL. Uninitialized inputs cause uninitialized outputs, displayed as ‘u’ in VHDL.
HDL Example 4.11 TRUTH TABLES WITH UNDEFINED AND FLOATING INPUTS
Table 4.5 Verilog AND gate truth table with z and x
& A
0 1zx
0 0 0 00
B 1 0 1 xx
z 0 x xx
x 0 x xx
Table 4.6 VHDL AND gate truth table with z, x, and u
AND A
01zxu
0 00000
1 01xxu
B z 0xxxu
x 0xxxu
u 0uuuu

Seeing x or u values in simulation is almost always an indication of a
bug or bad coding practice. In the synthesized circuit, this corresponds
to a floating gate input, uninitialized state, or contention. The x or u
may be interpreted randomly by the circuit as 0 or 1, leading to unpredictable behavior.
4.2.9 Bit Swizzling
Often it is necessary to operate on a subset of a bus or to concatenate
(join together) signals to form busses. These operations are collectively
known as bit swizzling. In HDL Example 4.12, y is given the 9-bit value
c2c1d0d0d0c0101 using bit swizzling operations.
4.2.10 Delays
HDL statements may be associated with delays specified in arbitrary
units. They are helpful during simulation to predict how fast a circuit
will work (if you specify meaningful delays) and also for debugging
purposes to understand cause and effect (deducing the source of a bad
output is tricky if all signals change simultaneously in the simulation
results). These delays are ignored during synthesis; the delay of a gate
produced by the synthesizer depends on its tpd and tcd specifications, not
on numbers in HDL code.
HDL Example 4.13 adds delays to the original function from HDL
Example 4.1, It assumes that inverters have a delay
of 1 ns, three-input AND gates have a delay of 2 ns, and three-input OR
gates have a delay of 4 ns. Figure 4.10 shows the simulation waveforms,
with y lagging 7 ns after the inputs. Note that y is initially unknown at
the beginning of the simulation.
y  abcabc  abc.
182 CHAPTER FOUR Hardware Description Languages
Verilog
assign y  {c[2:1], {3{d[0]}}, c[0], 3’b101};
The {} operator is used to concatenate busses. {3{d[0]}}
indicates three copies of d[0].
Don’t confuse the 3-bit binary constant 3’b101 with a bus
named b. Note that it was critical to specify the length of 3
bits in the constant; otherwise, it would have had an unknown
number of leading zeros that might appear in the middle of y.
If y were wider than 9 bits, zeros would be placed in the
most significant bits.
VHDL
y  c(2 downto 1) & d(0) & d(0) & d(0) &
c(0) & “101”;
The & operator is used to concatenate busses. y must be a
9-bit STD_LOGIC_VECTOR. Do not confuse & with the and operator in VHDL.
HDL Example 4.12 BIT SWIZZLING
Chapt
4.2 Combinational Logic 183
4.2.11 VHDL Libraries and Types*
(This section may be skipped by Verilog users.) Unlike Verilog, VHDL
enforces a strict data typing system that can protect the user from some
errors but that is also clumsy at times.
Despite its fundamental importance, the STD_LOGIC type is not built
into VHDL. Instead, it is part of the IEEE.STD_LOGIC_1164 library.
Thus, every file must contain the library statements shown in the previous examples.
Verilog
‘timescale 1ns/1ps
module example (input a, b, c,
output y);
wire ab, bb, cb, n1, n2, n3;
assign #1 {ab, bb, cb}  ~ {a, b, c};
assign #2 n1  ab & bb & cb;
assign #2 n2  a & bb & cb;
assign #2 n3  a & bb & c;
assign #4 y  n1 | n2 | n3;
endmodule
Verilog files can include a timescale directive that indicates
the value of each time unit. The statement is of the form
‘timescale unit/precision. In this file, each unit is 1 ns, and
the simulation has 1 ps precision. If no timescale directive is
given in the file, a default unit and precision (usually 1 ns for
both) is used. In Verilog, a # symbol is used to indicate the
number of units of delay. It can be placed in assign statements, as well as non-blocking () and blocking () assignments, which will be discussed in Section 4.5.4.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity example is
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of example is
signal ab, bb, cb, n1, n2, n3: STD_LOGIC;
begin
ab  not a after 1 ns;
bb  not b after 1 ns;
cb  not c after 1 ns;
n1  ab and bb and cb after 2 ns;
n2  a and bb and cb after 2 ns;
n3  a and bb and c after 2 ns;
y  n1 or n2 or n3 after 4 ns;
end;
In VHDL, the after clause is used to indicate delay. The
units, in this case, are specified as nanoseconds.
HDL Example 4.13 LOGIC GATES WITH DELAYS
Figure 4.10 Example simulation waveforms with delays (from the ModelSim simulator)
Chapter 04.qxd
Moreover, IEEE.STD_LOGIC_1164 lacks basic operations such as
addition, comparison, shifts, and conversion to integers for the
STD_LOGIC_VECTOR data. Most CAD vendors have adopted yet more
libraries containing these functions: IEEE.STD_LOGIC_UNSIGNED and
IEEE.STD_LOGIC_SIGNED. See Section 1.4 for a discussion of unsigned
and signed numbers and examples of these operations.
VHDL also has a BOOLEAN type with two values: true and false.
BOOLEAN values are returned by comparisons (such as the equality comparison, s  ‘0’) and are used in conditional statements such as when.
Despite the temptation to believe a BOOLEAN true value should be
equivalent to a STD_LOGIC ‘1’ and BOOLEAN false should mean
STD_LOGIC ‘0’, these types are not interchangeable. Thus, the following
code is illegal:
y  d1 when s else d0;
q  (state  S2);
Instead, we must write
y  d1 when (s  ‘1’) else d0;
q  ‘1’ when (state  S2) else ‘0’;
Although we do not declare any signals to be BOOLEAN, they are
automatically implied by comparisons and used by conditional statements.
Similarly, VHDL has an INTEGER type that represents both positive
and negative integers. Signals of type INTEGER span at least the values
231 to 231  1. Integer values are used as indices of busses. For example, in the statement
y  a(3) and a(2) and a(1) and a(0);
0, 1, 2, and 3 are integers serving as an index to choose bits of the a
signal. We cannot directly index a bus with a STD_LOGIC or
STD_LOGIC_VECTOR signal. Instead, we must convert the signal to an
INTEGER. This is demonstrated in HDL Example 4.14 for an 8:1 multiplexer that selects one bit from a vector using a 3-bit index. The
CONV_INTEGER function is defined in the IEEE.STD_LOGIC_UNSIGNED
library and performs the conversion from STD_LOGIC_VECTOR to
INTEGER for positive (unsigned) values.
VHDL is also strict about out ports being exclusively for output.
For example, the following code for two and three-input AND gates is
illegal VHDL because v is an output and is also used to compute w.
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity and23 is
port(a, b, c: in STD_LOGIC;
v, w: out STD_LOGIC);
end;
184 CHAPTER FOUR Hardware Description Languages
Chapter 0
architecture synth of and23 is
begin
v  a and b;
w  v and c;
end;
VHDL defines a special port type, buffer, to solve this problem.
A signal connected to a buffer port behaves as an output but may also
be used within the module. The corrected entity definition follows.
Verilog does not have this limitation and does not require buffer ports.
entity and23 is
port(a, b, c: in STD_LOGIC;
v: buffer STD_LOGIC;
w: out STD_LOGIC);
end;
VHDL supports enumeration types as an abstract way of representing information without assigning specific binary encodings. For example, the divide-by-3 FSM described in Section 3.4.2 uses three states. We
can give the states names using the enumeration type rather than referring to them by binary values. This is powerful because it allows VHDL
to search for the best state encoding during synthesis, rather than
depending on an arbitrary encoding specified by the user.
type statetype is (S0, S1, S2);
signal state, nextstate: statetype;
4.3 STRUCTURAL MODELING
The previous section discussed behavioral modeling, describing a module
in terms of the relationships between inputs and outputs. This section
4.3 Structural Modeling 185
library IEEE;
use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
entity mux8 is
port(d: in STD_LOGIC_VECTOR(7 downto 0);
s: in STD_LOGIC_VECTOR(2 downto 0);
y: out STD_LOGIC);
end;
architecture synth of mux8 is
begin
y  d(CONV_INTEGER(s));
end;
HDL Example 4.14 8:1 MULTIPLEXER WITH TYPE CONVERSION
Figure follows on next page.
Cha
186 CHAPTER FOUR Hardware Description Languages
un1_s_3
un1_s_4
un1_s_5
un1_s_6
un1_s_7
un1_s_8
un1_s_9
un1_s_10
y
e
d
e
d
e
d
e
d
e
d
e
d
e
d
e
d
y
s[2:0] [2:0]
d[7:0] [7:0]
[0]
[1]
[2]
[0]
[1]
[2]
[0]
[1]
[2]
[1]
[0]
[2]
[0]
[1]
[2]
[2]
[0]
[1]
[0]
[2]
[1]
[1]
[2]
[0]
[0]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
Figure 4.11 mux8 synthesized
circuit

examines structural modeling, describing a module in terms of how it is
composed of simpler modules.
For example, HDL Example 4.15 shows how to assemble a 4:1
multiplexer from three 2:1 multiplexers. Each copy of the 2:1 multiplexer
4.3 Structural Modeling 187
v w
w
v
c
b
a Figure 4.12 and23 synthesized
circuit
Verilog
module mux4 (input [3:0] d0, d1, d2, d3,
input [1:0] s,
output [3:0] y);
wire [3:0] low, high;
mux2 lowmux (d0, d1, s[0], low);
mux2 highmux (d2, d3, s[0], high);
mux2 finalmux (low, high, s[1], y);
endmodule
The three mux2 instances are called lowmux, highmux, and
finalmux. The mux2 module must be defined elsewhere in the
Verilog code.
HDL Example 4.15 STRUCTURAL MODEL OF 4:1 MULTIPLEXER
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux4 is
port (d0, d1,
d2, d3: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC_VECTOR (1 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture struct of mux4 is
component mux2
port (d0,
d1: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
signal low, high: STD_LOGIC_VECTOR (3 downto 0);
begin
lowmux: mux2 port map (d0, d1, s(0), low);
highmux: mux2 port map (d2, d3, s(0), high);
finalmux: mux2 port map (low, high, s(1), y);
end;
The architecture must first declare the mux2 ports using the
component declaration statement. This allows VHDL tools to
check that the component you wish to use has the same ports
as the entity that was declared somewhere else in another
entity statement, preventing errors caused by changing the
entity but not the instance. However, component declaration
makes VHDL code rather cumbersome.
Note that this architecture of mux4 was named struct,
whereas architectures of modules with behavioral descriptions from Section 4.2 were named synth. VHDL allows
multiple architectures (implementations) for the same entity;
the architectures are distinguished by name. The names
themselves have no significance to the CAD tools, but struct
and synth are common. Synthesizable VHDL code generally
contains only one architecture for each entity, so we will not
discuss the VHDL syntax to configure which architecture is
used when multiple architectures are defined.

188 CHAPTER FOUR Hardware Description Languages
mux2
lowmux
mux2
highmux
mux2
finalmux
y[3:0]
s[1:0] [1:0]
d3[3:0]
d2[3:0]
d1[3:0]
d0[3:0]
[0] s
d0[3:0]
d1[3:0]
y[3:0]
[0] s [3:0] d0[3:0]
[3:0] d1[3:0]
y[3:0]
[1] s [3:0] d0[3:0]
[3:0] d1[3:0]
[3:0] y[3:0]
Figure 4.13 mux4 synthesized circuit
Verilog
module mux2 (input [3:0] d0, d1,
input s,
output [3:0] y);
tristate t0 (d0, ~s, y);
tristate t1 (d1, s, y);
endmodule
In Verilog, expressions such as ~s are permitted in the port
list for an instance. Arbitrarily complicated expressions are
legal but discouraged because they make the code difficult to
read.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is
port (d0, d1: in STD_LOGIC_VECTOR (3 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture struct of mux2 is
component tristate
port (a: in STD_LOGIC_VECTOR (3 downto 0);
en: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
signal sbar: STD_LOGIC;
begin
sbar  not s;
t0: tristate port map (d0, sbar, y);
t1: tristate port map (d1, s, y);
end;
In VHDL, expressions such as not s are not permitted in the
port map for an instance. Thus, sbar must be defined as a
separate signal.
HDL Example 4.16 STRUCTURAL MODEL OF 2:1 MULTIPLEXER
is called an instance. Multiple instances of the same module are distinguished by distinct names, in this case lowmux, highmux, and finalmux.
This is an example of regularity, in which the 2:1 multiplexer is reused
many times.
HDL Example 4.16 uses structural modeling to construct a 2:1
multiplexer from a pair of tristate buffers.
C
4.3 Structural Modeling 189
HDL Example 4.17 shows how modules can access part of a bus. An
8-bit wide 2:1 multiplexer is built using two of the 4-bit 2:1 multiplexers
already defined, operating on the low and high nibbles of the byte.
In general, complex systems are designed hierarchically. The overall
system is described structurally by instantiating its major components.
Each of these components is described structurally from its building
blocks, and so forth recursively until the pieces are simple enough to
describe behaviorally. It is good style to avoid (or at least to minimize)
mixing structural and behavioral descriptions within a single module.
tristate
t0
tristate
t1
y[3:0] s
d1[3:0]
d0[3:0]
en [3:0]
a[3:0]
[3:0]
y[3:0]
en [3:0]
a[3:0]
[3:0] y[3:0]
Figure 4.14 mux2 synthesized circuit
Verilog
module mux2_8 (input [7:0] d0, d1,
input s,
output [7:0] y);
mux2 lsbmux (d0[3:0], d1[3:0], s, y[3:0]);
mux2 msbmux (d0[7:4], d1[7:4], s, y[7:4]);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2_8 is
port (d0, d1: in STD_LOGIC_VECTOR (7 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (7 downto 0));
end;
architecture struct of mux2_8 is
component mux2
port (d0, d1: in STD_LOGIC_VECTOR(3
downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
begin
lsbmux: mux2
port map (d0 (3 downto 0), d1 (3 downto 0),
s, y (3 downto 0));
msbhmux: mux2
port map (d0 (7 downto 4), d1 (7 downto 4),
s, y (7 downto 4));
end;
HDL Example 4.17 ACCESSING PARTS OF BUSSES

4.4 SEQUENTIAL LOGIC
HDL synthesizers recognize certain idioms and turn them into specific
sequential circuits. Other coding styles may simulate correctly but synthesize into circuits with blatant or subtle errors. This section presents
the proper idioms to describe registers and latches.
4.4.1 Registers
The vast majority of modern commercial systems are built with registers
using positive edge-triggered D flip-flops. HDL Example 4.18 shows the
idiom for such flip-flops.
In Verilog always statements and VHDL process statements, signals keep their old value until an event in the sensitivity list takes place
that explicitly causes them to change. Hence, such code, with appropriate sensitivity lists, can be used to describe sequential circuits with memory. For example, the flip-flop includes only clk in the sensitive list. It
remembers its old value of q until the next rising edge of the clk, even if
d changes in the interim.
In contrast, Verilog continuous assignment statements (assign) and
VHDL concurrent assignment statements () are reevaluated anytime
any of the inputs on the right hand side changes. Therefore, such code
necessarily describes combinational logic.
190 CHAPTER FOUR Hardware Description Languages
mux2
lsbmux
mux2
msbmux
y[7:0] [7:0]
s
d1[7:0]
[7:0]
d0[7:0] [7:0]
s
[3:0]
d0[3:0]
[3:0]
d1[3:0]
[3:0] y[3:0]
s
[7:4]
d0[3:0]
[7:4]
d1[3:0]
[7:4]
y[3:0]
Figure 4.15 mux2_8 synthesized circuit
C
4.4 Sequential Logic 191
d[3:0] q[3:0]
clk [3:0] [3:0] D[3:0] Q[3:0]
Figure 4.16 flop synthesized circuit
Verilog
module flop (input clk,
input [3:0] d,
output reg [3:0] q);
always @ (posedge clk)
q  d;
endmodule
A Verilog always statement is written in the form
always @ (sensitivity list)
statement;
The statement is executed only when the event specified in
the sensitivity list occurs. In this example, the statement is
q  d (pronounced “q gets d”). Hence, the flip-flop copies d
to q on the positive edge of the clock and otherwise remembers the old state of q.
 is called a nonblocking assignment. Think of it as a
regular  sign for now; we’ll return to the more subtle
points in Section 4.5.4. Note that  is used instead of
assign inside an always statement.
All signals on the left hand side of  or  in an always
statement must be declared as reg. In this example, q is both
an output and a reg, so it is declared as output reg [3:0] q.
Declaring a signal as reg does not mean the signal is actually
the output of a register! All it means is that the signal
appears on the left hand side of an assignment in an always
statement. We will see later examples of always statements
describing combinational logic in which the output is
declared reg but does not come from a flip-flop.
HDL Example 4.18 REGISTER
4.4.2 Resettable Registers
When simulation begins or power is first applied to a circuit, the output of
a flop or register is unknown. This is indicated with x in Verilog and ‘u’ in
VHDL. Generally, it is good practice to use resettable registers so that on
powerup you can put your system in a known state. The reset may be
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity flop is
port (clk: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0) ;
q: out STD_LOGIC_VECTOR (3 downto 0)) ;
end;
architecture synth of flop is
begin
process (clk) begin
if clk’event and clk  ‘1’ then
q  d;
end if;
end process;
end;
A VHDL process is written in the form
process (sensitivity list) begin
statement;
end process;
The statement is executed when any of the variables in the
sensitivity list change. In this example, the if statement is
executed when clk changes, indicated by clk’event. If the
change is a rising edge (clk  ‘1’ after the event), then
q  d (pronounced “q gets d”). Hence, the flip-flop copies d
to q on the positive edge of the clock and otherwise remembers the old state of q.
An alternative VHDL idiom for a flip-flop is
process (clk) begin
if RISING_EDGE (clk) then
q  d;
end if;
end process;
RISING_EDGE (clk) is synonymous with clk’event and
clk  1.
Chapter 04.qx
either asynchronous or synchronous. Recall that asynchronous reset
occurs immediately, whereas synchronous reset clears the output only on
the next rising edge of the clock. HDL Example 4.19 demonstrates the
idioms for flip-flops with asynchronous and synchronous resets. Note that
distinguishing synchronous and asynchronous reset in a schematic can be
difficult. The schematic produced by Synplify Pro places asynchronous
reset at the bottom of a flip-flop and synchronous reset on the left side.
192 CHAPTER FOUR Hardware Description Languages
Verilog
module flopr (input clk,
input reset,
input [3:0] d,
output reg [3:0] q);
// asynchronous reset
always @ (posedge clk, posedge reset)
if (reset) q  4’b0;
else q  d;
endmodule
module flopr (input clk,
input reset,
input [3:0] d,
output reg [3:0] q);
// synchronous reset
always @ (posedge clk)
if (reset) q  4’b0;
else q  d;
endmodule
Multiple signals in an always statement sensitivity list are
separated with a comma or the word or. Notice that posedge
reset is in the sensitivity list on the asynchronously resettable flop, but not on the synchronously resettable flop.
Thus, the asynchronously resettable flop immediately
responds to a rising edge on reset, but the synchronously
resettable flop responds to reset only on the rising edge of
the clock.
Because the modules have the same name, flopr, you
may include only one or the other in your design.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity flopr is
port (clk,
reset: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0);
q: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture asynchronous of flopr is
begin
process (clk, reset) begin
if reset  ‘1’ then
q  “0000”;
elsif clk’ event and clk  ‘1’ then
q  d;
end if;
end process;
end;
architecture synchronous of flopr is
begin
process (clk) begin
if clk’event and clk  ‘1’ then
if reset  ‘1’ then
q  “0000”;
else q  d;
end if;
end if;
end process;
end;
Multiple signals in a process sensitivity list are separated with a
comma. Notice that reset is in the sensitivity list on the asynchronously resettable flop, but not on the synchronously resettable flop. Thus, the asynchronously resettable flop immediately
responds to a rising edge on reset, but the synchronously resettable flop responds to reset only on the rising edge of the clock.
Recall that the state of a flop is initialized to ‘u’ at
startup during VHDL simulation.
As mentioned earlier, the name of the architecture
(asynchronous or synchronous, in this example) is ignored by
the VHDL tools but may be helpful to the human reading the
code. Because both architectures describe the entity flopr,
you may include only one or the other in your design.
HDL Example 4.19 RESETTABLE REGISTER
Chapter 04.q
4.4 Sequential Logic 193
4.4.3 Enabled Registers
Enabled registers respond to the clock only when the enable is asserted.
HDL Example 4.20 shows an asynchronously resettable enabled register
that retains its old value if both reset and en are FALSE.
R
d[3:0] q[3:0]
reset
clk [3:0] Q[3:0] [3:0] D[3:0]
(a)
d[3:0] q[3:0]
reset
clk [3:0] Q[3:0] [3:0] D[3:0]
R
(b)
Figure 4.17 flopr synthesized circuit (a) asynchronous reset, (b) synchronous reset
Verilog
module flopenr (input clk,
input reset,
input en,
input [3:0] d,
output reg [3:0] q);
// asynchronous reset
always @ (posedge clk, posedge reset)
if (reset) q  4’b0;
else if (en) q  d;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity flopenr is
port (clk,
reset,
en: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0);
q: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture asynchronous of flopenr is
— — asynchronous reset
begin
process (clk, reset) begin
if reset  ‘1’ then
q  “0000”;
elsif clk’event and clk  ‘1’ then
if en  ‘1’ then
q  d;
end if;
end if;
end process;
end;
HDL Example 4.20 RESETTABLE ENABLED REGISTER
Chapter
4.4.4 Multiple Registers
A single always/process statement can be used to describe multiple
pieces of hardware. For example, consider the synchronizer from Section
3.5.5 made of two back-to-back flip-flops, as shown in Figure 4.19.
HDL Example 4.21 describes the synchronizer. On the rising edge of
clk, d is copied to n1. At the same time, n1 is copied to q.
194 CHAPTER FOUR Hardware Description Languages
R
d[3:0] q[3:0]
en
reset
clk
[3:0] [3:0] D[3:0] Q[3:0]
E
Figure 4.18 flopenr synthesized circuit
CLK CLK
D
N1
Q
Figure 4.19 Synchronizer circuit
Verilog
module sync (input clk,
input d,
output reg q);
reg n1;
always @ (posedge clk)
begin
n1  d;
q  n1;
end
endmodule
n1 must be declared as a reg because it is an internal signal
used on the left hand side of  in an always statement.
Also notice that the begin/end construct is necessary because
multiple statements appear in the always statement. This is
analogous to { } in C or Java. The begin/end was not
needed in the flopr example because if/else counts as a
single statement.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity sync is
port (clk: in STD_LOGIC;
d: in STD_LOGIC;
q: out STD_LOGIC);
end;
architecture good of sync is
signal n1: STD_LOGIC;
begin
process (clk) begin
if clk’event and clk  ‘1’ then
n1  d;
q  n1;
end if;
end process;
end;
n1 must be declared as a signal because it is an internal
signal used in the module.
HDL Example 4.21 SYNCHRONIZER
n1 q
d q
clk
QD QD
Figure 4.20 sync synthesized circuit
Chapte
4.5 More Combinational Logic 195
4.4.5 Latches
Recall from Section 3.2.2 that a D latch is transparent when the clock is
HIGH, allowing data to flow from input to output. The latch becomes
opaque when the clock is LOW, retaining its old state. HDL Example
4.22 shows the idiom for a D latch.
Not all synthesis tools support latches well. Unless you know that your
tool does support latches and you have a good reason to use them, avoid
them and use edge-triggered flip-flops instead. Furthermore, take care that
your HDL does not imply any unintended latches, something that is easy to
do if you aren’t attentive. Many synthesis tools warn you when a latch is
created; if you didn’t expect one, track down the bug in your HDL.
4.5 MORE COMBINATIONAL LOGIC
In Section 4.2, we used assignment statements to describe combinational
logic behaviorally. Verilog always statements and VHDL process statements are used to describe sequential circuits, because they remember the
old state when no new state is prescribed. However, always/process
lat
q[3:0]
q[3:0] d[3:0]
clk
[3:0] D[3:0] [3:0]
Q[3:0] C
Figure 4.21 latch synthesized circuit
Verilog
module latch (input clk,
input [3:0] d,
output reg [3:0] q);
always @ (clk, d)
if (clk) q  d;
endmodule
The sensitivity list contains both clk and d, so the always
statement evaluates any time clk or d changes. If clk is
HIGH, d flows through to q.
q must be declared to be a reg because it appears on the
left hand side of  in an always statement. This does not
always mean that q is the output of a register.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity latch is
port (clk: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (3 downto 0);
q: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of latch is
begin
process (clk, d) begin
if clk  ‘1’ then q  d;
end if;
end process;
end;
The sensitivity list contains both clk and d, so the process
evaluates anytime clk or d changes. If clk is HIGH, d flows
through to q.
HDL Example 4.22 D LATCH
Chap
statements can also be used to describe combinational logic behaviorally
if the sensitivity list is written to respond to changes in all of the inputs
and the body prescribes the output value for every possible input combination. HDL Example 4.23 uses always/process statements to describe
a bank of four inverters (see Figure 4.3 for the synthesized circuit).
HDLs support blocking and nonblocking assignments in an
always/process statement. A group of blocking assignments are evaluated in the order in which they appear in the code, just as one would
expect in a standard programming language. A group of nonblocking
assignments are evaluated concurrently; all of the statements are evaluated before any of the signals on the left hand sides are updated.
HDL Example 4.24 defines a full adder using intermediate signals
p and g to compute s and cout. It produces the same circuit from
Figure 4.8, but uses always/process statements in place of assignment
statements.
These two examples are poor applications of always/process statements for modeling combinational logic because they require more lines
than the equivalent approach with assignment statements from Section
4.2.1. Moreover, they pose the risk of inadvertently implying sequential
logic if the inputs are left out of the sensitivity list. However, case and
if statements are convenient for modeling more complicated combinational logic. case and if statements must appear within always/process
statements and are examined in the next sections.
196 CHAPTER FOUR Hardware Description Languages
Verilog
module inv (input [3:0] a,
output reg [3:0] y);
always @ (*)
y  ~a;
endmodule
always @ (*) reevaluates the statements inside the always
statement any time any of the signals on the right hand side
of  or  inside the always statement change. Thus, @ (*)
is a safe way to model combinational logic. In this particular
example, @ (a) would also have sufficed.
The  in the always statement is called a blocking assignment, in contrast to the  nonblocking assignment. In
Verilog, it is good practice to use blocking assignments for
combinational logic and nonblocking assignments for sequential logic. This will be discussed further in Section 4.5.4.
Note that y must be declared as reg because it appears
on the left hand side of a  or  sign in an always statement. Nevertheless, y is the output of combinational logic,
not a register.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity inv is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture proc of inv is
begin
process (a) begin
y  not a;
end process;
end;
The begin and end process statements are required in VHDL
even though the process contains only one assignment.
HDL Example 4.23 INVERTER USING always/process
Chapter 
4.5 More Combinational Logic 197
Verilog
In a Verilog always statement,  indicates a blocking assignment and  indicates a nonblocking assignment (also called
a concurrent assignment).
Do not confuse either type with continuous assignment
using the assign statement. assign statements must be used
outside always statements and are also evaluated concurrently.
VHDL
In a VHDL process statement, :  indicates a blocking
assignment and  indicates a nonblocking assignment (also
called a concurrent assignment). This is the first section
where :  is introduced.
Nonblocking assignments are made to outputs and to
signals. Blocking assignments are made to variables, which
are declared in process statements (see HDL Example
4.24).
 can also appear outside process statements, where it
is also evaluated concurrently.
Verilog
module fulladder (input a, b, cin,
output reg s, cout);
reg p, g;
always @ (*)
begin
p  a  b; // blocking
g  a & b; // blocking
s  p  cin; // blocking
cout  g | (p & cin); // blocking
end
endmodule
In this case, an @ (a, b, cin) would have been equivalent to
@ (*). However, @ (*) is better because it avoids common
mistakes of missing signals in the stimulus list.
For reasons that will be discussed in Section 4.5.4, it is
best to use blocking assignments for combinational logic.
This example uses blocking assignments, first computing
p, then g, then s, and finally cout.
Because p and g appear on the left hand side of an
assignment in an always statement, they must be declared to
be reg.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fulladder is
port (a, b, cin: in STD_LOGIC;
s, cout: out STD_LOGIC);
end;
architecture synth of fulladder is
begin
process (a, b, cin)
variable p, g: STD_LOGIC;
begin
p : a xor b; — — blocking
g : a and b; — — blocking
s  p xor cin;
cout  g or (p and cin);
end process;
end;
The process sensitivity list must include a, b, and cin because
combinational logic should respond to changes of any input.
For reasons that will be discussed in Section 4.5.4, it is
best to use blocking assignments for intermediate variables in
combinational logic. This example uses blocking assignments
for p and g so that they get their new values before being
used to compute s and cout that depend on them.
Because p and g appear on the left hand side of a
blocking assignment (:) in a process statement, they must
be declared to be variable rather than signal. The variable
declaration appears before the begin in the process where
the variable is used.
HDL Example 4.24 FULL ADDER USING always/process
Chapter 04.qxd 1/
4.5.1 Case Statements
A better application of using the always/process statement for combinational logic is a seven-segment display decoder that takes advantage of the
case statement that must appear inside an always/process statement.
As you might have noticed in Example 2.10, the design process for
large blocks of combinational logic is tedious and prone to error. HDLs
offer a great improvement, allowing you to specify the function at a
higher level of abstraction, and then automatically synthesize the function into gates. HDL Example 4.25 uses case statements to describe a
seven-segment display decoder based on its truth table. (See Example
2.10 for a description of the seven-segment display decoder.) The case
198 CHAPTER FOUR Hardware Description Languages
Verilog
module sevenseg (input [3:0] data,
output reg [6:0] segments);
always @ (*)
case (data)
// abc_defg
0: segments  7’b111_1110;
1: segments  7’b011_0000;
2: segments  7’b110_1101;
3: segments  7’b111_1001;
4: segments  7’b011_0011;
5: segments  7’b101_1011;
6: segments  7’b101_1111;
7: segments  7’b111_0000;
8: segments  7’b111_1111;
9: segments  7’b111_1011;
default: segments  7’b000_0000;
endcase
endmodule
The case statement checks the value of data. When data is 0,
the statement performs the action after the colon, setting
segments to 1111110. The case statement similarly checks
other data values up to 9 (note the use of the default base,
base 10).
The default clause is a convenient way to define the
output for all cases not explicitly listed, guaranteeing combinational logic.
In Verilog, case statements must appear inside always
statements.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity seven_seg_decoder is
port (data: in STD_LOGIC_VECTOR (3 downto 0);
segments: out STD_LOGIC_VECTOR (6 downto 0));
end;
architecture synth of seven_seg_decoder is
begin
process (data) begin
case data is
— — abcdefg
when X“0”  segments  “1111110”;
when X“1”  segments  “0110000”;
when X“2”  segments  “1101101”;
when X“3”  segments  “1111001”;
when X“4”  segments  “0110011”;
when X“5”  segments  “1011011”;
when X“6”  segments  “1011111”;
when X“7”  segments  “1110000”;
when X“8”  segments  “1111111”;
when X“9”  segments  “1111011”;
when others  segments  “0000000”;
end case;
end process;
end;
The case statement checks the value of data. When data is 0,
the statement performs the action after the , setting
segments to 1111110. The case statement similarly checks
other data values up to 9 (note the use of X for hexadecimal
numbers). The others clause is a convenient way to define
the output for all cases not explicitly listed, guaranteeing
combinational logic.
Unlike Verilog, VHDL supports selected signal assignment statements (see HDL Example 4.6), which are much like
case statements but can appear outside processes. Thus, there
is less reason to use processes to describe combinational logic.
HDL Example 4.25 SEVEN-SEGMENT DISPLAY DECODER
Chapter 04.qxd 1/31/07 8:17 PM Pag
4.5 More Combinational Logic 199
statement performs different actions depending on the value of its
input. A case statement implies combinational logic if all possible input
combinations are defined; otherwise it implies sequential logic, because
the output will keep its old value in the undefined cases.
Synplify Pro synthesizes the seven-segment display decoder into a
read-only memory (ROM) containing the 7 outputs for each of the 16
possible inputs. ROMs are discussed further in Section 5.5.6.
If the default or others clause were left out of the case statement,
the decoder would have remembered its previous output anytime data
were in the range of 10–15. This is strange behavior for hardware.
Ordinary decoders are also commonly written with case statements.
HDL Example 4.26 describes a 3:8 decoder.
4.5.2 If Statements
always/process statements may also contain if statements. The if
statement may be followed by an else statement. If all possible input
rom
segments_1[6:0]
data[3:0] segments[6:0] [6:0]
DOUT[6:0]
[3:0]
A[3:0]
Figure 4.22 sevenseg synthesized circuit
Verilog
module decoder3_8 (input [2:0] a,
output reg [7:0] y);
always @ (*)
case (a)
3’b000: y  8’b00000001;
3’b001: y  8’b00000010;
3’b010: y  8’b00000100;
3’b011: y  8’b00001000;
3’b100: y  8’b00010000;
3’b101: y  8’b00100000;
3’b110: y  8’b01000000;
3’b111: y  8’b10000000;
endcase
endmodule
No default statement is needed because all cases are covered.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity decoder3_8 is
port (a: in STD_LOGIC_VECTOR (2 downto 0);
y: out STD_LOGIC_VECTOR (7 downto 0));
end;
architecture synth of decoder3_8 is
begin
process (a) begin
case a is
when “000”  y  “00000001”;
when “001”  y  “00000010”;
when “010”  y  “00000100”;
when “011”  y  “00001000”;
when “100”  y  “00010000”;
when “101”  y  “00100000”;
when “110”  y  “01000000”;
when “111”  y  “10000000”;
end case;
end process;
end;
No others clause is needed because all cases are covered.
HDL Example 4.26 3:8 DECODER
Chapter 04.qxd 1/31/07 8
200 CHAPTER FOUR Hardware Description Languages
y41
y34
y35
y36
y37
y38
y39
y40
y[7:0]
a[2:0] [2:0]
[0]
[1]
[2]
[0]
[1]
[2]
[2]
[0]
[1]
[0]
[2]
[1]
[1]
[2]
[0]
[1]
[0]
[2]
[0]
[1]
[2]
[0]
[1]
[2]
Figure 4.23 decoder3_8 synthesized circuit

4.5 More Combinational Logic 201
combinations are handled, the statement implies combinational logic;
otherwise, it produces sequential logic (like the latch in Section 4.4.5).
HDL Example 4.27 uses if statements to describe a priority circuit,
defined in Section 2.4. Recall that an N-input priority circuit sets the
output TRUE that corresponds to the most significant input that is
TRUE.
4.5.3 Verilog casez*
(This section may be skipped by VHDL users.) Verilog also provides the
casez statement to describe truth tables with don’t cares (indicated with
? in the casez statement). HDL Example 4.28 shows how to describe a
priority circuit with casez.
Synplify Pro synthesizes a slightly different circuit for this module,
shown in Figure 4.25, than it did for the priority circuit in Figure 4.24.
However, the circuits are logically equivalent.
4.5.4 Blocking and Nonblocking Assignments
The guidelines on page 203 explain when and how to use each type of
assignment. If these guidelines are not followed, it is possible to write
Verilog
module priority (input [3:0] a,
output reg [3:0] y);
always @ (*)
if (a[3]) y  4’b1000;
else if (a[2]) y  4’b0100;
else if (a[1]) y  4’b0010;
else if (a[0]) y  4’b0001;
else y  4’b0000;
endmodule
In Verilog, if statements must appear inside of always
statements.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity priority is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (3 downto 0));
end;
architecture synth of priority is
begin
process (a) begin
if a(3)  ‘1’ then y  “1000”;
elsif a(2)  ‘1’ then y  “0100”;
elsif a(1)  ‘1’ then y  “0010”;
elsif a(0)  ‘1’ then y  “0001”;
else y  “0000”;
end if;
end process;
end;
Unlike Verilog, VHDL supports conditional signal assignment statements (see HDL Example 4.6), which are much
like if statements but can appear outside processes. Thus,
there is less reason to use processes to describe combinational logic. (Figure follows on next page.)
HDL Example 4.27 PRIORITY CIRCUIT
Chapter 04.qxd
code that appears to work in simulation but synthesizes to incorrect
hardware. The optional remainder of this section explains the principles
behind the guidelines.
Combinational Logic*
The full adder from HDL Example 4.24 is correctly modeled using
blocking assignments. This section explores how it operates and how it
would differ if nonblocking assignments had been used.
Imagine that a, b, and cin are all initially 0. p, g, s, and cout are
thus 0 as well. At some time, a changes to 1, triggering the
always/process statement. The four blocking assignments evaluate in
202 CHAPTER FOUR Hardware Description Languages
module priority_casez(input [3:0] a,
output reg [3:0] y);
always @ (*)
casez (a)
4’b1???: y  4’b1000;
4’b01??: y  4’b0100;
4’b001?: y  4’b0010;
4’b0001: y  4’b0001;
default: y  4’b0000;
endcase
endmodule
HDL Example 4.28 PRIORITY CIRCUIT USING casez
y_1[2]
un13_y
un21_y
y_1[1]
y_1[0]
y[3:0]
a[3:0] [3:0]
[3]
[2:0]
[2] [2] [3]
[2]
[3]
[1]
[2]
[3]
[1] [1]
[0] [0]
Figure 4.24 priority
synthesized circuit
(See figure 4.25.)
Chapt
4.5 More Combinational Logic 203
y23[0]
y24[0]
y25
a[3:0] y[3:0] [3:0] [3]
[2]
[2]
[1]
[0]
[3]
[1]
[2]
[3]
[0]
[1]
[2]
[3]
Figure 4.25 priority_casez
synthesized circuit
Verilog
1. Use always @ (posedge clk) and nonblocking assignments
to model synchronous sequential logic.
always @ (posedge clk)
begin
nl  d; // nonblocking
q  nl; // nonblocking
end
2. Use continuous assignments to model simple combinational logic.
assign y  s ? d1 : d0;
3. Use always @ (*) and blocking assignments to model
more complicated combinational logic where the always
statement is helpful.
always @ (*)
begin
p  a  b; // blocking
g  a & b; // blocking
s  p  cin;
cout  g | (p & cin);
end
4. Do not make assignments to the same signal in more
than one always statement or continuous assignment
statement.
VHDL
1. Use process (clk) and nonblocking assignments to model
synchronous sequential logic.
process (clk) begin
if clk’event and clk  ‘1’ then
nl  d; — — nonblocking
q  nl; — — nonblocking
end if;
end process;
2. Use concurrent assignments outside process statements to
model simple combinational logic.
y  d0 when s  ‘0’ else d1;
3. Use process (in1, in2, ... ) to model more complicated
combinational logic where the process is helpful. Use
blocking assignments for internal variables.
process (a, b, cin)
variable p, g: STD_LOGIC;
begin
p : a xor b; — — blocking
g : a and b; — — blocking
s  p xor cin;
cout  g or (p and cin);
end process;
4. Do not make assignments to the same variable in more
than one process or concurrent assignment statement.
BLOCKING AND NONBLOCKING ASSIGNMENT GUIDELINES
Chapter 04.qxd 1/3
the order shown here. (In the VHDL code, s and cout are assigned concurrently.) Note that p and g get their new values before s and cout are
computed because of the blocking assignments. This is important because
we want to compute s and cout using the new values of p and g.
1. p 1  0  1
2. g 1 • 0  0
3. s 1  0  1
4. cout 0  1 • 0  0
In contrast, HDL Example 4.29 illustrates the use of nonblocking
assignments.
Now consider the same case of a rising from 0 to 1 while b and cin
are 0. The four nonblocking assignments evaluate concurrently:
p 1  01 g 1• 0 0 s 0  0 0 cout 00 • 0  0
Observe that s is computed concurrently with p and hence uses the
old value of p, not the new value. Therefore, s remains 0 rather than
; ; ; ;
;
;
;
;
204 CHAPTER FOUR Hardware Description Languages
Verilog
// nonblocking assignments (not recommended)
module fulladder (input a, b, cin,
output reg s, cout);
reg p, g;
always @ (*)
begin
p  a  b; // nonblocking
g  a & b; // nonblocking
s  p  cin;
cout  g | (p & cin);
end
endmodule
Because p and g appear on the left hand side of an assignment
in an always statement, they must be declared to be reg.
VHDL
— — nonblocking assignments (not recommended)
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fulladder is
port (a, b, cin: in STD_LOGIC;
s, cout: out STD_LOGIC);
end;
architecture nonblocking of fulladder is
signal p, g: STD_LOGIC;
begin
process (a, b, cin, p, g) begin
p  a xor b; — — nonblocking
g  a and b; — — nonblocking
s  p xor cin;
cout  g or (p and cin);
end process;
end;
Because p and g appear on the left hand side of a nonblocking assignment in a process statement, they must be declared
to be signal rather than variable. The signal declaration
appears before the begin in the architecture, not the
process.
HDL Example 4.29 FULL ADDER USING NONBLOCKING ASSIGNMENTS
Chapter 04.qxd 1/31/07 8
4.5 More Combinational Logic 205
becoming 1. However, p does change from 0 to 1. This change triggers
the always/process statement to evaluate a second time, as follows:
p 1 0 1 g 1• 0  0 s 1  0 1 cout 0 1•0  0
This time, p is already 1, so s correctly changes to 1. The nonblocking
assignments eventually reach the right answer, but the always/process
statement had to evaluate twice. This makes simulation slower, though it
synthesizes to the same hardware.
Another drawback of nonblocking assignments in modeling combinational logic is that the HDL will produce the wrong result if you
forget to include the intermediate variables in the sensitivity list.
; ; ; ;
Worse yet, some synthesis tools will synthesize the correct hardware
even when a faulty sensitivity list causes incorrect simulation. This leads
to a mismatch between the simulation results and what the hardware
actually does.
Sequential Logic*
The synchronizer from HDL Example 4.21 is correctly modeled using nonblocking assignments. On the rising edge of the clock, d is copied to n1 at
the same time that n1 is copied to q, so the code properly describes two
registers. For example, suppose initially that d  0, n1  1, and q  0.
On the rising edge of the clock, the following two assignments occur
concurrently, so that after the clock edge, n1  0 and q  1.
nl d  0 q n1  1
HDL Example 4.30 tries to describe the same module using blocking
assignments. On the rising edge of clk, d is copied to n1. Then this new
value of n1 is copied to q, resulting in d improperly appearing at both n1
and q. The assignments occur one after the other so that after the clock
edge, q  n1  0.
1. n1 d  0
2. q n1 ;  0
;
; ;
Verilog
If the sensitivity list of the always statement in HDL Example
4.29 were written as always @ (a, b, cin) rather than always
@ (*), then the statement would not reevaluate when p or g
changes. In the previous example, s would be incorrectly left
at 0, not 1.
VHDL
If the sensitivity list of the process statement in HDL
Example 4.29 were written as process (a, b, cin) rather
than process (a, b, cin, p, g), then the statement would not
reevaluate when p or g changes. In the previous example,
s would be incorrectly left at 0, not 1.
Chapter 04.qxd 1/3
Because n1 is invisible to the outside world and does not influence
the behavior of q, the synthesizer optimizes it away entirely, as shown in
Figure 4.26.
The moral of this illustration is to exclusively use nonblocking
assignment in always statements when modeling sequential logic. With
sufficient cleverness, such as reversing the orders of the assignments, you
could make blocking assignments work correctly, but blocking assignments offer no advantages and only introduce the risk of unintended
behavior. Certain sequential circuits will not work with blocking assignments no matter what the order.
4.6 FINITE STATE MACHINES
Recall that a finite state machine (FSM) consists of a state register and two
blocks of combinational logic to compute the next state and the output
given the current state and the input, as was shown in Figure 3.22. HDL
descriptions of state machines are correspondingly divided into three parts
to model the state register, the next state logic, and the output logic.
206 CHAPTER FOUR Hardware Description Languages
Verilog
// Bad implementation using blocking assignments
module syncbad (input clk,
input d,
output reg q);
reg n1;
always @ (posedge clk)
begin
n1  d; // blocking
q  n1; // blocking
end
endmodule
VHDL
— — Bad implementation using blocking assignment
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity syncbad is
port (clk: in STD_LOGIC;
d: in STD_LOGIC;
q: out STD_LOGIC);
end;
architecture bad of syncbad is
begin
process (clk)
variable n1: STD_LOGIC;
begin
if clk’event and clk  ‘1’ then
n1 : d; — — blocking
q  n1;
end if;
end process;
end;
HDL Example 4.30 BAD SYNCHRONIZER WITH BLOCKING ASSIGNMENTS
q
d q
clk
D Q
Figure 4.26 syncbad synthesized circuit
Chapt
4.6 Finite State Machines 207
HDL Example 4.31 describes the divide-by-3 FSM from Section
3.4.2. It provides an asynchronous reset to initialize the FSM. The state
register uses the ordinary idiom for flip-flops. The next state and output
logic blocks are combinational.
The Synplify Pro Synthesis tool just produces a block diagram and
state transition diagram for state machines; it does not show the logic
gates or the inputs and outputs on the arcs and states. Therefore, be
careful that you have specified the FSM correctly in your HDL code. The
state transition diagram in Figure 4.27 for the divide-by-3 FSM is analogous to the diagram in Figure 3.28(b). The double circle indicates that
Verilog
module divideby3FSM (input clk,
input reset,
output y);
reg [1:0] state, nextstate;
parameter S0  2b00;
parameter S1  2b01;
parameter S2  2b10;
// state register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// next state logic
always @ (*)
case (state)
S0: nextstate  S1;
S1: nextstate  S2;
S2: nextstate  S0;
default: nextstate  S0;
endcase
// output logic
assign y  (state  S0);
endmodule
The parameter statement is used to define constants within a
module. Naming the states with parameters is not required,
but it makes changing state encodings much easier and
makes the code more readable.
Notice how a case statement is used to define the state
transition table. Because the next state logic should be combinational, a default is necessary even though the state 2b11
should never arise.
The output, y, is 1 when the state is S0. The equality
comparison a  b evaluates to 1 if a equals b and 0 otherwise. The inequality comparison a ! b does the inverse,
evaluating to 1 if a does not equal b.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity divideby3FSM is
port (clk, reset: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of divideby3FSM is
type statetype is (S0, S1, S2);
signal state, nextstate: statetype;
begin
— — state register
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
— — next state logic
nextstate  S1 when state  S0 else
S2 when state  S1 else
S0;
— — output logic
y  ‘1’ when state  S0 else ‘0’;
end;
This example defines a new enumeration data type,
statetype, with three possibilities: S0, S1, and S2. state and
nextstate are statetype signals. By using an enumeration
instead of choosing the state encoding, VHDL frees the
synthesizer to explore various state encodings to choose the
best one.
The output, y, is 1 when the state is S0. The inequalitycomparison uses /. To produce an output of 1 when the state
is anything but S0, change the comparison to state / S0.
HDL Example 4.31 DIVIDE-BY-3 FINITE STATE MACHINE
Notice that the synthesis tool
uses a 3-bit encoding (Q[2:0])
instead of the 2-bit encoding
suggested in the Verilog code.
Chapter 04.qxd 1/31/07 8:1
S0 is the reset state. Gate-level implementations of the divide-by-3 FSM
were shown in Section 3.4.2.
If, for some reason, we had wanted the output to be HIGH in states
S0 and S1, the output logic would be modified as follows.
The next two examples describe the snail pattern recognizer FSM
from Section 3.4.3. The code shows how to use case and if statements to handle next state and output logic that depend on the inputs
as well as the current state. We show both Moore and Mealy modules. In the Moore machine (HDL Example 4.32), the output depends
only on the current state, whereas in the Mealy machine (HDL
Example 4.33), the output logic depends on both the current state
and inputs.
208 CHAPTER FOUR Hardware Description Languages
S0
S1
S2
statemachine
state[2:0]
y [0]
reset
clk C
[2:0] Q[2:0] R
Figure 4.27 divideby3fsm
synthesized circuit
Verilog
// Output Logic
assign y  (state  S0 | state  S1);
VHDL
— — output logic
y  ‘1’ when (state  S0 or state  S1) else ‘0’;
Chapter 
Verilog
module patternMoore (input clk,
input reset,
input a,
output y);
reg [2:0] state, nextstate;
parameter S0  3b000;
parameter S1  3b001;
parameter S2  3b010;
parameter S3  3b011;
parameter S4  3b100;
// state register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// next state logic
always @ (*)
case (state)
S0: if (a) nextstate  S1;
else nextstate  S0;
S1: if (a) nextstate  S2;
else nextstate  S0;
S2: if (a) nextstate  S2;
else nextstate  S3;
S3: if (a) nextstate  S4;
else nextstate  S0;
S4: if (a) nextstate  S2;
else nextstate  S0;
default: nextstate  S0;
endcase
// output logic
assign y  (state  S4);
endmodule
Note how nonblocking assignments () are used in the
state register to describe sequential logic, whereas blocking
assignments () are used in the next state logic to describe
combinational logic.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity patternMoore is
port (clk, reset: in STD_LOGIC;
a: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of patternMoore is
type statetype is (S0, S1, S2, S3, S4);
signal state, nextstate: statetype;
begin
— — state register
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
— — next state logic
process (state, a) begin
case state is
when S0  if a  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if a  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if a  ‘1’ then
nextstate  S2;
else nextstate  S3;
end if;
when S3  if a  ‘1’ then
nextstate  S4;
else nextstate  S0;
end if;
when S4  if a  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when others  nextstate  S0;
end case;
end process;
— — output logic
y  ‘1’ when state  S4 else ‘0’;
end;
HDL Example 4.32 PATTERN RECOGNIZER MOORE FSM
S0
S1
S2
S3
S4
statemachine
state[4:0]
y
[4] a
reset
clk
I[0]
[4:0]
C Q[4:0]
R
Figure 4.28 patternMoore synthesized circuit
Chapter 04.qxd 1/31/07 8:17 PM Page 209
210 CHAPTER FOUR Hardware Description Languages
S0
S1
S3 S2
statemachine
state[3:0]
y
y a
reset
clk
I[0]
[3:0]
C Q[3:0]
R
[3]
Figure 4.29 patternMealy synthesized circuit
Verilog
module patternMealy (input clk,
input reset,
input a,
output y);
reg [1:0] state, nextstate;
parameter S0  2b00;
parameter S1  2b01;
parameter S2  2b10;
parameter S3  2b11;
// state register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// next state logic
always @ (*)
case (state)
S0: if (a) nextstate  S1;
else nextstate  S0;
S1: if (a) nextstate  S2;
else nextstate  S0;
S2: if (a) nextstate  S2;
else nextstate  S3;
S3: if (a) nextstate  S1;
else nextstate  S0;
default: nextstate  S0;
endcase
// output logic
assign y  (a & state  S3);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity patternMealy is
port (clk, reset: in STD_LOGIC;
a: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of patternMealy is
type statetype is (S0, S1, S2, S3);
signal state, nextstate: statetype;
begin
— — state register
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
— — next state logic
process(state, a) begin
case state is
when S0  if a  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if a  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if a  ‘1’ then
nextstate  S2;
else nextstate  S3;
end if;
when S3  if a  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when others  nextstate  S0;
end case;
end process;
— — output logic
y  ‘1’ when (a  ‘1’ and state  S3) else ‘0’;
end;
HDL Example 4.33 PATTERN RECOGNIZER MEALY FSM
Chapter 04.qxd 1/31/07 8:17 PM Page 210
4.7 Parameterized Modules 211
Verilog
module mux2
# (parameter width  8)
(input [width1:0] d0, d1,
input s,
output [width1:0] y);
assign y  s ? d1 : d0;
endmodule
Verilog allows a # (parameter ... ) statement before the
inputs and outputs to define parameters. The parameter
statement includes a default value (8) of the parameter,
width. The number of bits in the inputs and outputs can
depend on this parameter.
module mux4_8 (input [7:0] d0, d1, d2, d3,
input [1:0] s,
output [7:0] y);
wire [7:0] low, hi;
mux2 lowmux (d0, d1, s[0], low);
mux2 himux (d2, d3, s[1], hi);
mux2 outmux (low, hi, s[1], y);
endmodule
The 8-bit 4:1 multiplexer instantiates three 2:1 multiplexers
using their default widths.
In contrast, a 12-bit 4:1 multiplexer, mux4_12, would
need to override the default width using #() before the
instance name, as shown below.
module mux4_12 (input [11:0] d0, d1, d2, d3,
input [1:0] s,
output [11:0] y);
wire [11:0] low, hi;
mux2 #(12) lowmux(d0, d1, s[0], low);
mux2 #(12) himux(d2, d3, s[1], hi);
mux2 #(12) outmux(low, hi, s[1], y);
endmodule
Do not confuse the use of the # sign indicating delays with
the use of # ( ... ) in defining and overriding parameters.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is
generic(width: integer : 8);
port (d0,
d1: in STD_LOGIC_VECTOR (width1 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (width1 downto 0));
end;
architecture synth of mux2 is
begin
y  d0 when s  ‘0’ else d1;
end;
The generic statement includes a default value (8) of width.
The value is an integer.
entity mux4_8 is
port (d0, d1, d2,
d3: in STD_LOGIC_VECTOR (7 downto 0);
s: in STD_LOGIC_VECTOR (1 downto 0);
y: out STD_LOGIC_VECTOR (7 downto 0));
end;
architecture struct of mux4_8 is
component mux2
generic (width: integer);
port (d0,
d1: in STD_LOGIC_VECTOR (width1 downto 0) ;
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (width1 downto 0));
end component;
signal low, hi: STD_LOGIC_VECTOR(7 downto 0);
begin
lowmux: mux2 port map(d0, d1, s(0), low);
himux: mux2 port map(d2, d3, s(0), hi);
outmux: mux2 port map(low, hi, s(1), y);
end;
The 8-bit 4:1 multiplexer, mux4_8, instantiates three 2:1
multiplexers using their default widths.
In contrast, a 12-bit 4:1 multiplexer, mux4_12, would
need to override the default width using generic map, as
shown below.
lowmux: mux2 generic map (12)
port map (d0, d1, s(0), low);
himux: mux2 generic map (12)
port map (d2, d3, s(0), hi);
outmux: mux2 generic map (12)
port map (low, hi, s(1), y);
HDL Example 4.34 PARAMETERIZED N-BIT MULTIPLEXERS
4.7 PARAMETERIZED MODULES*
So far all of our modules have had fixed-width inputs and outputs. For
example, we had to define separate modules for 4- and 8-bit wide 2:1 multiplexers. HDLs permit variable bit widths using parameterized modules.
Chapt
HDL Example 4.34 declares a parameterized 2:1 multiplexer with a default
width of 8, then uses it to create 8- and 12-bit 4:1 multiplexers.
HDL Example 4.35 shows a decoder, which is an even better application of parameterized modules. A large N:2N decoder is cumbersome to
212 CHAPTER FOUR Hardware Description Languages
mux2_12
lowmux
mux2_12
himux
mux2_12
outmux
y[11:0]
s[1:0] [1:0]
d3[11:0]
d2[11:0]
d1[11:0]
d0[11:0]
[0]
s
[11:0]
d0[11:0]
[11:0]
d1[11:0]
y[11:0]
[1]
s
[11:0]
d0[11:0]
[11:0]
d1[11:0]
y[11:0]
[1]
s
[11:0]
d0[11:0]
[11:0]
d1[11:0]
[11:0]
y[11:0]
Figure 4.30 mux4_12 synthesized circuit
Verilog
module decoder # (parameter N  3)
(input [N1:0] a,
output reg [2**N1:0] y);
always @ (*)
begin
y  0;
y[a]  1;
end
endmodule
2**N indicates 2N.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
use IEEE.STD_LOGIC_ARITH.all;
entity decoder is
generic (N: integer : 3);
port (a: in STD_LOGIC_VECTOR (N1 downto 0);
y: out STD_LOGIC_VECTOR (2**N1 downto 0));
end;
architecture synth of decoder is
begin
process (a)
variable tmp: STD_LOGIC_VECTOR (2**N1 downto 0);
begin
tmp : CONV_STD_LOGIC_VECTOR(0, 2**N);
tmp (CONV_INTEGER(a)) : ‘1’;
y  tmp;
end process;
end;
2**N indicates 2N.
CONV_STD_LOGIC_VECTOR(0, 2**N) produces a
STD_LOGIC_VECTOR of length 2N containing all 0’s. It requires
the STD_LOGIC_ARITH library. The function is useful in other
parameterized functions, such as resettable flip-flops that
need to be able to produce constants with a parameterized
number of bits. The bit index in VHDL must be an integer,
so the CONV_INTEGER function is used to convert a from a
STD_LOGIC_VECTOR to an integer.
HDL Example 4.35 PARAMETERIZED N:2N DECODER
Chapter
4.7 Parameterized Modules 213
specify with case statements, but easy using parameterized code that simply sets the appropriate output bit to 1. Specifically, the decoder uses
blocking assignments to set all the bits to 0, then changes the appropriate
bit to 1.
HDLs also provide generate statements to produce a variable
amount of hardware depending on the value of a parameter. generate
supports for loops and if statements to determine how many of what
types of hardware to produce. HDL Example 4.36 demonstrates how to
use generate statements to produce an N-input AND function from a
cascade of two-input AND gates.
Use generate statements with caution; it is easy to produce a large
amount of hardware unintentionally!
Verilog
module andN
# (parameter width  8)
(input [width1:0] a,
output y);
genvar i;
wire [width1:1] x;
generate
for (i1; iwidth; ii1) begin:forloop
if (i  1)
assign x[1]  a[0] & a[1];
else
assign x[i]  a[i] & x[i1];
end
endgenerate
assign y  x[width1];
endmodule
The for statement loops through i  1, 2, . . . , width1 to
produce many consecutive AND gates. The begin in a
generate for loop must be followed by a : and an arbitrary
label (forloop, in this case).
Of course, writing assign y  &a would be much
easier!
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity andN is
generic (width: integer : 8);
port (a: in STD_LOGIC_VECTOR (width1 downto 0);
y: out STD_LOGIC);
end;
architecture synth of andN is
signal i: integer;
signal x: STD_LOGIC_VECTOR (width1 downto 1);
begin
AllBits: for i in 1 to width1 generate
LowBit: if i  1 generate
A1: x(1)  a(0) and a(1);
end generate;
OtherBits: if i / 1 generate
Ai: x(i)  a(i) and x(i1);
end generate;
end generate;
y  x(width1);
end;
HDL Example 4.36 PARAMETERIZED N-INPUT AND GATE
x[1] x[2] x[3] x[4] x[5] x[6] x[7]
y [7]
a[7:0] [7:0]
[0] [1] [1]
[2] [2] [1]
[3] [3] [2]
[4] [4] [3]
[5] [5] [4]
[6] [6] [5]
[7] [7] [6]
Figure 4.31 andN synthesized circuit
Chapter 04.qxd 1/
4.8 TESTBENCHES
A testbench is an HDL module that is used to test another module, called
the device under test (DUT). The testbench contains statements to apply
inputs to the DUT and, ideally, to check that the correct outputs are produced. The input and desired output patterns are called test vectors.
Consider testing the sillyfunction module from Section 4.1.1 that
computes . This is a simple module, so we can
perform exhaustive testing by applying all eight possible test vectors.
HDL Example 4.37 demonstrates a simple testbench. It instantiates
the DUT, then applies the inputs. Blocking assignments and delays are
used to apply the inputs in the appropriate order. The user must view the
results of the simulation and verify by inspection that the correct outputs
are produced. Testbenches are simulated the same as other HDL modules.
However, they are not synthesizeable.
y a b c  ab c  abc
214 CHAPTER FOUR Hardware Description Languages
Verilog
module testbench1 ();
reg a, b, c;
wire y;
// instantiate device under test
sillyfunction dut (a, b, c, y);
// apply inputs one at a time
initial begin
a  0; b  0; c  0; #10;
c  1; #10;
b  1; c  0; #10;
c  1; #10;
a  1; b  0; c  0; #10;
c  1; #10;
b  1; c  0; #10;
c  1; #10;
end
endmodule
The initial statement executes the statements in its body at
the start of simulation. In this case, it first applies the input
pattern 000 and waits for 10 time units. It then applies 001
and waits 10 more units, and so forth until all eight possible
inputs have been applied. initial statements should be used
only in testbenches for simulation, not in modules intended
to be synthesized into actual hardware. Hardware has no
way of magically executing a sequence of special steps when
it is first turned on.
Like signals in always statements, signals in initial
statements must be declared to be reg.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity testbench1 is — — no inputs or outputs
end;
architecture sim of testbench1 is
component sillyfunction
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end component;
signal a, b, c, y: STD_LOGIC;
begin
— — instantiate device under test
dut: sillyfunction port map (a, b, c, y);
— — apply inputs one at a time
process begin
a  ‘0’; b  ‘0’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
b  ‘1’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
a  ‘1’; b  ‘0’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
b  ‘1’; c  ‘0’; wait for 10 ns;
c  ‘1’; wait for 10 ns;
wait; — — wait forever
end process;
end;
The process statement first applies the input pattern 000 and
waits for 10 ns. It then applies 001 and waits 10 more ns,
and so forth until all eight possible inputs have been applied.
At the end, the process waits indefinitely; otherwise, the
process would begin again, repeatedly applying the pattern
of test vectors.
HDL Example 4.37 TESTBENCH
Some tools also call the
module to be tested the unit
under test (UUT).
Chapter 04.qxd 1/31/07 8:17 PM 
4.8 Testbenches 215
Checking for correct outputs is tedious and error-prone. Moreover,
determining the correct outputs is much easier when the design is fresh in
your mind; if you make minor changes and need to retest weeks later,
determining the correct outputs becomes a hassle. A much better approach
is to write a self-checking testbench, shown in HDL Example 4.38.
Writing code for each test vector also becomes tedious, especially for
modules that require a large number of vectors. An even better approach
is to place the test vectors in a separate file. The testbench simply reads
the test vectors from the file, applies the input test vector to the DUT,
waits, checks that the output values from the DUT match the output
vector, and repeats until reaching the end of the test vectors file.
Verilog
module testbench2 ();
reg a, b, c;
wire y;
// instantiate device under test
sillyfunction dut (a, b, c, y);
// apply inputs one at a time
// checking results
initial begin
a  0; b  0; c  0; #10;
if (y ! 1) $display(“000 failed.”);
c  1; #10;
if (y ! 0) $display(“001 failed.”);
b  1; c  0; #10;
if (y ! 0) $display(“010 failed.”);
c  1; #10;
if (y ! 0) $display(“011 failed.”);
a  1; b  0; c  0; #10;
if (y ! 1) $display(“100 failed.”);
c  1; #10;
if (y ! 1) $display(“101 failed.”);
b  1; c  0; #10;
if (y ! 0) $display(“110 failed.”);
c  1; #10;
if (y ! 0) $display(“111 failed.”);
end
endmodule
This module checks y against expectations after each input
test vector is applied. In Verilog, comparison using  or !
is effective between signals that do not take on the values of
x and z. Testbenches use the  and ! operators for
comparisons of equality and inequality, respectively, because
these operators work correctly with operands that could be x
or z. It uses the $display system task to print a message on
the simulator console if an error occurs. $display is meaningful only in simulation, not synthesis.
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity testbench2 is — — no inputs or outputs
end;
architecture sim of testbench2 is
component sillyfunction
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end component;
signal a, b, c, y: STD_LOGIC;
begin
— — instantiate device under test
dut: sillyfunction port map (a, b, c, y);
— — apply inputs one at a time
— — checking results
process begin
a  ‘0’; b  ‘0’; c  ‘0’; wait for 10 ns;
assert y  ‘1’ report “000 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘0’ report “001 failed.”;
b  ‘1’; c  ‘0’; wait for 10 ns;
assert y  ‘0’ report “010 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘0’ report “011 failed.”;
a  ‘1’; b  ‘0’; c  ‘0’; wait for 10 ns;
assert y  ‘1’ report “100 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘1’ report “101 failed.”;
b  ‘1’; c  ‘0’; wait for 10 ns;
assert y  ‘0’ report “110 failed.”;
c  ‘1’; wait for 10 ns;
assert y  ‘0’ report “111 failed.”;
wait; — — wait forever
end process;
end;
The assert statement checks a condition and prints the message
given in the report clause if the condition is not satisfied. assert
is meaningful only in simulation, not in synthesis.
HDL Example 4.38 SELF-CHECKING TESTBENCH
Chapter 04.qxd 1/31/07 8:17 PM Page 215
HDL Example 4.39 demonstrates such a testbench. The testbench generates a clock using an always/process statement with no stimulus list,
so that it is continuously reevaluated. At the beginning of the simulation, it
reads the test vectors from a text file and pulses reset for two cycles.
example.tv is a text file containing the inputs and expected output written
in binary:
000_1
001_0
010_0
011_0
100_1
101_1
110_0
111_0
216 CHAPTER FOUR Hardware Description Languages
Verilog
module testbench3 ();
reg clk, reset;
reg a, b, c, yexpected;
wire y;
reg [31:0] vectornum, errors;
reg [3:0] testvectors [10000:0];
// instantiate device under test
sillyfunction dut (a, b, c, y);
// generate clock
always
begin
clk  1; #5; clk  0; #5;
end
// at start of test, load vectors
// and pulse reset
initial
begin
$readmemb (“example.tv”, testvectors);
vectornum  0; errors  0;
reset  1; #27; reset  0;
end
// apply test vectors on rising edge of clk
always @ (posedge clk)
begin
#1; {a, b, c, yexpected}
testvectors[vectornum];
end
// check results on falling edge of clk
always @ (negedge clk)
if (~reset) begin // skip during reset
if (y ! yexpected) begin
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use STD.TEXTIO.all;
entity testbench3 is — — no inputs or outputs
end;
architecture sim of testbench3 is
component sillyfunction
port (a, b, c: in STD_LOGIC;
y: out STD_LOGIC);
end component;
signal a, b, c, y: STD_LOGIC;
signal clk, reset: STD_LOGIC;
signal yexpected: STD_LOGIC;
constant MEMSIZE: integer : 10000;
type tvarray is array (MEMSIZE downto 0) of
STD_LOGIC_VECTOR (3 downto 0);
signal testvectors: tvarray;
shared variable vectornum, errors: integer;
begin
— — instantiate device under test
dut: sillyfunction port map (a, b, c, y);
— — generate clock
process begin
clk  ‘1’; wait for 5 ns;
clk  ‘0’; wait for 5 ns;
end process;
— — at start of test, load vectors
— — and pulse reset
process is
file tv: TEXT;
variable i, j: integer;
variable L: line;
variable ch: character;
HDL Example 4.39 TESTBENCH WITH TEST VECTOR FILE
Chapter 04.q
4.8 Testbenches 217
$display (“Error: inputs  %b”, {a, b, c});
$display (“ outputs  %b (%b expected)”,
y, yexpected);
errors  errors  1;
end
vectornum  vectornum  1;
if (testvectors[vectornum]  4’bx) begin
$display (“%d tests completed with %d errors”,
vectornum, errors);
$finish;
end
end
endmodule
$readmemb reads a file of binary numbers into the testvectors
array. $readmemh is similar but reads a file of hexadecimal
numbers.
The next block of code waits one time unit after the
rising edge of the clock (to avoid any confusion if clock and
data change simultaneously), then sets the three inputs and
the expected output based on the four bits in the current test
vector. The next block of code checks the output of the DUT
at the negative edge of the clock, after the inputs have had
time to propagate through the DUT to produce the output, y.
The testbench compares the generated output, y, with the
expected output, yexpected, and prints an error if they don’t
match. %b and %d indicate to print the values in binary and
decimal, respectively. For example, $display (“%b %b”, y,
yexpected); prints the two values, y and yexpected, in
binary. %h prints a value in hexadecimal.
This process repeats until there are no more valid test
vectors in the testvectors array. $finish terminates the
simulation.
Note that even though the Verilog module supports up
to 10,001 test vectors, it will terminate the simulation after
executing the eight vectors in the file.
begin
— — read file of test vectors
i : 0;
FILE_OPEN (tv, “example.tv”, READ_MODE);
while not endfile (tv) loop
readline (tv, L);
for j in 0 to 3 loop
read (L, ch);
if (ch  ‘_’) then read (L, ch);
end if;
if (ch  ‘0’) then
testvectors (i) (j)  ‘0’;
else testvectors (i) (j)  ‘1’;
end if;
end loop;
i : i  1;
end loop;
vectornum : 0; errors : 0;
reset  ‘1’; wait for 27 ns; reset  ‘0’;
wait;
end process;
— — apply test vectors on rising edge of clk
process (clk) begin
if (clk’event and clk  ‘1’) then
a  testvectors (vectornum) (0) after 1 ns;
b  testvectors (vectornum) (1) after 1 ns;
c  testvectors (vectornum) (2) after 1 ns;
yexpected  testvectors (vectornum) (3)
after 1 ns;
end if;
end process;
— — check results on falling edge of clk
process (clk) begin
if (clk’event and clk  ‘0’ and reset  ‘0’) then
assert y  yexpected
report “Error: y  ” & STD_LOGIC’image(y);
if (y / yexpected) then
errors : errors  1;
end if;
vectornum : vectornum  1;
if (is_x (testvectors(vectornum))) then
if (errors  0) then
report “Just kidding — —” &
integer’image (vectornum) &
“tests completed successfully.”
severity failure;
else
report integer’image (vectornum) &
“tests completed, errors  ” &
integer’image (errors)
severity failure;
end if;
end if;
end if;
end process;
end;
The VHDL code is rather ungainly and uses file reading commands beyond the scope of this chapter, but it gives the sense
of what a self-checking testbench looks like.
Chapter 04.qxd 1/31/07 8:17 PM Page 
218 CHAPTER FOUR Hardware Description Languages
New inputs are applied on the rising edge of the clock, and the output
is checked on the falling edge of the clock. This clock (and reset) would
also be provided to the DUT if sequential logic were being tested. Errors
are reported as they occur. At the end of the simulation, the testbench
prints the total number of test vectors applied and the number of errors
detected.
The testbench in HDL Example 4.39 is overkill for such a simple
circuit. However, it can easily be modified to test more complex circuits
by changing the example.tv file, instantiating the new DUT, and changing a few lines of code to set the inputs and check the outputs.
4.9 SUMMARY
Hardware description languages (HDLs) are extremely important tools
for modern digital designers. Once you have learned Verilog or VHDL,
you will be able to specify digital systems much faster than if you had to
draw the complete schematics. The debug cycle is also often much faster,
because modifications require code changes instead of tedious schematic
rewiring. However, the debug cycle can be much longer using HDLs if
you don’t have a good idea of the hardware your code implies.
HDLs are used for both simulation and synthesis. Logic simulation
is a powerful way to test a system on a computer before it is turned into
hardware. Simulators let you check the values of signals inside your system that might be impossible to measure on a physical piece of hardware. Logic synthesis converts the HDL code into digital logic circuits.
The most important thing to remember when you are writing HDL
code is that you are describing real hardware, not writing a computer
program. The most common beginner’s mistake is to write HDL code
without thinking about the hardware you intend to produce. If you
don’t know what hardware you are implying, you are almost certain not
to get what you want. Instead, begin by sketching a block diagram of
your system, identifying which portions are combinational logic, which
portions are sequential circuits or finite state machines, and so forth.
Then write HDL code for each portion, using the correct idioms to
imply the kind of hardware you need.

Exercises 219
Exercises
The following exercises may be done using your favorite HDL. If you have a simulator available, test your design. Print the waveforms and explain how they prove
that it works. If you have a synthesizer available, synthesize your code. Print the
generated circuit diagram, and explain why it matches your expectations.
Exercise 4.1 Sketch a schematic of the circuit described by the following HDL
code. Simplify the schematic so that it shows a minimum number of gates.
Exercise 4.2 Sketch a schematic of the circuit described by the following HDL
code. Simplify the schematic so that it shows a minimum number of gates.
Exercise 4.3 Write an HDL module that computes a four-input XOR function.
The input is a3:0, and the output is y.
Verilog
module exercise1 (input a, b, c,
output y, z);
assign y  a & b & c | a & b & ~c | a & ~b & c;
assign z  a & b | ~a & ~b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity exercise1 is
port (a, b, c: in STD_LOGIC;
y, z: out STD_LOGIC);
end;
architecture synth of exercisel is
begin
y  (a and b and c) or (a and b and (not c)) or
(a and (not b) and c);
z  (a and b) or ((not a) and (not b));
end;
Verilog
module exercise2 (input [3:0] a,
output reg [1:0] y);
always @ (*)
if (a[0]) y  2’b11;
else if (a[1]) y  2’b10;
else if (a[2]) y  2’b01;
else if (a[3]) y  2’b00;
else y  a[1:0];
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity exercise2 is
port (a: in STD_LOGIC_VECTOR (3 downto 0);
y: out STD_LOGIC_VECTOR (1 downto 0));
end;
architecture synth of exercise2 is
begin
process (a) begin
if a(0)  ‘1’ then y  “11”;
elsif a(1)  ‘1’ then y  “10”;
elsif a(2)  ‘1’ then y  “01”;
elsif a(3)  ‘1’ then y  “00”;
else y  a(1 downto 0);
end if;
end process;
end;
Chapter 04.qxd 1/3
220 CHAPTER FOUR Hardware Description Languages
Exercise 4.4 Write a self-checking testbench for Exercise 4.3. Create a test vector
file containing all 16 test cases. Simulate the circuit and show that it works. Introduce an error in the test vector file and show that the testbench reports a mismatch.
Exercise 4.5 Write an HDL module called minority. It receives three inputs,
a, b, and c. It produces one output, y, that is TRUE if at least two of the inputs
are FALSE.
Exercise 4.6 Write an HDL module for a hexadecimal seven-segment display
decoder. The decoder should handle the digits A, B, C, D, E, and F as well as 0–9.
Exercise 4.7 Write a self-checking testbench for Exercise 4.6. Create a test vector file
containing all 16 test cases. Simulate the circuit and show that it works. Introduce an
error in the test vector file and show that the testbench reports a mismatch.
Exercise 4.8 Write an 8:1 multiplexer module called mux8 with inputs s2:0, d0,
d1, d2, d3, d4, d5, d6, d7, and output y.
Exercise 4.9 Write a structural module to compute the logic function,
, using multiplexer logic. Use the 8:1 multiplexer from
Exercise 4.8.
Exercise 4.10 Repeat Exercise 4.9 using a 4:1 multiplexer and as many NOT
gates as you need.
Exercise 4.11 Section 4.5.4 pointed out that a synchronizer could be correctly
described with blocking assignments if the assignments were given in the proper
order. Think of a simple sequential circuit that cannot be correctly described
with blocking assignments, regardless of order.
Exercise 4.12 Write an HDL module for an eight-input priority circuit.
Exercise 4.13 Write an HDL module for a 2:4 decoder.
Exercise 4.14 Write an HDL module for a 6:64 decoder using three instances of
the 2:4 decoders from Exercise 4.13 and a bunch of three-input AND gates.
Exercise 4.15 Write HDL modules that implement the Boolean equations from
Exercise 2.7.
Exercise 4.16 Write an HDL module that implements the circuit from
Exercise 2.18.
Exercise 4.17 Write an HDL module that implements the logic function from
Exercise 2.19. Pay careful attention to how you handle don’t cares.
y abb cabc
Cha
Exercises 221
Exercise 4.18 Write an HDL module that implements the functions from
Exercise 2.24.
Exercise 4.19 Write an HDL module that implements the priority encoder from
Exercise 2.25.
Exercise 4.20 Write an HDL module that implements the binary-to-thermometer
code converter from Exercise 2.27.
Exercise 4.21 Write an HDL module implementing the days-in-month function
from Question 2.2.
Exercise 4.22 Sketch the state transition diagram for the FSM described by the
following HDL code.
Verilog
module fsm2 (input clk, reset,
input a, b,
output y);
reg [1:0] state, nextstate;
parameter S0  2’b00;
parameter S1  2’b01;
parameter S2  2’b10;
parameter S3  2’b11;
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
always @ (*)
case (state)
S0: if (a  b) nextstate  S1;
else nextstate  S0;
S1: if (a & b) nextstate  S2;
else nextstate  S0;
S2: if (a | b) nextstate  S3;
else nextstate  S0;
S3: if (a | b) nextstate  S3;
else nextstate  S0;
endcase
assign y  (state  S1) | (state  S2);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fsm2 is
port (clk, reset: in STD_LOGIC;
a, b: in STD_LOGIC;
y: out STD_LOGIC);
end;
architecture synth of fsm2 is
type statetype is (S0, S1, S2, S3);
signal state, nextstate: statetype;
begin
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
process (state, a, b) begin
case state is
when S0  if (a xor b)  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if (a and b)  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if (a or b)  ‘1’ then
nextstate  S3;
else nextstate  S0;
end if;
when S3  if (a or b)  ‘1’ then
nextstate  S3;
else nextstate  S0;
end if;
end case;
end process;
y  ‘1’ when ((state  S1) or (state  S2))
else ‘0’;
end;
Chapter 04.qxd 1/31/07 8:17 PM Page 221
222 CHAPTER FOUR Hardware Description Languages
Exercise 4.23 Sketch the state transition diagram for the FSM described by the
following HDL code. An FSM of this nature is used in a branch predictor on
some microprocessors.
Verilog
module fsm1 (input clk, reset,
input taken, back,
output predicttaken);
reg [4:0] state, nextstate;
parameter S0  5’b00001;
parameter S1  5’b00010;
parameter S2  5’b00100;
parameter S3  5’b01000;
parameter S4  5’b10000;
always @ (posedge clk, posedge reset)
if (reset) state  S2;
else state  nextstate;
always @ (*)
case (state)
S0: if (taken) nextstate  S1;
else nextstate  S0;
S1: if (taken) nextstate  S2;
else nextstate  S0;
S2: if (taken) nextstate  S3;
else nextstate  S1;
S3: if (taken) nextstate  S4;
else nextstate  S2;
S4: if (taken) nextstate  S4;
else nextstate  S3;
default: nextstate  S2;
endcase
assign predicttaken  (state  S4) | |
(state  S3) | |
(state  S2 && back);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity fsm1 is
port (clk, reset: in STD_LOGIC;
taken, back: in STD_LOGIC;
predicttaken: out STD_LOGIC);
end;
architecture synth of fsm1 is
type statetype is (S0, S1, S2, S3, S4);
signal state, nextstate: statetype;
begin
process (clk, reset) begin
if reset  ‘1’ then state  S2;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
process (state, taken) begin
case state is
when S0  if taken  ‘1’ then
nextstate  S1;
else nextstate  S0;
end if;
when S1  if taken  ‘1’ then
nextstate  S2;
else nextstate  S0;
end if;
when S2  if taken  ‘1’ then
nextstate  S3;
else nextstate  S1;
end if;
when S3  if taken  ‘1’ then
nextstate  S4;
else nextstate  S2;
end if;
when S4  if taken  ‘1’ then
nextstate  S4;
else nextstate  S3;
end if;
when others  nextstate  S2;
end case;
end process;
— — output logic
predicttaken  ‘1’ when
((state  S4) or (state  S3) or
(state  S2 and back  ‘1’))
else ‘0’;
end;
Chapter 04.qxd 1/31/07 8:17 PM Page 222
Exercises 223
Exercise 4.24 Write an HDL module for an SR latch.
Exercise 4.25 Write an HDL module for a JK flip-flop. The flip-flop has inputs,
clk, J, and K, and output Q. On the rising edge of the clock, Q keeps its old
value if J  K  0. It sets Q to 1 if J  1, resets Q to 0 if K  1, and inverts Q if
J  K  1.
Exercise 4.26 Write an HDL module for the latch from Figure 3.18. Use one
assignment statement for each gate. Specify delays of 1 unit or 1 ns to each gate.
Simulate the latch and show that it operates correctly. Then increase the inverter
delay. How long does the delay have to be before a race condition causes the
latch to malfunction?
Exercise 4.27 Write an HDL module for the traffic light controller from
Section 3.4.1.
Exercise 4.28 Write three HDL modules for the factored parade mode traffic
light controller from Example 3.8. The modules should be called controller,
mode, and lights, and they should have the inputs and outputs shown in
Figure 3.33(b).
Exercise 4.29 Write an HDL module describing the circuit in Figure 3.40.
Exercise 4.30 Write an HDL module for the FSM with the state transition
diagram given in Figure 3.65 from Exercise 3.19.
Exercise 4.31 Write an HDL module for the FSM with the state transition
diagram given in Figure 3.66 from Exercise 3.20.
Exercise 4.32 Write an HDL module for the improved traffic light controller
from Exercise 3.21.
Exercise 4.33 Write an HDL module for the daughter snail from Exercise 3.22.
Exercise 4.34 Write an HDL module for the soda machine dispenser from
Exercise 3.23.
Exercise 4.35 Write an HDL module for the Gray code counter from
Exercise 3.24.
Exercise 4.36 Write an HDL module for the UP/DOWN Gray code counter
from Exercise 3.25.
Exercise 4.37 Write an HDL module for the FSM from Exercise 3.26.
Chapte
224 CHAPTER FOUR Hardware Description Languages
Exercise 4.38 Write an HDL module for the FSM from Exercise 3.27.
Exercise 4.39 Write an HDL module for the serial two’s complementer from
Question 3.2.
Exercise 4.40 Write an HDL module for the circuit in Exercise 3.28.
Exercise 4.41 Write an HDL module for the circuit in Exercise 3.29.
Exercise 4.42 Write an HDL module for the circuit in Exercise 3.30.
Exercise 4.43 Write an HDL module for the circuit in Exercise 3.31. You may
use the full adder from Section 4.2.5.
Verilog Exercises
The following exercises are specific to Verilog.
Exercise 4.44 What does it mean for a signal to be declared reg in Verilog?
Exercise 4.45 Rewrite the syncbad module from HDL Example 4.30. Use
nonblocking assignments, but change the code to produce a correct
synchronizer with two flip-flops.
Exercise 4.46 Consider the following two Verilog modules. Do they have the
same function? Sketch the hardware each one implies.
module code1 (input clk, a, b, c,
output reg y);
reg x;
always @ (posedge clk) begin
x  a & b;
y  x | c;
end
endmodule
module code2 (input a, b, c, clk,
output reg y);
reg x;
always @ (posedge clk) begin
y  x | c;
x  a & b;
end
endmodule
Exercise 4.47 Repeat Exercise 4.46 if the  is replaced by  in every assignment.
Chapte
Exercises 225
Exercise 4.48 The following Verilog modules show errors that the authors have
seen students make in the laboratory. Explain the error in each module and show
how to fix it.
(a) module latch (input clk,
input [3:0] d,
output reg [3:0] q);
always @ (clk)
if (clk) q  d;
endmodule
(b) module gates (input [3:0] a, b,
output reg [3:0] y1, y2, y3, y4, y5);
always @ (a)
begin
y1  a & b;
y2  a | b;
y3  a  b;
y4  ~(a & b);
y5  ~(a | b);
end
endmodule
(c) module mux2 (input [3:0] d0, d1,
input s,
output reg [3:0] y);
always @ (posedge s)
if (s) y  d1;
else y  d0;
endmodule
(d) module twoflops (input clk,
input d0, d1,
output reg q0, q1);
always @ (posedge clk)
q1  d1;
q0  d0;
endmodule
Chapter 04.
226 CHAPTER FOUR Hardware Description Languages
(e) module FSM (input clk,
input a,
output reg out1, out2);
reg state;
// next state logic and register (sequential)
always @ (posedge clk)
if (state  0) begin
if (a) state  1;
end else begin
if (~a) state  0;
end
always @ (*) // output logic (combinational)
if (state  0) out1  1;
else out2  1;
endmodule
(f) module priority (input [3:0] a,
output reg [3:0] y);
always @ (*)
if (a[3]) y  4’b1000;
else if (a[2]) y  4’b0100;
else if (a[1]) y  4b0010;
else if (a[0]) y  4b0001;
endmodule
(g) module divideby3FSM (input clk,
input reset,
output out);
reg [1:0] state, nextstate;
parameter S0  2b00;
parameter S1  2b01;
parameter S2  2b10;
// State Register
always @ (posedge clk, posedge reset)
if (reset) state  S0;
else state  nextstate;
// Next State Logic
always @ (state)
case (state)
S0: nextstate  S1;
S1: nextstate  S2;
2: nextstate  S0;
endcase
// Output Logic
assign out  (state  S2);
endmodule
Chapter 04.qxd 1/31/07 
Exercises 227
(h) module mux2tri (input [3:0] d0, d1,
input s,
output [3:0] y);
tristate t0 (d0, s, y);
tristate t1 (d1, s, y);
endmodule
(i) module floprsen (input clk,
input reset,
input set,
input [3:0] d,
output reg [3:0] q);
always @ (posedge clk, posedge reset)
if (reset) q  0;
else q  d;
always @ (set)
if (set) q  1;
endmodule
(j) module and3 (input a, b, c,
output reg y);
reg tmp;
always @ (a, b, c)
begin
tmp  a & b;
y  tmp & c;
end
endmodule
VHDL Exercises
The following exercises are specific to VHDL.
Exercise 4.49 In VHDL, why is it necessary to write
q  ‘1’ when state  S0 else ‘0’;
rather than simply
q  (state  S0);
Chapter 0
228 CHAPTER FOUR Hardware Description Languages
Exercise 4.50 Each of the following VHDL modules contains an error. For
brevity, only the architecture is shown; assume that the library use clause and
entity declaration are correct. Explain the error and show how to fix it.
(a) architecture synth of latch is
begin
process (clk) begin
if clk  ‘1’ then q  d;
end if;
end process;
end;
(b) architecture proc of gates is
begin
process (a) begin
y1  a and b;
y2  a or b;
y3  a xor b;
y4  a nand b;
y5  a nor b;
end process;
end;
(c) architecture synth of flop is
begin
process (clk)
if clk’event and clk  ‘1’ then
q  d;
end;
(d) architecture synth of priority is
begin
process (a) begin
if a (3) ‘1’ then y  “1000”;
elsif a (2)  ‘1’ then y  “0100”;
elsif a (1)  ‘1’ then y  “0010”;
elsif a (0)  ‘1’ then y  “0001”;
end if;
end process;
end;
(e) architecture synth of divideby3FSM is
type statetype is (S0, S1, S2);
signal state, nextstate: statetype;
begin
process (clk, reset) begin
if reset  ‘1’ then state  S0;
elsif clk’event and clk  ‘1’ then
state  nextstate;
end if;
end process;
Chapter 04.qxd 1/31/0
Exercises 229
process (state) begin
case state is
when S0  nextstate  S1;
when S1  nextstate  S2;
when S2  nextstate  S0;
end case;
end process;
q  ‘1’ when state
 S0 else ‘0’;
end;
(f) architecture struct of mux2 is
component tristate
port (a: in STD_LOGIC_VECTOR (3 downto 0);
en: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (3 downto 0));
end component;
begin
t0: tristate port map (d0, s, y);
t1: tristate port map (d1, s, y);
end;
(g) architecture asynchronous of flopr is
begin
process (clk, reset) begin
if reset
 ‘1’ then
q  ‘0’;
elsif clk’event and clk
 ‘1’ then
q  d;
end if;
end process;
process (set) begin
if set
 ‘1’ then
q  ‘1’;
end if;
end process;
end;
(h) architecture synth of mux3 is
begin
y  d2 when s(1) else
d1 when s(0) else d0;
end;
Chapter 04.qxd 
Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 4.1 Write a line of HDL code that gates a 32-bit bus called data
with another signal called sel to produce a 32-bit result. If sel is TRUE,
result  data. Otherwise, result should be all 0’s.
Question 4.2 Explain the difference between blocking and nonblocking
assignments in Verilog. Give examples.
Question 4.3 What does the following Verilog statement do?
result  | (data[15:0] & 16hC820);
230 CHAPTER FOUR Hardware Description Languages
Ch


5
5.1 Introduction
5.2 Arithmetic Circuits
5.3 Number Systems
5.4 Sequential Building Blocks
5.5 Memory Arrays
5.6 Logic Arrays
5.7 Summary
Exercises
Interview Questions
Digital Building Blocks
5.1 INTRODUCTION
Up to this point, we have examined the design of combinational and
sequential circuits using Boolean equations, schematics, and HDLs. This
chapter introduces more elaborate combinational and sequential building
blocks used in digital systems. These blocks include arithmetic circuits,
counters, shift registers, memory arrays, and logic arrays. These building
blocks are not only useful in their own right, but they also demonstrate the
principles of hierarchy, modularity, and regularity. The building blocks are
hierarchically assembled from simpler components such as logic gates, multiplexers, and decoders. Each building block has a well-defined interface
and can be treated as a black box when the underlying implementation is
unimportant. The regular structure of each building block is easily
extended to different sizes. In Chapter 7, we use many of these building
blocks to build a microprocessor.
5.2 ARITHMETIC CIRCUITS
Arithmetic circuits are the central building blocks of computers.
Computers and digital logic perform many arithmetic functions: addition, subtraction, comparisons, shifts, multiplication, and division. This
section describes hardware implementations for all of these operations.
5.2.1 Addition
Addition is one of the most common operations in digital systems. We
first consider how to add two 1-bit binary numbers. We then extend to
N-bit binary numbers. Adders also illustrate trade-offs between speed
and complexity.
Half Adder
We begin by building a 1-bit half adder. As shown in Figure 5.1, the half
adder has two inputs, A and B, and two outputs, S and Cout. S is the
233
A B
0
0
1
1
0
1
1
0
out SC
0
0
0
1
0
1
0
1
S = A ⊕ B
Cout = AB
Half
Adder
A B
S
Cout +
FIGURE 5.1 1-bit half adder

234 CHAPTER FIVE Digital Building Blocks
0001
+0101
0110
1
Figure 5.2 Carry bit
A B
0
0
1
1
0
1
1
0
out SC
0
0
0
1
S=A ⊕ B ⊕ Cin
Cout =AB+ACin +BCin
Full
Adder
Cin
0
0
1
1
0
1
0
1
0
1
0
1
0
0
0
0
1
1
1
1
1
0
0
1
0
1
1
1
A B
S
Cout Cin +
FIGURE 5.3 1-bit full adder
A B
S
Cout Cin +
N
N N
Figure 5.4 Carry propagate
adder
sum of A and B. If A and B are both 1, S is 2, which cannot be represented with a single binary digit. Instead, it is indicated with a carry out,
Cout, in the next column. The half adder can be built from an XOR gate
and an AND gate.
In a multi-bit adder, Cout is added or carried in to the next most significant bit. For example, in Figure 5.2, the carry bit shown in blue is the
output, Cout, of the first column of 1-bit addition and the input, Cin, to
the second column of addition. However, the half adder lacks a Cin input
to accept Cout of the previous column. The full adder, described in the
next section, solves this problem.
Full Adder
A full adder, introduced in Section 2.1, accepts the carry in, Cin, as shown
in Figure 5.3. The figure also shows the output equations for S and Cout.
Carry Propagate Adder
An N-bit adder sums two N-bit inputs, A and B, and a carry in, Cin, to
produce an N-bit result, S, and a carry out, Cout. It is commonly called a
carry propagate adder (CPA) because the carry out of one bit propagates
into the next bit. The symbol for a CPA is shown in Figure 5.4; it is
drawn just like a full adder except that A, B, and S are busses rather
than single bits. Three common CPA implementations are called ripplecarry adders, carry-lookahead adders, and prefix adders.
Ripple-Carry Adder
The simplest way to build an N-bit carry propagate adder is to chain
together N full adders. The Cout of one stage acts as the Cin of the next
stage, as shown in Figure 5.5 for 32-bit addition. This is called a ripplecarry adder. It is a good application of modularity and regularity: the full
adder module is reused many times to form a larger system. The ripplecarry adder has the disadvantage of being slow when N is large. S31
depends on C30, which depends on C29, which depends on C28, and so
forth all the way back to Cin, as shown in blue in Figure 5.5. We say that
the carry ripples through the carry chain. The delay of the adder, tripple,
grows directly with the number of bits, as given in Equation 5.1, where
tFA is the delay of a full adder.
t (5.1) ripple  NtFA
S31
A30 B30
S30
A1 B1
S1
A0 B0
S0
C30 C29 C1 C0
Cout + + + +
A31 B31
Cin
Figure 5.5 32-bit ripple-carry adder
Schematics typically show
signals flowing from left to
right. Arithmetic circuits
break this rule because the
carries flow from right to
left (from the least significant column to the most
significant column).
C
Carry-Lookahead Adder
The fundamental reason that large ripple-carry adders are slow is that the
carry signals must propagate through every bit in the adder. A carrylookahead adder is another type of carry propagate adder that solves this
problem by dividing the adder into blocks and providing circuitry to
quickly determine the carry out of a block as soon as the carry in is
known. Thus it is said to look ahead across the blocks rather than waiting
to ripple through all the full adders inside a block. For example, a 32-bit
adder may be divided into eight 4-bit blocks.
Carry-lookahead adders use generate (G) and propagate (P) signals
that describe how a column or block determines the carry out. The ith
column of an adder is said to generate a carry if it produces a carry out
independent of the carry in. The ith column of an adder is guaranteed to
generate a carry, Ci, if Ai and Bi are both 1. Hence Gi, the generate signal for column i, is calculated as Gi  AiBi. The column is said to propagate a carry if it produces a carry out whenever there is a carry in. The
ith column will propagate a carry in, Ci1, if either Ai or Bi is 1. Thus,
Pi  Ai  Bi. Using these definitions, we can rewrite the carry logic for a
particular column of the adder. The ith column of an adder will generate
a carryout, Ci, if it either generates a carry, Gi, or propagates a carry in,
Pi Ci1. In equation form,
(5.2)
The generate and propagate definitions extend to multiple-bit
blocks. A block is said to generate a carry if it produces a carry out
independent of the carry in to the block. The block is said to propagate a
carry if it produces a carry out whenever there is a carry in to the block.
We define Gi:j and Pi:j as generate and propagate signals for blocks spanning columns i through j.
A block generates a carry if the most significant column generates a
carry, or if the most significant column propagates a carry and the previous column generated a carry, and so forth. For example, the generate
logic for a block spanning columns 3 through 0 is
(5.3)
A block propagates a carry if all the columns in the block propagate the
carry. For example, the propagate logic for a block spanning columns 3
through 0 is
(5.4)
Using the block generate and propagate signals, we can quickly compute
the carry out of the block, Ci, using the carry in to the block, Cj.
C (5.5) i  Gi:j  Pi:j Cj
P3:0  P3 P2 P1 P0
G3:0  G3  P3 (G2  P2 (G1  P1 G0))
Ci  Ai Bi  (Ai  Bi) Ci1  Gi  Pi Ci1
5.2 Arithmetic Circuits 235
Throughout the ages, people
have used many devices to
perform arithmetic. Toddlers
count on their fingers (and
some adults stealthily do too).
The Chinese and Babylonians
invented the abacus as early as
2400 BC. Slide rules, invented
in 1630, were in use until the
1970’s, when scientific hand
calculators became prevalent.
Computers and digital calculators are ubiquitous today.
What will be next?
Chapter 05.
Figure 5.6(a) shows a 32-bit carry-lookahead adder composed of
eight 4-bit blocks. Each block contains a 4-bit ripple-carry adder and
some lookahead logic to compute the carry out of the block given the
carry in, as shown in Figure 5.6(b). The AND and OR gates needed to
compute the single-bit generate and propagate signals, Gi and Pi, from
Ai and Bi are left out for brevity. Again, the carry-lookahead adder
demonstrates modularity and regularity.
All of the CLA blocks compute the single-bit and block generate and
propagate signals simultaneously. The critical path starts with computing G0 and G3:0 in the first CLA block. Cin then advances directly to
Cout through the AND/OR gate in each block until the last. For a large
adder, this is much faster than waiting for the carries to ripple through
each consecutive bit of the adder. Finally, the critical path through the
last block contains a short ripple-carry adder. Thus, an N-bit adder
divided into k-bit blocks has a delay
tCLA  tpg  tpg (5.6) block 
N
k  1  tAND OR  ktFA
236 CHAPTER FIVE Digital Building Blocks
B0
+ + + +
P3:0
G3
P3
G2
P2
G1
P1
G0
P3
P2
P1
P0
G3:0
Cin
Cout
A0
S0
C0
B1 A1
S1
C1
B2 A2
S2
C2
B3 A3
S3
Cin
(b)
(a)
B3:0 A3:0
S3:0
Cin
B7:4 A7:4
S7:4
C7 C3
B27:24 A27:24
S27:24
C23
B31:28 A31:28
S31:28
4-bit CLA
Block
4-bit CLA
Block
4-bit CLA
Block
4-bit CLA
Block
C27
Cout
Figure 5.6 (a) 32-bit carrylookahead adder (CLA), (b) 4-bit
CLA block
Chapte
where tpg is the delay of the individual generate/propagate gates (a single
AND or OR gate) to generate P and G, tpg_block is the delay to find the generate/propagate signals Pi:j and Gi:j for a k-bit block, and tAND_OR is the
delay from Cin to Cout through the AND/OR logic of the k-bit CLA block.
For N  16, the carry-lookahead adder is generally much faster than the
ripple-carry adder. However, the adder delay still increases linearly with N.
Example 5.1 RIPPLE-CARRY ADDER AND CARRY-LOOKAHEAD
ADDER DELAY
Compare the delays of a 32-bit ripple-carry adder and a 32-bit carry-lookahead
adder with 4-bit blocks. Assume that each two-input gate delay is 100 ps and
that a full adder delay is 300 ps.
Solution: According to Equation 5.1, the propagation delay of the 32-bit ripplecarry adder is 32  300 ps  9.6 ns.
The CLA has tpg  100 ps, tpg_block  6  100 ps  600 ps, and tAND_OR  2 
100 ps  200 ps. According to Equation 5.6, the propagation delay of the 32-bit
carry-lookahead adder with 4-bit blocks is thus 100 ps  600 ps  (32/4  1) 
200 ps  (4  300 ps)  3.3 ns, almost three times faster than the ripple-carry
adder.
Prefix Adder*
Prefix adders extend the generate and propagate logic of the carrylookahead adder to perform addition even faster. They first compute G
and P for pairs of columns, then for blocks of 4, then for blocks of 8,
then 16, and so forth until the generate signal for every column is
known. The sums are computed from these generate signals.
In other words, the strategy of a prefix adder is to compute the carry
in, Ci1, for each column, i, as quickly as possible, then to compute the
sum, using
Si  (Ai  Bi)  Ci1 (5.7)
Define column i  1 to hold Cin, so G1  Cin and P1  0. Then
Ci1  Gi1:1 because there will be a carry out of column i1 if
the block spanning columns i1 through 1 generates a carry. The
generated carry is either generated in column i1 or generated in a
previous column and propagated. Thus, we rewrite Equation 5.7 as
Si  (Ai  Bi)  Gi1:1 (5.8)
Hence, the main challenge is to rapidly compute all the block generate
signals G1:1, G0:1, G1:1, G2:1, . . . , GN2:1. These signals, along
with P1:1, P0:1, P1:1, P2:1, . . . , PN2:1, are called prefixes.
5.2 Arithmetic Circuits 237
Early computers used ripple
carry adders, because
components were expensive
and ripple carry adders used
the least hardware. Virtually
all modern PCs use prefix
adders on critical paths,
because transistors are now
cheap and speed is of great
importance.
Chapter 05.qxd 1/27/07 10:27 AM Page 237
Figure 5.7 shows an N  16-bit prefix adder. The adder begins
with a precomputation to form Pi and Gi for each column from Ai and
Bi using AND and OR gates. It then uses log2N  4 levels of black
cells to form the prefixes of Gi:j and Pi:j. A black cell takes inputs from
the upper part of a block spanning bits i:k and from the lower part
spanning bits k1:j. It combines these parts to form generate and
propagate signals for the entire block spanning bits i:j, using the
equations.
(5.9)
(5.10)
In other words, a block spanning bits i:j will generate a carry if the
upper part generates a carry or if the upper part propagates a carry generated in the lower part. The block will propagate a carry if both the
Pi:j  Pi:k Pk1:j
Gi:j  Gi:k  Pi:k Gk1:j
238 CHAPTER FIVE Digital Building Blocks
0:–1
–1
2:1
–1 1:–12:
012
4:3
3
6:5
5:36:3
456
–1 5:–16: –1 3:–14:
8:7
7
10:9
9:710:7
8910
12:11
11
14:13
14:11 13:11
121314
13:714:7 11:712:7
14:-1 13:–1 12:-1 11:–1 10:–1 9:–1 8:–1 7:–1
15
0123456789101112131415
Ai Bi
Pi:i Gi:i
Pi:kPk–1:j Gi:k Gk–1:j
Pi:j Gi:j
Gi–1:–1Ai Bi
Si
i
i:j
Legend i
Figure 5.7 16-bit prefix adder
Chapter
upper and lower parts propagate the carry. Finally, the prefix adder computes the sums using Equation 5.8.
In summary, the prefix adder achieves a delay that grows logarithmically rather than linearly with the number of columns in the adder.
This speedup is significant, especially for adders with 32 or more bits,
but it comes at the expense of more hardware than a simple carrylookahead adder. The network of black cells is called a prefix tree.
The general principle of using prefix trees to perform computations
in time that grows logarithmically with the number of inputs is a powerful technique. With some cleverness, it can be applied to many other
types of circuits (see, for example, Exercise 5.7).
The critical path for an N-bit prefix adder involves the precomputation
of Pi and Gi followed by log2N stages of black prefix cells to obtain all the
prefixes. Gi1:1 then proceeds through the final XOR gate at the bottom
to compute Si
. Mathematically, the delay of an N-bit prefix adder is
(5.11)
where tpg_prefix is the delay of a black prefix cell.
Example 5.2 PREFIX ADDER DELAY
Compute the delay of a 32-bit prefix adder. Assume that each two-input gate
delay is 100 ps.
Solution: The propagation delay of each black prefix cell, tpg_prefix, is 200 ps
(i.e., two gate delays). Thus, using Equation 5.11, the propagation delay of
the 32-bit prefix adder is 100 ps  log2(32)  200 ps  100 ps  1.2 ns,
which is about three times faster than the carry-lookahead adder and eight
times faster than the ripple-carry adder from Example 5.1. In practice, the
benefits are not quite this great, but prefix adders are still substantially faster
than the alternatives.
Putting It All Together
This section introduced the half adder, full adder, and three types of
carry propagate adders: ripple-carry, carry-lookahead, and prefix adders.
Faster adders require more hardware and therefore are more expensive
and power-hungry. These trade-offs must be considered when choosing
an appropriate adder for a design.
Hardware description languages provide the  operation to specify
a CPA. Modern synthesis tools select among many possible implementations, choosing the cheapest (smallest) design that meets the speed
requirements. This greatly simplifies the designer’s job. HDL Example
5.1 describes a CPA with carries in and out.
tPA  tpg  log2N(tpgprefix )  tXOR
5.2 Arithmetic Circuits 239
Chapt
240 CHAPTER FIVE Digital Building Blocks
Verilog
module adder #(parameter N  8)
(input [N1:0] a, b,
input cin,
output [N1:0] s,
output cout);
assign {cout, s}  a  b  cin;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity adder is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
cin: in STD_LOGIC;
s: out STD_LOGIC_VECTOR(N1 downto 0);
cout: out STD_LOGIC);
end;
architecture synth of adder is
signal result: STD_LOGIC_VECTOR(N downto 0);
begin
result  (“0” & a)  (“0” & b)  cin;
s  result (N1 downto 0);
cout  result (N);
end;
HDL Example 5.1 ADDER
5.2.2 Subtraction
Recall from Section 1.4.6 that adders can add positive and negative
numbers using two’s complement number representation. Subtraction is
almost as easy: flip the sign of the second number, then add. Flipping the
sign of a two’s complement number is done by inverting the bits and
adding 1.
To compute Y  A  B, first create the two’s complement of B:
Invert the bits of B to obtain and add 1 to get . Add this
quantity to A to get . This sum can be performed with a single CPA by adding with Cin  1. Figure 5.9
shows the symbol for a subtractor and the underlying hardware for performing Y  A  B. HDL Example 5.2 describes a subtractor.
5.2.3 Comparators
A comparator determines whether two binary numbers are equal or if
one is greater or less than the other. A comparator receives two N-bit
binary numbers, A and B. There are two common types of comparators.
A  B
Y  A  B  1  A  B
B B  B  1
A B
–
Y
(a)
N N
N
+
Y
A B
(b)
N N
N
N
Figure 5.9 Subtractor:
(a) symbol, (b) implementation
+
cout [8]
s[7:0] [7:0]
cin
b[7:0]
a[7:0]
[7:0]
[8:0]
[7:0] [7:0]
Figure 5.8 Synthesized adder
Chapter 05.qxd 1/27/0
5.2 Arithmetic Circuits 241
Verilog
module subtractor #(parameter N  8)
(input [N1:0] a, b,
output [N1:0] y);
assign y  a  b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity subtractor is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
y: out STD_LOGIC_VECTOR(N1 downto 0));
end;
architecture synth of subtractor is
begin
y  a  b;
end;
HDL Example 5.2 SUBTRACTOR
An equality comparator produces a single output indicating whether A is
equal to B (A  B). A magnitude comparator produces one or more
outputs indicating the relative values of A and B.
The equality comparator is the simpler piece of hardware. Figure
5.11 shows the symbol and implementation of a 4-bit equality comparator. It first checks to determine whether the corresponding bits in each
column of A and B are equal, using XNOR gates. The numbers are
equal if all of the columns are equal.
Magnitude comparison is usually done by computing A  B and
looking at the sign (most significant bit) of the result, as shown in Figure
5.12. If the result is negative (i.e., the sign bit is 1), then A is less than B.
Otherwise A is greater than or equal to B.
HDL Example 5.3 shows how to use various comparison operations.
A3
B3
A2
B2
A1
B1
A0
B0
Equal
(b)
Figure 5.11 4-bit equality
comparator: (a) symbol,
(b) implementation
+ y[7:0] [7:0] b[7:0]
a[7:0] [7:0]
[7:0]
1
Figure 5.10 Synthesized subtractor
(a)
=
A B
Equal
4 4
A < B
–
BA
[N –1]
N
N N
Figure 5.12 N-bit magnitude
comparator
Chapter 05.qx
242 CHAPTER FIVE Digital Building Blocks
Verilog
module comparators # (parameter N  8)
(input [N1:0] a, b,
output eq, neq,
output lt, lte,
output gt, gte);
assign eq  (a  b);
assign neq  (a ! b);
assign lt  (a  b);
assign lte  (a  b);
assign gt  (a  b);
assign gte  (a  b);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
entity comparators is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
eq, neq, lt,
lte, gt, gte: out STD_LOGIC);
end;
architecture synth of comparators is
begin
eq  ‘1’ when (a  b) else ‘0’;
neq  ‘1’ when (a / b) else ‘0’;
lt  ‘1’ when (a  b) else ‘0’;
lte  ‘1’ when (a  b) else ‘0’;
gt  ‘1’ when (a  b) else ‘0’;
gte  ‘1’ when (a  b) else ‘0’;
end;
HDL Example 5.3 COMPARATORS
5.2.4 ALU
An Arithmetic/Logical Unit (ALU) combines a variety of mathematical
and logical operations into a single unit. For example, a typical ALU
might perform addition, subtraction, magnitude comparison, AND, and
OR operations. The ALU forms the heart of most computer systems.
Figure 5.14 shows the symbol for an N-bit ALU with N-bit inputs
and outputs. The ALU receives a control signal, F, that specifies which
=
<
<
gte
gt
lte
lt
neq
eq
b[7:0]
a[7:0] [7:0]
[7:0]
[7:0]
[7:0]
[7:0]
[7:0]
Figure 5.13 Synthesized comparators
ALU
N N
N
3
A B
Y
F
Figure 5.14 ALU symbol
Chapter 05.qxd 1/27/07 10
function to perform. Control signals will generally be shown in blue to
distinguish them from the data. Table 5.1 lists typical functions that the
ALU can perform. The SLT function is used for magnitude comparison
and will be discussed later in this section.
Figure 5.15 shows an implementation of the ALU. The ALU contains an N-bit adder and N two-input AND and OR gates. It also
contains an inverter and a multiplexer to optionally invert input B when
the F2 control signal is asserted. A 4:1 multiplexer chooses the desired
function based on the F1:0 control signals.
5.2 Arithmetic Circuits 243
+
A B
Cout
Y
F2
F1:0
[N–1] S
N N
N
N
N N N N
N
2
1
0
3
2
1
0
extend
Zero
BB
Figure 5.15 N-bit ALU
Table 5.1 ALU operations
F2:0 Function
000 A AND B
001 A OR B
010 A  B
011 not used
100 A AND
101 A OR
110 A  B
111 SLT
B
B
C
More specifically, the arithmetic and logical blocks in the ALU
operate on A and BB. BB is either B or , depending on F2. If F1:0  00,
the output multiplexer chooses A AND BB. If F1:0  01, the ALU
computes A OR BB. If F1:0  10, the ALU performs addition or subtraction. Note that F2 is also the carry in to the adder. Also remember that
in two’s complement arithmetic. If F2  0, the ALU computes A  B. If F2  1, the ALU computes .
When F2:0  111, the ALU performs the set if less than (SLT) operation. When A  B, Y  1. Otherwise, Y  0. In other words, Y is set to
1 if A is less than B.
SLT is performed by computing S  A  B. If S is negative (i.e., the
sign bit is set), A  B. The zero extend unit produces an N-bit output by
concatenating its 1-bit input with 0’s in the most significant bits. The
sign bit (the N1th bit) of S is the input to the zero extend unit.
Example 5.3 SET LESS THAN
Configure a 32-bit ALU for the SLT operation. Suppose A  2510 and B  3210.
Show the control signals and output, Y.
Solution: Because A  B, we expect Y to be 1. For SLT, F2:0  111. With F2  1,
this configures the adder unit as a subtractor with an output, S, of 2510  3210
710  1111 . . . 10012. With F1:0  11, the final multiplexer sets Y  S31  1.
Some ALUs produce extra outputs, called flags, that indicate information about the ALU output. For example, an overflow flag indicates
that the result of the adder overflowed. A zero flag indicates that the
ALU output is 0.
The HDL for an N-bit ALU is left to Exercise 5.9. There are many
variations on this basic ALU that support other functions, such as XOR
or equality comparison.
5.2.5 Shifters and Rotators
Shifters and rotators move bits and multiply or divide by powers of 2. As
the name implies, a shifter shifts a binary number left or right by a specified
number of positions. There are several kinds of commonly used shifters:
 Logical shifter—shifts the number to the left (LSL) or right (LSR)
and fills empty spots with 0’s.
Ex: 11001 LSR 2  00110; 11001 LSL 2  00100
 Arithmetic shifter—is the same as a logical shifter, but on right shifts
fills the most significant bits with a copy of the old most significant
bit (msb). This is useful for multiplying and dividing signed numbers
A  B  1  A  B
B  1  B
B
244 CHAPTER FIVE Digital Building Blocks
Chapter 05.qxd 1/27/07 10:27 A
(see Sections 5.2.6 and 5.2.7). Arithmetic shift left (ASL) is the same
as logical shift left (LSL).
Ex: 11001 ASR 2  11110; 11001 ASL 2  00100
 Rotator—rotates number in circle such that empty spots are filled
with bits shifted off the other end.
Ex: 11001 ROR 2  01110; 11001 ROL 2  00111
An N-bit shifter can be built from N N:1 multiplexers. The input is
shifted by 0 to N1 bits, depending on the value of the log2N-bit select
lines. Figure 5.16 shows the symbol and hardware of 4-bit shifters. The
operators , , and  typically indicate shift left, logical shift
right, and arithmetic shift right, respectively. Depending on the value of
the 2-bit shift amount, shamt1:0, the output, Y, receives the input, A,
shifted by 0 to 3 bits. For all shifters, when shamt1:0  00, Y  A.
Exercise 5.14 covers rotator designs.
A left shift is a special case of multiplication. A left shift by N bits
multiplies the number by 2N. For example, 0000112  4  1100002 is
equivalent to 310  24  4810.
5.2 Arithmetic Circuits 245
Figure 5.16 4-bit shifters: (a) shift left, (b) logical shift right, (c) arithmetic shift right
shamt A3 A2 A1 A0 1:0
Y3
Y2
Y1
Y0
(a)
<<
S1:0
S1:0
S1:0
S1:0
00
01
10
11
00
01
10
11
00
01
10
11
00
01
10
11
2
4 4
2
(b)
A3 A2 A1 A0
Y3
Y2
Y1
Y0
shamt1:0
00
01
10
11
A3:0 Y3:0
shamt1:0
>>
S1:0
S1:0
S1:0
S1:0
00
01
10
11
00
01
10
11
00
01
10
11
2
4 4
2
(c)
A3 A2 A1 A0
Y3
Y2
Y1
Y0
shamt1:0
>>>
S1:0
S1:0
S1:0
S1:0
00
01
10
11
00
01
10
11
00
01
10
11
00
01
10
11
2
4 4
2
A3:0 Y3:0
shamt1:0
A3:0 Y3:0
shamt1:0
Chapter 05
An arithmetic right shift is a special case of division. An arithmetic
right shift by N bits divides the number by 2N. For example, 111002
 2  111112 is equivalent to 410/22  110.
5.2.6 Multiplication*
Multiplication of unsigned binary numbers is similar to decimal multiplication but involves only 1’s and 0’s. Figure 5.17 compares multiplication
in decimal and binary. In both cases, partial products are formed by
multiplying a single digit of the multiplier with the entire multiplicand.
The shifted partial products are summed to form the result.
In general, an N  N multiplier multiplies two N-bit numbers and
produces a 2N-bit result. The partial products in binary multiplication
are either the multiplicand or all 0’s. Multiplication of 1-bit binary numbers is equivalent to the AND operation, so AND gates are used to form
the partial products.
Figure 5.18 shows the symbol, function, and implementation of a 4
 4 multiplier. The multiplier receives the multiplicand and multiplier,
A and B, and produces the product, P. Figure 5.18(b) shows how partial
products are formed. Each partial product is a single multiplier bit (B3,
B2, B1, or B0) AND the multiplicand bits (A3, A2, A1, A0). With N-bit
246 CHAPTER FIVE Digital Building Blocks
Figure 5.17 Multiplication:
(a) decimal, (b) binary
Figure 5.18 4  4 multiplier:
(a) symbol, (b) function,
(c) implementation
230
× 42
(a)
460
+ 920
9660
2 30 × 42 = 9660
multiplier
multiplicand
partial
products
result
0101
0111
5 × 7 = 35
0101
0101
0101
0000
×
+
0100011
(b)
(a)
x
A B
P
4 4
8
(b)
× B3 B2 B1 B0
A3B0 A2B0 A1B0 A0B0
A3 A2 A1 A0
A3B1 A2B1 A1B1 A0B1
A3B2 A2B2 A1B2 A0B2
+ A3B3 A2B3 A1B3 A0B3
P7 P6 P5 P4 P3 P2 P1 P0
0
P2
0
0
(c)
0
P7 P6 P5 P4 P3 P1 P0
A3 A2 A1 A0
B0
B1
B2
B3
Chap
operands, there are N partial products and N  1 stages of 1-bit adders.
For example, for a 4  4 multiplier, the partial product of the first row
is B0 AND (A3, A2, A1, A0). This partial product is added to the shifted
second partial product, B1 AND (A3, A2, A1, A0). Subsequent rows of
AND gates and adders form and add the remaining partial products.
The HDL for a multiplier is in HDL Example 5.4. As with adders,
many different multiplier designs with different speed/cost trade-offs
exist. Synthesis tools may pick the most appropriate design given the
timing constraints.
5.2 Arithmetic Circuits 247
Verilog
module multiplier # (parameter N  8)
(input [N1:0] a, b,
output [2*N1:0] y);
assign y  a * b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity multiplier is
generic (N: integer : 8);
port (a, b: in STD_LOGIC_VECTOR(N1 downto 0);
y: out STD_LOGIC_VECTOR(2*N1 downto 0));
end;
architecture synth of multiplier is
begin
y  a * b;
end;
HDL Example 5.4 MULTIPLIER
5.2.7 Division*
Binary division can be performed using the following algorithm for normalized unsigned numbers in the range [2N1, 2N1]:
R  A
for i  N1 to 0
D  R  B
if D  0 then Qi  0, R  R // R  B
else Qi  1, R  D // R 	 B
if i =/ 0 then R  2R
The partial remainder, R, is initialized to the dividend, A. The divisor, B,
is repeatedly subtracted from this partial remainder to determine whether
it fits. If the difference, D, is negative (i.e., the sign bit of D is 1), then the
quotient bit, Qi, is 0 and the difference is discarded. Otherwise, Qi is 1,
Figure 5.19 Synthesized multiplier
* y[15:0] [15:0]
b[7:0]
a[7:0] [7:0]
[7:0]
Chapter 05.qxd 1/27/0
and the partial remainder is updated to be the difference. In any event,
the partial remainder is then doubled (left-shifted by one column), and
the process repeats. The result satisfies
Figure 5.20 shows a schematic of a 4-bit array divider. The divider
computes A/B and produces a quotient, Q, and a remainder, R. The legend shows the symbol and schematic for each block in the array divider.
The signal P indicates whether R  B is negative. It is obtained from the
Cout output of the leftmost block in the row, which is the sign of the
difference.
The delay of an N-bit array divider increases proportionally to N2
because the carry must ripple through all N stages in a row before the
sign is determined and the multiplexer selects R or D. This repeats for all
N rows. Division is a slow and expensive operation in hardware and
therefore should be used as infrequently as possible.
5.2.8 Further Reading
Computer arithmetic could be the subject of an entire text. Digital
Arithmetic, by Ercegovac and Lang, is an excellent overview of the
entire field. CMOS VLSI Design, by Weste and Harris, covers highperformance circuit designs for arithmetic operations.
A
B  Q  R
B2
(N1)
.
248 CHAPTER FIVE Digital Building Blocks
Figure 5.20 Array divider
+
R B
D
R′
P
Cout Cin
0
1
R B
P R′
Cout Cin 1
A3 A2 A1 A0
Q3
1
1 0
Q2
1
1 0
Q1
1
1 0
Q0
B3 B2 B1 B0
R3 R2 R1 R0
Legend
Chapte
5.3 NUMBER SYSTEMS
Computers operate on both integers and fractions. So far, we have only
considered representing signed or unsigned integers, as introduced in
Section 1.4. This section introduces fixed- and floating-point number
systems that can also represent rational numbers. Fixed-point numbers
are analogous to decimals; some of the bits represent the integer part,
and the rest represent the fraction. Floating-point numbers are analogous to scientific notation, with a mantissa and an exponent.
5.3.1 Fixed-Point Number Systems
Fixed-point notation has an implied binary point between the integer and
fraction bits, analogous to the decimal point between the integer and fraction digits of an ordinary decimal number. For example, Figure 5.21(a)
shows a fixed-point number with four integer bits and four fraction bits.
Figure 5.21(b) shows the implied binary point in blue, and Figure 5.21(c)
shows the equivalent decimal value.
Signed fixed-point numbers can use either two’s complement or
sign/magnitude notations. Figure 5.22 shows the fixed-point representation of 2.375 using both notations with four integer and four fraction
bits. The implicit binary point is shown in blue for clarity. In sign/magnitude form, the most significant bit is used to indicate the sign. The two’s
complement representation is formed by inverting the bits of the
absolute value and adding a 1 to the least significant (rightmost) bit. In
this case, the least significant bit position is in the 24 column.
Like all binary number representations, fixed-point numbers are just a
collection of bits. There is no way of knowing the existence of the binary
point except through agreement of those people interpreting the number.
Example 5.4 ARITHMETIC WITH FIXED-POINT NUMBERS
Compute 0.75  0.625 using fixed-point numbers.
Solution: First convert 0.625, the magnitude of the second number, to fixedpoint binary notation. 0.625 	 21, so there is a 1 in the 21 column, leaving
0.6250.5  0.125. Because 0.125  22, there is a 0 in the 22 column. Because
0.125 	 23, there is a 1 in the 23 column, leaving 0.125  0.125  0. Thus, there
must be a 0 in the 24 column. Putting this all together, 0.62510  0000.10102
Use two’s complement representation for signed numbers so that addition works
correctly. Figure 5.23 shows the conversion of 0.625 to fixed-point two’s complement notation.
Figure 5.24 shows the fixed-point binary addition and the decimal equivalent for
comparison. Note that the leading 1 in the binary fixed-point addition of Figure
5.24(a) is discarded from the 8-bit result.
5.3 Number Systems 249
Figure 5.21 Fixed-point notation
of 6.75 with four integer bits
and four fraction bits
(a) 01101100
(b) 0110.1100
(c) 22 + 21 + 2–1 + 2–2 = 6.75
Figure 5.22 Fixed-point
representation of 2.375:
(a) absolute value, (b) sign
and magnitude, (c) two’s
complement
(a) 0010.0110
(b) 1010.0110
(c) 1101.1010
Fixed-point number systems
are commonly used for banking and financial applications
that require precision but not
a large range.
Chapter 05.qxd 1/
250 CHAPTER FIVE Digital Building Blocks
Figure 5.23 Fixed-point two’s
complement conversion
0000.1010
1111.0101
+ 1 Add 1
1111.0110 Two's Complement
One's Complement
Binary Magnitude
Figure 5.24 Addition: (a) binary
fixed-point (b) decimal
equivalent
0000.1100
10000.0010
+ 1111.0110
0.75
 0.125
+ (–0.625)
(a) (b)
Figure 5.25 Floating-point
numbers
Figure 5.26 32-bit floatingpoint version 1
± M × BE
0 00000111 111 0010 0000 0000 0000 0000
Sign Exponent Mantissa
1 bit 8 bits 23 bits
5.3.2 Floating-Point Number Systems*
Floating-point numbers are analogous to scientific notation. They circumvent the limitation of having a constant number of integer and fractional bits, allowing the representation of very large and very small
numbers. Like scientific notation, floating-point numbers have a sign,
mantissa (M), base (B), and exponent (E), as shown in Figure 5.25. For
example, the number 4.1  103 is the decimal scientific notation for
4100. It has a mantissa of 4.1, a base of 10, and an exponent of 3. The
decimal point floats to the position right after the most significant digit.
Floating-point numbers are base 2 with a binary mantissa. 32 bits are
used to represent 1 sign bit, 8 exponent bits, and 23 mantissa bits.
Example 5.5 32-BIT FLOATING-POINT NUMBERS
Show the floating-point representation of the decimal number 228.
Solution: First convert the decimal number into binary: 22810  111001002
1.110012  27. Figure 5.26 shows the 32-bit encoding, which will be modified
later for efficiency. The sign bit is positive (0), the 8 exponent bits give the value
7, and the remaining 23 bits are the mantissa.
In binary floating-point, the first bit of the mantissa (to the left of
the binary point) is always 1 and therefore need not be stored. It is
called the implicit leading one. Figure 5.27 shows the modified floatingpoint representation of 22810  111001002  20  1.110012  27.
The implicit leading one is not included in the 23-bit mantissa for
efficiency. Only the fraction bits are stored. This frees up an extra bit
for useful data.
Chap
We make one final modification to the exponent field. The exponent
needs to represent both positive and negative exponents. To do so, floating-point uses a biased exponent, which is the original exponent plus a
constant bias. 32-bit floating-point uses a bias of 127. For example, for
the exponent 7, the biased exponent is 7  127  134  100001102.
For the exponent 4, the biased exponent is: 4  127  123
011110112. Figure 5.28 shows 1.110012  27 represented in floatingpoint notation with an implicit leading one and a biased exponent of
134 (7  127). This notation conforms to the IEEE 754 floating-point
standard.
Special Cases: 0, 
, and NaN
The IEEE floating-point standard has special cases to represent numbers
such as zero, infinity, and illegal results. For example, representing the
number zero is problematic in floating-point notation because of the
implicit leading one. Special codes with exponents of all 0’s or all 1’s are
reserved for these special cases. Table 5.2 shows the floating-point
representations of 0, 
, and NaN. As with sign/magnitude numbers,
floating-point has both positive and negative 0. NaN is used for numbers that don’t exist, such as or log2 (5).
Single- and Double-Precision Formats
So far, we have examined 32-bit floating-point numbers. This format is
also called single-precision, single, or float. The IEEE 754 standard also
√1
5.3 Number Systems 251
Figure 5.27 Floating-point
version 2 0 00000111 110 0100 0000 0000 0000 0000
Sign Exponent Fraction
1 bit 8 bits 23 bits
Figure 5.28 IEEE 754 floatingpoint notation
0 10000110
Sign Biased
Exponent
Fraction
1 bit 8 bits 23 bits
110 0100 0000 0000 0000 0000
Table 5.2 IEEE 754 floating-point notations for 0, , and NaN
Number Sign Exponent Fraction
0 X 00000000 00000000000000000000000
 0 11111111 00000000000000000000000
 1 11111111 00000000000000000000000
NaN X 11111111 non-zero
As may be apparent, there are
many reasonable ways to represent floating-point numbers.
For many years, computer
manufacturers used incompatible floating-point formats.
Results from one computer
could not directly be interpreted by another computer.
The Institute of Electrical
and Electronics Engineers
solved this problem by defining the IEEE 754 floatingpoint standard in 1985
defining floating-point numbers. This floating-point format is now almost universally
used and is the one discussed
in this section.
Chapter 05.
defines 64-bit double-precision (also called double) numbers that provide greater precision and greater range. Table 5.3 shows the number of
bits used for the fields in each format.
Excluding the special cases mentioned earlier, normal single-precision
numbers span a range of 
1.175494  1038 to 
3.402824  1038.
They have a precision of about seven significant decimal digits (because
224 ≈ 107). Similarly, normal double-precision numbers span a range
of 
2.22507385850720  10308 to 
1.79769313486232  10308 and
have a precision of about 15 significant decimal digits.
Rounding
Arithmetic results that fall outside of the available precision must round
to a neighboring number. The rounding modes are: (1) round down,
(2) round up, (3) round toward zero, and (4) round to nearest. The
default rounding mode is round to nearest. In the round to nearest
mode, if two numbers are equally near, the one with a 0 in the least
significant position of the fraction is chosen.
Recall that a number overflows when its magnitude is too large to
be represented. Likewise, a number underflows when it is too tiny to be
represented. In round to nearest mode, overflows are rounded up to 

and underflows are rounded down to 0.
Floating-Point Addition
Addition with floating-point numbers is not as simple as addition with
two’s complement numbers. The steps for adding floating-point numbers
with the same sign are as follows:
1. Extract exponent and fraction bits.
2. Prepend leading 1 to form the mantissa.
3. Compare exponents.
4. Shift smaller mantissa if necessary.
5. Add mantissas.
6. Normalize mantissa and adjust exponent if necessary.
7. Round result.
8. Assemble exponent and fraction back into floating-point number.
252 CHAPTER FIVE Digital Building Blocks
Floating-point cannot represent some numbers exactly,
like 1.7. However, when you
type 1.7 into your calculator,
you see exactly 1.7, not
1.69999. .. . To handle this,
some applications, such as
calculators and financial
software, use binary coded
decimal (BCD) numbers or
formats with a base 10 exponent. BCD numbers encode
each decimal digit using four
bits with a range of 0 to 9.
For example the BCD fixedpoint notation of 1.7 with
four integer bits and four
fraction bits would be
0001.0111. Of course,
nothing is free. The cost is
increased complexity in
arithmetic hardware and
wasted encodings (A–F
encodings are not used), and
thus decreased performance.
So for compute-intensive
applications, floating-point
is much faster.
Table 5.3 Single- and double-precision floating-point formats
Format Total Bits Sign Bits Exponent Bits Fraction Bits
single 32 1 8 23
double 64 1 11 52
Chap
Figure 5.29 shows the floating-point addition of 7.875 (1.11111  22)
and 0.1875 (1.1  23). The result is 8.0625 (1.0000001  23). After the
fraction and exponent bits are extracted and the implicit leading 1 is
prepended in steps 1 and 2, the exponents are compared by subtracting the
smaller exponent from the larger exponent. The result is the number of bits
by which the smaller number is shifted to the right to align the implied
binary point (i.e., to make the exponents equal) in step 4. The aligned numbers are added. Because the sum has a mantissa that is greater than or
equal to 2.0, the result is normalized by shifting it to the right one bit and
incrementing the exponent. In this example, the result is exact, so no
rounding is necessary. The result is stored in floating-point notation by
removing the implicit leading one of the mantissa and prepending the
sign bit.
5.3 Number Systems 253
Figure 5.29 Floating-point
addition
111 1100 0000 0000 0000 0000
Step 1
10000001
Exponent
01111100 100 0000 0000 0000 0000 0000
1.111 1100 0000 0000 0000 0000
Step 2
10000001
01111100 1.100 0000 0000 0000 0000 0000
1.111 1100 0000 0000 0000 0000
Step 3
10000001
– 01111100 1.100 0000 0000 0000 0000 0000
101 (shift amount)
1.111 1100 0000 0000 0000 0000
Step 4
10000001
10000001 0.000 0110 0000 0000 0000 0000
1.111 1100 0000 0000 0000 0000
Step 5
10000001
10000001 + 0.000 0110 0000 0000 0000 0000
10.000 0010 0000 0000 0000 0000
Step 6
Step 7
Floating-point numbers
1.000 0001 0000 0000 0000 0000
10000001
1
10.000 0010 0000 0000 0000 0000 >> 1
10000010
0
0
Step 8 0
(No rounding necessary)
Fraction
111 1100 0000 0000 0000 0000
100 0000 0000 0000 0000 0000
10000001
01111100
10000010 000 0001 0000 0000 0000 0000
00000
+
Floating-point arithmetic is
usually done in hardware to
make it fast. This hardware,
called the floating-point unit
(FPU), is typically distinct
from the central processing
unit (CPU). The infamous
floating-point division (FDIV)
bug in the Pentium FPU cost
Intel $475 million to recall
and replace defective chips.
The bug occurred simply
because a lookup table was
not loaded correctly.
C
5.4 SEQUENTIAL BUILDING BLOCKS
This section examines sequential building blocks, including counters and
shift registers.
5.4.1 Counters
An N-bit binary counter, shown in Figure 5.30, is a sequential arithmetic
circuit with clock and reset inputs and an N-bit output, Q. Reset initializes the output to 0. The counter then advances through all 2N possible
outputs in binary order, incrementing on the rising edge of the clock.
Figure 5.31 shows an N-bit counter composed of an adder and a resettable register. On each cycle, the counter adds 1 to the value stored in the register. HDL Example 5.5 describes a binary counter with asynchronous reset.
Other types of counters, such as Up/Down counters, are explored in
Exercises 5.37 through 5.40.
254 CHAPTER FIVE Digital Building Blocks
Figure 5.31 N-bit counter
Figure 5.30 Counter symbol
Q
CLK
Reset
N
N
1
CLK
Reset
B
S
A N
Q3:0 N
r
Verilog
module counter #(parameter N  8)
(input clk,
input reset,
output reg [N1:0] q);
always @ (posedge clk or posedge reset)
if (reset) q  0;
else q  q  1;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
use IEEE.STD_LOGIC_ARITH.ALL;
entity counter is
generic (N: integer : 8);
port (clk,
reset: in STD_LOGIC;
q: buffer STD_LOGIC_VECTOR(N1 downto 0));
end;
architecture synth of counter is
begin
process (clk, reset) begin
if reset  ‘1’ then
q  CONV_STD_LOGIC_VECTOR (0, N);
elsif clk’event and clk  ‘1’ then
q  q  1;
end if;
end process;
end;
HDL Example 5.5 COUNTER
Figure 5.32 Synthesized counter
+ R
q[7:0] [7:0]
reset
clk
[7:0]
1 Q[7:0] [7:0] D[7:0]
Chapter 05
5.4.2 Shift Registers
A shift register has a clock, a serial input, Sin, a serial output, Sout, and
N parallel outputs, QN-1:0, as shown in Figure 5.33. On each rising
edge of the clock, a new bit is shifted in from Sin and all the subsequent contents are shifted forward. The last bit in the shift register is
available at Sout. Shift registers can be viewed as serial-to-parallel converters. The input is provided serially (one bit at a time) at Sin. After
N cycles, the past N inputs are available in parallel at Q.
A shift register can be constructed from N flip-flops connected in
series, as shown in Figure 5.34. Some shift registers also have a reset signal to initialize all of the flip-flops.
A related circuit is a parallel-to-serial converter that loads N bits in
parallel, then shifts them out one at a time. A shift register can be modified to perform both serial-to-parallel and parallel-to-serial operations
by adding a parallel input, DN-1:0, and a control signal, Load, as shown
in Figure 5.35. When Load is asserted, the flip-flops are loaded in parallel from the D inputs. Otherwise, the shift register shifts normally. HDL
Example 5.6 describes such a shift register.
Scan Chains*
Shift registers are often used to test sequential circuits using a technique called scan chains. Testing combinational circuits is relatively
straightforward. Known inputs called test vectors are applied, and the
outputs are checked against the expected result. Testing sequential circuits is more difficult, because the circuits have state. Starting from a
known initial condition, a large number of cycles of test vectors may
be needed to put the circuit into a desired state. For example, testing
that the most significant bit of a 32-bit counter advances from 0 to 1
requires resetting the counter, then applying 231 (about two billion)
clock pulses!
5.4 Sequential Building Blocks 255
Don’t confuse shift registers
with the shifters from Section
5.2.5. Shift registers are
sequential logic blocks that
shift in a new bit on each
clock edge. Shifters are
unclocked combinational
logic blocks that shift an
input by a specified amount.
Figure 5.33 Shift register
symbol
N Q
Sin Sout
Figure 5.34 Shift register
schematic
Figure 5.35 Shift register with
parallel load
CLK
Sin Sout
Q0 Q1 Q2 QN–1
CLK
0
1
0
1
0
1
0
1
D0 D1 D2 DN–1
Q0 Q1 Q2 QN – 1
Sin Sout
Load

To solve this problem, designers like to be able to directly observe and
control all the state of the machine. This is done by adding a test mode in
which the contents of all flip-flops can be read out or loaded with desired
values. Most systems have too many flip-flops to dedicate individual pins
to read and write each flip-flop. Instead, all the flip-flops in the system are
connected together into a shift register called a scan chain. In normal
operation, the flip-flops load data from their D input and ignore the scan
chain. In test mode, the flip-flops serially shift their contents out and shift
256 CHAPTER FIVE Digital Building Blocks
Verilog
module shiftreg # (parameter N  8)
(input clk,
input reset, load,
input sin,
input [N1:0] d,
output reg [N1:0] q,
output sout);
always @ (posedge clk or posedge reset)
if (reset) q  0;
else if (load) q  d;
else q  {q[N2:0], sin};
assign sout  q[N1];
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_ARITH.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity shiftreg is
generic (N: integer : 8);
port(clk, reset,
load: in STD_LOGIC;
sin: in STD_LOGIC;
d: in STD_LOGIC_VECTOR(N1 downto 0);
q: buffer STD_LOGIC_VECTOR(N1 downto 0);
sout: out STD_LOGIC);
end;
architecture synth of shiftreg is
begin
process (clk, reset) begin
if reset  ‘1’ then
q  CONV_STD_LOGIC_VECTOR (0, N);
elsif clk’event and clk  ‘1’ then
if load  ‘1’ then
q  d;
else
q  q(N2 downto 0) & sin;
end if;
end if;
end process;
sout  q(N1);
end;
HDL Example 5.6 SHIFT REGISTER WITH PARALLEL LOAD
Figure 5.36 Synthesized shiftreg
0
1 R
sout
[7]
q[7:0]
d[7:0]
sin
load
reset
clk
[6:0]
[7:0] [7:0] [7:0] D[7:0] Q[7:0]
Chapter 05.qxd 1/27/0
5.5 Memory Arrays 257
Figure 5.37 Scannable flip-flop: (a) schematic, (b) symbol, and (c) N-bit scannable register
0
1
Test
D
Sin
Q
Sout
(a)
D Q
Sin Sout
Test
(b)
D Q
Sin Sout
Test
D Q
Sin Sout
Test
D Q
Sin Sout
Test
D Q
Sin Sout
Test
(c)
Test
CLK
CLK
CLK
D0
Q0
D1
Q1
D2
Q2
DN–1
QN–1
Sin Sout
in new contents using Sin and Sout. The load multiplexer is usually integrated into the flip-flop to produce a scannable flip-flop. Figure 5.37
shows the schematic and symbol for a scannable flip-flop and illustrates
how the flops are cascaded to build an N-bit scannable register.
For example, the 32-bit counter could be tested by shifting in the
pattern 011111 . . . 111 in test mode, counting for one cycle in normal
mode, then shifting out the result, which should be 100000 . . . 000.
This requires only 32  1  32  65 cycles.
5.5 MEMORY ARRAYS
The previous sections introduced arithmetic and sequential circuits for
manipulating data. Digital systems also require memories to store the
data used and generated by such circuits. Registers built from flipflops are a kind of memory that stores small amounts of data. This
section describes memory arrays that can efficiently store large
amounts of data.
The section begins with an overview describing characteristics
shared by all memory arrays. It then introduces three types of memory
arrays: dynamic random access memory (DRAM), static random access
memory (SRAM), and read only memory (ROM). Each memory differs
in the way it stores data. The section briefly discusses area and delay
trade-offs and shows how memory arrays are used, not only to store
data but also to perform logic functions. The section finishes with the
HDL for a memory array.
5.5.1 Overview
Figure 5.38 shows a generic symbol for a memory array. The memory is
organized as a two-dimensional array of memory cells. The memory
reads or writes the contents of one of the rows of the array. This row is
Address
Data
Array N
M
Figure 5.38 Generic memory
array symbol
C
specified by an Address. The value read or written is called Data.
An array with N-bit addresses and M-bit data has 2N rows and M
columns. Each row of data is called a word. Thus, the array contains 2N
M-bit words.
Figure 5.39 shows a memory array with two address bits and three
data bits. The two address bits specify one of the four rows (data words)
in the array. Each data word is three bits wide. Figure 5.39(b) shows
some possible contents of the memory array.
The depth of an array is the number of rows, and the width is the
number of columns, also called the word size. The size of an array is
given as depth  width. Figure 5.39 is a 4-word  3-bit array, or simply
4  3 array. The symbol for a 1024-word  32-bit array is shown in
Figure 5.40. The total size of this array is 32 kilobits (Kb).
Bit Cells
Memory arrays are built as an array of bit cells, each of which stores
1 bit of data. Figure 5.41 shows that each bit cell is connected to a
wordline and a bitline. For each combination of address bits, the memory asserts a single wordline that activates the bit cells in that row. When
the wordline is HIGH, the stored bit transfers to or from the bitline.
Otherwise, the bitline is disconnected from the bit cell. The circuitry to
store the bit varies with memory type.
To read a bit cell, the bitline is initially left floating (Z). Then the
wordline is turned ON, allowing the stored value to drive the bitline to 0
or 1. To write a bit cell, the bitline is strongly driven to the desired value.
Then the wordline is turned ON, connecting the bitline to the stored bit.
The strongly driven bitline overpowers the contents of the bit cell, writing the desired value into the stored bit.
Organization
Figure 5.42 shows the internal organization of a 4  3 memory array.
Of course, practical memories are much larger, but the behavior of larger
arrays can be extrapolated from the smaller array. In this example, the
array stores the data from Figure 5.39(b).
During a memory read, a wordline is asserted, and the corresponding row of bit cells drives the bitlines HIGH or LOW. During a
memory write, the bitlines are driven HIGH or LOW first and then a
wordline is asserted, allowing the bitline values to be stored in that
row of bit cells. For example, to read Address 10, the bitlines are left
floating, the decoder asserts wordline2, and the data stored in that
row of bit cells, 100, reads out onto the Data bitlines. To write the
value 001 to Address 11, the bitlines are driven to the value 001,
then wordline3 is asserted and the new value (001) is stored in the
bit cells.
258 CHAPTER FIVE Digital Building Blocks
(a)
Address
Data
Array 2
3
(b)
Address
11
10
01
00
depth
0
1
1
0
1
0
1
1
0
0
0
1
width
Data
Figure 5.39 4  3 memory
array: (a) symbol, (b) function
Address
Data
1024-word ×
32-bit
Array
10
32
Figure 5.40 32 Kb array: depth
210  1024 words, width  32 bits
stored
bit
wordline
bitline
Figure 5.41 Bit cell
Cha
Memory Ports
All memories have one or more ports. Each port gives read and/or write
access to one memory address. The previous examples were all singleported memories.
Multiported memories can access several addresses simultaneously. Figure 5.43 shows a three-ported memory with two read ports
and one write port. Port 1 reads the data from address A1 onto the
read data output RD1. Port 2 reads the data from address A2 onto
RD2. Port 3 writes the data from the write data input, WD3, into
address A3 on the rising edge of the clock if the write enable, WE3, is
asserted.
Memory Types
Memory arrays are specified by their size (depth  width) and the number and type of ports. All memory arrays store data as an array of bit
cells, but they differ in how they store bits.
Memories are classified based on how they store bits in the bit cell.
The broadest classification is random access memory (RAM) versus read
only memory (ROM). RAM is volatile, meaning that it loses its data
when the power is turned off. ROM is nonvolatile, meaning that it
retains its data indefinitely, even without a power source.
RAM and ROM received their names for historical reasons that are
no longer very meaningful. RAM is called random access memory
because any data word is accessed with the same delay as any other.
A sequential access memory, such as a tape recorder, accesses nearby
data more quickly than faraway data (e.g., at the other end of the tape).
5.5 Memory Arrays 259
wordline3 11
10
2:4
Decoder
Address
01
00
stored
bit = 0
stored
bit = 1
stored
bit = 0
stored
bit = 1
stored
bit = 0
stored
bit = 0
stored
bit = 1
stored
bit = 1
stored
bit = 0
stored
bit = 0
stored
bit = 1
stored
bit = 1
wordline2
wordline1
wordline0
bitline2 bitline1 bitline0
Data2 Data1 Data0
2
Figure 5.42 4  3 memory array
A1
A3
WD3
WE3
A2
CLK
Array
RD2
RD1 M
M
N
N
N
M
Figure 5.43 Three-ported
memory

ROM is called read only memory because, historically, it could only be
read but not written. These names are confusing, because ROMs are
randomly accessed too. Worse yet, most modern ROMs can be written
as well as read! The important distinction to remember is that RAMs are
volatile and ROMs are nonvolatile.
The two major types of RAMs are dynamic RAM (DRAM) and
static RAM (SRAM). Dynamic RAM stores data as a charge on a capacitor, whereas static RAM stores data using a pair of cross-coupled
inverters. There are many flavors of ROMs that vary by how they are
written and erased. These various types of memories are discussed in the
subsequent sections.
5.5.2 Dynamic Random Access Memory
Dynamic RAM (DRAM, pronounced “dee-ram”) stores a bit as the
presence or absence of charge on a capacitor. Figure 5.44 shows a
DRAM bit cell. The bit value is stored on a capacitor. The nMOS
transistor behaves as a switch that either connects or disconnects the
capacitor from the bitline. When the wordline is asserted, the
nMOS transistor turns ON, and the stored bit value transfers to or
from the bitline.
As shown in Figure 5.45(a), when the capacitor is charged to VDD,
the stored bit is 1; when it is discharged to GND (Figure 5.45(b)), the
stored bit is 0. The capacitor node is dynamic because it is not actively
driven HIGH or LOW by a transistor tied to VDD or GND.
Upon a read, data values are transferred from the capacitor to the
bitline. Upon a write, data values are transferred from the bitline to
the capacitor. Reading destroys the bit value stored on the capacitor, so
the data word must be restored (rewritten) after each read. Even when
DRAM is not read, the contents must be refreshed (read and rewritten)
every few milliseconds, because the charge on the capacitor gradually
leaks away.
5.5.3 Static Random Access Memory (SRAM)
Static RAM (SRAM, pronounced “es-ram”) is static because stored
bits do not need to be refreshed. Figure 5.46 shows an SRAM bit cell.
The data bit is stored on cross-coupled inverters like those described
in Section 3.2. Each cell has two outputs, bitline and . When the
wordline is asserted, both nMOS transistors turn on, and data values
are transferred to or from the bitlines. Unlike DRAM, if noise
degrades the value of the stored bit, the cross-coupled inverters restore
the value.
bitline
260 CHAPTER FIVE Digital Building Blocks
Robert Dennard, 1932–.
Invented DRAM in 1966 at
IBM. Although many were
skeptical that the idea would
work, by the mid-1970s
DRAM was in virtually all
computers. He claims to have
done little creative work until,
arriving at IBM, they handed
him a patent notebook and
said, “put all your ideas in
there.” Since 1965, he has
received 35 patents in
semiconductors and microelectronics. (Photo courtesy
of IBM.)
wordline
bitline
stored
bit
Figure 5.44 DRAM bit cell

5.5.4 Area and Delay
Flip-flops, SRAMs, and DRAMs are all volatile memories, but each has
different area and delay characteristics. Table 5.4 shows a comparison of
these three types of volatile memory. The data bit stored in a flip-flop is
available immediately at its output. But flip-flops take at least 20 transistors to build. Generally, the more transistors a device has, the more area,
power, and cost it requires. DRAM latency is longer than that of SRAM
because its bitline is not actively driven by a transistor. DRAM must
wait for charge to move (relatively) slowly from the capacitor to the bitline. DRAM also has lower throughput than SRAM, because it must
refresh data periodically and after a read.
Memory latency and throughput also depend on memory size; larger
memories tend to be slower than smaller ones if all else is the same. The
best memory type for a particular design depends on the speed, cost, and
power constraints.
5.5.5 Register Files
Digital systems often use a number of registers to store temporary variables. This group of registers, called a register file, is usually built as a
small, multiported SRAM array, because it is more compact than an
array of flip-flops.
Figure 5.47 shows a 32-register  32-bit three-ported register file
built from a three-ported memory similar to that of Figure 5.43. The
5.5 Memory Arrays 261
Figure 5.45 DRAM stored values
stored
bit
wordline
bitline bitline
Figure 5.46 SRAM bit cell
Table 5.4 Memory comparison
Memory Transistors per Latency
Type Bit Cell
flip-flop ~20 fast
SRAM 6 medium
DRAM 1 slow
5
5
5
32
32
32
CLK
A1
A3
WD3
RD2
RD1 WE3
A2
Register
File
Figure 5.47 32  32 register
file with two read ports and one
write port
wordline
bitline
(a)
stored + +
bit = 1
wordline
bitline
(b)
stored
bit = 0

register file has two read ports (A1/RD1 and A2/RD2) and one write
port (A3/WD3). The 5-bit addresses, A1, A2, and A3, can each access all
25  32 registers. So, two registers can be read and one register written
simultaneously.
5.5.6 Read Only Memory
Read only memory (ROM) stores a bit as the presence or absence of
a transistor. Figure 5.48 shows a simple ROM bit cell. To read the
cell, the bitline is weakly pulled HIGH. Then the wordline is
turned ON. If the transistor is present, it pulls the bitline LOW. If it
is absent, the bitline remains HIGH. Note that the ROM bit cell is
a combinational circuit and has no state to “forget” if power is
turned off.
The contents of a ROM can be indicated using dot notation. Figure
5.49 shows the dot notation for a 4-word  3-bit ROM containing the
data from Figure 5.39. A dot at the intersection of a row (wordline) and
a column (bitline) indicates that the data bit is 1. For example, the top
wordline has a single dot on Data1, so the data word stored at Address
11 is 010.
Conceptually, ROMs can be built using two-level logic with a
group of AND gates followed by a group of OR gates. The AND gates
produce all possible minterms and hence form a decoder. Figure 5.50
shows the ROM of Figure 5.49 built using a decoder and OR gates.
Each dotted row in Figure 5.49 is an input to an OR gate in Figure
5.50. For data bits with a single dot, in this case Data0, no OR gate is
needed. This representation of a ROM is interesting because it shows
how the ROM can perform any two-level logic function. In practice,
ROMs are built from transistors instead of logic gates, to reduce their
size and cost. Section 5.6.3 explores the transistor-level implementation further.
262 CHAPTER FIVE Digital Building Blocks
wordline
bitline
wordline
bitline
bit cell
containing 0
bit cell
containing 1
Figure 5.48 ROM bit cells
containing 0 and 1
11
10
2:4
Decoder
Address
Data2 Data1 Data0
01
00
2
Figure 5.49 4  3 ROM: dot
notation
C
The contents of the ROM bit cell in Figure 5.48 are specified during
manufacturing by the presence or absence of a transistor in each bit cell.
A programmable ROM (PROM, pronounced like the dance) places a
transistor in every bit cell but provides a way to connect or disconnect
the transistor to ground.
Figure 5.51 shows the bit cell for a fuse-programmable ROM. The
user programs the ROM by applying a high voltage to selectively
blow fuses. If the fuse is present, the transistor is connected to GND
and the cell holds a 0. If the fuse is destroyed, the transistor is disconnected from ground and the cell holds a 1. This is also called a onetime programmable ROM, because the fuse cannot be repaired once it
is blown.
Reprogrammable ROMs provide a reversible mechanism for connecting or disconnecting the transistor to GND. Erasable PROMs
(EPROMs, pronounced “e-proms”) replace the nMOS transistor and
fuse with a floating-gate transistor. The floating gate is not physically
attached to any other wires. When suitable high voltages are applied,
electrons tunnel through an insulator onto the floating gate, turning
on the transistor and connecting the bitline to the wordline (decoder
output). When the EPROM is exposed to intense ultraviolet (UV) light
for about half an hour, the electrons are knocked off the floating gate,
turning the transistor off. These actions are called programming and
erasing, respectively. Electrically erasable PROMs (EEPROMs, pronounced “e-e-proms” or “double-e proms”) and Flash memory use
similar principles but include circuitry on the chip for erasing as well
as programming, so no UV light is necessary. EEPROM bit cells are
individually erasable; Flash memory erases larger blocks of bits and is
cheaper because fewer erasing circuits are needed. In 2006, Flash
5.5 Memory Arrays 263
11
10
2:4
Decoder
01
00
Data2 Data1 Data0
Address 2
Figure 5.50 4  3 ROM implementation using gates
wordline
bitline
bit cell containing 0
intact
fuse
wordline
bitline
bit cell containing 1
blown
fuse
Figure 5.51 Fuse-programmable
ROM bit cell
Fujio Masuoka, 1944–. Received
a Ph.D. in electrical engineering
from Tohoku University, Japan.
Developed memories and highspeed circuits at Toshiba from
1971 to 1994. Invented Flash
memory as an unauthorized
project pursued during nights
and weekends in the late 1970s.
Flash received its name because
the process of erasing the
memory reminds one of the flash
of a camera. Toshiba was slow to
commercialize the idea; Intel was
first to market in 1988. Flash has
grown into a $25 billion per year
market. Dr. Masuoka later joined
the faculty at Tohoku University.

memory costs less than $25 per GB, and the price continues to drop
by 30 to 40% per year. Flash has become an extremely popular way to
store large amounts of data in portable battery-powered systems such
as cameras and music players.
In summary, modern ROMs are not really read only; they can be
programmed (written) as well. The difference between RAM and ROM
is that ROMs take a longer time to write but are nonvolatile.
5.5.7 Logic Using Memory Arrays
Although they are used primarily for data storage, memory arrays can
also perform combinational logic functions. For example, the Data2
output of the ROM in Figure 5.49 is the XOR of the two Address
inputs. Likewise Data0 is the NAND of the two inputs. A 2N-word 
M-bit memory can perform any combinational function of N inputs
and M outputs. For example, the ROM in Figure 5.49 performs three
functions of two inputs.
Memory arrays used to perform logic are called lookup tables
(LUTs). Figure 5.52 shows a 4-word  1-bit memory array used as a
lookup table to perform the function Y  AB. Using memory to perform
logic, the user can look up the output value for a given input combination (address). Each address corresponds to a row in the truth table, and
each data bit corresponds to an output value.
5.5.8 Memory HDL
HDL Example 5.7 describes a 2N-word  M-bit RAM. The RAM has a
synchronous enabled write. In other words, writes occur on the rising
264 CHAPTER FIVE Digital Building Blocks
stored
bit = 1
stored
bit = 0
00
01
2:4
Decoder
A
stored
bit = 0
bitline
stored
bit = 0
Y
B
10
11
4-word x1-bit Array
ABY
0 0
0 1
1 0
1 1
0
0
0
1
Truth
Table
A1
A0
Figure 5.52 4-word  1-bit memory array used as a lookup table
Programmable ROMs can be
configured with a device
programmer like the one
shown below. The device
programmer is attached to a
computer, which specifies the
type of ROM and the data
values to program. The device
programmer blows fuses or
injects charge onto a floating
gate on the ROM. Thus the
programming process is sometimes called burning a ROM.
Flash memory drives with
Universal Serial Bus (USB)
connectors have replaced
floppy disks and CDs for
sharing files because Flash
costs have dropped so
dramatically.
C
edge of the clock if the write enable, we, is asserted. Reads occur
immediately. When power is first applied, the contents of the RAM are
unpredictable.
HDL Example 5.8 describes a 4-word  3-bit ROM. The contents
of the ROM are specified in the HDL case statement. A ROM as small
as this one may be synthesized into logic gates rather than an array.
Note that the seven-segment decoder from HDL Example 4.25 synthesizes into a ROM in Figure 4.22.
5.5 Memory Arrays 265
Verilog
module ram # (parameter N  6, M  32)
(input clk,
input we,
input [N1:0] adr,
input [M1:0] din,
output [M1:0] dout);
reg [M1:0] mem [2**N1:0];
always @ (posedge clk)
if (we) mem [adr]  din;
assign dout  mem[adr];
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.ALL;
use IEEE.STD_LOGIC_ARITH.ALL;
use IEEE.STD_LOGIC_UNSIGNED.ALL;
entity ram_array is
generic (N_
: integer : 6; M: integer : 32);
port (clk,
we: in STD_LOGIC;
adr: in STD_LOGIC_VECTOR(N1 downto 0);
din: in STD_LOGIC_VECTOR(M1 downto 0);
dout: out STD_LOGIC_VECTOR(M1 downto 0));
end;
architecture synth of ram_array is
type mem_array is array ((2**N1) downto 0)
of STD_LOGIC_VECTOR (M1 downto 0);
signal mem: mem_array;
begin
process (clk) begin
if clk’ event and clk  ‘1’ then
if we  ‘1’ then
mem (CONV_INTEGER (adr))  din;
end if;
end if;
end process;
dout  mem (CONV_INTEGER (adr));
end;
HDL Example 5.7 RAM
ram1
mem[31:0]
dout[31:0] [31:0] din[31:0]
adr[5:0]
we
clk
[5:0]
RADDR[5:0]
[31:0]
DATA[31:0]
DOUT[31:0] [5:0]
WADDR[5:0]
WE[0]
CLK
Figure 5.53 Synthesized ram
Chapter 05.qxd 1/27/
5.6 LOGIC ARRAYS
Like memory, gates can be organized into regular arrays. If the connections are made programmable, these logic arrays can be configured to
perform any function without the user having to connect wires in specific ways. The regular structure simplifies design. Logic arrays are mass
produced in large quantities, so they are inexpensive. Software tools
allow users to map logic designs onto these arrays. Most logic arrays are
also reconfigurable, allowing designs to be modified without replacing
the hardware. Reconfigurability is valuable during development and is
also useful in the field, because a system can be upgraded by simply
downloading the new configuration.
This section introduces two types of logic arrays: programmable
logic arrays (PLAs), and field programmable gate arrays (FPGAs). PLAs,
the older technology, perform only combinational logic functions.
FPGAs can perform both combinational and sequential logic.
5.6.1 Programmable Logic Array
Programmable logic arrays (PLAs) implement two-level combinational
logic in sum-of-products (SOP) form. PLAs are built from an AND
array followed by an OR array, as shown in Figure 5.54. The inputs (in
true and complementary form) drive an AND array, which produces
implicants, which in turn are ORed together to form the outputs. An
M  N  P-bit PLA has M inputs, N implicants, and P outputs.
266 CHAPTER FIVE Digital Building Blocks
Verilog
module rom (input [1:0] adr,
output reg [2:0] dout);
always @ (adr)
case (adr)
2b00: dout  3b011;
2b01: dout  3b110;
2b10: dout  3b100;
2b11: dout  3b010;
endcase
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity rom is
port (adr: in STD_LOGIC_VECTOR(1 downto 0);
dout: out STD_LOGIC_VECTOR(2 downto 0));
end;
architecture synth of rom is
begin
process (adr) begin
case adr is
when “00”  dout  “011”;
when “01”  dout  “110”;
when “10”  dout  “100”;
when “11”  dout  “010”;
end case;
end process;
end;
HDL Example 5.8 ROM
Chapter 05.q
Figure 5.55 shows the dot notation for a 3  3  2-bit PLA performing the functions and . Each row in the
AND array forms an implicant. Dots in each row of the AND array indicate which literals comprise the implicant. The AND array in Figure
5.55 forms three implicants: , and . Dots in the OR array
indicate which implicants are part of the output function.
Figure 5.56 shows how PLAs can be built using two-level logic. An
alternative implementation is given in Section 5.6.3.
ROMs can be viewed as a special case of PLAs. A 2M-word 
N-bit ROM is simply an M  2M  N-bit PLA. The decoder behaves
as an AND plane that produces all 2M minterms. The ROM array
behaves as an OR plane that produces the outputs. If the function
does not depend on all 2M minterms, a PLA is likely to be smaller
than a ROM. For example, an 8-word  2-bit ROM is required to
perform the same functions performed by the 3  3  2-bit PLA
shown in Figures 5.55 and 5.56.
ABC, ABC AB
X  ABC  ABC Y  AB
5.6 Logic Arrays 267
AND
Array
OR
Array
Inputs
Outputs
Implicants
N
M
P
Figure 5.54 M  N  P-bit PLA
X Y
ABC
AB
ABC
ABC
AND Array
OR Array
Figure 5.55 3  3  2-bit PLA:
dot notation
Ch
Programmable logic devices (PLDs) are souped-up PLAs that add
registers and various other features to the basic AND/OR planes.
However, PLDs and PLAs have largely been displaced by FPGAs, which
are more flexible and efficient for building large systems.
5.6.2 Field Programmable Gate Array
A field programmable gate array (FPGA) is an array of reconfigurable
gates. Using software programming tools, a user can implement designs
on the FPGA using either an HDL or a schematic. FPGAs are more powerful and more flexible than PLAs for several reasons. They can implement both combinational and sequential logic. They can also implement
multilevel logic functions, whereas PLAs can only implement two-level
logic. Modern FPGAs integrate other useful functions such as built-in
multipliers and large RAM arrays.
FPGAs are built as an array of configurable logic blocks (CLBs).
Figure 5.57 shows the block diagram of the Spartan FPGA introduced by
Xilinx in 1998. Each CLB can be configured to perform combinational or
sequential functions. The CLBs are surrounded by input/output blocks
(IOBs) for interfacing with external devices. The IOBs connect CLB
inputs and outputs to pins on the chip package. CLBs can connect to
other CLBs and IOBs through programmable routing channels.
The remaining blocks shown in the figure aid in programming the device.
Figure 5.58 shows a single CLB for the Spartan FPGA. Other brands
of FPGAs are organized somewhat differently, but the same general principles apply. The CLB contains lookup tables (LUTs), configurable multiplexers, and registers. The FPGA is configured by specifying the
contents of the lookup tables and the select signals for the multiplexers.
268 CHAPTER FIVE Digital Building Blocks
FPGAs are the brains of many
consumer products, including
automobiles, medical equipment, and media devices like
MP3 players. The Mercedes
Benz S-Class series, for example, has over a dozen Xilinx
FPGAs or PLDs for uses ranging from entertainment to
navigation to cruise control
systems. FPGAs allow for
quick time to market and
make debugging or adding
features late in the design
process easier.
X Y
ABC
AND ARRAY
OR ARRAY
ABC
AB
ABC
Figure 5.56 3  3  2-bit PLA
using two-level logic

Each Spartan CLB has three LUTs: the four-input F- and G-LUTs,
and the three-input H-LUT. By loading the appropriate values into the
lookup tables, the F- and G-LUTs can each be configured to perform any
function of up to four variables, and the H-LUT can perform any function of up to three variables.
Configuring the FPGA also involves choosing the select signals that
determine how the multiplexers route data through the CLB. For example,
depending on the multiplexer configuration, the H-LUT may receive one of
its inputs from either DIN or the F-LUT. Similarly, it receives another input
from either SR or the G-LUT. The third input always comes from H1.
5.6 Logic Arrays 269
Figure 5.57 Spartan block diagram

The FPGA produces two combinational outputs, X and Y. Depending
on the multiplexer configuration, X comes from either the F- or H-LUT.
Y comes from either the G- or H-LUT. These outputs can be connected to
other CLBs via the routing channels
The CLB also contains two flip-flops. Depending on the configuration, the flip-flop inputs may come from DIN or from the F-, G-, or
H-LUT. The flip-flop outputs, XQ and YQ, also can be connected to
other CLBs via the routing channels.
In summary, the CLB can perform up to two combinational and/or
two registered functions. All of the functions can involve at least four
variables, and some can involve up to nine.
The designer configures an FPGA by first creating a schematic or
HDL description of the design. The design is then synthesized onto
the FPGA. The synthesis tool determines how the LUTs, multiplexers,
and routing channels should be configured to perform the
specified functions. This configuration information is then downloaded to the FPGA.
Because Xilinx FPGAs store their configuration information in SRAM,
they can be easily reprogrammed. They may download the SRAM contents
from a computer in the laboratory or from an EEPROM chip when the
270 CHAPTER FIVE Digital Building Blocks
Figure 5.58 Spartan CLB

system is turned on. Some manufacturers include EEPROM directly on the
FPGA or use one-time programmable fuses to configure the FPGA.
Example 5.6 FUNCTIONS BUILT USING CLBS
Explain how to configure a CLB to perform the following functions: (a)
and ; (b) Y  JKLMPQR; (c) a divide-by-3 counter
with binary state encoding (see Figure 3.29(a)).
Solution: (a) Configure the F-LUT to compute X and the G-LUT to compute Y,
as shown in Figure 5.59. Inputs F3, F2, and F1 are A, B, and C, respectively
(these connections are set by the routing channels). Inputs G2 and G1 are A
and B. F4, G4, and G3 are don’t cares (and may be connected to 0). Configure
the final multiplexers to select X from the F-LUT and Y from the G-LUT. In
general, a CLB can compute any two functions, of up to four variables each, in
this fashion.
(b) Configure the F-LUT to compute F  JKLM and the G-LUT to compute
G  PQR. Then configure the H-LUT to compute H  FG. Configure the
final multiplexer to select Y from the H-LUT. This configuration is shown in
Figure 5.60. In general, a CLB can compute certain functions of up to nine
variables in this way.
(c) The FSM has two bits of state (S1:0) and one output (Y). The next state
depends on the two bits of current state. Use the F-LUT and G-LUT to compute
the next state from the current state, as shown in Figure 5.61. Use the two flipflops to hold this state. The flip-flops have a dedicated reset input from the SR
signal in the CLB. The registered outputs are fed back to the inputs using the
routing channels, as indicated by the dashed blue lines. In general, another CLB
might be necessary to compute the output Y. However, in this case, Y  , so Y
can come from the same F-LUT used to compute . Hence, the entire FSM fits
in a single CLB. In general, an FSM requires at least one CLB for every two bits
of state, and it may require more CLBs for the output or next state logic if they
are too complex to fit in a single LUT.
S
0
S
0
X  ABC  ABC Y  AB
5.6 Logic Arrays 271
F4
F3
F2
F1
F
F2 F1 F
0 0
0 1
1 0
1 1
0
1
0
0
F3
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
1
0
X
X
X
X
X
X
X
X
F4
(A) (B) (C) (X )
G2 G1 G
0 0
0 1
1 0
1 1
0
0
1
0
G3
X
X
X
X
X
X
X
X
G4
(A) (B) (Y ) G4
G3
G2
G1
G 0
A
B
0
A
B
C
0
Y
X
Figure 5.59 CLB configuration
for two functions of up to four
inputs each
Chapter
Example 5.7 CLB DELAY
Alyssa P. Hacker is building a finite state machine that must run at 200
MHz. She uses a Spartan 3 FPGA with the following specifications: tCLB
0.61 ns per CLB; tsetup  0.53 ns and tpcq  0.72 ns for all flip-flops. What
is the maximum number of CLBs her design can use? You can ignore interconnect delay.
Solution: Alyssa uses Equation 3.13 to solve for the maximum propagation delay
of the logic: tpd  Tc  (tpcq  tsetup).
Thus, tpd  5 ns  (0.72 ns 0.53 ns), so tpd  3.75 ns. The delay of each CLB,
tCLB, is 0.61 ns, and the maximum number of CLBs, N, is NtCLB  3.75 ns.
Thus, N  6.
272 CHAPTER FIVE Digital Building Blocks
F4
F3
F2
F1
F
F2 F1 F
0 0
0 1
1 0
1 1
1
0
0
0
F3
X
X
X
X
X
X
X
X
F4
(S0) (S0')
G2 G1 G
0 0
0 1
1 0
1 1
0
1
0
0
G3
X
X
X
X
X
X
X
X
G4 G4
G3
G2
G1
G 0
S0
0
0
0
YQ
XQ
(S1) ( S1) ( S0)( S1')
S1
clk
clk
Reset
Reset
Y
S1'
S0'
Figure 5.61 CLB configuration for FSM with two bits of state
F4
F3
F2
F1
F
F2 F1 F
0 0
0 1
1 0
1 1
0
0
0
0
F3
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
F4
(K) (L) (M)
G3G4 G2 G1 G
(P) (Q) G4
G3
G2
G1
G P
Q
R
0
K
L
M
J
(J ) (R)
(Y )
0 1
1 0
1 1
0
0
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
1
X
X
X
X
X
X
X
0X 000
0 0
0 1
1 0
1 1
0
0
0
0
0
0
0
0 0
0 1
1 0
1 1
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1
1
0
H1 FGH
0 1
1 0
1 1
0
0
1
X
X
X
X 000
G
H1
F
0 H Y
Figure 5.60 CLB configuration for one function of more than four inputs
Chapte
5.6.3 Array Implementations*
To minimize their size and cost, ROMs and PLAs commonly use pseudonMOS or dynamic circuits (see Section 1.7.8) instead of conventional
logic gates.
Figure 5.62(a) shows the dot notation for a 4  3-bit ROM that performs the following functions: X  A  B, , and Z  .
These are the same functions as those of Figure 5.49, with the address
inputs renamed A and B and the data outputs renamed X, Y, and Z. The
pseudo-nMOS implementation is given in Figure 5.62(b). Each decoder
output is connected to the gates of the nMOS transistors in its row.
Remember that in pseudo-nMOS circuits, the weak pMOS transistor
pulls the output HIGH only if there is no path to GND through the pulldown (nMOS) network.
Pull-down transistors are placed at every junction without a dot.
The dots from the dot notation diagram of Figure 5.62(a) are left faintly
visible in Figure 5.62(b) for easy comparison. The weak pull-up transistors pull the output HIGH for each wordline without a pull-down transistor. For example, when AB  11, the 11 wordline is HIGH and
transistors on X and Z turn on and pull those outputs LOW. The Y output has no transistor connecting to the 11 wordline, so Y is pulled
HIGH by the weak pull-up.
PLAs can also be built using pseudo-nMOS circuits, as shown in
Figure 5.63 for the PLA from Figure 5.55. Pull-down (nMOS) transistors are placed on the complement of dotted literals in the AND array
and on dotted rows in the OR array. The columns in the OR array are
sent through an inverter before they are fed to the output bits. Again, the
blue dots from the dot notation diagram of Figure 5.55 are left faintly
visible in Figure 5.63 for easy comparison.
Y  A  B AB
5.6 Logic Arrays 273
11
10
2:4
Decoder
01
00
A1
A0
X
(a)
A
B
Y Z
11
10
2:4
Decoder
01
00
A1
A0
A
B
weak
(b)
XYZ
Figure 5.62 ROM implementation: (a) dot notation, (b) pseudo-nMOS circuit
Many ROMs and PLAs use
dynamic circuits in place of
pseudo-nMOS circuits.
Dynamic gates turn the pMOS
transistor ON for only part of
the time, saving power when
the pMOS is OFF and the
result is not needed. Aside
from this, dynamic and
pseudo-nMOS memory arrays
are similar in design and
behavior.
Chapt
5.7 SUMMARY
This chapter introduced digital building blocks used in many digital systems. These blocks include arithmetic circuits such as adders, subtractors, comparators, shifters, multipliers, and dividers; sequential circuits
such as counters and shift registers; and arrays for memory and logic.
The chapter also explored fixed-point and floating-point representations
of fractional numbers. In Chapter 7, we use these building blocks to
build a microprocessor.
Adders form the basis of most arithmetic circuits. A half adder adds
two 1-bit inputs, A and B, and produces a sum and a carry out. A full
adder extends the half adder to also accept a carry in. N full adders can
be cascaded to form a carry propagate adder (CPA) that adds two N-bit
numbers. This type of CPA is called a ripple-carry adder because the
carry ripples through each of the full adders. Faster CPAs can be constructed using lookahead or prefix techniques.
A subtractor negates the second input and adds it to the first.
A magnitude comparator subtracts one number from another and
determines the relative value based on the sign of the result. A multiplier forms partial products using AND gates, then sums these bits
using full adders. A divider repeatedly subtracts the divisor from the
partial remainder and checks the sign of the difference to determine the
quotient bits. A counter uses an adder and a register to increment a
running count.
Fractional numbers are represented using fixed-point or floating-point
forms. Fixed-point numbers are analogous to decimals, and floating-point
numbers are analogous to scientific notation. Fixed-point numbers use ordinary arithmetic circuits, whereas floating-point numbers require more elaborate hardware to extract and process the sign, exponent, and mantissa.
274 CHAPTER FIVE Digital Building Blocks
X Y
ABC
AB
ABC
ABC
AND Array
OR Array
weak
weak
Figure 5.63 3  3  2-bit PLA
using pseudo-nMOS circuits

Large memories are organized into arrays of words. The memories
have one or more ports to read and/or write the words. Volatile memories, such as SRAM and DRAM, lose their state when the power is
turned off. SRAM is faster than DRAM but requires more transistors.
A register file is a small multiported SRAM array. Nonvolatile memories, called ROMs, retain their state indefinitely. Despite their names,
most modern ROMs can be written.
Arrays are also a regular way to build logic. Memory arrays can be
used as lookup tables to perform combinational functions. PLAs are
composed of dedicated connections between configurable AND and
OR arrays; they only implement combinational logic. FPGAs are composed of many small lookup tables and registers; they implement
combinational and sequential logic. The lookup table contents and
their interconnections can be configured to perform any logic function.
Modern FPGAs are easy to reprogram and are large and cheap enough
to build highly sophisticated digital systems, so they are widely used in
low- and medium-volume commercial products as well as in education.
5.7 Summary 275

EXERCISES
Exercise 5.1 What is the delay for the following types of 64-bit adders? Assume
that each two-input gate delay is 150 ps and that a full adder delay is 450 ps.
(a) a ripple-carry adder
(b) a carry-lookahead adder with 4-bit blocks
(c) a prefix adder
Exercise 5.2 Design two adders: a 64-bit ripple-carry adder and a 64-bit carrylookahead adder with 4-bit blocks. Use only two-input gates. Each two-input
gate is 15 m2, has a 50 ps delay, and has 20 pF of total gate capacitance. You
may assume that the static power is negligible.
(a) Compare the area, delay, and power of the adders (operating at 100 MHz).
(b) Discuss the trade-offs between power, area, and delay.
Exercise 5.3 Explain why a designer might choose to use a ripple-carry adder
instead of a carry-lookahead adder.
Exercise 5.4 Design the 16-bit prefix adder of Figure 5.7 in an HDL. Simulate
and test your module to prove that it functions correctly.
Exercise 5.5 The prefix network shown in Figure 5.7 uses black cells to compute all of the prefixes. Some of the block propagate signals are not actually necessary. Design a “gray cell” that receives G and P signals for bits i:k and k  1:j
but produces only Gi:j, not Pi:j. Redraw the prefix network, replacing black cells
with gray cells wherever possible.
Exercise 5.6 The prefix network shown in Figure 5.7 is not the only way to calculate all of the prefixes in logarithmic time. The Kogge-Stone network is another
common prefix network that performs the same function using a different connection of black cells. Research Kogge-Stone adders and draw a schematic similar to
Figure 5.7 showing the connection of black cells in a Kogge-Stone adder.
Exercise 5.7 Recall that an N-input priority encoder has log2N outputs that
encodes which of the N inputs gets priority (see Exercise 2.25).
(a) Design an N-input priority encoder that has delay that increases logarithmically with N. Sketch your design and give the delay of the circuit in terms of
the delay of its circuit elements.
(b) Code your design in HDL. Simulate and test your module to prove that it
functions correctly.
276 CHAPTER FIVE Digital Building Blocks
C
Exercise 5.8 Design the following comparators for 32-bit numbers. Sketch the
schematics.
(a) not equal
(b) greater than
(c) less than or equal to
Exercise 5.9 Design the 32-bit ALU shown in Figure 5.15 using your favorite
HDL. You can make the top-level module either behavioral or structural.
Exercise 5.10 Add an Overflow output to the 32-bit ALU from Exercise 5.9. The
output is TRUE when the result of the adder overflows. Otherwise, it is FALSE.
(a) Write a Boolean equation for the Overflow output.
(b) Sketch the Overflow circuit.
(c) Design the modified ALU in an HDL.
Exercise 5.11 Add a Zero output to the 32-bit ALU from Exercise 5.9. The output is TRUE when Y  0.
Exercise 5.12 Write a testbench to test the 32-bit ALU from Exercise 5.9, 5.10,
or 5.11. Then use it to test the ALU. Include any test vector files necessary. Be
sure to test enough corner cases to convince a reasonable skeptic that the ALU
functions correctly.
Exercise 5.13 Design a shifter that always shifts a 32-bit input left by 2 bits. The
input and output are both 32 bits. Explain the design in words and sketch a
schematic. Implement your design in your favourite HDL.
Exercise 5.14 Design 4-bit left and right rotators. Sketch a schematic of your
design. Implement your design in your favourite HDL.
Exercise 5.15 Design an 8-bit left shifter using only 24 2:1 multiplexers. The
shifter accepts an 8-bit input, A, and a 3-bit shift amount, shamt2:0. It produces
an 8-bit output, Y. Sketch the schematic.
Exercise 5.16 Explain how to build any N-bit shifter or rotator using only
Nlog2N 2:1 multiplexers.
Exercise 5.17 The funnel shifter in Figure 5.64 can perform any N-bit shift or
rotate operation. It shifts a 2N-bit input right by k bits. The output, Y, is the
N least significant bits of the result. The most significant N bits of the input are
Exercises 277
Ch
called B and the least significant N bits are called C. By choosing appropriate
values of B, C, and k, the funnel shifter can perform any type of shift or rotate.
Explain what these values should be in terms of A, shamt, and N for
(a) logical right shift of A by shamt.
(b) arithmetic right shift of A by shamt.
(c) left shift of A by shamt.
(d) right rotate of A by shamt.
(e) left rotate of A by shamt.
278 CHAPTER FIVE Digital Building Blocks
B C
k + N – 1 k
2N – 1 N – 1 0
Y
N – 1 0
Figure 5.64 Funnel shifter
Exercise 5.18 Find the critical path for the 4  4 multiplier from Figure 5.18 in
terms of an AND gate delay (tAND) and a full adder delay (tFA). What is the
delay of an N  N multiplier built in the same way?
Exercise 5.19 Design a multiplier that handles two’s complement numbers.
Exercise 5.20 A sign extension unit extends a two’s complement number from M
to N (N  M) bits by copying the most significant bit of the input into the upper
bits of the output (see Section 1.4.6). It receives an M-bit input, A, and produces
an N-bit output, Y. Sketch a circuit for a sign extension unit with a 4-bit input
and an 8-bit output. Write the HDL for your design.
Exercise 5.21 A zero extension unit extends an unsigned number from M to N
bits (N  M) by putting zeros in the upper bits of the output. Sketch a circuit for
a zero extension unit with a 4-bit input and an 8-bit output. Write the HDL for
your design.
Exercise 5.22 Compute 111001.0002/001100.0002 in binary using the standard
division algorithm from elementary school. Show your work.

Exercise 5.23 What is the range of numbers that can be represented by the
following number systems?
(a) 24-bit unsigned fixed-point numbers with 12 integer bits and 12 fraction bits
(b) 24-bit sign and magnitude fixed-point numbers with 12 integer bits and
12 fraction bits
(c) 24-bit two’s complement fixed-point numbers with 12 integer bits and
12 fraction bits
Exercise 5.24 Express the following base 10 numbers in 16-bit fixed-point
sign/magnitude format with eight integer bits and eight fraction bits. Express
your answer in hexadecimal.
(a) 13.5625
(b) 42.3125
(c) 17.15625
Exercise 5.25 Express the base 10 numbers in Exercise 5.24 in 16-bit fixedpoint two’s complement format with eight integer bits and eight fraction bits.
Express your answer in hexadecimal.
Exercise 5.26 Express the base 10 numbers in Exercise 5.24 in IEEE 754
single-precision floating-point format. Express your answer in hexadecimal.
Exercise 5.27 Convert the following two’s complement binary fixed-point
numbers to base 10.
(a) 0101.1000
(b) 1111.1111
(c) 1000.0000
Exercise 5.28 When adding two floating-point numbers, the number with the
smaller exponent is shifted. Why is this? Explain in words and give an example
to justify your explanation.
Exercise 5.29 Add the following IEEE 754 single-precision floating-point
numbers.
(a) C0D20004  72407020
(b) C0D20004  40DC0004
(c) (5FBE4000  3FF80000)  DFDE4000
(Why is the result counterintuitive? Explain.)
Exercises 279
Ch
Exercise 5.30 Expand the steps in section 5.3.2 for performing floating-point
addition to work for negative as well as positive floating-point numbers.
Exercise 5.31 Consider IEEE 754 single-precision floating-point numbers.
(a) How many numbers can be represented by IEEE 754 single-precision
floating-point format? You need not count 
 or NaN.
(b) How many additional numbers could be represented if 
 and NaN were
not represented?
(c) Explain why 
 and NaN are given special representations.
Exercise 5.32 Consider the following decimal numbers: 245 and 0.0625.
(a) Write the two numbers using single-precision floating-point notation. Give
your answers in hexadecimal.
(b) Perform a magnitude comparison of the two 32-bit numbers from part (a).
In other words, interpret the two 32-bit numbers as two’s complement
numbers and compare them. Does the integer comparison give the correct
result?
(c) You decide to come up with a new single-precision floating-point notation.
Everything is the same as the IEEE 754 single-precision floating-point standard, except that you represent the exponent using two’s complement
instead of a bias. Write the two numbers using your new standard. Give
your answers in hexadecimal.
(e) Does integer comparison work with your new floating-point notation from
part (d)?
(f) Why is it convenient for integer comparison to work with floating-point
numbers?
Exercise 5.33 Design a single-precision floating-point adder using your
favorite HDL. Before coding the design in an HDL, sketch a schematic of your
design. Simulate and test your adder to prove to a skeptic that it functions correctly. You may consider positive numbers only and use round toward zero
(truncate). You may also ignore the special cases given in Table 5.2.
Exercise 5.34 In this problem, you will explore the design of a 32-bit floatingpoint multiplier. The multiplier has two 32-bit floating-point inputs and produces a 32-bit floating-point output. You may consider positive numbers only
280 CHAPTER FIVE Digital Building Blocks

and use round toward zero (truncate). You may also ignore the special cases
given in Table 5.2.
(a) Write the steps necessary to perform 32-bit floating-point multiplication.
(b) Sketch the schematic of a 32-bit floating-point multiplier.
(c) Design a 32-bit floating-point multiplier in an HDL. Simulate and test your
multiplier to prove to a skeptic that it functions correctly.
Exercise 5.35 In this problem, you will explore the design of a 32-bit prefix
adder.
(a) Sketch a schematic of your design.
(b) Design the 32-bit prefix adder in an HDL. Simulate and test your adder to
prove that it functions correctly.
(c) What is the delay of your 32-bit prefix adder from part (a)? Assume that
each two-input gate delay is 100 ps.
(d) Design a pipelined version of the 32-bit prefix adder. Sketch the schematic
of your design. How fast can your pipelined prefix adder run? Make the
design run as fast as possible.
(e) Design the pipelined 32-bit prefix adder in an HDL.
Exercise 5.36 An incrementer adds 1 to an N-bit number. Build an 8-bit incrementer using half adders.
Exercise 5.37 Build a 32-bit synchronous Up/Down counter. The inputs are
Reset and Up. When Reset is 1, the outputs are all 0. Otherwise, when Up  1,
the circuit counts up, and when Up  0, the circuit counts down.
Exercise 5.38 Design a 32-bit counter that adds 4 at each clock edge. The
counter has reset and clock inputs. Upon reset, the counter output is all 0.
Exercise 5.39 Modify the counter from Exercise 5.38 such that the counter will
either increment by 4 or load a new 32-bit value, D, on each clock edge, depending on a control signal, PCSrc. When PCSrc  1, the counter loads the new
value D.
Exercise 5.40 An N-bit Johnson counter consists of an N-bit shift register with a
reset signal. The output of the shift register (Sout) is inverted and fed back to the
input (Sin). When the counter is reset, all of the bits are cleared to 0.
(a) Show the sequence of outputs, Q3:0, produced by a 4-bit Johnson counter
starting immediately after the counter is reset.
Exercises 281
Cha
(b) How many cycles elapse until an N-bit Johnson counter repeats its
sequence? Explain.
(c) Design a decimal counter using a 5-bit Johnson counter, ten AND gates, and
inverters. The decimal counter has a clock, a reset, and ten one-hot outputs,
Y9:0. When the counter is reset, Y0 is asserted. On each subsequent cycle,
the next output should be asserted. After ten cycles, the counter should
repeat. Sketch a schematic of the decimal counter.
(d) What advantages might a Johnson counter have over a conventional counter?
Exercise 5.41 Write the HDL for a 4-bit scannable flip-flop like the one shown in
Figure 5.37. Simulate and test your HDL module to prove that it functions
correctly.
Exercise 5.42 The English language has a good deal of redundancy that
allows us to reconstruct garbled transmissions. Binary data can also be
transmitted in redundant form to allow error correction. For example, the
number 0 could be coded as 00000 and the number 1 could be coded as
11111. The value could then be sent over a noisy channel that might flip up
to two of the bits. The receiver could reconstruct the original data because
a 0 will have at least three of the five received bits as 0’s; similarly a 1 will
have at least three 1’s.
(a) Propose an encoding to send 00, 01, 10, or 11 encoded using five bits of
information such that all errors that corrupt one bit of the encoded data can
be corrected. Hint: the encodings 00000 and 11111 for 00 and 11, respectively, will not work.
(b) Design a circuit that receives your five-bit encoded data and decodes it to
00, 01, 10, or 11, even if one bit of the transmitted data has been changed.
(c) Suppose you wanted to change to an alternative 5-bit encoding. How might
you implement your design to make it easy to change the encoding without
having to use different hardware?
Exercise 5.43 Flash EEPROM, simply called Flash memory, is a fairly recent
invention that has revolutionized consumer electronics. Research and explain
how Flash memory works. Use a diagram illustrating the floating gate. Describe
how a bit in the memory is programmed. Properly cite your sources.
Exercise 5.44 The extraterrestrial life project team has just discovered aliens
living on the bottom of Mono Lake. They need to construct a circuit to
classify the aliens by potential planet of origin based on measured features
282 CHAPTER FIVE Digital Building Blocks

available from the NASA probe: greenness, brownness, sliminess, and
ugliness. Careful consultation with xenobiologists leads to the following
conclusions:
 If the alien is green and slimy or ugly, brown, and slimy, it might be from
Mars.
 If the critter is ugly, brown, and slimy, or green and neither ugly nor slimy, it
might be from Venus.
 If the beastie is brown and neither ugly nor slimy or is green and slimy, it
might be from Jupiter.
Note that this is an inexact science; for example, a life form which is mottled
green and brown and is slimy but not ugly might be from either Mars or
Jupiter.
(a) Program a 4  4  3 PLA to identify the alien. You may use dot notation.
(b) Program a 16  3 ROM to identify the alien. You may use dot notation.
(c) Implement your design in an HDL.
Exercise 5.45 Implement the following functions using a single 16  3 ROM.
Use dot notation to indicate the ROM contents.
(a)
(b)
(c) Z  A  B  C  D
Exercise 5.46 Implement the functions from Exercise 5.45 using an 4  8  3
PLA. You may use dot notation.
Exercise 5.47 Specify the size of a ROM that you could use to program each of
the following combinational circuits. Is using a ROM to implement these functions a good design choice? Explain why or why not.
(a) a 16-bit adder/subtractor with Cin and Cout
(b) an 8  8 multiplier
(c) a 16-bit priority encoder (see Exercise 2.25)
Exercise 5.48 Consider the ROM circuits in Figure 5.65. For each row, can the
circuit in column I be replaced by an equivalent circuit in column II by proper
programming of the latter’s ROM?
Y  AB  BD
X  AB  BCD  AB
Exercises 283
Chapte
284 CHAPTER FIVE Digital Building Blocks
K+1
I
A RD
ROM
CLK
N N
K
In
A RD
ROM
N N A RD
ROM
N
A RD
ROM
K+1
CLK
K
A RD Out
ROM
K+1 K+1
CLK
K
In A RD
ROM
K+1 K+1 Out
CLK
K
In A RD
ROM
K+1 K+N N A RD
ROM
N Out
CLK
K
In A RD
ROM
K+1 K+N N Out
CLK
N
In A RD
ROM
N+1 N N A RD
ROM
N Out
CLK
N
In A RD
ROM
N+1 N N Out
II
(a)
(b)
(c)
(d)
Figure 5.65 ROM circuits
Exercise 5.49 Give an example of a nine-input function that can be performed
using only one Spartan FPGA CLB. Give an example of an eight-input function
that cannot be performed using only one CLB.
Exercise 5.50 How many Spartan FPGA CLBs are required to perform each of
the following functions? Show how to configure one or more CLBs to perform
the function. You should be able to do this by inspection, without performing
logic synthesis.
(a) The combinational function from Exercise 2.7(c).
(b) The combinational function from Exercise 2.9(c).
(c) The two-output function from Exercise 2.15.
(d) The function from Exercise 2.24.
(e) A four-input priority encoder (see Exercise 2.25).
(f) An eight-input priority encoder (see Exercise 2.25).
(g) A 3:8 decoder.

(h) A 4-bit carry propagate adder (with no carry in or out).
(i) The FSM from Exercise 3.19.
(j) The Gray code counter from Exercise 3.24.
Exercise 5.51 Consider the Spartan CLB shown in Figure 5.58. It has the following specifications: tpd  tcd  2.7 ns per CLB; tsetup  3.9 ns, thold  0 ns, and
tpcq  2.8 ns for all flip-flops.
(a) What is the minimum number of Spartan CLBs required to implement the
FSM of Figure 3.26?
(b) Without clock skew, what is the fastest clock frequency at which this FSM
will run reliably?
(c) With 5 ns of clock skew, what is the fastest frequency at which the FSM will
run reliably?
Exercise 5.52 You would like to use an FPGA to implement an M&M sorter
with a color sensor and motors to put red candy in one jar and green candy in
another. The design is to be implemented as an FSM using a Spartan XC3S200
FPGA, a chip from the Spartan 3 series family. It is considerably faster than the
original Spartan FPGA. According to the data sheet, the FPGA has timing characteristics shown in Table 5.5. Assume that the design is small enough that wire
delay is negligible.
Exercises 285
Table 5.5 Spartan 3 XC3S200 timing
Name Value (ns)
tpcq 0.72
tsetup 0.53
thold 0
tpd (per CLB) 0.61
tskew 0
You would like your FSM to run at 100 MHz. What is the maximum number
of CLBs on the critical path? What is the fastest speed at which the FSM will run?
Chapt
Interview Questions
The following exercises present questions that have been asked at interviews for
digital design jobs.
Question 5.1 What is the largest possible result of multiplying two unsigned
N-bit numbers?
Question 5.2 Binary coded decimal (BCD) representation uses four bits to
encode each decimal digit. For example 4210 is represented as 01000010BCD.
Explain in words why processors might use BCD representation.
Question 5.3 Design hardware to add two 8-bit unsigned BCD numbers (see
Question 5.2). Sketch a schematic for your design, and write an HDL module for
the BCD adder. The inputs are A, B, and Cin, and the outputs are S and Cout. Cin
and Cout are 1-bit carries, and A, B, and S are 8-bit BCD numbers.
286 CHAPTER FIVE Digital Building Blocks



6
6.1 Introduction
6.2 Assembly Language
6.3 Machine Language
6.4 Programming
6.5 Addressing Modes
6.6 Lights, Camera, Action:
Compiling, Assembling,
and Loading
6.7 Odds and Ends*
6.8 Real World Perspective:
IA-32 Architecture*
6.9 Summary
Exercises
Interview Questions
Architecture
6.1 INTRODUCTION
The previous chapters introduced digital design principles and building
blocks. In this chapter, we jump up a few levels of abstraction to define
the architecture of a computer (see Figure 1.1). The architecture is the
programmer’s view of a computer. It is defined by the instruction set
(language), and operand locations (registers and memory). Many different architectures exist, such as IA-32, MIPS, SPARC, and PowerPC.
The first step in understanding any computer architecture is to
learn its language. The words in a computer’s language are called
instructions. The computer’s vocabulary is called the instruction set. All
programs running on a computer use the same instruction set. Even
complex software applications, such as word processing and spreadsheet applications, are eventually compiled into a series of simple
instructions such as add, subtract, and jump. Computer instructions
indicate both the operation to perform and the operands to use.
The operands may come from memory, from registers, or from the
instruction itself.
Computer hardware understands only 1’s and 0’s, so instructions
are encoded as binary numbers in a format called machine language.
Just as we use letters to encode human language, computers use binary
numbers to encode machine language. Microprocessors are digital systems that read and execute machine language instructions. However,
humans consider reading machine language to be tedious, so we prefer
to represent the instructions in a symbolic format, called assembly
language.
The instruction sets of different architectures are more like different
dialects than different languages. Almost all architectures define basic
instructions, such as add, subtract, and jump, that operate on memory
or registers. Once you have learned one instruction set, understanding
others is fairly straightforward.
289

A computer architecture does not define the underlying hardware
implementation. Often, many different hardware implementations of a
single architecture exist. For example, Intel and Advanced Micro Devices
(AMD) sell various microprocessors belonging to the same IA-32 architecture. They all can run the same programs, but they use different
underlying hardware and therefore offer trade-offs in performance, price,
and power. Some microprocessors are optimized for high-performance
servers, whereas others are optimized for long battery life in laptop computers. The specific arrangement of registers, memories, ALUs, and other
building blocks to form a microprocessor is called the microarchitecture
and will be the subject of Chapter 7. Often, many different microarchitectures exist for a single architecture.
In this text, we introduce the MIPS architecture that was first developed by John Hennessy and his colleagues at Stanford in the 1980s. MIPS
processors are used by, among others, Silicon Graphics, Nintendo, and
Cisco. We start by introducing the basic instructions, operand locations,
and machine language formats. We then introduce more instructions used
in common programming constructs, such as branches, loops, array
manipulations, and procedure calls.
Throughout the chapter, we motivate the design of the MIPS architecture using four principles articulated by Patterson and Hennessy: (1) simplicity favors regularity; (2) make the common case fast; (3) smaller is
faster; and (4) good design demands good compromises.
6.2 ASSEMBLY LANGUAGE
Assembly language is the human-readable representation of the computer’s native language. Each assembly language instruction specifies
both the operation to perform and the operands on which to operate. We
introduce simple arithmetic instructions and show how these operations
are written in assembly language. We then define the MIPS instruction
operands: registers, memory, and constants.
We assume that you already have some familiarity with a high-level
programming language such as C, C, or Java. (These languages are
practically identical for most of the examples in this chapter, but where
they differ, we will use C.)
6.2.1 Instructions
The most common operation computers perform is addition. Code
Example 6.1 shows code for adding variables b and c and writing the
result to a. The program is shown on the left in a high-level language
(using the syntax of C, C, and Java), and then rewritten on the right
in MIPS assembly language. Note that statements in a C program end
with a semicolon.
290 CHAPTER SIX Architecture
What is the best architecture
to study when first learning
the subject?
Commercially successful
architectures such as IA-32
are satisfying to study because
you can use them to write
programs on real computers.
Unfortunately, many of these
architectures are full of warts
and idiosyncrasies accumulated over years of haphazard
development by different
engineering teams, making
the architectures difficult to
understand and implement.
Many textbooks teach
imaginary architectures that
are simplified to illustrate the
key concepts.
We follow the lead of David
Patterson and John Hennessy
in their text, Computer
Organization and Design, by
focusing on the MIPS architecture. Hundreds of millions of
MIPS microprocessors have
shipped, so the architecture is
commercially very important.
Yet it is a clean architecture
with little odd behavior. At the
end of this chapter, we briefly
visit the IA-32 architecture to
compare and contrast it with
MIPS.
Chap
6.2 Assembly Language 291
High-Level Code
a  b  c;
MIPS Assembly Code
add a, b, c
Code Example 6.1 ADDITION
High-Level Code
a  b  c;
MIPS Assembly Code
sub a, b, c
Code Example 6.2 SUBTRACTION
High-Level Code
a  b  c  d; // single-line comment
/* multiple-line
comment */
MIPS Assembly Code
sub t, c, d # t  c  d
add a, b, t # a  b  t
Code Example 6.3 MORE COMPLEX CODE
The first part of the assembly instruction, add, is called the
mnemonic and indicates what operation to perform. The operation is
performed on b and c, the source operands, and the result is written to
a, the destination operand.
Code Example 6.2 shows that subtraction is similar to addition. The
instruction format is the same as the add instruction except for the operation specification, sub. This consistent instruction format is an example
of the first design principle:
Design Principle 1: Simplicity favors regularity.
Instructions with a consistent number of operands—in this case, two
sources and one destination—are easier to encode and handle in hardware. More complex high-level code translates into multiple MIPS
instructions, as shown in Code Example 6.3.
In the high-level language examples, single-line comments begin with
// and continue until the end of the line. Multiline comments begin with
/* and end with */. In assembly language, only single-line comments are
used. They begin with # and continue until the end of the line. The
assembly language program in Code Example 6.3 requires a temporary
variable, t, to store the intermediate result. Using multiple assembly
mnemonic (pronounced
ni-mon-ik) comes from the
Greek word E	
,
to remember. The assembly
language mnemonic is easier
to remember than a machine
language pattern of 0’s and
1’s representing the same
operation.
Chapter 
language instructions to perform more complex operations is an example
of the second design principle of computer architecture:
Design Principle 2: Make the common case fast.
The MIPS instruction set makes the common case fast by including
only simple, commonly used instructions. The number of instructions is
kept small so that the hardware required to decode the instruction and
its operands can be simple, small, and fast. More elaborate operations
that are less common are performed using sequences of multiple simple
instructions. Thus, MIPS is a reduced instruction set computer (RISC)
architecture. Architectures with many complex instructions, such as
Intel’s IA-32 architecture, are complex instruction set computers (CISC).
For example, IA-32 defines a “string move” instruction that copies a
string (a series of characters) from one part of memory to another. Such
an operation requires many, possibly even hundreds, of simple instructions in a RISC machine. However, the cost of implementing complex
instructions in a CISC architecture is added hardware and overhead that
slows down the simple instructions.
A RISC architecture minimizes the hardware complexity and the necessary instruction encoding by keeping the set of distinct instructions small.
For example, an instruction set with 64 simple instructions would need
log264  6 bits to encode the operation. An instruction set with 256 complex instructions would need log2256  8 bits of encoding per instruction.
In a CISC machine, even though the complex instructions may be used
only rarely, they add overhead to all instructions, even the simple ones.
6.2.2 Operands: Registers, Memory, and Constants
An instruction operates on operands. In Code Example 6.1 the variables a, b, and c are all operands. But computers operate on 1’s and 0’s,
not variable names. The instructions need a physical location from
which to retrieve the binary data. Operands can be stored in registers or
memory, or they may be constants stored in the instruction itself.
Computers use various locations to hold operands, to optimize for
speed and data capacity. Operands stored as constants or in registers
are accessed quickly, but they hold only a small amount of data.
Additional data must be accessed from memory, which is large but slow.
MIPS is called a 32-bit architecture because it operates on 32-bit data.
(The MIPS architecture has been extended to 64 bits in commercial
products, but we will consider only the 32-bit form in this book.)
Registers
Instructions need to access operands quickly so that they can run fast.
But operands stored in memory take a long time to retrieve. Therefore,
292 CHAPTER SIX Architecture
Ch
most architectures specify a small number of registers that hold commonly used operands. The MIPS architecture uses 32 registers, called the
register set or register file. The fewer the registers, the faster they can be
accessed. This leads to the third design principle:
Design Principle 3: Smaller is faster.
Looking up information from a small number of relevant books on
your desk is a lot faster than searching for the information in the stacks at a
library. Likewise, reading data from a small set of registers (for example,
32) is faster than reading it from 1000 registers or a large memory. A small
register file is typically built from a small SRAM array (see Section 5.5.3).
The SRAM array uses a small decoder and bitlines connected to relatively
few memory cells, so it has a shorter critical path than a large memory does.
Code Example 6.4 shows the add instruction with register operands.
MIPS register names are preceded by the $ sign. The variables a, b, and c
are arbitrarily placed in $s0, $s1, and $s2. The name $s1 is pronounced
“register s1” or “dollar s1”. The instruction adds the 32-bit values contained in $s1 (b) and $s2 (c) and writes the 32-bit result to $s0 (a).
MIPS generally stores variables in 18 of the 32 registers: $s0 – $s7,
and $t0 – $t9. Register names beginning with $s are called saved registers. Following MIPS convention, these registers store variables such as
a, b, and c. Saved registers have special connotations when they are
used with procedure calls (see Section 6.4.6). Register names beginning
with $t are called temporary registers. They are used for storing temporary variables. Code Example 6.5 shows MIPS assembly code using a
temporary register, $t0, to store the intermediate calculation of cd.
6.2 Assembly Language 293
High-Level Code
a  b  c;
MIPS Assembly Code
# $s0  a, $s1  b, $s2 = c
add $s0, $s1, $s2 # a = b + c
Code Example 6.4 REGISTER OPERANDS
High-Level Code
a  b  c  d;
MIPS Assembly Code
# $s0  a, $s1  b, $s2  c, $s3  d
sub $t0, $s2, $s3 # t  c  d
add $s0, $s1, $t0 # a  b  t
Code Example 6.5 TEMPORARY REGISTERS
Chapter 06.qx
Example 6.1 TRANSLATING HIGH-LEVEL CODE TO ASSEMBLY
LANGUAGE
Translate the following high-level code into assembly language. Assume variables
a–c are held in registers $s0–$s2 and f–j are in $s3–$s7.
a  b  c;
f  (g  h)  (i  j);
Solution: The program uses four assembly language instructions.
# MIPS assembly code
# $s0  a, $s1  b, $s2  c, $s3  f, $s4  g, $s5  h,
# $s6  i, $s7  j
sub $s0, $s1, $s2 # a  b  c
add $t0, $s4, $s5 # $t0  g  h
add $t1, $s6, $s7 # $t1  i  j
sub $s3, $t0, $t1 # f  (g  h)  (i  j)
The Register Set
The MIPS architecture defines 32 registers. Each register has a name and a
number ranging from 0 to 31. Table 6.1 lists the name, number, and use for
each register. $0 always contains the value 0 because this constant is so frequently used in computer programs. We have also discussed the $s and $t
registers. The remaining registers will be described throughout this chapter.
294 CHAPTER SIX Architecture
Table 6.1 MIPS register set
Name Number Use
$0 0 the constant value 0
$at 1 assembler temporary
$v0–$v1 2–3 procedure return values
$a0–$a3 4–7 procedure arguments
$t0–$t7 8–15 temporary variables
$s0–$s7 16–23 saved variables
$t8–$t9 24–25 temporary variables
$k0–$k1 26–27 operating system (OS) temporaries
$gp 28 global pointer
$sp 29 stack pointer
$fp 30 frame pointer
$ra 31 procedure return address
Chapter 06.qxd 1/31/
Memory
If registers were the only storage space for operands, we would be confined to simple programs with no more than 32 variables. However, data
can also be stored in memory. When compared to the register file, memory has many data locations, but accessing it takes a longer amount of
time. Whereas the register file is small and fast, memory is large and
slow. For this reason, commonly used variables are kept in registers. By
using a combination of memory and registers, a program can access a
large amount of data fairly quickly. As described in Section 5.5, memories are organized as an array of data words. The MIPS architecture uses
32-bit memory addresses and 32-bit data words.
MIPS uses a byte-addressable memory. That is, each byte in memory
has a unique address. However, for explanation purposes only, we first
introduce a word-addressable memory, and afterward describe the MIPS
byte-addressable memory.
Figure 6.1 shows a memory array that is word-addressable. That is,
each 32-bit data word has a unique 32-bit address. Both the 32-bit word
address and the 32-bit data value are written in hexadecimal in Figure 6.1.
For example, data 0xF2F1AC07 is stored at memory address 1.
Hexadecimal constants are written with the prefix 0x. By convention,
memory is drawn with low memory addresses toward the bottom and
high memory addresses toward the top.
MIPS uses the load word instruction, lw, to read a data word from
memory into a register. Code Example 6.6 loads memory word 1 into $s3.
The lw instruction specifies the effective address in memory as
the sum of a base address and an offset. The base address (written in
parentheses in the instruction) is a register. The offset is a constant
(written before the parentheses). In Code Example 6.6, the base address
6.2 Assembly Language 295
Data
00000003 40F30788
01 E E2 8 42
F2F 1AC07
ABC DEF 7 8
00000002
00000001
00000000
Word
Address
Word 3
Word 2
Word 1
Word 0
Figure 6.1 Word-addressable
memory
Assembly Code
# This assembly code (unlike MIPS) assumes word-addressable memory
lw $s3, 1($0) # read memory word 1 into $s3
Code Example 6.6 READING WORD-ADDRESSABLE MEMORY

is $0, which holds the value 0, and the offset is 1, so the lw instruction
reads from memory address ($0 + 1)  1. After the load word instruction
(lw) is executed, $s3 holds the value 0xF2F1AC07, which is the data
value stored at memory address 1 in Figure 6.1.
Similarly, MIPS uses the store word instruction, sw, to write a data
word from a register into memory. Code Example 6.7 writes the contents of register $s7 into memory word 5. These examples have used $0
as the base address for simplicity, but remember that any register can be
used to supply the base address.
The previous two code examples have shown a computer architecture with a word-addressable memory. The MIPS memory model,
however, is byte-addressable, not word-addressable. Each data byte
has a unique address. A 32-bit word consists of four 8-bit bytes.
So each word address is a multiple of 4, as shown in Figure 6.2.
Again, both the 32-bit word address and the data value are given in
hexadecimal.
Code Example 6.8 shows how to read and write words in the
MIPS byte-addressable memory. The word address is four times
the word number. The MIPS assembly code reads words 0, 2, and 3
and writes words 1, 8, and 100. The offset can be written in decimal
or hexadecimal.
The MIPS architecture also provides the lb and sb instructions that
load and store single bytes in memory rather than words. They are similar
to lw and sw and will be discussed further in Section 6.4.5.
Byte-addressable memories are organized in a big-endian or littleendian fashion, as shown in Figure 6.3. In both formats, the most
significant byte (MSB) is on the left and the least significant byte (LSB) is
on the right. In big-endian machines, bytes are numbered starting with 0
296 CHAPTER SIX Architecture
Assembly Code
# This assembly code (unlike MIPS) assumes word-addressable memory
sw $s7, 5($0) # write $s7 to memory word 5
Code Example 6.7 WRITING WORD-ADDRESSABLE MEMORY
Word
Address Data
0000000C
00000008
00000004
00000000
width = 4 bytes
40F30788
01 EE 2 8 42
F2F1 AC07
ABCD EF 7 8
Word 3
Word 2
Word 1
Word 0
Figure 6.2 Byte-addressable
memory
0 123
MSB LSB
4 567
8 9AB
C DEF
Byte
Address
0 3 210
4 7 654
8 B A98
C F EDC
Byte
Address
Word
Address
Big-Endian Little-Endian
MSB LSB
Figure 6.3 Big- and littleendian memory addressing
C
at the big (most significant) end. In little-endian machines, bytes are
numbered starting with 0 at the little (least significant) end. Word
addresses are the same in both formats and refer to the same four bytes.
Only the addresses of bytes within a word differ.
Example 6.2 BIG- AND LITTLE-ENDIAN MEMORY
Suppose that $s0 initially contains 0x23456789. After the following program is
run on a big-endian system, what value does $s0 contain? In a little-endian
system? lb $s0, 1($0) loads the data at byte address (1  $0)  1 into the least
significant byte of $s0. lb is discussed in detail in Section 6.4.5.
sw $s0, 0($0)
lb $s0, 1($0)
Solution: Figure 6.4 shows how big- and little-endian machines store the value
0x23456789 in memory word 0. After the load byte instruction, lb $s0, 1($0),
$s0 would contain 0x00000045 on a big-endian system and 0x00000067 on a
little-endian system.
6.2 Assembly Language 297
MIPS Assembly Code
1w $s0, 0($0) # read data word 0 (0xABCDEF78) into $s0
1w $s1, 8($0) # read data word 2 (0x01EE2842) into $s1
1w $s2, 0xC($0) # read data word 3 (0x40F30788) into $s2
sw $s3, 4($0) # write $s3 to data word 1
sw $s4, 0x20($0) # write $s4 to data word 8
sw $s5, 400($0) # write $s5 to data word 100
Code Example 6.8 ACCESSING BYTE-ADDRESSABLE MEMORY
The terms big-endian and littleendian come from Jonathan
Swift’s Gulliver’s Travels, first
published in 1726 under the
pseudonym of Isaac Bickerstaff.
In his stories the Lilliputian king
required his citizens (the LittleEndians) to break their eggs on
the little end. The Big-Endians
were rebels who broke their
eggs on the big end.
The terms were first
applied to computer architectures by Danny Cohen in his
paper “On Holy Wars and a
Plea for Peace” published on
April Fools Day, 1980
(USC/ISI IEN 137). (Photo
courtesy The Brotherton
Collection, IEEDS University
Library.)
1 SPIM, the MIPS simulator that comes with this text, uses the endianness of the machine
it is run on. For example, when using SPIM on an Intel IA-32 machine, the memory is
little-endian. With an older Macintosh or Sun SPARC machine, memory is big-endian.
23 45 67 89
0123
0 23 45 67 89
3 210
Word
Address
Big-Endian Little-Endian
Byte Address
Data Value
Byte Address
Data Value
MSB LSB MSB LSB
Figure 6.4 Big-endian and little-endian data storage
IBM’s PowerPC (formerly found in Macintosh computers) uses
big-endian addressing. Intel’s IA-32 architecture (found in PCs) uses
little-endian addressing. Some MIPS processors are little-endian, and
some are big-endian.1 The choice of endianness is completely arbitrary
but leads to hassles when sharing data between big-endian and littleendian computers. In examples in this text, we will use little-endian
format whenever byte ordering matters.
Ch
In the MIPS architecture, word addresses for lw and sw must be
word aligned. That is, the address must be divisible by 4. Thus, the
instruction lw $s0, 7($0) is an illegal instruction. Some architectures,
such as IA-32, allow non-word-aligned data reads and writes, but MIPS
requires strict alignment for simplicity. Of course, byte addresses for
load byte and store byte, lb and sb, need not be word aligned.
Constants/Immediates
Load word and store word, lw and sw, also illustrate the use of constants in MIPS instructions. These constants are called immediates,
because their values are immediately available from the instruction and
do not require a register or memory access. Add immediate, addi, is
another common MIPS instruction that uses an immediate operand.
addi adds the immediate specified in the instruction to a value in a register, as shown in Code Example 6.9.
The immediate specified in an instruction is a 16-bit two’s complement number in the range [32768, 32767]. Subtraction is equivalent to
adding a negative number, so, in the interest of simplicity, there is no
subi instruction in the MIPS architecture.
Recall that the add and sub instructions use three register operands.
But the lw, sw, and addi instructions use two register operands and a
constant. Because the instruction formats differ, lw and sw instructions
violate design principle 1: simplicity favors regularity. However, this
issue allows us to introduce the last design principle:
298 CHAPTER SIX Architecture
High-Level Code
a  a  4;
b  a  12;
MIPS Assembly Code
# $s0  a, $s1  b
addi $s0, $s0, 4 # a  a  4
addi $s1, $s0, 12 # b  a  12
Code Example 6.9 IMMEDIATE OPERANDS
Design Principle 4: Good design demands good compromises.
A single instruction format would be simple but not flexible. The
MIPS instruction set makes the compromise of supporting three instruction formats. One format, used for instructions such as add and sub, has
three register operands. Another, used for instructions such as lw and
addi, has two register operands and a 16-bit immediate. A third, to be
discussed later, has a 26-bit immediate and no registers. The next section
discusses the three MIPS instruction formats and shows how they are
encoded into binary.
Chapter 
6.3 MACHINE LANGUAGE
Assembly language is convenient for humans to read. However, digital
circuits understand only 1’s and 0’s. Therefore, a program written in
assembly language is translated from mnemonics to a representation
using only 1’s and 0’s, called machine language.
MIPS uses 32-bit instructions. Again, simplicity favors regularity,
and the most regular choice is to encode all instructions as words that
can be stored in memory. Even though some instructions may not
require all 32 bits of encoding, variable-length instructions would add
too much complexity. Simplicity would also encourage a single
instruction format, but, as already mentioned, that is too restrictive.
MIPS makes the compromise of defining three instruction formats:
R-type, I-type, and J-type. This small number of formats allows for
some regularity among all the types, and thus simpler hardware, while
also accommodating different instruction needs, such as the need to
encode large constants in the instruction. R-type instructions operate
on three registers. I-type instructions operate on two registers and
a 16-bit immediate. J-type (jump) instructions operate on one
26-bit immediate. We introduce all three formats in this section but
leave the discussion of J-type instructions for Section 6.4.2.
6.3.1 R-type Instructions
The name R-type is short for register-type. R-type instructions use
three registers as operands: two as sources, and one as a destination.
Figure 6.5 shows the R-type machine instruction format. The 32-bit
instruction has six fields: op, rs, rt, rd, shamt, and funct. Each
field is five or six bits, as indicated.
The operation the instruction performs is encoded in the two fields
highlighted in blue: op (also called opcode or operation code) and funct
(also called the function). All R-type instructions have an opcode of 0.
The specific R-type operation is determined by the funct field.
For example, the opcode and funct fields for the add instruction are
0 (0000002) and 32 (1000002), respectively. Similarly, the sub instruction has an opcode and funct field of 0 and 34.
The operands are encoded in the three fields: rs, rt, and rd. The first
two registers, rs and rt, are the source registers; rd is the destination
6.3 Machine Language 299
op rs rt rd shamt funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
R-type Figure 6.5 R-type machine
instruction format

register. The fields contain the register numbers that were given in
Table 6.1. For example, $s0 is register 16.
The fifth field, shamt, is used only in shift operations. In those
instructions, the binary value stored in the 5-bit shamt field indicates the
amount to shift. For all other R-type instructions, shamt is 0.
Figure 6.6 shows the machine code for the R-type instructions add
and sub. Notice that the destination is the first register in an assembly
language instruction, but it is the third register field (rd) in the machine
language instruction. For example, the assembly instruction add $s0,
$s1, $s2 has rs  $s1 (17), rt  $s2 (18), and rd  $s0 (16).
Tables B.1 and B.2 in Appendix B define the opcode values for all
MIPS instructions and the funct field values for R-type instructions.
Example 6.3 TRANSLATING ASSEMBLY LANGUAGE TO MACHINE
LANGUAGE
Translate the following assembly language statement into machine language.
add $t0, $s4, $s5
Solution: According to Table 6.1, $t0, $s4, and $s5 are registers 8, 20, and
21. According to Tables B.1 and B.2, add has an opcode of 0 and a funct
code of 32. Thus, the fields and machine code are given in Figure 6.7. The
easiest way to write the machine language in hexadecimal is to first write it in
binary, then look at consecutive groups of four bits, which correspond to
hexadecimal digits (indicated in blue). Hence, the machine language instruction is 0x02954020.
300 CHAPTER SIX Architecture
000000 10001 10010 10000 00000 100000
000000
add $s0, $s1, $s2
sub $t0, $t3, $t5
Assembly Code Machine Code
0 17 18 16 0 32
0 11 13 8 0 34
Field Values
(0x02328020)
(0x016D4022)
op rs rt rd shamt funct op rs rt rd shamt funct
01011 01101 01000 00000 100010
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits 6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
Figure 6.6 Machine code for R-type instructions
add $t0, $s4, $s5 000000 10100 10101 01000 00000 100000
Assembly Code Machine Code
0 20 21 8 0 32
Field Values
(0x 02954020)
0 4 2 95 0 2 0
op rs rt rd shamt funct op rs rt rd shamt funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
Figure 6.7 Machine code for the R-type instruction of Example 6.3
rs is short for “register
source.” rt comes after rs
alphabetically and usually
indicates the second register
source.
Cha
6.3.2 I-Type Instructions
The name I-type is short for immediate-type. I-type instructions
use two register operands and one immediate operand. Figure 6.8
shows the I-type machine instruction format. The 32-bit instruction
has four fields: op, rs, rt, and imm. The first three fields, op, rs, and
rt, are like those of R-type instructions. The imm field holds the 16-bit
immediate.
The operation is determined solely by the opcode, highlighted in
blue. The operands are specified in the three fields, rs, rt, and imm. rs
and imm are always used as source operands. rt is used as a destination
for some instructions (such as addi and lw) but as another source for
others (such as sw).
Figure 6.9 shows several examples of encoding I-type instructions.
Recall that negative immediate values are represented using 16-bit
two’s complement notation. rt is listed first in the assembly language
instruction when it is used as a destination, but it is the second register
field in the machine language instruction.
6.3 Machine Language 301
op rs rt imm
6 bits 5 bits 5 bits 16 bits
I-type Figure 6.8 I-type instruction
format
(0x22300005)
(0x2268FFF4)
(0x8C0A0020)
(0xAD310004)
001000 10001 10000 0000 0000 0000 0101
Assembly Code Machine Code
8 17 16
Field Values
op rs rt imm op rs rt imm
addi $s0, $s1, 5
addi $t0, $s3, –12
lw $t2, 32($0)
sw $s1, 4($t1)
8 19 8 –12
35 0 10 32
43 9 17
001000 10011 01000 1111 1111 1111 0100
100011 00000 01010 0000 0000 0010 0000
101011 01001 10001 0000 0000 0000 0100
6 bits 5 bits 5 bits 16 bits 6 bits 5 bits 5 bits 16 bits
4
5
Figure 6.9 Machine code for I-type instructions
Example 6.4 TRANSLATING I-TYPE ASSEMBLY INSTRUCTIONS INTO
MACHINE CODE
Translate the following I-type instruction into machine code.
lw $s3, 24($s4)
Solution: According to Table 6.1, $s3 and $s4 are registers 19 and 20, respectively.
Table B.1 indicates that lw has an opcode of 35. rs specifies the base address, $s4,
and rt specifies the destination register, $s3. The immediate, imm, encodes the
16-bit offset, 24. Thus, the fields and machine code are given in Figure 6.10.

I-type instructions have a 16-bit immediate field, but the immediates
are used in 32-bit operations. For example, lw adds a 16-bit offset to a
32-bit base register. What should go in the upper half of the 32 bits? For
positive immediates, the upper half should be all 0’s, but for negative
immediates, the upper half should be all 1’s. Recall from Section 1.4.6
that this is called sign extension. An N-bit two’s complement number is
sign-extended to an M-bit number (M  N) by copying the sign bit
(most significant bit) of the N-bit number into all of the upper bits of the
M-bit number. Sign-extending a two’s complement number does not
change its value.
Most MIPS instructions sign-extend the immediate. For example,
addi, lw, and sw do sign extension to support both positive and negative immediates. An exception to this rule is that logical operations
(andi, ori, xori) place 0’s in the upper half; this is called zero extension rather than sign extension. Logical operations are discussed further
in Section 6.4.1.
6.3.3 J-type Instructions
The name J-type is short for jump-type. This format is used only with
jump instructions (see Section 6.4.2). This instruction format uses a single 26-bit address operand, addr, as shown in Figure 6.11. Like other
formats, J-type instructions begin with a 6-bit opcode. The remaining
bits are used to specify an address, addr. Further discussion and
machine code examples of J-type instructions are given in Sections 6.4.2
and 6.5.
6.3.4 Interpreting Machine Language Code
To interpret machine language, one must decipher the fields of each
32-bit instruction word. Different instructions use different formats, but
all formats start with a 6-bit opcode field. Thus, the best place to begin is
to look at the opcode. If it is 0, the instruction is R-type; otherwise it is
I-type or J-type.
302 CHAPTER SIX Architecture
100011 10100 10011 1111 1111 1110 1000
op rs rt imm op rs rt imm
lw $s3, –24($s4)
Assembly Code Machine Code
 35 20 19 –24
Field Values
(0x8E93FFE8)
6 bits 5 bits 5 bits 16 bits 8 E 93 FF E8
Figure 6.10 Machine code for the I-type instruction
op addr
6 bits 26 bits
J-type Figure 6.11 J-type instruction
format

Example 6.5 TRANSLATING MACHINE LANGUAGE TO ASSEMBLY
LANGUAGE
Translate the following machine language code into assembly language.
0x2237FFF1
0x02F34022
Solution: First, we represent each instruction in binary and look at the six most
significant bits to find the opcode for each instruction, as shown in Figure 6.12.
The opcode determines how to interpret the rest of the bits. The opcodes are
0010002 (810) and 0000002 (010), indicating an addi and R-type instruction,
respectively. The funct field of the R-type instruction is 1000102 (3410), indicating that it is a sub instruction. Figure 6.12 shows the assembly code equivalent
of the two machine instructions.
6.3 Machine Language 303
addi $s7, $s1, –15
Machine Code Assembly Code
 8 17 23 –15
Field Values
(0x2237FFF1)
op rs rt imm op rs rt imm
2 2 37 FF F1
(0x02F34022) 0 23 19 8 0 34 sub $t0, $s7, $s3
op
0 2 F340 22
001000
000000 10111 10011 01000 00000 100010
10001 10111 1111 1111 1111 0001
op rs rt rd shamt funct rs rt rd shamt funct
Figure 6.12 Machine code to assembly code translation
6.3.5 The Power of the Stored Program
A program written in machine language is a series of 32-bit numbers representing the instructions. Like other binary numbers, these instructions
can be stored in memory. This is called the stored program concept, and
it is a key reason why computers are so powerful. Running a different
program does not require large amounts of time and effort to reconfigure
or rewire hardware; it only requires writing the new program to memory.
Instead of dedicated hardware, the stored program offers general purpose
computing. In this way, a computer can execute applications ranging
from a calculator to a word processor to a video player simply by changing the stored program.
Instructions in a stored program are retrieved, or fetched, from
memory and executed by the processor. Even large, complex programs
are simplified to a series of memory reads and instruction executions.
Figure 6.13 shows how machine instructions are stored in memory.
In MIPS programs, the instructions are normally stored starting at
address 0x00400000. Remember that MIPS memory is byte addressable,
so 32-bit (4-byte) instruction addresses advance by 4 bytes, not 1.

To run or execute the stored program, the processor fetches the
instructions from memory sequentially. The fetched instructions are then
decoded and executed by the digital hardware. The address of the current
instruction is kept in a 32-bit register called the program counter (PC).
The PC is separate from the 32 registers shown previously in Table 6.1.
To execute the code in Figure 6.13, the operating system sets the PC
to address 0x00400000. The processor reads the instruction at that
memory address and executes the instruction, 0x8C0A0020. The processor then increments the PC by 4, to 0x00400004, fetches and executes
that instruction, and repeats.
The architectural state of a microprocessor holds the state of a program. For MIPS, the architectural state consists of the register file and
PC. If the operating system saves the architectural state at some point in
the program, it can interrupt the program, do something else, then
restore the state such that the program continues properly, unaware that
it was ever interrupted. The architectural state is also of great importance when we build a microprocessor in Chapter 7.
6.4 PROGRAMMING
Software languages such as C or Java are called high-level programming
languages, because they are written at a more abstract level than assembly
language. Many high-level languages use common software constructs such
as arithmetic and logical operations, if/else statements, for and while
loops, array indexing, and procedure calls. In this section, we explore how
to translate these high-level constructs into MIPS assembly code.
6.4.1 Arithmetic/Logical Instructions
The MIPS architecture defines a variety of arithmetic and logical
instructions. We introduce these instructions briefly here, because they
are necessary to implement higher-level constructs.
304 CHAPTER SIX Architecture
Figure 6.13 Stored program
addi $t0, $s3, –12
Assembly Code Machine Code
lw $t2, 32($0)
add $s0, $s1, $s2
sub $t0, $t3, $t5
0x8C0A0020
0x02328020
0x2268FFF4
0x016D4022
Address Instructions
0040000C 01 6 D4 0 22
226 8FFF4
023 28020
8C0 A0 0 20
00400008
00400004
00400000
Stored Program
Main Memory
PC
Ada Lovelace, 1815–1852.
Wrote the first computer program. It calculated the Bernoulli
numbers using Charles
Babbage’s Analytical Engine.
She was the only legitimate
child of the poet Lord Byron.

Logical Instructions
MIPS logical operations include and, or, xor, and nor. These R-type
instructions operate bit-by-bit on two source registers and write the
result to the destination register. Figure 6.14 shows examples of these
operations on the two source values 0xFFFF0000 and 0x46A1F0B7.
The figure shows the values stored in the destination register, rd, after
the instruction executes.
The and instruction is useful for masking bits (i.e., forcing
unwanted bits to 0). For example, in Figure 6.14, 0xFFFF0000 AND
0x46A1F0B7  0x46A10000. The and instruction masks off the bottom two bytes and places the unmasked top two bytes of $s2, 0x46A1,
in $s3. Any subset of register bits can be masked.
The or instruction is useful for combining bits from two registers.
For example, 0x347A0000 OR 0x000072FC  0x347A72FC, a combination of the two values.
MIPS does not provide a NOT instruction, but A NOR $0  NOT
A, so the NOR instruction can substitute.
Logical operations can also operate on immediates. These I-type
instructions are andi, ori, and xori. nori is not provided, because the
same functionality can be easily implemented using the other instructions, as will be explored in Exercise 6.11. Figure 6.15 shows examples
of the andi, ori, and xori instructions. The figure gives the values of
6.4 Programming 305
$s1 1111 1111 1111 1111 0000 0000 0000 0000
$s2 0100 0110 1010 0001 1111 0000 1011 0111
$s3 0100 0110 1010 0001 0000 0000 0000 0000
$s4 1111 1111 1111 1111 1111 0000 1011 0111
$s5 1011 1001 0101 1110 1111 0000 1011 0111
$s6 0000 0000 0000 0000 0000 1111 0100 1000
Source Registers
Assembly Code Result
and $s3, $s1, $s2
or $s4, $s1, $s2
xor $s5, $s1, $s2
nor $s6, $s1, $s2
Figure 6.14 Logical operations
$s1 0000 0000 0000 0000 0000 0000 1111 1111
Assembly Code
imm 0000 0000 0000 0000 1111 1010 0011 0100
$s2 0000 0000 0000 0000 0000 0000 0011 0100
$s3 0000 0000 0000 0000 1111 1010 1111 1111
$s4 0000 0000 0000 0000 1111 1010 1100 1011
andi $s2, $s1, 0xFA34
Source Values
Result
ori $s3, $s1, 0xFA34
xori $s4, $s1, 0xFA34
zero-extended Figure 6.15 Logical operations
with immediates
Cha
the source register and immediate, and the value of the destination register, rt, after the instruction executes. Because these instructions operate
on a 32-bit value from a register and a 16-bit immediate, they first zeroextend the immediate to 32 bits.
Shift Instructions
Shift instructions shift the value in a register left or right by up to
31 bits. Shift operations multiply or divide by powers of two. MIPS shift
operations are sll (shift left logical), srl (shift right logical), and sra
(shift right arithmetic).
As discussed in Section 5.2.5, left shifts always fill the least significant bits with 0’s. However, right shifts can be either logical (0’s shift
into the most significant bits) or arithmetic (the sign bit shifts into the
most significant bits). Figure 6.16 shows the machine code for the
R-type instructions sll, srl, and sra. rt (i.e., $s1) holds the 32-bit
value to be shifted, and shamt gives the amount by which to shift (4).
The shifted result is placed in rd.
Figure 6.17 shows the register values for the shift instructions sll,
srl, and sra. Shifting a value left by N is equivalent to multiplying it by
2N. Likewise, arithmetically shifting a value right by N is equivalent to
dividing it by 2N, as discussed in Section 5.2.5.
MIPS also has variable-shift instructions: sllv (shift left logical variable), srlv (shift right logical variable), and srav (shift right arithmetic
variable). Figure 6.18 shows the machine code for these instructions.
306 CHAPTER SIX Architecture
sll $t0, $s1, 4
srl $s2, $s1, 4
sra $s3, $s1, 4
000000 00000 10001 01000 00100 000000
op rs rt rd shamt funct op rs rt rd shamt funct
Assembly Code Machine Code
 0 0 17 8 4 0
Field Values
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits 6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
 0 0 17 18 4 2
 0 0 17 19 4 3
000000 00000 10001 10010 00100 000010
000000 00000 10001 10011 00100 000011
(0x00114100)
(0x00119102)
(0x00119903)
Figure 6.16 Shift instruction machine code
$s1 1111 0011 0000 0000 0000 0010 1010 1000
Assembly Code
shamt 00100
$t0 0011 0000 0000 0000 0010 1010 1000 0000
$s2 0000 1111 0011 0000 0000 0000 0010 1010
$s3
sll $t0, $s1, 4
Source Values
Result
srl $s2, $s1, 4
sra $s3, $s1, 4 1111 1111 0011 0000 0000 0000 0010 1010
Figure 6.17 Shift operations

rt (i.e., $s1) holds the value to be shifted, and the five least significant
bits of rs (i.e., $s2) give the amount to shift. The shifted result is placed
in rd, as before. The shamt field is ignored and should be all 0’s. Figure
6.19 shows register values for each type of variable-shift instruction.
Generating Constants
The addi instruction is helpful for assigning 16-bit constants, as shown
in Code Example 6.10.
6.4 Programming 307
sllv $s3, $s1, $s2
srlv $s4, $s1, $s2
srav $s5, $s1, $s2
op rs rt rd shamt funct
Machine Code Assembly Code Field Values
op rs rt rd shamt funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits 6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
(0x02519804)
(0x0251A006)
(0x0251A807)
401917180
602017180
702117180
000000 10010 10001 10011 00000 000100
000000 10010 10001 10100 00000 000110
000000 10010 10001 10101 00000 000111
Figure 6.18 Variable-shift instruction machine code
$s1
$s3 0000 0000
$s4 0000 0000
$s5 1111 1111
Assembly Code
sllv $s3, $s1, $s2
srlv $s4, $s1, $s2
srav $s5, $s1, $s2
Source Values
Result
$s2
1111 0011 0000 0100 0000 0010 1010 1000
0000 0000 0000 0000 0000 0000 0000 1000
0000 0100 0000 00101010 1000
1111 0011 0000 0100 0000 0010
1111 0011 0000 0100 0000 0010
Figure 6.19 Variable-shift
operations
High-Level Code
int a  0x4f3c;
MIPS Assembly code
# $s0  a
addi $s0, $0, 0x4f3c # a  0x4f3c
Code Example 6.10 16-BIT CONSTANT
High-Level Code
int a  0x6d5e4f3c;
MIPS Assembly Code
# $s0  a
lui $s0, 0x6d5e # a  0x6d5e0000
ori $s0, $s0, 0x4f3c # a  0x6d5e4f3c
Code Example 6.11 32-BIT CONSTANT
Chapter
To assign 32-bit constants, use a load upper immediate instruction
(lui) followed by an or immediate (ori) instruction, as shown in Code
Example 6.11. lui loads a 16-bit immediate into the upper half of a register and sets the lower half to 0. As mentioned earlier, ori merges a 16-
bit immediate into the lower half.
Multiplication and Division Instructions*
Multiplication and division are somewhat different from other arithmetic operations. Multiplying two 32-bit numbers produces a 64-bit
product. Dividing two 32-bit numbers produces a 32-bit quotient and a
32-bit remainder.
The MIPS architecture has two special-purpose registers, hi and lo,
which are used to hold the results of multiplication and division. mult
$s0, $s1 multiplies the values in $s0 and $s1. The 32 most significant
bits are placed in hi and the 32 least significant bits are placed in lo.
Similarly, div $s0, $s1 computes $s0/$s1. The quotient is placed in
lo and the remainder is placed in hi.
6.4.2 Branching
An advantage of a computer over a calculator is its ability to make decisions. A computer performs different tasks depending on the input. For
example, if/else statements, case statements, while loops, and for
loops all conditionally execute code depending on some test.
To sequentially execute instructions, the program counter increments by 4 after each instruction. Branch instructions modify the program counter to skip over sections of code or to go back to repeat
previous code. Conditional branch instructions perform a test and
branch only if the test is TRUE. Unconditional branch instructions,
called jumps, always branch.
Conditional Branches
The MIPS instruction set has two conditional branch instructions: branch
if equal (beq) and branch if not equal (bne). beq branches when the values in two registers are equal, and bne branches when they are not equal.
Code Example 6.12 illustrates the use of beq. Note that branches are
written as beq $rs, $rt, imm, where $rs is the first source register. This
order is reversed from most I-type instructions.
When the program in Code Example 6.12 reaches the branch if
equal instruction (beq), the value in $s0 is equal to the value in $s1, so
the branch is taken. That is, the next instruction executed is the add
instruction just after the label called target. The two instructions
directly after the branch and before the label are not executed.
Assembly code uses labels to indicate instruction locations in the
program. When the assembly code is translated into machine code, these
308 CHAPTER SIX Architecture
The int data type in C refers
to a word of data representing
a two’s complement integer.
MIPS uses 32-bit words, so an
int represents a number in the
range [231, 2311].
hi and lo are not among the
usual 32 MIPS registers, so
special instructions are needed
to access them. mfhi $s2 (move
from hi) copies the value in hi
to $s2. mflo $s3 (move from
lo) copies the value in lo to
$s3. hi and lo are technically
part of the architectural state;
however, we generally ignore
these registers in this book.

6.4 Programming 309
MIPS Assembly Code
addi $s0, $0, 4 # $s0  0  4  4
addi $s1, $0, 1 # $s1  0  1  1
sll $s1, $s1, 2 # $s1  1  2  4
beq $s0, $s1, target # $s0  $s1, so branch is taken
addi $s1, $s1, 1 # not executed
sub $s1, $s1, $s0 # not executed
target:
add $s1, $s1, $s0 # $s1  4  4  8
Code Example 6.12 CONDITIONAL BRANCHING USING beq
MIPS Assembly Code
addi $s0, $0, 4 # $s0  0  4  4
addi $s1, $0, 1 # $s1  0  1  1
s11 $s1, $s1, 2 # $s1  1  2  4
bne $s0, $s1, target # $s0  $s1, so branch is not taken
addi $s1, $s1, 1 # $s1  4  1  5
sub $s1, $s1, $s0 # $s1  5  4  1
target:
add $s1, $s1, $s0 # $s1  1  4  5
Code Example 6.13 CONDITIONAL BRANCHING USING bne
labels are translated into instruction addresses (see Section 6.5). MIPS
assembly labels are followed by a (:) and cannot use reserved words,
such as instruction mnemonics. Most programmers indent their instructions but not the labels, to help make labels stand out.
Code Example 6.13 shows an example using the branch if not equal
instruction (bne). In this case, the branch is not taken because $s0 is
equal to $s1, and the code continues to execute directly after the bne
instruction. All instructions in this code snippet are executed.
Jump
A program can unconditionally branch, or jump, using the three types of
jump instructions: jump (j), jump and link (jal), and jump register (jr).
Jump (j) jumps directly to the instruction at the specified label. Jump
and link (jal) is similar to j but is used by procedures to save a return
address, as will be discussed in Section 6.4.6. Jump register (jr) jumps to
the address held in a register. Code Example 6.14 shows the use of the
jump instruction (j).
After the j target instruction, the program in Code Example 6.14 unconditionally continues executing the add instruction at the label target.
All of the instructions between the jump and the label are skipped.
j and jal are J-type instructions. jr is an R-type instruction that uses only the rs
operand.
Chapter 06.qxd 1/31/07 8:21 PM 
310 CHAPTER SIX Architecture
MIPS Assembly Code
addi $s0, $0, 4 # $s0  4
addi $s1, $0, 1 # $s1  1
j target # jump to target
addi $s1, $s1, 1 # not executed
sub $s1, $s1, $s0 # not executed
target:
add $s1, $s1, $s0 # $s1  1  4  5
Code Example 6.14 UNCONDITIONAL BRANCHING USING j
MIPS Assembly Code
0x00002000 addi $s0, $0, 0x2010 # $s0  0x2010
0x00002004 jr $s0 # jump to 0x00002010
0x00002008 addi $s1, $0, 1 # not executed
0x0000200c sra $s1, $s1, 2 # not executed
0x00002010 lw $s3, 44 ($s1) # executed after jr instruction
Code Example 6.15 UNCONDITIONAL BRANCHING USING jr
Code Example 6.15 shows the use of the jump register instruction
(jr). Instruction addresses are given to the left of each instruction. jr
$s0 jumps to the address held in $s0, 0x00002010.
6.4.3 Conditional Statements
if statements, if/else statements, and case statements are conditional
statements commonly used by high-level languages. They each conditionally execute a block of code consisting of one or more instructions.
This section shows how to translate these high-level constructs into
MIPS assembly language.
If Statements
An if statement executes a block of code, the if block, only when a
condition is met. Code Example 6.16 shows how to translate an if
statement into MIPS assembly code.
High-Level Code
if (i  j)
f  g  h;
f  f  i;
MIPS Assembly Code
# $s0  f, $s1  g, $s2  h, $s3  i, $s4  j
bne $s3, $s4, L1 # if i !  j, skip if block
add $s0, $s1, $s2 # if block: f  g  h
L1:
sub $s0, $s0, $s3 # f  f  i
Code Example 6.16 if STATEMENT
Chapter 06.qxd 1/31/
The assembly code for the if statement tests the opposite condition of the one in the high-level code. In Code Example 6.16, the
high-level code tests for i  j, and the assembly code tests for
i ! j. The bne instruction branches (skips the if block) when i !
j. Otherwise, i  j, the branch is not taken, and the if block is
executed as desired.
If/Else Statements
if/else statements execute one of two blocks of code depending on a
condition. When the condition in the if statement is met, the if block is
executed. Otherwise, the else block is executed. Code Example 6.17
shows an example if/else statement.
Like if statements, if/else assembly code tests the opposite condition of the one in the high-level code. For example, in Code Example 6.17,
the high-level code tests for i  j. The assembly code tests for the opposite condition (i ! j). If that opposite condition is TRUE, bne skips the
if block and executes the else block. Otherwise, the if block executes
and finishes with a jump instruction (j) to jump past the else block.
6.4 Programming 311
High-Level Code
if (i  j)
f  g  h;
else
f  f  i;
MIPS Assembly Code
# $s0  f, $s1  g, $s2  h, $s3  i, $s4  j
bne $s3, $s4, else # if i !  j, branch to else
add $s0, $s1, $s2 # if block: f  g  h
j L2 # skip past the else block
else:
sub $s0, $s0, $s3 # else block: f  f  i
L2:
Code Example 6.17 if/else STATEMENT
Switch/Case Statements*
switch/case statements execute one of several blocks of code depending
on the conditions. If no conditions are met, the default block is executed.
A case statement is equivalent to a series of nested if/else statements.
Code Example 6.18 shows two high-level code snippets with the same
functionality: they calculate the fee for an ATM (automatic teller machine)
withdrawal of $20, $50, or $100, as defined by amount. The MIPS assembly implementation is the same for both high-level code snippets.
6.4.4 Getting Loopy
Loops repeatedly execute a block of code depending on a condition.
for loops and while loops are common loop constructs used by highlevel languages. This section shows how to translate them into MIPS
assembly language.
Chapter 06.qxd 1/31/07 
312 CHAPTER SIX Architecture
High-Level Code
switch (amount) {
case 20: fee  2; break;
case 50: fee  3; break;
case 100: fee  5; break;
default: fee  0;
}
// equivalent function using if/else statements
if (amount  20) fee  2;
else if (amount  50) fee  3;
else if (amount  100) fee  5;
else fee  0;
MIPS Assembly Code
# $s0  amount, $s1  fee
case20:
addi $t0, $0, 20 # $t0  20
bne $s0, $t0, case50 # i  20? if not,
# skip to case50
addi $s1, $0, 2 # if so, fee  2
j done # and break out of case
case50:
addi $t0, $0, 50 # $t0  50
bne $s0, $t0, case100 # i  50? if not,
# skip to case100
addi $s1, $0, 3 # if so, fee  3
j done # and break out of case
case100:
addi $t0, $0, 100 # $t0  100
bne $s0, $t0, default # i  100? if not,
# skip to default
addi $s1, $0, 5 # if so, fee  5
j done # and break out of case
default:
add $s1, $0, $0 # charge  0
done:
Code Example 6.18 switch/case STATEMENT
While Loops
while loops repeatedly execute a block of code until a condition is not
met. The while loop in Code Example 6.19 determines the value of x
such that 2x  128. It executes seven times, until pow  128.
Like if/else statements, the assembly code for while loops tests
the opposite condition of the one given in the high-level code. If that
opposite condition is TRUE, the while loop is finished.
High-Level Code
int pow  1;
int x  0;
while (pow ! 128)
{
pow  pow * 2;
x  x  1;
}
MIPS Assembly Code
# $s0  pow, $s1  x
addi $s0, $0, 1 # pow  1
addi $s1, $0, 0 # x  0
addi $t0, $0, 128 # t0  128 for comparison
while:
beq $s0, $t0, done # if pow  128, exit while
sll $s0, $s0, 1 # pow  pow * 2
addi $s1, $s1, 1 # x  x  1
j while
done:
Code Example 6.19 while LOOP
Chapter 06.qxd 1/31/07 8:21 PM Page 312
In Code Example 6.19, the while loop compares pow to 128 and
exits the loop if it is equal. Otherwise it doubles pow (using a left shift),
increments x, and jumps back to the start of the while loop.
For Loops
for loops, like while loops, repeatedly execute a block of code until a
condition is not met. However, for loops add support for a loop variable, which typically keeps track of the number of loop executions.
A general format of the for loop is
for (initialization; condition; loop operation)
The initialization code executes before the for loop begins. The
condition is tested at the beginning of each loop. If the condition
is not met, the loop exits. The loop operation executes at the end of
each loop.
Code Example 6.20 adds the numbers from 0 to 9. The loop
variable, i, is initialized to 0 and is incremented at the end of each loop
iteration. At the beginning of each iteration, the for loop executes only
when i is not equal to 10. Otherwise, the loop is finished. In this case,
the for loop executes 10 times. for loops can be implemented using
a while loop, but the for loop is often convenient.
Magnitude Comparison
So far, the examples have used beq and bne to perform equality or
inequality comparisons and branches. MIPS provides the set less than
instruction, slt, for magnitude comparison. slt sets rd to 1 when rs 
rt. Otherwise, rd is 0.
6.4 Programming 313
High-Level Code
int sum  0;
for (i  0; i !  10; i  i  1) {
sum  sum  i;
}
// equivalent to the following while loop
int sum  0;
int i  0;
while (i ! 10) {
sum  sum  i;
i  i  1;
}
MIPS Assembly Code
# $s0  i, $s1  sum
add $s1, $0, $0 # sum  0
addi $s0, $0, 0 # i  0
addi $t0, $0, 10 # $t0  10
for:
beq $s0, $t0, done # if i  10, branch to done
add $s1, $s1, $s0 # sum  sum  i
addi $s0, $s0, 1 # increment i
j for
done:
Code Example 6.20 for LOOP
Chapter 06.qxd 1/31/07 
Example 6.6 LOOPS USING slt
The following high-level code adds the powers of 2 from 1 to 100.
Translate it into assembly language.
// high-level code
int sum  0;
for (i  1; i  101; i  i * 2)
sum  sum  i;
Solution: The assembly language code uses the set less than (slt) instruction to
perform the less than comparison in the for loop.
# MIPS assembly code
# $s0  i, $s1  sum
addi $s1, $0, 0 # sum  0
addi $s0, $0, 1 # i  1
addi $t0, $0, 101 # $t0  101
loop:
slt $t1, $s0, $t0 # if (i  101) $t1  1, else $t1  0
beq $t1, $0, done # if $t1  0 (i  101), branch to done
add $s1, $s1, $s0 # sum  sum  i
sll $s0, $s0, 1 # i  i * 2
j loop
done:
Exercise 6.12 explores how to use slt for other magnitude comparisons including greater than, greater than or equal, and less than or equal.
6.4.5 Arrays
Arrays are useful for accessing large amounts of similar data. An array is
organized as sequential data addresses in memory. Each array element is
identified by a number called its index. The number of elements in the
array is called the size of the array. This section shows how to access
array elements in memory.
Array Indexing
Figure 6.20 shows an array of five integers stored in memory. The index
ranges from 0 to 4. In this case, the array is stored in a processor’s main
memory starting at base address 0x10007000. The base address gives
the address of the first array element, array[0].
Code Example 6.21 multiplies the first two elements in array by 8
and stores them back in the array.
The first step in accessing an array element is to load the base address
of the array into a register. Code Example 6.21 loads the base address
314 CHAPTER SIX Architecture
Chapter 06.qxd 1/3
into $s0. Recall that the load upper immediate (lui) and or immediate
(ori) instructions can be used to load a 32-bit constant into a register.
Code Example 6.21 also illustrates why lw takes a base address and an
offset. The base address points to the start of the array. The offset can be
used to access subsequent elements of the array. For example, array[1] is
stored at memory address 0x10007004 (one word or four bytes after
array[0]), so it is accessed at an offset of 4 past the base address.
You might have noticed that the code for manipulating each of the
two array elements in Code Example 6.21 is essentially the same except
for the index. Duplicating the code is not a problem when accessing two
array elements, but it would become terribly inefficient for accessing all
of the elements in a large array. Code Example 6.22 uses a for loop to
multiply by 8 all of the elements of a 1000-element array stored at a
base address of 0x23B8F000.
Figure 6.21 shows the 1000-element array in memory. The index
into the array is now a variable (i) rather than a constant, so we cannot
take advantage of the immediate offset in lw. Instead, we compute the
address of the ith element and store it in $t0. Remember that each array
element is a word but that memory is byte addressed, so the offset from
6.4 Programming 315
array[4]
array[3]
array[2]
array[1]
0x10007000 array[0]
0x10007004
0x10007008
0x1000700C
0x10007010
Main Memory
Address Data
Figure 6.20 Five-entry
array with base address
of 0x10007000
High-Level Code
int array [5];
array[0]  array[0] * 8;
array[1]  array[1] * 8;
MIPS Assembly Code
# $s0  base address of array
lui $s0, 0x1000 # $s0  0x10000000
ori $s0, $s0, 0x7000 # $s0  0x10007000
lw $t1, 0($s0) # $t1  array[0]
sll $t1, $t1, 3 # $t1  $t1  3  $t1 * 8
sw $t1, 0($s0) # array[0]  $t1
lw $t1, 4($s0) # $t1  array[1]
sll $t1, $t1, 3 # $t1  $t1  3  $t1 * 8
sw $t1, 4($s0) # array[1]  $t1
Code Example 6.21 ACCESSING ARRAYS
Chapter 06.qx
the base address is i * 4. Shifting left by 2 is a convenient way to multiply by 4 in MIPS assembly language. This example readily extends to an
array of any size.
Bytes and Characters
Numbers in the range [128, 127] can be stored in a single byte rather
than an entire word. Because there are much fewer than 256 characters on
an English language keyboard, English characters are often represented by
bytes. The C language uses the type char to represent a byte or character.
Early computers lacked a standard mapping between bytes and
English characters, so exchanging text between computers was difficult.
In 1963, the American Standards Association published the American
Standard Code for Information Interchange (ASCII), which assigns each
text character a unique byte value. Table 6.2 shows these character
encodings for printable characters. The ASCII values are given in hexadecimal. Lower-case and upper-case letters differ by 0x20 (32).
316 CHAPTER SIX Architecture
High-Level Code
int i;
int array[1000];
for (i0; i  1000; i  i  1) {
array[i]  array[i] * 8;
}
MIPS Assembly Code
# $s0  array base address, $s1  i
# initialization code
lui $s0, 0x23B8 # $s0  0x23B80000
ori $s0, $s0, 0xF000 # $s0  0x23B8F000
addi $s1, $0 # i  0
addi $t2, $0, 1000 # $t2  1000
loop:
slt $t0, $s1, $t2 # i  1000?
beq $t0, $0, done # if not then done
sll $t0, $s1, 2 # $t0  i * 4 (byte offset)
add $t0, $t0, $s0 # address of array[i]
lw $t1, 0($t0) # $t1  array[i]
sll $t1, $t1, 3 # $t1  array[i] * 8
sw $t1, 0($t0) # array[i]  array[i] * 8
addi $s1, $s1, 1 # i  i  1
j loop # repeat
done:
Code Example 6.22 ACCESSING ARRAYS USING A for LOOP
Figure 6.21 Memory holding
array[1000] starting at base
address 0x23B8F000
Other program languages,
such as Java, use different
character encodings, most
notably Unicode. Unicode
uses 16 bits to represent each
character, so it supports
accents, umlauts, and Asian
languages. For more information, see www.unicode.org.
23B8FF9C array[999]
23B8FF98
23B8F004
23B8F000
array[998]
array[1]
array[0]
Main Memory
Address Data
Chapter 06.qxd 1
MIPS provides load byte and store byte instructions to manipulate
bytes or characters of data: load byte unsigned (lbu), load byte (lb), and
store byte (sb). All three are illustrated in Figure 6.22.
6.4 Programming 317
ASCII codes developed from
earlier forms of character
encoding. Beginning in 1838,
telegraph machines used
Morse code, a series of dots (.)
and dashes (), to represent
characters. For example, the
letters A, B, C, and D were
represented as .,  ...,
.., and .., respectively.
The number of dots and
dashes varied with each letter.
For efficiency, common letters
used shorter codes.
In 1874, Jean-MauriceEmile Baudot invented a 5-bit
code called the Baudot code.
For example, A, B, C, and D,
were represented as 00011,
11001, 01110, and 01001.
However, the 32 possible
encodings of this 5-bit code
were not sufficient for all the
English characters. But 8-bit
encoding was. Thus, as electronic communication became
prevalent, 8-bit ASCII encoding emerged as the standard.
Table 6.2 ASCII encodings
# Char # Char # Char # Char # Char # Char
20 space 30 0 40 @ 50 P 60 ′ 70 p
21 ! 31 1 41 A 51 Q 61 a 71 q
22 ″ 32 2 42 B 52 R 62 b 72 r
23 # 33 3 43 C 53 S 63 c 73 s
24 $ 34 4 44 D 54 T 64 d 74 t
25 % 35 5 45 E 55 U 65 e 75 u
26 & 36 6 46 F 56 V 66 f 76 v
27 ′ 37 7 47 G 57 W 67 g 77 w
28 ( 38 8 48 H 58 X 68 h 78 x
29 ) 39 9 49 I 59 Y 69 i 79 y
2A * 3A : 4A J 5A Z 6A j 7A z
2B  3B ; 4B K 5B [ 6B k 7B {
2C , 3C  4C L 5C \ 6C l 7C |
2D – 3D  4D M 5D ] 6D m 7D }
2E . 3E  4E N 5E  6E n 7E ~
2F / 3F ? 4F O 5F _ 6F o
Byte Address
Data 428CF7 03
3210
$s1 00 8C lbu $s1, 2($0)
Little-Endian Memory
0000
Registers
$s2 FFFF FF 8C lb $s2, 2($0)
$s3 XX XX XX 9B sb $s3, 3($0)
Figure 6.22 Instructions for
loading and storing bytes
Cha
Load byte unsigned (lbu) zero-extends the byte, and load byte (lb)
sign-extends the byte to fill the entire 32-bit register. Store byte (sb)
stores the least significant byte of the 32-bit register into the specified
byte address in memory. In Figure 6.22, lbu loads the byte at memory
address 2 into the least significant byte of $s1 and fills the remaining
register bits with 0. lb loads the sign-extended byte at memory address 2
into $s2. sb stores the least significant byte of $s3 into memory byte 3;
it replaces 0xF7 with 0x9B. The more significant bytes of $s3 are
ignored.
Example 6.7 USING lb AND sb TO ACCESS A CHARACTER ARRAY
The following high-level code converts a ten-entry array of characters from
lower-case to upper-case by subtracting 32 from each array entry. Translate it
into MIPS assembly language. Remember that the address difference between
array elements is now 1 byte, not 4 bytes. Assume that $s0 already holds the
base address of chararray.
// high-level code
char chararray[10];
int i;
for (i  0; i ! 10; i  i  1)
chararray[i]  chararray[i]  32;
Solution:
# MIPS assembly code
# $s0  base address of chararray, $s1  i
addi $s1, $0, 0 # i  0
addi $t0, $0, 10 # $t0  10
loop: beq $t0, $s1, done # if i  10, exit loop
add $t1, $s1, $s0 # $t1  address of chararray[i]
lb $t2, 0($t1) # $t2  array[i]
addi $t2, $t2, 32 # convert to upper case: $t1  $t1  32
sb $t2, 0($t1) # store new value in array:
# chararray[i]  $t1
addi $s1, $s1, 1 # i  i  1
j loop # repeat
done:
A series of characters is called a string. Strings have a variable
length, so programming languages must provide a way to determine the
length or end of the string. In C, the null character (0x00) signifies the
end of a string. For example, Figure 6.23 shows the string “Hello!”
(0x48 65 6C 6C 6F 21 00) stored in memory. The string is seven bytes
long and extends from address 0x1522FFF0 to 0x1522FFF6. The first
character of the string (H  0x48) is stored at the lowest byte address
(0x1522FFF0).
318 CHAPTER SIX Architecture
Figure 6.23 The string “Hello!”
stored in memory
Word
Address
1522FFF4
1522FFF0
Data
6C6C 65 48
2100 6F
Little-Endian Memory
Byte 3 Byte 0
Chapter 06.qxd 1/3
6.4.6 Procedure Calls
High-level languages often use procedures (also called functions) to reuse
frequently accessed code and to make a program more readable.
Procedures have inputs, called arguments, and an output, called the
return value. Procedures should calculate the return value and cause no
other unintended side effects.
When one procedure calls another, the calling procedure, the caller,
and the called procedure, the callee, must agree on where to put the
arguments and the return value. In MIPS, the caller conventionally
places up to four arguments in registers $a0–$a3 before making the procedure call, and the callee places the return value in registers $v0–$v1
before finishing. By following this convention, both procedures know
where to find the arguments and return value, even if the caller and
callee were written by different people.
The callee must not interfere with the function of the caller. Briefly,
this means that the callee must know where to return to after it completes
and it must not trample on any registers or memory needed by the caller.
The caller stores the return address in $ra at the same time it jumps to the
callee using the jump and link instruction (jal). The callee must not overwrite any architectural state or memory that the caller is depending on.
Specifically, the callee must leave the saved registers, $s0–$s7, $ra, and
the stack, a portion of memory used for temporary variables, unmodified.
This section shows how to call and return from a procedure. It
shows how procedures access input arguments and the return value and
how they use the stack to store temporary variables.
Procedure Calls and Returns
MIPS uses the jump and link instruction (jal) to call a procedure and
the jump register instruction (jr) to return from a procedure. Code
Example 6.23 shows the main procedure calling the simple procedure.
main is the caller, and simple is the callee. The simple procedure is
called with no input arguments and generates no return value; it simply
returns to the caller. In Code Example 6.23, instruction addresses are
given to the left of each MIPS instruction in hexadecimal.
6.4 Programming 319
High-Level Code MIPS Assembly Code
int main() {
simple(); 0x00400200 main: jal simple # call procedure
... 0x00400204 ...
}
// void means the function returns no value
void simple() {
return; 0x00401020 simple: jr $ra # return
}
Code Example 6.23 simple PROCEDURE CALL

Jump and link (jal) and jump register (jr $ra) are the two essential
instructions needed for a procedure call. jal performs two functions: it
stores the address of the next instruction (the instruction after jal) in
the return address register ($ra), and it jumps to the target instruction.
In Code Example 6.23, the main procedure calls the simple procedure by executing the jump and link (jal) instruction. jal jumps to the
simple label and stores 0x00400204 in $ra. The simple procedure
returns immediately by executing the instruction jr $ra, jumping to the
instruction address held in $ra. The main procedure then continues executing at this address, 0x00400204.
Input Arguments and Return Values
The simple procedure in Code Example 6.23 is not very useful, because
it receives no input from the calling procedure (main) and returns no output. By MIPS convention, procedures use $a0–$a3 for input arguments
and $v0–$v1 for the return value. In Code Example 6.24, the procedure
diffofsums is called with four arguments and returns one result.
According to MIPS convention, the calling procedure, main, places
the procedure arguments, from left to right, into the input registers,
$a0–$a3. The called procedure, diffofsums, stores the return value in
the return register, $v0.
A procedure that returns a 64-bit value, such as a double-precision
floating point number, uses both return registers, $v0 and $v1. When a
procedure with more than four arguments is called, the additional input
arguments are placed on the stack, which we discuss next.
320 CHAPTER SIX Architecture
High-Level Code MIPS Assembly Code
# $s0  y
int main () main:
{ ...
int y; addi $a0, $0, 2 # argument 0  2
addi $a1, $0, 3 # argument 1  3
... addi $a2, $0, 4 # argument 2  4
addi $a3, $0, 5 # argument 3  5
y  diffofsums (2, 3, 4, 5); jal diffofsums # call procedure
add $s0, $v0, $0 # y  returned value
... ...
}
# $s0  result
int diffofsums (int f, int g, int h, int i) diffofsums:
{ add $t0, $a0, $a1 # $t0  f  g
int result; add $t1, $a2, $a3 # $t1  h  i
sub $s0, $t0, $t1 # result  (f  g)  (h  i)
result  (f  g)  (h  i); add $v0, $s0, $0 # put return value in $v0
return result; jr $ra # return to caller
}
Code Example 6.24 PROCEDURE CALL WITH ARGUMENTS AND RETURN VALUES
Code Example 6.24 has some
subtle errors. Code Examples
6.25 and 6.26 on page 323
show improved versions of
the program.
Chapter 06.qxd 1/3
6.4 Programming 321
The Stack
The stack is memory that is used to save local variables within a procedure.
The stack expands (uses more memory) as the processor needs more
scratch space and contracts (uses less memory) when the processor no
longer needs the variables stored there. Before explaining how procedures
use the stack to store temporary variables, we explain how the stack works.
The stack is a last-in-first-out (LIFO) queue. Like a stack of dishes, the
last item pushed onto the stack (the top dish) is the first one that can be
pulled (popped) off. Each procedure may allocate stack space to store local
variables but must deallocate it before returning. The top of the stack, is
the most recently allocated space. Whereas a stack of dishes grows up in
space, the MIPS stack grows down in memory. The stack expands to lower
memory addresses when a program needs more scratch space.
Figure 6.24 shows a picture of the stack. The stack pointer, $sp, is a
special MIPS register that points to the top of the stack. A pointer is a
fancy name for a memory address. It points to (gives the address of)
data. For example, in Figure 6.24(a) the stack pointer, $sp, holds the
address value 0x7FFFFFFC and points to the data value 0x12345678.
$sp points to the top of the stack, the lowest accessible memory address
on the stack. Thus, in Figure 6.24(a), the stack cannot access memory
below memory word 0x7FFFFFFC.
The stack pointer ($sp) starts at a high memory address and decrements to expand as needed. Figure 6.24(b) shows the stack expanding to
allow two more data words of temporary storage. To do so, $sp decrements by 8 to become 0x7FFFFFF4. Two additional data words,
0xAABBCCDD and 0x11223344, are temporarily stored on the stack.
One of the important uses of the stack is to save and restore registers
that are used by a procedure. Recall that a procedure should calculate a
return value but have no other unintended side effects. In particular, it
should not modify any registers besides the one containing the return
value, $v0. The diffofsums procedure in Code Example 6.24 violates
this rule because it modifies $t0, $t1, and $s0. If main had been using
$t0, $t1, or $s0 before the call to diffofsums, the contents of these
registers would have been corrupted by the procedure call.
To solve this problem, a procedure saves registers on the stack
before it modifies them, then restores them from the stack before it
returns. Specifically, it performs the following steps.
1. Makes space on the stack to store the values of one or more registers.
2. Stores the values of the registers on the stack.
3. Executes the procedure using the registers.
4. Restores the original values of the registers from the stack.
5. Deallocates space on the stack.
Data
7FFFFFFC 12345678
7FFFFFF8
7FFFFFF4
7FFFFFF0
Address
$sp
(a)
7FFFFFFC
7FFFFFF8
7FFFFFF4
7FFFFFF0
Address
(b)
Data
12345678
$sp
AABBCCDD
11223344
Figure 6.24 The stack

Code Example 6.25 shows an improved version of diffofsums that
saves and restores $t0, $t1, and $s0. The new lines are indicated in
blue. Figure 6.25 shows the stack before, during, and after a call to the
diffofsums procedure from Code Example 6.25. diffofsums makes
room for three words on the stack by decrementing the stack pointer
($sp) by 12. It then stores the current values of $s0, $t0, and $t1 in
the newly allocated space. It executes the rest of the procedure, changing the values in these three registers. At the end of the procedure,
diffofsums restores the values of $s0, $t0, and $t1 from the stack,
deallocates its stack space, and returns. When the procedure returns,
$v0 holds the result, but there are no other side effects: $s0, $t0, $t1,
and $sp have the same values as they did before the procedure call.
The stack space that a procedure allocates for itself is called its stack
frame. diffofsums’s stack frame is three words deep. The principle of
modularity tells us that each procedure should access only its own stack
frame, not the frames belonging to other procedures.
Preserved Registers
Code Example 6.25 assumes that temporary registers $t0 and $t1 must
be saved and restored. If the calling procedure does not use those registers, the effort to save and restore them is wasted. To avoid this waste,
MIPS divides registers into preserved and nonpreserved categories. The
preserved registers include $s0–$s7 (hence their name, saved). The
nonpreserved registers include $t0–$t9 (hence their name, temporary).
A procedure must save and restore any of the preserved registers that it
wishes to use, but it can change the nonpreserved registers freely.
Code Example 6.26 shows a further improved version of diffofsums
that saves only $s0 on the stack. $t0 and $t1 are nonpreserved registers,
so they need not be saved.
Remember that when one procedure calls another, the former is the
caller and the latter is the callee. The callee must save and restore any
preserved registers that it wishes to use. The callee may change any of
the nonpreserved registers. Hence, if the caller is holding active data in a
322 CHAPTER SIX Architecture
Data
FC
F8
F4
F0
Address
$sp
(a)
?
Data
$sp
(c)
FC
F8
F4
F0
Address
?
Data
FC
F8
F4
F0
Address
$sp
(b)
$s0
$t0
?
stack frame
$t1
Figure 6.25 The stack
(a) before, (b) during, and
(c) after diffofsums procedure
call

nonpreserved register, the caller needs to save that nonpreserved register
before making the procedure call and then needs to restore it afterward.
For these reasons, preserved registers are also called callee-save, and
nonpreserved registers are called caller-save.
Table 6.3 summarizes which registers are preserved. $s0–$s7 are
generally used to hold local variables within a procedure, so they must
be saved. $ra must also be saved, so that the procedure knows where to
return. $t0–$t9 are used to hold temporary results before they are
assigned to local variables. These calculations typically complete before
a procedure call is made, so they are not preserved, and it is rare that
the caller needs to save them. $a0–$a3 are often overwritten in the
process of calling a procedure. Hence, they must be saved by the caller
if the caller depends on any of its own arguments after a called procedure returns. $v0–$v1 certainly should not be preserved, because the
callee returns its result in these registers.
6.4 Programming 323
MIPS Assembly Code
# $s0  result
diffofsums:
addi $sp, $sp, 12 # make space on stack to store three registers
sw $s0, 8($sp) # save $s0 on stack
sw $t0, 4($sp) # save $t0 on stack
sw $t1, 0($sp) # save $t1 on stack
add $t0, $a0, $a1 # $t0  f  g
add $t1, $a2, $a3 # $t1  h  i
sub $s0, $t0, $t1 # result  (f  g)  (h  i)
add $v0, $s0, $0 # put return value in $v0
lw $t1, 0($sp) # restore $t1 from stack
lw $t0, 4($sp) # restore $t0 from stack
lw $s0, 8($sp) # restore $s0 from stack
addi $sp, $sp, 12 # deallocate stack space
jr $ra # return to caller
Code Example 6.25 PROCEDURE SAVING REGISTERS ON THE STACK
MIPS Assembly Code
# $s0  result
diffofsums:
addi $sp, $sp, 4 # make space on stack to store one register
sw $s0, 0($sp) # save $s0 on stack
add $t0, $a0, $a1 # $t0  f  g
add $t1, $a2, $a3 # $t1  h  i
sub $s0, $t0, $t1 # result  (f  g)  (h  i)
add $v0, $s0, $0 # put return value in $v0
lw $s0, 0($sp) # restore $s0 from stack
addi $sp, $sp, 4 # deallocate stack space
jr $ra # return to caller
Code Example 6.26 PROCEDURE SAVING PRESERVED REGISTERS
ON THE STACK
Chapter 06.qxd 1
324 CHAPTER SIX Architecture
The stack above the stack pointer is automatically preserved as long
as the callee does not write to memory addresses above $sp. In this way,
it does not modify the stack frame of any other procedures. The stack
pointer itself is preserved, because the callee deallocates its stack frame
before returning by adding back the same amount that it subtracted
from $sp at the beginning of the procedure.
Recursive Procedure Calls
A procedure that does not call others is called a leaf procedure; an
example is diffofsums. A procedure that does call others is called a
nonleaf procedure. As mentioned earlier, nonleaf procedures are somewhat more complicated because they may need to save nonpreserved
registers on the stack before they call another procedure, and then
restore those registers afterward. Specifically, the caller saves any nonpreserved registers ($t0–$t9 and $a0–$a3) that are needed after the
call. The callee saves any of the preserved registers ($s0–$s7 and $ra)
that it intends to modify.
A recursive procedure is a nonleaf procedure that calls itself. The factorial function can be written as a recursive procedure call. Recall that
factorial(n)  n  (n  1)  (n  2)  ...  2  1. The factorial function can be rewritten recursively as factorial(n)  n  factorial(n  1).
The factorial of 1 is simply 1. Code Example 6.27 shows the factorial
function written as a recursive procedure. To conveniently refer to program addresses, we assume that the program starts at address 0x90.
The factorial procedure might modify $a0 and $ra, so it saves
them on the stack. It then checks whether n  2. If so, it puts the return
value of 1 in $v0, restores the stack pointer, and returns to the caller. It
does not have to reload $ra and $a0 in this case, because they were never
modified. If n  1, the procedure recursively calls factorial(n1). It
then restores the value of n ($a0) and the return address ($ra) from the
stack, performs the multiplication, and returns this result. The multiply
instruction (mul $v0, $a0, $v0) multiplies $a0 and $v0 and places the
result in $v0. It is discussed further in Section 6.7.1.
Table 6.3 Preserved and nonpreserved registers
Preserved Nonpreserved
Saved registers: $s0–$s7 Temporary registers: $t0–$t9
Return address: $ra Argument registers: $a0–$a3
Stack pointer: $sp Return value registers: $v0–$v1
Stack above the stack pointer Stack below the stack pointer
Ch
6.4 Programming 325
High-Level Code
int factorial (int n) {
if (n  1)
return 1;
else
return (n * factorial (n1));
}
MIPS Assembly Code
0x90 factorial: addi $sp, $sp, 8 # make room on stack
0x94 sw $a0, 4($sp) # store $a0
0x98 sw $ra, 0($sp) # store $ra
0x9C addi $t0, $0, 2 # $t0  2
0xA0 slt $t0, $a0, $t0 # a  1 ?
0xA4 beq $t0, $0, else # no: goto else
0xA8 addi $v0, $0, 1 # yes: return 1
0xAC addi $sp, $sp, 8 # restore $sp
0xB0 jr $ra # return
0xB4 else: addi $a0, $a0, 1 # n  n  1
0xB8 jal factorial # recursive call
0xBC lw $ra, 0($sp) # restore $ra
0xC0 lw $a0, 4($sp) # restore $a0
0xC4 addi $sp, $sp, 8 # restore $sp
0xC8 mul $v0, $a0, $v0 # n * factorial (n1)
0xCC jr $ra # return
Code Example 6.27 factorial RECURSIVE PROCEDURE CALL
Figure 6.26 shows the stack when executing factorial(3). We
assume that $sp initially points to 0xFC, as shown in Figure 6.26(a).
The procedure creates a two-word stack frame to hold $a0 and $ra. On
the first invocation, factorial saves $a0 (holding n  3) at 0xF8 and
$ra at 0xF4, as shown in Figure 6.26(b). The procedure then changes
$a0 to n  2 and recursively calls factorial(2), making $ra hold
0xBC. On the second invocation, it saves $a0 (holding n  2) at 0xF0
and $ra at 0xEC. This time, we know that $ra contains 0xBC. The procedure then changes $a0 to n  1 and recursively calls factorial(1).
On the third invocation, it saves $a0 (holding n  1) at 0xE8 and $ra at
0xE4. This time, $ra again contains 0xBC. The third invocation of
Figure 6.26 Stack during
factorial procedure call when
n  3: (a) before call, (b) after
last recursive call, (c) after
return
$sp
(a)
FC
F8
F4
F0
EC
E8
E4
E0
DC
Address Data
FC
F8
F4
F0
(b)
$ra
EC
E8
E4
E0
DC
$sp
$sp
$sp
$sp
Address Data
$a0 (0x3)
$ra (0xBC)
$a0 (0x2)
$ra (0xBC)
$a0 (0x1)
FC
F8
F4
F0
(c)
EC
E8
E4
E0
DC
$sp
$sp
$sp
$sp
$a0 = 1
$v0 = 1 x 1
$a0 = 2
$v0 = 2 x 1
$a0 = 3
$v0 = 3 x 2
$v0 = 6
Address Data
$ra
$a0 (0x3)
$ra (0xBC)
$a0 (0x2)
$ra (0xBC)
$a0 (0x1)
Chapter 06
factorial returns the value 1 in $v0 and deallocates the stack frame
before returning to the second invocation. The second invocation
restores n to 2, restores $ra to 0xBC (it happened to already have this
value), deallocates the stack frame, and returns $v0  2  1  2 to the
first invocation. The first invocation restores n to 3, restores $ra to the
return address of the caller, deallocates the stack frame, and returns
$v0  3  2  6. Figure 6.26(c) shows the stack as the recursively
called procedures return. When factorial returns to the caller, the
stack pointer is in its original position (0xFC), none of the contents of
the stack above the pointer have changed, and all of the preserved registers hold their original values. $v0 holds the return value, 6.
Additional Arguments and Local Variables*
Procedures may have more than four input arguments and local variables. The stack is used to store these temporary values. By MIPS convention, if a procedure has more than four arguments, the first four are
passed in the argument registers as usual. Additional arguments are
passed on the stack, just above $sp. The caller must expand its stack to
make room for the additional arguments. Figure 6.27(a) shows the
caller’s stack for calling a procedure with more than four arguments.
A procedure can also declare local variables or arrays. Local variables are declared within a procedure and can be accessed only within
that procedure. Local variables are stored in $s0–$s7; if there are too
many local variables, they can also be stored in the procedure’s stack
frame. In particular, local arrays are stored on the stack.
Figure 6.27(b) shows the organization of a callee’s stack frame. The
frame holds the procedure’s own arguments (if it calls other procedures),
the return address, and any of the saved registers that the procedure will
326 CHAPTER SIX Architecture
$sp
$ra (if needed)
additional arguments
$a0–$a3
(if needed)
$s0–$s7
(if needed)
local variables or
arrays
$spstack
frame
additional arguments
Figure 6.27 Stack usage:
(left) before call,
(right) after call
Chap
modify. It also holds local arrays and any excess local variables. If the
callee has more than four arguments, it finds them in the caller’s stack
frame. Accessing additional input arguments is the one exception in
which a procedure can access stack data not in its own stack frame.
6.5 ADDRESSING MODES
MIPS uses five addressing modes: register-only, immediate, base, PCrelative, and pseudo-direct. The first three modes (register-only, immediate, and base addressing) define modes of reading and writing
operands. The last two (PC-relative and pseudo-direct addressing)
define modes of writing the program counter, PC.
Register-Only Addressing
Register-only addressing uses registers for all source and destination
operands. All R-type instructions use register-only addressing.
Immediate Addressing
Immediate addressing uses the 16-bit immediate along with registers as
operands. Some I-type instructions, such as add immediate (addi) and
load upper immediate (lui), use immediate addressing.
Base Addressing
Memory access instructions, such as load word (lw) and store word (sw),
use base addressing. The effective address of the memory operand is
found by adding the base address in register rs to the sign-extended
16-bit offset found in the immediate field.
PC-relative Addressing
Conditional branch instructions use PC-relative addressing to specify the
new value of the PC if the branch is taken. The signed offset in the immediate field is added to the PC to obtain the new PC; hence, the branch
destination address is said to be relative to the current PC.
Code Example 6.28 shows part of the factorial procedure
from Code Example 6.27. Figure 6.28 shows the machine code for the
beq instruction. The branch target address (BTA) is the address of
the next instruction to execute if the branch is taken. The beq instruction in Figure 6.28 has a BTA of 0xB4, the instruction address of the
else label.
The 16-bit immediate field gives the number of instructions between
the BTA and the instruction after the branch instruction (the instruction
at PC4). In this case, the value in the immediate field of beq is 3
because the BTA (0xB4) is 3 instructions past PC4 (0xA8).
The processor calculates the BTA from the instruction by signextending the 16-bit immediate, multiplying it by 4 (to convert words to
bytes), and adding it to PC4.
6.5 Addressing Modes 327
Cha
Example 6.8 CALCULATING THE IMMEDIATE FIELD
FOR PC-RELATIVE ADDRESSING
Calculate the immediate field and show the machine code for the branch not
equal (bne) instruction in the following program.
# MIPS assembly code
0x40 loop: add $t1, $a0, $s0
0x44 lb $t1, 0($t1)
0x48 add $t2, $a1, $s0
0x4C sb $t1, 0($t2)
0x50 addi $s0, $s0, 1
0x54 bne $t1, $0, loop
0x58 lw $s0, 0($sp)
Solution: Figure 6.29 shows the machine code for the bne instruction. Its branch
target address, 0x40, is 6 instructions behind PC4 (0x58), so the immediate
field is 6.
328 CHAPTER SIX Architecture
op rs imm
beq $t0, $0, else
Machine Code Assembly Code
6 bits 5 bits 5 bits 16 bits
(0x11000003)
6 bits
Field Values
op rs rt imm
4 80 3
5 bits 5 bits 16 bits
000100 01000 00000 0000 0000 0000 0011
rt
Figure 6.28 Machine code for beq
MIPS Assembly Code
0xA4 beq $t0, $0, else
0xA8 addi $v0, $0, 1
0xAC addi $sp, $sp, 8
0xB0 jr $ra
0xB4 else: addi $a0, $a0, 1
0xB8 jal factorial
Code Example 6.28 CALCULATING THE BRANCH TARGET ADDRESS
bne $t1, $0, loop 000101 01001 00000
Assembly Code Machine Code
5 9 0 -6
6 bits 5 bits 5 bits 16 bits
(0x1520FFFA)
6 bits 5 bits 5 bits 16 bits
op rs rt imm op rs rt imm
1111 1111 1111 1010
Field Values
Figure 6.29 bne machine code
Pseudo-Direct Addressing
In direct addressing, an address is specified in the instruction. The jump
instructions, j and jal, ideally would use direct addressing to specify a
C
32-bit jump target address (JTA) to indicate the instruction address to
execute next.
Unfortunately, the J-type instruction encoding does not have enough
bits to specify a full 32-bit JTA. Six bits of the instruction are used for
the opcode, so only 26 bits are left to encode the JTA. Fortunately, the
two least significant bits, JTA1:0, should always be 0, because instructions are word aligned. The next 26 bits, JTA27:2, are taken from the
addr field of the instruction. The four most significant bits, JTA31:28, are
obtained from the four most significant bits of PC4. This addressing
mode is called pseudo-direct.
Code Example 6.29 illustrates a jal instruction using pseudo-direct
addressing. The JTA of the jal instruction is 0x004000A0. Figure 6.30
shows the machine code for this jal instruction. The top four bits and
bottom two bits of the JTA are discarded. The remaining bits are stored
in the 26-bit address field (addr).
The processor calculates the JTA from the J-type instruction by
appending two 0’s and prepending the four most significant bits of
PC4 to the 26-bit address field (addr).
Because the four most significant bits of the JTA are taken from
PC4, the jump range is limited. The range limits of branch and jump
instructions are explored in Exercises 6.23 to 6.26. All J-type instructions, j and jal, use pseudo-direct addressing.
Note that the jump register instruction, jr, is not a J-type instruction. It is an R-type instruction that jumps to the 32-bit value held in
register rs.
6.5 Addressing Modes 329
MIPS Assembly Code
0x0040005C jal sum
...
0x004000A0 sum: add $v0, $a0, $a1
Code Example 6.29 CALCULATING THE JUMP TARGET ADDRESS
op
jal sum
Assembly Code Machine Code
3 (0x0C100028)
op
6 bits
JTA 0000 0000 0100 0000 0000 0000 1010 0000
26-bit addr (0x0100028)
(0x004000A0)
0000 0000
0 0 0 2 8
addr
0x0100028
26 bits 6 bits 26 bits
000011 00 0001 0000 0000 0000 0010 1000
addr
0000 0100 0000 0000 0000 1010
1 0
Field Values
Figure 6.30 jal machine code
Cha
330 CHAPTER SIX Architecture
6.6 LIGHTS, CAMERA, ACTION: COMPILING,
ASSEMBLING, AND LOADING
Up until now, we have shown how to translate short high-level code
snippets into assembly and machine code. This section describes how to
compile and assemble a complete high-level program and how to load
the program into memory for execution.
We begin by introducing the MIPS memory map, which defines
where code, data, and stack memory are located. We then show the steps
of code execution for a sample program.
6.6.1 The Memory Map
With 32-bit addresses, the MIPS address space spans 232 bytes  4 gigabytes (GB). Word addresses are divisible by 4 and range from 0 to
0xFFFFFFFC. Figure 6.31 shows the MIPS memory map. The MIPS
architecture divides the address space into four parts or segments: the
text segment, global data segment, dynamic data segment, and reserved
segments. The following sections describes each segment.
The Text Segment
The text segment stores the machine language program. It is large
enough to accommodate almost 256 MB of code. Note that the four
most significant bits of the address in the text space are all 0, so the j
instruction can directly jump to any address in the program.
The Global Data Segment
The global data segment stores global variables that, in contrast to local
variables, can be seen by all procedures in a program. Global variables
Address Segment
$sp = 0x7FFFFFFC
0xFFFFFFFC
0x80000000
0x7FFFFFFC
0x10010000
0x1000FFFC
0x10000000
0x0FFFFFFC
0x00400000
0x003FFFFC
0x00000000
Reserved
Stack
Heap
Global Data
Text
Reserved
$gp = 0x10008000
PC = 0x00400000
Figure 6.31 MIPS memory map Dynamic Data
C
are defined at start-up, before the program begins executing. These variables are declared outside the main procedure in a C program and can
be accessed by any procedure. The global data segment is large enough
to store 64 KB of global variables.
Global variables are accessed using the global pointer ($gp), which is
initialized to 0x100080000. Unlike the stack pointer ($sp), $gp does not
change during program execution. Any global variable can be accessed
with a 16-bit positive or negative offset from $gp. The offset is known at
assembly time, so the variables can be efficiently accessed using base
addressing mode with constant offsets.
The Dynamic Data Segment
The dynamic data segment holds the stack and the heap. The data in this
segment are not known at start-up but are dynamically allocated and deallocated throughout the execution of the program. This is the largest
segment of memory used by a program, spanning almost 2 GB of the
address space.
As discussed in Section 6.4.6, the stack is used to save and restore
registers used by procedures and to hold local variables such as arrays.
The stack grows downward from the top of the dynamic data segment
(0x7FFFFFFC) and is accessed in last-in-first-out (LIFO) order.
The heap stores data that is allocated by the program during runtime. In C, memory allocations are made by the malloc function; in
C and Java, new is used to allocate memory. Like a heap of clothes
on a dorm room floor, heap data can be used and discarded in any order.
The heap grows upward from the bottom of the dynamic data segment.
If the stack and heap ever grow into each other, the program’s data
can become corrupted. The memory allocator tries to ensure that this
never happens by returning an out-of-memory error if there is insufficient space to allocate more dynamic data.
The Reserved Segments
The reserved segments are used by the operating system and cannot
directly be used by the program. Part of the reserved memory is used for
interrupts (see Section 7.7) and for memory-mapped I/O (see Section 8.5).
6.6.2 Translating and Starting a Program
Figure 6.32 shows the steps required to translate a program from a highlevel language into machine language and to start executing that program.
First, the high-level code is compiled into assembly code. The assembly
code is assembled into machine code in an object file. The linker combines
the machine code with object code from libraries and other files to produce an entire executable program. In practice, most compilers perform all
three steps of compiling, assembling, and linking. Finally, the loader loads
6.6 Lights, Camera, Action: Compiling, Assembling, and Loading 331
Grace Hopper, 1906–1992.
Graduated from Yale
University with a Ph.D. in
mathematics. Developed the
first compiler while working
for the Remington Rand
Corporation and was instrumental in developing the
COBOL programming language. As a naval officer, she
received many awards, including a World War II Victory
Medal and the National
Defence Service Medal.
Ch
the program into memory and starts execution. The remainder of this section walks through these steps for a simple program.
Step 1: Compilation
A compiler translates high-level code into assembly language. Code
Example 6.30 shows a simple high-level program with three global
332 CHAPTER SIX Architecture
Assembly Code
High Level Code
Compiler
Object File
Assembler
Executable
Linker
Memory
Loader
Object Files
Library Files
Figure 6.32 Steps for
translating and starting
a program
High-Level Code
int f, g, y; // global variables
int main (void)
{
f  2;
g  3;
y  sum (f, g);
return y;
}
int sum (int a, int b) {
return (a  b);
}
MIPS Assembly Code
.data
f:
g:
y:
.text
main:
addi $sp, $sp, 4 # make stack frame
sw $ra, 0($sp) # store $ra on stack
addi $a0, $0, 2 # $a0  2
sw $a0, f # f  2
addi $a1, $0, 3 # $a1  3
sw $a1, g # g  3
jal sum # call sum procedure
sw $v0, y # y  sum (f, g)
lw $ra, 0($sp) # restore $ra from stack
addi $sp, $sp, 4 # restore stack pointer
jr $ra # return to operating system
sum:
add $v0, $a0, $a1 # $v0  a  b
jr $ra # return to caller
Code Example 6.30 COMPILING A HIGH-LEVEL PROGRAM
Chapter 06.
variables and two procedures, along with the assembly code produced by
a typical compiler. The .data and .text keywords are assembler directives
that indicate where the text and data segments begin. Labels are used for
global variables f, g, and y. Their storage location will be determined by
the assembler; for now, they are left as symbols in the code.
Step 2: Assembling
The assembler turns the assembly language code into an object file containing machine language code. The assembler makes two passes
through the assembly code. On the first pass, the assembler assigns
instruction addresses and finds all the symbols, such as labels and global
variable names. The code after the first assembler pass is shown here.
0x00400000 main: addi $sp, $sp, 4
0x00400004 sw $ra, 0($sp)
0x00400008 addi $a0, $0, 2
0x0040000C sw $a0, f
0x00400010 addi $a1, $0, 3
0x00400014 sw $a1, g
0x00400018 jal sum
0x0040001C sw $v0, y
0x00400020 lw $ra, 0($sp)
0x00400024 addi $sp, $sp, 4
0x00400028 jr $ra
0x0040002C sum: add $v0, $a0, $a1
0x00400030 jr $ra
The names and addresses of the symbols are kept in a symbol table,
as shown in Table 6.4 for this code. The symbol addresses are filled
in after the first pass, when the addresses of labels are known. Global
variables are assigned storage locations in the global data segment of
memory, starting at memory address 0x10000000.
On the second pass through the code, the assembler produces the
machine language code. Addresses for the global variables and labels are
taken from the symbol table. The machine language code and symbol
table are stored in the object file.
6.6 Lights, Camera, Action: Compiling, Assembling, and Loading 333
Table 6.4 Symbol table
Symbol Address
f 0x10000000
g 0x10000004
y 0x10000008
main 0x00400000
sum 0x0040002C

Executable file header Text Size Data Size
Text segment
Data segment
Address
Address
0x00400000
0x00400004
0x00400008
0x0040000C
0x00400010
0x00400014
0x00400018
0x0040001C
0x00400020
0x00400024
0x00400028
0x0040002C
0x00400030
addi $sp, $sp, –4
sw $ra, 0 ($sp)
addi $a0, $0, 2
sw $a0, 0x8000 ($gp)
addi $a1, $0, 3
sw $a1, 0x8004 ($gp)
jal 0x0040002C
sw $v0, 0x8008 ($gp)
lw $ra, 0 ($sp)
addi $sp, $sp, –4
jr $ra
add $v0, $a0, $a1
jr $ra
0x10000000
0x10000004
0x10000008
f
g
y
0x34 (52 bytes) 0xC (12 bytes)
0x23BDFFFC
0xAFBF0000
0x20040002
0xAF848000
0x20050003
0xAF858004
0x0C10000B
0xAF828008
0x8FBF0000
0x23BD0004
0x03E00008
0x00851020
0x03E0008
Instruction
Data
334 CHAPTER SIX Architecture
Step 3: Linking
Most large programs contain more than one file. If the programmer
changes only one of the files, it would be wasteful to recompile and
reassemble the other files. In particular, programs often call procedures
in library files; these library files almost never change. If a file of highlevel code is not changed, the associated object file need not be updated.
The job of the linker is to combine all of the object files into one
machine language file called the executable. The linker relocates the data
and instructions in the object files so that they are not all on top of each
other. It uses the information in the symbol tables to adjust the addresses
of global variables and of labels that are relocated.
In our example, there is only one object file, so no relocation is
necessary. Figure 6.33 shows the executable file. It has three sections: the
executable file header, the text segment, and the data segment. The executable file header reports the text size (code size) and data size (amount
of globally declared data). Both are given in units of bytes. The text
segment gives the instructions and the addresses where they are to be
stored.
The figure shows the instructions in human-readable format next to
the machine code for ease of interpretation, but the executable file
includes only machine instructions. The data segment gives the address
of each global variable. The global variables are addressed with respect
to the base address given by the global pointer, $gp. For example, the
Figure 6.33 Executable

y
g
f
0x03E00008
0x00851020
0x03E00008
0x23BD0004
0x8FBF0000
0xAF828008
0x0C10000B
0xAF858004
0x20050003
0xAF848000
0x20040002
0xAFBF0000
0x23BDFFFC
Address Memory
0x7FFFFFFC $sp = 0x7FFFFFFC
0x10010000
0x00400000
Stack
Heap
$gp = 0x10008000
PC = 0x00400000
0x10000000
Reserved
Reserved
first store instruction, sw $a0, 0x8000($gp), stores the value 2 to the
global variable f, which is located at memory address 0x10000000.
Remember that the offset, 0x8000, is a 16-bit signed number that is
sign-extended and added to the base address, $gp. So, $gp  0x8000
0x10008000  0xFFFF8000  0x10000000, the memory address of
variable f.
Step 4: Loading
The operating system loads a program by reading the text segment of
the executable file from a storage device (usually the hard disk) into the
text segment of memory. The operating system sets $gp to 0x10008000
(the middle of the global data segment) and $sp to 0x7FFFFFFC (the
top of the dynamic data segment), then performs a jal 0x00400000 to
jump to the beginning of the program. Figure 6.34 shows the memory
map at the beginning of program execution.
6.6 Lights, Camera, Action: Compiling, Assembling, and Loading 335
Figure 6.34 Executable loaded
in memory
Chap
6.7 ODDS AND ENDS*
This section covers a few optional topics that do not fit naturally elsewhere
in the chapter. These topics include pseudoinstructions, exceptions, signed
and unsigned arithmetic instructions, and floating-point instructions.
6.7.1 Pseudoinstructions
If an instruction is not available in the MIPS instruction set, it is probably because the same operation can be performed using one or more
existing MIPS instructions. Remember that MIPS is a reduced instruction set computer (RISC), so the instruction size and hardware complexity are minimized by keeping the number of instructions small.
However, MIPS defines pseudoinstructions that are not actually
part of the instruction set but are commonly used by programmers and
compilers. When converted to machine code, pseudoinstructions are
translated into one or more MIPS instructions.
Table 6.5 gives examples of pseudoinstructions and the MIPS
instructions used to implement them. For example, the load immediate
pseudoinstruction (li) loads a 32-bit constant using a combination of
lui and ori instructions. The multiply pseudoinstruction (mul) provides a three-operand multiply, multiplying two registers and putting the
32 least significant bits of the result into a third register. The no operation pseudoinstruction (nop, pronounced “no op”) performs no operation. The PC is incremented by 4 upon its execution. No other registers
or memory values are altered. The machine code for the nop instruction
is 0x00000000.
Some pseudoinstructions require a temporary register for intermediate
calculations. For example, the pseudoinstruction beq $t2, imm15:0, Loop
compares $t2 to a 16-bit immediate, imm15:0. This pseudoinstruction
336 CHAPTER SIX Architecture
Table 6.5 Pseudoinstructions
Corresponding
Pseudoinstruction MIPS Instructions
li $s0, 0x1234AA77 lui $s0, 0x1234
ori $s0, 0xAA77
mul $s0, $s1, $s2 mult $s1, $s2
mflo $s0
clear $t0 add $t0, $0, $0
move $s1, $s2 add $s2, $s1, $0
nop sll $0, $0, 0

requires a temporary register in which to store the 16-bit immediate.
Assemblers use the assembler register, $at, for such purposes. Table 6.6
shows how the assembler uses $at in converting a pseudoinstruction to
real MIPS instructions. We leave it as Exercise 6.31 to implement other
pseudoinstructions such as rotate left (rol) and rotate right (ror).
6.7.2 Exceptions
An exception is like an unscheduled procedure call that jumps to a new
address. Exceptions may be caused by hardware or software. For example,
the processor may receive notification that the user pressed a key on a keyboard. The processor may stop what it is doing, determine which key was
pressed, save it for future reference, then resume the program that was running. Such a hardware exception triggered by an input/output (I/O) device
such as a keyboard is often called an interrupt. Alternatively, the program
may encounter an error condition such as an undefined instruction. The
program then jumps to code in the operating system (OS), which may
choose to terminate the offending program. Software exceptions are sometimes called traps. Other causes of exceptions include division by zero,
attempts to read nonexistent memory, hardware malfunctions, debugger
breakpoints, and arithmetic overflow (see Section 6.7.3).
The processor records the cause of an exception and the value of the PC
at the time the exception occurs. It then jumps to the exception handler procedure. The exception handler is code (usually in the OS) that examines the
cause of the exception and responds appropriately (by reading the keyboard
on a hardware interrupt, for example). It then returns to the program that
was executing before the exception took place. In MIPS, the exception handler is always located at 0x80000180. When an exception occurs, the
processor always jumps to this instruction address, regardless of the cause.
The MIPS architecture uses a special-purpose register, called the
Cause register, to record the cause of the exception. Different codes
are used to record different exception causes, as given in Table 6.7. The
exception handler code reads the Cause register to determine how to
handle the exception. Some other architectures jump to a different exception handler for each different cause instead of using a Cause register.
MIPS uses another special-purpose register called the Exception
Program Counter (EPC) to store the value of the PC at the time an exception
6.7 Odds and Ends 337
Table 6.6 Pseudoinstruction using $at
Corresponding
Pseudoinstruction MIPS Instructions
beq $t2, imm15:0, Loop addi $at, $0, imm15:0
beq $t2, $at, Loop

takes place. The processor returns to the address in EPC after handling the
exception. This is analogous to using $ra to store the old value of the PC
during a jal instruction.
The EPC and Cause registers are not part of the MIPS register file.
The mfc0 (move from coprocessor 0) instruction copies these and other
special-purpose registers into one of the general purpose registers.
Coprocessor 0 is called the MIPS processor control; it handles interrupts
and processor diagnostics. For example, mfc0 $t0, Cause copies the
Cause register into $t0.
The syscall and break instructions cause traps to perform system
calls or debugger breakpoints. The exception handler uses the EPC to
look up the instruction and determine the nature of the system call or
breakpoint by looking at the fields of the instruction.
In summary, an exception causes the processor to jump to the
exception handler. The exception handler saves registers on the stack,
then uses mfc0 to look at the cause and respond accordingly. When the
handler is finished, it restores the registers from the stack, copies the
return address from EPC to $k0 using mfc0, and returns using jr $k0.
6.7.3 Signed and Unsigned Instructions
Recall that a binary number may be signed or unsigned. The MIPS architecture uses two’s complement representation of signed numbers. MIPS
has certain instructions that come in signed and unsigned flavors, including addition and subtraction, multiplication and division, set less than,
and partial word loads.
Addition and Subtraction
Addition and subtraction are performed identically whether the number
is signed or unsigned. However, the interpretation of the results is different.
As mentioned in Section 1.4.6, if two large signed numbers are added
together, the result may incorrectly produce the opposite sign. For example, adding the following two huge positive numbers gives a negative
338 CHAPTER SIX Architecture
Table 6.7 Exception cause codes
Exception Cause
hardware interrupt 0x00000000
system call 0x00000020
breakpoint/divide by 0 0x00000024
undefined instruction 0x00000028
arithmetic overflow 0x00000030
$k0 and $k1 are included in the
MIPS register set. They are
reserved by the OS for exception handling. They do not
need to be saved and restored
during exceptions.

result: 0x7FFFFFFF  0x7FFFFFFF  0xFFFFFFFE  2. Similarly,
adding two huge negative numbers gives a positive result, 0x80000001
0x80000001  0x00000002. This is called arithmetic overflow.
The C language ignores arithmetic overflows, but other languages,
such as Fortran, require that the program be notified. As mentioned in
Section 6.7.2, the MIPS processor takes an exception on arithmetic overflow. The program can decide what to do about the overflow (for example, it might repeat the calculation with greater precision to avoid the
overflow), then return to where it left off.
MIPS provides signed and unsigned versions of addition and subtraction. The signed versions are add, addi, and sub. The unsigned versions are addu, addiu, and subu. The two versions are identical except
that signed versions trigger an exception on overflow, whereas unsigned
versions do not. Because C ignores exceptions, C programs technically
use the unsigned versions of these instructions.
Multiplication and Division
Multiplication and division behave differently for signed and unsigned
numbers. For example, as an unsigned number, 0xFFFFFFFF represents
a large number, but as a signed number it represents 1. Hence,
0xFFFFFFFF  0xFFFFFFFF would equal 0xFFFFFFFE00000001 if the
numbers were unsigned but 0x0000000000000001 if the numbers were
signed.
Therefore, multiplication and division come in both signed and
unsigned flavors. mult and div treat the operands as signed numbers.
multu and divu treat the operands as unsigned numbers.
Set Less Than
Set less than instructions can compare either two registers (slt) or a register and an immediate (slti). Set less than also comes in signed (slt
and slti) and unsigned (sltu and sltiu) versions. In a signed comparison, 0x80000000 is less than any other number, because it is the most
negative two’s complement number. In an unsigned comparison,
0x80000000 is greater than 0x7FFFFFFF but less than 0x80000001,
because all numbers are positive.
Beware that sltiu sign-extends the immediate before treating it as
an unsigned number. For example, sltiu $s0, $s1, 0x8042 compares
$s1 to 0xFFFF8042, treating the immediate as a large positive number.
Loads
As described in Section 6.4.5, byte loads come in signed (lb) and
unsigned (lbu) versions. lb sign-extends the byte, and lbu zero-extends
the byte to fill the entire 32-bit register. Similarly, MIPS provides signed
and unsigned half-word loads (lh and lhu), which load two bytes into
the lower half and sign- or zero-extend the upper half of the word.
6.7 Odds and Ends 339
Chapt
cop ft fs fd funct
6 bits 5 bits 5 bits 5 bits 5 bits 6 bits
F-type
op
6.7.4 Floating-Point Instructions
The MIPS architecture defines an optional floating-point coprocessor,
known as coprocessor 1. In early MIPS implementations, the floatingpoint coprocessor was a separate chip that users could purchase if
they needed fast floating-point math. In most recent MIPS implementations, the floating-point coprocessor is built in alongside the main
processor.
MIPS defines 32 32-bit floating-point registers, $f0–$f31. These are
separate from the ordinary registers used so far. MIPS supports both
single- and double-precision IEEE floating point arithmetic. Doubleprecision (64-bit) numbers are stored in pairs of 32-bit registers, so only
the 16 even-numbered registers ($f0, $f2, $f4, . . . , $f30) are used
to specify double-precision operations. By convention, certain registers
are reserved for certain purposes, as given in Table 6.8.
Floating-point instructions all have an opcode of 17 (100012). They
require both a funct field and a cop (coprocessor) field to indicate the
type of instruction. Hence, MIPS defines the F-type instruction format
for floating-point instructions, shown in Figure 6.35. Floating-point
instructions come in both single- and double-precision flavors. cop  16
(100002) for single-precision instructions or 17 (100012) for doubleprecision instructions. Like R-type instructions, F-type instructions have
two source operands, fs and ft, and one destination, fd.
Instruction precision is indicated by .s and .d in the mnemonic.
Floating-point arithmetic instructions include addition (add.s, add.d),
subtraction (sub.sz, sub.d), multiplication (mul.s, mul.d), and division (div.s, div.d) as well as negation (neg.s, neg.d) and absolute
value (abs.s, abs.d).
340 CHAPTER SIX Architecture
Table 6.8 MIPS floating-point register set
Name Number Use
$fv0–$fv1 0, 2 procedure return values
$ft0–$ft3 4, 6, 8, 10 temporary variables
$fa0–$fa1 12, 14 procedure arguments
$ft4–$ft5 16, 18 temporary variables
$fs0–$fs5 20, 22, 24, 26, 28, 30 saved variables
Figure 6.35 F-type machine
instruction format
C
Floating-point branches have two parts. First, a compare instruction
is used to set or clear the floating-point condition flag
(fpcond). Then, a
conditional branch checks the value of the flag. The compare instructions include equality (c.seq.s/c.seq.d), less than (c.lt.s/c.lt.d),
and less than or equal to (c.le.s/c.le.d). The conditional branch
instructions are bc1f and bc1t that branch if fpcond is FALSE or
TRUE, respectively. Inequality, greater than or equal to, and greater than
comparisons are performed with seq
, lt, and le, followed by bc1f
.
Floating-point registers are loaded and stored from memory using
lwc1 and swc1. These instructions move 32 bits, so two are necessary to
handle a double-precision number.
6.8 REAL-WORLD PERSPECTIVE: IA-32 ARCHITECTURE*
Almost all personal computers today use IA-32 architecture microprocessors. IA-32 is a 32-bit architecture originally developed by Intel.
AMD also sells IA-32 compatible microprocessors.
The IA-32 architecture has a long and convoluted history dating
back to 1978, when Intel announced the 16-bit 8086 microprocessor.
IBM selected the 8086 and its cousin, the 8088, for IBM’s first personal
computers. In 1985, Intel introduced the 32-bit 80386 microprocessor,
which was backward compatible with the 8086, so it could run software
developed for earlier PCs. Processor architectures compatible with the
80386 are called IA-32 or x86 processors. The Pentium, Core, and
Athlon processors are well known IA-32 processors. Section 7.9
describes the evolution of IA-32 microprocessors in more detail.
Various groups at Intel and AMD over many years have shoehorned
more instructions and capabilities into the antiquated architecture. The
result is far less elegant than MIPS. As Patterson and Hennessy explain,
“this checkered ancestry has led to an architecture that is difficult to
explain and impossible to love.” However, software compatibility is far
more important than technical elegance, so IA-32 has been the de facto
PC standard for more than two decades. More than 100 million IA-32
processors are sold every year. This huge market justifies more than
$5 billion of research and development annually to continue improving
the processors.
IA-32 is an example of a Complex Instruction Set Computer
(CISC
)
architecture. In contrast to RISC architectures such as MIPS, each CISC
instruction can do more work. Programs for CISC architectures usually
require fewer instructions. The instruction encodings were selected to be
more compact, so as to save memory, when RAM was far more expensive than it is today; instructions are of variable length and are often less
than 32 bits. The trade-off is that complicated instructions are more difficult to decode and tend to execute more slowly.
6.8 Real-World Perspective: IA-32 Architecture 341

This section introduces the IA-32 architecture. The goal is not to
make you into an IA-32 assembly language programmer, but rather to
illustrate some of the similarities and differences between IA-32 and MIPS.
We think it is interesting to see how IA-32 works. However, none of the
material in this section is needed to understand the rest of the book.
Major differences between IA-32 and MIPS are summarized in Table 6.9.
6.8.1 IA-32 Registers
The 8086 microprocessor provided eight 16-bit registers. It could separately access the upper and lower eight bits of some of these registers.
When the 32-bit 80386 was introduced, the registers were extended to
32 bits. These registers are called EAX, ECX, EDX, EBX, ESP, EBP,
ESI, and EDI. For backward compatibility, the bottom 16 bits and some
of the bottom 8-bit portions are also usable, as shown in Figure 6.36.
The eight registers are almost, but not quite, general purpose.
Certain instructions cannot use certain registers. Other instructions
always put their results in certain registers. Like $sp in MIPS, ESP is normally reserved for the stack pointer.
The IA-32 program counter is called EIP (the extended instruction
pointer). Like the MIPS PC, it advances from one instruction to the next
or can be changed with branch, jump, and subroutine call instructions.
6.8.2 IA-32 Operands
MIPS instructions always act on registers or immediates. Explicit load
and store instructions are needed to move data between memory and
the registers. In contrast, IA-32 instructions may operate on registers,
immediates, or memory. This partially compensates for the small set of
registers.
342 CHAPTER SIX Architecture
Table 6.9 Major differences between MIPS and IA-32
Feature MIPS IA-32
# of registers 32 general purpose 8, some restrictions on purpose
# of operands 3 (2 source, 1 destination) 2 (1 source, 1 source/destination)
operand location registers or immediates registers, immediates, or memory
operand size 32 bits 8, 16, or 32 bits
condition codes no yes
instruction types simple simple and complicated
instruction encoding fixed, 4 bytes variable, 1–15 bytes
Figure 6.36 IA-32 registers
EAX
0
AH
AX
ECX
1
CH
CX
EDX
2
Byte 3
Byte 2
Byte 1
Byte 0
DH
DX
EBX
3
BH
BX
ESP
4 SP
EBP
5 BP
ESI
6 SI
EDI
7 DI
AL
CL
DL
BL

MIPS instructions generally specify three operands: two sources
and one destination. IA-32 instructions specify only two operands.
The first is a source. The second is both a source and the destination.
Hence, IA-32 instructions always overwrite one of their sources
with the result. Table 6.10 lists the combinations of operand locations
in IA-32. All of the combinations are possible except memory to
memory.
Like MIPS, IA-32 has a 32-bit memory space that is byte-addressable.
However, IA-32 also supports a much wider variety of memory addressing
modes. The memory location is specified with any combination of a base
register, displacement, and a scaled index register. Table 6.11 illustrates
these combinations. The displacement can be an 8-, 16-, or 32-bit value.
The scale multiplying the index register can be 1, 2, 4, or 8. The base
displacement mode is equivalent to the MIPS base addressing mode for
loads and stores. The scaled index provides an easy way to access arrays
or structures of 2-, 4-, or 8-byte elements without having to issue a
sequence of instructions to generate the address.
6.8 Real-World Perspective: IA-32 Architecture 343
Table 6.10 Operand locations
Source/
Destination Source Example Meaning
register register add EAX, EBX EAX – EAX  EBX
register immediate add EAX, 42 EAX – EAX  42
register memory add EAX, [20] EAX – EAX  Mem[20]
memory register add [20], EAX Mem[20] – Mem[20]  EAX
memory immediate add [20], 42 Mem[20] – Mem[20]  42
Table 6.11 Memory addressing modes
Example Meaning Comment
add EAX, [20] EAX – EAX  Mem[20] displacement
add EAX, [ESP] EAX – EAX  Mem[ESP] base addressing
add EAX, [EDX40] EAX – EAX  Mem[EDX40] base  displacement
add EAX, [60EDI*4] EAX – EAX  Mem[60EDI*4] displacement  scaled index
add EAX, [EDX80EDI*2] EAX – EAX  Mem[EDX80EDI*2] base  displacement
scaled index
Chapter 06.qxd 1/31/07 
While MIPS always acts on 32-bit words, IA-32 instructions can
operate on 8-, 16-, or 32-bit data. Table 6.12 illustrates these variations.
6.8.3 Status Flags
IA-32, like many CISC architectures, uses status flags (also called condition codes) to make decisions about branches and to keep track of carries and arithmetic overflow. IA-32 uses a 32-bit register, called EFLAGS,
that stores the status flags. Some of the bits of the EFLAGS register are
given in Table 6.13. Other bits are used by the operating system.
The architectural state of an IA-32 processor includes EFLAGS as
well as the eight registers and the EIP.
6.8.4 IA-32 Instructions
IA-32 has a larger set of instructions than MIPS. Table 6.14 describes some
of the general purpose instructions. IA-32 also has instructions for floatingpoint arithmetic and for arithmetic on multiple short data elements packed
into a longer word. D indicates the destination (a register or memory location), and S indicates the source (a register, memory location, or immediate).
Note that some instructions always act on specific registers. For
example, 32  32-bit multiplication always takes one of the sources
from EAX and always puts the 64-bit result in EDX and EAX. LOOP always
344 CHAPTER SIX Architecture
Table 6.12 Instructions acting on 8-, 16-, or 32-bit data
Example Meaning Data Size
add AH, BL AH – AH  BL 8-bit
add AX, 1 AX – AX  0xFFFF 16-bit
add EAX, EDX EAX – EAX  EDX 32-bit
Table 6.13 Selected EFLAGS
Name Meaning
CF (Carry Flag) Carry out generated by last arithmetic operation.
Indicates overflow in unsigned arithmetic.
Also used for propagating the carry between
words in multiple-precision arithmetic.
ZF (Zero Flag) Result of last operation was zero.
SF (Sign Flag) Result of last operation was negative (msb  1).
OF (Overflow Flag) Overflow of two’s complement arithmetic.
Chap
6.8 Real World Perspective: IA-32 Architecture 345
Table 6.14 Selected IA-32 instructions
Instruction Meaning Function
ADD/SUB add/subtract D  D  S / D  D  S
ADDC add with carry D  D  S  CF
INC/DEC increment/decrement D  D  1 / D  D  1
CMP compare Set flags based on D  S
NEG negate D  D
AND/OR/XOR logical AND/OR/XOR D  D op S
NOT logical NOT D  D

IMUL/MUL signed/unsigned multiply EDX:EAX  EAX  D
IDIV/DIV signed/unsigned divide EDX:EAX/D
EAX  Quotient; EDX  Remainder
SAR/SHR arithmetic/logical shift right D  D  S / D  D  S
SAL/SHL left shift D  D  S
ROR/ROL rotate right/left Rotate D by S
RCR/RCL rotate right/left with carry Rotate CF and D by S
BT bit test CF  D[S] (the Sth bit of D)
BTR/BTS bit test and reset/set CF  D[S]; D[S]  0 / 1
TEST set flags based on masked bits Set flags based on D AND S
MOV move D  S
PUSH push onto stack ESP  ESP  4; Mem[ESP]  S
POP pop off stack D  MEM[ESP]; ESP  ESP  4
CLC, STC clear/set carry flag CF  0 / 1
JMP unconditional jump relative jump: EIP  EIP  S
absolute jump: EIP  S
Jcc conditional jump if (flag) EIP  EIP  S
LOOP loop ECX  ECX  1
if ECX  0 EIP  EIP  imm
CALL procedure call ESP  ESP  4;
MEM[ESP]  EIP; EIP  S
RET procedure return EIP  MEM[ESP]; ESP  ESP  4
Chapter 06.qxd 1/31/07 8:21 PM Page 345
346 CHAPTER SIX Architecture
2 It is possible to construct 17-byte instructions if all the optional fields are used.
However, IA-32 places a 15-byte limit on the length of legal instructions.
Table 6.15 Selected branch conditions
Instruction Meaning Function After cmp d, s
JZ/JE jump if ZF  1 jump if D  S
JNZ/JNE jump if ZF  0 jump if D  S
JGE jump if SF  OF jump if D  S
JG jump if SF  OF and ZF  0 jump if D  S
JLE jump if SF  OF or ZF  1 jump if D  S
JL jump if SF  OF jump if D  S
JC/JB jump if CF  1
JNC jump if CF  0
JO jump if OF  1
JNO jump if OF  0
JS jump if SF  1
JNS jump if SF  0
stores the loop counter in ECX. PUSH, POP, CALL, and RET use the stack
pointer, ESP.
Conditional jumps check the flags and branch if the appropriate
condition is met. They come in many flavors. For example, JZ jumps if
the zero flag (ZF) is 1. JNZ jumps if the zero flag is 0. The jumps usually
follow an instruction, such as the compare instruction (CMP), that sets
the flags. Table 6.15 lists some of the conditional jumps and how they
depend on the flags set by a prior compare operation.
6.8.5 IA-32 Instruction Encoding
The IA-32 instruction encodings are truly messy, a legacy of decades of
piecemeal changes. Unlike MIPS, whose instructions are uniformly 32
bits, IA-32 instructions vary from 1 to 15 bytes, as shown in Figure
6.37.2 The opcode may be 1, 2, or 3 bytes. It is followed by four
optional fields: ModR/M, SIB, Displacement, and Immediate. ModR/M
specifies an addressing mode. SIB specifies the scale, index, and base
Chapter 06.qx
6.8 Real World Perspective: IA-32 Architecture 347
registers in certain addressing modes. Displacement indicates a 1-, 2-,
or 4-byte displacement in certain addressing modes. And Immediate is a
1-, 2-, or 4-byte constant for instructions using an immediate as the
source operand. Moreover, an instruction can be preceded by up to four
optional byte-long prefixes that modify its behavior.
The ModR/M byte uses the 2-bit Mod and 3-bit R/M field to specify the
addressing mode for one of the operands. The operand can come from
one of the eight registers, or from one of 24 memory addressing modes.
Due to artifacts in the encodings, the ESP and EBP registers are not available for use as the base or index register in certain addressing modes.
The Reg field specifies the register used as the other operand. For certain
instructions that do not require a second operand, the Reg field is used
to specify three more bits of the opcode.
In addressing modes using a scaled index register, the SIB byte specifies the index register and the scale (1, 2, 4, or 8). If both a base and
index are used, the SIB byte also specifies the base register.
MIPS fully specifies the instruction in the opcode and funct fields
of the instruction. IA-32 uses a variable number of bits to specify different instructions. It uses fewer bits to specify more common instructions, decreasing the average length of the instructions. Some
instructions even have multiple opcodes. For example, add AL, imm8
performs an 8-bit add of an immediate to AL. It is represented with the
1-byte opcode, 0x04, followed by a 1-byte immediate. The A register
(AL, AX, or EAX) is called the accumulator. On the other hand, add D,
imm8 performs an 8-bit add of an immediate to an arbitrary destination, D (memory or a register). It is represented with the 1-byte opcode,
0x80, followed by one or more bytes specifying D, followed by a 1-byte
immediate. Many instructions have shortened encodings when the destination is the accumulator.
In the original 8086, the opcode specified whether the instruction
acted on 8- or 16-bit operands. When the 80386 introduced 32-bit
operands, no new opcodes were available to specify the 32-bit form.
Prefixes ModR/M SIB Displacement Immediate
Up to 4 optional
prefixes
of 1 byte each
1-, 2-, or 3-byte
opcode
1 byte
(for certain
addressing
modes)
1 byte
(for certain
addressing
modes)
1, 2, or 4 bytes
for addressing
modes with
displacement
1, 2, or 4 bytes
for addressing
modes with
immediate
Mod R/M Scale Index Base Reg/
Opcode
Opcode
2 bits 3 bits 3 bits 2 bits 3 bits 3 bits
Figure 6.37 IA-32 instruction encodings

Instead, the same opcode was used for both 16- and 32-bit forms.
An additional bit in the code segment descriptor used by the OS specifies
which form the processor should choose. The bit is set to 0 for backward compatibility with 8086 programs, defaulting the opcode to 16-bit
operands. It is set to 1 for programs to default to 32-bit operands.
Moreover, the programmer can specify prefixes to change the form for a
particular instruction. If the prefix 0x66 appears before the opcode, the
alternative size operand is used (16 bits in 32-bit mode, or 32 bits in
16-bit mode).
6.8.6 Other IA-32 Peculiarities
The 80286 introduced segmentation to divide memory into segments
of up to 64 KB in length. When the OS enables segmentation,
addresses are computed relative to the beginning of the segment. The
processor checks for addresses that go beyond the end of the segment
and indicates an error, thus preventing programs from accessing memory outside their own segment. Segmentation proved to be a hassle for
programmers and is not used in modern versions of the Windows operating system.
IA-32 contains string instructions that act on entire strings of bytes
or words. The operations include moving, comparing, or scanning for a
specific value. In modern processors, these instructions are usually
slower than performing the equivalent operation with a series of simpler
instructions, so they are best avoided.
As mentioned earlier, the 0x66 prefix is used to choose between
16- and 32-bit operand sizes. Other prefixes include ones used to lock
the bus (to control access to shared variables in a multiprocessor system), to predict whether a branch will be taken or not, and to repeat the
instruction during a string move.
The bane of any architecture is to run out of memory capacity. With
32-bit addresses, IA-32 can access 4 GB of memory. This was far more
than the largest computers had in 1985, but by the early 2000s it had
become limiting. In 2003, AMD extended the address space and register
sizes to 64 bits, calling the enhanced architecture AMD64. AMD64 has
a compatibility mode that allows it to run 32-bit programs unmodified
while the OS takes advantage of the bigger address space. In 2004, Intel
gave in and adopted the 64-bit extensions, renaming them Extended
Memory 64 Technology (EM64T). With 64-bit addresses, computers can
access 16 exabytes (16 billion GB) of memory.
For those curious about more details of the IA-32 architecture, the
IA-32 Intel Architecture Software Developer’s Manual, is freely available
on Intel’s Web site.
348 CHAPTER SIX Architecture
Intel and Hewlett-Packard
jointly developed a new 64-bit
architecture called IA-64 in
the mid 1990’s. It was
designed from a clean slate,
bypassing the convoluted
history of IA-32, taking
advantage of 20 years of new
research in computer architecture, and providing a 64-bit
address space. However,
IA-64 has yet to become a
market success. Most computers needing the large address
space now use the 64-bit
extensions of IA-32.

6.8.7 The Big Picture
This section has given a taste of some of the differences between the MIPS
RISC architecture and the IA-32 CISC architecture. IA-32 tends to have
shorter programs, because a complex instruction is equivalent to a series
of simple MIPS instructions and because the instructions are encoded to
minimize memory use. However, the IA-32 architecture is a hodgepodge
of features accumulated over the years, some of which are no longer useful
but must be kept for compatibility with old programs. It has too few registers, and the instructions are difficult to decode. Merely explaining the
instruction set is difficult. Despite all these failings, IA-32 is firmly
entrenched as the dominant computer architecture for PCs, because the
value of software compatibility is so great and because the huge market
justifies the effort required to build fast IA-32 microprocessors.
6.9 SUMMARY
To command a computer, you must speak its language. A computer
architecture defines how to command a processor. Many different computer architectures are in widespread commercial use today, but once
you understand one, learning others is much easier. The key questions to
ask when approaching a new architecture are
 What is the data word length?
 What are the registers?
 How is memory organized?
 What are the instructions?
MIPS is a 32-bit architecture because it operates on 32-bit data. The
MIPS architecture has 32 general-purpose registers. In principle, almost
any register can be used for any purpose. However, by convention, certain
registers are reserved for certain purposes, for ease of programming and so
that procedures written by different programmers can communicate easily.
For example, register 0, $0, always holds the constant 0, $ra holds
the return address after a jal instruction, and $a0–$a3 and $v0 – $v1
hold the arguments and return value of a procedure. MIPS has a byteaddressable memory system with 32-bit addresses. The memory map was
described in Section 6.6.1. Instructions are 32 bits long and must be
word aligned. This chapter discussed the most commonly used MIPS
instructions.
The power of defining a computer architecture is that a program
written for any given architecture can run on many different implementations of that architecture. For example, programs written for the Intel
6.9 Summary 349
Chap
Pentium processor in 1993 will generally still run (and run much faster)
on the Intel Core 2 Duo or AMD Athlon processors in 2006.
In the first part of this book, we learned about the circuit and
logic levels of abstraction. In this chapter, we jumped up to the architecture level. In the next chapter, we study microarchitecture, the
arrangement of digital building blocks that implement a processor
architecture. Microarchitecture is the link between hardware and software engineering. And, we believe it is one of the most exciting topics
in all of engineering: you will learn to build your own microprocessor!
350 CHAPTER SIX Architecture

Exercises
Exercise 6.1 Give three examples from the MIPS architecture of each of the
architecture design principles: (1) simplicity favors regularity; (2) make the common case fast; (3) smaller is faster; and (4) good design demands good compromises. Explain how each of your examples exhibits the design principle.
Exercise 6.2 The MIPS architecture has a register set that consists of 32 32-bit
registers. Is it possible to design a computer architecture without a register
set? If so, briefly describe the architecture, including the instruction set.
What are advantages and disadvantages of this architecture over the MIPS
architecture?
Exercise 6.3 Consider memory storage of a 32-bit word stored at memory word
42 in a byte addressable memory.
(a) What is the byte address of memory word 42?
(b) What are the byte addresses that memory word 42 spans?
(c) Draw the number 0xFF223344 stored at word 42 in both big-endian and
little-endian machines. Your drawing should be similar to Figure 6.4.
Clearly label the byte address corresponding to each data byte value.
Exercise 6.4 Explain how the following program can be used to determine
whether a computer is big-endian or little-endian:
li $t0, 0xABCD9876
sw $t0, 100($0)
lb $s5, 101($0)
Exercise 6.5 Write the following strings using ASCII encoding. Write your final
answers in hexadecimal.
(a) SOS
(b) Cool!
(c) (your own name)
Exercise 6.6 Show how the strings in Exercise 6.5 are stored in a byte-addressable
memory on (a) a big-endian machine and (b) a little-endian machine starting at
memory address 0x1000100C. Use a memory diagram similar to Figure 6.4.
Clearly indicate the memory address of each byte on each machine.
Exercises 351

Exercise 6.7 Convert the following MIPS assembly code into machine language.
Write the instructions in hexadecimal.
add $t0, $s0, $s1
lw $t0, 0x20($t7)
addi $s0, $0, 10
Exercise 6.8 Repeat Exercise 6.7 for the following MIPS assembly code:
addi $s0, $0, 73
sw $t1, 7($t2)
sub $t1, $s7, $s2
Exercise 6.9 Consider I-type instructions.
(a) Which instructions from Exercise 6.8 are I-type instructions?
(b) Sign-extend the 16-bit immediate of each instruction from part (a) so that it
becomes a 32-bit number.
Exercise 6.10 Convert the following program from machine language into
MIPS assembly language. The numbers on the left are the instruction address
in memory, and the numbers on the right give the instruction at that address.
Then reverse engineer a high-level program that would compile into this
assembly language routine and write it. Explain in words what the program
does. $a0 is the input, and it initially contains a positive number, n. $v0 is
the output.
0x00400000 0x20080000
0x00400004 0x20090001
0x00400008 0x0089502a
0x0040000c 0x15400003
0x00400010 0x01094020
0x00400014 0x21290002
0x00400018 0x08100002
0x0040001c 0x01001020
Exercise 6.11 The nori instruction is not part of the MIPS instruction set,
because the same functionality can be implemented using existing instructions.
Write a short assembly code snippet that has the following functionality:
$t0  $t1 NOR 0xF234. Use as few instructions as possible.
352 CHAPTER SIX Architecture
C
Exercise 6.12 Implement the following high-level code segments using the slt
instruction. Assume the integer variables g and h are in registers $s0 and $s1,
respectively.
(a) if (g  h)
g  g  h;
else
g  g  h;
(b) if (g  h)
g  g  1;
else
h  h  1;
(c) if (g  h)
g  0;
else
h  0;
Exercise 6.13 Write a procedure in a high-level language for int find42(int
array[], int size). size specifies the number of elements in the array. array
specifies the base address of the array. The procedure should return the index
number of the first array entry that holds the value 42. If no array entry is 42, it
should return the value 1.
Exercise 6.14 The high-level procedure strcpy copies the character string x to
the character string y.
// high-level code
void strcpy(char x[], char y[]) {
int i  0;
while (x[i] ! 0) {
y[i]  x[i];
i  i  1;
}
}
(a) Implement the strcpy procedure in MIPS assembly code. Use $s0 for i.
(b) Draw a picture of the stack before, during, and after the strcpy procedure
call. Assume $sp  0x7FFFFF00 just before strcpy is called.
Exercise 6.15 Convert the high-level procedure from Exercise 6.13 into MIPS
assembly code.
Exercises 353
This simple string copy program has a serious flaw: it has
no way of knowing that y has
enough space to receive x. If a
malicious programmer were
able to execute strcpy with
a long string x, the programmer might be able to write
bytes all over memory, possibly even modifying code
stored in subsequent memory
locations. With some cleverness, the modified code might
take over the machine. This
is called a buffer overflow
attack; it is employed by
several nasty programs,
including the infamous Blaster
worm, which caused an estimated $525 million in damages in 2003.
Chapter 06.qxd 1
Exercise 6.16 Each number in the Fibonacci series is the sum of the previous two
numbers. Table 6.16 lists the first few numbers in the series, fib(n).
(a) What is fib(n) for n  0 and n  1?
(b) Write a procedure called fib in a high-level language that returns the
Fibonacci number for any nonnegative value of n. Hint: You probably will
want to use a loop. Clearly comment your code.
(c) Convert the high-level procedure of part (b) into MIPS assembly code. Add
comments after every line of code that explain clearly what it does. Use the
SPIM simulator to test your code on fib(9).
Exercise 6.17 Consider the MIPS assembly code below. proc1, proc2, and
proc3 are non-leaf procedures. proc4 is a leaf procedure. The code is not shown
for each procedure, but the comments indicate which registers are used within
each procedure.
0x00401000 proc1: . . . # proc1 uses $s0 and $s1
0x00401020 jal proc2
...
0x00401100 proc2: . . . # proc2 uses $s2 – $s7
0x0040117C jal proc3
...
0x00401400 proc3: . . . # proc3 uses $s1 – $s3
0x00401704 jal proc4
...
0x00403008 proc4: . . . # proc4 uses no preserved
# registers
0x00403118 jr $ra
(a) How many words are the stack frames of each procedure?
(b) Sketch the stack after proc4 is called. Clearly indicate which registers are
stored where on the stack. Give values where possible.
354 CHAPTER SIX Architecture
Table 6.16 Fibonacci series
n 1 2 3 4 5 6 7 8 9 10 11 . . .
fib(n) 1 1 2 3 5 8 13 21 34 55 89 . . .
Ch
Exercise 6.18 Ben Bitdiddle is trying to compute the function f(a, b)  2a  3b
for nonnegative b. He goes overboard in the use of procedure calls and recursion
and produces the following high-level code for procedures f and f2.
// high-level code for procedures f and f2
int f(int a, int b) {
int j;
j  a;
return j  a  f2(b);
}
int f2(int x)
{
int k;
k  3;
if (x  0) return 0;
else return k  f2(x1);
}
Ben then translates the two procedures into assembly language as follows.
He also writes a procedure, test, that calls the procedure f(5, 3).
# MIPS assembly code
# f: $a0  a, $a1  b, $s0  j f2: $a0  x, $s0  k
0x00400000 test: addi $a0, $0, 5 # $a0  5 (a  5)
0x00400004 addi $a1, $0, 3 # $a1  3 (b  3)
0x00400008 jal f # call f(5,3)
0x0040000c loop: j loop # and loop forever
0x00400010 f: addi $sp, $sp, 16 # make room on the stack
# for $s0, $a0, $a1, and $ra
0x00400014 sw $a1, 12($sp) # save $a1 (b)
0x00400018 sw $a0, 8($sp) # save $a0 (a)
0x0040001c sw $ra, 4($sp) # save $ra
0x00400020 sw $s0, 0($sp) # save $s0
0x00400024 add $s0, $a0, $0 # $s0  $a0 (j  a)
0x00400028 add $a0, $a1, $0 # place b as argument for f2
0x0040002c jal f2 # call f2(b)
0x00400030 lw $a0, 8($sp) # restore $a0 (a) after call
0x00400034 lw $a1, 12($sp) # restore $a1 (b) after call
0x00400038 add $v0, $v0, $s0 # $v0  f2(b)  j
0x0040003c add $v0, $v0, $a0 # $v0  (f2(b)  j)  a
0x00400040 lw $s0, 0($sp) # restore $s0
0x00400044 lw $ra, 4($sp) # restore $ra
0x00400048 addi $sp, $sp, 16 # restore $sp (stack pointer)
0x0040004c jr $ra # return to point of call
0x00400050 f2: addi $sp, $sp, 12 # make room on the stack for
# $s0, $a0, and $ra
0x00400054 sw $a0, 8($sp) # save $a0 (x)
0x00400058 sw $ra, 4($sp) # save return address
0x0040005c sw $s0, 0($sp) # save $s0
0x00400060 addi $s0, $0, 3 # k  3
Exercises 355
Chapter 06.qxd 1/31/07 8:2
0x00400064 bne $a0, $0, else # x  0?
0x00400068 addi $v0, $0, 0 # yes: return value should be 0
0x0040006c j done # and clean up
0x00400070 else: addi $a0, $a0, 1 # no: $a0  $a0  1 (x  x  1)
0x00400074 jal f2 # call f2(x  1)
0x00400078 lw $a0, 8($sp) # restore $a0 (x)
0x0040007c add $v0, $v0, $s0 # $v0  f2(x  1)  k
0x00400080 done: lw $s0, 0($sp) # restore $s0
0x00400084 lw $ra, 4($sp) # restore $ra
0x00400088 addi $sp, $sp, 12 # restore $sp
0x0040008c jr $ra # return to point of call
You will probably find it useful to make drawings of the stack similar to the
one in Figure 6.26 to help you answer the following questions.
(a) If the code runs starting at test, what value is in $v0 when the program
gets to loop? Does his program correctly compute 2a  3b?
(b) Suppose Ben deletes the instructions at addresses 0x0040001C and
0x00400040 that save and restore $ra. Will the program (1) enter an infinite loop but not crash; (2) crash (cause the stack to grow beyond the
dynamic data segment or the PC to jump to a location outside the program);
(3) produce an incorrect value in $v0 when the program returns to loop (if
so, what value?), or (4) run correctly despite the deleted lines?
(c) Repeat part (b) when the instructions at the following instruction addresses
are deleted:
(i) 0x00400018 and 0x00400030 (instructions that save and restore $a0)
(ii) 0x00400014 and 0x00400034 (instructions that save and restore $a1)
(iii) 0x00400020 and 0x00400040 (instructions that save and restore $s0)
(iv) 0x00400050 and 0x00400088 (instructions that save and restore $sp)
(v) 0x0040005C and 0x00400080 (instructions that save and restore $s0)
(vi) 0x00400058 and 0x00400084 (instructions that save and restore $ra)
(vii) 0x00400054 and 0x00400078 (instructions that save and restore $a0)
Exercise 6.19 Convert the following beq, j, and jal assembly instructions into
machine code. Instruction addresses are given to the left of each instruction.
(a)
0x00401000 beq $t0, $s1, Loop
0x00401004 . . .
0x00401008 . . .
0x0040100C Loop: . . .
356 CHAPTER SIX Architecture
Chapte
(b)
0x00401000 beq $t7, $s4, done
... ...
0x00402040 done: . . .
(c)
0x0040310C back: . . .
... ...
0x00405000 beq $t9, $s7, back
(d)
0x00403000 jal proc
... ...
0x0041147C proc: . . .
(e)
0x00403004 back: . . .
... ...
0x0040400C j back
Exercise 6.20 Consider the following MIPS assembly language snippet. The
numbers to the left of each instruction indicate the instruction address.
0x00400028 add $a0, $a1, $0
0x0040002c jal f2
0x00400030 f1: jr $ra
0x00400034 f2: sw $s0, 0($s2)
0x00400038 bne $a0, $0, else
0x0040003c j f1
0x00400040 else: addi $a0, $a0, 1
0x00400044 j f2
(a) Translate the instruction sequence into machine code. Write the machine
code instructions in hexadecimal.
(b) List the addressing mode used at each line of code.
Exercise 6.21 Consider the following C code snippet.
// C code
void set_array(int num) {
int i;
int array[10];
for (i  0; i  10; i  i  1) {
array[i]  compare(num, i);
}
}
int compare(int a, int b) {
if (sub(a, b) > 0)
return 1;
else
Exercises 357
Chapt
return 0;
}
int sub (int a, int b) {
return a  b;
}
(a) Implement the C code snippet in MIPS assembly language. Use $s0 to hold
the variable i. Be sure to handle the stack pointer appropriately. The array
is stored on the stack of the set_array procedure (see Section 6.4.6).
(b) Assume set_array is the first procedure called. Draw the status of the
stack before calling set_array and during each procedure call. Indicate the
names of registers and variables stored on the stack and mark the location
of $sp.
(c) How would your code function if you failed to store $ra on the stack?
Exercise 6.22 Consider the following high-level procedure.
// high-level code
int f(int n, int k) {
int b;
b  k  2;
if (n  0) b  10;
else b  b  (n * n)  f(n  1, k  1);
return b * k;
}
(a) Translate the high-level procedure f into MIPS assembly language. Pay particular attention to properly saving and restoring registers across procedure calls
and using the MIPS preserved register conventions. Clearly comment your
code. You can use the MIPS mult, mfhi, and mflo instructions. The procedure starts at instruction address 0x00400100. Keep local variable b in $s0.
(b) Step through your program from part (a) by hand for the case of f(2, 4).
Draw a picture of the stack similar to the one in Figure 6.26(c). Write the
register name and data value stored at each location in the stack and keep
track of the stack pointer value ($sp). You might also find it useful to keep
track of the values in $a0, $a1, $v0, and $s0 throughout execution. Assume
that when f is called, $s0  0xABCD and $ra  0x400004. What is the
final value of $v0?
Exercise 6.23 What is the range of instruction addresses to which conditional
branches, such as beq and bne, can branch in MIPS? Give your answer in number of instructions relative to the conditional branch instruction.
358 CHAPTER SIX Architecture
Chapter 06.
Exercise 6.24 The following questions examine the limitations of the jump instruction, j. Give your answer in number of instructions relative to the jump instruction.
(a) In the worst case, how far can the jump instruction (j) jump forward
(i.e., to higher addresses)? (The worst case is when the jump instruction
cannot jump far.) Explain using words and examples, as needed.
(b) In the best case, how far can the jump instruction (j) jump forward? (The
best case is when the jump instruction can jump the farthest.) Explain.
(c) In the worst case, how far can the jump instruction (j) jump backward (to
lower addresses)? Explain.
(d) In the best case, how far can the jump instruction (j) jump backward? Explain.
Exercise 6.25 Explain why it is advantageous to have a large address field,
addr, in the machine format for the jump instructions, j and jal.
Exercise 6.26 Write assembly code that jumps to the instruction 64
Minstructions from the first instruction. Recall that 1 Minstruction  220
instructions  1,048,576 instructions. Assume that your code begins at address
0x00400000. Use a minimum number of instructions.
Exercise 6.27 Write a procedure in high-level code that takes a ten-entry array
of 32-bit integers stored in little-endian format and converts it to big-endian format. After writing the high-level code, convert it to MIPS assembly code.
Comment all your code and use a minimum number of instructions.
Exercise 6.28 Consider two strings: string1 and string2.
(a) Write high-level code for a procedure called concat that concatenates (joins
together) the two strings: void concat(char[] string1, char[] string2,
char[] stringconcat). The procedure does not return a value. It concatenates string1 and string2 and places the resulting string in stringconcat.
You may assume that the character array stringconcat is large enough to
accommodate the concatenated string.
(b) Convert the procedure from part (a) into MIPS assembly language.
Exercise 6.29 Write a MIPS assembly program that adds two positive singleprecision floating point numbers held in $s0 and $s1. Do not use any of the
MIPS floating-point instructions. You need not worry about any of the encodings that are reserved for special purposes (e.g., 0, NANs, INF) or numbers that
overflow or underflow. Use the SPIM simulator to test your code. You will need
to manually set the values of $s0 and $s1 to test your code. Demonstrate that
your code functions reliably.
Exercises 359
Ch
Exercise 6.30 Show how the following MIPS program would be loaded into
memory and executed.
# MIPS assembly code
main:
lw $a0, x
lw $a1, y
jal diff
jr $ra
diff:
sub $v0, $a0, $a1
jr $ra
(a) First show the instruction address next to each assembly instruction.
(b) Draw the symbol table showing the labels and their addresses.
(c) Convert all instructions into machine code.
(d) How big (how many bytes) are the data and text segments?
(e) Sketch a memory map showing where data and instructions are stored.
Exercise 6.31 Show the MIPS instructions that implement the following
pseudoinstructions. You may use the assembler register, $at, but you may not
corrupt (overwrite) any other registers.
(a) beq $t1, imm31:0, L
(b) ble $t3, $t5, L
(c) bgt $t3, $t5, L
(d) bge $t3, $t5, L
(e) addi $t0, $2, imm31:0
(f) lw $t5, imm31:0($s0)
(g) rol $t0, $t1, 5 (rotate $t1 left by 5 and put the result in $t0)
(h) ror $s4, $t6, 31 (rotate $t6 right by 31 and put the result in $s4)
360 CHAPTER SIX Architecture

Interview Questions
The following exercises present questions that have been asked at
interviews for digital design jobs (but are usually open to any assembly
language).
Question 6.1 Write MIPS assembly code for swapping the contents of two
registers, $t0 and $t1. You may not use any other registers.
Question 6.2 Suppose you are given an array of both positive and negative
integers. Write MIPS assembly code that finds the subset of the array with the
largest sum. Assume that the array’s base address and the number of array
elements are in $a0 and $a1, respectively. Your code should place the resulting
subset of the array starting at base address $a2. Write code that runs as fast as
possible.
Question 6.3 You are given an array that holds a C string. The string forms a
sentence. Design an algorithm for reversing the words in the sentence and storing
the new sentence back in the array. Implement your algorithm using MIPS
assembly code.
Question 6.4 Design an algorithm for counting the number of 1’s in a 32-bit
number. Implement your algorithm using MIPS assembly code.
Question 6.5 Write MIPS assembly code to reverse the bits in a register. Use as
few instructions as possible. Assume the register of interest is $t3.
Question 6.6 Write MIPS assembly code to test whether overflow occurs when
$t2 and $t3 are added. Use a minimum number of instructions.
Question 6.7 Design an algorithm for testing whether a given string is a palindrome. (Recall, that a palindrome is a word that is the same forward and backward. For example, the words “wow” and “racecar” are palindromes.)
Implement your algorithm using MIPS assembly code.
Interview Questions 361


7
7.1 Introduction
7.2 Performance Analysis
7.3 Single-Cycle Processor
7.4 Multicycle Processor
7.5 Pipelined Processor
7.6 HDL Representation*
7.7 Exceptions*
7.8 Advanced
Microarchitecture*
7.9 Real-World Perspective:
IA-32 Microarchitecture*
7.10 Summary
Exercises
Interview Questions
Microarchitecture
7.1 INTRODUCTION
In this chapter, you will learn how to piece together a MIPS microprocessor. Indeed, you will puzzle out three different versions, each with
different trade-offs between performance, cost, and complexity.
To the uninitiated, building a microprocessor may seem like black
magic. But it is actually relatively straightforward, and by this point you
have learned everything you need to know. Specifically, you have learned
to design combinational and sequential logic given functional and timing
specifications. You are familiar with circuits for arithmetic and memory.
And you have learned about the MIPS architecture, which specifies the
programmer’s view of the MIPS processor in terms of registers, instructions, and memory.
This chapter covers microarchitecture, which is the connection
between logic and architecture. Microarchitecture is the specific arrangement of registers, ALUs, finite state machines (FSMs), memories, and
other logic building blocks needed to implement an architecture. A particular architecture, such as MIPS, may have many different microarchitectures, each with different trade-offs of performance, cost, and
complexity. They all run the same programs, but their internal designs
vary widely. We will design three different microarchitectures in this
chapter to illustrate the trade-offs.
This chapter draws heavily on David Patterson and John Hennessy’s
classic MIPS designs in their text Computer Organization and Design.
They have generously shared their elegant designs, which have the virtue
of illustrating a real commercial architecture while being relatively simple and easy to understand.
7.1.1 Architectural State and Instruction Set
Recall that a computer architecture is defined by its instruction set and
architectural state. The architectural state for the MIPS processor consists
363

of the program counter and the 32 registers. Any MIPS microarchitecture
must contain all of this state. Based on the current architectural state, the
processor executes a particular instruction with a particular set of data to
produce a new architectural state. Some microarchitectures contain
additional nonarchitectural state to either simplify the logic or improve
performance; we will point this out as it arises.
To keep the microarchitectures easy to understand, we consider only
a subset of the MIPS instruction set. Specifically, we handle the following
instructions:
 R-type arithmetic/logic instructions: add, sub, and, or, slt
 Memory instructions: lw, sw
 Branches: beq
After building the microarchitectures with these instructions, we extend
them to handle addi and j. These particular instructions were chosen
because they are sufficient to write many interesting programs. Once you
understand how to implement these instructions, you can expand the
hardware to handle others.
7.1.2 Design Process
We will divide our microarchitectures into two interacting parts: the
datapath and the control. The datapath operates on words of data. It
contains structures such as memories, registers, ALUs, and multiplexers.
MIPS is a 32-bit architecture, so we will use a 32-bit datapath. The control unit receives the current instruction from the datapath and tells the
datapath how to execute that instruction. Specifically, the control unit
produces multiplexer select, register enable, and memory write signals to
control the operation of the datapath.
A good way to design a complex system is to start with hardware
containing the state elements. These elements include the memories and
the architectural state (the program counter and registers). Then, add
blocks of combinational logic between the state elements to compute the
new state based on the current state. The instruction is read from part of
memory; load and store instructions then read or write data from
another part of memory. Hence, it is often convenient to partition the
overall memory into two smaller memories, one containing instructions
and the other containing data. Figure 7.1 shows a block diagram with
the four state elements: the program counter, register file, and instruction
and data memories.
In Figure 7.1, heavy lines are used to indicate 32-bit data busses.
Medium lines are used to indicate narrower busses, such as the 5-bit
address busses on the register file. Narrow blue lines are used to indicate
364 CHAPTER SEVEN Microarchitecture
David Patterson was the first
in his family to graduate from
college (UCLA, 1969). He has
been a professor of computer
science at UC Berkeley since
1977, where he coinvented
RISC, the Reduced Instruction
Set Computer. In 1984, he
developed the SPARC architecture used by Sun Microsystems. He is also the father
of RAID (Redundant Array of
Inexpensive Disks) and NOW
(Network of Workstations).
John Hennessy is president
of Stanford University and
has been a professor of electrical engineering and computer science there since
1977. He coinvented RISC.
He developed the MIPS architecture at Stanford in 1984
and cofounded MIPS Computer Systems. As of 2004, more
than 300 million MIPS microprocessors have been sold.
In their copious free time,
these two modern paragons
write textbooks for recreation
and relaxation.
Cha
control signals, such as the register file write enable. We will use this
convention throughout the chapter to avoid cluttering diagrams with bus
widths. Also, state elements usually have a reset input to put them into a
known state at start-up. Again, to save clutter, this reset is not shown.
The program counter is an ordinary 32-bit register. Its output, PC,
points to the current instruction. Its input, PC, indicates the address of
the next instruction.
The instruction memory has a single read port.1 It takes a 32-bit
instruction address input, A, and reads the 32-bit data (i.e., instruction)
from that address onto the read data output, RD.
The 32-element  32-bit register file has two read ports and one write
port. The read ports take 5-bit address inputs, A1 and A2, each specifying
one of 25  32 registers as source operands. They read the 32-bit register
values onto read data outputs RD1 and RD2, respectively. The write port
takes a 5-bit address input, A3; a 32-bit write data input, WD; a write
enable input, WE3; and a clock. If the write enable is 1, the register file
writes the data into the specified register on the rising edge of the clock.
The data memory has a single read/write port. If the write enable,
WE, is 1, it writes data WD into address A on the rising edge of the
clock. If the write enable is 0, it reads address A onto RD.
The instruction memory, register file, and data memory are all read
combinationally. In other words, if the address changes, the new data
appears at RD after some propagation delay; no clock is involved. They
are written only on the rising edge of the clock. In this fashion, the state
of the system is changed only at the clock edge. The address, data, and
write enable must setup sometime before the clock edge and must
remain stable until a hold time after the clock edge.
Because the state elements change their state only on the rising edge of
the clock, they are synchronous sequential circuits. The microprocessor is
7.1 Introduction 365
Resetting the PC
At the very least, the program
counter must have a reset
signal to initialize its value
when the processor turns on.
MIPS processors initialize the
PC to 0xBFC00000 on reset and
begin executing code to start
up the operating system (OS).
The OS then loads an application program at 0x00400000
and begins executing it.
For simplicity in this chapter,
we will reset the PC to
0x00000000 and place our
programs there instead.
1 This is an oversimplification used to treat the instruction memory as a ROM; in most
real processors, the instruction memory must be writable so that the OS can load a new
program into memory. The multicycle microarchitecture described in Section 7.4 is more
realistic in that it uses a combined memory for instructions and data that can be both read
and written.
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1
WE3
A2
CLK
Register
File
A RD
Data
Memory
WD
PC' PC WE
CLK
32 32
32 32
32
32
32 32
32
32
5
5
5
Figure 7.1 State elements of MIPS processor
Ch
built of clocked state elements and combinational logic, so it too is a
synchronous sequential circuit. Indeed, the processor can be viewed as a
giant finite state machine, or as a collection of simpler interacting state
machines.
7.1.3 MIPS Microarchitectures
In this chapter, we develop three microarchitectures for the MIPS processor architecture: single-cycle, multicycle, and pipelined. They differ in
the way that the state elements are connected together and in the
amount of nonarchitectural state.
The single-cycle microarchitecture executes an entire instruction
in one cycle. It is easy to explain and has a simple control unit.
Because it completes the operation in one cycle, it does not require
any nonarchitectural state. However, the cycle time is limited by the
slowest instruction.
The multicycle microarchitecture executes instructions in a series of
shorter cycles. Simpler instructions execute in fewer cycles than complicated ones. Moreover, the multicycle microarchitecture reduces the hardware cost by reusing expensive hardware blocks such as adders and
memories. For example, the adder may be used on several different
cycles for several purposes while carrying out a single instruction. The
multicycle microprocessor accomplishes this by adding several nonarchitectural registers to hold intermediate results. The multicycle processor
executes only one instruction at a time, but each instruction takes multiple clock cycles.
The pipelined microarchitecture applies pipelining to the single-cycle
microarchitecture. It therefore can execute several instructions simultaneously, improving the throughput significantly. Pipelining must add
logic to handle dependencies between simultaneously executing instructions. It also requires nonarchitectural pipeline registers. The added logic
and registers are worthwhile; all commercial high-performance processors use pipelining today.
We explore the details and trade-offs of these three microarchitectures in the subsequent sections. At the end of the chapter, we briefly
mention additional techniques that are used to get even more speed in
modern high-performance microprocessors.
7.2 PERFORMANCE ANALYSIS
As we mentioned, a particular processor architecture can have many
microarchitectures with different cost and performance trade-offs. The
cost depends on the amount of hardware required and the implementation technology. Each year, CMOS processes can pack more transistors
on a chip for the same amount of money, and processors take advantage
366 CHAPTER SEVEN Microarchitecture

of these additional transistors to deliver more performance. Precise cost
calculations require detailed knowledge of the implementation technology, but in general, more gates and more memory mean more dollars.
This section lays the foundation for analyzing performance.
There are many ways to measure the performance of a computer
system, and marketing departments are infamous for choosing the
method that makes their computer look fastest, regardless of whether
the measurement has any correlation to real world performance. For
example, Intel and Advanced Micro Devices (AMD) both sell compatible microprocessors conforming to the IA-32 architecture. Intel
Pentium III and Pentium 4 microprocessors were largely advertised
according to clock frequency in the late 1990s and early 2000s,
because Intel offered higher clock frequencies than its competitors.
However, Intel’s main competitor, AMD, sold Athlon microprocessors
that executed programs faster than Intel’s chips at the same clock
frequency. What is a consumer to do?
The only gimmick-free way to measure performance is by measuring
the execution time of a program of interest to you. The computer that executes your program fastest has the highest performance. The next best
choice is to measure the total execution time of a collection of programs
that are similar to those you plan to run; this may be necessary if you
haven’t written your program yet or if somebody else who doesn’t have
your program is making the measurements. Such collections of programs
are called benchmarks, and the execution times of these programs are commonly published to give some indication of how a processor performs.
The execution time of a program, measured in seconds, is given by
Equation 7.1.
(7.1)
The number of instructions in a program depends on the processor architecture. Some architectures have complicated instructions that do more
work per instruction, thus reducing the number of instructions in a program. However, these complicated instructions are often slower to execute
in hardware. The number of instructions also depends enormously on the
cleverness of the programmer. For the purposes of this chapter, we will
assume that we are executing known programs on a MIPS processor, so
the number of instructions for each program is constant, independent of
the microarchitecture.
The number of cycles per instruction, often called CPI, is the number of clock cycles required to execute an average instruction. It is the
reciprocal of the throughput (instructions per cycle, or IPC). Different
microarchitectures have different CPIs. In this chapter, we will assume
Execution Time

# instructions cycles
instructionseconds
cycle

7.2 Performance Analysis 367
Chapte
we have an ideal memory system that does not affect the CPI. In
Chapter 8, we examine how the processor sometimes has to wait for the
memory, which increases the CPI.
The number of seconds per cycle is the clock period, Tc. The clock
period is determined by the critical path through the logic on the processor. Different microarchitectures have different clock periods. Logic and
circuit designs also significantly affect the clock period. For example, a
carry-lookahead adder is faster than a ripple-carry adder. Manufacturing
advances have historically doubled transistor speeds every 4–6 years, so
a microprocessor built today will be much faster than one from last
decade, even if the microarchitecture and logic are unchanged.
The challenge of the microarchitect is to choose the design that
minimizes the execution time while satisfying constraints on cost and/or
power consumption. Because microarchitectural decisions affect both
CPI and Tc and are influenced by logic and circuit designs, determining
the best choice requires careful analysis.
There are many other factors that affect overall computer performance. For example, the hard disk, the memory, the graphics system, and
the network connection may be limiting factors that make processor
performance irrelevant. The fastest microprocessor in the world doesn’t
help surfing the Internet on a dial-up connection. But these other factors
are beyond the scope of this book.
7.3 SINGLE-CYCLE PROCESSOR
We first design a MIPS microarchitecture that executes instructions in a
single cycle. We begin constructing the datapath by connecting the state
elements from Figure 7.1 with combinational logic that can execute the
various instructions. Control signals determine which specific instruction
is carried out by the datapath at any given time. The controller contains
combinational logic that generates the appropriate control signals based
on the current instruction. We conclude by analyzing the performance of
the single-cycle processor.
7.3.1 Single-Cycle Datapath
This section gradually develops the single-cycle datapath, adding one
piece at a time to the state elements from Figure 7.1. The new connections are emphasized in black (or blue, for new control signals), while
the hardware that has already been studied is shown in gray.
The program counter (PC) register contains the address of the
instruction to execute. The first step is to read this instruction from
instruction memory. Figure 7.2 shows that the PC is simply connected to
the address input of the instruction memory. The instruction memory
reads out, or fetches, the 32-bit instruction, labeled Instr.
368 CHAPTER SEVEN Microarchitecture

The processor’s actions depend on the specific instruction that was
fetched. First we will work out the datapath connections for the lw
instruction. Then we will consider how to generalize the datapath to
handle the other instructions.
For a lw instruction, the next step is to read the source register
containing the base address. This register is specified in the rs field of
the instruction, Instr25:21. These bits of the instruction are connected
to the address input of one of the register file read ports, A1, as shown in
Figure 7.3. The register file reads the register value onto RD1.
The lw instruction also requires an offset. The offset is stored in the
immediate field of the instruction, Instr15:0. Because the 16-bit immediate might be either positive or negative, it must be sign-extended to
32 bits, as shown in Figure 7.4. The 32-bit sign-extended value is
called SignImm. Recall from Section 1.4.6 that sign extension simply
copies the sign bit (most significant bit) of a short input into all of the
upper bits of the longer output. Specifically, SignImm15:0  Instr15:0
and SignImm31:16  Instr15.
7.3 Single-Cycle Processor 369
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC PC' Instr
A RD
Data
Memory
WD
WE
CLK
Figure 7.2 Fetch instruction
from memory
Instr
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC' PC
25:21
A RD
Data
Memory
WD
WE
CLK
Figure 7.3 Read source operand
from register file
SignImm
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
A RD
Data
Memory
WD
WE
CLK
Figure 7.4 Sign-extend the
immediate

The processor must add the base address to the offset to find the
address to read from memory. Figure 7.5 introduces an ALU to perform
this addition. The ALU receives two operands, SrcA and SrcB. SrcA
comes from the register file, and SrcB comes from the sign-extended
immediate. The ALU can perform many operations, as was described in
Section 5.2.4. The 3-bit ALUControl signal specifies the operation. The
ALU generates a 32-bit ALUResult and a Zero flag, that indicates
whether ALUResult  0. For a lw instruction, the ALUControl signal
should be set to 010 to add the base address and offset. ALUResult is
sent to the data memory as the address for the load instruction, as
shown in Figure 7.5.
The data is read from the data memory onto the ReadData bus,
then written back to the destination register in the register file at the
end of the cycle, as shown in Figure 7.6. Port 3 of the register file is the
370 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
15:0
SrcB
ALUResult
SrcA Zero
CLK
ALUControl2:0
010ALU
Figure 7.5 Compute memory address
A1
A3
WD3
RD2
RD1 WE3
A2
SignImm
CLK
A RD
Instruction
Memory
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
15:0
20:16 SrcB
ALUResult ReadData
SrcA
RegWrite
Zero
CLK
ALUControl2:0
1 010ALU
Figure 7.6 Write data back to register file

write port. The destination register for the lw instruction is specified in
the rt field, Instr20:16, which is connected to the port 3 address input,
A3, of the register file. The ReadData bus is connected to the port 3
write data input, WD3, of the register file. A control signal called
RegWrite is connected to the port 3 write enable input, WE3, and is
asserted during a lw instruction so that the data value is written into the
register file. The write takes place on the rising edge of the clock at the
end of the cycle.
While the instruction is being executed, the processor must
compute the address of the next instruction, PC. Because instructions
are 32 bits  4 bytes, the next instruction is at PC  4. Figure 7.7 uses
another adder to increment the PC by 4. The new address is written into
the program counter on the next rising edge of the clock. This completes
the datapath for the lw instruction.
Next, let us extend the datapath to also handle the sw instruction.
Like the lw instruction, the sw instruction reads a base address from port
1 of the register and sign-extends an immediate. The ALU adds the base
address to the immediate to find the memory address. All of these functions are already supported by the datapath.
The sw instruction also reads a second register from the register file
and writes it to the data memory. Figure 7.8 shows the new connections for
this function. The register is specified in the rt field, Instr20:16. These bits
of the instruction are connected to the second register file read port, A2.
The register value is read onto the RD2 port. It is connected to the write
data port of the data memory. The write enable port of the data memory,
WE, is controlled by MemWrite. For a sw instruction, MemWrite  1, to
write the data to memory; ALUControl  010, to add the base address
7.3 Single-Cycle Processor 371
CLK
SignImm
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
15:0
SrcB 20:16
ALUResult ReadData
SrcA
PCPlus4
Result
RegWrite
Zero
CLK
ALUControl2:0
1 010ALU
+
Figure 7.7 Determine address of next instruction for PC
C
and offset; and RegWrite  0, because nothing should be written to the
register file. Note that data is still read from the address given to the data
memory, but that this ReadData is ignored because RegWrite  0.
Next, consider extending the datapath to handle the R-type instructions add, sub, and, or, and slt. All of these instructions read two registers from the register file, perform some ALU operation on them, and
write the result back to a third register file. They differ only in the specific ALU operation. Hence, they can all be handled with the same hardware, using different ALUControl signals.
Figure 7.9 shows the enhanced datapath handling R-type instructions. The register file reads two registers. The ALU performs an operation on these two registers. In Figure 7.8, the ALU always received its
SrcB operand from the sign-extended immediate (SignImm). Now, we
add a multiplexer to choose SrcB from either the register file RD2 port
or SignImm.
The multiplexer is controlled by a new signal, ALUSrc. ALUSrc is 0
for R-type instructions to choose SrcB from the register file; it is 1 for lw
and sw to choose SignImm. This principle of enhancing the datapath’s
capabilities by adding a multiplexer to choose inputs from several possibilities is extremely useful. Indeed, we will apply it twice more to complete the handling of R-type instructions.
In Figure 7.8, the register file always got its write data from
the data memory. However, R-type instructions write the ALUResult to
the register file. Therefore, we add another multiplexer to choose
between ReadData and ALUResult. We call its output Result. This multiplexer is controlled by another new signal, MemtoReg. MemtoReg is 0
372 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
A RD
Data
Memory
WD
WE PC' PC Instr 25:21
20:16
15:0
SrcB 20:16
ALUResult ReadData
WriteData
SrcA
PCPlus4
Result
RegWrite MemWrite
Zero
CLK
ALUControl2:0
0 010 1 ALU
+
Figure 7.8 Write data to memory for sw instruction

for R-type instructions to choose Result from the ALUResult; it is 1 for
lw to choose ReadData. We don’t care about the value of MemtoReg for
sw, because sw does not write to the register file.
Similarly, in Figure 7.8, the register to write was specified by the rt
field of the instruction, Instr20:16. However, for R-type instructions, the
register is specified by the rd field, Instr15:11. Thus, we add a third multiplexer to choose WriteReg from the appropriate field of the instruction. The multiplexer is controlled by RegDst. RegDst is 1 for R-type
instructions to choose WriteReg from the rd field, Instr15:11; it is 0 for
lw to choose the rt field, Instr20:16. We don’t care about the value of
RegDst for sw, because sw does not write to the register file.
Finally, let us extend the datapath to handle beq. beq compares
two registers. If they are equal, it takes the branch by adding the branch
offset to the program counter. Recall that the offset is a positive or negative number, stored in the imm field of the instruction, Instr31:26. The offset indicates the number of instructions to branch past. Hence, the
immediate must be sign-extended and multiplied by 4 to get the new
program counter value: PC  PC  4  SignImm  4.
Figure 7.10 shows the datapath modifications. The next PC value
for a taken branch, PCBranch, is computed by shifting SignImm left by
2 bits, then adding it to PCPlus4. The left shift by 2 is an easy way to
multiply by 4, because a shift by a constant amount involves just wires.
The two registers are compared by computing SrcA  SrcB using the
ALU. If ALUResult is 0, as indicated by the Zero flag from the ALU, the
registers are equal. We add a multiplexer to choose PC from either
PCPlus4 or PCBranch. PCBranch is selected if the instruction is
7.3 Single-Cycle Processor 373
SignImm
CLK
A RD
Instruction
Memory
4
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
PC' PC Instr 25:21
20:16
15:0
SrcB
20:16
15:11
ALUResult ReadData
WriteData
SrcA
PCPlus4 WriteReg4:0
Result
RegWrite RegDst ALUSrc MemWrite MemtoReg
Zero
CLK
ALUControl2:0
0 1 1 0 varies 0 ALU
Figure 7.9 Datapath enhancements for R-type instruction
Cha
a branch and the Zero flag is asserted. Hence, Branch is 1 for beq and
0 for other instructions. For beq, ALUControl  110, so the ALU
performs a subtraction. ALUSrc  0 to choose SrcB from the register
file. RegWrite and MemWrite are 0, because a branch does not write to
the register file or memory. We don’t care about the values of RegDst
and MemtoReg, because the register file is not written.
This completes the design of the single-cycle MIPS processor datapath. We have illustrated not only the design itself, but also the design
process in which the state elements are identified and the combinational
logic connecting the state elements is systematically added. In the next
section, we consider how to compute the control signals that direct the
operation of our datapath.
7.3.2 Single-Cycle Control
The control unit computes the control signals based on the opcode and
funct fields of the instruction, Instr31:26 and Instr5:0. Figure 7.11 shows
the entire single-cycle MIPS processor with the control unit attached to
the datapath.
Most of the control information comes from the opcode, but R-type
instructions also use the funct field to determine the ALU operation.
Thus, we will simplify our design by factoring the control unit into two
blocks of combinational logic, as shown in Figure 7.12. The main
decoder computes most of the outputs from the opcode. It also determines a 2-bit ALUOp signal. The ALU decoder uses this ALUOp signal
in conjunction with the funct field to compute ALUControl. The meaning of the ALUOp signal is given in Table 7.1.
374 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch
WriteReg4:0
Result
RegWrite RegDst ALUSrc Branch MemWrite MemtoReg
Zero
PCSrc
CLK
ALUControl2:0
0 0 x 0 110 1 x ALU
Figure 7.10 Datapath enhancements for beq instruction
RegDst
Branch
MemWrite
MemtoReg
ALUSrc Opcode5:0
Control
Unit
Funct5:0 ALUControl2:0
Main
Decoder
ALUOp1:0
ALU
Decoder
RegWrite
Figure 7.12 Control unit
internal structure

7.3 Single-Cycle Processor 375
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch +
+
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
ALU
Figure 7.11 Complete single-cycle MIPS processor
Table 7.1 ALUOp encoding
ALUOp Meaning
00 add
01 subtract
10 look at funct field
11 n/a
Table 7.2 is a truth table for the ALU decoder. Recall that the meanings of the three ALUControl signals were given in Table 5.1. Because
ALUOp is never 11, the truth table can use don’t care’s X1 and 1X
instead of 01 and 10 to simplify the logic. When ALUOp is 00 or 01, the
ALU should add or subtract, respectively. When ALUOp is 10, the
decoder examines the funct field to determine the ALUControl. Note
that, for the R-type instructions we implement, the first two bits of the
funct field are always 10, so we may ignore them to simplify the decoder.
The control signals for each instruction were described as we built
the datapath. Table 7.3 is a truth table for the main decoder that summarizes the control signals as a function of the opcode. All R-type
instructions use the same main decoder values; they differ only in the

ALU decoder output. Recall that, for instructions that do not write to
the register file (e.g., sw and beq), the RegDst and MemtoReg control
signals are don’t cares (X); the address and data to the register write
port do not matter because RegWrite is not asserted. The logic for the
decoder can be designed using your favorite techniques for combinational logic design.
Example 7.1 SINGLE-CYCLE PROCESSOR OPERATION
Determine the values of the control signals and the portions of the datapath that
are used when executing an or instruction.
Solution: Figure 7.13 illustrates the control signals and flow of data during
execution of the or instruction. The PC points to the memory location holding
the instruction, and the instruction memory fetches this instruction.
The main flow of data through the register file and ALU is represented with a
dashed blue line. The register file reads the two source operands specified by
Instr25:21 and Instr20:16. SrcB should come from the second port of the register
376 CHAPTER SEVEN Microarchitecture
Table 7.2 ALU decoder truth table
ALUOp Funct ALUControl
00 X 010 (add)
X1 X 110 (subtract)
1X 100000 (add) 010 (add)
1X 100010 (sub) 110 (subtract)
1X 100100 (and) 000 (and)
1X 100101 (or) 001 (or)
1X 101010 (slt) 111 (set less than)
Table 7.3 Main decoder truth table
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp
R-type 000000 1 1 0 0 0 0 10
lw 100011 1 0 1 0 0 1 00
sw 101011 0 X 1 0 1 X 00
beq 000100 0 X 0 1 0 X 01

file (not SignImm), so ALUSrc must be 0. or is an R-type instruction, so ALUOp
is 10, indicating that ALUControl should be determined from the funct field
to be 001. Result is taken from the ALU, so MemtoReg is 0. The result is written
to the register file, so RegWrite is 1. The instruction does not write memory, so
MemWrite  0.
The selection of the destination register is also shown with a dashed blue
line. The destination register is specified in the rd field, Instr15:11, so RegDst  1.
The updating of the PC is shown with the dashed gray line. The instruction is
not a branch, so Branch  0 and, hence, PCSrc is also 0. The PC gets its next
value from PCPlus4.
Note that data certainly does flow through the nonhighlighted paths, but that
the value of that data is unimportant for this instruction. For example, the
immediate is sign-extended and data is read from memory, but these values do
not influence the next state of the system.
7.3.3 More Instructions
We have considered a limited subset of the full MIPS instruction set.
Adding support for the addi and j instructions illustrates the principle
of how to handle new instructions and also gives us a sufficiently rich
instruction set to write many interesting programs. We will see that
7.3 Single-Cycle Processor 377
SignImm
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
PC
1
Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
001 0
0
1 0
0
1
0
ALU
0 PC'
Figure 7.13 Control signals and data flow while executing or instruction

supporting some instructions simply requires enhancing the main
decoder, whereas supporting others also requires more hardware in the
datapath.
Example 7.2 addi INSTRUCTION
The add immediate instruction, addi, adds the value in a register to the immediate and writes the result to another register. The datapath already is capable of
this task. Determine the necessary changes to the controller to support addi.
Solution: All we need to do is add a new row to the main decoder truth table
showing the control signal values for addi, as given in Table 7.4. The result
should be written to the register file, so RegWrite  1. The destination register is
specified in the rt field of the instruction, so RegDst  0. SrcB comes from the
immediate, so ALUSrc  1. The instruction is not a branch, nor does it write
memory, so Branch  MemWrite  0. The result comes from the ALU, not
memory, so MemtoReg  0. Finally, the ALU should add, so ALUOp  00.
378 CHAPTER SEVEN Microarchitecture
Table 7.4 Main decoder truth table enhanced to support addi
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp
R-type 000000 1 1 0 0 0 0 10
lw 100011 1 0 1 0 0 1 00
sw 101011 0 X 1 0 1 X 00
beq 000100 0 X 0 1 0 X 01
addi 001000 1 0 1 0 0 0 00
Example 7.3 j INSTRUCTION
The jump instruction, j, writes a new value into the PC. The two least significant bits of the PC are always 0, because the PC is word aligned (i.e., always a
multiple of 4). The next 26 bits are taken from the jump address field in
Instr25:0. The upper four bits are taken from the old value of the PC.
The existing datapath lacks hardware to compute PC in this fashion. Determine
the necessary changes to both the datapath and controller to handle j.
Solution: First, we must add hardware to compute the next PC value, PC, in the
case of a j instruction and a multiplexer to select this next PC, as shown in
Figure 7.14. The new multiplexer uses the new Jump control signal.
Ch
Now we must add a row to the main decoder truth table for the j instruction
and a column for the Jump signal, as shown in Table 7.5. The Jump control
signal is 1 for the j instruction and 0 for all others. j does not write the register
file or memory, so RegWrite  MemWrite  0. Hence, we don’t care about
the computation done in the datapath, and RegDst  ALUSrc  Branch 
MemtoReg  ALUOp  X.
7.3 Single-Cycle Processor 379
SignImm
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
0
1
25:0 <<2
27:0 31:28
PCJump
Jump
ALU
Figure 7.14 Single-cycle MIPS datapath enhanced to support the j instruction
Table 7.5 Main decoder truth table enhanced to support j
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp Jump
R-type 000000 1 1 0 0 0 0 10 0
lw 100011 1 0 1 0 0 1 00 0
sw 101011 0 X 1 0 1 X 00 0
beq 000100 0 X 0 1 0 X 01 0
addi 001000 1 0 1 0 0 0 00 0
j 000010 0 X X X 0 X XX 1

7.3.4 Performance Analysis
Each instruction in the single-cycle processor takes one clock cycle, so the
CPI is 1. The critical path for the lw instruction is shown in Figure 7.15
with a heavy dashed blue line. It starts with the PC loading a new address
on the rising edge of the clock. The instruction memory reads the next
instruction. The register file reads SrcA. While the register file is reading,
the immediate field is sign-extended and selected at the ALUSrc multiplexer to determine SrcB. The ALU adds SrcA and SrcB to find the effective address. The data memory reads from this address. The MemtoReg
multiplexer selects ReadData. Finally, Result must setup at the register file
before the next rising clock edge, so that it can be properly written. Hence,
the cycle time is
(7.2)
In most implementation technologies, the ALU, memory, and register file
accesses are substantially slower than other operations. Therefore, the
cycle time simplifies to
(7.3)
The numerical values of these times will depend on the specific implementation technology.
Tc  tpcqPC 2tmem  tRFread 2tmux  tALU  tRFsetup
 tALU  tmem  tmux  tRFsetup
Tc  tpcqPC  tmem  max[tRFread, tsext]  tmux
380 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD1 WE3
CLK
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
5:0
SrcB
20:16
15:11
<<2
ALUResult ReadData
WriteData
SrcA
PCPlus4
PCBranch +
+
WriteReg4:0
Result
31:26
RegDst
Branch
MemWrite
MemtoReg
ALUSrc
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
ALUControl2:0
1
010 0
1
0
1
0 0
A2 RD2
ALU
Sign Extend
Figure 7.15 Critical path for lw instruction

Other instructions have shorter critical paths. For example, R-type
instructions do not need to access data memory. However, we are
disciplining ourselves to synchronous sequential design, so the clock
period is constant and must be long enough to accommodate the
slowest instruction.
Example 7.4 SINGLE-CYCLE PROCESSOR PERFORMANCE
Ben Bitdiddle is contemplating building the single-cycle MIPS processor in a 65 nm
CMOS manufacturing process. He has determined that the logic elements have the
delays given in Table 7.6. Help him compare the execution time for a program
with 100 billion instructions.
Solution: According to Equation 7.3, the cycle time of the single-cycle processor is
Tc1  30  2(250)  150  2(25)  200  20  950 ps. We use the subscript
“1” to distinguish it from subsequent processor designs. According to Equation
7.1, the total execution time is T1  (100  109 instructions)(1 cycle/instruction)
(950  1012 s/cycle)  95 seconds.
7.4 Multicycle Processor 381
Table 7.6 Delays of circuit elements
Element Parameter Delay (ps)
register clk-to-Q tpcq 30
register setup tsetup 20
multiplexer tmux 25
ALU tALU 200
memory read tmem 250
register file read tRFread 150
register file setup tRFsetup 20
7.4 MULTICYCLE PROCESSOR
The single-cycle processor has three primary weaknesses. First, it
requires a clock cycle long enough to support the slowest instruction
(lw), even though most instructions are faster. Second, it requires three
adders (one in the ALU and two for the PC logic); adders are relatively
expensive circuits, especially if they must be fast. And third, it has separate instruction and data memories, which may not be realistic. Most
computers have a single large memory that holds both instructions and
data and that can be read and written.
Ch
The multicycle processor addresses these weaknesses by breaking an
instruction into multiple shorter steps. In each short step, the processor
can read or write the memory or register file or use the ALU. Different
instructions use different numbers of steps, so simpler instructions can
complete faster than more complex ones. The processor needs only one
adder; this adder is reused for different purposes on various steps. And
the processor uses a combined memory for instructions and data. The
instruction is fetched from memory on the first step, and data may be
read or written on later steps.
We design a multicycle processor following the same procedure we
used for the single-cycle processor. First, we construct a datapath by connecting the architectural state elements and memories with combinational
logic. But, this time, we also add nonarchitectural state elements to hold
intermediate results between the steps. Then we design the controller. The
controller produces different signals on different steps during execution
of a single instruction, so it is now a finite state machine rather than combinational logic. We again examine how to add new instructions to the
processor. Finally, we analyze the performance of the multicycle processor
and compare it to the single-cycle processor.
7.4.1 Multicycle Datapath
Again, we begin our design with the memory and architectural state of the
MIPS processor, shown in Figure 7.16. In the single-cycle design, we used
separate instruction and data memories because we needed to read the
instruction memory and read or write the data memory all in one cycle.
Now, we choose to use a combined memory for both instructions and data.
This is more realistic, and it is feasible because we can read the instruction
in one cycle, then read or write the data in a separate cycle. The PC and
register file remain unchanged. We gradually build the datapath by adding
components to handle each step of each instruction. The new connections
are emphasized in black (or blue, for new control signals), whereas the
hardware that has already been studied is shown in gray.
The PC contains the address of the instruction to execute. The first
step is to read this instruction from instruction memory. Figure 7.17
shows that the PC is simply connected to the address input of the instruction memory. The instruction is read and stored in a new nonarchitectural
382 CHAPTER SEVEN Microarchitecture
CLK
A RD
Instr/Data
Memory
PC' PC
WD
WE
CLK
EN
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
Figure 7.16 State elements with
unified instruction/data memory

Instruction Register so that it is available for future cycles. The Instruction
Register receives an enable signal, called IRWrite, that is asserted when it
should be updated with a new instruction.
As we did with the single-cycle processor, we will work out the datapath connections for the lw instruction. Then we will enhance the datapath to handle the other instructions. For a lw instruction, the next step
is to read the source register containing the base address. This register is
specified in the rs field of the instruction, Instr25:21. These bits of the
instruction are connected to one of the address inputs, A1, of the register
file, as shown in Figure 7.18. The register file reads the register onto
RD1. This value is stored in another nonarchitectural register, A.
The lw instruction also requires an offset. The offset is stored in
the immediate field of the instruction, Instr15:0 and must be signextended to 32 bits, as shown in Figure 7.19. The 32-bit sign-extended
value is called SignImm. To be consistent, we might store SignImm in
another nonarchitectural register. However, SignImm is a combinational function of Instr and will not change while the current instruction is being processed, so there is no need to dedicate a register to
hold the constant value.
The address of the load is the sum of the base address and offset. We
use an ALU to compute this sum, as shown in Figure 7.20. ALUControl
should be set to 010 to perform an addition. ALUResult is stored in a
nonarchitectural register called ALUOut.
The next step is to load the data from the calculated address in the
memory. We add a multiplexer in front of the memory to choose the
7.4 Multicycle Processor 383
PC Instr
CLK
EN
IRWrite
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC'
WD
WE
CLK
Figure 7.17 Fetch instruction
from memory
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Register
File
PC' PC Instr 25:21
CLK
WD
WE
CLK CLK
A
EN
IRWrite
Figure 7.18 Read source operand
from register file

384 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
CLK
WD
WE
CLK CLK
A
EN
IRWrite
Figure 7.19 Sign-extend the immediate
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl2:0
WD
WE
CLK CLK
A CLK
EN
IRWrite
ALU
Figure 7.20 Add base address to offset
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
SrcB
ALUResult
SrcA
ALUOut
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A CLK
EN
IorD IRWrite
0
1
ALU
Figure 7.21 Load data from memory

memory address, Adr, from either the PC or ALUOut, as shown in
Figure 7.21. The multiplexer select signal is called IorD, to indicate
either an instruction or data address. The data read from the memory is
stored in another nonarchitectural register, called Data. Notice that the
address multiplexer permits us to reuse the memory during the lw
instruction. On the first step, the address is taken from the PC to fetch
the instruction. On a later step, the address is taken from ALUOut to
load the data. Hence, IorD must have different values on different steps.
In Section 7.4.2, we develop the FSM controller that generates these
sequences of control signals.
Finally, the data is written back to the register file, as shown in
Figure 7.22. The destination register is specified by the rt field of the
instruction, Instr20:16.
While all this is happening, the processor must update the program
counter by adding 4 to the old PC. In the single-cycle processor, a separate adder was needed. In the multicycle processor, we can use the existing ALU on one of the steps when it is not busy. To do so, we must
insert source multiplexers to choose the PC and the constant 4 as ALU
inputs, as shown in Figure 7.23. A two-input multiplexer controlled by
ALUSrcA chooses either the PC or register A as SrcA. A four-input multiplexer controlled by ALUSrcB chooses either 4 or SignImm as SrcB.
We use the other two multiplexer inputs later when we extend the datapath to handle other instructions. (The numbering of inputs to the multiplexer is arbitrary.) To update the PC, the ALU adds SrcA (PC) to
SrcB (4), and the result is written into the program counter register. The
PCWrite control signal enables the PC register to be written only on
certain cycles.
7.4 Multicycle Processor 385
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
PC' PC Instr 25:21
15:0
SrcB 20:16
ALUResult
SrcA
ALUOut
RegWrite
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A CLK
EN
IorD IRWrite
0
1
ALU
Figure 7.22 Write data back to register file

This completes the datapath for the lw instruction. Next, let us
extend the datapath to also handle the sw instruction. Like the lw
instruction, the sw instruction reads a base address from port 1 of the
register file and sign-extends the immediate. The ALU adds the base
address to the immediate to find the memory address. All of these functions are already supported by existing hardware in the datapath.
The only new feature of sw is that we must read a second register
from the register file and write it into the memory, as shown in
Figure 7.24. The register is specified in the rt field of the instruction,
Instr20:16, which is connected to the second port of the register file. When
the register is read, it is stored in a nonarchitectural register, B. On the
next step, it is sent to the write data port (WD) of the data memory to be
written. The memory receives an additional MemWrite control signal to
indicate that the write should occur.
386 CHAPTER SEVEN Microarchitecture
PCWrite
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
PC' PC Instr 1 25:21
15:0
SrcB
20:16
ALUResult
SrcA
ALUOut
RegWrite ALUSrcA
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A
00
01
10
11
4
CLK
EN EN
ALUSrcB1:0 IorD IRWrite
0
1
ALU
Figure 7.23 Increment PC by 4
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
SrcB 20:16
ALUResult
SrcA
ALUOut
MemWrite RegWrite ALUSrcA
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A
00
01
10
11
4
CLK
EN EN
PCWrite IorD IRWrite ALUSrcB1:0
B
ALU
Figure 7.24 Enhanced datapath for sw instruction

7.4 Multicycle Processor 387
0
1
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
SrcB 20:16
15:11
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA
CLK
ALUControl2:0
WD
WE
CLK
Adr
Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
PCWrite IorD IRWrite ALUSrcB1:0
ALU
Figure 7.25 Enhanced datapath for R-type instructions
For R-type instructions, the instruction is again fetched, and the
two source registers are read from the register file. Another input of the
SrcB multiplexer is used to choose register B as the second source register for the ALU, as shown in Figure 7.25. The ALU performs the appropriate operation and stores the result in ALUOut. On the next step,
ALUOut is written back to the register specified by the rd field of the
instruction, Instr15:11. This requires two new multiplexers. The
MemtoReg multiplexer selects whether WD3 comes from ALUOut (for
R-type instructions) or from Data (for lw). The RegDst instruction
selects whether the destination register is specified in the rt or rd field
of the instruction.
For the beq instruction, the instruction is again fetched, and the two
source registers are read from the register file. To determine whether the
registers are equal, the ALU subtracts the registers and examines the Zero
flag. Meanwhile, the datapath must compute the next value of the PC if
the branch is taken: PC  PC  4  SignImm  4. In the single-cycle
processor, yet another adder was needed to compute the branch address.
In the multicycle processor, the ALU can be reused again to save hardware. On one step, the ALU computes PC  4 and writes it back to the
program counter, as was done for other instructions. On another step, the
ALU uses this updated PC value to compute PC  SignImm  4.
SignImm is left-shifted by 2 to multiply it by 4, as shown in Figure 7.26.
The SrcB multiplexer chooses this value and adds it to the PC. This sum
represents the destination of the branch and is stored in ALUOut. A new
multiplexer, controlled by PCSrc, chooses what signal should be sent to
PC. The program counter should be written either when PCWrite is
asserted or when a branch is taken. A new control signal, Branch, indicates that the beq instruction is being executed. The branch is taken if
Zero is also asserted. Hence, the datapath computes a new PC write
Chap
enable, called PCEn, which is TRUE either when PCWrite is asserted or
when both Branch and Zero are asserted.
This completes the design of the multicycle MIPS processor datapath. The design process is much like that of the single-cycle processor in
that hardware is systematically connected between the state elements to
handle each instruction. The main difference is that the instruction is
executed in several steps. Nonarchitectural registers are inserted to hold
the results of each step. In this way, the ALU can be reused several times,
saving the cost of extra adders. Similarly, the instructions and data can
be stored in one shared memory. In the next section, we develop an FSM
controller to deliver the appropriate sequence of control signals to the
datapath on each step of each instruction.
7.4.2 Multicycle Control
As in the single-cycle processor, the control unit computes the control
signals based on the opcode and funct fields of the instruction,
Instr31:26 and Instr5:0. Figure 7.27 shows the entire multicycle MIPS
processor with the control unit attached to the datapath. The datapath is
shown in black, and the control unit is shown in blue.
As in the single-cycle processor, the control unit is partitioned into a
main controller and an ALU decoder, as shown in Figure 7.28. The
ALU decoder is unchanged and follows the truth table of Table 7.2.
Now, however, the main controller is an FSM that applies the proper
control signals on the proper cycles or steps. The sequence of control
signals depends on the instruction being executed. In the remainder of
this section, we will develop the FSM state transition diagram for the
main controller.
388 CHAPTER SEVEN Microarchitecture
SignImm
b
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
IorD IRWrite ALUSrcB1:0 PCWrite
PCEn
ALU
Figure 7.26 Enhanced datapath for beq instruction

The main controller produces multiplexer select and register enable
signals for the datapath. The select signals are MemtoReg, RegDst,
IorD, PCSrc, ALUSrcB, and ALUSrcA. The enable signals are IRWrite,
MemWrite, PCWrite, Branch, and RegWrite.
To keep the following state transition diagrams readable, only the
relevant control signals are listed. Select signals are listed only when
their value matters; otherwise, they are don’t cares. Enable signals are
listed only when they are asserted; otherwise, they are 0.
7.4 Multicycle Processor 389
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File 0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
tsDgeR
Branch
MemWrite
geRotmeM ALUSrcA
RegWrite Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
ULA
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
IRWrite ALUSrcB1:0
IorD
PCWrite
PCEn
Figure 7.27 Complete multicycle MIPS processor
ALUSrcA
PCSrc
Branch
ALUSrcB1:0
Opcode5:0
Control
Unit
ALUControl2:0 Funct5:0
Main
Controller
(FSM)
ALUOp1:0
ALU
Decoder
RegWrite
PCWrite
IorD
MemWrite
IRWrite
RegDst
MemtoReg
Register
Enables
Multiplexer
Selects
Figure 7.28 Control unit internal
structure

The first step for any instruction is to fetch the instruction from
memory at the address held in the PC. The FSM enters this state on
reset. To read memory, IorD  0, so the address is taken from the PC.
IRWrite is asserted to write the instruction into the instruction register,
IR. Meanwhile, the PC should be incremented by 4 to point to the next
instruction. Because the ALU is not being used for anything else, the
processor can use it to compute PC  4 at the same time that it fetches
the instruction. ALUSrcA  0, so SrcA comes from the PC. ALUSrcB 
01, so SrcB is the constant 4. ALUOp  00, so the ALU decoder
produces ALUControl  010 to make the ALU add. To update the
PC with this new value, PCSrc  0, and PCWrite is asserted. These
control signals are shown in Figure 7.29. The data flow on this step is
shown in Figure 7.30, with the instruction fetch shown using the dashed
blue line and the PC increment shown using the dashed gray line.
The next step is to read the register file and decode the instruction.
The register file always reads the two sources specified by the rs and rt
fields of the instruction. Meanwhile, the immediate is sign-extended.
Decoding involves examining the opcode of the instruction to determine
what to do next. No control signals are necessary to decode the instruction, but the FSM must wait 1 cycle for the reading and decoding to
complete, as shown in Figure 7.31. The new state is highlighted in blue.
The data flow is shown in Figure 7.32.
390 CHAPTER SEVEN Microarchitecture
Reset
S0: Fetch
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
Figure 7.29 Fetch
ALU
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
RegDst
MemtoReg
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
Branch
MemWrite
ALUSrcA
RegWrite
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB IRWrite 1:0
IorD
PCWrite
PCEn
0
1 1
0
X
X
0 0
01
010 0
1
0
Figure 7.30 Data flow during the fetch step

Now the FSM proceeds to one of several possible states, depending
on the opcode. If the instruction is a memory load or store (lw or sw),
the multicycle processor computes the address by adding the base
address to the sign-extended immediate. This requires ALUSrcA  1 to
select register A and ALUSrcB  10 to select SignImm. ALUOp  00,
so the ALU adds. The effective address is stored in the ALUOut register
for use on the next step. This FSM step is shown in Figure 7.33, and the
data flow is shown in Figure 7.34.
If the instruction is lw, the multicycle processor must next read data
from memory and write it to the register file. These two steps are shown
in Figure 7.35. To read from memory, IorD  1 to select the memory
address that was just computed and saved in ALUOut. This address in
memory is read and saved in the Data register during step S3. On the
next step, S4, Data is written to the register file. MemtoReg  1 to select
7.4 Multicycle Processor 391
Reset
S0: Fetch S1: Decode
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
Figure 7.31 Decode
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1
WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB IRWrite 1:0
IorD
PCWrite
PCEn
X
0 0
0
X
X
0 X
XX
XXX X
0
0
ALU
Figure 7.32 Data flow during the decode step

Data, and RegDst  0 to pull the destination register from the rt field
of the instruction. RegWrite is asserted to perform the write, completing
the lw instruction. Finally, the FSM returns to the initial state, S0, to
fetch the next instruction. For these and subsequent steps, try to visualize the data flow on your own.
From state S2, if the instruction is sw, the data read from the second
port of the register file is simply written to memory. IorD  1 to select
the address computed in S2 and saved in ALUOut. MemWrite is
asserted to write the memory. Again, the FSM returns to S0 to fetch the
next instruction. The added step is shown in Figure 7.36.
392 CHAPTER SEVEN Microarchitecture
ALUSrcA = 1
Reset
S0: Fetch
S2: MemAdr
S1: Decode
Op = LW
or
Op = SW
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
ALUOp = 00
ALUSrcB = 10
Figure 7.33 Memory address
computation
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1 0
1
PC 0
1
PC' Instr 25:21
20:16
15:0
5:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
31:26
Branch
MemWrite
ALUSrcA
RegWrite
RegDst
MemtoReg
Op
Funct
Control
Unit
Zero
PCSrc
CLK
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB IRWrite 1:0
IorD
PCWrite
PCEn
X
0 0
0
X
X
0 1
10
010 X
0
0
ALU
Figure 7.34 Data flow during memory address computation

If the opcode indicates an R-type instruction, the multicycle processor must calculate the result using the ALU and write that result to the
register file. Figure 7.37 shows these two steps. In S6, the instruction is
executed by selecting the A and B registers (ALUSrcA  1, ALUSrcB 
00) and performing the ALU operation indicated by the funct field
of the instruction. ALUOp  10 for all R-type instructions. The
ALUResult is stored in ALUOut. In S7, ALUOut is written to the register file, RegDst  1, because the destination register is specified in the
rd field of the instruction. MemtoReg  0 because the write data, WD3,
comes from ALUOut. RegWrite is asserted to write the register file.
For a beq instruction, the processor must calculate the destination
address and compare the two source registers to determine whether the
branch should be taken. This requires two uses of the ALU and hence
might seem to demand two new states. Notice, however, that the ALU was
not used during S1 when the registers were being read. The processor
might as well use the ALU at that time to compute the destination address
by adding the incremented PC, PC  4, to SignImm  4, as shown in
7.4 Multicycle Processor 393
IorD = 1
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
Op = LW
or
Op = SW
Op = LW
S4: Mem
Writeback
PCWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
MemtoReg = 1
RegWrite
Figure 7.35 Memory read
C
Figure 7.38 (see page 396). ALUSrcA  0 to select the incremented PC,
ALUSrcB  11 to select SignImm  4, and ALUOp  00 to add. The
destination address is stored in ALUOut. If the instruction is not beq, the
computed address will not be used in subsequent cycles, but its computation was harmless. In S8, the processor compares the two registers by subtracting them and checking to determine whether the result is 0. If it is, the
processor branches to the address that was just computed. ALUSrcA  1
to select register A; ALUSrcB  00 to select register B; ALUOp  01 to
subtract; PCSrc  1 to take the destination address from ALUOut, and
Branch  1 to update the PC with this address if the ALU result is 0.2
Putting these steps together, Figure 7.39 shows the complete main
controller state transition diagram for the multicycle processor (see page
397). Converting it to hardware is a straightforward but tedious task
using the techniques of Chapter 3. Better yet, the FSM can be coded in
an HDL and synthesized using the techniques of Chapter 4.
394 CHAPTER SEVEN Microarchitecture
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
Op = LW
or
Op = SW
Op = LW
Op = SW
S4: Mem
Writeback
IRWrite
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
PCWrite
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
Figure 7.36 Memory write
2 Now we see why the PCSrc multiplexer is necessary to choose PC from either ALUResult
(in S0) or ALUOut (in S8).
Ch
7.4.3 More Instructions
As we did in Section 7.3.3 for the single-cycle processor, let us now
extend the multicycle processor to support the addi and j instructions.
The next two examples illustrate the general design process to support
new instructions.
Example 7.5 addi INSTRUCTION
Modify the multicycle processor to support addi.
Solution: The datapath is already capable of adding registers to immediates, so all
we need to do is add new states to the main controller FSM for addi, as shown in
Figure 7.40 (see page 398). The states are similar to those for R-type instructions.
In S9, register A is added to SignImm (ALUSrcA  1, ALUSrcB  10, ALUOp 
00) and the result, ALUResult, is stored in ALUOut. In S10, ALUOut is written
7.4 Multicycle Processor 395
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
Op = LW
or
Op = SW
Op = LW
S4: Mem
Writeback
S5: MemWrite
S6: Execute
S7: ALU
Writeback
Op = R-type
Op = SW
PCWrite
IRWrite
IorD = 0
PCSrc = 0
ALUOp = 00
ALUSrcB = 01
AluSrcA = 0
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
RegDst = 1
RegWrite
MemtoReg = 0
ALUSrcA = 1
ALUOp = 10
ALUSrcB = 00
Figure 7.37 Execute R-type
operation

to the register specified by the rt field of the instruction (RegDst  0, MemtoReg
 0, RegWrite asserted). The astute reader may notice that S2 and S9 are identical
and could be merged into a single state.
Example 7.6 j INSTRUCTION
Modify the multicycle processor to support j.
Solution: First, we must modify the datapath to compute the next PC value in
the case of a j instruction. Then we add a state to the main controller to handle
the instruction.
Figure 7.41 shows the enhanced datapath (see page 399). The jump destination
address is formed by left-shifting the 26-bit addr field of the instruction by two
bits, then prepending the four most significant bits of the already incremented
PC. The PCSrc multiplexer is extended to take this address as a third input.
396 CHAPTER SEVEN Microarchitecture
IorD = 0
AluSrcA = 0
ALUSrcB = 01
ALUOp = 00
PCSrc = 0
IRWrite
PCWrite
ALUSrcA = 0
ALUSrcB = 11
ALUOp = 00
ALUSrcA = 1
ALUSrcB = 10
ALUOp = 00
IorD = 1
RegDst = 1
MemtoReg = 0
RegWrite
IorD = 1
MemWrite
ALUSrcA = 1
ALUSrcB = 00
ALUOp = 10
ALUSrcA = 1
ALUSrcB = 00
ALUOp = 01
PCSrc = 1
Branch
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
RegDst = 0
MemtoReg = 1
RegWrite
S4: Mem
Writeback
Figure 7.38 Branch

Figure 7.42 shows the enhanced main controller (see page 400). The new state,
S11, simply selects PC as the PCJump value (PCSrc  10) and writes the PC.
Note that the PCSrc select signal is extended to two bits in S0 and S8 as well.
7.4.4 Performance Analysis
The execution time of an instruction depends on both the number of cycles
it uses and the cycle time. Whereas the single-cycle processor performed all
instructions in one cycle, the multicycle processor uses varying numbers of
cycles for the various instructions. However, the multicycle processor does
less work in a single cycle and, thus, has a shorter cycle time.
The multicycle processor requires three cycles for beq and j instructions, four cycles for sw, addi, and R-type instructions, and five cycles
for lw instructions. The CPI depends on the relative likelihood that each
instruction is used.
7.4 Multicycle Processor 397
IorD = 0
ALUSrcA = 0
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
S4: Mem
Writeback
PCWrite
IRWrite
PCSrc = 0
ALUOp = 00
ALUSrcB = 01
AluSrcA = 0
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
RegDst = 1
RegWrite
MemtoReg = 0
ALUOp = 00
ALUSrcB = 11
ALUSrcA = 1
ALUOp = 10
ALUSrcB = 00
ALUSrcA = 1
Branch
PCSrc = 1
ALUOp = 01
ALUSrcB = 00
Figure 7.39 Complete multicycle
control FSM
C
Example 7.7 MULTICYCLE PROCESSOR CPI
The SPECINT2000 benchmark consists of approximately 25% loads, 10%
stores, 11% branches, 2% jumps, and 52% R-type instructions.3 Determine the
average CPI for this benchmark.
Solution: The average CPI is the sum over each instruction of the CPI for that
instruction multiplied by the fraction of the time that instruction is used. For this
benchmark, Average CPI  (0.11  0.02)(3)  (0.52  0.10)(4)  (0.25)(5) 
4.12. This is better than the worst-case CPI of 5, which would be required if all
instructions took the same time.
398 CHAPTER SEVEN Microarchitecture
IorD = 1 IorD = 1
MemWrite
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
S4: Mem
Writeback
Op = ADDI
S9: ADDI
Execute
S10: ADDI
Writeback
IorD = 0
PCWrite
IRWrite
PCSrc = 0
ALUOp = 00
ALUSrcB = 01
AluSrcA = 0
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
RegDst = 0
RegWrite
MemtoReg = 1
RegDst = 1
RegWrite
MemtoReg = 0
RegDst = 1
RegWrite
MemtoReg = 0
ALUSrcA = 1
ALUOp = 10
ALUSrcB = 00
ALUSrcA = 1
ALUOp = 00
ALUSrcB = 10
ALUSrcA = 0
ALUOp = 00
ALUSrcB = 11
ALUSrcA = 1
Branch
PCSrc = 1
ALUOp = 01
ALUSrcB = 00
Figure 7.40 Main controller states for addi
3 Data from Patterson and Hennessy, Computer Organization and Design, 3rd Edition,
Morgan Kaufmann, 2005.

Recall that we designed the multicycle processor so that each cycle
involved one ALU operation, memory access, or register file access. Let
us assume that the register file is faster than the memory and that writing memory is faster than reading memory. Examining the datapath
reveals two possible critical paths that would limit the cycle time:
(7.4)
The numerical values of these times will depend on the specific implementation technology.
Example 7.8 PROCESSOR PERFORMANCE COMPARISON
Ben Bitdiddle is wondering whether he would be better off building the multicycle processor instead of the single-cycle processor. For both designs, he plans on
using a 65 nm CMOS manufacturing process with the delays given in Table 7.6.
Help him compare each processor’s execution time for 100 billion instructions
from the SPECINT2000 benchmark (see Example 7.7).
Solution: According to Equation 7.4, the cycle time of the multicycle processor is
Tc2  30  25  250  20  325 ps. Using the CPI of 4.12 from Example 7.7,
the total execution time is T2  (100  109 instructions)(4.12 cycles/instruction)
(325  1012 s/cycle)  133.9 seconds. According to Example 7.4, the singlecycle processor had a cycle time of Tc1  950 ps, a CPI of 1, and a total execution time of 95 seconds.
One of the original motivations for building a multicycle processor was to
avoid making all instructions take as long as the slowest one. Unfortunately,
this example shows that the multicycle processor is slower than the single-cycle
Tc  tpcq  tmux  max(tALU  tmux, tmem)  tsetup
7.4 Multicycle Processor 399
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ULA
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc1:0
CLK
ALUControl2:0
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
IorD IRWrite ALUSrcB1:0 PCWrite
PCEn
00
01
10
<<2
25:0 (addr)
31:28
27:0
PCJump
Figure 7.41 Multicycle MIPS datapath enhanced to support the j instruction
Ch
processor given the assumptions of CPI and circuit element delays. The fundamental problem is that even though the slowest instruction, lw, was broken into
five steps, the multicycle processor cycle time was not nearly improved fivefold. This is partly because not all of the steps are exactly the same length, and
partly because the 50-ps sequencing overhead of the register clk-to-Q and setup
time must now be paid on every step, not just once for the entire instruction. In
general, engineers have learned that it is difficult to exploit the fact that some
computations are faster than others unless the differences are large.
Compared with the single-cycle processor, the multicycle processor is likely to be
less expensive because it eliminates two adders and combines the instruction and
data memories into a single unit. It does, however, require five nonarchitectural
registers and additional multiplexers.
400 CHAPTER SEVEN Microarchitecture
IorD = 0
AluSrcA= 0
ALUSrcB= 01
ALUOp = 00
PCSrc = 00
IRWrite
PCWrite
ALUSrcA= 0
ALUSrcB= 11
ALUOp = 00
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
IorD = 1
RegDst = 1
MemtoReg = 0
RegWrite
IorD = 1
MemWrite
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 10
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 01
PCSrc = 01
Branch
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
RegDst = 0
MemtoReg = 1
RegWrite
S4: Mem
Writeback
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
RegDst = 0
MemtoReg = 0
RegWrite
Op = ADDI
S9: ADDI
Execute
S10: ADDI
Writeback
PCSrc = 10
PCWrite
Op = J
S11: Jump
Figure 7.42 Main controller state for j

7.5 PIPELINED PROCESSOR
Pipelining, introduced in Section 3.6, is a powerful way to improve the
throughput of a digital system. We design a pipelined processor by subdividing the single-cycle processor into five pipeline stages. Thus,
five instructions can execute simultaneously, one in each stage. Because
each stage has only one-fifth of the entire logic, the clock frequency
is almost five times faster. Hence, the latency of each instruction is
ideally unchanged, but the throughput is ideally five times better.
Microprocessors execute millions or billions of instructions per second,
so throughput is more important than latency. Pipelining introduces
some overhead, so the throughput will not be quite as high as we
might ideally desire, but pipelining nevertheless gives such great advantage for so little cost that all modern high-performance microprocessors are pipelined.
Reading and writing the memory and register file and using the ALU
typically constitute the biggest delays in the processor. We choose five
pipeline stages so that each stage involves exactly one of these slow
steps. Specifically, we call the five stages Fetch, Decode, Execute,
Memory, and Writeback. They are similar to the five steps that the
multicycle processor used to perform lw. In the Fetch stage, the processor reads the instruction from instruction memory. In the Decode stage,
the processor reads the source operands from the register file and
decodes the instruction to produce the control signals. In the Execute
stage, the processor performs a computation with the ALU. In the
Memory stage, the processor reads or writes data memory. Finally, in the
Writeback stage, the processor writes the result to the register file, when
applicable.
Figure 7.43 shows a timing diagram comparing the single-cycle and
pipelined processors. Time is on the horizontal axis, and instructions are
on the vertical axis. The diagram assumes the logic element delays from
Table 7.6 but ignores the delays of multiplexers and registers. In the single-cycle processor, Figure 7.43(a), the first instruction is read from
memory at time 0; next the operands are read from the register file; and
then the ALU executes the necessary computation. Finally, the data
memory may be accessed, and the result is written back to the register
file by 950 ps. The second instruction begins when the first completes.
Hence, in this diagram, the single-cycle processor has an instruction
latency of 250
 150
 200
 250
 100
 950 ps and a throughput
of 1 instruction per 950 ps (1.05 billion instructions per second).
In the pipelined processor, Figure 7.43(b), the length of a pipeline
stage is set at 250 ps by the slowest stage, the memory access (in the
Fetch or Memory stage). At time 0, the first instruction is fetched from
memory. At 250 ps, the first instruction enters the Decode stage, and
7.5 Pipelined Processor 401

402
Time (ps)
Instr
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg 1
2
0 100 200 300 400 500 600 700 800 900 1100 1200 1300 1400 1500 1600 1700 1800 1900 1000
(a)
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Instr
1
2
(b)
3
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Fetch
Instruction
Decode
Read Reg
Execute
ALU
Memory
Read/Write
Write
Reg
Fetch
Instruction
Figure 7.43 Timing diagrams: (a) single-cycle processor, (b) pipelined processor

a second instruction is fetched. At 500 ps, the first instruction executes,
the second instruction enters the Decode stage, and a third instruction is
fetched. And so forth, until all the instructions complete. The instruction latency is 5  250  1250 ps. The throughput is 1 instruction per
250 ps (4 billion instructions per second). Because the stages are not
perfectly balanced with equal amounts of logic, the latency is slightly
longer for the pipelined than for the single-cycle processor. Similarly,
the throughput is not quite five times as great for a five-stage pipeline as
for the single-cycle processor. Nevertheless, the throughput advantage is
substantial.
Figure 7.44 shows an abstracted view of the pipeline in operation in
which each stage is represented pictorially. Each pipeline stage is represented with its major component—instruction memory (IM), register file
(RF) read, ALU execution, data memory (DM), and register file writeback—to illustrate the flow of instructions through the pipeline. Reading
across a row shows the clock cycles in which a particular instruction is
in each stage. For example, the sub instruction is fetched in cycle 3 and
executed in cycle 5. Reading down a column shows what the various
pipeline stages are doing on a particular cycle. For example, in cycle 6,
the or instruction is being fetched from instruction memory, while $s1 is
being read from the register file, the ALU is computing $t5 AND $t6,
the data memory is idle, and the register file is writing a sum to $s3.
Stages are shaded to indicate when they are used. For example, the data
memory is used by lw in cycle 4 and by sw in cycle 8. The instruction
memory and ALU are used in every cycle. The register file is written by
7.5 Pipelined Processor 403
Time (cycles)
lw $s2, 40($0) RF 40
$0
RF $s2 + DM
RF $t2
$t1
RF $s3 + DM
RF $s5
$s1
RF $s4 - DM
RF $t6
$t5
RF $s5 & DM
RF 20
$s1
RF $s6 + DM
RF $t4
$t3
RF $s7
| DM
add $s3, $t1, $t2
sub $s4, $s1, $s5
and $s5, $t5, $t6
sw $s6, 20($s1)
or $s7, $t3, $t4
1 2 3 4 5 6 7 8 9 10
add
IM
IM
IM
IM
IM
IM lw
sub
and
sw
or
Figure 7.44 Abstract view of pipeline in operation
C
every instruction except sw. We assume that in the pipelined processor,
the register file is written in the first part of a cycle and read in the second part, as suggested by the shading. This way, data can be written and
read back within a single cycle.
A central challenge in pipelined systems is handling hazards that
occur when the results of one instruction are needed by a subsequent
instruction before the former instruction has completed. For example, if
the add in Figure 7.44 used $s2 rather than $t2, a hazard would occur
because the $s2 register has not been written by the lw by the time it is
read by the add. This section explores forwarding, stalls, and flushes as
methods to resolve hazards. Finally, this section revisits performance
analysis considering sequencing overhead and the impact of hazards.
7.5.1 Pipelined Datapath
The pipelined datapath is formed by chopping the single-cycle datapath
into five stages separated by pipeline registers. Figure 7.45(a) shows the
single-cycle datapath stretched out to leave room for the pipeline registers. Figure 7.45(b) shows the pipelined datapath formed by inserting
four pipeline registers to separate the datapath into five stages. The
stages and their boundaries are indicated in blue. Signals are given a suffix (F, D, E, M, or W) to indicate the stage in which they reside.
The register file is peculiar because it is read in the Decode stage and
written in the Writeback stage. It is drawn in the Decode stage, but the
write address and data come from the Writeback stage. This feedback
will lead to pipeline hazards, which are discussed in Section 7.5.3.
One of the subtle but critical issues in pipelining is that all signals
associated with a particular instruction must advance through the
pipeline in unison. Figure 7.45(b) has an error related to this issue. Can
you find it?
The error is in the register file write logic, which should operate in
the Writeback stage. The data value comes from ResultW, a Writeback
stage signal. But the address comes from WriteRegE, an Execute stage
signal. In the pipeline diagram of Figure 7.44, during cycle 5, the result
of the lw instruction would be incorrectly written to register $s4 rather
than $s2.
Figure 7.46 shows a corrected datapath. The WriteReg signal is now
pipelined along through the Memory and Writeback stages, so it remains
in sync with the rest of the instruction. WriteRegW and ResultW are fed
back together to the register file in the Writeback stage.
The astute reader may notice that the PC logic is also problematic, because it might be updated with a Fetch or a Memory stage
signal (PCPlus4F or PCBranchM). This control hazard will be fixed in
Section 7.5.3.
404 CHAPTER SEVEN Microarchitecture
C
7.5.2 Pipelined Control
The pipelined processor takes the same control signals as the single-cycle
processor and therefore uses the same control unit. The control unit
examines the opcode and funct fields of the instruction in the Decode
stage to produce the control signals, as was described in Section 7.3.2.
These control signals must be pipelined along with the data so that they
remain synchronized with the instruction.
The entire pipelined processor with control is shown in Figure 7.47.
RegWrite must be pipelined into the Writeback stage before it feeds back
to the register file, just as WriteReg was pipelined in Figure 7.46.
7.5 Pipelined Processor 405
SignImm
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PC
1
PC' Instr 25:21
20:16
15:0
SrcB
20:16
15:11
<<2
ALUResult
ALU
ReadData
WriteData
SrcA
PCPlus4 +
+ PCBranch
WriteReg4:0
Result
Zero
CLK
(a)
SignImmE
CLK
A RD
Instruction
Memory
4
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PCF
1
PC' InstrD 25:21
20:16
15:0
SrcBE
20:16
15:11
RtE
RdE
<<2
ALUOutM
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchM
ResultW
PCPlus4F PCPlus4E
ZeroM
CLK CLK
WriteRegE4:0
CLK
CLK
CLK
ALU
+
+
(b)
Fetch Decode Execute Memory Writeback
Figure 7.45 Single-cycle and pipelined datapaths

7.5.3 Hazards
In a pipelined system, multiple instructions are handled concurrently.
When one instruction is dependent on the results of another that has not
yet completed, a hazard occurs.
406 CHAPTER SEVEN Microarchitecture
SignImmE
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PCF
1
PC' InstrD 25:21
20:16
15:0
5:0
SrcBE
20:16
15:11
RtE
RdE
<<2
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchM
WriteRegM4:0
ResultW
PCPlus4F PCPlus4E
31:26
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
ZeroM
PCSrcM
CLK CLK CLK
CLK CLK
WriteRegW4:0
ALUControlE2:0
RegWriteE RegWriteM RegWriteW
MemtoRegE MemtoRegM MemtoRegW
MemWriteE MemWriteM
BranchE BranchM
RegDstE
ALUSrcE
WriteRegE4:0
Figure 7.47 Pipelined processor with control
SignImmE
CLK
A RD
Instruction
Memory
4
+
+
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
0
1
0 PCF
1
PC' InstrD 25:21
20:16
15:0
SrcBE
20:16
15:11
RtE
RdE
<<2
ALUOutM
ALU
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchM
WriteRegM4:0
ResultW
PCPlus4F PCPlus4E
ZeroM
CLK CLK
WriteRegE4:0 WriteRegW4:0
CLK
CLK
CLK
Fetch Decode Execute Memory Writeback
Figure 7.46 Corrected pipelined datapath

The register file can be read and written in the same cycle. Let us
assume that the write takes place during the first half of the cycle and the
read takes place during the second half of the cycle, so that a register can
be written and read back in the same cycle without introducing a hazard.
Figure 7.48 illustrates hazards that occur when one instruction
writes a register ($s0) and subsequent instructions read this register. This
is called a read after write (RAW) hazard. The add instruction writes a
result into $s0 in the first half of cycle 5. However, the and instruction
reads $s0 on cycle 3, obtaining the wrong value. The or instruction
reads $s0 on cycle 4, again obtaining the wrong value. The sub instruction reads $s0 in the second half of cycle 5, obtaining the correct value,
which was written in the first half of cycle 5. Subsequent instructions
also read the correct value of $s0. The diagram shows that hazards may
occur in this pipeline when an instruction writes a register and either of
the two subsequent instructions read that register. Without special treatment, the pipeline will compute the wrong result.
On closer inspection, however, observe that the sum from the add
instruction is computed by the ALU in cycle 3 and is not strictly needed
by the and instruction until the ALU uses it in cycle 4. In principle, we
should be able to forward the result from one instruction to the next to
resolve the RAW hazard without slowing down the pipeline. In other
situations explored later in this section, we may have to stall the pipeline
to give time for a result to be computed before the subsequent instruction uses the result. In any event, something must be done to solve
hazards so that the program executes correctly despite the pipelining.
Hazards are classified as data hazards or control hazards. A data
hazard occurs when an instruction tries to read a register that has not
yet been written back by a previous instruction. A control hazard occurs
when the decision of what instruction to fetch next has not been made
by the time the fetch takes place. In the remainder of this section, we will
7.5 Pipelined Processor 407
Time (cycles)
add $s0, $s2, $s3 RF $s3
$s2
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM add
or
sub
Figure 7.48 Abstract pipeline diagram illustrating hazards

enhance the pipelined processor with a hazard unit that detects hazards
and handles them appropriately, so that the processor executes the
program correctly.
Solving Data Hazards with Forwarding
Some data hazards can be solved by forwarding (also called bypassing) a
result from the Memory or Writeback stage to a dependent instruction in
the Execute stage. This requires adding multiplexers in front of the ALU
to select the operand from either the register file or the Memory or
Writeback stage. Figure 7.49 illustrates this principle. In cycle 4, $s0 is
forwarded from the Memory stage of the add instruction to the Execute
stage of the dependent and instruction. In cycle 5, $s0 is forwarded from
the Writeback stage of the add instruction to the Execute stage of the
dependent or instruction.
Forwarding is necessary when an instruction in the Execute stage
has a source register matching the destination register of an instruction
in the Memory or Writeback stage. Figure 7.50 modifies the pipelined
processor to support forwarding. It adds a hazard detection unit and
two forwarding multiplexers. The hazard detection unit receives the two
source registers from the instruction in the Execute stage and the destination registers from the instructions in the Memory and Writeback
stages. It also receives the RegWrite signals from the Memory and
Writeback stages to know whether the destination register will actually
be written (for example, the sw and beq instructions do not write results
to the register file and hence do not need to have their results forwarded). Note that the RegWrite signals are connected by name. In
other words, rather than cluttering up the diagram with long wires running from the control signals at the top to the hazard unit at the bottom,
the connections are indicated by a short stub of wire labeled with the
control signal name to which it is connected.
408 CHAPTER SEVEN Microarchitecture
Time (cycles)
add $s0, $s2, $s3 RF $s3
$s2
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM add
or
sub
Figure 7.49 Abstract pipeline diagram illustrating forwarding

409
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
4
+
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
25:21
20:16
15:0
5:0
EBcrS
25:21
15:11
EsR
EdR
2<<
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
MhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
31:26
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
DmmIngiS
20:16 EtR
DsR
DdR
DtR
tinU
ForwardAE
ForwardBE
RegWriteM
RegWriteW
 drazaH
E4sulPCP
EhcnarB MhcnarB
MoreZ
ALU
Figure 7.50 Pipelined processor with forwarding to solve hazards

The hazard detection unit computes control signals for the forwarding multiplexers to choose operands from the register file or from the
results in the Memory or Writeback stage. It should forward from a
stage if that stage will write a destination register and the destination
register matches the source register. However, $0 is hardwired to 0 and
should never be forwarded. If both the Memory and Writeback stages
contain matching destination registers, the Memory stage should have
priority, because it contains the more recently executed instruction. In
summary, the function of the forwarding logic for SrcA is given below.
The forwarding logic for SrcB (ForwardBE) is identical except that it
checks rt rather than rs.
if ((rsE ! 0) AND (rsE  WriteRegM) AND RegWriteM) then
ForwardAE  10
else if ((rsE ! 0) AND (rsE  WriteRegW) AND RegWriteW) then
ForwardAE  01
else ForwardAE  00
Solving Data Hazards with Stalls
Forwarding is sufficient to solve RAW data hazards when the result is
computed in the Execute stage of an instruction, because its result can
then be forwarded to the Execute stage of the next instruction.
Unfortunately, the lw instruction does not finish reading data until the
end of the Memory stage, so its result cannot be forwarded to the
Execute stage of the next instruction. We say that the lw instruction has
a two-cycle latency, because a dependent instruction cannot use its result
until two cycles later. Figure 7.51 shows this problem. The lw instruction
receives data from memory at the end of cycle 4. But the and instruction
needs that data as a source operand at the beginning of cycle 4. There is
no way to solve this hazard with forwarding.
410 CHAPTER SEVEN Microarchitecture
Time (cycles)
lw $s0, 40($0) RF 40
$0
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM lw
or
sub
Trouble!
Figure 7.51 Abstract pipeline diagram illustrating trouble forwarding from lw

The alternative solution is to stall the pipeline, holding up operation
until the data is available. Figure 7.52 shows stalling the dependent
instruction (and) in the Decode stage. and enters the Decode stage in
cycle 3 and stalls there through cycle 4. The subsequent instruction (or)
must remain in the Fetch stage during both cycles as well, because the
Decode stage is full.
In cycle 5, the result can be forwarded from the Writeback stage of
lw to the Execute stage of and. In cycle 6, source $s0 of the or instruction is read directly from the register file, with no need for forwarding.
Notice that the Execute stage is unused in cycle 4. Likewise,
Memory is unused in Cycle 5 and Writeback is unused in cycle 6. This
unused stage propagating through the pipeline is called a bubble, and it
behaves like a nop instruction. The bubble is introduced by zeroing out
the Execute stage control signals during a Decode stall so that the bubble
performs no action and changes no architectural state.
In summary, stalling a stage is performed by disabling the pipeline
register, so that the contents do not change. When a stage is stalled, all
previous stages must also be stalled, so that no subsequent instructions
are lost. The pipeline register directly after the stalled stage must be
cleared to prevent bogus information from propagating forward. Stalls
degrade performance, so they should only be used when necessary.
Figure 7.53 modifies the pipelined processor to add stalls for lw
data dependencies. The hazard unit examines the instruction in the
Execute stage. If it is lw and its destination register (rtE) matches either
source operand of the instruction in the Decode stage (rsD or rtD), that
instruction must be stalled in the Decode stage until the source operand
is ready.
Stalls are supported by adding enable inputs (EN) to the Fetch and
Decode pipeline registers and a synchronous reset/clear (CLR) input to
the Execute pipeline register. When a lw stall occurs, StallD and StallF
7.5 Pipelined Processor 411
Time (cycles)
lw $s0, 40($0) RF 40
$0
RF $s0 + DM
RF $s1
$s0
RF $t0 & DM
RF $s0
$s4
RF $t1 | DM
RF $s5
$s0
RF $t2 - DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM lw
or
sub
9
RF $s1
$s0
IM or
Stall
Figure 7.52 Abstract pipeline diagram illustrating stall to solve hazards

412
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
4
+
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
12:52
61:02
0:51
0:5
EBcrS
12:52
11:51
EsR
EdR
2<<
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
MhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
62:13
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
McrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
DmmIngiS
61:02 EtR
DsR
DdR
DtR
tinU drazaH
E4sulPCP
EhcnarB MhcnarB
MoreZ
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallD EN
EN
ALU
CLR
StallF
RegWriteW
Figure 7.53 Pipelined processor with stalls to solve lw data hazard

are asserted to force the Decode and Fetch stage pipeline registers to
hold their old values. FlushE is also asserted to clear the contents of the
Execute stage pipeline register, introducing a bubble.4
The MemtoReg signal is asserted for the lw instruction. Hence, the
logic to compute the stalls and flushes is
lwstall  ((rsD  rtE) OR (rtD  rtE)) AND MemtoRegE
StallF  StallD  FlushE  lwstall
Solving Control Hazards
The beq instruction presents a control hazard: the pipelined processor
does not know what instruction to fetch next, because the branch decision has not been made by the time the next instruction is fetched.
One mechanism for dealing with the control hazard is to stall the
pipeline until the branch decision is made (i.e., PCSrc is computed).
Because the decision is made in the Memory stage, the pipeline would
have to be stalled for three cycles at every branch. This would severely
degrade the system performance.
An alternative is to predict whether the branch will be taken and
begin executing instructions based on the prediction. Once the branch
decision is available, the processor can throw out the instructions if the
prediction was wrong. In particular, suppose that we predict that
branches are not taken and simply continue executing the program in
order. If the branch should have been taken, the three instructions following the branch must be flushed (discarded) by clearing the pipeline
registers for those instructions. These wasted instruction cycles are called
the branch misprediction penalty.
Figure 7.54 shows such a scheme, in which a branch from address 20
to address 64 is taken. The branch decision is not made until cycle 4, by
which point the and, or, and sub instructions at addresses 24, 28, and 2C
have already been fetched. These instructions must be flushed, and the
slt instruction is fetched from address 64 in cycle 5. This is somewhat of
an improvement, but flushing so many instructions when the branch is
taken still degrades performance.
We could reduce the branch misprediction penalty if the branch
decision could be made earlier. Making the decision simply requires
comparing the values of two registers. Using a dedicated equality comparator is much faster than performing a subtraction and zero detection.
If the comparator is fast enough, it could be moved back into the
Decode stage, so that the operands are read from the register file and
compared to determine the next PC by the end of the Decode stage.
7.5 Pipelined Processor 413
4 Strictly speaking, only the register designations (RsE, RtE, and RdE) and the control signals that might update memory or architectural state (RegWrite, MemWrite, and Branch)
need to be cleared; as long as these signals are cleared, the bubble can contain random data
that has no effect.

Figure 7.55 shows the pipeline operation with the early branch decision being made in cycle 2. In cycle 3, the and instruction is flushed and
the slt instruction is fetched. Now the branch misprediction penalty is
reduced to only one instruction rather than three.
Figure 7.56 modifies the pipelined processor to move the branch
decision earlier and handle control hazards. An equality comparator is
added to the Decode stage and the PCSrc AND gate is moved earlier, so
414 CHAPTER SEVEN Microarchitecture
Time (cycles)
beq $t1, $t2, 40 RF $t2
$t1
- RF DM
RF $s1
$s0
RF DM
RF $s0
$s4
| RF DM
RF $s5
$s0
- RF DM
and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and
IM
IM
IM
IM lw
or
sub
20
24
28
2C
30
...
...
9
Flush
these
instructions
64 slt $t3, $s2, $s3 RF $s3
$s2
RF DM $t3 IM slt slt
&
Figure 7.54 Abstract pipeline diagram illustrating flushing when a branch is taken
Time (cycles)
beq $t1, $t2, 40 RF $t2
$t1
- RF DM
RF $s1
$s0
& RF DM and $t0, $s0, $s1
or $t1, $s4, $s0
sub $t2, $s0, $s5
12345678
and IM
IM lw 20
24
28
2C
30
...
...
9
Flush
this
instruction
64 slt $t3, $s2, $s3 RF $s3
$s2
RF DM $t3 IM slt slt
Figure 7.55 Abstract pipeline diagram illustrating earlier branch decision

415
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
yromeM
4
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
12:52
61:02
0:51
0:5
EBcrS
12:52
11:51
EsR
EdR
2<<
+
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
DhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
62:13
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
lortnoC
tinU
DcrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
=
DmmIngiS
61:02 EtR
DsR
DdR
DtR
tinU drazaH
ForwardAE
ForwardBE
RegWriteM
MemtoRegE
FlushE
StallF
StallD
RegWriteW
CLR
EN
EN
ALU
CLR
Figure 7.56 Pipelined processor handling branch control hazard

that PCSrc can be determined in the Decoder stage rather than the
Memory stage. The PCBranch adder must also be moved into the
Decode stage so that the destination address can be computed in time.
The synchronous clear input (CLR) connected to PCSrcD is added to the
Decode stage pipeline register so that the incorrectly fetched instruction
can be flushed when a branch is taken.
Unfortunately, the early branch decision hardware introduces a new
RAW data hazard. Specifically, if one of the source operands for the branch
was computed by a previous instruction and has not yet been written into
the register file, the branch will read the wrong operand value from the register file. As before, we can solve the data hazard by forwarding the correct
value if it is available or by stalling the pipeline until the data is ready.
Figure 7.57 shows the modifications to the pipelined processor needed
to handle the Decode stage data dependency. If a result is in the Writeback
stage, it will be written in the first half of the cycle and read during
the second half, so no hazard exists. If the result of an ALU instruction is
in the Memory stage, it can be forwarded to the equality comparator
through two new multiplexers. If the result of an ALU instruction is in the
Execute stage or the result of a lw instruction is in the Memory stage, the
pipeline must be stalled at the Decode stage until the result is ready.
The function of the Decode stage forwarding logic is given below.
ForwardAD  (rsD ! 0) AND (rsD  WriteRegM) AND RegWriteM
ForwardBD  (rtD ! 0) AND (rtD  WriteRegM) AND RegWriteM
The function of the stall detection logic for a branch is given below.
The processor must make a branch decision in the Decode stage. If either
of the sources of the branch depends on an ALU instruction in the
Execute stage or on a lw instruction in the Memory stage, the processor
must stall until the sources are ready.
branchstall 
BranchD AND RegWriteE AND (WriteRegE  rsD OR WriteRegE  rtD)
OR
BranchD AND MemtoRegM AND (WriteRegM  rsD OR WriteRegM  rtD)
Now the processor might stall due to either a load or a branch hazard:
StallF  StallD  FlushE  lwstall OR branchstall
Hazard Summary
In summary, RAW data hazards occur when an instruction depends on
the result of another instruction that has not yet been written into the
register file. The data hazards can be resolved by forwarding if the result
is computed soon enough; otherwise, they require stalling the pipeline
until the result is available. Control hazards occur when the decision of
what instruction to fetch has not been made by the time the next instruction must be fetched. Control hazards are solved by predicting which
416 CHAPTER SEVEN Microarchitecture

417
DlauqE
EmmIngiS
KLC
DRA
noitcurtsnI
omeM yr
4
+
+
1A
3A
3DW
2DR
1DR
3EW
2A
KLC
ngiS
dnetxE
retsigeR
eliF
0
1
0
1
DRA
ataD
yromeM
DW
EW
1
0
0 FCP
1
'CP DrtsnI
12:52
61:02
0:51
0:5
EBcrS
12:52
11:51
EsR
EdR
2<<
MtuOULA
WtuOULA
WataDdaeR
EataDetirW MataDetirW
EAcrS
D4sulPCP
DhcnarBCP
geRetirW M4:0
WtluseR
F4sulPCP
62:13
DtsDgeR
DhcnarB
DetirWmeM
DgeRotmeM
D2:0 lortnoCULA
DcrSULA
DetirWgeR
pO
tcnuF
Control
Unit
DcrSCP
KLC KLC KLC
KLC KLC
geRetirW W4:0
E2:0 lortnoCULA
EetirWgeR MetirWgeR WetirWgeR
EgeRotmeM MgeRotmeM WgeRotmeM
EetirWmeM MetirWmeM
EtsDgeR
EcrSULA
geRetirW E4:0
00
10
01
00
10
01
0
1
0
1
=
DmmIngiS
61:02 EtR
DsR
DdR
DtR
tinU drazaH
ForwardAD
ForwardAE
ForwardBE
BranchD
ForwardBD
RegWriteE
RegWriteM
MemtoRegE
MemtoRegM
FlushE
CLR
CLR
EN
EN
ALU
StallF
StallD
RegWriteW
Figure 7.57 Pipelined processor handling data dependencies for branch instructions

instruction should be fetched and flushing the pipeline if the prediction is
later determined to be wrong. Moving the decision as early as possible
minimizes the number of instructions that are flushed on a misprediction.
You may have observed by now that one of the challenges of designing a
pipelined processor is to understand all the possible interactions between
instructions and to discover all the hazards that may exist. Figure 7.58
shows the complete pipelined processor handling all of the hazards.
7.5.4 More Instructions
Supporting new instructions in the pipelined processor is much like supporting them in the single-cycle processor. However, new instructions
may introduce hazards that must be detected and solved.
In particular, supporting addi and j instructions on the pipelined
processor requires enhancing the controller, exactly as was described in
Section 7.3.3, and adding a jump multiplexer to the datapath after the
branch multiplexer. Like a branch, the jump takes place in the Decode
stage, so the subsequent instruction in the Fetch stage must be flushed.
Designing this flush logic is left as Exercise 7.29.
7.5.5 Performance Analysis
The pipelined processor ideally would have a CPI of 1, because a new
instruction is issued every cycle. However, a stall or a flush wastes a
cycle, so the CPI is slightly higher and depends on the specific program
being executed.
Example 7.9 PIPELINED PROCESSOR CPI
The SPECINT2000 benchmark considered in Example 7.7 consists of approximately 25% loads, 10% stores, 11% branches, 2% jumps, and 52% R-type
instructions. Assume that 40% of the loads are immediately followed by an
instruction that uses the result, requiring a stall, and that one quarter of the
branches are mispredicted, requiring a flush. Assume that jumps always flush
the subsequent instruction. Ignore other hazards. Compute the average CPI of
the pipelined processor.
Solution: The average CPI is the sum over each instruction of the CPI for that
instruction multiplied by the fraction of time that instruction is used. Loads take
one clock cycle when there is no dependency and two cycles when the processor
must stall for a dependency, so they have a CPI of (0.6)(1)  (0.4)(2)  1.4.
Branches take one clock cycle when they are predicted properly and two when they
are not, so they have a CPI of (0.75)(1)  (0.25)(2)  1.25. Jumps always have a
CPI of 2. All other instructions have a CPI of 1. Hence, for this benchmark, Average
CPI  (0.25)(1.4)  (0.1)(1)  (0.11)(1.25)  (0.02)(2)  (0.52)(1)  1.15.
418 CHAPTER SEVEN Microarchitecture

419
EqualD
SignImmE
CLK
A RD
Instruction
Memory
+
4
A1
A3
WD3
RD2
RD1
WE3
A2
CLK
Sign
Extend
Register
File
0
1
0
1
A RD
Data
Memory
WD
WE
1
0
0 PCF
1
PC' InstrD
25:21
20:16
15:0
5:0
SrcBE
25:21
15:11
RsE
RdE
<<2
+
ALUOutM
ALUOutW
ReadDataW
WriteDataE WriteDataM
SrcAE
PCPlus4D
PCBranchD
WriteRegM4:0
ResultW
PCPlus4F
31:26
RegDstD
BranchD
MemWriteD
MemtoRegD
ALUControlD2:0
ALUSrcD
RegWriteD
Op
Funct
Control
Unit
PCSrcD
CLK CLK CLK
CLK CLK
WriteRegW4:0
ALUControlE2:0
ALU
RegWriteE RegWriteM RegWriteW
MemtoRegE MemtoRegM MemtoRegW
MemWriteE MemWriteM
RegDstE
ALUSrcE
WriteRegE4:0
00
01
10
00
01
10
0
1
0
1
=
SignImmD
20:16 RtE
RsD
RdD
RtD
Hazard Unit
StallF
StallD
ForwardAD
ForwardBD
ForwardAE
ForwardBE
MemtoRegE
RegWriteE
RegWriteM
RegWriteW
BranchD
FlushE
EN
EN CLR
CLR
Figure 7.58 Pipelined processor with full hazard handling

We can determine the cycle time by considering the critical path in
each of the five pipeline stages shown in Figure 7.58. Recall that the register file is written in the first half of the Writeback cycle and read in the second half of the Decode cycle. Therefore, the cycle time of the Decode and
Writeback stages is twice the time necessary to do the half-cycle of work.
(7.5)
Example 7.10 PROCESSOR PERFORMANCE COMPARISON
Ben Bitdiddle needs to compare the pipelined processor performance to that of
the single-cycle and multicycle processors considered in Example 7.8. Most of
the logic delays were given in Table 7.6. The other element delays are 40 ps for
an equality comparator, 15 ps for an AND gate, 100 ps for a register file write,
and 220 ps for a memory write. Help Ben compare the execution time of 100
billion instructions from the SPECINT2000 benchmark for each processor.
Solution: According to Equation 7.5, the cycle time of the pipelined processor is
Tc3  max[30  250  20, 2(150  25  40  15  25  20), 30  25  25 
200  20, 30  220  20, 2(30  25  100)]  550 ps. According to Equation
7.1, the total execution time is T3  (100  109 instructions)(1.15 cycles/ instruction)(550  1012 s/cycle)  63.3 seconds. This compares to 95 seconds for the
single-cycle processor and 133.9 seconds for the multicycle processor.
The pipelined processor is substantially faster than the others. However, its advantage over the single-cycle processor is nowhere near the five-fold speedup one might
hope to get from a five-stage pipeline. The pipeline hazards introduce a small CPI
penalty. More significantly, the sequencing overhead (clk-to-Q and setup times) of
the registers applies to every pipeline stage, not just once to the overall datapath.
Sequencing overhead limits the benefits one can hope to achieve from pipelining.
The careful reader might observe that the Decode stage is substantially slower than
the others, because the register file write, read, and branch comparison must all
happen in half a cycle. Perhaps moving the branch comparison to the Decode stage
was not such a good idea. If branches were resolved in the Execute stage instead,
the CPI would increase slightly, because a mispredict would flush two instructions,
but the cycle time would decrease substantially, giving an overall speedup.
The pipelined processor is similar in hardware requirements to the
single-cycle processor, but it adds a substantial number of pipeline registers, along with multiplexers and control logic to resolve hazards.
Tc  max tpcq  tmem  tsetup
2(tRFread tmux  teq  tAND  tmux tsetup)
tpcq  tmux  tmux  tALU  tsetup
tpcq  tmemwrite  tsetup
2(tpcq  tmux  tRFwrite)

420 CHAPTER SEVEN Microarchitecture
Fetch
Decode
Execute
Memory
Writeback
Chap
7.6 HDL REPRESENTATION*
This section presents HDL code for the single-cycle MIPS processor
supporting all of the instructions discussed in this chapter, including
addi and j. The code illustrates good coding practices for a moderately
complex system. HDL code for the multicycle processor and pipelined
processor are left to Exercises 7.22 and 7.33.
In this section, the instruction and data memories are separated from
the main processor and connected by address and data busses. This is
more realistic, because most real processors have external memory. It also
illustrates how the processor can communicate with the outside world.
The processor is composed of a datapath and a controller. The
controller, in turn, is composed of the main decoder and the ALU
decoder. Figure 7.59 shows a block diagram of the single-cycle MIPS
processor interfaced to external memories.
The HDL code is partitioned into several sections. Section 7.6.1
provides HDL for the single-cycle processor datapath and controller.
Section 7.6.2 presents the generic building blocks, such as registers and
multiplexers, that are used by any microarchitecture. Section 7.6.3
7.6 HDL Representation 421
Opcode5:0
Controller Funct5:0
Datapath
A RD
Instruction
Memory
PC Instr
A RD
Data
Memory
WD
WE
CLK
ALUOut
WriteData
ReadData
ALUControl
Branch
MemtoReg
ALUSrc
RegDst
MemWrite
2:0
Main
Decoder
ALUOp1:0 ALU
Decoder
RegWrite
5:0 31:26
CLK
Reset
MIPS Processor External Memory PCSrc Zero
Figure 7.59 MIPS single-cycle processor interfaced to external memory

introduces the testbench and external memories. The HDL is available
in electronic form on the this book’s Web site (see the preface).
7.6.1 Single-Cycle Processor
The main modules of the single-cycle MIPS processor module are given
in the following HDL examples.
422 CHAPTER SEVEN Microarchitecture
Verilog
module mips (input clk, reset,
output [31:0] pc,
input [31:0] instr,
output memwrite,
output [31:0] aluout, writedata,
input [31:0] readdata);
wire memtoreg, branch,
alusrc, regdst, regwrite, jump;
wire [2:0] alucontrol;
controller c(instr[31:26], instr[5:0], zero,
memtoreg, memwrite, pcsrc,
alusrc, regdst, regwrite, jump,
alucontrol);
datapath dp(clk, reset, memtoreg, pcsrc,
alusrc, regdst, regwrite, jump,
alucontrol,
zero, pc, instr,
aluout, writedata, readdata);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mips is — — single cycle MIPS processor
port (clk, reset: in STD_LOGIC;
pc: out STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR (31 downto 0);
memwrite: out STD_LOGIC;
aluout, writedata: out STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR (31 downto 0));
end;
architecture struct of mips is
component controller
port (op, funct: in STD_LOGIC_VECTOR (5 downto 0);
zero: in STD_LOGIC;
memtoreg, memwrite: out STD_LOGIC;
pcsrc, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end component;
component datapath
port (clk, reset: in STD_LOGIC;
memtoreg, pcsrc: in STD_LOGIC;
alusrc, regdst: in STD_LOGIC;
regwrite, jump: in STD_LOGIC;
alucontrol: in STD_LOGIC_VECTOR (2 downto 0);
zero: out STD_LOGIC;
pc: buffer STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR (31 downto 0);
aluout, writedata: buffer STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR (31 downto 0));
end component;
signal memtoreg, alusrc, regdst, regwrite, jump, pcsrc:
STD_LOGIC;
signal zero: STD_LOGIC;
signal alucontrol: STD_LOGIC_VECTOR (2 downto 0);
begin
cont: controller port map (instr (31 downto 26), instr
(5 downto 0), zero, memtoreg,
memwrite, pcsrc, alusrc, regdst,
regwrite, jump, alucontrol);
dp: datapath port map (clk, reset, memtoreg, pcsrc, alusrc,
regdst, regwrite, jump, alucontrol,
zero, pc, instr, aluout, writedata,
readdata);
end;
HDL Example 7.1 SINGLE-CYCLE MIPS PROCESSOR

7.6 HDL Representation 423
Verilog
module controller (input [5:0] op, funct,
input zero,
output memtoreg, memwrite,
output pcsrc, alusrc,
output regdst, regwrite,
output jump,
output [2:0] alucontrol);
wire [1:0] aluop;
wire branch;
maindec md (op, memtoreg, memwrite, branch,
alusrc, regdst, regwrite, jump,
aluop);
aludec ad (funct, aluop, alucontrol);
assign pcsrc  branch & zero;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity controller is — — single cycle control decoder
port (op, funct: in STD_LOGIC_VECTOR (5 downto 0);
zero: in STD_LOGIC;
memtoreg, memwrite: out STD_LOGIC;
pcsrc, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end;
architecture struct of controller is
component maindec
port (op: in STD_LOGIC_VECTOR (5 downto 0);
memtoreg, memwrite: out STD_LOGIC;
branch, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
aluop: out STD_LOGIC_VECTOR (1 downto 0));
end component;
component aludec
port (funct: in STD_LOGIC_VECTOR (5 downto 0);
aluop: in STD_LOGIC_VECTOR (1 downto 0);
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end component;
signal aluop: STD_LOGIC_VECTOR (1 downto 0);
signal branch: STD_LOGIC;
begin
md: maindec port map (op, memtoreg, memwrite, branch,
alusrc, regdst, regwrite, jump, aluop);
ad: aludec port map (funct, aluop, alucontrol);
pcsrc  branch and zero;
end;
HDL Example 7.2 CONTROLLER

Verilog
module maindec(input [5:0] op,
output memtoreg, memwrite,
output branch, alusrc,
output regdst, regwrite,
output jump,
output [1:0] aluop);
reg [8:0] controls;
assign {regwrite, regdst, alusrc,
branch, memwrite,
memtoreg, jump, aluop}  controls;
always @ (*)
case(op)
6b000000: controls  9b110000010; //Rtyp
6b100011: controls  9b101001000; //LW
6b101011: controls  9b001010000; //SW
6b000100: controls  9b000100001; //BEQ
6b001000: controls  9b101000000; //ADDI
6b000010: controls  9b000000100; //J
default: controls  9bxxxxxxxxx; //???
endcase
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity maindec is — — main control decoder
port (op: in STD_LOGIC_VECTOR (5 downto 0);
memtoreg, memwrite: out STD_LOGIC;
branch, alusrc: out STD_LOGIC;
regdst, regwrite: out STD_LOGIC;
jump: out STD_LOGIC;
aluop: out STD_LOGIC_VECTOR (1 downto 0));
end;
architecture behave of maindec is
signal controls: STD_LOGIC_VECTOR(8 downto 0);
begin
process(op) begin
case op is
when "000000"  controls  "110000010"; — — Rtyp
when "100011"  controls  "101001000"; — — LW
when "101011"  controls  "001010000"; — — SW
when "000100"  controls  "000100001"; — — BEQ
when "001000"  controls  "101000000"; — — ADDI
when "000010"  controls  "000000100"; — — J
when others  controls  "---------"; — — illegal op
end case;
end process;
regwrite  controls(8);
regdst  controls(7);
alusrc  controls(6);
branch  controls(5);
memwrite  controls(4);
memtoreg  controls(3);
jump  controls(2);
aluop  controls(1 downto 0);
end;
HDL Example 7.3 MAIN DECODER
Verilog
module aludec (input [5:0] funct,
input [1:0] aluop,
output reg [2:0] alucontrol);
always @ (*)
case (aluop)
2b00: alucontrol  3b010; // add
2b01: alucontrol  3b110; // sub
default: case(funct) // RTYPE
6b100000: alucontrol  3b010; // ADD
6b100010: alucontrol  3b110; // SUB
6b100100: alucontrol  3b000; // AND
6b100101: alucontrol  3b001; // OR
6b101010: alucontrol  3b111; // SLT
default: alucontrol  3bxxx; // ???
endcase
endcase
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity aludec is — — ALU control decoder
port (funct: in STD_LOGIC_VECTOR (5 downto 0);
aluop: in STD_LOGIC_VECTOR (1 downto 0);
alucontrol: out STD_LOGIC_VECTOR (2 downto 0));
end;
architecture behave of aludec is
begin
process (aluop, funct) begin
case aluop is
when "00"  alucontrol  "010"; — — add (for 1b/sb/addi)
when "01"  alucontrol  "110"; — — sub (for beq)
when others  case funct is — — R-type instructions
when "100000"  alucontrol 
"010"; — — add
when "100010"  alucontrol 
"110"; — — sub
when "100100"  alucontrol 
"000"; — — and
when "100101"  alucontrol 
"001"; — — or
when "101010"  alucontrol 
"111"; — — slt
when others  alucontrol 
"— — —"; — — ???
end case;
end case;
end process;
end;
HDL Example 7.4 ALU DECODER
Chapter 07.qxd 2/1/07 9:33 P
Verilog
module datapath (input clk, reset,
input memtoreg, pcsrc,
input alusrc, regdst,
input regwrite, jump,
input [2:0] alucontrol,
output zero,
output [31:0] pc,
input [31:0] instr,
output [31:0] aluout, writedata,
input [31:0] readdata);
wire [4:0] writereg;
wire [31:0] pcnext, pcnextbr, pcplus4, pcbranch;
wire [31:0] signimm, signimmsh;
wire [31:0] srca, srcb;
wire [31:0] result;
// next PC logic
flopr #(32) pcreg(clk, reset, pcnext, pc);
adder pcadd1 (pc, 32b100, pcplus4);
sl2 immsh(signimm, signimmsh);
adder pcadd2(pcplus4, signimmsh, pcbranch);
mux2 #(32) pcbrmux(pcplus4, pcbranch, pcsrc,
pcnextbr);
mux2 #(32) pcmux(pcnextbr, {pcplus4[31:28],
instr[25:0], 2b00},
jump, pcnext);
// register file logic
regfile rf(clk, regwrite, instr[25:21],
instr[20:16], writereg,
result, srca, writedata);
mux2 #(5) wrmux(instr[20:16], instr[15:11],
regdst, writereg);
mux2 #(32) resmux(aluout, readdata,
memtoreg, result);
signext se(instr[15:0], signimm);
// ALU logic
mux2 #(32) srcbmux(writedata, signimm, alusrc,
srcb);
alu alu(srca, srcb, alucontrol,
aluout, zero);
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all; use
IEEE.STD_LOGIC_ARITH.all;
entity datapath is — — MIPS datapath
port(clk, reset: in STD_LOGIC;
memtoreg, pcsrc: in STD_LOGIC;
alusrc, regdst: in STD_LOGIC;
regwrite, jump: in STD_LOGIC;
alucontrol: in STD_LOGIC_VECTOR (2 downto 0);
zero: out STD_LOGIC;
pc: buffer STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR(31 downto 0);
aluout, writedata: buffer STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR(31 downto 0));
end;
architecture struct of datapath is
component alu
port(a, b: in STD_LOGIC_VECTOR(31 downto 0);
alucontrol: in STD_LOGIC_VECTOR(2 downto 0);
result: buffer STD_LOGIC_VECTOR(31 downto 0);
zero: out STD_LOGIC);
end component;
component regfile
port(clk: in STD_LOGIC;
we3: in STD_LOGIC;
ra1, ra2, wa3: in STD_LOGIC_VECTOR (4 downto 0);
wd3: in STD_LOGIC_VECTOR (31 downto 0);
rd1, rd2: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component adder
port(a, b: in STD_LOGIC_VECTOR (31 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component sl2
port(a: in STD_LOGIC_VECTOR (31 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component signext
port(a: in STD_LOGIC_VECTOR (15 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component flopr generic (width: integer);
port(clk, reset: in STD_LOGIC;
d: in STD_LOGIC_VECTOR (width-1 downto 0);
q: out STD_LOGIC_VECTOR (width-1 downto 0));
end component;
component mux2 generic (width: integer);
port(d0, d1: in STD_LOGIC_VECTOR (width-1 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR (width-1 downto 0));
end component;
signal writereg: STD_LOGIC_VECTOR (4 downto 0);
signal pcjump, pcnext, pcnextbr,
pcplus4, pcbranch: STD_LOGIC_VECTOR (31 downto 0);
signal signimm, signimmsh: STD_LOGIC_VECTOR (31 downto 0);
signal srca, srcb, result: STD_LOGIC_VECTOR (31 downto 0);
begin
— — next PC logic
pcjump  pcplus4 (31 downto 28) & instr (25 downto 0) & "00";
pcreg: flopr generic map(32) port map(clk, reset, pcnext, pc);
pcadd1: adder port map(pc, X"00000004", pcplus4);
immsh: sl2 port map(signimm, signimmsh);
pcadd2: adder port map(pcplus4, signimmsh, pcbranch);
pcbrmux: mux2 generic map(32) port map(pcplus4, pcbranch,
pcsrc, pcnextbr);
pcmux: mux2 generic map(32) port map(pcnextbr, pcjump, jump,
pcnext);
— — register file logic
rf: regfile port map(clk, regwrite, instr(25 downto 21),
instr(20 downto 16), writereg, result, srca,
writedata);
wrmux: mux2 generic map(5) port map(instr(20 downto 16),
instr(15 downto 11), regdst, writereg);
resmux: mux2 generic map(32) port map(aluout, readdata,
memtoreg, result);
se: signext port map(instr(15 downto 0), signimm);
— — ALU logic
srcbmux: mux2 generic map (32) port map(writedata, signimm,
alusrc, srcb);
mainalu: alu port map(srca, srcb, alucontrol, aluout, zero);
end;
HDL Example 7.5 DATAPATH
425
Ch
Verilog
module regfile (input clk,
input we3,
input [4:0] ra1, ra2, wa3,
input [31:0] wd3,
output [31:0] rd1, rd2);
reg [31:0] rf[31:0];
// three ported register file
// read two ports combinationally
// write third port on rising edge of clock
// register 0 hardwired to 0
always @ (posedge clk)
if (we3) rf[wa3]  wd3;
assign rd1  (ra1 !  0) ? rf[ra1] : 0;
assign rd2  (ra2 !  0) ? rf[ra2] : 0;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
entity regfile is — — three-port register file
port(clk: in STD_LOGIC;
we3: in STD_LOGIC;
ra1, ra2, wa3:in STD_LOGIC_VECTOR(4 downto 0);
wd3: in STD_LOGIC_VECTOR(31 downto 0);
rd1, rd2: out STD_LOGIC_VECTOR(31 downto 0));
end;
architecture behave of regfile is
type ramtype is array (31 downto 0) of STD_LOGIC_VECTOR (31
downto 0);
signal mem: ramtype;
begin
— — three-ported register file
— — read two ports combinationally
— — write third port on rising edge of clock
process(clk) begin
if clk'event and clk  '1' then
if we3  '1' then mem(CONV_INTEGER(wa3))  wd3;
end if;
end if;
end process;
process (ra1, ra2) begin
if (conv_integer (ra1)  0) then rd1  X"00000000";
— — register 0 holds 0
else rd1  mem(CONV_INTEGER (ra1));
end if;
if (conv_integer(ra2)  0) then rd2  X"00000000";
else rd2  mem(CONV_INTEGER(ra2));
end if;
end process;
end;
HDL Example 7.6 REGISTER FILE
Verilog
module adder (input [31:0] a, b,
output [31:0] y);
assign y  a  b;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
use IEEE.STD_LOGIC_UNSIGNED.all;
entity adder is — adder
port (a, b: in STD_LOGIC_VECTOR(31 downto 0);
y: out STD_LOGIC_VECTOR(31 downto 0));
end;
architecture behave of adder is
begin
y  a  b;
end;
HDL Example 7.7 ADDER
7.6.2 Generic Building Blocks
This section contains generic building blocks that may be useful in any
MIPS microarchitecture, including a register file, adder, left shift unit,
sign-extension unit, resettable flip-flop, and multiplexer. The HDL for
the ALU is left to Exercise 5.9.

Verilog
module sl2 (input [31:0] a,
output [31:0] y);
// shift left by 2
assign y  {a[29:01], 2'b00};
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity sl2 is — — shift left by 2
port (a: in STD_LOGIC_VECTOR (31 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of sl2 is
begin
y  a(29 downto 0) & "00";
end;
HDL Example 7.8 LEFT SHIFT (MULTIPLY BY 4)
Verilog
module signext (input [15:0] a,
output [31:0] y);
assign y  {{16{a[15]}}, a};
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity signext is — — sign extender
port(a: in STD_LOGIC_VECTOR (15 downto 0);
y: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of signext is
begin
y  X"0000" & a when a (15)  '0' else X"ffff" & a;
end;
HDL Example 7.9 SIGN EXTENSION
Verilog
module flopr # (parameter WIDTH  8)
(input clk, reset,
input [WIDTH-1:0] d,
output reg [WIDTH-1:0] q);
always @ (posedge clk, posedge reset)
if (reset) q  0;
else q  d;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all; use
IEEE.STD_LOGIC_ARITH.all;
entity flopr is — — flip-flop with synchronous reset
generic (width: integer);
port (clk, reset: in STD_LOGIC;
d: in STD_LOGIC_VECTOR(width-1 downto 0);
q: out STD_LOGIC_VECTOR(width-1 downto 0));
end;
architecture asynchronous of flopr is
begin
process (clk, reset) begin
if reset  '1' then q  CONV_STD_LOGIC_VECTOR(0, width);
elsif clk’event and clk  '1' then
q  d;
end if;
end process;
end;
HDL Example 7.10 RESETTABLE FLIP-FLOP
7.6 HDL Representation 427

428 CHAPTER SEVEN Microarchitecture
Verilog
module mux2 # (parameter WIDTH  8)
(input [WIDTH-1:0] d0, d1,
input s,
output [WIDTH-1:0] y);
assign y  s ? d1 : d0;
endmodule
VHDL
library IEEE; use IEEE.STD_LOGIC_1164.all;
entity mux2 is — — two-input multiplexer
generic (width: integer);
port (d0, d1: in STD_LOGIC_VECTOR(width-1 downto 0);
s: in STD_LOGIC;
y: out STD_LOGIC_VECTOR(width-1 downto 0));
end;
architecture behave of mux2 is
begin
y  d0 when s  '0' else d1;
end;
HDL Example 7.11 2:1 MULTIPLEXER
# mipstest.asm
# David_Harris@hmc.edu 9 November 2005
#
# Test the MIPS processor.
# add, sub, and, or, slt, addi, lw, sw, beq, j
# If successful, it should write the value 7 to address 84
# Assembly Description Address Machine
main: addi $2, $0, 5 # initialize $2  5 0 20020005
addi $3, $0, 12 # initialize $3  12 4 2003000c
addi $7, $3, 9 # initialize $7  3 8 2067fff7
or $4, $7, $2 # $4  3 or 5  7 c 00e22025
and $5, $3, $4 # $5  12 and 7  4 10 00642824
add $5, $5, $4 # $5  4  7  11 14 00a42820
beq $5, $7, end # shouldn’t be taken 18 10a7000a
slt $4, $3, $4 # $4  12  7  0 1c 0064202a
beq $4, $0, around # should be taken 20 10800001
addi $5, $0, 0 # shouldn’t happen 24 20050000
around: slt $4, $7, $2 # $4  3  5  1 28 00e2202a
add $7, $4, $5 # $7  1  11  12 2c 00853820
sub $7, $7, $2 # $7  12  5  7 30 00e23822
sw $7, 68($3) # [80]  7 34 ac670044
lw $2, 80($0) # $2  [80]  7 38 8c020050
j end # should be taken 3c 08000011
addi $2, $0, 1 # shouldn’t happen 40 20020001
end: sw $2, 84($0) # write adr 84  7 44 ac020054
Figure 7.60 Assembly and machine code for MIPS test program
20020005
2003000c
2067fff7
00e22025
00642824
00a42820
10a7000a
0064202a
10800001
20050000
00e2202a
00853820
00e23822
ac670044
8c020050
08000011
20020001
ac020054
Figure 7.61 Contents of
memfile.dat
7.6.3 Testbench
The MIPS testbench loads a program into the memories. The program in
Figure 7.60 exercises all of the instructions by performing a computation
that should produce the correct answer only if all of the instructions are
functioning properly. Specifically, the program will write the value 7 to
address 84 if it runs correctly, and is unlikely to do so if the hardware is
buggy. This is an example of ad hoc testing.

7.6 HDL Representation 429
The machine code is stored in a hexadecimal file called memfile.dat
(see Figure 7.61), which is loaded by the testbench during simulation.
The file consists of the machine code for the instructions, one instruction
per line.
The testbench, top-level MIPS module, and external memory HDL
code are given in the following examples. The memories in this example
hold 64 words each.
Verilog
module testbench();
reg clk;
reg reset;
wire [31:0] writedata, dataadr;
wire memwrite;
// instantiate device to be tested
top dut (clk, reset, writedata, dataadr, memwrite);
// initialize test
initial
begin
reset  1; # 22; reset  0;
end
// generate clock to sequence tests
always
begin
clk  1; # 5; clk  0; # 5;
end
// check results
always @ (negedge clk)
begin
if (memwrite) begin
if (dataadr  84 & writedata  7) begin
$display ("Simulation succeeded");
$stop;
end else if (dataadr ! 80) begin
$display ("Simulation failed");
$stop;
end
end
end
endmodule
VHDL
library IEEE;
use IEEE.STD_LOGIC_1164.all; use IEEE.STD_LOGIC_UNSIGNED.all;
entity testbench is
end;
architecture test of testbench is
component top
port(clk, reset: in STD_LOGIC;
writedata, dataadr: out STD_LOGIC_VECTOR(31 downto 0);
memwrite: out STD_LOGIC);
end component;
signal writedata, dataadr: STD_LOGIC_VECTOR(31 downto 0);
signal clk, reset, memwrite: STD_LOGIC;
begin
— — instantiate device to be tested
dut: top port map (clk, reset, writedata, dataadr, memwrite);
— — Generate clock with 10 ns period
process begin
clk  '1';
wait for 5 ns;
clk  '0';
wait for 5 ns;
end process;
— — Generate reset for first two clock cycles
process begin
reset  '1';
wait for 22 ns;
reset  '0';
wait;
end process;
— — check that 7 gets written to address 84
— — at end of program
process (clk) begin
if (clk'event and clk  '0' and memwrite  '1') then
if (conv_integer(dataadr)  84 and conv_integer
(writedata)  7) then
report "Simulation succeeded";
elsif (dataadr / 80) then
report "Simulation failed";
end if;
end if;
end process;
end;
HDL Example 7.12 MIPS TESTBENCH

Verilog
module top (input clk, reset,
output [31:0] writedata, dataadr,
output memwrite);
wire [31:0] pc, instr, readdata;
// instantiate processor and memories
mips mips (clk, reset, pc, instr, memwrite, dataadr,
writedata, readdata);
imem imem (pc[7:2], instr);
dmem dmem (clk, memwrite, dataadr, writedata,
readdata);
endmodule
VHDL
library IEEE;
use IEEE.STD_LOGIC_1164.all; use IEEE.STD_LOGIC_UNSIGNED.all;
entity top is — — top-level design for testing
port (clk, reset: in STD_LOGIC;
writedata, dataadr: buffer STD_LOGIC_VECTOR (31 downto
0);
memwrite: buffer STD_LOGIC);
end;
architecture test of top is
component mips
port (clk, reset: in STD_LOGIC;
pc: out STD_LOGIC_VECTOR (31 downto 0);
instr: in STD_LOGIC_VECTOR (31 downto 0);
memwrite: out STD_LOGIC;
aluout, writedata: out STD_LOGIC_VECTOR (31 downto 0);
readdata: in STD_LOGIC_VECTOR (31 downto 0));
end component;
component imem
port (a: in STD_LOGIC_VECTOR (5 downto 0)
rd: out STD_LOGIC_VECTOR (31 downto 0));
end component;
component dmem
port (clk, we: in STD_LOGIC;
a, wd: in STD_LOGIC_VECTOR (31 downto 0);
rd: out STD_LOGIC_VECTOR (31 downto 0));
end component;
signal pc, instr,
readdata: STD_LOGIC_VECTOR (31 downto 0);
begin
— — instantiate processor and memories
mips1: mips port map (clk, reset, pc, instr, memwrite,
dataadr, writedata, readdata);
imem1: imem port map (pc (7 downto 2), instr);
dmem1: dmem port map (clk, memwrite, dataadr, writedata,
readdata);
end;
HDL Example 7.13 MIPS TOP-LEVEL MODULE
module dmem (input clk, we,
input [31:0] a, wd,
output [31:0] rd);
reg [31:0] RAM[63:0];
assign rd  RAM[a[31:2]]; // word aligned
always @ (posedge clk)
if (we)
RAM[a[31:2]]  wd;
endmodule
library IEEE;
use IEEE.STD_LOGIC_1164.all; use STD.TEXTIO.all;
use IEEE.STD_LOGIC_UNSIGNED.all; use IEEE.STD_LOGIC_ARITH.all;
entity dmem is — — data memory
port (clk, we: in STD_LOGIC;
a, wd: in STD_LOGIC_VECTOR (31 downto 0);
rd: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of dmem is
begin
process is
type ramtype is array (63 downto 0) of STD_LOGIC_VECTOR
(31 downto 0);
variable mem: ramtype;
begin
— — read or write memory
loop
if clk'event and clk  '1' then
if (we  '1') then mem (CONV_INTEGER (a(7 downto
2))):  wd;
end if;
end if;
rd  mem (CONV_INTEGER (a (7 downto 2)));
wait on clk, a;
end loop;
end process;
end;
HDL Example 7.14 MIPS DATA MEMORY

7.7 EXCEPTIONS*
Section 6.7.2 introduced exceptions, which cause unplanned changes in
the flow of a program. In this section, we enhance the multicycle
processor to support two types of exceptions: undefined instructions
Verilog
module imem (input [5:0] a,
output [31:0] rd);
reg [31:0] RAM[63:0];
initial
begin
$readmemh ("memfile.dat",RAM);
end
assign rd  RAM[a]; // word aligned
endmodule
VHDL
library IEEE;
use IEEE.STD_LOGIC_1164.all; use STD.TEXTIO.all;
use IEEE.STD_LOGIC_UNSIGNED.all; use IEEE.STD_LOGIC_ARITH.all;
entity imem is — — instruction memory
port (a: in STD_LOGIC_VECTOR (5 downto 0);
rd: out STD_LOGIC_VECTOR (31 downto 0));
end;
architecture behave of imem is
begin
process is
file mem_file: TEXT;
variable L: line;
variable ch: character;
variable index, result: integer;
type ramtype is array (63 downto 0) of STD_LOGIC_VECTOR
(31 downto 0);
variable mem: ramtype;
begin
— — initialize memory from file
for i in 0 to 63 loop — set all contents low
mem (conv_integer(i)) : CONV_STD_LOGIC_VECTOR (0, 32);
end loop;
index : 0;
FILE_OPEN (mem_file, "C:/mips/memfile.dat", READ_MODE);
while not endfile(mem_file) loop
readline (mem_file, L);
result : 0;
for i in 1 to 8 loop
read (L, ch);
if '0'  ch and ch  '9' then
result : result*16  character'pos(ch) 
character'pos('0');
elsif 'a'  ch and ch  'f' then
result : result*16  character'pos(ch) 
character'pos('a')  10;
else report "Format error on line" & integer'image
(index) severity error;
end if;
end loop;
mem (index) : CONV_STD_LOGIC_VECTOR (result, 32);
index : index  1;
end loop;
— — read memory
loop
rd  mem(CONV_INTEGER(a));
wait on a;
end loop;
end process;
end;
HDL Example 7.15 MIPS INSTRUCTION MEMORY

and arithmetic overflow. Supporting exceptions in other microarchitectures follows similar principles.
As described in Section 6.7.2, when an exception takes place, the
processor copies the PC to the EPC register and stores a code in the
Cause register indicating the source of the exception. Exception causes
include 0x28 for undefined instructions and 0x30 for overflow (see Table
6.7). The processor then jumps to the exception handler at memory
address 0x80000180. The exception handler is code that responds to the
exception. It is part of the operating system.
Also as discussed in Section 6.7.2, the exception registers are part of
Coprocessor 0, a portion of the MIPS processor that is used for system
functions. Coprocessor 0 defines up to 32 special-purpose registers,
including Cause and EPC. The exception handler may use the mfc0
(move from coprocessor 0) instruction to copy these special-purpose
registers into a general-purpose register in the register file; the Cause register is Coprocessor 0 register 13, and EPC is register 14.
To handle exceptions, we must add EPC and Cause registers to the
datapath and extend the PCSrc multiplexer to accept the exception handler address, as shown in Figure 7.62. The two new registers have write
enables, EPCWrite and CauseWrite, to store the PC and exception cause
when an exception takes place. The cause is generated by a multiplexer
432 CHAPTER SEVEN Microarchitecture
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc1:0
CLK
ALUControl2:0
ALU
WD
WE
CLK
Adr
0
1 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB1:0 IorD IRWrite PCWrite
PCEn
<<2
25:0 (jump)
31:28
27:0
PCJump
00
01
10
11
0x80000180
Overflow
CLK
EN
EPCWrite
CLK
EN
CauseWrite
0
1
IntCause
0x30
0x28 EPC
Cause
Figure 7.62 Datapath supporting overflow and undefined instruction exceptions

7.7 Exceptions 433
that selects the appropriate code for the exception. The ALU must also
generate an overflow signal, as was discussed in Section 5.2.4.5
To support the mfc0 instruction, we also add a way to select the
Coprocessor 0 registers and write them to the register file, as shown in
Figure 7.63. The mfc0 instruction specifies the Coprocessor 0 register by
Instr15:11; in this diagram, only the Cause and EPC registers are supported. We add another input to the MemtoReg multiplexer to select the
value from Coprocessor 0.
The modified controller is shown in Figure 7.64. The controller
receives the overflow flag from the ALU. It generates three new control
signals: one to write the EPC, a second to write the Cause register, and a
third to select the Cause. It also includes two new states to support the
two exceptions and another state to handle mfc0.
If the controller receives an undefined instruction (one that it does
not know how to handle), it proceeds to S12, saves the PC in EPC,
writes 0x28 to the Cause register, and jumps to the exception handler.
Similarly, if the controller detects arithmetic overflow on an add or
sub instruction, it proceeds to S13, saves the PC in EPC, writes 0x30
5 Strictly speaking, the ALU should assert overflow only for add and sub, not for other
ALU instructions.
SignImm
CLK
A RD
Instr/Data
Memory
A1
A3
WD3
RD2
RD1 WE3
A2
CLK
Sign Extend
Register
File
0
1
0
PC 1 0
1
PC' Instr 25:21
20:16
15:0
20:16 SrcB
15:11
<<2
ALUResult
SrcA
ALUOut
MemWrite RegDst MemtoReg RegWrite ALUSrcA Branch
Zero
PCSrc1:0
CLK
ALUControl2:0
ALU
WD
WE
CLK
Adr
00
01 Data
CLK
CLK
A
B 00
01
10
11
4
CLK
EN EN
ALUSrcB1:0 IorD IRWrite PCWrite
PCEn
<<2
25:0 (jump)
31:28
27:0
PCJump
00
01
10
11
0x8000 0180
CLK
EN
EPCWrite
CLK
EN
CauseWrite
0
1
IntCause
0x30
0x28 EPC
Cause
Overflow
...
01101
01110
... 15:11
10
C0
Figure 7.63 Datapath supporting mfc0

434 CHAPTER SEVEN Microarchitecture
IorD = 0
AluSrcA= 0
ALUSrcB= 01
ALUOp = 00
PCSrc = 00
IRWrite
ALUSrcA= 0
ALUSrcB= 11
ALUOp = 00
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
IorD = 1
RegDst = 1
MemtoReg = 00
RegWrite
IorD = 1
MemWrite
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 10
ALUSrcA= 1
ALUSrcB= 00
ALUOp = 01
PCSrc = 01
Branch
Reset
S0: Fetch
S2: MemAdr
S1: Decode
S3: MemRead
S5: MemWrite
S6: Execute
S7: ALU
Writeback
S8: Branch
Op = LW
or
Op = SW
Op = R-type
Op = BEQ
Op = LW
Op = SW
RegDst = 0
MemtoReg = 01
RegWrite
S4: Mem
Writeback
ALUSrcA= 1
ALUSrcB= 10
ALUOp = 00
RegDst = 0
MemtoReg = 00
RegWrite
Op = ADDI
S9: ADDI
Execute
S10: ADDI
Writeback
PCSrc = 10
PCWrite
Op = J
S11: Jump
Overflow Overflow
S13:
Overflow
PCSrc = 11
PCWrite
IntCause = 0
CauseWrite
EPCWrite
Op = others
 PCSrc = 11
PCWrite
IntCause = 1
CauseWrite
EPCWrite
S12: Undefined
Memtoreg = 10
RegWrite
Op=MFC0
S14: MFC0
PCWrite
RegDst = 0
Figure 7.64 Controller supporting exceptions and mfc0
in the Cause register, and jumps to the exception handler. Note that,
when an exception occurs, the instruction is discarded and the register
file is not written. When a mfc0 instruction is decoded, the processor
goes to S14 and writes the appropriate Coprocessor 0 register to the
main register file.

7.8 ADVANCED MICROARCHITECTURE*
High-performance microprocessors use a wide variety of techniques to
run programs faster. Recall that the time required to run a program is
proportional to the period of the clock and to the number of clock cycles
per instruction (CPI). Thus, to increase performance we would like to
speed up the clock and/or reduce the CPI. This section surveys some
existing speedup techniques. The implementation details become quite
complex, so we will focus on the concepts. Hennessy & Patterson’s
Computer Architecture text is a definitive reference if you want to fully
understand the details.
Every 2 to 3 years, advances in CMOS manufacturing reduce transistor dimensions by 30% in each direction, doubling the number of
transistors that can fit on a chip. A manufacturing process is characterized by its feature size, which indicates the smallest transistor that can
be reliably built. Smaller transistors are faster and generally consume
less power. Thus, even if the microarchitecture does not change, the
clock frequency can increase because all the gates are faster. Moreover,
smaller transistors enable placing more transistors on a chip.
Microarchitects use the additional transistors to build more complicated processors or to put more processors on a chip. Unfortunately,
power consumption increases with the number of transistors and the
speed at which they operate (see Section 1.8). Power consumption is
now an essential concern. Microprocessor designers have a challenging
task juggling the trade-offs among speed, power, and cost for chips
with billions of transistors in some of the most complex systems that
humans have ever built.
7.8.1 Deep Pipelines
Aside from advances in manufacturing, the easiest way to speed up the
clock is to chop the pipeline into more stages. Each stage contains less
logic, so it can run faster. This chapter has considered a classic five-stage
pipeline, but 10 to 20 stages are now commonly used.
The maximum number of pipeline stages is limited by pipeline hazards, sequencing overhead, and cost. Longer pipelines introduce more
dependencies. Some of the dependencies can be solved by forwarding,
but others require stalls, which increase the CPI. The pipeline registers
between each stage have sequencing overhead from their setup time and
clk-to-Q delay (as well as clock skew). This sequencing overhead makes
adding more pipeline stages give diminishing returns. Finally, adding
more stages increases the cost because of the extra pipeline registers and
hardware required to handle hazards.
7.8 Advanced Microarchitecture 435

Example 7.11 DEEP PIPELINES
Consider building a pipelined processor by chopping up the single-cycle processor into N stages (N 	 5). The single-cycle processor has a propagation delay of
900 ps through the combinational logic. The sequencing overhead of a register is
50 ps. Assume that the combinational delay can be arbitrarily divided into any
number of stages and that pipeline hazard logic does not increase the delay. The
five-stage pipeline in Example 7.9 has a CPI of 1.15. Assume that each additional stage increases the CPI by 0.1 because of branch mispredictions and other
pipeline hazards. How many pipeline stages should be used to make the processor execute programs as fast as possible?
Solution: If the 900-ps combinational logic delay is divided into N stages and
each stage also pays 50 ps of sequencing overhead for its pipeline register, the
cycle time is Tc  900/N  50. The CPI is 1.15  0.1(N  5). The time per
instruction, or instruction time, is the product of the cycle time and the CPI.
Figure 7.65 plots the cycle time and instruction time versus the number of stages.
The instruction time has a minimum of 231 ps at N  11 stages. This minimum
is only slightly better than the 250 ps per instruction achieved with a six-stage
pipeline.
In the late 1990s and early 2000s, microprocessors were marketed largely based
on clock frequency (1/Tc). This pushed microprocessors to use very deep
pipelines (20 to 31 stages on the Pentium 4) to maximize the clock frequency,
even if the benefits for overall performance were questionable. Power is proportional to clock frequency and also increases with the number of pipeline registers, so now that power consumption is so important, pipeline depths are
decreasing.
436 CHAPTER SEVEN Microarchitecture
0
50
100
150
200
250
300
5 6 7 8 9 10 11 12 13 14 15
Tc Instruction Time
Number of pipeline stages
Time (ps)
Figure 7.65 Cycle time and
instruction time versus the
number of pipeline stages

7.8.2 Branch Prediction
An ideal pipelined processor would have a CPI of 1. The branch misprediction penalty is a major reason for increased CPI. As pipelines get
deeper, branches are resolved later in the pipeline. Thus, the branch misprediction penalty gets larger, because all the instructions issued after the
mispredicted branch must be flushed. To address this problem, most
pipelined processors use a branch predictor to guess whether the branch
should be taken. Recall that our pipeline from Section 7.5.3 simply predicted that branches are never taken.
Some branches occur when a program reaches the end of a loop
(e.g., a for or while statement) and branches back to repeat the loop.
Loops tend to be executed many times, so these backward branches are
usually taken. The simplest form of branch prediction checks the direction of the branch and predicts that backward branches should be taken.
This is called static branch prediction, because it does not depend on the
history of the program.
Forward branches are difficult to predict without knowing more
about the specific program. Therefore, most processors use dynamic
branch predictors, which use the history of program execution to guess
whether a branch should be taken. Dynamic branch predictors maintain
a table of the last several hundred (or thousand) branch instructions that
the processor has executed. The table, sometimes called a branch target
buffer, includes the destination of the branch and a history of whether
the branch was taken.
To see the operation of dynamic branch predictors, consider the following loop code from Code Example 6.20. The loop repeats 10 times,
and the beq out of the loop is taken only on the last time.
add $s1, $0, $0 # sum  0
add $s0, $0, $0 # i  0
addi $t0, $0, 10 # $t0  10
for:
beq $s0, $t0, done # if i  10, branch to done
add $s1, $s1, $s0 # sum  sum  i
addi $s0, $s0, 1 # increment i
j for
done:
A one-bit dynamic branch predictor remembers whether the branch
was taken the last time and predicts that it will do the same thing the
next time. While the loop is repeating, it remembers that the beq was
not taken last time and predicts that it should not be taken next time.
This is a correct prediction until the last branch of the loop, when the
branch does get taken. Unfortunately, if the loop is run again, the
branch predictor remembers that the last branch was taken. Therefore,
7.8 Advanced Microarchitecture 437

it incorrectly predicts that the branch should be taken when the loop is
first run again. In summary, a 1-bit branch predictor mispredicts the
first and last branches of a loop.
A 2-bit dynamic branch predictor solves this problem by having four
states: strongly taken, weakly taken, weakly not taken, and strongly not
taken, as shown in Figure 7.66. When the loop is repeating, it enters the
“strongly not taken” state and predicts that the branch should not be
taken next time. This is correct until the last branch of the loop, which is
taken and moves the predictor to the “weakly not taken” state. When
the loop is first run again, the branch predictor correctly predicts that
the branch should not be taken and reenters the “strongly not taken”
state. In summary, a 2-bit branch predictor mispredicts only the last
branch of a loop.
As one can imagine, branch predictors may be used to track even
more history of the program to increase the accuracy of predictions.
Good branch predictors achieve better than 90% accuracy on typical
programs.
The branch predictor operates in the Fetch stage of the pipeline so
that it can determine which instruction to execute on the next cycle.
When it predicts that the branch should be taken, the processor fetches
the next instruction from the branch destination stored in the branch
target buffer. By keeping track of both branch and jump destinations in
the branch target buffer, the processor can also avoid flushing the
pipeline during jump instructions.
7.8.3 Superscalar Processor
A superscalar processor contains multiple copies of the datapath hardware to execute multiple instructions simultaneously. Figure 7.67 shows
a block diagram of a two-way superscalar processor that fetches and
executes two instructions per cycle. The datapath fetches two instructions at a time from the instruction memory. It has a six-ported register
file to read four source operands and write two results back in each
cycle. It also contains two ALUs and a two-ported data memory to
execute the two instructions at the same time.
438 CHAPTER SEVEN Microarchitecture
A scalar processor acts on one
piece of data at a time. A vector processor acts on several
pieces of data with a single
instruction. A superscalar
processor issues several
instructions at a time, each of
which operates on one piece
of data.
Our MIPS pipelined
processor is a scalar processor. Vector processors were
popular for supercomputers
in the 1980s and 1990s
because they efficiently handled the long vectors of data
common in scientific computations. Modern highperformance microprocessors
are superscalar, because
issuing several independent
instructions is more flexible
than processing vectors.
However, modern processors also include hardware to
handle short vectors of data
that are common in multimedia and graphics applications. These are called single
instruction multiple data
(SIMD) units.
strongly
taken
predict
taken
weakly
taken
predict
taken
weakly
not taken
predict
not taken
strongly
not taken
predict
not taken
taken taken taken
taken taken taken
taken
taken
Figure 7.66 2-bit branch predictor state transition diagram

Figure 7.68 shows a pipeline diagram illustrating the two-way
superscalar processor executing two instructions on each cycle. For this
program, the processor has a CPI of 0.5. Designers commonly refer to
the reciprocal of the CPI as the instructions per cycle, or IPC. This
processor has an IPC of 2 on this program.
Executing many instructions simultaneously is difficult because of
dependencies. For example, Figure 7.69 shows a pipeline diagram running a program with data dependencies. The dependencies in the code
are shown in blue. The add instruction is dependent on $t0, which is
produced by the lw instruction, so it cannot be issued at the same time
as lw. Indeed, the add instruction stalls for yet another cycle so that lw
can forward $t0 to add in cycle 5. The other dependencies (between
7.8 Advanced Microarchitecture 439
CLK CLK CLK CLK
A RD A1
A2
A3 RD1
WD3
WD6
A4
A5
A6
RD4
RD2
RD5
Instruction
Memory
Register
File Data
Memory
PC
CLK
A1
A2
WD1
ALUs
WD2
RD1
RD2
Figure 7.67 Superscalar datapath
Time (cycles)
12345678
RF
40
$s0
RF
$t0 +
DM IM
lw
add
lw $t0, 40($s0)
add $t1, $s1, $s2
sub $t2, $s1, $s3
and $t3, $s3, $s4
or $t4, $s1, $s5
sw $s5, 80($s0)
$t1 $s2
$s1
+
RF
$s3
$s1
RF
$t2 -
DM IM
sub
and $t3 $s4
$s3
&
RF
+
$s5
$s1
RF
$t4
|
DM IM
or
sw 80
$s0 $s5
Figure 7.68 Abstract view of a superscalar pipeline in operation

440 CHAPTER SEVEN Microarchitecture
Stall
Time (cycles)
12345678
RF
40
$s0
RF
$t0 + DM IM
lw lw $t0, 40($s0)
add $t1, $t0, $s1
sub $t0, $s2, $s3
and $t2, $s4, $t0
sw $s7, 80($t3)
$s1
$t0 add
RF
$s1
$t0
RF
$t1 +
DM
RF
$t0
$s4
RF
$t2 &
DM IM
and
IM or
and
sub
$s6 |
$s5 $t3
RF
80
$t3
RF
+
DM
sw
IM
$s7
9
$s3
$s2
$s3
$s2
- $t0
or $t3, $s5, $s6
IM
or
RF
sub and and based on $t0, and between or and sw based on $t3) are
handled by forwarding results produced in one cycle to be consumed in
the next. This program, also given below, requires five cycles to issue
six instructions, for an IPC of 1.17.
lw $t0, 40($s0)
add $t1, $t0, $s1
sub $t0, $s2, $s3
and $t2, $s4, $t0
or $t3, $s5, $s6
sw $s7, 80($t3)
Recall that parallelism comes in temporal and spatial forms.
Pipelining is a case of temporal parallelism. Multiple execution units is a
case of spatial parallelism. Superscalar processors exploit both forms of
parallelism to squeeze out performance far exceeding that of our singlecycle and multicycle processors.
Commercial processors may be three-, four-, or even six-way superscalar. They must handle control hazards such as branches as well as
data hazards. Unfortunately, real programs have many dependencies, so
wide superscalar processors rarely fully utilize all of the execution units.
Moreover, the large number of execution units and complex forwarding
networks consume vast amounts of circuitry and power.
Figure 7.69 Program with data dependencies

7.8.4 Out-of-Order Processor
To cope with the problem of dependencies, an out-of-order processor looks
ahead across many instructions to issue, or begin executing, independent
instructions as rapidly as possible. The instructions can be issued in a different order than that written by the programmer, as long as dependencies
are honored so that the program produces the intended result.
Consider running the same program from Figure 7.69 on a two-way
superscalar out-of-order processor. The processor can issue up to two
instructions per cycle from anywhere in the program, as long as dependencies are observed. Figure 7.70 shows the data dependencies and the
operation of the processor. The classifications of dependencies as RAW
and WAR will be discussed shortly. The constraints on issuing instructions are described below.
 Cycle 1
– The lw instruction issues.
– The add, sub, and and instructions are dependent on lw by way
of $t0, so they cannot issue yet. However, the or instruction is
independent, so it also issues.
7.8 Advanced Microarchitecture 441
Time (cycles)
12345678
RF
40
$s0
RF
$t0 +
DM IM
lw lw $t0, 40($s0)
add $t1, $t0, $s1
sub $t0, $s2, $s3
and $t2, $s4, $t0
sw $s7, 80($t3)
or $s6 |
$s5 $t3
RF
80
$t3
RF
+
DM
sw $s7
or $t3, $s5, $s6
IM
RF
$s1
$t0
RF
$t1 +
DM IM
add
sub $s3 -
$s2 $t0
Two-cycle latency
between lW
and use of $t0
RAW
WAR
RAW
RF
$t0
$s4
RF
&
DM
and
IM
$t2
RAW
Figure 7.70 Out-of-order execution of a program with dependencies
C
 Cycle 2
– Remember that there is a two-cycle latency between when a lw
instruction issues and when a dependent instruction can use its
result, so add cannot issue yet because of the $t0 dependence.
sub writes $t0, so it cannot issue before add, lest add receive the
wrong value of $t0. and is dependent on sub.
– Only the sw instruction issues.
 Cycle 3
– On cycle 3, $t0 is available, so add issues. sub issues simultaneously, because it will not write $t0 until after add consumes
$t0.
 Cycle 4
– The and instruction issues. $t0 is forwarded from sub to and.
The out-of-order processor issues the six instructions in four cycles, for
an IPC of 1.5.
The dependence of add on lw by way of $t0 is a read after write
(RAW) hazard. add must not read $t0 until after lw has written it. This
is the type of dependency we are accustomed to handling in the pipelined
processor. It inherently limits the speed at which the program can run,
even if infinitely many execution units are available. Similarly, the
dependence of sw on or by way of $t3 and of and on sub by way of
$t0 are RAW dependencies.
The dependence between sub and add by way of $t0 is called a write
after read (WAR) hazard or an antidependence. sub must not write $t0
before add reads $t0, so that add receives the correct value according to
the original order of the program. WAR hazards could not occur in the
simple MIPS pipeline, but they may happen in an out-of-order processor
if the dependent instruction (in this case, sub) is moved too early.
A WAR hazard is not essential to the operation of the program. It is
merely an artifact of the programmer’s choice to use the same register
for two unrelated instructions. If the sub instruction had written $t4
instead of $t0, the dependency would disappear and sub could be issued
before add. The MIPS architecture only has 32 registers, so sometimes
the programmer is forced to reuse a register and introduce a hazard just
because all the other registers are in use.
A third type of hazard, not shown in the program, is called write
after write (WAW) or an output dependence. A WAW hazard occurs if
an instruction attempts to write a register after a subsequent instruction
has already written it. The hazard would result in the wrong value being
442 CHAPTER SEVEN Microarchitecture
Ch
written to the register. For example, in the following program, add and
sub both write $t0. The final value in $t0 should come from sub
according to the order of the program. If an out-of-order processor
attempted to execute sub first, the WAW hazard would occur.
add $t0, $s1, $s2
sub $t0, $s3, $s4
WAW hazards are not essential either; again, they are artifacts
caused by the programmer’s using the same register for two unrelated
instructions. If the sub instruction were issued first, the program could
eliminate the WAW hazard by discarding the result of the add instead of
writing it to $t0. This is called squashing the add.
6
Out-of-order processors use a table to keep track of instructions
waiting to issue. The table, sometimes called a scoreboard, contains
information about the dependencies. The size of the table determines
how many instructions can be considered for issue. On each cycle, the
processor examines the table and issues as many instructions as it can,
limited by the dependencies and by the number of execution units (e.g.,
ALUs, memory ports) that are available.
The instruction level parallelism (ILP) is the number of instructions
that can be executed simultaneously for a particular program and
microarchitecture. Theoretical studies have shown that the ILP can be
quite large for out-of-order microarchitectures with perfect branch predictors and enormous numbers of execution units. However, practical
processors seldom achieve an ILP greater than 2 or 3, even with six-way
superscalar datapaths with out-of-order execution.
7.8.5 Register Renaming
Out-of-order processors use a technique called register renaming to eliminate WAR hazards. Register renaming adds some nonarchitectural
renaming registers to the processor. For example, a MIPS processor
might add 20 renaming registers, called $r0–$r19. The programmer
cannot use these registers directly, because they are not part of the architecture. However, the processor is free to use them to eliminate hazards.
For example, in the previous section, a WAR hazard occurred
between the sub and add instructions based on reusing $t0. The out-oforder processor could rename $t0 to $r0 for the sub instruction. Then
7.8 Advanced Microarchitecture 443
6 You might wonder why the add needs to be issued at all. The reason is that out-of-order
processors must guarantee that all of the same exceptions occur that would have occurred
if the program had been executed in its original order. The add potentially may produce an
overflow exception, so it must be issued to check for the exception, even though the result
can be discarded.

sub could be executed sooner, because $r0 has no dependency on the
add instruction. The processor keeps a table of which registers were
renamed so that it can consistently rename registers in subsequent
dependent instructions. In this example, $t0 must also be renamed to
$r0 in the and instruction, because it refers to the result of sub.
Figure 7.71 shows the same program from Figure 7.70 executing on
an out-of-order processor with register renaming. $t0 is renamed to $r0
in sub and and to eliminate the WAR hazard. The constraints on issuing
instructions are described below.
 Cycle 1
– The lw instruction issues.
– The add instruction is dependent on lw by way of $t0, so it cannot issue yet. However, the sub instruction is independent now
that its destination has been renamed to $r0, so sub also issues.
 Cycle 2
– Remember that there is a two-cycle latency between when a lw
issues and when a dependent instruction can use its result, so
add cannot issue yet because of the $t0 dependence.
– The and instruction is dependent on sub, so it can issue. $r0 is
forwarded from sub to and.
– The or instruction is independent, so it also issues.
444 CHAPTER SEVEN Microarchitecture
Time (cycles)
1234567
RF
40
$s0
RF
$t0 +
DM IM
lw lw $t0, 40($s0)
add $t1, $t0, $s1
sub $r0, $s2, $s3
and $t2, $s4, $r0
sw $s7, 80($t3)
sub $s3 -
$s2 $r0
RF
$r0
$s4
RF
&
DM
and
$s7
or $t3, $s5, $s6
IM
RF
$s1
$t0
RF
$t1 +
DM IM
add
sw 80 +
$t3
RAW
$s6
$s5
| or
RAW
RAW
$t2
$t3
Figure 7.71 Out-of-order execution of a program using register renaming
Ch
 Cycle 3
– On cycle 3, $t0 is available, so add issues. $t3 is also available,
so sw issues.
The out-of-order processor with register renaming issues the six
instructions in three cycles, for an IPC of 2.
7.8.6 Single Instruction Multiple Data
The term SIMD (pronounced “sim-dee”) stands for single instruction
multiple data, in which a single instruction acts on multiple pieces of
data in parallel. A common application of SIMD is to perform many
short arithmetic operations at once, especially for graphics processing.
This is also called packed arithmetic.
For example, a 32-bit microprocessor might pack four 8-bit data
elements into one 32-bit word. Packed add and subtract instructions
operate on all four data elements within the word in parallel.
Figure 7.72 shows a packed 8-bit addition summing four pairs of 8-bit
numbers to produce four results. The word could also be divided
into two 16-bit elements. Performing packed arithmetic requires
modifying the ALU to eliminate carries between the smaller data
elements. For example, a carry out of a0  b0 should not affect the
result of a1  b1.
Short data elements often appear in graphics processing. For example, a pixel in a digital photo may use 8 bits to store each of the red,
green, and blue color components. Using an entire 32-bit word to
process one of these components wastes the upper 24 bits. When the
components from four adjacent pixels are packed into a 32-bit word, the
processing can be performed four times faster.
SIMD instructions are even more helpful for 64-bit architectures,
which can pack eight 8-bit elements, four 16-bit elements, or two 32-bit
elements into a single 64-bit word. SIMD instructions are also used for
floating-point computations; for example, four 32-bit single-precision
floating-point values can be packed into a single 128-bit word.
7.8 Advanced Microarchitecture 445
padd8 $s2, $s0, $s1
a0
32 24 23 16 15 78 0
$s0
Bit position
a3 a2 a1
b3 b2 b1 b0 $s1
a3 + b3 a2 + b2 a1 + b1 a0 + b0 $s2
+
Figure 7.72 Packed arithmetic:
four simultaneous 8-bit
additions

7.8.7 Multithreading
Because the ILP of real programs tends to be fairly low, adding more
execution units to a superscalar or out-of-order processor gives diminishing returns. Another problem, discussed in Chapter 8, is that memory
is much slower than the processor. Most loads and stores access a
smaller and faster memory, called a cache. However, when the instructions or data are not available in the cache, the processor may stall for
100 or more cycles while retrieving the information from the main memory. Multithreading is a technique that helps keep a processor with many
execution units busy even if the ILP of a program is low or the program
is stalled waiting for memory.
To explain multithreading, we need to define a few new terms. A program running on a computer is called a process. Computers can run multiple processes simultaneously; for example, you can play music on a PC
while surfing the web and running a virus checker. Each process consists
of one or more threads that also run simultaneously. For example, a word
processor may have one thread handling the user typing, a second thread
spell-checking the document while the user works, and a third thread
printing the document. In this way, the user does not have to wait, for
example, for a document to finish printing before being able to type again.
In a conventional processor, the threads only give the illusion of running simultaneously. The threads actually take turns being executed on the
processor under control of the OS. When one thread’s turn ends, the OS
saves its architectural state, loads the architectural state of the next thread,
and starts executing that next thread. This procedure is called context
switching. As long as the processor switches through all the threads fast
enough, the user perceives all of the threads as running at the same time.
A multithreaded processor contains more than one copy of its architectural state, so that more than one thread can be active at a time. For
example, if we extended a MIPS processor to have four program counters and 128 registers, four threads could be available at one time. If one
thread stalls while waiting for data from main memory, the processor
could context switch to another thread without any delay, because the
program counter and registers are already available. Moreover, if one
thread lacks sufficient parallelism to keep all the execution units busy,
another thread could issue instructions to the idle units.
Multithreading does not improve the performance of an individual
thread, because it does not increase the ILP. However, it does improve
the overall throughput of the processor, because multiple threads can use
processor resources that would have been idle when executing a single
thread. Multithreading is also relatively inexpensive to implement,
because it replicates only the PC and register file, not the execution units
and memories.
446 CHAPTER SEVEN Microarchitecture

7.8.8 Multiprocessors
A multiprocessor system consists of multiple processors and a method
for communication between the processors. A common form of multiprocessing in computer systems is symmetric multiprocessing (SMP), in
which two or more identical processors share a single main memory.
The multiple processors may be separate chips or multiple cores on
the same chip. Modern processors have enormous numbers of transistors
available. Using them to increase the pipeline depth or to add more execution units to a superscalar processor gives little performance benefit
and is wasteful of power. Around the year 2005, computer architects
made a major shift to build multiple copies of the processor on the same
chip; these copies are called cores.
Multiprocessors can be used to run more threads simultaneously or
to run a particular thread faster. Running more threads simultaneously
is easy; the threads are simply divided up among the processors.
Unfortunately typical PC users need to run only a small number of
threads at any given time. Running a particular thread faster is much
more challenging. The programmer must divide the thread into pieces
to perform on each processor. This becomes tricky when the processors
need to communicate with each other. One of the major challenges for
computer designers and programmers is to effectively use large numbers
of processor cores.
Other forms of multiprocessing include asymmetric multiprocessing
and clusters. Asymmetric multiprocessors use separate specialized microprocessors for separate tasks. For example, a cell phone contains a digital signal processor (DSP) with specialized instructions to decipher the
wireless data in real time and a separate conventional processor to interact with the user, manage the phone book, and play games. In clustered
multiprocessing, each processor has its own local memory system.
Clustering can also refer to a group of PCs connected together on the
network running software to jointly solve a large problem.
7.9 REAL-WORLD PERSPECTIVE: IA-32 MICROARCHITECTURE*
Section 6.8 introduced the IA-32 architecture used in almost all PCs.
This section tracks the evolution of IA-32 processors through progressively faster and more complicated microarchitectures. The same
principles we have applied to the MIPS microarchitectures are used in
IA-32.
Intel invented the first single-chip microprocessor, the 4-bit 4004,
in 1971 as a flexible controller for a line of calculators. It contained
2300 transistors manufactured on a 12-mm2 sliver of silicon in a
process with a 10-
m feature size and operated at 750 KHz. A photograph of the chip taken under a microscope is shown in Figure 7.73.
7.9 Real-World Perspective: IA-32 Microarchitecture 447
Scientists searching for signs
of extraterrestrial intelligence
use the world’s largest clustered multiprocessors to analyze radio telescope data for
patterns that might be signs of
life in other solar systems.
The cluster consists of personal computers owned by
more than 3.8 million volunteers around the world.
When a computer in the
cluster is idle, it fetches
a piece of the data from a
centralized server, analyzes
the data, and sends the
results back to the server.
You can volunteer your
computer’s idle time
for the cluster by visiting
setiathome.berkeley.edu.

In places, columns of four similar-looking structures are visible, as one
would expect in a 4-bit microprocessor. Around the periphery are
bond wires, which are used to connect the chip to its package and the
circuit board.
The 4004 inspired the 8-bit 8008, then the 8080, which eventually
evolved into the 16-bit 8086 in 1978 and the 80286 in 1982. In 1985,
Intel introduced the 80386, which extended the 8086 architecture to
32 bits and defined the IA-32 architecture. Table 7.7 summarizes
major Intel IA-32 microprocessors. In the 35 years since the 4004,
transistor feature size has shrunk 160-fold, the number of transistors
448 CHAPTER SEVEN Microarchitecture
Figure 7.73 4004
microprocessor chip
Table 7.7 Evolution of Intel IA-32 microprocessors
Processor Year Feature Size (
m) Transistors Frequency (MHz) Microarchitecture
80386 1985 1.5–1.0 275k 16–25 multicycle
80486 1989 1.0–0.6 1.2M 25–100 pipelined
Pentium 1993 0.8–0.35 3.2–4.5M 60–300 superscalar
Pentium II 1997 0.35–0.25 7.5M 233–450 out of order
Pentium III 1999 0.25–0.18 9.5M–28M 450–1400 out of order
Pentium 4 2001 0.18–0.09 42–178M 1400–3730 out of order
Pentium M 2003 0.13–0.09 77–140M 900–2130 out of order
Core Duo 2005 0.065 152M 1500–2160 dual core

on a chip has increased by five orders of magnitude, and the operating
frequency has increased by almost four orders of magnitude. No other
field of engineering has made such astonishing progress in such a
short time.
The 80386 is a multicycle processor. The major components are
labeled on the chip photograph in Figure 7.74. The 32-bit datapath is
clearly visible on the left. Each of the columns processes one bit of data.
Some of the control signals are generated using a microcode PLA that
steps through the various states of the control FSM. The memory management unit in the upper right controls access to the external memory.
The 80486, shown in Figure 7.75, dramatically improved performance using pipelining. The datapath is again clearly visible, along with
the control logic and microcode PLA. The 80486 added an on-chip
floating-point unit; previous Intel processors either sent floating-point
instructions to a separate coprocessor or emulated them in software. The
80486 was too fast for external memory to keep up, so it incorporated
an 8-KB cache onto the chip to hold the most commonly used instructions and data. Chapter 8 describes caches in more detail and revisits the
cache systems on Intel IA-32 processors.
7.9 Real-World Perspective: IA-32 Microarchitecture 449
Microcode
PLA
32-bit
Datapath
Memory
Management
Unit
Controller
Figure 7.74 80386
microprocessor chip

The Pentium processor, shown in Figure 7.76, is a superscalar
processor capable of executing two instructions simultaneously. Intel
switched to the name Pentium instead of 80586 because AMD was
becoming a serious competitor selling interchangeable 80486 chips, and
part numbers cannot be trademarked. The Pentium uses separate
instruction and data caches. It also uses a branch predictor to reduce the
performance penalty for branches.
The Pentium Pro, Pentium II, and Pentium III processors all share a
common out-of-order microarchitecture, code named P6. The complex
IA-32 instructions are broken down into one or more micro-ops similar
450 CHAPTER SEVEN Microarchitecture
32-bit
Data path
Controller
8 KB
Cache
Floating
Point
Unit
Microcode
PLA
Figure 7.75 80486
microprocessor chip

7.9 Real-World Perspective: IA-32 Microarchitecture 451
to MIPS instructions. The micro-ops are then executed on a fast out-oforder execution core with an 11-stage pipeline. Figure 7.77 shows the
Pentium III. The 32-bit datapath is called the Integer Execution Unit
(IEU). The floating-point datapath is called the Floating Point Unit
8 KB
Data
Cache Floating
Point
Unit
8 KB
Instruction
Cache
Multiprocessor Logic
Instruction
Fetch & Decode
Complex
Instruction
Support Bus Interface Unit 32-bit Datapath Superscalar Controller
Branch
Prediction
Figure 7.76 Pentium
microprocessor chip
Instruction Fetch
&
16 KB Cache
16 KB
Data Cache
Data TLB
Branch
Target
Buffer
FPU
IEU
SIMD
Register
Renaming
Microcode
PLA
256 KB
Level 2 Cache
Out of
Order
Issue
Logic
Bus Logic
Figure 7.77 Pentium III
microprocessor chip

452 CHAPTER SEVEN Microarchitecture
IEU
8 KB
Data Cache
Trace
Cache
256 KB
Level 2
Cache
FPU
&
SIMD
Bus
Logic
Figure 7.78 Pentium 4
microprocessor chip
(FPU). The processor also has a SIMD unit to perform packed operations on short integer and floating-point data. A larger portion of the
chip is dedicated to issuing instructions out-of-order than to actually
executing the instructions. The instruction and data caches have grown
to 16 KB each. The Pentium III also has a larger but slower 256-KB
second-level cache on the same chip.
By the late 1990s, processors were marketed largely on clock speed.
The Pentium 4 is another out-of-order processor with a very deep
pipeline to achieve extremely high clock frequencies. It started with 20
stages, and later versions adopted 31 stages to achieve frequencies
greater than 3 GHz. The chip, shown in Figure 7.78, packs in 42 to 178
million transistors (depending on the cache size), so even the major execution units are difficult to see on the photograph. Decoding three IA-32
instructions per cycle is impossible at such high clock frequencies
because the instruction encodings are so complex and irregular. Instead,
the processor predecodes the instructions into simpler micro-ops, then
stores the micro-ops in a memory called a trace cache. Later versions of
the Pentium 4 also perform multithreading to increase the throughput of
multiple threads.
The Pentium 4’s reliance on deep pipelines and high clock speed
led to extremely high power consumption, sometimes more than 100 W.
This is unacceptable in laptops and makes cooling of desktops expensive.

Intel discovered that the older P6 architecture could achieve comparable
performance at much lower clock speed and power. The Pentium M uses
an enhanced version of the P6 out-of-order microarchitecture with 32-KB
instruction and data caches and a 1- to 2-MB second-level cache. The
Core Duo is a multicore processor based on two Pentium M cores connected to a shared 2-MB second-level cache. The individual functional
units in Figure 7.79 are difficult to see, but the two cores and the large
cache are clearly visible.
7.10 SUMMARY
This chapter has described three ways to build MIPS processors, each
with different performance and cost trade-offs. We find this topic
almost magical: how can such a seemingly complicated device as a
microprocessor actually be simple enough to fit in a half-page
schematic? Moreover, the inner workings, so mysterious to the uninitiated, are actually reasonably straightforward.
The MIPS microarchitectures have drawn together almost every
topic covered in the text so far. Piecing together the microarchitecture
puzzle illustrates the principles introduced in previous chapters, including the design of combinational and sequential circuits, covered in
Chapters 2 and 3; the application of many of the building blocks
described in Chapter 5; and the implementation of the MIPS architecture, introduced in Chapter 6. The MIPS microarchitectures can be
described in a few pages of HDL, using the techniques from Chapter 4.
Building the microarchitectures has also heavily used our techniques for managing complexity. The microarchitectural abstraction
forms the link between the logic and architecture abstractions, forming
7.10 Summary 453
Core 1 Core 2
2 MB
Shared
Level 2
Cache
Figure 7.79 Core Duo
microprocessor chip

the crux of this book on digital design and computer architecture. We
also use the abstractions of block diagrams and HDL to succinctly
describe the arrangement of components. The microarchitectures
exploit regularity and modularity, reusing a library of common building
blocks such as ALUs, memories, multiplexers, and registers. Hierarchy
is used in numerous ways. The microarchitectures are partitioned into
the datapath and control units. Each of these units is built from logic
blocks, which can be built from gates, which in turn can be built from
transistors using the techniques developed in the first five chapters.
This chapter has compared single-cycle, multicycle, and pipelined
microarchitectures for the MIPS processor. All three microarchitectures
implement the same subset of the MIPS instruction set and have the
same architectural state. The single-cycle processor is the most straightforward and has a CPI of 1.
The multicycle processor uses a variable number of shorter steps to
execute instructions. It thus can reuse the ALU, rather than requiring
several adders. However, it does require several nonarchitectural registers to store results between steps. The multicycle design in principle
could be faster, because not all instructions must be equally long. In
practice, it is generally slower, because it is limited by the slowest steps
and by the sequencing overhead in each step.
The pipelined processor divides the single-cycle processor into five
relatively fast pipeline stages. It adds pipeline registers between the
stages to separate the five instructions that are simultaneously executing.
It nominally has a CPI of 1, but hazards force stalls or flushes that
increase the CPI slightly. Hazard resolution also costs some extra hardware and design complexity. The clock period ideally could be five times
shorter than that of the single-cycle processor. In practice, it is not that
short, because it is limited by the slowest stage and by the sequencing
overhead in each stage. Nevertheless, pipelining provides substantial performance benefits. All modern high-performance microprocessors use
pipelining today.
Although the microarchitectures in this chapter implement only a
subset of the MIPS architecture, we have seen that supporting more
instructions involves straightforward enhancements of the datapath and
controller. Supporting exceptions also requires simple modifications.
A major limitation of this chapter is that we have assumed an ideal
memory system that is fast and large enough to store the entire program
and data. In reality, large fast memories are prohibitively expensive. The
next chapter shows how to get most of the benefits of a large fast memory with a small fast memory that holds the most commonly used information and one or more larger but slower memories that hold the rest of
the information.
454 CHAPTER SEVEN Microarchitecture

Exercises 455
Exercises
Exercise 7.1 Suppose that one of the following control signals in the single-cycle
MIPS processor has a stuck-at-0 fault, meaning that the signal is always 0,
regardless of its intended value. What instructions would malfunction? Why?
(a) RegWrite
(b) ALUOp1
(c) MemWrite
Exercise 7.2 Repeat Exercise 7.1, assuming that the signal has a stuck-at-1 fault.
Exercise 7.3 Modify the single-cycle MIPS processor to implement one of the
following instructions. See Appendix B for a definition of the instructions. Mark
up a copy of Figure 7.11 to indicate the changes to the datapath. Name any new
control signals. Mark up a copy of Table 7.8 to show the changes to the main
decoder. Describe any other changes that are required.
(a) sll
(b) lui
(c) slti
(d) blez
(e) jal
(f) lh
Exercise 7.4 Many processor architectures have a load with postincrement
instruction, which updates the index register to point to the next memory word
after completing the load. lwinc $rt, imm($rs) is equivalent to the following
two instructions:
lw $rt, imm($rs)
addi $rs, $rs, 4
Table 7.8 Main decoder truth table to mark up with changes
Instruction Opcode RegWrite RegDst ALUSrc Branch MemWrite MemtoReg ALUOp
R-type 000000 1 1 0 0 0 0 10
lw 100011 1 0 1 0 0 1 00
sw 101011 0 X 1 0 1 X 00
beq 000100 0 X 0 1 0 X 01

Repeat Exercise 7.3 for the lwinc instruction. Is it possible to add the
instruction without modifying the register file?
Exercise 7.5 Add a single-precision floating-point unit to the single-cycle MIPS
processor to handle add.s, sub.s, and mul.s. Assume that you have single-precision floating-point adder and multiplier units available. Explain what changes
must be made to the datapath and the controller.
Exercise 7.6 Your friend is a crack circuit designer. She has offered to redesign
one of the units in the single-cycle MIPS processor to have half the delay. Using
the delays from Table 7.6, which unit should she work on to obtain the greatest
speedup of the overall processor, and what would the cycle time of the improved
machine be?
Exercise 7.7 Consider the delays given in Table 7.6. Ben Bitdiddle builds a prefix
adder that reduces the ALU delay by 20 ps. If the other element delays stay the
same, find the new cycle time of the single-cycle MIPS processor and determine
how long it takes to execute a benchmark with 100 billion instructions.
Exercise 7.8 Suppose one of the following control signals in the multicycle MIPS
processor has a stuck-at-0 fault, meaning that the signal is always 0, regardless
of its intended value. What instructions would malfunction? Why?
(a) MemtoReg
(b) ALUOp0
(c) PCSrc
Exercise 7.9 Repeat Exercise 7.8, assuming that the signal has a stuck-at-1 fault.
Exercise 7.10 Modify the HDL code for the single-cycle MIPS processor, given in
Section 7.6.1, to handle one of the new instructions from Exercise 7.3. Enhance
the testbench, given in Section 7.6.3 to test the new instruction.
Exercise 7.11 Modify the multicycle MIPS processor to implement one of the following instructions. See Appendix B for a definition of the instructions. Mark up
a copy of Figure 7.27 to indicate the changes to the datapath. Name any new
control signals. Mark up a copy of Figure 7.39 to show the changes to the controller FSM. Describe any other changes that are required.
(a) srlv
(b) ori
(c) xori
(d) jr
(e) bne
(f) lbu
456 CHAPTER SEVEN Microarchitecture

Exercise 7.12 Repeat Exercise 7.4 for the multicycle MIPS processor. Show the
changes to the multicycle datapath and control FSM. Is it possible to add the
instruction without modifying the register file?
Exercise 7.13 Repeat Exercise 7.5 for the multicycle MIPS processor.
Exercise 7.14 Suppose that the floating-point adder and multiplier from
Exercise 7.13 each take two cycles to operate. In other words, the inputs are
applied at the beginning of one cycle, and the output is available in the second
cycle. How does your answer to Exercise 7.13 change?
Exercise 7.15 Your friend, the crack circuit designer, has offered to redesign
one of the units in the multicycle MIPS processor to be much faster. Using the
delays from Table 7.6, which unit should she work on to obtain the greatest
speedup of the overall processor? How fast should it be? (Making it faster than
necessary is a waste of your friend’s effort.) What is the cycle time of the
improved processor?
Exercise 7.16 Repeat Exercise 7.7 for the multicycle processor.
Exercise 7.17 Suppose the multicycle MIPS processor has the component delays
given in Table 7.6. Alyssa P. Hacker designs a new register file that has 40% less
power but twice as much delay. Should she switch to the slower but lower power
register file for her multicycle processor design?
Exercise 7.18 Goliath Corp claims to have a patent on a three-ported register
file. Rather than fighting Goliath in court, Ben Bitdiddle designs a new register
file that has only a single read/write port (like the combined instruction and
data memory). Redesign the MIPS multicycle datapath and controller to use his
new register file.
Exercise 7.19 What is the CPI of the redesigned multicycle MIPS processor from
Exercise 7.18? Use the instruction mix from Example 7.7.
Exercise 7.20 How many cycles are required to run the following program on
the multicycle MIPS processor? What is the CPI of this program?
addi $s0, $0, 5 # sum  5
while:
beq $s0, $0, done# if result  0, execute the while block
addi $s0, $s0, 1 # while block: result  result  1
j while
done:
Exercises 457

Exercise 7.21 Repeat Exercise 7.20 for the following program.
add $s0, $0, $0 # i  0
add $s1, $0, $0 # sum  0
addi $t0, $0, 10 # $t0  10
loop:
slt $t1, $s0, $t0 # if (i  10), $t1  1, else $t1  0
beq $t1, $0, done # if $t1  0 (i  10), branch to done
add $s1, $s1, $s0 # sum  sum  i
addi $s0, $s0, 1 # increment i
j loop
done:
Exercise 7.22 Write HDL code for the multicycle MIPS processor. The processor
should be compatible with the following top-level module. The mem module is
used to hold both instructions and data. Test your processor using the testbench
from Section 7.6.3.
module top(input clk, reset,
output [31:0] writedata, adr,
output memwrite);
wire [31:0] readdata;
// instantiate processor and memories
mips mips(clk, reset, adr, writedata, memwrite, readdata);
mem mem(clk, memwrite, adr, writedata, readdata);
endmodule
module mem(input clk, we,
input [31:0] a, wd,
output [31:0] rd);
reg [31:0] RAM[63:0];
initial
begin
$readmemh(“memfile.dat”,RAM);
end
assign rd  RAM[a[31:2]]; // word aligned
always @ (posedge clk)
if (we)
RAM[a[31:2]]  wd;
endmodule
Exercise 7.23 Extend your HDL code for the multicycle MIPS processor from
Exercise 7.22 to handle one of the new instructions from Exercise 7.11. Enhance
the testbench to test the new instruction.
458 CHAPTER SEVEN Microarchitecture

Exercise 7.24 The pipelined MIPS processor is running the following program.
Which registers are being written, and which are being read on the fifth cycle?
add $s0, $t0, $t1
sub $s1, $t2, $t3
and $s2, $s0, $s1
or $s3, $t4, $t5
slt $s4, $s2, $s3
Exercise 7.25 Using a diagram similar to Figure 7.52, show the forwarding and
stalls needed to execute the following instructions on the pipelined MIPS processor.
add $t0, $s0, $s1
sub $t0, $t0, $s2
lw $t1, 60($t0)
and $t2, $t1, $t0
Exercise 7.26 Repeat Exercise 7.25 for the following instructions.
add $t0, $s0, $s1
lw $t1, 60($s2)
sub $t2, $t0, $s3
and $t3, $t1, $t0
Exercise 7.27 How many cycles are required for the pipelined MIPS processor to
issue all of the instructions for the program in Exercise 7.21? What is the CPI of
the processor on this program?
Exercise 7.28 Explain how to extend the pipelined MIPS processor to handle the
addi instruction.
Exercise 7.29 Explain how to extend the pipelined processor to handle the j
instruction. Give particular attention to how the pipeline is flushed when a jump
takes place.
Exercise 7.30 Examples 7.9 and 7.10 point out that the pipelined MIPS processor performance might be better if branches take place during the Execute stage
rather than the Decode stage. Show how to modify the pipelined processor from
Figure 7.58 to branch in the Execute stage. How do the stall and flush signals
change? Redo Examples 7.9 and 7.10 to find the new CPI, cycle time, and
overall time to execute the program.
Exercise 7.31 Your friend, the crack circuit designer, has offered to redesign one
of the units in the pipelined MIPS processor to be much faster. Using the delays
from Table 7.6 and Example 7.10, which unit should she work on to obtain the
greatest speedup of the overall processor? How fast should it be? (Making it
faster than necessary is a waste of your friend’s effort.) What is the cycle time
of the improved processor?
Exercises 459

Exercise 7.32 Consider the delays from Table 7.6 and Example 7.10. Now
suppose that the ALU were 20% faster. Would the cycle time of the pipelined
MIPS processor change? What if the ALU were 20% slower?
Exercise 7.33 Write HDL code for the pipelined MIPS processor. The processor
should be compatible with the top-level module from HDL Example 7.13. It
should support all of the instructions described in this chapter, including addi
and j (see Exercises 7.28 and 7.29). Test your design using the testbench from
HDL Example 7.12.
Exercise 7.34 Design the hazard unit shown in Figure 7.58 for the pipelined
MIPS processor. Use an HDL to implement your design. Sketch the hardware
that a synthesis tool might generate from your HDL.
Exercise 7.35 A nonmaskable interrupt (NMI) is triggered by an input pin to the
processor. When the pin is asserted, the current instruction should finish, then
the processor should set the Cause register to 0 and take an exception. Show
how to modify the multicycle processor in Figures 7.63 and 7.64 to handle
nonmaskable interrupts.
460 CHAPTER SEVEN Microarchitecture

Interview Questions
The following exercises present questions that have been asked at
interviews for digital design jobs.
Question 7.1 Explain the advantages of pipelined microprocessors.
Question 7.2 If additional pipeline stages allow a processor to go faster, why
don’t processors have 100 pipeline stages?
Question 7.3 Describe what a hazard is in a microprocessor and explain ways in
which it can be resolved. What are the pros and cons of each way?
Question 7.4 Describe the concept of a superscalar processor and its pros
and cons.
Interview Questions 461


8
8.1 Introduction
8.2 Memory System
Performance Analysis
8.3 Caches
8.4 Virtual Memory
8.5 Memory-Mapped I/O*
8.6 Real-World Perspective:
IA-32 Memory and I/O
Systems*
8.7 Summary
Exercises
Interview Questions
Memory Systems
8.1 INTRODUCTION
Computer system performance depends on the memory system as well
as the processor microarchitecture. Chapter 7 assumed an ideal memory system that could be accessed in a single clock cycle. However, this
would be true only for a very small memory—or a very slow processor!
Early processors were relatively slow, so memory was able to keep up.
But processor speed has increased at a faster rate than memory speeds.
DRAM memories are currently 10 to 100 times slower than processors. The increasing gap between processor and DRAM memory
speeds demands increasingly ingenious memory systems to try to
approximate a memory that is as fast as the processor. This chapter
investigates practical memory systems and considers trade-offs of
speed, capacity, and cost.
The processor communicates with the memory system over a memory interface. Figure 8.1 shows the simple memory interface used in our
multicycle MIPS processor. The processor sends an address over the
Address bus to the memory system. For a read, MemWrite is 0 and the
memory returns the data on the ReadData bus. For a write, MemWrite
is 1 and the processor sends data to memory on the WriteData bus.
The major issues in memory system design can be broadly explained
using a metaphor of books in a library. A library contains many books
on the shelves. If you were writing a term paper on the meaning of
dreams, you might go to the library1 and pull Freud’s The Interpretation
of Dreams off the shelf and bring it to your cubicle. After skimming it,
you might put it back and pull out Jung’s The Psychology of the
463
1 We realize that library usage is plummeting among college students because of the Internet.
But we also believe that libraries contain vast troves of hard-won human knowledge that are
not electronically available. We hope that Web searching does not completely displace the art
of library research.

Unconscious. You might then go back for another quote from
Interpretation of Dreams, followed by yet another trip to the stacks for
Freud’s The Ego and the Id. Pretty soon you would get tired of walking
from your cubicle to the stacks. If you are clever, you would save time by
keeping the books in your cubicle rather than schlepping them back and
forth. Furthermore, when you pull a book by Freud, you could also pull
several of his other books from the same shelf.
This metaphor emphasizes the principle, introduced in Section 6.2.1,
of making the common case fast. By keeping books that you have
recently used or might likely use in the future at your cubicle, you reduce
the number of time-consuming trips to the stacks. In particular, you use
the principles of temporal and spatial locality. Temporal locality means
that if you have used a book recently, you are likely to use it again soon.
Spatial locality means that when you use one particular book, you are
likely to be interested in other books on the same shelf.
The library itself makes the common case fast by using these principles of locality. The library has neither the shelf space nor the budget to
accommodate all of the books in the world. Instead, it keeps some of the
lesser-used books in deep storage in the basement. Also, it may have an
interlibrary loan agreement with nearby libraries so that it can offer
more books than it physically carries.
In summary, you obtain the benefits of both a large collection and
quick access to the most commonly used books through a hierarchy of
storage. The most commonly used books are in your cubicle. A larger
collection is on the shelves. And an even larger collection is available,
with advanced notice, from the basement and other libraries. Similarly,
memory systems use a hierarchy of storage to quickly access the most
commonly used data while still having the capacity to store large
amounts of data.
Memory subsystems used to build this hierarchy were introduced in
Section 5.5. Computer memories are primarily built from dynamic RAM
(DRAM) and static RAM (SRAM). Ideally, the computer memory system is fast, large, and cheap. In practice, a single memory only has two
of these three attributes; it is either slow, small, or expensive. But computer systems can approximate the ideal by combining a fast small cheap
memory and a slow large cheap memory. The fast memory stores the
most commonly used data and instructions, so on average the memory
464 CHAPTER EIGHT Memory Systems
Processor Memory Address
MemWrite
WriteData
ReadData
WE
CLK
Figure 8.1 The memory
interface

system appears fast. The large memory stores the remainder of the data
and instructions, so the overall capacity is large. The combination of two
cheap memories is much less expensive than a single large fast memory.
These principles extend to using an entire hierarchy of memories of
increasing capacity and decreasing speed.
Computer memory is generally built from DRAM chips. In 2006, a
typical PC had a main memory consisting of 256 MB to 1 GB of
DRAM, and DRAM cost about $100 per gigabyte (GB). DRAM prices
have declined at about 30% per year for the last three decades, and
memory capacity has grown at the same rate, so the total cost of the
memory in a PC has remained roughly constant. Unfortunately, DRAM
speed has improved by only about 7% per year, whereas processor
performance has improved at a rate of 30 to 50% per year, as shown in
Figure 8.2. The plot shows memory and processor speeds with the 1980
speeds as a baseline. In about 1980, processor and memory speeds were
the same. But performance has diverged since then, with memories
badly lagging.
DRAM could keep up with processors in the 1970s and early
1980’s, but it is now woefully too slow. The DRAM access time is one to
two orders of magnitude longer than the processor cycle time (tens of
nanoseconds, compared to less than one nanosecond).
To counteract this trend, computers store the most commonly used
instructions and data in a faster but smaller memory, called a cache. The
cache is usually built out of SRAM on the same chip as the processor.
The cache speed is comparable to the processor speed, because SRAM is
inherently faster than DRAM, and because the on-chip memory eliminates lengthy delays caused by traveling to and from a separate chip. In
2006, on-chip SRAM costs were on the order of $10,000/GB, but the
cache is relatively small (kilobytes to a few megabytes), so the overall
8.1 Introduction 465
Year
CPU
100,000
10,000
100
1000
Performance
10
1
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
Memory
Figure 8.2 Diverging processor
and memory performance
Adapted with permission from
Hennessy and Patterson, Computer
Architecture: A Quantitative Approach,
3rd ed., Morgan Kaufmann, 2003.

cost is low. Caches can store both instructions and data, but we will
refer to their contents generically as “data.”
If the processor requests data that is available in the cache, it is
returned quickly. This is called a cache hit. Otherwise, the processor
retrieves the data from main memory (DRAM). This is called a cache
miss. If the cache hits most of the time, then the processor seldom has to
wait for the slow main memory, and the average access time is low.
The third level in the memory hierarchy is the hard disk, or hard
drive. In the same way that a library uses the basement to store books
that do not fit in the stacks, computer systems use the hard disk to store
data that does not fit in main memory. In 2006, a hard disk cost less than
$1/GB and had an access time of about 10 ms. Hard disk costs have
decreased at 60%/year but access times scarcely improved. The hard disk
provides an illusion of more capacity than actually exists in the main
memory. It is thus called virtual memory. Like books in the basement,
data in virtual memory takes a long time to access. Main memory, also
called physical memory, holds a subset of the virtual memory. Hence, the
main memory can be viewed as a cache for the most commonly used data
from the hard disk.
Figure 8.3 summarizes the memory hierarchy of the computer system discussed in the rest of this chapter. The processor first seeks data in
a small but fast cache that is usually located on the same chip. If the data
is not available in the cache, the processor then looks in main memory. If
the data is not there either, the processor fetches the data from virtual
memory on the large but slow hard disk. Figure 8.4 illustrates this
capacity and speed trade-off in the memory hierarchy and lists typical
costs and access times in 2006 technology. As access time decreases,
speed increases.
Section 8.2 introduces memory system performance analysis.
Section 8.3 explores several cache organizations, and Section 8.4 delves
into virtual memory systems. To conclude, this chapter explores how
processors can access input and output devices, such as keyboards and
monitors, in much the same way as they access memory. Section 8.5
investigates such memory-mapped I/O.
466 CHAPTER EIGHT Memory Systems
CPU Cache
Main
Memory
Processor Chip CLK
Hard
Disk
Figure 8.3 A typical memory
hierarchy

8.2 MEMORY SYSTEM PERFORMANCE ANALYSIS
Designers (and computer buyers) need quantitative ways to measure the
performance of memory systems to evaluate the cost-benefit trade-offs
of various alternatives. Memory system performance metrics are miss
rate or hit rate and average memory access time. Miss and hit rates are
calculated as:
Number of misses Miss Rate  ------------------------------------------------------------------------------------- 1  Hit Rate
(8.1)
Number of total memory accesses
Number of hits Hit Rate  ------------------------------------------------------------------------------------- 1  Miss Rate
Number of total memory accesses
Example 8.1 CALCULATING CACHE PERFORMANCE
Suppose a program has 2000 data access instructions (loads or stores), and 1250
of these requested data values are found in the cache. The other 750 data values
are supplied to the processor by main memory or disk memory. What are the
miss and hit rates for the cache?
Solution: The miss rate is 750/2000  0.375  37.5%. The hit rate is
1250/2000  0.625  1  0.375  62.5%.
Average memory access time (AMAT) is the average time a processor
must wait for memory per load or store instruction. In the typical
computer system from Figure 8.3, the processor first looks for the data
in the cache. If the cache misses, the processor then looks in main
memory. If the main memory misses, the processor accesses virtual
memory on the hard disk. Thus, AMAT is calculated as:
AMAT  tcache  MRcache(tMM  MRMMtVM) (8.2)
8.2 Memory System Performance Analysis 467
Cache
Main Memory
Speed
Virtual Memory
Capacity
Technology Cost/GB Access Time
SRAM ~ $10,000 ~ 1 ns
DRAM ~ $100 ~ 100 ns
Hard Disk ~ $1 ~ 10,000,000 ns
Figure 8.4 Memory hierarchy
components, with typical
characteristics in 2006
Chapter 08.qx
where tcache, tMM, and tVM are the access times of the cache, main
memory, and virtual memory, and MRcache and MRMM are the cache and
main memory miss rates, respectively.
Example 8.2 CALCULATING AVERAGE MEMORY ACCESS TIME
Suppose a computer system has a memory organization with only two levels of
hierarchy, a cache and main memory. What is the average memory access time
given the access times and miss rates given in Table 8.1?
Solution: The average memory access time is 1  0.1(100)  11 cycles.
468 CHAPTER EIGHT Memory Systems
Table 8.1 Access times and miss rates
Memory Access Time Miss
Level (Cycles) Rate
Cache 1 10%
Main Memory 100 0%
Gene Amdahl, 1922–. Most
famous for Amdahl’s Law, an
observation he made in 1965.
While in graduate school, he
began designing computers in
his free time. This side work
earned him his Ph.D. in theoretical physics in 1952. He
joined IBM immediately after
graduation, and later went on
to found three companies,
including one called Amdahl
Corporation in 1970.
Example 8.3 IMPROVING ACCESS TIME
An 11-cycle average memory access time means that the processor spends ten
cycles waiting for data for every one cycle actually using that data. What cache
miss rate is needed to reduce the average memory access time to 1.5 cycles given
the access times in Table 8.1?
Solution: If the miss rate is m, the average access time is 1  100m. Setting this
time to 1.5 and solving for m requires a cache miss rate of 0.5%.
As a word of caution, performance improvements might not always
be as good as they sound. For example, making the memory system ten
times faster will not necessarily make a computer program run ten times
as fast. If 50% of a program’s instructions are loads and stores, a tenfold memory system improvement only means a 1.82-fold improvement
in program performance. This general principle is called Amdahl’s Law,
which says that the effort spent on increasing the performance of a subsystem is worthwhile only if the subsystem affects a large percentage of
the overall performance.
8.3 CACHES
A cache holds commonly used memory data. The number of data
words that it can hold is called the capacity, C. Because the capacity
C
of the cache is smaller than that of main memory, the computer system designer must choose what subset of the main memory is kept in
the cache.
When the processor attempts to access data, it first checks the cache
for the data. If the cache hits, the data is available immediately. If the
cache misses, the processor fetches the data from main memory and
places it in the cache for future use. To accommodate the new data, the
cache must replace old data. This section investigates these issues in
cache design by answering the following questions: (1) What data is held
in the cache? (2) How is the data found? and (3) What data is replaced
to make room for new data when the cache is full?
When reading the next sections, keep in mind that the driving force
in answering these questions is the inherent spatial and temporal locality
of data accesses in most applications. Caches use spatial and temporal
locality to predict what data will be needed next. If a program accesses
data in a random order, it would not benefit from a cache.
As we explain in the following sections, caches are specified by their
capacity (C), number of sets (S), block size (b), number of blocks (B),
and degree of associativity (N).
Although we focus on data cache loads, the same principles apply
for fetches from an instruction cache. Data cache store operations are
similar and are discussed further in Section 8.3.4.
8.3.1 What Data Is Held in the Cache?
An ideal cache would anticipate all of the data needed by the processor
and fetch it from main memory ahead of time so that the cache has a
zero miss rate. Because it is impossible to predict the future with perfect
accuracy, the cache must guess what data will be needed based on the
past pattern of memory accesses. In particular, the cache exploits temporal and spatial locality to achieve a low miss rate.
Recall that temporal locality means that the processor is likely to
access a piece of data again soon if it has accessed that data recently.
Therefore, when the processor loads or stores data that is not in the
cache, the data is copied from main memory into the cache. Subsequent
requests for that data hit in the cache.
Recall that spatial locality means that, when the processor accesses a
piece of data, it is also likely to access data in nearby memory locations.
Therefore, when the cache fetches one word from memory, it may also
fetch several adjacent words. This group of words is called a cache
block. The number of words in the cache block, b, is called the block
size. A cache of capacity C contains B  C/b blocks.
The principles of temporal and spatial locality have been experimentally verified in real programs. If a variable is used in a program, the
8.3 Caches 469
Cache: a hiding place
especially for concealing and
preserving provisions or
implements.
– Merriam Webster Online
Dictionary. 2006. http://
www.merriam-webster.com
C
same variable is likely to be used again, creating temporal locality. If an
element in an array is used, other elements in the same array are also
likely to be used, creating spatial locality.
8.3.2 How Is the Data Found?
A cache is organized into S sets, each of which holds one or more blocks
of data. The relationship between the address of data in main memory
and the location of that data in the cache is called the mapping. Each
memory address maps to exactly one set in the cache. Some of the
address bits are used to determine which cache set contains the data. If
the set contains more than one block, the data may be kept in any of the
blocks in the set.
Caches are categorized based on the number of blocks in a set. In a
direct mapped cache, each set contains exactly one block, so the cache
has S  B sets. Thus, a particular main memory address maps to a
unique block in the cache. In an N-way set associative cache, each set
contains N blocks. The address still maps to a unique set, with S  B/N
sets. But the data from that address can go in any of the N blocks in that
set. A fully associative cache has only S  1 set. Data can go in any of
the B blocks in the set. Hence, a fully associative cache is another name
for a B-way set associative cache.
To illustrate these cache organizations, we will consider a MIPS
memory system with 32-bit addresses and 32-bit words. The memory is
byte-addressable, and each word is four bytes, so the memory consists of
230 words aligned on word boundaries. We analyze caches with an eightword capacity (C) for the sake of simplicity. We begin with a one-word
block size (b), then generalize later to larger blocks.
Direct Mapped Cache
A direct mapped cache has one block in each set, so it is organized into
S  B sets. To understand the mapping of memory addresses onto cache
blocks, imagine main memory as being mapped into b-word blocks, just
as the cache is. An address in block 0 of main memory maps to set 0 of
the cache. An address in block 1 of main memory maps to set 1 of the
cache, and so forth until an address in block B  1 of main memory
maps to block B  1 of the cache. There are no more blocks of the
cache, so the mapping wraps around, such that block B of main memory
maps to block 0 of the cache.
This mapping is illustrated in Figure 8.5 for a direct mapped cache
with a capacity of eight words and a block size of one word. The cache
has eight sets, each of which contains a one-word block. The bottom
two bits of the address are always 00, because they are word aligned.
The next log28  3 bits indicate the set onto which the memory address
maps. Thus, the data at addresses 0x00000004, 0x00000024, . . . ,
470 CHAPTER EIGHT Memory Systems
Chapter
0xFFFFFFE4 all map to set 1, as shown in blue. Likewise, data at
addresses 0x00000010, . . . , 0xFFFFFFF0 all map to set 4, and so
forth. Each main memory address maps to exactly one set in the cache.
Example 8.4 CACHE FIELDS
To what cache set in Figure 8.5 does the word at address 0x00000014 map?
Name another address that maps to the same set.
Solution: The two least significant bits of the address are 00, because the address
is word aligned. The next three bits are 101, so the word maps to set 5. Words at
addresses 0x34, 0x54, 0x74, . . . , 0xFFFFFFF4 all map to this same set.
Because many addresses map to a single set, the cache must also
keep track of the address of the data actually contained in each set.
The least significant bits of the address specify which set holds the
data. The remaining most significant bits are called the tag and indicate which of the many possible addresses is held in that set.
In our previous example, the two least significant bits of the 32-bit
address are called the byte offset, because they indicate the byte within
the word. The next three bits are called the set bits, because they indicate
the set to which the address maps. (In general, the number of set bits is
log2S.) The remaining 27 tag bits indicate the memory address of the
data stored in a given cache set. Figure 8.6 shows the cache fields for
address 0xFFFFFFE4. It maps to set 1 and its tag is all 1’s.
8.3 Caches 471
00...00010000
230-Word Main Memory
mem[0x00000000]
mem[0x00000004]
mem[0x00000008]
mem[0x0000000C]
mem[0x00000010]
mem[0x00000014]
mem[0x00000018]
mem[0x0000001C]
mem[0x00000020]
mem[0x00000024]
mem[0xFFFFFFE0]
mem[0xFFFFFFE4]
mem[0xFFFFFFE8]
mem[0xFFFFFFEC]
mem[0xFFFFFFF0]
mem[0xFFFFFFF4]
mem[0xFFFFFFF8]
mem[0xFFFFFFFC]
23-Word Cache
Address
00...00000000
00...00000100
00...00001000
00...00001100
00...00010100
00...00011000
00...00011100
00...00100000
00...00100100
11...11110000
11...11100000
11...11100100
11...11101000
11...11101100
11...11110100
11...11111000
11...11111100
Set 7 (111)
Set 6 (110)
Set 5 (101)
Set 4 (100)
Set 3 (011)
Set 2 (010)
Set 1 (001)
Set 0 (000)
Data
Figure 8.5 Mapping of main
memory to a direct mapped
cache

Example 8.5 CACHE FIELDS
Find the number of set and tag bits for a direct mapped cache with 1024 (210)
sets and a one-word block size. The address size is 32 bits.
Solution: A cache with 210 sets requires log2(210)  10 set bits. The two least significant bits of the address are the byte offset, and the remaining 32  10  2  20
bits form the tag.
Sometimes, such as when the computer first starts up, the cache sets
contain no data at all. The cache uses a valid bit for each set to indicate
whether the set holds meaningful data. If the valid bit is 0, the contents
are meaningless.
Figure 8.7 shows the hardware for the direct mapped cache of
Figure 8.5. The cache is constructed as an eight-entry SRAM. Each
entry, or set, contains one line consisting of 32 bits of data, 27 bits of
tag, and 1 valid bit. The cache is accessed using the 32-bit address. The
two least significant bits, the byte offset bits, are ignored for word
accesses. The next three bits, the set bits, specify the entry or set in the
cache. A load instruction reads the specified entry from the cache and
checks the tag and valid bits. If the tag matches the most significant
472 CHAPTER EIGHT Memory Systems
00
Tag Set
Byte
Offset Memory
Address 111 ... 111 001
FFFFFF E 4
Figure 8.6 Cache fields for
address 0xFFFFFFE4 when
mapping to the cache in
Figure 8.5
Tag Data
00
Tag Set
Byte
Offset Memory
Address
Hit Data
V
=
27 3
27 32
8-entry x
(1+27+32)-bit
SRAM
Set 7
Set 6
Set 5
Set 4
Set 3
Set 2
Set 1
Set 0
Figure 8.7 Direct mapped cache
with 8 sets
Chap
27 bits of the address and the valid bit is 1, the cache hits and the data is
returned to the processor. Otherwise, the cache misses and the memory
system must fetch the data from main memory.
Example 8.6 TEMPORAL LOCALITY WITH A DIRECT MAPPED CACHE
Loops are a common source of temporal and spatial locality in applications.
Using the eight-entry cache of Figure 8.7, show the contents of the cache after
executing the following silly loop in MIPS assembly code. Assume that the cache
is initially empty. What is the miss rate?
addi $t0, $0, 5
loop: beq $t0, $0, done
lw $t1, 0x4($0)
lw $t2, 0xC($0)
lw $t3, 0x8($0)
addi $t0, $t0, 1
j loop
done:
Solution: The program contains a loop that repeats for five iterations. Each
iteration involves three memory accesses (loads), resulting in 15 total memory
accesses. The first time the loop executes, the cache is empty and the data must
be fetched from main memory locations 0x4, 0xC, and 0x8 into cache sets 1,
3, and 2, respectively. However, the next four times the loop executes, the data
is found in the cache. Figure 8.8 shows the contents of the cache during the
last request to memory address 0x4. The tags are all 0 because the upper 27
bits of the addresses are 0. The miss rate is 3/15  20%.
8.3 Caches 473
V Tag Data
1 00...00 mem[0x00...04]
0
0
0
0
0
00
Tag Set
Byte
Offset Memory
Address
V
3
00...00 001
1
00...00
00...00
1
mem[0x00...0C]
mem[0x00...08]
Set 7 (111)
Set 6 (110)
Set 5 (101)
Set 4 (100)
Set 3 (011)
Set 2 (010)
Set 1 (001)
Set 0 (000)
Figure 8.8 Direct mapped cache
contents
When two recently accessed addresses map to the same cache block,
a conflict occurs, and the most recently accessed address evicts the previous one from the block. Direct mapped caches have only one block in
each set, so two addresses that map to the same set always cause a conflict. The example on the next page illustrates conflicts.
Ch
Example 8.7 CACHE BLOCK CONFLICT
What is the miss rate when the following loop is executed on the eight-word
direct mapped cache from Figure 8.7? Assume that the cache is initially empty.
addi $t0, $0, 5
loop: beq $t0, $0, done
lw $t1, 0x4($0)
lw $t2, 0x24($0)
addi $t0, $t0, 1
j loop
done:
Solution: Memory addresses 0x4 and 0x24 both map to set 1. During the initial
execution of the loop, data at address 0x4 is loaded into set 1 of the cache. Then
data at address 0x24 is loaded into set 1, evicting the data from address 0x4.
Upon the second execution of the loop, the pattern repeats and the cache must
refetch data at address 0x4, evicting data from address 0x24. The two addresses
conflict, and the miss rate is 100%.
Multi-way Set Associative Cache
An N-way set associative cache reduces conflicts by providing N blocks
in each set where data mapping to that set might be found. Each memory address still maps to a specific set, but it can map to any one of the
N blocks in the set. Hence, a direct mapped cache is another name for a
one-way set associative cache. N is also called the degree of associativity
of the cache.
Figure 8.9 shows the hardware for a C  8-word, N  2-way
set associative cache. The cache now has only S  4 sets rather than 8.
474 CHAPTER EIGHT Memory Systems
Tag Data
Tag Set
Byte
Offset Memory
Address
Data
1
0
Hit1
V
=
00
32 32
32
V Tag Data
=
Hit Hit1 0
Hit
28 2
28 28
Way 1 Way 0
Set 3
Set 2
Set 1
Set 0
Figure 8.9 Two-way set
associative cache
Chap
Thus, only log24  2 set bits rather than 3 are used to select the set.
The tag increases from 27 to 28 bits. Each set contains two ways or
degrees of associativity. Each way consists of a data block and the
valid and tag bits. The cache reads blocks from both ways in the
selected set and checks the tags and valid bits for a hit. If a hit occurs
in one of the ways, a multiplexer selects data from that way.
Set associative caches generally have lower miss rates than direct
mapped caches of the same capacity, because they have fewer conflicts.
However, set associative caches are usually slower and somewhat more
expensive to build because of the output multiplexer and additional
comparators. They also raise the question of which way to replace when
both ways are full; this is addressed further in Section 8.3.3. Most
commercial systems use set associative caches.
Example 8.8 SET ASSOCIATIVE CACHE MISS RATE
Repeat Example 8.7 using the eight-word two-way set associative cache from
Figure 8.9.
Solution: Both memory accesses, to addresses 0x4 and 0x24, map to set 1.
However, the cache has two ways, so it can accommodate data from both
addresses. During the first loop iteration, the empty cache misses both
addresses and loads both words of data into the two ways of set 1, as shown in
Figure 8.10. On the next four iterations, the cache hits. Hence, the miss rate is
2/10  20%. Recall that the direct mapped cache of the same size from
Example 8.7 had a miss rate of 100%.
8.3 Caches 475
V Tag Data V Tag Data
00...001 mem[0x00...24] 00...101 mem[0x00...04]
0
0
0
0
0
0
Way 1 Way 0
Set 3
Set 2
Set 1
Set 0
Figure 8.10 Two-way set
associative cache contents
Fully Associative Cache
A fully associative cache contains a single set with B ways, where B is
the number of blocks. A memory address can map to a block in any of
these ways. A fully associative cache is another name for a B-way set
associative cache with one set.
Figure 8.11 shows the SRAM array of a fully associative cache with
eight blocks. Upon a data request, eight tag comparisons (not shown) must
be made, because the data could be in any block. Similarly, an 8:1 multiplexer chooses the proper data if a hit occurs. Fully associative caches tend
Ch
to have the fewest conflict misses for a given cache capacity, but they
require more hardware for additional tag comparisons. They are best suited
to relatively small caches because of the large number of comparators.
Block Size
The previous examples were able to take advantage only of temporal
locality, because the block size was one word. To exploit spatial locality,
a cache uses larger blocks to hold several consecutive words.
The advantage of a block size greater than one is that when a miss
occurs and the word is fetched into the cache, the adjacent words in the
block are also fetched. Therefore, subsequent accesses are more likely to
hit because of spatial locality. However, a large block size means that a
fixed-size cache will have fewer blocks. This may lead to more conflicts,
increasing the miss rate. Moreover, it takes more time to fetch the missing
cache block after a miss, because more than one data word is fetched
from main memory. The time required to load the missing block into the
cache is called the miss penalty. If the adjacent words in the block are not
accessed later, the effort of fetching them is wasted. Nevertheless, most
real programs benefit from larger block sizes.
Figure 8.12 shows the hardware for a C  8-word direct mapped
cache with a b  4-word block size. The cache now has only B  C/b  2
blocks. A direct mapped cache has one block in each set, so this cache is
476 CHAPTER EIGHT Memory Systems
V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data V Tag Data
Way 7 Way 6 Way 5 Way 4 Way 3 Way 2 Way 1 Way 0
Tag Data
00
Tag
Byte
Offset Memory
Address
Data
V
Block
Offset
32 32 32 32
32
Hit
=
Set
27
27 2
Set 1
Set 0
11
10
01
00
Figure 8.11 Eight-block fully associative cache
Figure 8.12 Direct mapped cache with two sets and a four-word block size
Chap
organized as two sets. Thus, only log22  1 bit is used to select the set.
A multiplexer is now needed to select the word within the block. The
multiplexer is controlled by the log24  2 block offset bits of the address.
The most significant 27 address bits form the tag. Only one tag is needed
for the entire block, because the words in the block are at consecutive
addresses.
Figure 8.13 shows the cache fields for address 0x8000009C when
it maps to the direct mapped cache of Figure 8.12. The byte offset
bits are always 0 for word accesses. The next log2b  2 block offset
bits indicate the word within the block. And the next bit indicates the
set. The remaining 27 bits are the tag. Therefore, word 0x8000009C
maps to set 1, word 3 in the cache. The principle of using larger
block sizes to exploit spatial locality also applies to associative
caches.
Example 8.9 SPATIAL LOCALITY WITH A DIRECT MAPPED CACHE
Repeat Example 8.6 for the eight-word direct mapped cache with a four-word
block size.
Solution: Figure 8.14 shows the contents of the cache after the first memory
access. On the first loop iteration, the cache misses on the access to memory
address 0x4. This access loads data at addresses 0x0 through 0xC into the cache
block. All subsequent accesses (as shown for address 0xC) hit in the cache.
Hence, the miss rate is 1/15  6.67%.
8.3 Caches 477
00
Tag
Byte
Offset Memory
Address 100...100 11
Block
Offset
1
800000 9 C
Set
Figure 8.13 Cache fields for
address 0x8000009C when
mapping to the cache of
Figure 8.12
Set 1
V Tag Data
1 00...00 mem[0x00...0C] Set 0
0
mem[0x00...08] mem[0x00...04] mem[0x00...00]
00
Tag
Byte
Offset Memory
Address
V
Block
Set Offset
00...00 0 11
Figure 8.14 Cache contents with a block size (b) of four words
Putting It All Together
Caches are organized as two-dimensional arrays. The rows are called
sets, and the columns are called ways. Each entry in the array consists of
Chap
a data block and its associated valid and tag bits. Caches are
characterized by
 capacity C
 block size b (and number of blocks, B  C/b)
 number of blocks in a set (N)
Table 8.2 summarizes the various cache organizations. Each address in
memory maps to only one set but can be stored in any of the ways.
Cache capacity, associativity, set size, and block size are typically
powers of 2. This makes the cache fields (tag, set, and block offset bits)
subsets of the address bits.
Increasing the associativity, N, usually reduces the miss rate caused
by conflicts. But higher associativity requires more tag comparators.
Increasing the block size, b, takes advantage of spatial locality to reduce
the miss rate. However, it decreases the number of sets in a fixed sized
cache and therefore could lead to more conflicts. It also increases the
miss penalty.
8.3.3 What Data Is Replaced?
In a direct mapped cache, each address maps to a unique block and set.
If a set is full when new data must be loaded, the block in that set is
replaced with the new data. In set associative and fully associative
caches, the cache must choose which block to evict when a cache set is
full. The principle of temporal locality suggests that the best choice is to
evict the least recently used block, because it is least likely to be used
again soon. Hence, most associative caches have a least recently used
(LRU) replacement policy.
In a two-way set associative cache, a use bit, U, indicates which way
within a set was least recently used. Each time one of the ways is used, U
is adjusted to indicate the other way. For set associative caches with
more than two ways, tracking the least recently used way becomes complicated. To simplify the problem, the ways are often divided into two
groups and U indicates which group of ways was least recently used.
478 CHAPTER EIGHT Memory Systems
Table 8.2 Cache organizations
Number of Ways Number of Sets
Organization (N) (S)
direct mapped 1 B
set associative 1  N  B B/N
fully associative B 1
Chap
Upon replacement, the new block replaces a random block within the
least recently used group. Such a policy is called pseudo-LRU and is
good enough in practice.
Example 8.10 LRU REPLACEMENT
Show the contents of an eight-word two-way set associative cache after executing the following code. Assume LRU replacement, a block size of one word, and
an initially empty cache.
lw $t0, 0x04($0)
lw $t1, 0x24($0)
lw $t2, 0x54($0)
Solution: The first two instructions load data from memory addresses 0x4 and
0x24 into set 1 of the cache, shown in Figure 8.15(a). U  0 indicates that data
in way 0 was the least recently used. The next memory access, to address 0x54,
also maps to set 1 and replaces the least recently used data in way 0, as shown in
Figure 8.15(b), The use bit, U, is set to 1 to indicate that data in way 1 was the
least recently used.
8.3 Caches 479
V Tag Data
0
V Tag Data
0
0
0
0
0
U
1 00...010 mem[0x00...24] 1 00...000 mem[0x00...04]
0
0
0
0
(a)
Way 1 Way 0
Set 3 (11)
Set 2 (10)
Set 1 (01)
Set 0 (00)
V Tag Data
0
V Tag Data
0
0
0
0
0
U
1 00...010 mem[0x00...24] 1 00...101 mem[0x00...54]
0
0
0
1
(b)
Way 1 Way 0
Set 3 (11)
Set 2 (10)
Set 1 (01)
Set 0 (00)
Figure 8.15 Two-way associative
cache with LRU replacement
8.3.4 Advanced Cache Design*
Modern systems use multiple levels of caches to decrease memory access
time. This section explores the performance of a two-level caching system and examines how block size, associativity, and cache capacity
affect miss rate. The section also describes how caches handle stores, or
writes, by using a write-through or write-back policy.
C
Multiple-Level Caches
Large caches are beneficial because they are more likely to hold data of
interest and therefore have lower miss rates. However, large caches tend
to be slower than small ones. Modern systems often use two levels of
caches, as shown in Figure 8.16. The first-level (L1) cache is small
enough to provide a one- or two-cycle access time. The second-level
(L2) cache is also built from SRAM but is larger, and therefore slower,
than the L1 cache. The processor first looks for the data in the L1
cache. If the L1 cache misses, the processor looks in the L2 cache. If the
L2 cache misses, the processor fetches the data from main memory.
Some modern systems add even more levels of cache to the memory
hierarchy, because accessing main memory is so slow.
Example 8.11 SYSTEM WITH AN L2 CACHE
Use the system of Figure 8.16 with access times of 1, 10, and 100 cycles for the
L1 cache, L2 cache, and main memory, respectively. Assume that the L1 and L2
caches have miss rates of 5% and 20%, respectively. Specifically, of the 5% of
accesses that miss the L1 cache, 20% of those also miss the L2 cache. What is
the average memory access time (AMAT)?
Solution: Each memory access checks the L1 cache. When the L1 cache misses
(5% of the time), the processor checks the L2 cache. When the L2 cache
misses (20% of the time), the processor fetches the data from main memory.
Using Equation 8.2, we calculate the average memory access time as follows:
1 cycle  0.05[10 cycles  0.2(100 cycles)]  2.5 cycles
The L2 miss rate is high because it receives only the “hard” memory accesses,
those that miss in the L1 cache. If all accesses went directly to the L2 cache, the
L2 miss rate would be about 1%.
480 CHAPTER EIGHT Memory Systems
L1
Cache
L2 Cache
Main Memory
Capacity
Virtual Memory
Speed
Figure 8.16 Memory hierarchy
with two levels of cache
C
Reducing Miss Rate
Cache misses can be reduced by changing capacity, block size, and/or
associativity. The first step to reducing the miss rate is to understand
the causes of the misses. The misses can be classified as compulsory,
capacity, and conflict. The first request to a cache block is called a
compulsory miss, because the block must be read from memory regardless of the cache design. Capacity misses occur when the cache is too
small to hold all concurrently used data. Conflict misses are caused
when several addresses map to the same set and evict blocks that are
still needed.
Changing cache parameters can affect one or more type of cache
miss. For example, increasing cache capacity can reduce conflict and
capacity misses, but it does not affect compulsory misses. On the other
hand, increasing block size could reduce compulsory misses (due to spatial locality) but might actually increase conflict misses (because more
addresses would map to the same set and could conflict).
Memory systems are complicated enough that the best way to evaluate their performance is by running benchmarks while varying cache
parameters. Figure 8.17 plots miss rate versus cache size and degree of
associativity for the SPEC2000 benchmark. This benchmark has a
small number of compulsory misses, shown by the dark region near the
x-axis. As expected, when cache size increases, capacity misses
decrease. Increased associativity, especially for small caches, decreases
the number of conflict misses shown along the top of the curve.
8.3 Caches 481
0.10
1-way
2-way
4-way
8-way
Capacity Compulsory
Cache Size (KB)
0.09
0.08
0.07
0.06
0.05 Miss Rate
per Type
0.04
0.03
0.02
0.01
0.00
84 16 32 64 128 256 512 1024
Figure 8.17 Miss rate versus
cache size and associativity on
SPEC2000 benchmark
Adapted with permission from
Hennessy and Patterson, Computer
Architecture: A Quantitative Approach,
3rd ed., Morgan Kaufmann, 2003.

Increasing associativity beyond four or eight ways provides only small
decreases in miss rate.
As mentioned, miss rate can also be decreased by using larger
block sizes that take advantage of spatial locality. But as block size
increases, the number of sets in a fixed size cache decreases, increasing
the probability of conflicts. Figure 8.18 plots miss rate versus block
size (in number of bytes) for caches of varying capacity. For small
caches, such as the 4-KB cache, increasing the block size beyond
64 bytes increases the miss rate because of conflicts. For larger caches,
increasing the block size does not change the miss rate. However, large
block sizes might still increase execution time because of the larger
miss penalty, the time required to fetch the missing cache block from
main memory on a miss.
Write Policy
The previous sections focused on memory loads. Memory stores, or
writes, follow a similar procedure as loads. Upon a memory store, the
processor checks the cache. If the cache misses, the cache block is
fetched from main memory into the cache, and then the appropriate
word in the cache block is written. If the cache hits, the word is simply
written to the cache block.
Caches are classified as either write-through or write-back. In a
write-through cache, the data written to a cache block is simultaneously
written to main memory. In a write-back cache, a dirty bit (D) is associated with each cache block. D is 1 when the cache block has been written and 0 otherwise. Dirty cache blocks are written back to main
memory only when they are evicted from the cache. A write-through
482 CHAPTER EIGHT Memory Systems
10%
Miss
Rate 5%
0%
16 32 64
Block Size
128 256
256 K
64 K
16 K
4 K
Figure 8.18 Miss rate versus block size and cache size on SPEC92 benchmark Adapted with permission from
Hennessy and Patterson, Computer Architecture: A Quantitative Approach, 3rd ed., Morgan Kaufmann, 2003.

cache requires no dirty bit but usually requires more main memory
writes than a write-back cache. Modern caches are usually write-back,
because main memory access time is so large.
Example 8.12 WRITE-THROUGH VERSUS WRITE-BACK
Suppose a cache has a block size of four words. How many main memory
accesses are required by the following code when using each write policy: writethrough or write-back?
sw $t0, 0x0($0)
sw $t0, 0xC($0)
sw $t0, 0x8($0)
sw $t0, 0x4($0)
Solution: All four store instructions write to the same cache block. With a writethrough cache, each store instruction writes a word to main memory, requiring
four main memory writes. A write-back policy requires only one main memory
access, when the dirty cache block is evicted.
8.3.5 The Evolution of MIPS Caches*
Table 8.3 traces the evolution of cache organizations used by the MIPS
processor from 1985 to 2004. The major trends are the introduction of
multiple levels of cache, larger cache capacity, and increased associativity. These trends are driven by the growing disparity between CPU
frequency and main memory speed and the decreasing cost of transistors. The growing difference between CPU and memory speeds necessitates a lower miss rate to avoid the main memory bottleneck, and the
decreasing cost of transistors allows larger cache sizes.
8.3 Caches 483
Table 8.3 MIPS cache evolution*
Year CPU MHz L1 Cache L2 Cache
1985 R2000 16.7 none none
1990 R3000 33 32 KB direct mapped none
1991 R4000 100 8 KB direct mapped 1 MB direct mapped
1995 R10000 250 32 KB two-way 4 MB two-way
2001 R14000 600 32 KB two-way 16 MB two-way
2004 R16000A 800 64 KB two-way 16 MB two-way
* Adapted from D. Sweetman, See MIPS Run, Morgan Kaufmann, 1999.

8.4 VIRTUAL MEMORY
Most modern computer systems use a hard disk (also called a hard
drive) as the lowest level in the memory hierarchy (see Figure 8.4).
Compared with the ideal large, fast, cheap memory, a hard disk is large
and cheap but terribly slow. The disk provides a much larger capacity
than is possible with a cost-effective main memory (DRAM). However, if
a significant fraction of memory accesses involve the disk, performance
is dismal. You may have encountered this on a PC when running too
many programs at once.
Figure 8.19 shows a hard disk with the lid of its case removed. As the
name implies, the hard disk contains one or more rigid disks or platters,
each of which has a read/write head on the end of a long triangular arm.
The head moves to the correct location on the disk and reads or writes
data magnetically as the disk rotates beneath it. The head takes several
milliseconds to seek the correct location on the disk, which is fast from a
human perspective but millions of times slower than the processor.
484 CHAPTER EIGHT Memory Systems
Figure 8.19 Hard disk

The objective of adding a hard disk to the memory hierarchy is to
inexpensively give the illusion of a very large memory while still providing
the speed of faster memory for most accesses. A computer with only 128
MB of DRAM, for example, could effectively provide 2 GB of memory
using the hard disk. This larger 2-GB memory is called virtual memory,
and the smaller 128-MB main memory is called physical memory. We will
use the term physical memory to refer to main memory throughout this
section.
Programs can access data anywhere in virtual memory, so they must
use virtual addresses that specify the location in virtual memory. The
physical memory holds a subset of most recently accessed virtual memory. In this way, physical memory acts as a cache for virtual memory.
Thus, most accesses hit in physical memory at the speed of DRAM, yet
the program enjoys the capacity of the larger virtual memory.
Virtual memory systems use different terminologies for the same
caching principles discussed in Section 8.3. Table 8.4 summarizes the
analogous terms. Virtual memory is divided into virtual pages, typically
4 KB in size. Physical memory is likewise divided into physical pages of
the same size. A virtual page may be located in physical memory
(DRAM) or on the disk. For example, Figure 8.20 shows a virtual memory that is larger than physical memory. The rectangles indicate pages.
Some virtual pages are present in physical memory, and some are located
on the disk. The process of determining the physical address from
the virtual address is called address translation. If the processor attempts
to access a virtual address that is not in physical memory, a page fault
occurs, and the operating system loads the page from the hard disk into
physical memory.
To avoid page faults caused by conflicts, any virtual page can map
to any physical page. In other words, physical memory behaves as a fully
associative cache for virtual memory. In a conventional fully associative
cache, every cache block has a comparator that checks the most significant address bits against a tag to determine whether the request hits in
8.4 Virtual Memory 485
A computer with 32-bit
addresses can access a maximum of 232 bytes  4 GB of
memory. This is one of the
motivations for moving to
64-bit computers, which can
access far more memory.
Table 8.4 Analogous cache and virtual memory terms
Cache Virtual Memory
Block Page
Block size Page size
Block offset Page offset
Miss Page fault
Tag Virtual page number
C
the block. In an analogous virtual memory system, each physical page
would need a comparator to check the most significant virtual address
bits against a tag to determine whether the virtual page maps to that
physical page.
A realistic virtual memory system has so many physical pages that
providing a comparator for each page would be excessively expensive.
Instead, the virtual memory system uses a page table to perform address
translation. A page table contains an entry for each virtual page, indicating its location in physical memory or that it is on the disk. Each load or
store instruction requires a page table access followed by a physical
memory access. The page table access translates the virtual address used
by the program to a physical address. The physical address is then used
to actually read or write the data.
The page table is usually so large that it is located in physical
memory. Hence, each load or store involves two physical memory
accesses: a page table access, and a data access. To speed up address
translation, a translation lookaside buffer (TLB) caches the most commonly used page table entries.
The remainder of this section elaborates on address translation, page
tables, and TLBs.
8.4.1 Address Translation
In a system with virtual memory, programs use virtual addresses so that
they can access a large memory. The computer must translate these virtual addresses to either find the address in physical memory or take a
page fault and fetch the data from the hard disk.
Recall that virtual memory and physical memory are divided into
pages. The most significant bits of the virtual or physical address specify
the virtual or physical page number. The least significant bits specify the
word within the page and are called the page offset.
486 CHAPTER EIGHT Memory Systems
Physical Memory
Physical Addresses
Virtual Addresses
Hard Disk
Address Translation
Figure 8.20 Virtual and physical
pages

Figure 8.21 illustrates the page organization of a virtual memory
system with 2 GB of virtual memory and 128 MB of physical memory
divided into 4-KB pages. MIPS accommodates 32-bit addresses. With
a 2-GB  231-byte virtual memory, only the least significant 31 virtual
address bits are used; the 32nd bit is always 0. Similarly, with a
128-MB  227-byte physical memory, only the least significant
27 physical address bits are used; the upper 5 bits are always 0.
Because the page size is 4 KB  212 bytes, there are 231/212  219
virtual pages and 227/212  215 physical pages. Thus, the virtual and
physical page numbers are 19 and 15 bits, respectively. Physical memory
can only hold up to 1/16th of the virtual pages at any given time. The
rest of the virtual pages are kept on disk.
Figure 8.21 shows virtual page 5 mapping to physical page 1, virtual
page 0x7FFFC mapping to physical page 0x7FFE, and so forth. For
example, virtual address 0x53F8 (an offset of 0x3F8 within virtual
page 5) maps to physical address 0x13F8 (an offset of 0x3F8 within
physical page 1). The least significant 12 bits of the virtual and physical
addresses are the same (0x3F8) and specify the page offset within the
virtual and physical pages. Only the page number needs to be translated
to obtain the physical address from the virtual address.
Figure 8.22 illustrates the translation of a virtual address to a physical address. The least significant 12 bits indicate the page offset and
require no translation. The upper 19 bits of the virtual address specify
the virtual page number (VPN) and are translated to a 15-bit physical
page number (PPN). The next two sections describe how page tables and
TLBs are used to perform this address translation.
8.4 Virtual Memory 487
Physical Memory
Physical
Page
Number Physical Addresses
Virtual Memory
Virtual
Page
Number Virtual Addresses
7FFF 0x7FFF000 - 0x7FFFFFF
0x7FFE000 - 0x7FFEFFF
0x0000000 - 0x0000FFF
0x0001000 - 0x0001FFF
7FFE
0001
0000
7FFFA
7FFF9
00006
00005
7FFFC
7FFFB
7FFFE
7FFFD
7FFFF
00001
00000
00003
00002
00004
0x7FFFF000 - 0x7FFFFFFF
0x7FFFE000 - 0x7FFFEFFF
0x7FFFD000 - 0x7FFFDFFF
0x7FFFC000 - 0x7FFFCFFF
0x7FFFB000 - 0x7FFFBFFF
0x7FFFA000 - 0x7FFFAFFF
0x00005000 - 0x00005FFF
0x00003000 - 0x00003FFF
0x00001000 - 0x00001FFF
0x7FFF9000 - 0x7FFF9FFF
0x00006000 - 0x00006FFF
0x00004000 - 0x00004FFF
0x00002000 - 0x00002FFF
0x00000000 - 0x00000FFF
Figure 8.21 Physical and virtual
pages
Chapt
Example 8.13 VIRTUAL ADDRESS TO PHYSICAL ADDRESS
TRANSLATION
Find the physical address of virtual address 0x247C using the virtual memory
system shown in Figure 8.21.
Solution: The 12-bit page offset (0x47C) requires no translation. The remaining
19 bits of the virtual address give the virtual page number, so virtual address
0x247C is found in virtual page 0x2. In Figure 8.21, virtual page 0x2 maps to
physical page 0x7FFF. Thus, virtual address 0x247C maps to physical address
0x7FFF47C.
8.4.2 The Page Table
The processor uses a page table to translate virtual addresses to physical
addresses. Recall that the page table contains an entry for each virtual
page. This entry contains a physical page number and a valid bit. If the
valid bit is 1, the virtual page maps to the physical page specified in the
entry. Otherwise, the virtual page is found on disk.
Because the page table is so large, it is stored in physical memory.
Let us assume for now that it is stored as a contiguous array, as shown
in Figure 8.23. This page table contains the mapping of the memory
system of Figure 8.21. The page table is indexed with the virtual page
number (VPN). For example, entry 5 specifies that virtual page 5 maps
to physical page 1. Entry 6 is invalid (V  0), so virtual page 6 is
located on disk.
Example 8.14 USING THE PAGE TABLE TO PERFORM ADDRESS
TRANSLATION
Find the physical address of virtual address 0x247C using the page table shown
in Figure 8.23.
Solution: Figure 8.24 shows the virtual address to physical address translation
for virtual address 0x247C. The 12-bit page offset requires no translation. The
remaining 19 bits of the virtual address are the virtual page number, 0x2, and
488 CHAPTER EIGHT Memory Systems
PPN Page Offset
11 10 9 ... 2 1 0
VPN Page Offset
Virtual Address
Physical Address
30 29 28 ... 14 13 12
26 25 24 ... 13 12 11 10 9 ... 2 1 0
19
15
12 Translation
Figure 8.22 Translation from
virtual address to physical
address
Page Table
Virtual
Page
Number
7FFFA
00006
00005
7FFFC
7FFFB
7FFFE
7FFFD
7FFFF
00001
00000
00003
00002
00004
V
00007
Physical
Page
Number
0
0
1 0x0000
1 0x7FFE
0
0
0
0
1 0x0001
0
0
1 0x7FFF
0
0
Figure 8.23 The page table for
Figure 8.21
C
The page table can be stored anywhere in physical memory, at the
discretion of the OS. The processor typically uses a dedicated register,
called the page table register, to store the base address of the page table
in physical memory.
To perform a load or store, the processor must first translate the
virtual address to a physical address and then access the data at that
physical address. The processor extracts the virtual page number from
the virtual address and adds it to the page table register to find the
physical address of the page table entry. The processor then reads this
page table entry from physical memory to obtain the physical page
number. If the entry is valid, it merges this physical page number with
the page offset to create the physical address. Finally, it reads or
writes data at this physical address. Because the page table is stored in
physical memory, each load or store involves two physical memory
accesses.
8.4 Virtual Memory 489
0
0
1 0x0000
1 0x7FFE
0
0
0
0
1 0x0001
Page Table
0
0
1 0x7FFF
0
0
V
Virtual
Address 0x00002 47C
Hit
Physical
Page Number
19 12
15 12
Virtual
Page Number
Page
Offset
Physical
Address 0x7FFF 47C
Figure 8.24 Address translation
using the page table
give the index into the page table. The page table maps virtual page 0x2 to
physical page 0x7FFF. So, virtual address 0x247C maps to physical address
0x7FFF47C. The least significant 12 bits are the same in both the physical and
the virtual address.

8.4.3 The Translation Lookaside Buffer
Virtual memory would have a severe performance impact if it required
a page table read on every load or store, doubling the delay of loads
and stores. Fortunately, page table accesses have great temporal locality. The temporal and spatial locality of data accesses and the large
page size mean that many consecutive loads or stores are likely to reference the same page. Therefore, if the processor remembers the last
page table entry that it read, it can probably reuse this translation
without rereading the page table. In general, the processor can keep
the last several page table entries in a small cache called a translation
lookaside buffer (TLB). The processor “looks aside” to find the translation in the TLB before having to access the page table in physical
memory. In real programs, the vast majority of accesses hit in the
TLB, avoiding the time-consuming page table reads from physical
memory.
A TLB is organized as a fully associative cache and typically holds
16 to 512 entries. Each TLB entry holds a virtual page number and its
corresponding physical page number. The TLB is accessed using the
virtual page number. If the TLB hits, it returns the corresponding
physical page number. Otherwise, the processor must read the page
table in physical memory. The TLB is designed to be small enough
that it can be accessed in less than one cycle. Even so, TLBs typically
have a hit rate of greater than 99%. The TLB decreases the number of
memory accesses required for most load or store instructions from
two to one.
Example 8.15 USING THE TLB TO PERFORM ADDRESS TRANSLATION
Consider the virtual memory system of Figure 8.21. Use a two-entry TLB or
explain why a page table access is necessary to translate virtual addresses
0x247C and 0x5FB0 to physical addresses. Suppose the TLB currently holds
valid translations of virtual pages 0x2 and 0x7FFFD.
Solution: Figure 8.25 shows the two-entry TLB with the request for virtual
address 0x247C. The TLB receives the virtual page number of the incoming
address, 0x2, and compares it to the virtual page number of each entry. Entry
0 matches and is valid, so the request hits. The translated physical address is
the physical page number of the matching entry, 0x7FFF, concatenated with
the page offset of the virtual address. As always, the page offset requires no
translation.
The request for virtual address 0x5FB0 misses in the TLB. So, the request is forwarded to the page table for translation.
490 CHAPTER EIGHT Memory Systems

8.4.4 Memory Protection
So far this section has focused on using virtual memory to provide a
fast, inexpensive, large memory. An equally important reason to use virtual memory is to provide protection between concurrently running
programs.
As you probably know, modern computers typically run several programs or processes at the same time. All of the programs are simultaneously present in physical memory. In a well-designed computer system,
the programs should be protected from each other so that no program
can crash or hijack another program. Specifically, no program should be
able to access another program’s memory without permission. This is
called memory protection.
Virtual memory systems provide memory protection by giving each
program its own virtual address space. Each program can use as much
memory as it wants in that virtual address space, but only a portion of the
virtual address space is in physical memory at any given time. Each program can use its entire virtual address space without having to worry
about where other programs are physically located. However, a program
can access only those physical pages that are mapped in its page table. In
this way, a program cannot accidentally or maliciously access another program’s physical pages, because they are not mapped in its page table. In
some cases, multiple programs access common instructions or data. The
operating system adds control bits to each page table entry to determine
which programs, if any, can write to the shared physical pages.
8.4 Virtual Memory 491
Hit1
V
=
15 15
15
=
Hit
1
0
Hit 1 0
Hit
19 19
19
Virtual
Page Number
Physical
Page Number
Entry 1
1 0x7FFFD 0x0000 1 0x00002 0x7FFF
Virtual
Address 0x00002 47C
19 12
Virtual
Page Number
Page
Offset
V
Virtual
Page Number
Physical
Page Number
Entry 0
12 Physical
Address 0x7FFF 47C
TLB
Figure 8.25 Address translation
using a two-entry TLB

8.4.5 Replacement Policies*
Virtual memory systems use write-back and an approximate least recently
used (LRU) replacement policy. A write-through policy, where each write
to physical memory initiates a write to disk, would be impractical. Store
instructions would operate at the speed of the disk instead of the speed of
the processor (milliseconds instead of nanoseconds). Under the writeback policy, the physical page is written back to disk only when it is
evicted from physical memory. Writing the physical page back to disk and
reloading it with a different virtual page is called swapping, so the disk in
a virtual memory system is sometimes called swap space. The processor
swaps out one of the least recently used physical pages when a page fault
occurs, then replaces that page with the missing virtual page. To support
these replacement policies, each page table entry contains two additional
status bits: a dirty bit, D, and a use bit, U.
The dirty bit is 1 if any store instructions have changed the physical
page since it was read from disk. When a physical page is swapped out,
it needs to be written back to disk only if its dirty bit is 1; otherwise, the
disk already holds an exact copy of the page.
The use bit is 1 if the physical page has been accessed recently. As in
a cache system, exact LRU replacement would be impractically complicated. Instead, the OS approximates LRU replacement by periodically
resetting all the use bits in the page table. When a page is accessed, its
use bit is set to 1. Upon a page fault, the OS finds a page with U  0 to
swap out of physical memory. Thus, it does not necessarily replace the
least recently used page, just one of the least recently used pages.
8.4.6 Multilevel Page Tables*
Page tables can occupy a large amount of physical memory. For example, the page table from the previous sections for a 2 GB virtual memory
with 4 KB pages would need 219 entries. If each entry is 4 bytes, the page
table is 219  22 bytes  221 bytes  2 MB.
To conserve physical memory, page tables can be broken up into multiple (usually two) levels. The first-level page table is always kept in physical memory. It indicates where small second-level page tables are stored
in virtual memory. The second-level page tables each contain the actual
translations for a range of virtual pages. If a particular range of translations is not actively used, the corresponding second-level page table can
be swapped out to the hard disk so it does not waste physical memory.
In a two-level page table, the virtual page number is split into two
parts: the page table number and the page table offset, as shown in
Figure 8.26. The page table number indexes the first-level page table,
which must reside in physical memory. The first-level page table entry
gives the base address of the second-level page table or indicates that
492 CHAPTER EIGHT Memory Systems
Cha
it must be fetched from disk when V is 0. The page table offset indexes
the second-level page table. The remaining 12 bits of the virtual address
are the page offset, as before, for a page size of 212  4 KB.
In Figure 8.26 the 19-bit virtual page number is broken into 9 and 10
bits, to indicate the page table number and the page table offset, respectively. Thus, the first-level page table has 29  512 entries. Each of these
512 second-level page tables has 210  1 K entries. If each of the first- and
second-level page table entries is 32 bits (4 bytes) and only two secondlevel page tables are present in physical memory at once, the hierarchical
page table uses only (512  4 bytes)  2  (1 K  4 bytes)  10 KB of
physical memory. The two-level page table requires a fraction of the physical memory needed to store the entire page table (2 MB). The drawback
of a two-level page table is that it adds yet another memory access for
translation when the TLB misses.
Example 8.16 USING A MULTILEVEL PAGE TABLE FOR ADDRESS
TRANSLATION
Figure 8.27 shows the possible contents of the two-level page table from Figure
8.26. The contents of only one second-level page table are shown. Using this
two-level page table, describe what happens on an access to virtual address
0x003FEFB0.
8.4 Virtual Memory 493
First-Level
Page Table
Page Table
Address
210 = 1K entries
29 = 512 entries
Page Table
Number
Page Table
Offset Virtual
Address
V
9
Physical Page
V Number
10
Second-Level
Page Tables
Page
Offset
Figure 8.26 Hierarchical page
tables
Chap
8.5 MEMORY-MAPPED I/O*
Processors also use the memory interface to communicate with
input/output (I/O) devices such as keyboards, monitors, and printers.
A processor accesses an I/O device using the address and data busses in
the same way that it accesses memory.
A portion of the address space is dedicated to I/O devices rather
than memory. For example, suppose that addresses in the range
494 CHAPTER EIGHT Memory Systems
Page Table
Address
Page Table
Number
Page Table
Offset Virtual
Address
V
9
Physical Page
V Number
10
Page
Offset
0
0
0
1 0x40000
Valid1
1 0x2375000
First-Level Page Table
Second-Level Page Tables
0x0 3FE FB0
Valid2 15 12
Physical
Address 0x23F1 FB0
12
0
1 0x7FFE
0
0
0
0
1 0x0073
0
0
1 0x72FC
0
0
0
1 0x00C1
1 0x1003
1 0x23F1
Figure 8.27 Address translation
using a two-level page table
Solution: As always, only the virtual page number requires translation. The most
significant nine bits of the virtual address, 0x0, give the page table number, the
index into the first-level page table. The first-level page table at entry 0x0 indicates that the second-level page table is resident in memory (V  1) and its physical address is 0x2375000.
The next ten bits of the virtual address, 0x3FE, are the page table offset, which
gives the index into the second-level page table. Entry 0 is at the bottom of the
second-level page table, and entry 0x3FF is at the top. Entry 0x3FE in the secondlevel page table indicates that the virtual page is resident in physical memory
(V  1) and that the physical page number is 0x23F1. The physical page number
is concatenated with the page offset to form the physical address, 0x23F1FB0.
Ch
0xFFFF0000 to 0xFFFFFFFF are used for I/O. Recall from Section 6.6.1
that these addresses are in a reserved portion of the memory map. Each
I/O device is assigned one or more memory addresses in this range. A
store to the specified address sends data to the device. A load receives
data from the device. This method of communicating with I/O devices is
called memory-mapped I/O.
In a system with memory-mapped I/O, a load or store may access
either memory or an I/O device. Figure 8.28 shows the hardware needed to
support two memory-mapped I/O devices. An address decoder determines
which device communicates with the processor. It uses the Address and
MemWrite signals to generate control signals for the rest of the hardware.
The ReadData multiplexer selects between memory and the various I/O
devices. Write-enabled registers hold the values written to the I/O devices.
Example 8.17 COMMUNICATING WITH I/O DEVICES
Suppose I/O Device 1 in Figure 8.28 is assigned the memory address 0xFFFFFFF4.
Show the MIPS assembly code for writing the value 7 to I/O Device 1 and for
reading the output value from I/O Device 1.
Solution: The following MIPS assembly code writes the value 7 to I/O Device 1.2
addi $t0, $0, 7
sw $t0, 0xFFF4($0)
The address decoder asserts WE1 because the address is 0xFFFFFFF4 and
MemWrite is TRUE. The value on the WriteData bus, 7, is written into the register connected to the input pins of I/O Device 1.
8.5 Memory-Mapped I/O 495
Processor Memory Address
MemWrite
WriteData
I/O ReadData
Device 1
I/O
Device 2
CLK
EN
EN
Address Decoder WE1 WEM
RDsel1:0
WE2
WE
CLK
00
01
10
CLK
Figure 8.28 Support hardware
for memory-mapped I/O
Some architectures, notably
IA-32, use specialized instructions instead of memorymapped I/O to communicate
with I/O devices. These
instructions are of the following form, where devicel and
device2 are the unique ID of
the peripheral device:
lwio $t0, device1
swio $t0, device2
This type of communication
with I/O devices is called programmed I/O.
2 Recall that the 16-bit immediate 0xFFF4 is sign-extended to the 32-bit value
0xFFFFFFF4.

To read from I/O Device 1, the processor performs the following MIPS assembly
code.
lw $t1, 0xFFF4($0)
The address decoder sets RDsel1:0 to 01, because it detects the address
0xFFFFFFF4 and MemWrite is FALSE. The output of I/O Device 1 passes
through the multiplexer onto the ReadData bus and is loaded into $t1 in the
processor.
Software that communicates with an I/O device is called a device
driver. You have probably downloaded or installed device drivers for
your printer or other I/O device. Writing a device driver requires detailed
knowledge about the I/O device hardware. Other programs call functions in the device driver to access the device without having to understand the low-level device hardware.
To illustrate memory-mapped I/O hardware and software, the rest of
this section describes interfacing a commercial speech synthesizer chip to
a MIPS processor.
Speech Synthesizer Hardware
The Radio Shack SP0256 speech synthesizer chip generates robot-like
speech. Words are composed of one or more allophones, the fundamental
units of sound. For example, the word “hello” uses five allophones represented by the following symbols in the SP0256 speech chip: HH1 EH LL
AX OW. The speech synthesizer uses 6-bit codes to represent 64 different
allophones that appear in the English language. For example, the five
allophones for the word “hello” correspond to the hexadecimal values
0x1B, 0x07, 0x2D, 0x0F, 0x20, respectively. The processor sends a series
of allophones to the speech synthesizer, which drives a speaker to blabber
the sounds.
Figure 8.29 shows the pinout of the SP0256 speech chip. The I/O
pins highlighted in blue are used to interface with the MIPS processor to
produce speech. Pins A6:1 receive the 6-bit allophone encoding from the
processor. The allophone sound is produced on the Digital Out pin. The
Digital Out signal is first amplified and then sent to a speaker. The other
two highlighted pins, SBY and
_____
ALD, are status and control pins. When
the SBY output is 1, the speech chip is standing by and is ready to
receive a new allophone. On the falling edge of the address load input _____
ALD, the speech chip reads the allophone specified by A6:1. Other pins,
such as power and ground (VDD and VSS) and the clock (OSC1), must
be connected as shown but are not driven by the processor.
Figure 8.30 shows the speech synthesizer interfaced to the MIPS
processor. The processor uses three memory-mapped I/O addresses to
communicate with the speech synthesizer. We arbitrarily have chosen
496 CHAPTER EIGHT Memory Systems
See www.speechchips.com
for more information about
the SP0256 and the allophone
encodings.

that the A6:1 port is mapped to address 0xFFFFFF00, _____
ALD to
0xFFFFFF04, and SBY to 0xFFFFFF08. Although the WriteData bus is
32 bits, only the least significant 6 bits are used for A6:1, and the least
significant bit is used for
_____
ALD; the other bits are ignored. Similarly,
SBY is read on the least significant bit of the ReadData bus; the other
bits are 0.
8.5 Memory-Mapped I/O 497
1
2
3
4
5
6
7
8
9
10
11
12
13
14
28
27
26
25
24
23
22
21
20
19
18
17
16
15
OSC 2
OSC 1
ROM Clock
SBY Reset
Digital Out
VD1
Test
Ser In
ALD
SE
A1
A2
A3
A4
VSS
Reset
ROM Disable
C1
C2
C3
VDD
SBY To
Processor From
Processor
From
Processor
From
Processor
LRQ
A8
A7
Ser Out
A6
A5
SPO256
3.12 MHz
A
Amplifier Speaker
Processor Memory Address
MemWrite
WriteData
ReadData
SP0256
CLK
EN
EN
Address Decoder
WE
CLK CLK
5:0
0
A6:1
ALD
SBY
0
1
31:1
WE2
WE1
WEM
RDsel
Figure 8.29 SPO256 speech synthesizer chip pinout
Figure 8.30 Hardware for
driving the SPO256 speech
synthesizer

Speech Synthesizer Device Driver
The device driver controls the speech synthesizer by sending an appropriate series of allophones over the memory-mapped I/O interface. It follows
the protocol expected by the SPO256 chip, given below:
 Set
_____
ALD to 1
 Wait until the chip asserts SBY to indicate that it is finished speaking
the previous allophone and is ready for the next
 Write a 6-bit allophone to A6:1
 Reset
_____
ALD to 0 to initiate speech
This sequence can be repeated for any number of allophones. The
MIPS assembly in Code Example 8.1 writes five allophones to the speech
chip. The allophone encodings are stored as 32-bit values in a five-entry
array starting at memory address 0x10000000.
The assembly code in Code Example 8.1 polls, or repeatedly checks,
the SBY signal to determine when the speech chip is ready to receive a
new allophone. The code functions correctly but wastes valuable processor cycles that could be used to perform useful work. Instead of polling,
the processor could use an interrupt connected to SBY. When SBY rises,
the processor stops what it is doing and jumps to code that handles the
interrupt. In the case of the speech synthesizer, the interrupt handler
498 CHAPTER EIGHT Memory Systems
init:
addi $t1, $0, 1 # $t1  1 (value to write to ALD
___
)
addi $t2, $0, 20 # $t2  array size * 4
lui $t3, 0x1000 # $t3  array base address
addi $t4, $0, 0 # $t4  0 (array index)
start:
sw $t1, 0xFF04($0) # ALD
___
=1
loop:
lw $t5, 0xFF08($0) # $t5  SBY
beq $0, $t5, loop # loop until SBY  1
add $t5, $t3, $t4 # $t5  address of allophone
lw $t5, 0($t5) # $t5  allophone
sw $t5, 0xFF00($0) # A6:1  allophone
sw $0, 0xFF04($0) # ALD
___
= 0 to initiate speech
addi $t4, $t4, 4 # increment array index
beq $t4, $t2, done # last allophone in array?
j start # repeat
done:
Code Example 8.1 SPEECH CHIP DEVICE DRIVER
Chapter 08.qxd
would send the next allophone, then let the processor resume what it was
doing before the interrupt. As described in Section 6.7.2, the processor
handles interrupts like any other exception.
8.6 REAL-WORLD PERSPECTIVE: IA-32 MEMORY
AND I/O SYSTEMS*
As processors get faster, they need ever more elaborate memory hierarchies
to keep a steady supply of data and instructions flowing. This section
describes the memory systems of IA-32 processors to illustrate the progression. Section 7.9 contained photographs of the processors, highlighting the
on-chip caches. IA-32 also has an unusual programmed I/O system that
differs from the more common memory-mapped I/O.
8.6.1 IA-32 Cache Systems
The 80386, initially produced in 1985, operated at 16 MHz. It lacked a
cache, so it directly accessed main memory for all instructions and data.
Depending on the speed of the memory, the processor might get an
immediate response, or it might have to pause for one or more cycles for
the memory to react. These cycles are called wait states, and they
increase the CPI of the processor. Microprocessor clock frequencies have
increased by at least 25% per year since then, whereas memory latency
has scarcely diminished. The delay from when the processor sends an
address to main memory until the memory returns the data can now
exceed 100 processor clock cycles. Therefore, caches with a low miss
rate are essential to good performance. Table 8.5 summarizes the evolution of cache systems on Intel IA-32 processors.
The 80486 introduced a unified write-through cache to hold both
instructions and data. Most high-performance computer systems also provided a larger second-level cache on the motherboard using commercially
available SRAM chips that were substantially faster than main memory.
The Pentium processor introduced separate instruction and data
caches to avoid contention during simultaneous requests for data and
instructions. The caches used a write-back policy, reducing the communication with main memory. Again, a larger second-level cache (typically
256–512 KB) was usually offered on the motherboard.
The P6 series of processors (Pentium Pro, Pentium II, and Pentium
III) were designed for much higher clock frequencies. The second-level
cache on the motherboard could not keep up, so it was moved closer to
the processor to improve its latency and throughput. The Pentium Pro
was packaged in a multichip module (MCM) containing both the processor chip and a second-level cache chip, as shown in Figure 8.31. Like the
Pentium, the processor had separate 8-KB level 1 instruction and data
8.6 Real-World Perspective: IA-32 Memory and I/O Systems 499

caches. However, these caches were nonblocking, so that the out-oforder processor could continue executing subsequent cache accesses even
if the cache missed a particular access and had to fetch data from main
memory. The second-level cache was 256 KB, 512 KB, or 1 MB in size
and could operate at the same speed as the processor. Unfortunately, the
MCM packaging proved too expensive for high-volume manufacturing.
Therefore, the Pentium II was sold in a lower-cost cartridge containing
the processor and the second-level cache. The level 1 caches were doubled in size to compensate for the fact that the second-level cache operated at half the processor’s speed. The Pentium III integrated a full-speed
second-level cache directly onto the same chip as the processor. A cache
on the same chip can operate at better latency and throughput, so it is
substantially more effective than an off-chip cache of the same size.
The Pentium 4 offered a nonblocking level 1 data cache. It switched
to a trace cache to store instructions after they had been decoded into
micro-ops, avoiding the delay of redecoding each time instructions were
fetched from the cache.
The Pentium M design was adapted from the Pentium III. It further
increased the level 1 caches to 32 KB each and featured a 1- to 2-MB level
2 cache. The Core Duo contains two modified Pentium M processors and
500 CHAPTER EIGHT Memory Systems
Table 8.5 Evolution of Intel IA-32 microprocessor memory systems
Frequency Level 1 Level 1
Processor Year (MHz) Data Cache Instruction Cache Level 2 Cache
80386 1985 16–25 none none none
80486 1989 25–100 8 KB unified none on chip
Pentium 1993 60–300 8 KB 8 KB none on chip
Pentium Pro 1995 150–200 8 KB 8 KB 256 KB–1 MB
on MCM
Pentium II 1997 233–450 16 KB 16 KB 256–512 KB
on cartridge
Pentium III 1999 450–1400 16 KB 16 KB 256–512 KB
on chip
Pentium 4 2001 1400–3730 8–16 KB 12 K op 256 KB–2 MB
trace cache on chip
Pentium M 2003 900–2130 32 KB 32 KB 1–2 MB
on chip
Core Duo 2005 1500–2160 32 KB/core 32 KB/core 2 MB shared
on chip

a shared 2-MB cache on one chip. The shared cache is used for communication between the processors: one can write data to the cache, and the
other can read it.
8.6.2 IA-32 Virtual Memory
IA-32 processors operate in either real mode or protected mode. Real
mode is backward compatible with the original 8086. It only uses
20 bits of addresses, limiting memory to 1 MB, and it does not allow
virtual memory.
Protected mode was introduced with the 80286 and extended to
32-bit addresses with the 80386. It supports virtual memory with 4-KB
pages. It also provides memory protection so that one program cannot
access the pages belonging to other programs. Hence, a buggy or malicious program cannot crash or corrupt other programs. All modern
operating systems now use protected mode.
A 32-bit address permits up to 4 GB of memory. Processors since
the Pentium Pro have bumped the memory capacity to 64 GB using a
8.6 Real-World Perspective: IA-32 Memory and I/O Systems 501
Figure 8.31 Pentium Pro
multichip module with processor
(left) and 256-KB cache (right)
in a pin grid array (PGA)
package (Courtesy Intel.)
Although memory protection
became available in the hardware in the early 1980s,
Microsoft Windows took
almost 15 years to take
advantage of the feature and
prevent bad programs from
crashing the entire computer.
Until the release of Windows
2000, consumer versions of
Windows were notoriously
unstable. The lag between
hardware features and software support can be
extremely long.

technique called physical address extension. Each process uses 32-bit
addresses. The virtual memory system maps these addresses onto a
larger 36-bit virtual memory space. It uses different page tables for
each process, so that each process can have its own address space of up
to 4 GB.
8.6.3 IA-32 Programmed I/O
Most architectures use memory-mapped I/O, described in Section 8.5,
in which programs access I/O devices by reading and writing memory
locations. IA-32 uses programmed I/O, in which special IN and OUT
instructions are used to read and write I/O devices. IA-32 defines 216
I/O ports. The IN instruction reads one, two, or four bytes from the
port specified by DX into AL, AX, or EAX. OUT is similar, but writes
the port.
Connecting a peripheral device to a programmed I/O system is similar to connecting it to a memory-mapped system. When accessing an I/O
port, the processor sends the port number rather than the memory
address on the 16 least significant bits of the address bus. The device
reads or writes data from the data bus. The major difference is that the
processor also produces an M/IO
___ signal. When M/IO
___  1, the processor
is accessing memory. When it is 0, the process is accessing one of the I/O
devices. The address decoder must also look at M/IO
___ to generate the
appropriate enables for main memory and for the I/O devices. I/O
devices can also send interrupts to the processor to indicate that they are
ready to communicate.
8.7 SUMMARY
Memory system organization is a major factor in determining computer
performance. Different memory technologies, such as DRAM, SRAM,
and hard disks, offer trade-offs in capacity, speed, and cost. This chapter
introduced cache and virtual memory organizations that use a hierarchy
of memories to approximate an ideal large, fast, inexpensive memory.
Main memory is typically built from DRAM, which is significantly
slower than the processor. A cache reduces access time by keeping commonly used data in fast SRAM. Virtual memory increases the memory
capacity by using a hard disk to store data that does not fit in the main
memory. Caches and virtual memory add complexity and hardware to a
computer system, but the benefits usually outweigh the costs. All modern personal computers use caches and virtual memory. Most processors
also use the memory interface to communicate with I/O devices. This is
called memory-mapped I/O. Programs use load and store operations to
access the I/O devices.
502 CHAPTER EIGHT Memory Systems
C
EPILOGUE
This chapter brings us to the end of our journey together into the realm
of digital systems. We hope this book has conveyed the beauty and thrill
of the art as well as the engineering knowledge. You have learned to
design combinational and sequential logic using schematics and hardware description languages. You are familiar with larger building blocks
such as multiplexers, ALUs, and memories. Computers are one of the
most fascinating applications of digital systems. You have learned how
to program a MIPS processor in its native assembly language and how to
build the processor and memory system using digital building blocks.
Throughout, you have seen the application of abstraction, discipline,
hierarchy, modularity, and regularity. With these techniques, we have
pieced together the puzzle of a microprocessor’s inner workings. From
cell phones to digital television to Mars rovers to medical imaging systems, our world is an increasingly digital place.
Imagine what Faustian bargain Charles Babbage would have made
to take a similar journey a century and a half ago. He merely aspired
to calculate mathematical tables with mechanical precision. Today’s
digital systems are yesterday’s science fiction. Might Dick Tracy have
listened to iTunes on his cell phone? Would Jules Verne have launched
a constellation of global positioning satellites into space? Could
Hippocrates have cured illness using high-resolution digital images of
the brain? But at the same time, George Orwell’s nightmare of ubiquitous government surveillance becomes closer to reality each day. And
rogue states develop nuclear weapons using laptop computers more
powerful than the room-sized supercomputers that simulated Cold War
bombs. The microprocessor revolution continues to accelerate. The
changes in the coming decades will surpass those of the past. You now
have the tools to design and build these new systems that will shape
our future. With your newfound power comes profound responsibility.
We hope that you will use it, not just for fun and riches, but also for
the benefit of humanity.
Epilogue 503

Exercises
Exercise 8.1 In less than one page, describe four everyday activities that exhibit
temporal or spatial locality. List two activities for each type of locality, and be
specific.
Exercise 8.2 In one paragraph, describe two short computer applications that
exhibit temporal and/or spatial locality. Describe how. Be specific.
Exercise 8.3 Come up with a sequence of addresses for which a direct mapped
cache with a size (capacity) of 16 words and block size of 4 words outperforms
a fully associative cache with least recently used (LRU) replacement that has the
same capacity and block size.
Exercise 8.4 Repeat Exercise 8.3 for the case when the fully associative cache
outperforms the direct mapped cache.
Exercise 8.5 Describe the trade-offs of increasing each of the following cache
parameters while keeping the others the same:
(a) block size
(b) associativity
(c) cache size
Exercise 8.6 Is the miss rate of a two-way set associative cache always, usually,
occasionally, or never better than that of a direct mapped cache of the same
capacity and block size? Explain.
Exercise 8.7 Each of the following statements pertains to the miss rate of caches.
Mark each statement as true or false. Briefly explain your reasoning; present a
counterexample if the statement is false.
(a) A two-way set associative cache always has a lower miss rate than a direct
mapped cache with the same block size and total capacity.
(b) A 16-KB direct mapped cache always has a lower miss rate than an 8-KB
direct mapped cache with the same block size.
(c) An instruction cache with a 32-byte block size usually has a lower miss rate
than an instruction cache with an 8-byte block size, given the same degree
of associativity and total capacity.
504 CHAPTER EIGHT Memory Systems

Exercise 8.8 A cache has the following parameters: b, block size given in
numbers of words; S, number of sets; N, number of ways; and A, number of
address bits.
(a) In terms of the parameters described, what is the cache capacity, C?
(b) In terms of the parameters described, what is the total number of bits
required to store the tags?
(c) What are S and N for a fully associative cache of capacity C words with
block size b?
(d) What is S for a direct mapped cache of size C words and block size b?
Exercise 8.9 A 16-word cache has the parameters given in Exercise 8.8. Consider
the following repeating sequence of lw addresses (given in hexadecimal):
40 44 48 4C 70 74 78 7C 80 84 88 8C 90 94 98 9C 0 4 8 C 10 14 18 1C 20
Assuming least recently used (LRU) replacement for associative caches,
determine the effective miss rate if the sequence is input to the following
caches, ignoring startup effects (i.e., compulsory misses).
(a) direct mapped cache, S  16, b  1 word
(b) fully associative cache, N  16, b  1 word
(c) two-way set associative cache, S  8, b  1 word
(d) direct mapped cache, S  8, b  2 words
Exercise 8.10 Suppose you are running a program with the following data access
pattern. The pattern is executed only once.
0x0, 0x8, 0x10, 0x18, 0x20, 0x28
(a) If you use a direct mapped cache with a cache size of 1 KB and a block size
of 8 bytes (2 words), how many sets are in the cache?
(b) With the same cache and block size as in part (a), what is the miss rate of
the direct mapped cache for the given memory access pattern?
(c) For the given memory access pattern, which of the following would decrease
the miss rate the most? (Cache capacity is kept constant.) Circle one.
(i) Increasing the degree of associativity to 2.
(ii) Increasing the block size to 16 bytes.
Exercises 505
Chapter 
(iii) Either (i) or (ii).
(iv) Neither (i) nor (ii).
Exercise 8.11 You are building an instruction cache for a MIPS processor. It has
a total capacity of 4C  2c2 bytes. It is N  2n-way set associative (N ≥ 8),
with a block size of b  2b bytes (b ≥ 8). Give your answers to the following
questions in terms of these parameters.
(a) Which bits of the address are used to select a word within a block?
(b) Which bits of the address are used to select the set within the cache?
(c) How many bits are in each tag?
(d) How many tag bits are in the entire cache?
Exercise 8.12 Consider a cache with the following parameters:
N (associativity)  2, b (block size)  2 words, W (word size)  32 bits,
C (cache size)  32 K words, A (address size)  32 bits. You need consider only
word addresses.
(a) Show the tag, set, block offset, and byte offset bits of the address. State how
many bits are needed for each field.
(b) What is the size of all the cache tags in bits?
(c) Suppose each cache block also has a valid bit (V) and a dirty bit (D). What
is the size of each cache set, including data, tag, and status bits?
(d) Design the cache using the building blocks in Figure 8.32 and a small
number of two-input logic gates. The cache design must include tag storage,
data storage, address comparison, data output selection, and any other parts
you feel are relevant. Note that the multiplexer and comparator blocks may
be any size (n or p bits wide, respectively), but the SRAM blocks must be
16 K  4 bits. Be sure to include a neatly labeled block diagram.
506 CHAPTER EIGHT Memory Systems
16K × 4
SRAM
14
4
=
p p
0
1
n
n
n
Figure 8.32 Building blocks
Chapter 
Exercise 8.13 You’ve joined a hot new Internet startup to build wrist watches
with a built-in pager and Web browser. It uses an embedded processor with a
multilevel cache scheme depicted in Figure 8.33. The processor includes a small
on-chip cache in addition to a large off-chip second-level cache. (Yes, the watch
weighs 3 pounds, but you should see it surf!)
Assume that the processor uses 32-bit physical addresses but accesses data only
on word boundaries. The caches have the characteristics given in Table 8.6. The
DRAM has an access time of tm and a size of 512 MB.
(a) For a given word in memory, what is the total number of locations in which
it might be found in the on-chip cache and in the second-level cache?
(b) What is the size, in bits, of each tag for the on-chip cache and the secondlevel cache?
(c) Give an expression for the average memory read access time. The caches are
accessed in sequence.
(d) Measurements show that, for a particular problem of interest, the on-chip
cache hit rate is 85% and the second-level cache hit rate is 90%. However,
when the on-chip cache is disabled, the second-level cache hit rate shoots up
to 98.5%. Give a brief explanation of this behavior.
Exercises 507
Figure 8.33 Computer system
CPU Level 1
Cache
Level 2
Cache
Main
Memory
Processor Chip
Table 8.6 Memory characteristics
Characteristic On-chip Cache Off-chip Cache
organization four-way set associative direct mapped
hit rate A B
access time ta tb
block size 16 bytes 16 bytes
number of blocks 512 256K

Exercise 8.14 This chapter described the least recently used (LRU) replacement
policy for multiway associative caches. Other, less common, replacement policies
include first-in-first-out (FIFO) and random policies. FIFO replacement evicts the
block that has been there the longest, regardless of how recently it was accessed.
Random replacement randomly picks a block to evict.
(a) Discuss the advantages and disadvantages of each of these replacement
policies.
(b) Describe a data access pattern for which FIFO would perform better than
LRU.
Exercise 8.15 You are building a computer with a hierarchical memory system
that consists of separate instruction and data caches followed by main memory.
You are using the MIPS multicycle processor from Figure 7.41 running at
1 GHz.
(a) Suppose the instruction cache is perfect (i.e., always hits) but the data cache
has a 5% miss rate. On a cache miss, the processor stalls for 60 ns to access
main memory, then resumes normal operation. Taking cache misses into
account, what is the average memory access time?
(b) How many clock cycles per instruction (CPI) on average are required for
load and store word instructions considering the non-ideal memory system?
(c) Consider the benchmark application of Example 7.7 that has 25% loads,
10% stores, 11% branches, 2% jumps, and 52% R-type instructions.3
Taking the non-ideal memory system into account, what is the average CPI
for this benchmark?
(d) Now suppose that the instruction cache is also non-ideal and has a 7% miss
rate. What is the average CPI for the benchmark in part (c)? Take into
account both instruction and data cache misses.
Exercise 8.16 If a computer uses 64-bit virtual addresses, how much virtual
memory can it access? Note that 240 bytes  1 terabyte, 250 bytes  1 petabyte,
and 260 bytes  1 exabyte.
Exercise 8.17 A supercomputer designer chooses to spend $1 million on DRAM
and the same amount on hard disks for virtual memory. Using the prices from
Figure 8.4, how much physical and virtual memory will the computer have?
How many bits of physical and virtual addresses are necessary to access this
memory?
508 CHAPTER EIGHT Memory Systems
3 Data from Patterson and Hennessy, Computer Organization and Design, 3rd Edition,
Morgan Kaufmann, 2005. Used with permission.
Cha
Exercise 8.18 Consider a virtual memory system that can address a total of 232
bytes. You have unlimited hard disk space, but are limited to only 8 MB of
semiconductor (physical) memory. Assume that virtual and physical pages are
each 4 KB in size.
(a) How many bits is the physical address?
(b) What is the maximum number of virtual pages in the system?
(c) How many physical pages are in the system?
(d) How many bits are the virtual and physical page numbers?
(e) Suppose that you come up with a direct mapped scheme that maps virtual
pages to physical pages. The mapping uses the least significant bits of the
virtual page number to determine the physical page number. How many
virtual pages are mapped to each physical page? Why is this “direct
mapping” a bad plan?
(f) Clearly, a more flexible and dynamic scheme for translating virtual
addresses into physical addresses is required than the one described in part
(d). Suppose you use a page table to store mappings (translations from
virtual page number to physical page number). How many page table
entries will the page table contain?
(g) Assume that, in addition to the physical page number, each page table entry
also contains some status information in the form of a valid bit (V) and a
dirty bit (D). How many bytes long is each page table entry? (Round up to
an integer number of bytes.)
(h) Sketch the layout of the page table. What is the total size of the page table
in bytes?
Exercise 8.19 You decide to speed up the virtual memory system of Exercise
8.18 by using a translation lookaside buffer (TLB). Suppose your memory
system has the characteristics shown in Table 8.7. The TLB and cache miss
rates indicate how often the requested entry is not found. The main memory
miss rate indicates how often page faults occur.
Exercises 509
Table 8.7 Memory characteristics
Memory Unit Access Time (Cycles) Miss Rate
TLB 1 0.05%
cache 1 2%
main memory 100 0.0003%
disk 1,000,000 0%

(a) What is the average memory access time of the virtual memory system
before and after adding the TLB? Assume that the page table is always
resident in physical memory and is never held in the data cache.
(b) If the TLB has 64 entries, how big (in bits) is the TLB? Give numbers for
data (physical page number), tag (virtual page number), and valid bits of
each entry. Show your work clearly.
(c) Sketch the TLB. Clearly label all fields and dimensions.
(d) What size SRAM would you need to build the TLB described in part (c)?
Give your answer in terms of depth  width.
Exercise 8.20 Suppose the MIPS multicycle processor described in Section 7.4
uses a virtual memory system.
(a) Sketch the location of the TLB in the multicycle processor schematic.
(b) Describe how adding a TLB affects processor performance.
Exercise 8.21 The virtual memory system you are designing uses a single-level
page table built from dedicated hardware (SRAM and associated logic).
It supports 25-bit virtual addresses, 22-bit physical addresses, and 216-byte
(64 KB) pages. Each page table entry contains a physical page number, a valid
bit (V) and a dirty bit (D).
(a) What is the total size of the page table, in bits?
(b) The operating system team proposes reducing the page size from 64 to
16 KB, but the hardware engineers on your team object on the grounds of
added hardware cost. Explain their objection.
(c) The page table is to be integrated on the processor chip, along with the
on-chip cache. The on-chip cache deals only with physical (not virtual)
addresses. Is it possible to access the appropriate set of the on-chip cache
concurrently with the page table access for a given memory access? Explain
briefly the relationship that is necessary for concurrent access to the cache
set and page table entry.
(d) Is it possible to perform the tag comparison in the on-chip cache
concurrently with the page table access for a given memory access?
Explain briefly.
Exercise 8.22 Describe a scenario in which the virtual memory system might
affect how an application is written. Be sure to include a discussion of how
the page size and physical memory size affect the performance of the
application.
510 CHAPTER EIGHT Memory Systems

Exercise 8.23 Suppose you own a personal computer (PC) that uses 32-bit
virtual addresses.
(a) What is the maximum amount of virtual memory space each program
can use?
(b) How does the size of your PC’s hard disk affect performance?
(c) How does the size of your PC’s physical memory affect performance?
Exercise 8.24 Use MIPS memory-mapped I/O to interact with a user. Each time
the user presses a button, a pattern of your choice displays on five light-emitting
diodes (LEDs). Suppose the input button is mapped to address 0xFFFFFF10 and
the LEDs are mapped to address 0xFFFFFF14. When the button is pushed, its
output is 1; otherwise it is 0.
(a) Write MIPS code to implement this functionality.
(b) Draw a schematic similar to Figure 8.30 for this memory-mapped I/O
system.
(c) Write HDL code to implement the address decoder for your memorymapped I/O system.
Exercise 8.25 Finite state machines (FSMs), like the ones you built in Chapter 3,
can also be implemented in software.
(a) Implement the traffic light FSM from Figure 3.25 using MIPS assembly
code. The inputs (TA and TB) are memory-mapped to bit 1 and bit 0,
respectively, of address 0xFFFFF000. The two 3-bit outputs (LA and LB)
are mapped to bits 0–2 and bits 3–5, respectively, of address 0xFFFFF004.
Assume one-hot output encodings for each light, LA and LB; red is 100,
yellow is 010, and green is 001.
(b) Draw a schematic similar to Figure 8.30 for this memory-mapped I/O
system.
(c) Write HDL code to implement the address decoder for your memorymapped I/O system.
Exercises 511

Interview Questions
The following exercises present questions that have been asked on
interviews.
Question 8.1 Explain the difference between direct mapped, set associative, and
fully associative caches. For each cache type, describe an application for which
that cache type will perform better than the other two.
Question 8.2 Explain how virtual memory systems work.
Question 8.3 Explain the advantages and disadvantages of using a virtual
memory system.
Question 8.4 Explain how cache performance might be affected by the virtual
page size of a memory system.
Question 8.5 Can addresses used for memory-mapped I/O be cached? Explain
why or why not.
512 CHAPTER EIGHT Memory Systems



A
A.1 Introduction
A.2 74xx Logic
A.3 Programmable Logic
A.4 Application-Specific
Integrated Circuits
A.5 Data Sheets
A.6 Logic Families
A.7 Packaging and Assembly
A.8 Transmission Lines
A.9 Economics
Digital System Implementation
A.1 INTRODUCTION
This appendix introduces practical issues in the design of digital systems. The material in this appendix is not necessary for understanding
the rest of the book. However, it seeks to demystify the process of
building real digital systems. Moreover, we believe that the best way to
understand digital systems is to build and debug them yourself in the
laboratory.
Digital systems are usually built using one or more chips. One strategy is to connect together chips containing individual logic gates or
larger elements such as arithmetic/logical units (ALUs) or memories.
Another is to use programmable logic, which contains generic arrays of
circuitry that can be programmed to perform specific logic functions. Yet
a third is to design a custom integrated circuit containing the specific
logic necessary for the system. These three strategies offer trade-offs in
cost, speed, power consumption, and design time that are explored in
the following sections. This appendix also examines the physical packaging and assembly of circuits, the transmission lines that connect the
chips, and the economics of digital systems.
A.2 74XX LOGIC
In the 1970s and 1980s, many digital systems were built from simple
chips, each containing a handful of logic gates. For example, the 7404
chip contains six NOT gates, the 7408 contains four AND gates, and the
7474 contains two flip-flops. These chips are collectively referred to as
74xx-series logic. They were sold by many manufacturers, typically for
10 to 25 cents per chip. These chips are now largely obsolete, but they
are still handy for simple digital systems or class projects, because they
are so inexpensive and easy to use. 74xx-series chips are commonly sold
in 14-pin dual inline packages (DIPs).
515
74LS04 inverter chip in a
14-pin dual inline package.
The part number is on the
first line. LS indicates the
logic family (see Section A.6).
The N suffix indicates a DIP
package. The large S is the
logo of the manufacturer,
Signetics. The bottom two
lines of gibberish are codes
indicating the batch in which
the chip was manufactured.

A.2.1 Logic Gates
Figure A.1 shows the pinout diagrams for a variety of popular 74xx-series
chips containing basic logic gates. These are sometimes called small-scale
integration (SSI) chips, because they are built from a few transistors. The
14-pin packages typically have a notch at the top or a dot on the top left
to indicate orientation. Pins are numbered starting with 1 in the upper left
and going counterclockwise around the package. The chips need to receive
power (VDD  5 V) and ground (GND  0 V) at pins 14 and 7, respectively. The number of logic gates on the chip is determined by the number
of pins. Note that pins 3 and 11 of the 7421 chip are not connected (NC)
to anything. The 7474 flip-flop has the usual D, CLK, and Q terminals. It
also has a complementary output, . Moreover, it receives asynchronous
set (also called preset, or PRE) and reset (also called clear, or CLR) signals. These are active low; in other words, the flop sets when ,
resets when , and operates normally when .
A.2.2 Other Functions
The 74xx series also includes somewhat more complex logic functions,
including those shown in Figures A.2 and A.3. These are called mediumscale integration (MSI) chips. Most use larger packages to accommodate
more inputs and outputs. Power and ground are still provided at the
upper right and lower left, respectively, of each chip. A general functional description is provided for each chip. See the manufacturer’s data
sheets for complete descriptions.
A.3 PROGRAMMABLE LOGIC
Programmable logic consists of arrays of circuitry that can be configured
to perform specific logic functions. We have already introduced three
forms of programmable logic: programmable read only memories
(PROMs), programmable logic arrays (PLAs), and field programmable
gate arrays (FPGAs). This section shows chip implementations for each of
these. Configuration of these chips may be performed by blowing on-chip
fuses to connect or disconnect circuit elements. This is called one-time
programmable (OTP) logic because, once a fuse is blown, it cannot be
restored. Alternatively, the configuration may be stored in a memory that
can be reprogrammed at will. Reprogrammable logic is convenient in the
laboratory, because the same chip can be reused during development.
A.3.1 PROMs
As discussed in Section 5.5.7, PROMs can be used as lookup tables. A
2N-word  M-bit PROM can be programmed to perform any combinational function of N inputs and M outputs. Design changes simply
CLR  0 PRE  CLR  1
PRE  0
Q
516 APPENDIX A Digital System Implementation
Appendi
A.3 Programmable Logic 517
Q D
Q
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7400 NAND
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1Y VDD
1A
1B
2Y
2A
2B
4Y
4B
4A
3Y
3B
3A
7402 NOR
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1Y
2A
2Y
3A
3Y
6A
6Y
5A
5Y
4A
4Y
7404 NOT
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
2A
2B
2C
2Y
1C
1Y
3C
3B
3A
3Y
7411 AND3
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7408 AND
1
2
3
4
5
6
7
14
13
12
11
10
9
8
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7486 XOR
GND
1
2
3
4
5
6
7
14
13
12
11
10
9
8
1A VDD
1B
1Y
2A
2B
2Y
4B
4A
4Y
3B
3A
3Y
7432 OR
GND
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
1A VDD
1B
1C
1D
1Y
2D
2C
NC
2B
2A
2Y
7421 AND4
NC
1
2
3
4
5
6
7
14
13
12
11
10
9
8 GND
VDD
1D
1Q
7474 FLOP
1CLK
1CLR
1PRE
1Q
2CLR
2D
2PRE
2Q
2Q
2CLK
reset
set
D Q
Q reset
set
Figure A.1 Common 74xx-series logic gates

518 APPENDIX A Digital System Implementation
CLR
CLK
D0
D1
D2
D3
ENP
GND
74161/163 Counter
VDD
Q0
RCO
LOAD
ENT
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
Q1
Q2
Q3
always @(posedge CLK) // 74163
 if (~CLRb) Q <= 4'b0000;
 else if (~LOADb) Q <= D;
 else if (ENP & ENT) Q <= Q+1;
assign RCO = (Q == 4'b1111) & ENT;
4-bit Counter
CLK: clock
Q3:0: counter output
D3:0: parallel input
CLRb: async reset (161)
sync reset (163)
LOADb: load Q from D
ENP, ENT: enables
RCO: ripple carry out
1G
S1
1D3
1D2
1D1
1D0
1Y
GND
74153 4 :1 Mux
VDD
2G
S0
2D3
2D2
2D1
2D0
2Y
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
always @(1Gb, S, 1D)
 if (1Gb) 1Y = 0;
 else 1Y = 1D[S];
always @(2Gb, S, 2D)
 if (2Gb) 2Y = 0;
 else 2Y = 2D[S];
Two 4:1 Multiplexers
D3:0: data
S1:0: select
Y: output
Gb: enable
A0
A1
A2
G2A
G2B
G1
Y7
GND
74138 3:8 Decoder
VDD
Y0
Y1
Y2
Y3
Y4
Y5
Y6
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
3:8 Decoder
A2:0: address
output
G1: active high enable
G2: active low enables
G1 G2A G2B A2:0 Y7:0
0 x x xxx 11111111
1 1 x xxx 11111111
1 0 1 xxx 11111111
1 0 0
1Y
S
1D0
1D1
2D0
2D1
2Y
GND
74157 2 :1 Mux
VDD
G
4D0
4D1
4Y
3D0
3D1
3Y
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
always @(*)
 if (~Gb) 1Y = 0;
 else 1Y = S ? 1D[1] : 1D[0];
 if (~Gb) 2Y = 0;
 else 2Y = S ? 2D[1] : 2D[0];
 if (~Gb) 3Y = 0;
 else 3Y = S ? 3D[1] : 3D[0];
 if (~Gb) 4Y = 0;
 else 4Y = S ? 4D[1] : 4D[0];
Four 2:1 Multiplexers
D1:0: data
S: select
Y: output
Gb: enable
1EN
1A0
2Y3
1A1
2Y2
GND
74244 Tristate Buffer
VDD
2EN
1Y0
2A3
1Y1
2A2
1Y2
2A1
1
2
3
4
5
6
7
8
20
19
18
17
16
15
14
13
9
10
12
11
1A2
2Y1
1A3
1Y0 1Y3
2A0
assign 1Y =
 1ENb ? 4'bzzzz : 1A;
assign 2Y =
2ENB ? 4'bzzzz : 2A;
8-bit Tristate Buffer
A3:0: input
Y3:0: output
ENb: enable
always @(posedge clk)
 if (~ENb) Q <= D;
EN
Q0
D0
D1
Q1
GND
74377 Register
VDD
Q7
D7
D6
Q6
Q5
D5
D4
1
2
3
4
5
6
7
8
20
19
18
17
16
15
14
13
9
10
12
11
Q2
D2
D3
Q3 Q4
CLK
8-bit Enableable Register
CLK: clock
D7:0: data
Q7:0: output
ENb: enable
Yb7:0:
000 11111110
1 0 0 001 11111101
1 0 0 010 11111011
1 0 0 011 11110111
1 0 0 100 11101111
1 0 0 101 11011111
1 0 0 110 10111111
1 0 0 111 01111111
Figure A.2 Medium-scale integration chips
Note: Verilog variable names cannot start with numbers, but the names in the example code in Figure A.2
are chosen to match the manufacturer’s data sheet.

A.3 Programmable Logic 519
4-bit ALU
A3:0, B3:0 :
Y3:0 : output
F3:0 : function select
M : mode select
Cbn : carry in
Cbnplus4 : carry out
AeqB : equality
(in some modes)
X,Y : carry lookahead
adder outputs
B0
A0
S3
S2
S1
S0
Cn
M
F0
F1
F2
GND
74181 ALU
VDD
A1
B1
A2
B2
A3
B3
Y
Cn+4
X
A=B
F3
1
2
3
4
5
6
7
8
9
10
11
12
24
23
22
21
20
19
18
17
16
15
14
13
D1
D2
LT
RBO
RBI
D3
D0
GND
7447 7-Segment
Decoder
VDD
f
g
a
b
c
d
e
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
7-segment Display Decoder
D3:0 : data
a...f : segments
(low = ON)
LTb: light test
RBIb: ripple blanking in
RBOb: ripple blanking out
RBO LT RBI D3:0 a b c d e f g
0 x x x 1 1 1 1 1 1 1
1 0 x x 0 0 0 0 0 0 0
x 1 0 0000 1 1 1 1 1 1 1
1 1 1 0000 0 0 0 0 0 0 1
1 1 1 0001 1 0 0 1 1 1 1
1 1 1 0010 0 0 1 0 0 1 0
1 1 1 0011 0 0 0 0 1 1 0
1 1 1 0100 1 0 0 1 1 0 0
1 1 1 0101 0 1 0 0 1 0 0
1 1 1 0110 1 1 0 0 0 0 0
1 1 1 0111 0 0 0 1 1 1 1
1 1 1 1000 0 0 0 0 0 0 0
1 1 1 1001 0 0 0 1 1 0 0
1 1 1 1010 1 1 1 0 0 1 0
1 1 1 1011 1 1 0 0 1 1 0
1 1 1 1100 1 0 1 1 1 0 0
1 1 1 1101 0 1 1 0 1 0 0
1 1 1 1110 0 0 0 1 1 1 1
1 1 1 1111 0 0 0 0 0 0 0
a
b
c d e
f g
B3
AltBin
AeqBin
AgtBin
AgtBout
AeqBout
AltBout
GND
7485 Comparator
VDD
A3
B2
A2
A1
B1
A0
B0
1
2
3
4
5
6
7
8
16
15
14
13
12
11
10
9
always @(*)
 if (A > B | (A == B & AgtBin)) begin
 AgtBout = 1; AeqBout = 0; AltBout = 0;
 end
 else if (A < B | (A == B & AltBin) begin
 AgtBout = 0; AeqBout = 0; AltBout = 1;
 end else begin
 AgtBout = 0; AeqBout = 1; AltBout = 0;
 end
4-bit Comparator
A3:0, B3:0 : data
relin : input relation
relout : output relation
always @(*)
 case (F)
 0000: Y = M ? ~A : A + ~Cbn;
 0001: Y = M ? ~(A | B) : A + B + ~Cbn;
 0010: Y = M ? (~A) & B : A + ~B + ~Cbn;
 0011: Y = M ? 4'b0000 : 4'b1111 + ~Cbn;
 0100: Y = M ? ~(A & B) : A + (A & ~B) + ~Cbn;
 0101: Y = M ? ~B : (A | B) + (A & ~B) + ~Cbn;
 0110: Y = M ? A ^ B : A - B - Cbn;
 0111: Y = M ? A & ~B : (A & ~B) - Cbn;
 1000: Y = M ? ~A + B : A + (A & B) + ~Cbn;
 1001: Y = M ? ~(A ^ B) : A + B + ~Cbn;
 1010: Y = M ? B : (A | ~B) + (A & B) + ~Cbn;
 1011: Y = M ? A & B : (A & B) + ~Cbn;
 1100: Y = M ? 1 : A + A + ~Cbn;
 1101: Y = M ? A | ~B : (A | B) + A + ~Cbn;
 1110: Y = M ? A | B : (A | ~B) + A + ~Cbn;
 1111: Y = M ? A : A - Cbn;
 endcase
inputs
Figure A.3 More medium-scale integration (MSI) chips

involve replacing the contents of the PROM rather than rewiring connections between chips. Lookup tables are useful for small functions but
become prohibitively expensive as the number of inputs grows.
For example, the classic 2764 8-KB (64-Kb) erasable PROM
(EPROM) is shown in Figure A.4. The EPROM has 13 address lines to
specify one of the 8K words and 8 data lines to read the byte of data at
that word. The chip enable and output enable must both be asserted for
data to be read. The maximum propagation delay is 200 ps. In normal
operation, and VPP is not used. The EPROM is usually programmed on a special programmer that sets , applies 13 V to
VPP, and uses a special sequence of inputs to configure the memory.
Modern PROMs are similar in concept but have much larger capacities and more pins. Flash memory is the cheapest type of PROM, selling
for about $30 per gigabyte in 2006. Prices have historically declined by
30 to 40% per year.
A.3.2 PLAs
As discussed in Section 5.6.1, PLAs contain AND and OR planes to
compute any combinational function written in sum-of-products form.
The AND and OR planes can be programmed using the same techniques
for PROMs. A PLA has two columns for each input and one column for
each output. It has one row for each minterm. This organization is more
efficient than a PROM for many functions, but the array still grows
excessively large for functions with numerous I/Os and minterms.
Many different manufacturers have extended the basic PLA concept
to build programmable logic devices (PLDs) that include registers.
PGM  0
PGM  1
520 APPENDIX A Digital System Implementation
A6
VPP
A12
A7
A5
A4
A3
A2
VDD
PGM
NC
A8
A9
A11
OE
A10
1
2
3
4
5
6
7
8
28
27
26
25
24
23
22
21
9
10
11
12
13
14
20
19
18
17
16
15
A0
A1
D0
D1
D2
GND
CE
D7
D6
D5
D4
D3
assign D = (~CEb & ~OEb) ? ROM[A]
 : 8'bZ;
8 KB EPROM
A12:0: address input
D7:0 : data output
CEb : chip enable
OEb : output enable
PGMb : program
VPP : program voltage
NC : no connection
Figure A.4 2764 8KB EPROM
Ap
The 22V10 is one of the most popular classic PLDs. It has 12 dedicated
input pins and 10 outputs. The outputs can come directly from the PLA
or from clocked registers on the chip. The outputs can also be fed back
into the PLA. Thus, the 22V10 can directly implement FSMs with up to
12 inputs, 10 outputs, and 10 bits of state. The 22V10 costs about $2 in
quantities of 100. PLDs have been rendered mostly obsolete by the rapid
improvements in capacity and cost of FPGAs.
A.3.3 FPGAs
As discussed in Section 5.6.2, FPGAs consist of arrays of configurable
logic blocks (CLBs) connected together with programmable wires. The
CLBs contain small lookup tables and flip-flops. FPGAs scale gracefully
to extremely large capacities, with thousands of lookup tables. Xilinx
and Altera are two of the leading FPGA manufacturers.
Lookup tables and programmable wires are flexible enough to
implement any logic function. However, they are an order of magnitude
less efficient in speed and cost (chip area) than hard-wired versions of
the same functions. Thus, FPGAs often include specialized blocks, such
as memories, multipliers, and even entire microprocessors.
Figure A.5 shows the design process for a digital system on an
FPGA. The design is usually specified with a hardware description language (HDL), although some FPGA tools also support schematics. The
design is then simulated. Inputs are applied and compared against
expected outputs to verify that the logic is correct. Usually some debugging is required. Next, logic synthesis converts the HDL into Boolean
functions. Good synthesis tools produce a schematic of the functions,
and the prudent designer examines these schematics, as well as any
warnings produced during synthesis, to ensure that the desired logic
was produced. Sometimes sloppy coding leads to circuits that are much
larger than intended or to circuits with asynchronous logic. When the
synthesis results are good, the FPGA tool maps the functions onto the
CLBs of a specific chip. The place and route tool determines which
functions go in which lookup tables and how they are wired together.
Wire delay increases with length, so critical circuits should be placed
close together. If the design is too big to fit on the chip, it must be
reengineered. Timing analysis compares the timing constraints (e.g., an
intended clock speed of 100 MHz) against the actual circuit delays and
reports any errors. If the logic is too slow, it may have to be redesigned
or pipelined differently. When the design is correct, a file is generated
specifying the contents of all the CLBs and the programming of all
the wires on the FPGA. Many FPGAs store this configuration information in static RAM that must be reloaded each time the FPGA is turned
on. The FPGA can download this information from a computer in the
A.3 Programmable Logic 521
Design Entry
Synthesis
Logic Verification
Mapping
Timing Analysis
Configure FPGA
Place and Route
Debug
Debug
Too big
Too slow
Figure A.5 FPGA design flow

laboratory, or can read it from a nonvolatile ROM when power is first
applied.
Example A.1 FPGA TIMING ANALYSIS
Alyssa P. Hacker is using an FPGA to implement an M&M sorter with a color
sensor and motors to put red candy in one jar and green candy in another. Her
design is implemented as an FSM, and she is using a Spartan XC3S200 FPGA, a
chip from the Spartan 3 series family. According to the data sheet, the FPGA has
the timing characteristics shown in Table A.1. Assume that the design is small
enough that wire delay is negligible.
Alyssa would like her FSM to run at 100 MHz. What is the maximum number
of CLBs on the critical path? What is the fastest speed at which her FSM could
possibly run?
SOLUTION: At 100 MHz, the cycle time, Tc, is 10 ns. Alyssa uses Equation 3.13 figure to out the minimum combinational propagation delay, tpd, at this cycle time:
tpd  10 ns  (0.72 ns  0.53 ns)  8.75 ns (A.1)
Alyssa’s FSM can use at most 14 consecutive CLBs (8.75/0.61) to implement the
next-state logic.
The fastest speed at which an FSM will run on a Spartan 3 FPGA is when it
is using a single CLB for the next state logic. The minimum cycle time is
Tc  0.61 ns  0.72 ns  0.53 ns  1.86 ns (A.2)
Therefore, the maximum frequency is 538 MHz.
522 APPENDIX A Digital System Implementation
Table A.1 Spartan 3 XC3S200 timing
name value (ns)
tpcq 0.72
tsetup 0.53
thold 0
tpd (per CLB) 0.61
tskew 0
Xilinx advertises the XC3S100E FPGA with 1728 lookup tables and
flip-flops for $2 in quantities of 500,000 in 2006. In more modest quantities, medium-sized FPGAs typically cost about $10, and the largest
Ap
FPGAs cost hundreds or even thousands of dollars. The cost has
declined at approximately 30% per year, so FPGAs are becoming
extremely popular.
A.4 APPLICATION-SPECIFIC INTEGRATED CIRCUITS
Application-specific integrated circuits (ASICs) are chips designed for a
particular purpose. Graphics accelerators, network interface chips, and
cell phone chips are common examples of ASICS. The ASIC designer
places transistors to form logic gates and wires the gates together.
Because the ASIC is hardwired for a specific function, it is typically several times faster than an FPGA and occupies an order of magnitude less
chip area (and hence cost) than an FPGA with the same function.
However, the masks specifying where transistors and wires are located
on the chip cost hundreds of thousands of dollars to produce. The fabrication process usually requires 6 to 12 weeks to manufacture, package,
and test the ASICs. If errors are discovered after the ASIC is manufactured, the designer must correct the problem, generate new masks, and
wait for another batch of chips to be fabricated. Hence, ASICs are suitable only for products that will be produced in large quantities and
whose function is well defined in advance.
Figure A.6 shows the ASIC design process, which is similar to the
FPGA design process of Figure A.5. Logic verification is especially
important because correction of errors after the masks are produced is
expensive. Synthesis produces a netlist consisting of logic gates and connections between the gates; the gates in this netlist are placed, and the
wires are routed between gates. When the design is satisfactory, masks
are generated and used to fabricate the ASIC. A single speck of dust can
ruin an ASIC, so the chips must be tested after fabrication. The fraction
of manufactured chips that work is called the yield; it is typically 50 to
90%, depending on the size of the chip and the maturity of the
manufacturing process. Finally, the working chips are placed in packages, as will be discussed in Section A.7.
A.5 DATA SHEETS
Integrated circuit manufacturers publish data sheets that describe the
functions and performance of their chips. It is essential to read and
understand the data sheets. One of the leading sources of errors in digital systems comes from misunderstanding the operation of a chip.
Data sheets are usually available from the manufacturer’s Web
site. If you cannot locate the data sheet for a part and do not have clear
documentation from another source, don’t use the part. Some of the
entries in the data sheet may be cryptic. Often the manufacturer publishes data books containing data sheets for many related parts. The
A.5 Data Sheets 523
Design Entry
Synthesis
Logic Verification
Timing Analysis
Generate Masks
Place and Route
Debug
Debug
Too big
Too slow
Fabricate ASIC
Test ASIC
Package ASIC
Defective
Figure A.6 ASIC design flow

beginning of the data book has additional explanatory information. This
information can usually be found on the Web with a careful search.
This section dissects the Texas Instruments (TI) data sheet for a
74HC04 inverter chip. The data sheet is relatively simple but illustrates
many of the major elements. TI still manufacturers a wide variety of
74xx-series chips. In the past, many other companies built these chips
too, but the market is consolidating as the sales decline.
Figure A.7 shows the first page of the data sheet. Some of the key sections are highlighted in blue. The title is SN54HC04, SN74HC04 HEX
INVERTERS. HEX INVERTERS means that the chip contains six inverters.
SN indicates that TI is the manufacturer. Other manufacture codes include
MC for Motorola and DM for National Semiconductor. You can generally
ignore these codes, because all of the manufacturers build compatible 74xxseries logic. HC is the logic family (high speed CMOS). The logic family
determines the speed and power consumption of the chip, but not the function. For example, the 7404, 74HC04, and 74LS04 chips all contain six
inverters, but they differ in performance and cost. Other logic families are
discussed in Section A.6. The 74xx chips operate across the commercial or
industrial temperature range (0 to 70 C or 40 to 85 C, respectively),
whereas the 54xx chips operate across the military temperature range
(55 to 125 C) and sell for a higher price but are otherwise compatible.
The 7404 is available in many different packages, and it is important to order the one you intended when you make a purchase. The
packages are distinguished by a suffix on the part number. N indicates a
plastic dual inline package (PDIP), which fits in a breadboard or can be
soldered in through-holes in a printed circuit board. Other packages are
discussed in Section A.7.
The function table shows that each gate inverts its input. If A is
HIGH (H), Y is LOW (L) and vice versa. The table is trivial in this case
but is more interesting for more complex chips.
Figure A.8 shows the second page of the data sheet. The logic diagram indicates that the chip contains inverters. The absolute maximum
section indicates conditions beyond which the chip could be destroyed.
In particular, the power supply voltage (VCC, also called VDD in this
book) should not exceed 7 V. The continuous output current should not
exceed 25 mA. The thermal resistance or impedance, 	JA, is used to calculate the temperature rise caused by the chip’s dissipating power. If the
ambient temperature in the vicinity of the chip is TA and the chip dissipates Pchip, then the temperature on the chip itself at its junction with
the package is
TJ  TA  Pchip 	JA (A.3)
For example, if a 7404 chip in a plastic DIP package is operating in a
hot box at 50 C and consumes 20 mW, the junction temperature will
524 APPENDIX A Digital System Implementation
A
A.5 Data Sheets 525
SCLS078D – DECEMBER 1982 – REVISED JULY 2003
POST OFFICE BOX 655303 • DALLAS, TEXAS 75265
Wide Operating Voltage Range of 2 V to 6 V
Outputs Can Drive Up To 10 LSTTL Loads
Low Power Consumption, 20-µA Max ICC
Typical tpd = 8 ns
±4-mA Output Drive at 5V
Low Input Current of 1 µA Max
1
2
3
4
5
6
7
14
13
12
11
10
9
8
1A
1Y
2A
2Y
3A
3Y
GND
VCC
6A
6Y
5A
5Y
4A
4Y
SN54HC04 . . . J OR W PACKAGE
SN74HC04 . . . D, N, NS, OR PW PACKAGE
(TOPVIEW)
3 2 1 20 19
9 10 11 12 13
4
5
6
7
8
18
17
16
15
14
6Y
NC
5A
NC
5Y
2A
NC
2Y
NC
3A
Y1
A1
CN
Y4
A4
VCC
A6
Y3
DNG
CN
SN54HC04 . . . FK PACKAGE
(TOPVIEW)
NC – No internal connection
description/ordering information
ORDERING INFORMATION
TA PACKAGE† ORDERABLE
PARTNUMBER
TOP-SIDE
MARKING
PDIP – N Tube of 25 SN74HC04N SN74HC04N
Tube of 50 SN74HC04D
SOIC – D
Reel of 2500 SN74HC04DR HC04
Reel of 250 SN74HC04DT –40°C to 85°C SOP – NS Reel of 2000 SN74HC04NSR HC04
Tube of 90 SN74HC04PW
TSSOP – PW Reel of 2000 SN74HC04PWR HC04
Reel of 250 SN74HC04PWT
CDIP – J Tube of 25 SNJ54HC04J SNJ54HC04J
–55°C to 125°C CFP – W Tube of 150 SNJ54HC04W SNJ54HC04W
LCCC – FK Tube of 55 SNJ54HC04FK SNJ54HC04FK
† Package drawings, standard packing quantities, thermal data, symbolization, and PCB design guidelines are
available at www.ti.com/sc/package.
FUNCTION TABLE
(each inverter)
INPUT
A
OUTPUT
Y
H L
L H
Please be aware that an important notice concerning availability, standard warranty, and use in critical applications of
Texas Instruments semiconductor products and disclaimers there to appears at the end of this data sheet.
PRODUCTION DATA information is current as of publication date. Copyright (c)2003, Texas Instruments Incorporated
Products conform to specifications per the terms of Texas Instruments
standard warranty. Production processing does not necessarily include
testing of all parameters.
On products compliant to MIL-PRF-38535, all parameters are tested
unless otherwise noted. On all other products, production
processing does not necessarily include testing of all parameters.
SN54HC04, SN74HC04
HEX INVERTERS
The ’HC04 devices contain six independent inverters. They perform the Boolean function Y = A
in positive logic.
Figure A.7 7404 data sheet page 1

526 APPENDIX A Digital System Implementation
SN54HC04, SN74HC04
HEX INVERTERS
SCLS078D – DECEMBER 1982 – REVISED JULY 2003
POST OFFICE BOX 655303 • DALLAS, TEXAS 75265
logic diagram (positive logic)
A Y
absolute maximum ratings over operating free-air temperature range (unless otherwise noted)†
Supply voltage range, VCC .......................................................... –0.5 V to 7 V
Input clamp current, IIK (VI
 < 0 or VI > VCC) (see Note 1) .................................... ±20 mA
Output clamp current, IOK (VO < 0 or VO > VCC) (see Note 1) ................................ ±20 mA
Continuous output current, IO (VO = 0 to VCC) .............................................. ±25 mA
Continuous current through VCC or GND ................................................... ±50 mA
Package thermal impedance, θJA ( egakcapD:)2etoNees ................................... 86° C/W
N ................................... egakcap 80° C/W
N egakcapS ................................. 76° C/W
P egakcapW ................................ 131° C/W
Storage temperature range, Tstg ................................................... –65° C to 150° C
† Stresses beyond those listed under “absolute maximum ratings” may cause permanent damage to the device. These are stress ratings only, and
functional operation of the device at these or any other conditions beyond those indicated under “recommended operating conditions” is not
implied. Exposure to absolute-maximum-rated conditions for extended periods may affect device reliability.
NOTES: 1. The input and output voltage ratings may be exceeded if the input and output current ratings are observed.
2. The package thermal impedance is calculated in accordance with JESD 51-7.
recommended operating conditions (see Note 3)
SN54HC04 SN74HC04 UNIT MIN NOM MAX MIN NOM MAX UNIT
VCC Supply voltage 2 5 6 2 5 6 V
VCC =2V 1.5 1.5
VIH High-level input voltage VCC = 4.5 V 3.15 3.15 V
VCC =6V 4.2 4.2
VCC =2V 0.5 0.5
VIL Low-level input voltage VCC = 4.5 V 1.35 1.35 V
VCC =6V 1.8 1.8
VI Input voltage 0 VCC 0 VCC V
VO Output voltage 0 VCC 0 VCC V
VCC =2V 1000 1000
∆t/∆v Input transition rise/fall time VCC = 4.5 V 500 500 ns
VCC =6V 400 400
TA Operating free-air temperature –55 125 –40 85 °C
NOTE 3: All unused inputs of the device must be held at VCC or GND to ensure proper device operation. Refer to the TI application report,
Implications of Slow or Floating CMOS Inputs, literature number SCBA004.
TEXAS
INSTRUMENTS
Figure A.8 7404 datasheet page 2

climb to 50

C
 0.02 W
 80
C/W
 51.6
C. Internal power dissipation is seldom important for 74xx-series chips, but it becomes important
for modern chips that dissipate tens of watts or more.
The recommended operating conditions define the environment in
which the chip should be used. Within these conditions, the chip
should meet specifications. These conditions are more stringent than
the absolute maximums. For example, the power supply voltage
should be between 2 and 6 V. The input logic levels for the HC logic
family depend on
VDD. Use the 4.5 V entries when
VDD
 5 V,
to allow for a 10% droop in the power supply caused by noise in the
system.
Figure A.9 shows the third page of the data sheet. The electrical
characteristics describe how the device performs when used within the
recommended operating conditions if the inputs are held constant.
For example, if
VCC
 5 V (and droops to 4.5 V) and the output current, IOH/IOL does not exceed 20 
A, VOH  4.4 V and VOL  0.1 V
in the worst case. If the output current increases, the output voltages
become less ideal, because the transistors on the chip struggle to provide the current. The HC logic family uses CMOS transistors that
draw very little current. The current into each input is guaranteed
to be less than 1000 nA and is typically only 0.1 nA at room temperature. The quiescent power supply current (IDD) drawn while the
chip is idle is less than 20

A. Each input has less than 10 pF of
capacitance.
The switching characteristics define how the device performs when
used within the recommended operating conditions if the inputs change.
The propagation delay, tpd, is measured from when the input passes
through 0.5
VCC to when the output passes through 0.5
VCC. If
VCC is
nominally 5 V and the chip drives a capacitance of less than 50 pF, the
propagation delay will not exceed 24 ns (and typically will be much
faster). Recall that each input may present 10 pF, so the chip cannot
drive more than five identical chips at full speed. Indeed, stray capacitance from the wires connecting chips cuts further into the useful load.
The transition time, also called the rise/fall time, is measured as the output transitions between 0.1 VCC and 0.9 VCC.
Recall from Section 1.8 that chips consume both static and
dynamic power. Static power is low for HC circuits. At 85
C, the
maximum quiescent supply current is 20

A. At 5 V, this gives a static
power consumption of 0.1 mW. The dynamic power depends on the
capacitance being driven and the switching frequency. The 7404 has an
internal power dissipation capacitance of 20 pF per inverter. If all six
inverters on the 7404 switch at 10 MHz and drive external loads of
25 pF, then the dynamic power given by Equation 1.4 is (6)(20 pF

25 pF)(5
2)(10 MHz)
 33.75 mW and the maximum total power is
33.85 mW.
12
A.5 Data Sheets 527
Appendi
528 APPENDIX A Digital System Implementation
SN54HC04, SN74HC04
HEX INVERTERS
SCLS078D – DECEMBER 1982 – REVISED JULY 2003
POST OFFICE BOX 655303 • DALLAS, TEXAS 75265
TEXAS
INSTRUMENTS
electrical characteristics over recommended operating free-air temperature range (unless
otherwise noted)
PARAMETER TEST CONDITIONS VCC
TA=25°C SN54HC04 SN74HC04 UNIT MIN TYP MAX MIN MAX MIN MAX UNIT
2 V 1.9 1.998 1.9 1.9
I
OH = –20 µA
I
OL = 20 µA
4.5V 4.4 4.499 4.4 4.4
VOH
VOL
VI =VIH or VIL
VI =VIH or VIL
VI =VCC or 0
VI =VCC or 0,
6 V 5.9 5.999 5.9 5.9 V
I
OH = –4 mA
I
OL = 4 mA
4.5V 3.98 4.3 3.7 3.84
I
OH = –5.2 mA
I
OL = 5.2 mA
6 V 5.48 5.8 5.2 5.34
2 V 0.002 0.1 0.1 0.1
4.5V 0.001 0.1 0.1 0.1
6 V 0.001 0.1 0.1 0.1 V
4.5V 0.17 0.26 0.4 0.33
6 V 0.15 0.26 0.4 0.33
I
I 6 V ±0.1 ±100 ±1000 ±1000 nA
I
CC I
O = 0 6 V 2 40 20 µA
Ci 2 Vto 6 V 3 10 10 10 pF
switching characteristics over recommended operating free-air temperature range, CL = 50 pF
(unless otherwise noted) (see Figure 1)
FROM TO VCC
TA = 25° C SN54HC04 SN74HC04 PARAMETER UNIT (INPUT) (OUTPUT) MIN TYP MAX MIN MAX MIN MAX UNIT
2 V 45 95 145 120
t
pd A Y 4.5V 9 19 29 24 ns
6 V 8 16 25 20
2 V 38 75 110 95
t
t Y 4.5V 8 15 22 19 ns
6 V 6 13 19 16
operating characteristics, TA = 25° C
PARAMETER TEST CONDITIONS TYP UNIT
Cpd Power dissipation capacitance per inverter No load 20 pF
Figure A.9 7404 datasheet page 3

A.6 LOGIC FAMILIES
The 74xx-series logic chips have been manufactured using many different
technologies, called logic families, that offer different speed, power, and
logic level trade-offs. Other chips are usually designed to be compatible
with some of these logic families. The original chips, such as the 7404,
were built using bipolar transistors in a technology called TransistorTransistor Logic (TTL). Newer technologies add one or more letters after
the 74 to indicate the logic family, such as 74LS04, 74HC04, or
74AHCT04. Table A.2 summarizes the most common 5-V logic families.
Advances in bipolar circuits and process technology led to the
Schottky (S) and Low-Power Schottky (LS) families. Both are faster than
TTL. Schottky draws more power, whereas Low-power Schottky draws
less. Advanced Schottky (AS) and Advanced Low-Power Schottky (ALS)
have improved speed and power compared to S and LS. Fast (F) logic is
faster and draws less power than AS. All of these families provide more
current for LOW outputs than for HIGH outputs and hence have asymmetric logic levels. They conform to the “TTL” logic levels: VIH  2 V,
VIL  0.8 V, VOH  2.4 V, and VOL  0.5 V.
A.6 Logic Families 529
Table A.2 Typical specifications for 5-V logic families
CMOS / TTL
Bipolar / TTL CMOS Compatible
Characteristic TTL S LS AS ALS F HC AHC HCT AHCT
tpd (ns) 22 9 12 7.5 10 6 21 7.5 30 7.7
VIH (V) 2 2 2 2 2 2 3.15 3.15 2 2
VIL (V) 0.8 0.8 0.8 0.8 0.8 0.8 1.35 1.35 0.8 0.8
VOH (V) 2.4 2.7 2.7 2.5 2.5 2.5 3.84 3.8 3.84 3.8
VOL (V) 0.4 0.5 0.5 0.5 0.5 0.5 0.33 0.44 0.33 0.44
IOH (mA) 0.4 1 0.4 2 0.4 1 4 8 4 8
IOL (mA) 16 20 8 20 8 20 4 8 4 8
IIL (mA) 1.6 2 0.4 0.5 0.1 0.6 0.001 0.001 0.001 0.001
IIH (mA) 0.04 0.05 0.02 0.02 0.02 0.02 0.001 0.001 0.001 0.001
IDD (mA) 33 54 6.6 26 4.2 15 0.02 0.02 0.02 0.02
Cpd (pF) n/a 20 12 20 14
cost* (US $) obsolete 0.57 0.29 0.53 0.33 0.20 0.15 0.15 0.15 0.15
* Per unit in quantities of 1000 for the 7408 from Texas Instruments in 2006
Ap
As CMOS circuits matured in the 1980s and 1990s, they became
popular because they draw very little power supply or input current.
The High Speed CMOS (HC) and Advanced High Speed CMOS
(AHC) families draw almost no static power. They also deliver
the same current for HIGH and LOW outputs. They conform to
the “CMOS” logic levels: VIH  3.15 V, VIL  1.35 V, VOH  3.8 V,
and VOL  0.44 V. Unfortunately, these levels are incompatible with
TTL circuits, because a TTL HIGH output of 2.4 V may not be recognized as a legal CMOS HIGH input. This motivates the use of High
Speed TTL-compatible CMOS (HCT) and Advanced High Speed TTLcompatible CMOS (AHCT), which accept TTL input logic levels and
generate valid CMOS output logic levels. These families are slightly
slower than their pure CMOS counterparts. All CMOS chips are sensitive to electrostatic discharge (ESD) caused by static electricity.
Ground yourself by touching a large metal object before handling
CMOS chips, lest you zap them.
The 74xx-series logic is inexpensive. The newer logic families are
often cheaper than the obsolete ones. The LS family is widely available
and robust and is a popular choice for laboratory or hobby projects that
have no special performance requirements.
The 5-V standard collapsed in the mid-1990s, when transistors
became too small to withstand the voltage. Moreover, lower voltage
offers lower power consumption. Now 3.3, 2.5, 1.8, 1.2, and even lower
voltages are commonly used. The plethora of voltages raises challenges in
communicating between chips with different power supplies. Table A.3
lists some of the low-voltage logic families. Not all 74xx parts are available in all of these logic families.
All of the low-voltage logic families use CMOS transistors, the
workhorse of modern integrated circuits. They operate over a wide
range of VDD, but the speed degrades at lower voltage. Low-Voltage
CMOS (LVC) logic and Advanced Low-Voltage CMOS (ALVC) logic
are commonly used at 3.3, 2.5, or 1.8 V. LVC withstands inputs up
to 5.5 V, so it can receive inputs from 5-V CMOS or TTL circuits.
Advanced Ultra-Low-Voltage CMOS (AUC) is commonly used at 2.5,
1.8, or 1.2 V and is exceptionally fast. Both ALVC and AUC withstand inputs up to 3.6 V, so they can receive inputs from 3.3-V
circuits.
FPGAs often offer separate voltage supplies for the internal logic,
called the core, and for the input/output (I/O) pins. As FPGAs have
advanced, the core voltage has dropped from 5 to 3.3, 2.5, 1.8, and 1.2 V
to save power and avoid damaging the very small transistors. FPGAs
have configurable I/Os that can operate at many different voltages, so as
to be compatible with the rest of the system.
530 APPENDIX A Digital System Implementation
Ap
A.7 PACKAGING AND ASSEMBLY
Integrated circuits are typically placed in packages made of plastic or
ceramic. The packages serve a number of functions, including connecting
the tiny metal I/O pads of the chip to larger pins in the package for ease
of connection, protecting the chip from physical damage, and spreading
the heat generated by the chip over a larger area to help with cooling.
The packages are placed on a breadboard or printed circuit board and
wired together to assemble the system.
Packages
Figure A.10 shows a variety of integrated circuit packages. Packages can
be generally categorized as through-hole or surface mount (SMT).
Through-hole packages, as their name implies, have pins that can be
inserted through holes in a printed circuit board or into a socket. Dual
inline packages (DIPs) have two rows of pins with 0.1-inch spacing
between pins. Pin grid arrays (PGAs) support more pins in a smaller
package by placing the pins under the package. SMT packages are
soldered directly to the surface of a printed circuit board without using
holes. Pins on SMT parts are called leads. The thin small outline package
(TSOP) has two rows of closely spaced leads (typically 0.02-inch spacing). Plastic leaded chip carriers (PLCCs) have J-shaped leads on all four
A.7 Packaging and Assembly 531
Table A.3 Typical specifications for low-voltage logic families
LVC ALVC AUC
Vdd (V) 3.3 2.5 1.8 3.3 2.5 1.8 2.5 1.8 1.2
tpd (ns) 4.1 6.9 9.8 2.8 3 ?1 1.8 2.3 3.4
VIH (V) 2 1.7 1.17 2 1.7 1.17 1.7 1.17 0.78
VIL (V) 0.8 0.7 0.63 0.8 0.7 0.63 0.7 0.63 0.42
VOH (V) 2.2 1.7 1.2 2 1.7 1.2 1.8 1.2 0.8
VOL (V) 0.55 0.7 0.45 0.55 0.7 0.45 0.6 0.45 0.3
IO (mA) 24 8 4 24 12 12 9 8 3
II (mA) 0.02 0.005 0.005
IDD (mA) 0.01 0.01 0.01
Cpd (pF) 10 9.8 7 27.5 23 ?* 17 14 14
cost (US $) 0.17 0.20 not available
* Delay and capacitance not available at the time of writing

sides, with 0.05-inch spacing. They can be soldered directly to a board
or placed in special sockets. Quad flat packs (QFPs) accommodate a
large number of pins using closely spaced legs on all four sides. Ball grid
arrays (BGAs) eliminate the legs altogether. Instead, they have hundreds
of tiny solder balls on the underside of the package. They are carefully
placed over matching pads on a printed circuit board, then heated so
that the solder melts and joins the package to the underlying board.
Breadboards
DIPs are easy to use for prototyping, because they can be placed in a
breadboard. A breadboard is a plastic board containing rows of sockets,
as shown in Figure A.11. All five holes in a row are connected together.
Each pin of the package is placed in a hole in a separate row. Wires can
be placed in adjacent holes in the same row to make connections to the
pin. Breadboards often provide separate columns of connected holes
running the height of the board to distribute power and ground.
Figure A.11 shows a breadboard containing a majority gate built
with a 74LS08 AND chip and a 74LS32 OR chip. The schematic of the
circuit is shown in Figure A.12. Each gate in the schematic is labeled
with the chip (08 or 32) and the pin numbers of the inputs and outputs
(see Figure A.1). Observe that the same connections are made on the
breadboard. The inputs are connected to pins 1, 2, and 5 of the 08 chip,
and the output is measured at pin 6 of the 32 chip. Power and ground
are connected to pins 14 and 7, respectively, of each chip, from the vertical power and ground columns that are attached to the banana plug
receptacles, Vb and Va. Labeling the schematic in this way and checking
off connections as they are made is a good way to reduce the number of
mistakes made during breadboarding.
Unfortunately, it is easy to accidentally plug a wire in the wrong
hole or have a wire fall out, so breadboarding requires a great deal of
care (and usually some debugging in the laboratory). Breadboards are
suited only to prototyping, not production.
532 APPENDIX A Digital System Implementation
Figure A.10 Integrated circuit
packages

Printed Circuit Boards
Instead of breadboarding, chip packages may be soldered to a printed
circuit board (PCB). The PCB is formed of alternating layers of conducting copper and insulating epoxy. The copper is etched to form wires
called traces. Holes called vias are drilled through the board and plated
with metal to connect between layers. PCBs are usually designed with
computer-aided design (CAD) tools. You can etch and drill your own
simple boards in the laboratory, or you can send the board design to a
specialized factory for inexpensive mass production. Factories have turnaround times of days (or weeks, for cheap mass production runs) and
typically charge a few hundred dollars in setup fees and a few dollars per
board for moderately complex boards built in large quantities.
A.7 Packaging and Assembly 533
08
08
08
32
32
1
BC
Y
2 3
4
5 6
8 9
10
1
2
3
4
5 6
A
Figure A.12 Majority gate
schematic with chips and pins
identified
GND
Rows of 5 pins are
internally connected
VDD Figure A.11 Majority circuit on
breadboard

534 APPENDIX A Digital System Implementation
Signal Layer
Signal Layer
Power Plane
Ground Plane
Copper Trace Insulator
Figure A.13 Printed circuit
board cross-section
PCB traces are normally made of copper because of its low resistance.
The traces are embedded in an insulating material, usually a green, fireresistant plastic called FR4. A PCB also typically has copper power and
ground layers, called planes, between signal layers. Figure A.13 shows a
cross-section of a PCB. The signal layers are on the top and bottom, and
the power and ground planes are embedded in the center of the board.
The power and ground planes have low resistance, so they distribute stable power to components on the board. They also make the capacitance
and inductance of the traces uniform and predictable.
Figure A.14 shows a PCB for a 1970s vintage Apple II computer. At
the top is a Motorola 6502 microprocessor. Beneath are six 16-Kb ROM
chips forming 12 KB of ROM containing the operating system. Three rows
of eight 16-Kb DRAM chips provide 48 KB of RAM. On the right are
several rows of 74xx-series logic for memory address decoding and other
functions. The lines between chips are traces that wire the chips together.
The dots at the ends of some of the traces are vias filled with metal.
Putting It All Together
Most modern chips with large numbers of inputs and outputs use SMT
packages, especially QFPs and BGAs. These packages require a printed
circuit board rather than a breadboard. Working with BGAs is especially
challenging because they require specialized assembly equipment.
Moreover, the balls cannot be probed with a voltmeter or oscilloscope
during debugging in the laboratory, because they are hidden under the
package.
In summary, the designer needs to consider packaging early on to
determine whether a breadboard can be used during prototyping and
whether BGA parts will be required. Professional engineers rarely use
breadboards when they are confident of connecting chips together correctly without experimentation.
A.8 TRANSMISSION LINES
We have assumed so far that wires are equipotential connections that have
a single voltage along their entire length. Signals actually propagate along
wires at the speed of light in the form of electromagnetic waves. If the wires
are short enough or the signals change slowly, the equipotential assumption

is good enough. When the wire is long or the signal is very fast, the transmission time along the wire becomes important to accurately determine the
circuit delay. We must model such wires as transmission lines, in which a
wave of voltage and current propagates at the speed of light. When the
wave reaches the end of the line, it may reflect back along the line. The
reflection may cause noise and odd behaviors unless steps are taken to limit
it. Hence, the digital designer must consider transmission line behavior to
accurately account for the delay and noise effects in long wires.
Electromagnetic waves travel at the speed of light in a given medium,
which is fast but not instantaneous. The speed of light depends on the
permittivity, ε, and permeability, µ, of the medium2: . v  1
s
  1
sLC
A.8 Transmission Lines 535
Figure A.14 Apple II circuit board
2 The capacitance, C, and inductance, L, of a wire are related to the permittivity and
permeability of the physical medium in which the wire is located.
Ap
The speed of light in free space is   c  3  108 m/s. Signals in a PCB
travel at about half this speed, because the FR4 insulator has four times
the permittivity of air. Thus, PCB signals travel at about 1.5  108 m/s,
or 15 cm/ns. The time delay for a signal to travel along a transmission
line of length l is
td  l/v (A.4)
The characteristic impedance of a transmission line, Z0 (pronounced
“Z-naught”), is the ratio of voltage to current in a wave traveling along
the line: Z0  V/I. It is not the resistance of the wire (a good transmission line in a digital system typically has negligible resistance).
Z0 depends on the inductance and capacitance of the line (see the
derivation in Section A.8.7) and typically has a value of 50 to 75 .
(A.5)
Figure A.15 shows the symbol for a transmission line. The symbol
resembles a coaxial cable with an inner signal conductor and an outer
grounded conductor like that used in television cable wiring.
The key to understanding the behavior of transmission lines is to
visualize the wave of voltage propagating along the line at the speed
of light. When the wave reaches the end of the line, it may be
absorbed or reflected, depending on the termination or load at the
end. Reflections travel back along the line, adding to the voltage
already on the line. Terminations are classified as matched, open,
short, or mismatched. The following subsections explore how a wave
propagates along the line and what happens to the wave when it
reaches the termination.
A.8.1 Matched Termination
Figure A.16 shows a transmission line of length l with a matched termination, which means that the load impedance, ZL, is equal to the
characteristic impedance, Z0. The transmission line has a characteristic impedance of 50 . One end of the line is connected to a voltage
Z0  v L
C
536 APPENDIX A Digital System Implementation
I
V
–
+
Z0 =
C
L
Figure A.15 Transmission line
symbol
Appendi
A.8 Transmission Lines 537
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZL = 50 Ω
Figure A.16 Transmission line
with matched termination
Figure A.17 Voltage waveforms
for Figure A.16 at points A, B,
and C
VS ZL = 50 Ω
A, B, C
Figure A.18 Equivalent circuit of
Figure A.16 at steady state
source through a switch that closes at time t  0. The other end is
connected to the 50  matched load. This section analyzes the voltages and currents at points A, B, and C—at the beginning of the line,
one-third of the length along the line, and at the end of the line,
respectively.
Figure A.17 shows the voltages at points A, B, and C over time.
Initially, there is no voltage or current flowing in the transmission
line, because the switch is open. At time t  0, the switch closes, and
the voltage source launches a wave with voltage V  VS along the
line. Because the characteristic impedance is Z0, the wave has current
I  VS /Z0. The voltage reaches the beginning of the line (point A)
immediately, as shown in Figure A.17(a). The wave propagates along
the line at the speed of light. At time td/3, the wave reaches point B.
The voltage at this point abruptly rises from 0 to VS, as shown in
Figure A.17(b). At time td, the incident wave reaches point C at the
end of the line, and the voltage rises there too. All of the current, I,
flows into the resistor, ZL, producing a voltage across the resistor of
ZLI  ZL(VS /Z0)  VS because ZL  Z0. This voltage is consistent
with the wave flowing along the transmission line. Thus, the wave is
absorbed by the load impedance, and the transmission line reaches its
steady state.
In steady state, the transmission line behaves like an ideal equipotential wire because it is, after all, just a wire. The voltage at all points
along the line must be identical. Figure A.18 shows the steady-state
equivalent model of the circuit in Figure A.16. The voltage is VS everywhere along the wire.
Example A.2 TRANSMISSION LINE WITH MATCHED SOURCE AND
LOAD TERMINATIONS
Figure A.19 shows a transmission line with matched source and load impedances
ZS and ZL. Plot the voltage at nodes A, B, and C versus time. When does the
system reach steady-state, and what is the equivalent circuit at steady-state?
SOLUTION: When the voltage source has a source impedance, ZS, in series with
the transmission line, part of the voltage drops across ZS, and the remainder
(a)
t
t
d
VA
VS
(c)
t
VS
t
d
VC
(b)
t
VS
t
d/3 t
d
VB
Appendi
propagates down the transmission line. At first, the transmission line behaves
as an impedance Z0, because the load at the end of the line cannot possibly
influence the behavior of the line until a speed of light delay has elapsed.
Hence, by the voltage divider equation, the incident voltage flowing down the
line is
(A.6)
Thus, at t  0, a wave of voltage, , is sent down the line from point A.
Again, the signal reaches point B at time td/3 and point C at td, as shown in
Figure A.20. All of the current is absorbed by the load impedance ZL, so the
circuit enters steady-state at t  td. In steady-state, the entire line is at VS /2, just
as the steady-state equivalent circuit in Figure A.21 would predict.
A.8.2 Open Termination
When the load impedance is not equal to Z0, the termination cannot
absorb all of the current, and some of the wave must be reflected.
Figure A.22 shows a transmission line with an open load termination.
No current can flow through an open termination, so the current at
point C must always be 0.
The voltage on the line is initially zero. At t  0, the switch closes
and a wave of voltage, , begins propagating down the
line. Notice that this initial wave is the same as that of Example A.2 and
is independent of the termination, because the load at the end of the line
cannot influence the behavior at the beginning until at least td has
elapsed. This wave reaches point B at td/3 and point C at td as shown in
Figure A.23.
When the incident wave reaches point C, it cannot continue forward
because the wire is open. It must instead reflect back toward the source.
The reflected wave also has voltage , because the open termination reflects the entire wave.
The voltage at any point is the sum of the incident and reflected
waves. At time t  td, the voltage at point C is . The
reflected wave reaches point B at 5td/3 and point A at 2td. When
it reaches point A, the wave is absorbed by the source termination
V  VS
2  VS
2  VS
V  VS
2
VVS
Z0
Z0  ZS  VS
2
V  VS
2
V  VS Z0
Z0  ZS   VS
2
538 APPENDIX A Digital System Implementation
t
VA
(a)
VS/2
t
d
t
(b)
t
d /3
VS/2
VB
t
(c)
t
d
VC
VS/2
t
d
Figure A.20 Voltage waveforms
for Figure A.19 at points A, B,
and C
ZL = 50 Ω
ZS = 50 Ω
A, B, C
VS
Figure A.21 Equivalent circuit
of Figure A.19 at steady state
VS
t = 0
Z0 = 50 Ω
ZL = 50 Ω
ZS = 50 Ω
length = l
td = l/v
A B C
l/3
Figure A.19 Transmission line
with matched source and load
impedances
Appendix A.qxd
A.8 Transmission Lines 539
impedance that matches the characteristic impedance of the line. Thus,
the system reaches steady state at time t  2td, and the transmission
line becomes equivalent to an equipotential wire with voltage VS and
current I  0.
A.8.3 Short Termination
Figure A.24 shows a transmission line terminated with a short circuit to
ground. Thus, the voltage at point C must always be 0.
As in the previous examples, the voltages on the line are initially 0.
When the switch closes, a wave of voltage, , begins propagating
down the line (Figure A.25). When it reaches the end of the line, it must
reflect with opposite polarity. The reflected wave, with voltage ,
adds to the incident wave, ensuring that the voltage at point C
remains 0. The reflected wave reaches the source at time t  2td and is
absorbed by the source impedance. At this point, the system reaches
steady state, and the transmission line is equivalent to an equipotential
wire with voltage V  0.
V  VS
2
V  VS
2
t
VS/2
VS
5t
d t /3 d/3 t
d
VB
t
VS
t
d
VC
(c)
(b)
(a)
t
t
d
VA
VS/2
VS
2t
d
Figure A.23 Voltage waveforms
for Figure A.22 at points
A, B, and C
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZS = 50 Ω Figure A.22 Transmission line
with open load termination
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZS = 50 Ω
Figure A.24 Transmission line with short termination
A.8.4 Mismatched Termination
The termination impedance is said to be mismatched when it does not
equal the characteristic impedance of the line. In general, when an incident
wave reaches a mismatched termination, part of the wave is absorbed and
part is reflected. The reflection coefficient, kr, indicates the fraction of the
incident wave (Vi
) that is reflected: Vr  krVi
.
Section A.8.8 derives the reflection coefficient using conservation of
current arguments. It shows that, when an incident wave flowing along a
Appendi
transmission line of characteristic impedance Z0 reaches a termination
impedance, ZT, at the end of the line, the reflection coefficient is
(A.7)
Note a few special cases. If the termination is an open circuit (ZT  ),
kr  1, because the incident wave is entirely reflected (so the current out the
end of the line remains zero). If the termination is a short circuit (ZT  0),
kr  1, because the incident wave is reflected with negative polarity (so
the voltage at the end of the line remains zero). If the termination is a
matched load (ZT  Z0), kr  0, because the incident wave is absorbed.
Figure A.26 illustrates reflections in a transmission line with a mismatched load termination of 75 . ZT  ZL  75 , and Z0  50 , so
kr  1/5. As in previous examples, the voltage on the line is initially 0.
When the switch closes, a wave of voltage, , propagates down the
line, reaching the end at t  td. When the incident wave reaches the
termination at the end of the line, one fifth of the wave is reflected, and
the remaining four fifths flows into the load impedance. Thus, the
reflected wave has a voltage . The total voltage at point
C is the sum of the incoming and reflected voltages, .
At t  2td, the reflected wave reaches point A, where it is absorbed by the
matched 50  termination, ZS. Figure A.27 plots the voltages and currents
along the line. Again, note that, in steady state (in this case at time t 
2td), the transmission line is equivalent to an equipotential wire, as shown
in Figure A.28. At steady-state, the system acts like a voltage divider, so
Reflections can occur at both ends of the transmission line. Figure A.29
shows a transmission line with a source impedance, ZS, of 450  and an
open termination at the load. The reflection coefficients at the load and
source, krL and krS, are 4/5 and 1, respectively. In this case, waves reflect off
both ends of the transmission line until a steady state is reached.
The bounce diagram, shown in Figure A.30, helps visualize reflections
off both ends of the transmission line. The horizontal axis represents
VA  VB  VC  VS ZL
ZL  ZS   VS 75
75  50    3VS
5
VC VS
2  VS
10  3VS
5
V VS
2  1
5  VS
10
V  VS
2
kr  ZT Z0
ZT  Z0
540 APPENDIX A Digital System Implementation
t
t
d
VA
VS/2
2t
d
(a)
t
VS/2
5t
d t
d/3 t /3 d
VB
(b)
(c)
t
VS
t
d
VC
Figure A.25 Voltage waveforms
for Figure A.24 at points A, B,
and C
VS
t = 0
Z0 = 50 Ω
length = l
td = l/v
A C B
l/3
ZL = 75 Ω
ZS = 50 Ω
+
–
Figure A.26 Transmission line with mismatched termination
Appendix A.qxd 1/30/07 2:57 
distance along the transmission line, and the vertical axis represents time,
increasing downward. The two sides of the bounce diagram represent the
source and load ends of the transmission line, points A and C. The incoming and reflected signal waves are drawn as diagonal lines between points
A and C. At time t  0, the source impedance and transmission line
behave as a voltage divider, launching a voltage wave of from
point A toward point C. At time t  td, the signal reaches point C and is
completely reflected (krL  1). At time t  2td, the reflected wave of
reaches point A and is reflected with a reflection coefficient, krS  4/5,
to produce a wave of traveling toward point C, and so forth.
The voltage at a given time at any point on the transmission line is the
sum of all the incident and reflected waves. Thus, at time t  1.1td, the
voltage at point C is . At time t  3.1td, the voltage at point
C is , and so forth. Figure A.31 plots the voltages
against time. As t approaches infinity, the voltages approach steady-state
with VA  VB  VC  VS.
VS
10  VS
10  2VS
25  2VS
25  9VS
25
VS
10  VS
10  VS
5
2VS
25
VS
10
VS
10
A.8 Transmission Lines 541
Figure A.27 Voltage waveforms for Figure A.26 at points A, B, and C
VS
t = 0
length = l
td = l/v
A C B
l/3
ZS = 450 Ω
+
Z0 = 50 Ω
–
Figure A.29 Transmission line with mismatched source and load terminations
A, B, C
VS
ZS = 50 Ω
+ ZL = 75 Ω –
Figure A.28 Equivalent circuit of Figure A.26 at steady-state
Voltage
A C
10
VS
10
25
25
125
125
t = td
krS = 4/5
B
VS
2VS
2VS
8VS
8VS
t = 3td
t = 5td
t = 6td
t = 4td
t = 2td
t = 0
td krL = 1
Figure A.30 Bounce diagram for
Figure A.29
t t
d
VA
VS/2
3VS/5
2t
d
(a)
t
VB
3VS/5
VS/2
t
d 5t
d t /3 d /3
(b)
t t
d
VC
(c)
3VS/5
Appendix A.q
A.8.5 When to Use Transmission Line Models
Transmission line models for wires are needed whenever the wire delay,
td, is longer than a fraction (e.g., 20%) of the edge rates (rise or fall
times) of a signal. If the wire delay is shorter, it has an insignificant effect
on the propagation delay of the signal, and the reflections dissipate while
the signal is transitioning. If the wire delay is longer, it must be considered in order to accurately predict the propagation delay and waveform
of the signal. In particular, reflections may distort the digital characteristic of a waveform, resulting in incorrect logic operations.
Recall that signals travel on a PCB at about 15 cm/ns. For TTL
logic, with edge rates of 10 ns, wires must be modeled as transmission
lines only if they are longer than 30 cm (10 ns  15 cm/ns  20%). PCB
traces are usually less than 30 cm, so most traces can be modeled as
ideal equipotential wires. In contrast, many modern chips have edge
rates of 2 ns or less, so traces longer than about 6 cm (about 2.5 inches)
must be modeled as transmission lines. Clearly, use of edge rates that are
crisper than necessary just causes difficulties for the designer.
Breadboards lack a ground plane, so the electromagnetic fields of each
signal are nonuniform and difficult to model. Moreover, the fields interact
with other signals. This can cause strange reflections and crosstalk
between signals. Thus, breadboards are unreliable above a few megahertz.
In contrast, PCBs have good transmission lines with consistent
characteristic impedance and velocity along the entire line. As long as
they are terminated with a source or load impedance that is matched
to the impedance of the line, PCB traces do not suffer from reflections.
A.8.6 Proper Transmission Line Terminations
There are two common ways to properly terminate a transmission line,
shown in Figure A.32. In parallel termination (Figure A.32(a)), the driver
has a low impedance (ZS ≈ 0). A load resistor (ZL) with impedance Z0 is
placed in parallel with the load (between the input of the receiver gate and
542 APPENDIX A Digital System Implementation
Figure A.31 Voltage and current waveforms for Figure A.29
t
VS
10
25
25
VA
2td
7VS
16VS
4td 6td
(a)
t
3 3 3 3 3 3
10
25
25
5
25
VB
VS
VS
7VS
9VS
16VS
td 5td 7td 11td 13td 17td
(b)
t
5
25
125
VC
(c)
td 3td 5td
9VS
61VS
VS
Ap
ground). When the driver switches from 0 to VDD, it sends a wave with
voltage VDD down the line. The wave is absorbed by the matched load termination, and no reflections take place. In series termination (Figure
A.32(b)), a source resistor (Z5) is placed in series with the driver to raise
the source impedance to Z0. The load has a high impedance (ZL ≈ ).
When the driver switches, it sends a wave with voltage VDD/2 down
the line. The wave reflects at the open circuit load and returns, bringing the
voltage on the line up to VDD. The wave is absorbed at the source termination. Both schemes are similar in that the voltage at the receiver transitions
from 0 to VDD at t  td, just as one would desire. They differ in power
consumption and in the waveforms that appear elsewhere along the line.
Parallel termination dissipates power continuously through the load resistor when the line is at a high voltage. Series termination dissipates no DC
power, because the load is an open circuit. However, in series terminated
lines, points near the middle of the transmission line initially see a voltage
of VDD/2, until the reflection returns. If other gates are attached to the
A.8 Transmission Lines 543
driver A C B
gate
(b)
receiver
gate
series
termination
resistor
ZL ≈ ∞
t
t
td
t
VDD
VA
VB
VC
t
t
t
V DD
VDD
VB
VC
A C B
(a)
driver
gate
receiver
gate
parallel
termination
resistor
ZS ≈ 0
Z0
ZL = Z0
ZS = Z0
Z0
VA
VDD /2
VDD
td/3 td
VDD
td
td 2td
5td td /3 td /3
td
VDD
VDD /2
Figure A.32 Termination schemes: (a) parallel, (b) series
A
middle of the line, they will momentarily see an illegal logic level.
Therefore, series termination works best for point-to-point communication
with a single driver and a single receiver. Parallel termination is better for a
bus with multiple receivers, because receivers at the middle of the line never
see an illegal logic level.
A.8.7 Derivation of Z0*
Z0 is the ratio of voltage to current in a wave propagating along a transmission line. This section derives Z0; it assumes some previous knowledge
of resistor-inductor-capacitor (RLC) circuit analysis.
Imagine applying a step voltage to the input of a semi-infinite transmission line (so that there are no reflections). Figure A.33 shows the
semi-infinite line and a model of a segment of the line of length dx. R, L,
and C, are the values of resistance, inductance, and capacitance per unit
length. Figure A.33(b) shows the transmission line model with a resistive
component, R. This is called a lossy transmission line model, because
energy is dissipated, or lost, in the resistance of the wire. However, this loss
is often negligible, and we can simplify analysis by ignoring the resistive
component and treating the transmission line as an ideal transmission line,
as shown in Figure A.33(c).
Voltage and current are functions of time and space throughout the
transmission line, as given by Equations A.8 and A.9.
(A.8)
(A.9)
Taking the space derivative of Equation A.8 and the time derivative of
Equation A.9 and substituting gives Equation A.10, the wave equation.
(A.10)
Z0 is the ratio of voltage to current in the transmission line, as illustrated in Figure A.34(a). Z0 must be independent of the length of the
line, because the behavior of the wave cannot depend on things at a distance. Because it is independent of length, the impedance must still equal
Z0 after the addition of a small amount of transmission line, dx, as
shown in Figure A.34(b).
∂2
∂x2V(x, t)  LC ∂2
∂t
2 I(x, t)
∂
∂x I(x, t)C ∂
∂t V(x, t)
∂
∂xV(x, t) L∂
∂t I(x, t)
544 APPENDIX A Digital System Implementation
dx x
(a)
Cdx
Rdx
dx
Ldx
(b)
Ldx
Cdx
dx
(c)
Figure A.33 Transmission line
models: (a) semi-infinite cable,
(b) lossy, (c) ideal
App
Using the impedances of an inductor and a capacitor, we rewrite the
relationship of Figure A.34 in equation form:
Z0  jLdx  [Z0||(1/(jCdx))] (A.11)
Rearranging, we get
Z2
0(jC)jL 2Z0LCdx0 (A.12)
Taking the limit as dx approaches 0, the last term vanishes and we find that
(A.13)
A.8.8 Derivation of the Reflection Coefficient*
The reflection coefficient, kr, is derived using conservation of current.
Figure A.35 shows a transmission line with characteristic impedance, Z0,
and load impedance, ZL. Imagine an incident wave of voltage Vi and current Ii. When the wave reaches the termination, some current, IL, flows
through the load impedance, causing a voltage drop, VL. The remainder
of the current reflects back down the line in a wave of voltage, Vr, and
current, Ir. Z0 is the ratio of voltage to current in waves propagating
along the line, so .
The voltage on the line is the sum of the voltages of the incident
and reflected waves. The current flowing in the positive direction on the
line is the difference between the currents of the incident and reflected
waves.
VL  Vi  Vr (A.14)
IL  Ii  Ir (A.15)
Vi
Ii  Vr
Ir  Z0
Z0  v L
C
A.8 Transmission Lines 545
jωLdx
dx
jωCdx
1
V Z0
–
+
I
V
–
+
I
(a)
Z0
(b)
Figure A.34 Transmission line
model: (a) for entire line and
(b) with additional length, dx
Figure A.35 Transmission line
showing incoming, reflected, and
load voltages and currents
Z0
ZL
Ii ,Vi
+
–
IL
Ir ,Vr
VL
Appendi
546 APPENDIX A Digital System Implementation
Using Ohm’s law and substituting for IL, Ii, and Ir in Equation A.15,
we get
(A.16)
Rearranging, we solve for the reflection coefficient, kr:
(A.17)
A.8.9 Putting It All Together
Transmission lines model the fact that signals take time to propagate
down long wires because the speed of light is finite. An ideal transmission line has uniform inductance, L, and capacitance, C, per unit length
and zero resistance. The transmission line is characterized by its characteristic impedance, Z0, and delay, td, which can be derived from the
inductance, capacitance, and wire length. The transmission line has
significant delay and noise effects on signals whose rise/fall times are less
than about 5td. This means that, for systems with 2 ns rise/fall times,
PCB traces longer than about 6 cm must be analyzed as transmission
lines to accurately understand their behavior.
A digital system consisting of a gate driving a long wire attached to
the input of a second gate can be modeled with a transmission line as
shown in Figure A.36. The voltage source, source impedance (ZS), and
switch model the first gate switching from 0 to 1 at time 0. The driver
gate cannot supply infinite current; this is modeled by ZS. ZS is usually
small for a logic gate, but a designer may choose to add a resistor in
series with the gate to raise ZS and match the impedance of the line. The
input to the second gate is modeled as ZL. CMOS circuits usually have
little input current, so ZL may be close to infinity. The designer may also
choose to add a resistor in parallel with the second gate, between the
gate input and ground, so that ZL matches the impedance of the line.
Vr
Vi
 ZLZ0
ZL  Z0
 kr
Vi  Vr
ZL
 Vi
Z0
 Vr
Z0
(a)
long wire
driver
gate
receiver
gate
VS td , Z0
Z
–
S
(b)
receiver
gate long wire driver gate
ZL
t = 0
+
Figure A.36 Digital system
modeled with transmission line
App
When the first gate switches, a wave of voltage is driven onto the
transmission line. The source impedance and transmission line form a
voltage divider, so the voltage of the incident wave is
(A.18)
At time td, the wave reaches the end of the line. Part is absorbed by
the load impedance, and part is reflected. The reflection coefficient, kr,
indicates the portion that is reflected: kr  Vr/Vi, where Vr is the voltage
of the reflected wave and Vi is the voltage of the incident wave.
(A.19)
The reflected wave adds to the voltage already on the line. It reaches
the source at time 2td, where part is absorbed and part is again reflected.
The reflections continue back and forth, and the voltage on the line
eventually approaches the value that would be expected if the line were a
simple equipotential wire.
A.9 ECONOMICS
Although digital design is so much fun that some of us would do it for
free, most designers and companies intend to make money. Therefore,
economic considerations are a major factor in design decisions.
The cost of a digital system can be divided into nonrecurring
engineering costs (NRE), and recurring costs. NRE accounts for the cost
of designing the system. It includes the salaries of the design team, computer and software costs, and the costs of producing the first working
unit. The fully loaded cost of a designer in the United States in 2006
(including salary, health insurance, retirement plan, and a computer with
design tools) is roughly $200,000 per year, so design costs can be significant. Recurring costs are the cost of each additional unit; this includes
components, manufacturing, marketing, technical support, and shipping.
The sales price must cover not only the cost of the system but also
other costs such as office rental, taxes, and salaries of staff who do not
directly contribute to the design (such as the janitor and the CEO). After
all of these expenses, the company should still make a profit.
Example A.3 BEN TRIES TO MAKE SOME MONEY
Ben Bitdiddle has designed a crafty circuit for counting raindrops. He decides to
sell the device and try to make some money, but he needs help deciding what
implementation to use. He decides to use either an FPGA or an ASIC. The
kr  ZLZ0
ZL  Z0
Vi  VS
Z0
Z0  ZS
A.9 Economics 547
App
development kit to design and test the FPGA costs $1500. Each FPGA costs
$17. The ASIC costs $600,000 for a mask set and $4 per chip.
Regardless of what chip implementation he chooses, Ben needs to mount the
packaged chip on a printed circuit board (PCB), which will cost him $1.50 per
board. He thinks he can sell 1000 devices per month. Ben has coerced a team
of bright undergraduates into designing the chip for their senior project, so it
doesn’t cost him anything to design.
If the sales price has to be twice the cost (100% profit margin), and the product
life is 2 years, which implementation is the better choice?
SOLUTION: Ben figures out the total cost for each implementation over 2 years, as
shown in Table A.4. Over 2 years, Ben plans on selling 24,000 devices, and the
total cost is given in Table A.4 for each option. If the product life is only two
years, the FPGA option is clearly superior. The per-unit cost is $445,500/24,000
$18.56, and the sales price is $37.13 per unit to give a 100% profit margin. The
ASIC option would have cost $732,000/24,000  $30.50 and would have sold for
$61 per unit.
Example A.4 BEN GETS GREEDY
After seeing the marketing ads for his product, Ben thinks he can sell even more
chips per month than originally expected. If he were to choose the ASIC option,
how many devices per month would he have to sell to make the ASIC option
more profitable than the FPGA option?
SOLUTION: Ben solves for the minimum number of units, N, that he would need
to sell in 2 years:
$600,000  (N  $5.50)  $1500  (N  $18.50)
548 APPENDIX A Digital System Implementation
Table A.4 ASIC vs FPGA costs
Cost ASIC FPGA
NRE $600,000 $1500
chip $4 $17
PCB $1.50 $1.50
TOTAL $600,000  (24,000  $5.50) $1500  (24,000  $18.50)
 $732,000  $445,500
per unit $30.50 $18.56
Appendix 
A.9 Economics 549
Solving the equation gives N  46,039 units, or 1919 units per month. He would
need to almost double his monthly sales to benefit from the ASIC solution.
Example A.5 BEN GETS LESS GREEDY
Ben realizes that his eyes have gotten too big for his stomach, and he doesn’t
think he can sell more than 1000 devices per month. But he does think the product life can be longer than 2 years. At a sales volume of 1000 devices per month,
how long would the product life have to be to make the ASIC option worthwhile?
SOLUTION: If Ben sells more than 46,039 units in total, the ASIC option is the
best choice. So, Ben would need to sell at a volume of 1000 per month for at
least 47 months (rounding up), which is almost 4 years. By then, his product is
likely to be obsolete.
Chips are usually purchased from a distributor rather than directly
from the manufacturer (unless you are ordering tens of thousands of
units). Digikey (www.digikey.com) is a leading distributor that sells a
wide variety of electronics. Jameco (www.jameco.com) and All
Electronics (www.allelectronics.com) have eclectic catalogs that are
competitively priced and well suited to hobbyists.
A

MIPS Instructions B
This appendix summarizes MIPS instructions used in this book. Tables
B.1–B.3 define the opcode and funct fields for each instruction, along
with a short description of what the instruction does. The following notations are used:
 [reg]: contents of the register
 imm: 16-bit immediate field of the I-type instruction
 addr: 26-bit address field of the J-type instruction
 SignImm: sign-extended immediate
 {{16{imm[15]}}, imm}
 ZeroImm: zero-extended immediate
 {16b0, imm}
 Address: [rs]  SignImm
 [Address]: contents of memory location Address
 BTA: branch target address1
 PC  4  (SignImm  2)
 JTA: jump target address
 {(PC  4)[31:28], addr, 2b0}
551
1 The SPIM simulator has no branch delay slot, so BTA is PC  (SignImm  2). Thus, if you use the SPIM
assembler to create machine code for a real MIPS processor, you must decrement the immediate field by 1 to
compensate.
Appendix B.qxd 
Table B.1 Instructions, sorted by opcode
552 APPENDIX B MIPS Instructions
Opcode Name Description Operation
000000 (0) R-type all R-type instructions see Table B.2
000001 (1) bltz/bgez branch less than zero/ if ([rs]  0) PC  BTA/
(rt  0/1) branch greater than or if ([rs]  0) PC  BTA
equal to zero
000010 (2) j jump PC  JTA
000011 (3) jal jump and link $ra  PC4, PC  JTA
000100 (4) beq branch if equal if ([rs][rt]) PC  BTA
000101 (5) bne branch if not equal if ([rs]![rt]) PC  BTA
000110 (6) blez branch if less than or equal to zero if ([rs]  0) PC  BTA
000111 (7) bgtz branch if greater than zero if ([rs]  0) PC  BTA
001000 (8) addi add immediate [rt]  [rs]  SignImm
001001 (9) addiu add immediate unsigned [rt]  [rs]  SignImm
001010 (10) slti set less than immediate [rs]  SignImm ? [rt]1 : [rt]0
001011 (11) sltiu set less than immediate unsigned [rs]  SignImm ? [rt]1 : [rt]0
001100 (12) andi and immediate [rt]  [rs] & ZeroImm
001101 (13) ori or immediate [rt]  [rs] | ZeroImm
001110 (14) xori xor immediate [rt]  [rs]  ZeroImm
001111 (15) lui load upper immediate [rt]  {Imm, 16b0}
010000 (16) mfc0, move from/to coprocessor 0 [rt]  [rd]/[rd]  [rt]
(rs  0/4) mtc0 (rd is in coprocessor 0)
010001 (17) F-type fop  16/17: F-type instructions see Table B.3
010001 (17) bc1f/bc1t fop  8: branch if fpcond is if (fpcond  0) PC  BTA/
(rt  0/1) FALSE/TRUE if (fpcond  1) PC  BTA
100000 (32) lb load byte [rt]  SignExt ([Address]7:0)
100001 (33) lh load halfword [rt]  SignExt ([Address]15:0)
100011 (35) lw load word [rt]  [Address]
100100 (36) lbu load byte unsigned [rt]  ZeroExt ([Address]7:0)
100101 (37) lhu load halfword unsigned [rt]  ZeroExt ([Address]15:0)
101000 (40) sb store byte [Address]7:0  [rt]7:0
(continued)
Appendix B.qxd 2/1/07 9:36 PM Page 552
Table B.1 Instructions, sorted by opcode—Cont’d
Table B.2 R-type instructions, sorted by funct field
Appendix B 553
Funct Name Description Operation
000000 (0) sll shift left logical [rd]  [rt]  shamt
000010 (2) srl shift right logical [rd]  [rt]  shamt
000011 (3) sra shift right arithmetic [rd]  [rt]  shamt
000100 (4) sllv shift left logical [rd]  [rt]  [rs]4:0
variable assembly: sllv rd, rt, rs
000110 (6) srlv shift right logical [rd]  [rt]  [rs]4:0
variable assembly: srlv rd, rt, rs
000111 (7) srav shift right arithmetic [rd]  [rt]  [rs]4:0
variable assembly: srav rd, rt, rs
001000 (8) jr jump register PC  [rs]
001001 (9) jalr jump and link register $ra  PC  4, PC  [rs]
001100 (12) syscall system call system call exception
001101 (13) break break break exception
010000 (16) mfhi move from hi [rd]  [hi]
010001 (17) mthi move to hi [hi]  [rs]
010010 (18) mflo move from lo [rd]  [lo]
010011 (19) mtlo move to lo [lo]  [rs]
011000 (24) mult multiply {[hi], [lo]}  [rs] 	 [rt]
011001 (25) multu multiply unsigned {[hi], [lo]}  [rs] 	 [rt]
011010 (26) div divide [lo]  [rs]/[rt],
[hi]  [rs]%[rt]
101001 (41) sh store halfword [Address]15:0  [rt]15:0
101011 (43) sw store word [Address]  [rt]
110001 (49) lwc1 load word to FP coprocessor 1 [ft]  [Address]
111001 (56) swc1 store word to FP coprocessor 1 [Address]  [ft]
(continued)
Opcode Name Description Operation
Appendix B.qxd 2/1/07
554 APPENDIX B MIPS Instructions
Table B.2 R-type instructions, sorted by funct field—Cont’d
Table B.3 F-type instructions (fop  16/17)
Funct Name Description Operation
011011 (27) divu divide unsigned [lo]  [rs]/[rt],
[hi]  [rs]%[rt]
100000 (32) add add [rd]  [rs]  [rt]
100001 (33) addu add unsigned [rd]  [rs]  [rt]
100010 (34) sub subtract [rd]  [rs] 
 [rt]
100011 (35) subu subtract unsigned [rd]  [rs] 
 [rt]
100100 (36) and and [rd]  [rs] & [rt]
100101 (37) or or [rd]  [rs] | [rt]
100110 (38) xor xor [rd]  [rs]  [rt]
100111 (39) nor nor [rd]  ~([rs] | [rt])
101010 (42) slt set less than [rs]  [rt] ? [rd]  1 : [rd]  0
101011 (43) sltu set less than unsigned [rs]  [rt] ? [rd]  1 : [rd]  0
Funct Name Description Operation
000000 (0) add.s/add.d FP add [fd]  [fs]  [ft]
000001 (1) sub.s/sub.d FP subtract [fd]  [fs] 
 [ft]
000010 (2) mul.s/mul.d FP multiply [fd]  [fs] * [ft]
000011 (3) div.s/div.d FP divide [fd]  [fs]/[ft]
000101 (5) abs.s/abs.d FP absolute value [fd]  ([fs]  0) ? [
fs]
: [fs]
000111 (7) neg.s/neg.d FP negation [fd]  [
fs]
111010 (58) c.seq.s/c.seq.d FP equality comparison fpcond  ([fs]  [ft])
111100 (60) c.lt.s/c.lt.d FP less than comparison fpcond  ([fs]  [ft])
111110 (62) c.le.s/c.le.d FP less than or equal comparison fpcond  ([fs]  [ft])
Appendix B.qxd 2/1/07 9:36 
555
Further Reading
Berlin L., The Man Behind the Microchip: Robert Noyce and the
Invention of Silicon Valley, Oxford University Press, 2005.
The fascinating biography of Robert Noyce, an inventor of the microchip
and founder of Fairchild and Intel. For anyone thinking of working in
Silicon Valley, this book gives insights into the culture of the region, a culture influenced more heavily by Noyce than any other individual.
Colwell R., The Pentium Chronicles: The People, Passion, and Politics
Behind Intel’s Landmark Chips, Wiley, 2005.
An insider’s tale of the development of several generations of Intel’s
Pentium chips, told by one of the leaders of the project. For those considering a career in the field, this book offers views into the managment
of huge design projects and a behind-the-scenes look at one of the most
significant commercial microprocessor lines.
Ercegovac M., and Lang T., Digital Arithmetic, Morgan Kaufmann, 2003.
The most complete text on computer arithmetic systems. An excellent
resource for building high-quality arithmetic units for computers.
Hennessy J., and Patterson D., Computer Architecture: A Quantitative
Approach, 4th ed., Morgan Kaufmann, 2006.
The authoritative text on advanced computer architecture. If you are
intrigued about the inner workings of cutting-edge microprocessors, this
is the book for you.
Kidder T., The Soul of a New Machine, Back Bay Books, 1981.
A classic story of the design of a computer system. Three decades later,
the story is still a page-turner and the insights on project managment
and technology still ring true.
Pedroni V., Circuit Design with VHDL, MIT Press, 2004.
A reference showing how to design circuits with VHDL.
Thomas D., and Moorby P., The Verilog Hardware Description
Language, 5th ed., Kluwer Academic Publishers, 2002.
Ciletti M., Advanced Digital Design with the Verilog HDL, Prentice
Hall, 2003.
Both excellent references covering Verilog in more detail.
555

556 Further Reading
Verilog IEEE Standard (IEEE STD 1364).
The IEEE standard for the Verilog Hardware Description Language;
last updated in 2001. Available at ieeexplore.ieee.org.
VHDL IEEE Standard (IEEE STD 1076).
The IEEE standard for VHDL; last updated in 2004. Available from
IEEE. Available at ieeexplore.ieee.org.
Wakerly J., Digital Design: Principles and Practices, 4th ed., Prentice
Hall, 2006.
A comprehensive and readable text on digital design, and an excellent reference book.
Weste N., and Harris D., CMOS VLSI Design, 3rd ed., Addison-Wesley,
2005.
Very Large Scale Integration (VLSI) Design is the art and science of
building chips containing oodles of transistors. This book, coauthored
by one of our favorite writers, spans the field from the beginning
through the most advanced techniques used in commercial products.

557
Index
0, 23. See also LOW, OFF
1, 23. See also HIGH, ON
74xx series logic, 515–516
parts,
2:1 Mux (74157), 518
3:8 Decoder (74138), 518
4:1 Mux (74153), 518
AND (7408), 517
AND3 (7411), 517
AND4 (7421), 517
Counter (74161, 74163), 518
FLOP (7474), 515–517
NAND (7400), 517
NOR (7402), 517
NOT (7404), 515
OR (7432), 517
XOR (7486), 517
Register (74377), 518
Tristate buffer (74244), 518
schematics of, 517–518
A
add, 291
Adders, 233–240
carry-lookahead. See Carrylookahead adder
carry-propagate (CPA). See Carrypropagate adder
prefix. See Prefix adder
ripple-carry. See Ripple-carry adder
add immediate (addi), 327, 378
add immediate unsigned (addiu), 552
add unsigned (addu), 554
Addition, 14–15, 233–240, 291
floating-point, 252–255
overflow. See Overflow
two’s complement, 15, 240
underflow. See Underflow
unsigned binary, 15
Address, 485–490
physical, 485
translation, 486–489
virtual, 485–490
word alignment, 298
Addressing modes, 327–329
base, 327
immediate, 327
MIPS, 327–329
PC-relative, 327–328
pseudo-direct, 328–329
register-only, 327
Advanced microarchitecture, 435–447
branch prediction. See Branch
prediction
deep pipelines. See Deep pipelines
multiprocessors. See Multiprocessors
multithreading. See Multithreading
out-of-order processor.
See Out-of-order processor
register renaming. See Register
renaming
single instruction multiple data. See
Single instruction multiple
data (SIMD) units
superscalar processor. See
Superscalar processor
vector processor. See Single instruction multiple data (SIMD)
units
Alignment. See Word alignment
ALU. See Arithmetic/logical unit
ALU decoder, 374–376
ALU decoder truth table, 376
ALUControl, 370
ALUOp, 374–375
ALUOut, 383–385
ALUResult, 370–371
AMAT. See Average memory access time
Amdahl, Gene, 468
Amdahl’s Law, 468
Anodes, 27–28
and immediate (andi), 306, 346–347
AND gate, 20–22, 32–33
Application-specific integrated circuits
(ASICs), 523
Architectural state, 363–364
Architecture. See Instruction Set
Architecture
Arithmetic, 233–249, 305–308, 515.
See also Adders, Addition,
Comparator, Divider,
Multiplier
adders. See Adders
addition. See Addition
ALU. See Arithmetic/logical unit
circuits, 233–248
comparators. See Comparators
divider. See Divider
division. See Division
fixed-point, 249
floating-point, 252–255
logical instructions, 305–308
multiplier. See Multiplier
packed, 445
rotators. See Rotators
shifters. See Shifters
signed and unsigned, 338–339
subtraction. See Subtraction
557

Arithmetic (Continued)
subtractor. See Subtractor
underflow. See Addition, Underflow
Arithmetic/logical unit (ALU),
242–244, 515
32-bit, 244
adder. See Adders
ALUOp, 374–376
ALUResult, 370–371
ALUOut, 383–385
comparator. See Comparators
control, 242
subtractor. See Subtractor
Arrays, 314–318
accessing, 315–317
bytes and characters, 316–318
FPGA, See Field programmable gate
array
logic. See Logic arrays
memory. See Memory
RAM, 265
ROM, 266
ASCII (American Standard Code
for Information Interchange)
codes, 317
table of, 317
Assembler directives, 333
ASICs. See Application-specific integrated circuits
Assembly language, MIPS. See MIPS
assembly language
Associativity, 58–59
Average memory access time (AMAT),
467–468
Asynchronous circuits, 116–117
Asynchronous inputs, 144
Asynchronous resettable registers
HDL for, 192
Axioms. See Boolean axioms
B
Babbage, Charles, 7–8, 26
Base address, 295–296
Base register, 302, 343, 347
Base 2 number representations.
See Binary numbers
Base 8 number representations.
See Octal numbers
Base 16 number representations.
See Hexadecimal numbers
Block,
digital building. See Digital building
blocks
in code,
else block, 311
if block, 310–311
Base addressing, 327
Baudot, Jean-Maurice-Emile, 317
Behavioral modeling, 171–185
Benchmarks, 367
SPEC2000, 398–399
Biased numbers, 41. See also Floating
point
Big-Endian, 172, 296–297
Binary numbers, 9–11. See also
Arithmetic
ASCII, 317
binary coded decimal, 252
conversion. See Number
conversion
fixed point, 249–250
floating-point. See Floating-point
numbers
signed, 15–19
sign/magnitude. See Sign/
magnitude numbers
two’s complement. See Two’s
complement numbers
unsigned, 9–15
Binary to decimal conversion,
10–11
Binary to hexadecimal conversion, 12
Bit, 9–11
dirty, 482–483
least significant, 13–14
most significant, 13–14
sign, 16
use, 478–479
valid, 472
Bit cells, 258
Bit swizzling, 182
Bitwise operators, 171–174
Boole, George, 8
Boolean algebra, 56–62
axioms, 57
equation simplification, 61–62
theorems, 57–60
Boolean axioms, 57
Boolean equations, 54–56
product-of-sums (POS) canonical
form, 56
sum-of-products (SOP) canonical
form, 54–55
terminology, 54
Boolean theorems, 57–60
DeMorgan’s, 59
complement, 58, 59
consensus, 60
covering, 58
combining, 58
Branching, 308–310
calculating address, 309
conditional, 308
prediction, 437–438
target address (BTA), 327–328
unconditional (jump), 309
Branch equal (beq), 308–309
Branch not equal (bne), 308–309
Branch/control hazards, 407, 413–416.
See also Hazards
Branch prediction, 437–438
Breadboards, 532–533
Bubble, 59
pushing, 67–69
Buffer, 20
tristate, 70–71
“Bugs,” 169
Bus, 52
tristate, 71
Bypassing, 408
Bytes, 13–14
Byte-addressable memory, 295–297
Byte order, 296–297
Big-Endian, 296–297
Little-Endian, 296–297
C
C programming language, 290
overflows, 339
strings, 318
Caches, 468–484. See also Memory
accessing, 472–473, 476–477, 479
advanced design, 479–483
associativity, 469, 474–475,
478–479
block size, 476–477
blocks, 469–470, 476
capacity, 468–470
data placement, 469–470
data replacement, 478–479
558 Index

definition, 468
direct mapped, 470–474
dirty bit, 482–483
entry fields, 472–473
evolution of MIPS, 483
fully associative, 475–476
hit, 466
hit rate, 467
IA-32 systems, 499–500
level 2 (L2), 480
mapping, 470–478
miss, 466
capacity, 481
compulsory, 481
conflict, 481
miss rate, 467
miss rate versus cache parameters,
481–482
miss penalty, 476
multiway set associative, 474–475
nonblocking, 500
organizations, 478
performance, 467
set associative, 470, 474–475
tag, 471–472
use bit, 478–479
valid bit, 472–473
write policy, 482–483
CAD. See Computer-aided design
Canonical form, 53
Capacitors, 28
Capacity miss, 481
Carry-lookahead adder, 235–237
Carry propagate adder (CPA), 274
Cathodes, 27–28
Cause register, 337–338
Chips, 28, 449
74xx series logic. See 74xx series
logic
Circuits, 82–83, 87–88
74xx series logic. See 74xx series
logic
application-specific integrated
(ASIC), 523
arithmetic. See Arithmetic
asynchronous, 116–117
bistable device, 147
combinational, 53
definition, 51
delays, 73–75, 84–87
calculating, 86–87
dynamic, 273
multiple-output, 64
pipelining, 152
priority, 65, 202, 203
synchronous sequential, 114–116
synthesized, 186–190, 193–195,
199, 200
timing, 84–91
timing analysis, 138
types, 51–54
without glitch, 91
CISC. See Complex instruction set
computers
CLBs. See Configurable logic blocks
Clock cycle. See Clock period
Clock period, 135–139
Clock rate. See Clock period
Clock cycles per instruction (CPI),
367–368
Clustered computers, 447
Clock skew, 140–143
CMOS. See Complementary MetalOxide-Semiconductor Logic
Code, 303
Code size, 334
Combinational composition, 52–53
Combinational logic design, 51–100
Boolean algebra, 56–62
Boolean equations, 54–56
building blocks, 79–84
delays, 85–87
don’t cares, 65
HDLs and. See Hardware description languages
Karnaugh maps, 71–79
logic, 62–65
multilevel, 65–69
overview, 51–54
precedence, 54
timing, 84–91
two-level, 65–66
X’s (contention). See Contention
X’s (don’t cares). See Don’t cares
Z’s (floating). See Floating
Comparators, 240–241
equality, 241
magnitude, 241, 242
Compiler, 331–333
Complementary Metal-OxideSemiconductor Logic (CMOS), 25
bubble pushing, 69
logic gates, 31–33
NAND gate, 32
NOR gate, 32–33
NOT gate, 31
transistors, 26–34
Complex instruction set computers
(CISC), 292, 341
Compulsory miss, 481
Computer-aided design (CAD), 167
Computer Organization and Design
(Patterson and Hennessy),
290, 363
Complexity management, 4–6
abstraction, 4–5
discipline, 5–6
hierarchy, 6
modularity, 6
regularity, 6
Conditional assignment, 175–176
Conditional branches, 308
Condition codes, 344
Conflict misses, 481
Conditional statements, 310–311
Constants, 298. See also Immediates
Contamination delay, 84–88
Contention (X), 69–70
Context switch, 446
Control signals, 79, 242–243
Control unit, 364, 366, 374–406
multicycle MIPS processor FSM,
381–395
pipelined MIPS processor, 405–406
single-cycle MIPS processor,
374–377
Configurable logic blocks (CLBs),
268–272
Control hazards. See Branch/control
hazards, Pipelining
Coprocessor 0, 338
Counters, 254
Covalent bond, 27
CPA. See Carry propagate adder
CPI. See Clock cycles per instruction
Critical path, 85–89
Cyclic paths, 114
Cycle time. See Clock Period
D
Data hazards. See Hazards
Data memory, 365
Data sheets, 523–528
Datapath, 364. See also MIPS microprocessors
Index 559

Datapath (Continued)
elements, 364
multicycle MIPS processor, 382–388
pipelined MIPS processor, 404
single-cycle MIPS processor,
368–374
Data segments. See Memory map
Data types. See Hardware description
languages
DC. See Direct current
Decimal numbers, 9
conversion to binary and hexadecimal. See Number conversion
scientific notation, 249–250
Decimal to Binary conversion, 11
Decimal to hexadecimal conversion, 13
Decoders
implementation, 83
logic, 83
parameterized, 212
Deep pipelines, 435–436
Delay, 182
DeMorgan, Augustus, 56
DeMorgan’s theorem, 59, 60
Dennard, Robert, 260
Destination register, 301, 305–306,
370–371, 377–378
Device under test (DUT), 214–218.
See also Unit under test
Device driver, 496, 498
Dice, 28
Digital abstraction, 4–5, 8–9, 22–26
DC transfer characteristics, 23–24
logic levels, 22–23
noise margins, 23
supply voltage, 22
Digital design
abstraction, 7–9
discipline, 5–6
hierarchy, 6
modularity, 6
regularity, 6
Digital system implementation, 515–548
74xx series logic. See 74xx series
logic
application-specific integrated
circuits (ASICs), 523
data sheets, 523–528
economics, 546–548
logic families, 529–531
overview, 515
packaging and assembly,
531–534
breadboards, 532
packages, 531–532
printed circuit boards, 533–534
programmable logic, 516–523
transmission lines. See Transmission
lines
Diodes, 27–28
DIP. See Dual-inline package
Direct current (DC), 23, 24
transfer characteristics, 23–24, 25
Direct mapped cache, 470–474
Dirty bit, 482–483
Discipline, 5–6
Disk. See Hard disk
divide (div), 308
divide unsigned (divu), 339
Divider, 247–248
Division, 247–248
floating-point, 253
instructions, 308
Divisor, 247
Don’t care (X), 65
Dopant atoms, 27
Double. See Double-precision floatingpoint numbers
Double-precision floating point numbers, 251–252
DRAM. See Dynamic random access
memory
Driver. See Device driver
Dual-inline package (DIP), 28, 531
DUT. See Device under test
Dynamic discipline, 134
Dynamic data segment, 331
Dynamic random access memory
(DRAM), 257, 260, 463
E
Edison, Thomas, 169
Edge-triggered digital systems,
108, 112
Equations
simplification, 61–62
Electrically erasable programmable read
only memory (EEPROM), 263
Enabled registers, 193
HDL for, 193
EPC. See Exception Program Counter
register
Erasable programmable read only
memory (EPROM), 263
Exceptions, 337–339
Exception handler, 337–338
Exception program counter (EPC),
337–338
Exclusive or. See XOR
Executable file, 334
Execution time, 367
Exponent, 250–253
F
Failures, 144–146
FDIV bug, 253
FET. See Field effect transistors
Field programmable gate array (FPGA),
268–272, 521–523
Field effect transistors, 26
FIFO. See First-in-first-out queue. See
also Queue
Finite state machines (FSMs),
117–133
design example, 117–123
divide-by-3, 207–208
factoring, 129–132
HDLs and, 206–213
Mealy machines. See Mealy machines
Moore machines. See Moore
machines
Moore versus Mealy machines,
126–129
state encodings, 123–126
state transition diagram, 118–119
First-in-first-out (FIFO) queue, 508
Fixed-point numbers, 249–250
Flash memory, 263–264
Flip-flops, 103–112, 257. See also
Registers
comparison with latches, 106, 112
D, 108
enabled, 109–110
register, 108–109
resettable, 110, 427
asynchronous, 192
synchronous, 192
transistor count, 108
transistor-level, 110–111
Floating (Z), 69–71
Floating-point numbers, 250–253
560 Index

addition, 252–255. See also
Addition
converting binary or decimal to. See
Number conversions
division, 253. See also Division
double-precision, 251–252
FDIV bug. See FDIV bug
floating-point unit (FPU), 253
instructions, 340–341
rounding, 252
single-precision, 251–252
special cases
infinity, 251
not a number (NaN), 251
Forwarding, 408–409
Fractional numbers, 274
FPGA. See Field programmable gate
array
Frequency, 135. See also Clock period
FSM. See Finite state machines
Full adder, 52, 178. See also Adder,
Addition
HDL using always/process, 197
using nonblocking assignments,
204
Fully associative cache, 475–476
Funct field, 299–300
Functional specification, 51
Functions. See Procedure calls
Fuse, 263
G
Gates, 19–22
AND, 20–22, 32–33
NAND, 21, 32
NOR, 21, 32–33
OR, 21
transistor-level implementation, 262
XOR, 21
XNOR, 21
Gedanken-Experiments on Sequential
Machines (Moore), 111
Generate signal, 235, 237
Glitches, 75, 88–91
Global pointer ($gp), 294, 331. See
also Static data segment
Gray, Frank, 65
Gray codes, 65, 72
Gulliver’s Travels (Swift), 297
H
Half word, 339
Half adder, 233–234
Hard disk, 484
Hard drive. See Hard disk
Hardware reduction, 66–67
Hardware description languages
(HDLs), 167–230
assignment statements, 177
behavioral modeling, 171–184
combinational logic, 171–185,
195–206
bit swizzling, 182
bitwise operators, 171–174
blocking and nonblocking
assignments, 201–206
case statements, 198–199
conditional assignment, 175–176
delays, 182–183
if statements, 199–201
internal variables, 176–178
numbers, 179
precedence, 178–179
reduction operators, 174
synthesis tools. See Synthesis Tools
Verilog, 201
VHDL libraries and types,
183–185
Z’s and X’s, 179–182
finite state machines, 206–213
generate statement, 213
generic building blocks, 426
invalid logic level, 181
language origins, 168–169
modules, 167–168
origins of, 168–169
overview, 167
parameterized modules, 211–213
representation, 421–431
sequential logic, 190–195, 205
enabled registers, 193
latches, 195
multiple registers, 194–195
registers, 190–191. See also
Registers
resettable registers, 191–193
simulation and synthesis, 169–171
single-cycle processor, 422
structural modeling, 185–189
testbenches, 214–218, 428–431
Hazards, 75, 406–418. See also
Glitches
control hazards, 413–416
data hazards 408–413
solving, 408–416
forwarding, 408–410
stalls, 410–413
WAR. See Write after read
WAW. See Write after write
Hazard unit, 408, 411, 419
Heap, 331
HDLs. See Hardware description
languages
Hennessy, John, 290, 364
Hexadecimal numbers, 11–13
to binary conversion table, 12
Hierarchy, 6, 189
HIGH, 23. See also 1, ON
High-level programming languages,
290–294
translating into assembly, 290–291
compiling, linking, and launching,
330–331
Hit, 466
High impedance. See Floating, Z
High Z. See Floating, High
impedance, Z
Hold time, 133–154
I
I-type instruction, 301–302
IA-32 microprocessor, 290, 341–349,
447–453
branch conditions, 346
cache systems, 499–500
encoding, 346–348
evolution, 448, 500
instructions, 344, 345
memory and input/output (I/O)
systems, 499–502
operands, 342–344
programmed I/O, 502
registers, 342
status flags, 344
virtual memory, 501
IEEE 754 floating-point standard, 251
Idempotency, 58
Idioms, 171
IEEE, 169
Index 561

If statements, 199–201, 310
If/else statements, 311
ILP. See Instruction-level parallelism
Immediates, 298
Immediate addressing, 327
Information, amount of, 8
IorD, 385
I/O (input/output), 337, 494–502
communicating with, 494–495
device driver, 496, 498
devices, 494–496. See also
Peripheral devices
memory interface, 494, 502
memory-mapped I/O, 494–499
Inputs, asynchronous, 144–145
Instruction encoding. See Machine
Language
Instruction register (IR), 383, 390
Instruction set architecture (ISA),
289–361. See also MIPS instruction set architecture
Input/output blocks (IOBs), 268
Input terminals, 51
Institute of Electrical and Electronics
Engineers, 250
Instruction decode, 401–402
Instruction encoding. See Instruction
format
Instruction format
F-type, 340
I-type, 301–302
J-type, 302
R-type, 299–300
Instruction-level parallelism (ILP), 443,
446
Instruction memory, 365
Instruction set. See Instruction set
architecture
Instructions. See also Language
arithmetic/logical, 304–308
floating-point, 340–341
IA-32, 344–346
I-type, 301–302
J-type, 302
loads. See Loads
multiplication and division, 308
pseudoinstructions, 336–337
R-type, 299–300
set less than, 339
shift, 306–307
signed and unsigned, 338–339
Intel, 30, 111, 290, 348, 367
Inverter. See NOT gate
Integrated circuits (ICs), 26, 137,
515, 532
costs, 137, 169
manufacturing process, 515
Intel. See IA-32 microprocessors
Interrupts. See Exceptions
An Investigation of the Laws of
Thought (Boole), 8
Involution, 58
IOBs. See Input/output blocks
I-type instructions, 301–302
J
Java, 316. See also Language
JTA. See Jump target address
J-type instructions, 302
Jump, 309–310. See also Branch,
unconditional, Programming
Jump target address (JTA), 329
K
K-maps. See Karnaugh maps
Karnaugh, Maurice, 64, 71
Karnaugh maps (K-maps), 71–79
logic minimization using, 73–76
prime implicants, 61, 74
seven-segment display decoder,
75–77
with “don’t cares,” 78
without glitches, 91
Kilby, Jack, 26
Kilobyte, 14
K-maps. See Karnaugh maps
L
Labels, 308–309
Language. See also Instructions
assembly, 290–299
high-level, 290–294
machine, 299–304
mnemonic, 291
translating assembly to machine, 300
Last-in-first-out (LIFO) queue, 321. See
also Stack, Queue
Latches, 103–112
comparison with flip-flops, 106, 112
D, 107
SR, 105–107
transistor-level, 110–111
Latency, 149
Lattice, 27
Leaf procedures, 324–325
Least recently used (LRU) replacement,
478–479
Least significant bit (lsb), 13
Least significant byte (LSB), 296
LIFO. See Last-in-first-out queue
Literal, 54, 61, 167
Little-Endian, 296–297
Load, 255
byte (lb), 317
byte unsigned (lbu), 317
half (lh), 339
immediate (li), 336
upper immediate (lui), 308
word (lw), 295–296
Loading, 335
Locality, 464
Local variables, 326–327
Logic, 62–65. See also Multilevel combinational logic; Sequential logic
design
bubble pushing. See Bubble pushing
combinational. See Combinational
logic
families, 529–531
gates. See Logic gates
hardware reduction. See Hardware
reduction, Equation simplification
multilevel. See Multilevel combinational logic
programmable, 516–523
sequential. See Sequential logic
synthesis, 170–171
two-level, 65–66
using memory arrays, 264. See also
Logic arrays
Logic arrays, 266–274
field programmable gate array,
268–272
programmable logic array, 266–268
transistor-level implementations,
273–274
Logic families, 25, 529–531
562 Index

compatibility, 26
specifications, 529, 531
Logic gates, 19–22, 173
buffer, 20
delays, 183
multiple-input gates, 21–22
two-input gates, 21
types
AND. See AND gate
AOI (and-or-invert).
See And-or-invert gate
NAND. See NAND gate
NOR. See NOR gate
NOT. See NOT gate
OAI (or-and-invert).
See Or-and-invert gate
OR. See OR gate
XOR. See XOR gate
XNOR. See XNOR gate
Logic levels, 22–23
Logical operations, 304–308
Lookup tables (LUTs), 268
Loops, 311–314
for, 313
while, 312–313
LOW, 23. See also 0, OFF
Low Voltage CMOS Logic
(LVCMOS), 25
Low Voltage TTL Logic (LVTTL), 25
LRU. See Least recently used replacement
LSB. See Least significant byte
LUTs. See Lookup tables
LVCMOS. See Low Voltage CMOS
Logic
LVTTL. See Low Voltage TTL Logic
M
Machine language, 299–304
function fields, 299
interpreting code, 302–303
I-type instructions, 301–302
J-type instructions, 302
opcodes, 299–303
R-type instructions, 299–300
stored programs, 303–304
translating from assembly language,
300–302
translating into assembly
language, 303
Main decoder, 374–379
Main memory, 466–469
Mapping, 470
Mantissa, 250, 252–253. See also
Floating-point numbers
Masuoka, Fujio, 263
MCM. See Multichip module
Mealy, George H., 111
Mealy machines, 126–129, 130, 210
combined state transition and
output table, 127
state transition diagram, 118–119
timing diagram, 131
Mean time between failures (MTBF),
146
Memory, 51, 295–298
access, 298
average memory access time
(AMAT), 467
cache. See Caches
DRAM. See Dynamic random
access memory
hierarchy, 466
interface, 464
main, 466–469
map, 330–331
dynamic data segment, 331
global data segment, 330–331
reserved segment, 331
text segment, 330
nonvolatile, 259–260
performance, 465
physical, 466, 485–486
protection, 491. See also Virtual
memory
RAM, 259
ROM, 259
separate data and instruction,
430–431
shared, 71
stack. See Stack
types
flip-flops, 105–112
latches, 105–112
DRAM, 257
registers, 108–109
register file, 261–262
SRAM, 257
virtual, 466. See also Virtual
memory
volatile, 259–261
word-addressable, 29. See Wordaddressable memory
Memory arrays, 257–266
area, 261
bit cells, 258
delay, 261
DRAM. See Dynamic random
access memory
HDL code for, 264–266
logic implementation using, 264.
See also Logic arrays
organization, 258
overview, 257–260
ports, 259
register files built using, 261–262
types, 259–260
DRAM. See Dynamic random
access memory
ROM. See Read only memory
SRAM. See Static random access
memory
Memory-mapped I/O (input/output),
494–498
address decoder, 495–496
communicating with I/O devices,
495–496
hardware, 495
speech synthesizer device driver,
498
speech synthesizer hardware,
496–497
SP0256, 496
Memory protection, 491
Memory systems, 463–512
caches. See Caches
IA-32, 499–502
MIPS, 470–478
overview, 463–467
performance analysis, 467–468
virtual memory. See Virtual
memory
Mercedes Benz, 268
Metal-oxide-semiconductor field effect
transistors (MOSFETs), 26–31.
See also CMOS, nMOS, pMOS,
transistors
Metastability, 143–144
MTBF. See Mean time between
failures
metastable state, 143
probability of failure, 145
resolution time, 144
synchronizers, 144–146
A Method of Synthesizing Sequential
Circuits (Mealy), 111
Index 563

Microarchitecture, 290, 363–461
advanced. See Advanced
microarchitecture
architectural state. See Architectural
State. See also Architecture
design process, 364–366
exception handling, 431–434
HDL representation, 421–431.
IA-32. See IA-32 microprocessor
instruction set. See Instruction set
MIPS. See MIPS microprocessor
overview, 363–366
performance analysis, 366–368
types,
advanced. See Advanced
microarchitecture
multicycle. See Multicycle MIPS
processor
pipelined. See Pipelined MIPS
processor
single-cycle. See Single-cycle
MIPS processor
Microprocessors, 3, 13
advanced. See Advanced
microarchitecture
chips. See Chips
clock frequencies, 124
IA-32. See IA-32 microprocessor
instructions. See MIPS instructions,
IA-32 instructions
MIPS. See MIPS microprocessor
Microsoft Windows, 501
Minterms, 54
Maxterms, 54
MIPS (Millions of instructions per
second). See Millions of
instructions per second
MIPS architecture. See MIPS
instruction set architecture (ISA)
MIPS assembly language. See also
MIPS instruction set
architecture
addressing modes, 327–329
assembler directives, 333
instructions, 290–292
logical instructions, 304–308
mnemonic, 291
operands, 292–298
procedure calls, 319–327
table of instructions, 336
translating machine language to, 303
translating to machine language, 300
MIPS instruction set architecture
(ISA)
addressing modes, 327–329
assembly language, 290–299
compiling, assembling, and loading,
330–335
exceptions, 337–338
floating-point instructions, 340–341
IA-32 instructions, 344–346
machine language, 299–304
MIPS instructions, 290–292
overview, 289–290
programming, 304–327
pseudoinstructions, 336–337
signed and unsigned instructions,
338–339
SPARC, 364
translating and starting a program.
See Translating and starting
a program
MIPS instructions, 551–554
formats
F-type, 340
I-type, 301–302
J-type, 302
R-type, 299–300
tables of, 552–554
opcodes, 552
R-type funct fields, 553–554
types,
arithmetic, 304–308
branching. See Branching
division, 308
floating-point, 340–341
logical, 304–308
multiplication, 308
pseudoinstructions, 336–337
MIPS microprocessor, 364
ALU, 242–244
multicycle. See Multicycle MIPS
processor
pipelined. See Pipelined MIPS
processor
single-cycle. See Single-cycle MIPS
processor
MIPS processor. See MIPS microprocessor
MIPS registers, 293–294, 308
nonpreserved, 322–324
preserved, 322–324
table of, 294
MIPS single-cycle HDL implementation, 421–431
building blocks, 426–428
controller, 423
datapath, 425
testbench, 429
top-level module, 430
Misses, 466, 481
AMAT. See Average memory access
time
cache, 466
capacity, 481
compulsory, 481
conflict, 481
page fault, 485
Miss penalty, 476
Miss rate, 467
Mnemonic, 291
Modeling, structural. See Structural
modeling
Modeling, behavioral. See Behavioral
modeling
Modularity, 6, 168
Modules, in HDL 167–168. See also
Hardware description
languages
behavioral, 168, 171
parameterized, 211–213
structural, 168
Moore, Edward F., 111
Moore, Gordon, 30
Moore machine, 126–129, 130,
208, 209
output table, 128
state transition diagram, 127
state transition table, 128
timing diagram, 131
Moore’s law, 30
MOSFETs. See Metal-oxide-semiconductor field effect transistors
Most significant bit (msb), 13
Most significant byte (MSB), 296
Move from hi (mfhi), 308
Move from lo (mflo), 308
Move from coprocessor 0 (mfc0), 338
msb. See Most significant bit
MSB. See Most significant byte
MTBF. See Mean time before failure
Multichip module (MCM), 499
Multicycle MIPS processor, 366,
381–400
control, 388–394
control FSM, 394–395
datapath, 382–388
performance analysis, 397–400
Multilevel combinational logic, 65–69.
See also Logic
Multilevel page table, 493
Multiplexers, 79–82, 175, 176, 428
564 Index

instance, 188
logic, 80–82
parameterized, 211. See also
Hardware description
languages
symbol and truth table, 79
timing, 87–88
with type conversion, 185
wide, 80
Multiplicand, 246
Multiplication, 246–247. See also
Arithmetic, Multiplier
architecture, 339
instructions, 308
Multiplier, 246–247
multiply (mult), 308, 339
multiply unsigned (multu), 339
Multiprocessors, 447
chip, 448
Multithreading, 446
Mux. See Multiplexers
N
NaN. See Not a number
Negation, 340. See also Taking the
two’s complement
Not a number (NaN), 251
NAND gate, 21, 32
nor, 179
NOR gate, 21, 32–33
NOT gate, 24, 172. See also Inverter
in HDL using always/process, 196
Nested procedure calls, 324–326
Netlist, 170
Next state, 115
Nibbles, 13–14
nMOS, 28–31
nMOS transistors, 28–31
No operation. See nop
Noise margins, 23, 24
nop, 336
Noyce, Robert, 26
Number conversion, 9–19 See also
Number systems, Binary numbers
binary to decimal, 10–11
binary to hexadecimal, 12
decimal to binary, 11
decimal to hexadecimal, 13
taking the two’s complement, 15, 240
Number systems, 9–19, 249–253
addition. See Addition
binary numbers. See Binary numbers
comparison of, 18–19
conversion of. See Number
conversions
decimal numbers. See Decimal
numbers
estimating powers of two, 14
fixed-point, 249–250. See Fixedpoint numbers
floating-point, 250–253. See
Floating-point numbers
in hardware description languages,
179
hexadecimal numbers. See
Hexadecimal numbers
negative and positive, 15–19
rounding, 252. See also Floatingpoint numbers
sign bit, 16
signed, 15–19. See also Signed
binary numbers
sign/magnitude numbers. See
Sign/magnitude numbers
two’s complement numbers. See
Two’s complement
numbers
unsigned,
O
OAI gate. See Or-and-invert gate
Object files, 331–334
Octal numbers, 180
OFF, 23. See also 0, LOW
Offset, 295–296
ON, 23. See also 1, HIGH, Asserted
One-cold, 124
One-hot, 82
Opcode, 299
Operands, 292–298
Operators
bitwise, 171–174
or immediate (ori), 308
OR gate, 21
Or-and-invert (OAI) gate, 43
precedence table of, 179
reduction, 174
Out-of-order execution, 443
Out-of-order processor, 441–443
Output devices, 466
Output terminals, 51
Overflow, 15
detecting, 15
with addition, 15
P
Page, 485
size, 485
Page fault, 485
Page offset, 486–488
Page table, 486
Parity, 22
Parallelism, 149–153
pipelining. See Pipelining, Pipelined
MIPS processor
SIMD. See Single instruction
multiple data unit
spatial and temporal, 151
vector processor. See Vector processor
Patterson, David, 290, 364
PCBs. See Printed circuit boards
PC-relative addressing, 327–328.
See also Addressing modes
PCSrc, 281, 387–390
PCWrite, 385
Pentium processors, 449–452. See also
Intel, IA-32
Pentium 4, 452
Pentium II, 450, 499
Pentium III, 450, 451, 499
Pentium M, 453
Pentium Pro, 450, 499, 501
Perfect induction, 60
Performance, 366–368
Peripheral devices. See I/O devices
Perl programming language, 20
Physical memory, 466, 485–486
Physical pages, 486–494
Physical page number, 486
Pipelined MIPS processor, 151, 366,
401–421. See also MIPS,
Architecture, Microarchitecture
control, 405–406
datapath, 404
forwarding, 408–410
hazards. See Hazards
performance analysis, 418–421
processor performance
comparison, 420
stalls, 410–413. See also Hazards
timing, 402
Index 565

Pipelining hazards. See Hazards
PLAs. See Programmable logic arrays
Plastic leaded chip carriers (PLCCs),
531–532
PLCCs. See Plastic leaded chip carriers
PLDs. See Programmable logic devices
pMOS, 29–30
pMOS transistors, 28–31
Pointer, 321
global, 331
stack, 321
Pop, 345–346. See also Stack
Ports, 259
POS. See Product of sums form
Power consumption, 34–35
Prediction. See Branch prediction
Prefix adder, 237–239
Preserved registers, 322–324
Prime implicants, 61, 74
Printed circuit boards (PCBs),
533–534
Procedure calls, 319–327
arguments and variables, 326–327
nested, 324–326
preserved versus nonpreserved
registers, 323–324
returns, 319–320
return values, 320
stack frame, 322
stack usage, 321–322
Processors. See Microprocessors
Product-of-sums (POS) canonical
form, 56
Program counter (PC), 365
Programmable logic arrays (PLAs), 63,
266–268, 520–521
Programmable logic devices
(PLDs), 268
Programmable read only memories
(PROMs), 516–520. See also
Read only memories
Programming, 304–327
arithmetic/logical instructions. See
Arithmetic, 304–308
arrays. See Arrays
branching. See Branching
conditional statements. See
Conditional statements
constants, 307–308
immediates, 298
loops. See Loops
procedure calls. See Procedure calls
shift instructions, 306–307
translating and starting a program,
331–335
Programming languages, 290–294
Propagation delay, 84
Protection, memory. See Memory
protection
Proving Boolean theorems. See Perfect
induction
PROMs. See Programmable read only
memories
Pseudo-direct addressing, 328–329.
See also Addressing modes
Pseudo-nMOS logic, 33–34. See also
Transistors
Pseudoinstructions, 336–337
Push, 67–69. See also Stack
Q
Queue, 321
FIFO. See First-in-first-out queue
LIFO. See Last-in-first-out queue
Q output. See Sequential logic design
R
R-type instructions, 299–300
RAM. See Random access memory
Random access memory (RAM), 257,
262–264. See also Memory
arrays
synthesized, 265
Read only memory, 199, 257, 262–264.
See also Memory arrays
EPROM. See Erasable programmable read only memory
EEPROM. See Electrically erasable
programmable read only
memory
flash memory. See Flash memory
PROM. See Programmable read
only memory
transistor-level implementation, 273
Read/write head, 484
Recursive procedure calls, 324–325.
See also Procedure calls
Reduced instruction set computer
(RISC), 292, 364
Reduction operators, 174. See also
Hardware description languages,
Verilog
RegDst, 373–374
Register-only addressing, 327. See also
Addressing modes
Register renaming, 443–445. See also
Advanced microarchitecture
Register(s), 190–191, 261–262,
292–294. See also Flip-flops,
Register file
arguments ($a0-$a3)
assembler temporary ($at)
enabled. See Enabled registers
file, 261
global pointer ($gp), 331
multiple, 194–195
program counter (PC), 365
preserved and nonpreserved,
322–324
renaming, 443–445
resettable. See Resettable registers
asynchronous, 192
synchronous, 192
return address ($ra), 319–320
Register set, 294. See also Register file,
MIPS registers, IA-32 registers
Regularity, 6, 188
RegWrite, 371
Replacement policies, 492. See also
Caches, Virtual memory
Resettable registers,191–193
asynchronous. See Asynchronous
resettable registers
synchronous. See Synchronous
resettable registers
Return address ($ra), 319–320
Ripple-carry adder, 234
RISC. See Reduced instruction set
computer
ROM, See Read Only Memory
Rotators, 244–246
Rounding, 252
S
Scalar processor, 438
Scan chains, 255. See also Shift registers
Schematic, 62–65
Scientific notation, 249–250
566 Index

Seek time, 484
Segments, memory, 330–331
Semiconductor industry, sales, 3
Semiconductors, 27. See also
Transistors
CMOS. See Complementary metal
oxide silicon
diodes. See Diodes
transistors. See Transistors
MOSFET. See Metal oxide silicon
field effect transistors
nMOS. See nMOS transistors
pMOS. See pMOS transistors
pseudo nMOS. See Pseudo nMOS
Sensitivity list, 190, 191, 196
Sequential building blocks.
See Sequential logic
Sequential logic, 103–165, 190–195,
254–257
enabled registers, 193
latches, 103–112, 195
multiple registers, 194–195
overview, 103
registers, 190–191
shift registers, 255–257
synchronous, 113–117
timing of, 133–149. See also Timing
set if less than (slt), 313
set if less than immediate (slti), 339
set if less than immediate unsigned
(sltiu), 339
set if less than unsigned (sltu), 339
Setup time, 133, 135–136
Seven-segment display decoder, 75–77
with “don’t cares,” 78
Shared memory, 71
Shift amount (shamt), 245
shift left logical (sll), 306
shift left logical variable (sllv), 306
shift right arithmetic (sra), 306
shift right arithmetic variable (srav), 306
shift right logical (srl), 306
shift right logical variable (srlv), 306
Shifters, 244–246
arithmetic, 244
logical, 244
Shift instructions, 306–307
Shift registers, 255–257
Short path, 86
Sign/magnitude numbers, 15–16
Sign bit, 16
Sign extension, 18
Significand. See Mantissa
Silicon (Si), 27
Silicon dioxide (SO2), 28
Silicon Valley, 26
Simplicity, 291–292
SIMD. See Single instruction multiple
data units
Single-cycle MIPS processor, 366,
368–381. See also MIPS microprocessor, MIPS architecture,
MIPS microarchitecture
control, 374–377
ALU decoder truth table. See
ALU decoder
Main decoder truth table. See
Main decoder
datapath, 368–374
HDL representation, 422
operation, 376–377 performance
analysis, 380–381
timing, 402
Single instruction multiple data (SIMD)
units, 438, 445
Slash notation, 53
SPARC architecture, 364
SRAM. See Static random access
memory
SP0256, 496
Spatial locality, 464
Speech synthesis, 496–498
device driver, 498
SP0256, 496
Stack, 321–322. See also Memory map,
Procedure calls, Queue
dynamic data segment, 331
frame, 322
LIFO. See Last-in-first-out queue
pointer, 321
Stalls, 410–413. See also Hazards
Static discipline, 24–26
Static random access memory (SRAM),
257, 260
Status flags, 344
Stored program concept, 303–304
Stores
store byte (sb), 318
store half (sh), 553
store word (sw), 296
Strings, 318
Structural modeling, 185–189
subtract (sub), 291
subtract unsigned (subu), 339
Subtraction, 17–18, 240, 291
Subtractor, 240
Sum-of-products (SOP) canonical form,
54–55
Sun Microsystems, 364
Superscalar processor, 438–440
Supply voltage, 22
Swap space, 492
Swift, Jonathan, 297
Switch/case statements, 311
Symbol table, 333
Synchronizers, 144–146
asynchronous inputs, 146
MTBF. See Mean time before
failure
probability of failure. See
Probability of failure
Synchronous resettable registers, 192
HDL for, 192
Synchronous sequential logic,
113–117
problematic circuits, 113–114
Synthesis, 78–79, 186, 187, 188, 189,
190, 193, 194, 195, 199, 200
Synthesis Tools, 195
T
Taking the two’s complement, 16
Temporal locality, 464
Testbenches, 171, 214–218, 428–431
self-checking, 215
with test vector file, 216
Text segment, 330
Theorems. See Boolean Theorems
Thin small outline package (TSOP), 531
Threshold voltage, 29
Throughput, 149
Timing
analysis, 137–138
delay, 86–87
glitches, 88–91
of sequential logic, 133–149
clock skew, 140–143
dynamic discipline, 134
hold time. See Hold time
hold time constraint, 136–137.
See Hold time constraint,
Hold time violation
hold time violation. See Hold
time violation, Hold time
constraint, 139
Index 567

Timing (Continued)
metastability, 143–144
setup time. See Setup time
setup time constraint,
135–136
setup time violations
resolution time, 146–149. See
also Metastability
synchronizers, 144–146
system timing, 135–140
specification, 51
TLB. See Translation lookaside
buffer
Token, 149
Transistors, 23, 28–31, 34
CMOS. See Complement metal
oxide silicon
nMOS. See nMOS
pMOS. See pMOS
Transistor-Transistor Logic (TTL), 25
Translation lookaside buffer (TLB),
490
Translating and starting a program,
331–336
Transmission gates, 33. See also
Transistors
Transmission lines, 534–546
reflection coefficient, 544–545
Z0, 543–544
matched termination,
536–538
mismatched termination,
539–541
open termination, 538–539
proper terminations, 542
short termination, 539
when to use, 41–542
Tristate buffer, 70–71
Truth tables, 55, 56, 60, 61, 177
ALU decoder, 376
“don’t care,” 77
main decoder, 376, 379
multiplexer, 79
seven-segment display decoder, 76
SR latch, 106
with undefined and floating inputs,
181
TSOP. See Thin small outline
package
TTL. See Transistor-Transistor Logic
Two-level logic, 65–66
Two’s complement numbers, 16–18.
See also Binary numbers
U
Unconditional branches, 308. See also
Jumps
Unicode, 316. See also ASCII
Unit under test (UUT), 201
Unity gain points, 24
Unsigned numbers, 18
Use bit, 478–479
UUT. See Unit under test
V
Valid bit, 472. See also Caches, Virtual
memory
Vanity Fair (Carroll), 65
VCC, 23
VDD, 23
Vector processors, 438. See also
Advanced microarchitecture
Verilog, 167, 169, 172, 173, 174, 175,
176, 178, 180, 181, 201, 203,
205
3:8 decoder, 199
accessing parts of busses, 189
adder, 426
ALU decoder, 424
AND, 168
architecture body, 168
assign statement, 168, 197, 178
asynchronous reset, 192
bad synchronizer with blocking
assignments, 206
bit swizzling, 182
blocking assignment, 202
case sensitivity, 174
casez, 201
combinational logic, 168, 202
comments, 174
comparators, 242
continuous assignment statement,
173
controller, 423
counter, 254
datapath, 425
default, 198
divide-by-3 finite state machine,
207, 208
D latch, 195
eight-input AND, 174
entity declaration, 168
full adder, 178
using always/process, 197
using nonblocking assignments,
204
IEEE_STD_LOGIC_1164, 168, 183
IEEE_STD_LOGIC_SIGNED, 183
IEEE_STD_LOGIC_UNSIGNED,
183
inverters, 172
using always/process, 196
left shift, 427
library use clause, 168
logic gates, 173
with delays, 183
main decoder, 424
MIPS testbench, 429
MIPS top-level module, 430
multiplexers, 175, 176, 428
multiplier, 247
nonblocking assignment, 191,
202
NOT, 168
numbers, 180
operator precedence, 179
OR, 168
parameterized
N:2N decoder, 212
N-bit multiplexer, 211
N-input AND gate, 213
pattern recognizer
Mealy FSM, 210
Moore FSM, 209
priority circuit, 201
RAM, 265
reg, 191, 194
register, 191
register file, 42
resettable flip-flop, 427
resettable enabled register, 193
resettable register, 192
ROM, 266
self-checking testbench, 215
seven-segment display decoder,
198
shift register, 256
sign extension, 427
single-cycle MIPS processor,
422
STD_LOGIC, 168, 183
statement, 173
structural models
568 Index

2:1 multiplexer, 188
4:1 multiplexer, 187
subtractor, 241
synchronizer, 194
synchronous reset, 192
testbench, 214
with test vector file, 216
tristate buffer, 180
truth tables with undefined and
floating inputs, 181
type declaration, 168
wires, 178
VHDL libraries and types, 167, 169,
172, 173, 174, 175, 176, 178,
180, 181, 183–185, 203, 205
3:8 decoder, 199
accessing parts of busses, 189
adder, 426
architecture, 187
asynchronous reset, 192
bad synchronizer with blocking
assignments, 206
bit swizzling, 182
boolean, 183
case sensitivity, 174
clk’event, 191
combinational logic, 168
comments, 174
comparators, 242
concurrent signal assignment, 173,
178
controller, 423
CONV_STD_LOGIC_VECTOR,
212
counter, 254
datapath, 425
decoder, 424
divide-by-3 finite state machine,
207, 208
D latch, 195
eight-input AND, 174
expression, 173
full adder, 178
using always/process, 197
using nonblocking assignments,
204
generic statement, 211
inverters, 172
using always/process, 196
left shift, 427
logic gates, 173
with delays, 183
main decoder, 424
MIPS testbench, 429
MIPS top-level module, 430
multiplexers, 175, 176, 428
multiplier, 247
numbers, 180
operand, 173
operator precedence, 179
others, 198
parameterized
N-bit multiplexer, 211
N-input AND gate, 213
N:2N decoder, 212
pattern recognizer
Mealy FSM, 210
Moore FSM, 209
priority circuit, 201
process, 191
RAM, 265
register, 191
register file, 426
resettable enabled register, 193
resettable flip-flop, 427
resettable register, 192
RISING_EDGE, 191
ROM, 266
selected signal assignment statements, 176
self-checking testbench, 215
seven-segment display decoder, 198
shift register, 256
sign extension, 427
signals, 178, 194
simulation waveforms with delays,
170, 183
single-cycle MIPS processor, 422
STD_LOGIC_ARITH, 212
structural models
2:1 multiplexer, 188
4:1 multiplexer, 187
subtractor, 241
synchronizer, 194
synchronous reset, 192
testbench, 214
with test vector file, 216
tristate buffer, 180
truth tables with undefined and
floating inputs, 181
VHSIC, 169
Virtual address, 485–490
Virtual memory, 484–494
address translation, 486–488
IA-32, 501
memory protection, 491
pages, 485
page faults, 485
page offset, 486–488
page table, 488–489
multilevel page tables, 492–494
replacement policies, 492
translation lookaside buffer (TLB),
490. See Translation lookaside buffer
write policies, 482–483
Virtual pages, 485
Virtual page number, 487
Volatile memory, 259. See also DRAM,
SRAM, Flip-flops
Voltage, threshold, 29
VSS, 23
W
Wafers, 28
Wall, Larry, 20
WAR. See Write after read
WAW. See Write after write
While loop, 312–313
White space, 174
Whitmore, Georgiana, 7
Wire, 63
Word-addressable memory, 295
Write policies, 482–483
Write after read (WAR) hazard, 442.
See also Hazards
Write after write (WAW) hazard,
442–443. See also Hazards
X
XOR gate, 21
XNOR gate, 21
Xilinx FPGA, 268
X. See Contention, Don’t care.,
Z
Z,. See Floating
Zero extension, 302
Index 569
1
2
Preface.................................................................................................................................... 6
Preface to the first edition........................................................................................................8
Chapter 1 - A Tutorial Introduction.........................................................................................9
1.1 Getting Started..............................................................................................................9
1.2 Variables and Arithmetic Expressions..........................................................................11
1.3 The for statement.........................................................................................................15
1.4 Symbolic Constants......................................................................................................17
1.5 Character Input and Output.........................................................................................17
1.5.1 File Copying..........................................................................................................18
1.5.2 Character Counting...............................................................................................19
1.5.3 Line Counting.......................................................................................................20
1.5.4 Word Counting.....................................................................................................21
1.6 Arrays..........................................................................................................................23
1.7 Functions.....................................................................................................................25
1.8 Arguments - Call by Value...........................................................................................28
1.9 Character Arrays..........................................................................................................29
1.10 External Variables and Scope.....................................................................................31
Chapter 2 - Types, Operators and Expressions.......................................................................35
2.1 Variable Names............................................................................................................35
2.2 Data Types and Sizes...................................................................................................35
2.3 Constants.....................................................................................................................36
2.4 Declarations.................................................................................................................38
2.5 Arithmetic Operators...................................................................................................39
2.6 Relational and Logical Operators.................................................................................39
2.7 Type Conversions........................................................................................................40
2.8 Increment and Decrement Operators............................................................................43
2.9 Bitwise Operators........................................................................................................45
2.10 Assignment Operators and Expressions......................................................................46
2.11 Conditional Expressions.............................................................................................47
2.12 Precedence and Order of Evaluation..........................................................................48
Chapter 3 - Control Flow.......................................................................................................50
3.1 Statements and Blocks.................................................................................................50
3.2 If-Else..........................................................................................................................50
3.3 Else-If..........................................................................................................................51
3.4 Switch..........................................................................................................................52
3.5 Loops - While and For.................................................................................................53
3.6 Loops - Do-While........................................................................................................56
3.7 Break and Continue.....................................................................................................57
3.8 Goto and labels............................................................................................................57
Chapter 4 - Functions and Program Structure........................................................................59
4.1 Basics of Functions......................................................................................................59
4.2 Functions Returning Non-integers................................................................................61
4.3 External Variables........................................................................................................63
4.4 Scope Rules.................................................................................................................68
4.5 Header Files.................................................................................................................69
4.6 Static Variables............................................................................................................70
4.7 Register Variables........................................................................................................71
4.8 Block Structure............................................................................................................71
4.9 Initialization.................................................................................................................72
4.10 Recursion...................................................................................................................73
4.11 The C Preprocessor....................................................................................................74
4.11.1 File Inclusion.......................................................................................................75
4.11.2 Macro Substitution..............................................................................................75
3
4.11.3 Conditional Inclusion..........................................................................................77
Chapter 5 - Pointers and Arrays.............................................................................................78
5.1 Pointers and Addresses................................................................................................78
5.2 Pointers and Function Arguments.................................................................................79
5.3 Pointers and Arrays......................................................................................................81
5.4 Address Arithmetic......................................................................................................84
5.5 Character Pointers and Functions.................................................................................87
5.6 Pointer Arrays; Pointers to Pointers.............................................................................89
5.7 Multi-dimensional Arrays.............................................................................................92
5.8 Initialization of Pointer Arrays.....................................................................................93
5.9 Pointers vs. Multi-dimensional Arrays..........................................................................94
5.10 Command-line Arguments..........................................................................................95
5.11 Pointers to Functions.................................................................................................98
5.12 Complicated Declarations.........................................................................................100
Chapter 6 - Structures..........................................................................................................105
6.1 Basics of Structures...................................................................................................105
6.2 Structures and Functions............................................................................................107
6.3 Arrays of Structures...................................................................................................109
6.4 Pointers to Structures.................................................................................................112
6.5 Self-referential Structures...........................................................................................113
6.6 Table Lookup............................................................................................................117
6.7 Typedef......................................................................................................................119
6.8 Unions.......................................................................................................................120
6.9 Bit-fields....................................................................................................................121
Chapter 7 - Input and Output...............................................................................................124
7.1 Standard Input and Output.........................................................................................124
7.2 Formatted Output - printf...........................................................................................125
7.3 Variable-length Argument Lists..................................................................................127
7.4 Formatted Input - Scanf.............................................................................................128
7.5 File Access.................................................................................................................130
7.6 Error Handling - Stderr and Exit................................................................................132
7.7 Line Input and Output................................................................................................134
7.8 Miscellaneous Functions............................................................................................135
7.8.1 String Operations................................................................................................135
7.8.2 Character Class Testing and Conversion..............................................................135
7.8.3 Ungetc................................................................................................................135
7.8.4 Command Execution...........................................................................................135
7.8.5 Storage Management..........................................................................................136
7.8.6 Mathematical Functions.......................................................................................136
7.8.7 Random Number generation................................................................................136
Chapter 8 - The UNIX System Interface..............................................................................138
8.1 File Descriptors..........................................................................................................138
8.2 Low Level I/O - Read and Write................................................................................139
8.3 Open, Creat, Close, Unlink........................................................................................140
8.4 Random Access - Lseek.............................................................................................142
8.5 Example - An implementation of Fopen and Getc.......................................................142
8.6 Example - Listing Directories.....................................................................................145
8.7 Example - A Storage Allocator..................................................................................149
Appendix A - Reference Manual..........................................................................................154
A.1 Introduction..............................................................................................................154
A.2 Lexical Conventions..................................................................................................154
A.2.1 Tokens...............................................................................................................154
A.2.2 Comments..........................................................................................................154
4
A.2.3 Identifiers...........................................................................................................154
A.2.4 Keywords...........................................................................................................154
A.2.5 Constants...........................................................................................................155
A.2.6 String Literals.....................................................................................................156
A.3 Syntax Notation........................................................................................................156
A.4 Meaning of Identifiers...............................................................................................157
A.4.1 Storage Class.....................................................................................................157
A.4.2 Basic Types........................................................................................................157
A.4.3 Derived types.....................................................................................................158
A.4.4 Type Qualifiers...................................................................................................158
A.5 Objects and Lvalues..................................................................................................158
A.6 Conversions..............................................................................................................159
A.6.1 Integral Promotion.............................................................................................159
A.6.2 Integral Conversions...........................................................................................159
A.6.3 Integer and Floating...........................................................................................159
A.6.4 Floating Types....................................................................................................159
A.6.5 Arithmetic Conversions......................................................................................159
A.6.6 Pointers and Integers..........................................................................................160
A.6.7 Void...................................................................................................................160
A.6.8 Pointers to Void.................................................................................................161
A.7 Expressions...............................................................................................................161
A.7.1 Pointer Conversion.............................................................................................161
A.7.2 Primary Expressions...........................................................................................161
A.7.3 Postfix Expressions............................................................................................162
A.7.4 Unary Operators.................................................................................................164
A.7.5 Casts..................................................................................................................165
A.7.6 Multiplicative Operators.....................................................................................165
A.7.7 Additive Operators.............................................................................................166
A.7.8 Shift Operators...................................................................................................166
A.7.9 Relational Operators...........................................................................................167
A.7.10 Equality Operators...........................................................................................167
A.7.11 Bitwise AND Operator.....................................................................................167
A.7.12 Bitwise Exclusive OR Operator........................................................................167
A.7.13 Bitwise Inclusive OR Operator.........................................................................168
A.7.14 Logical AND Operator.....................................................................................168
A.7.15 Logical OR Operator........................................................................................168
A.7.16 Conditional Operator........................................................................................168
A.7.17 Assignment Expressions...................................................................................169
A.7.18 Comma Operator..............................................................................................169
A.7.19 Constant Expressions.......................................................................................169
A.8 Declarations..............................................................................................................170
A.8.1 Storage Class Specifiers.....................................................................................170
A.8.2 Type Specifiers...................................................................................................171
A.8.3 Structure and Union Declarations.......................................................................172
A.8.4 Enumerations.....................................................................................................174
A.8.5 Declarators.........................................................................................................175
A.8.6 Meaning of Declarators......................................................................................176
A.8.7 Initialization.......................................................................................................178
A.8.8 Type names........................................................................................................180
A.8.9 Typedef..............................................................................................................181
A.8.10 Type Equivalence.............................................................................................181
A.9 Statements................................................................................................................181
A.9.1 Labeled Statements.............................................................................................182
5
A.9.2 Expression Statement.........................................................................................182
A.9.3 Compound Statement.........................................................................................182
A.9.4 Selection Statements..........................................................................................183
A.9.5 Iteration Statements...........................................................................................183
A.9.6 Jump statements.................................................................................................184
A.10 External Declarations..............................................................................................184
A.10.1 Function Definitions.........................................................................................185
A.10.2 External Declarations.......................................................................................186
A.11 Scope and Linkage..................................................................................................186
A.11.1 Lexical Scope...................................................................................................187
A.11.2 Linkage............................................................................................................187
A.12 Preprocessing..........................................................................................................187
A.12.1 Trigraph Sequences..........................................................................................188
A.12.2 Line Splicing....................................................................................................188
A.12.3 Macro Definition and Expansion.......................................................................188
A.12.4 File Inclusion....................................................................................................190
A.12.5 Conditional Compilation...................................................................................191
A.12.6 Line Control.....................................................................................................192
A.12.7 Error Generation..............................................................................................192
A.12.8 Pragmas............................................................................................................192
A.12.9 Null directive....................................................................................................192
A.12.10 Predefined names............................................................................................192
A.13 Grammar.................................................................................................................193
Appendix B - Standard Library............................................................................................199
B.1 Input and Output: <stdio.h>......................................................................................199
B.1.1 File Operations...................................................................................................199
B.1.2 Formatted Output...............................................................................................200
B.1.3 Formatted Input..................................................................................................202
B.1.4 Character Input and Output Functions................................................................203
B.1.5 Direct Input and Output Functions......................................................................204
B.1.6 File Positioning Functions...................................................................................204
B.1.7 Error Functions..................................................................................................205
B.2 Character Class Tests: <ctype.h>...............................................................................205
B.3 String Functions: <string.h>......................................................................................205
B.4 Mathematical Functions: <math.h>............................................................................206
B.5 Utility Functions: <stdlib.h>......................................................................................207
B.6 Diagnostics: <assert.h>..............................................................................................209
B.7 Variable Argument Lists: <stdarg.h>.........................................................................209
B.8 Non-local Jumps: <setjmp.h>....................................................................................210
B.9 Signals: <signal.h>....................................................................................................210
B.10 Date and Time Functions: <time.h>.........................................................................210
B.11 Implementation-defined Limits: <limits.h> and <float.h>.........................................212
Appendix C - Summary of Changes.....................................................................................214
6
Preface
The computing world has undergone a revolution since the publication of The C Programming
Language in 1978. Big computers are much bigger, and personal computers have capabilities
that rival mainframes of a decade ago. During this time, C has changed too, although only
modestly, and it has spread far beyond its origins as the language of the UNIX operating
system.
The growing popularity of C, the changes in the language over the years, and the creation of
compilers by groups not involved in its design, combined to demonstrate a need for a more
precise and more contemporary definition of the language than the first edition of this book
provided. In 1983, the American National Standards Institute (ANSI) established a committee
whose goal was to produce ``an unambiguous and machine-independent definition of the
language C'', while still retaining its spirit. The result is the ANSI standard for C.
The standard formalizes constructions that were hinted but not described in the first edition,
particularly structure assignment and enumerations. It provides a new form of function
declaration that permits cross-checking of definition with use. It specifies a standard library,
with an extensive set of functions for performing input and output, memory management,
string manipulation, and similar tasks. It makes precise the behavior of features that were not
spelled out in the original definition, and at the same time states explicitly which aspects of the
language remain machine-dependent.
This Second Edition of The C Programming Language describes C as defined by the ANSI
standard. Although we have noted the places where the language has evolved, we have chosen
to write exclusively in the new form. For the most part, this makes no significant difference;
the most visible change is the new form of function declaration and definition. Modern
compilers already support most features of the standard.
We have tried to retain the brevity of the first edition. C is not a big language, and it is not well
served by a big book. We have improved the exposition of critical features, such as pointers,
that are central to C programming. We have refined the original examples, and have added new
examples in several chapters. For instance, the treatment of complicated declarations is
augmented by programs that convert declarations into words and vice versa. As before, all
examples have been tested directly from the text, which is in machine-readable form.
Appendix A, the reference manual, is not the standard, but our attempt to convey the essentials
of the standard in a smaller space. It is meant for easy comprehension by programmers, but not
as a definition for compiler writers -- that role properly belongs to the standard itself.
Appendix B is a summary of the facilities of the standard library. It too is meant for reference
by programmers, not implementers. Appendix C is a concise summary of the changes from the
original version.
As we said in the preface to the first edition, C ``wears well as one's experience with it grows''.
With a decade more experience, we still feel that way. We hope that this book will help you
learn C and use it well.
We are deeply indebted to friends who helped us to produce this second edition. Jon Bently,
Doug Gwyn, Doug McIlroy, Peter Nelson, and Rob Pike gave us perceptive comments on
almost every page of draft manuscripts. We are grateful for careful reading by Al Aho, Dennis
Allison, Joe Campbell, G.R. Emlin, Karen Fortgang, Allen Holub, Andrew Hume, Dave
Kristol, John Linderman, Dave Prosser, Gene Spafford, and Chris van Wyk. We also received
helpful suggestions from Bill Cheswick, Mark Kernighan, Andy Koenig, Robin Lake, Tom
7
London, Jim Reeds, Clovis Tondo, and Peter Weinberger. Dave Prosser answered many
detailed questions about the ANSI standard. We used Bjarne Stroustrup's C++ translator
extensively for local testing of our programs, and Dave Kristol provided us with an ANSI C
compiler for final testing. Rich Drechsler helped greatly with typesetting.
Our sincere thanks to all.
Brian W. Kernighan
Dennis M. Ritchie 
8
Preface to the first edition
C is a general-purpose programming language with features economy of expression, modern
flow control and data structures, and a rich set of operators. C is not a ``very high level''
language, nor a ``big'' one, and is not specialized to any particular area of application. But its
absence of restrictions and its generality make it more convenient and effective for many tasks
than supposedly more powerful languages.
C was originally designed for and implemented on the UNIX operating system on the DEC
PDP-11, by Dennis Ritchie. The operating system, the C compiler, and essentially all UNIX
applications programs (including all of the software used to prepare this book) are written in
C. Production compilers also exist for several other machines, including the IBM System/370,
the Honeywell 6000, and the Interdata 8/32. C is not tied to any particular hardware or system,
however, and it is easy to write programs that will run without change on any machine that
supports C.
This book is meant to help the reader learn how to program in C. It contains a tutorial
introduction to get new users started as soon as possible, separate chapters on each major
feature, and a reference manual. Most of the treatment is based on reading, writing and
revising examples, rather than on mere statements of rules. For the most part, the examples are
complete, real programs rather than isolated fragments. All examples have been tested directly
from the text, which is in machine-readable form. Besides showing how to make effective use
of the language, we have also tried where possible to illustrate useful algorithms and principles
of good style and sound design.
The book is not an introductory programming manual; it assumes some familiarity with basic
programming concepts like variables, assignment statements, loops, and functions.
Nonetheless, a novice programmer should be able to read along and pick up the language,
although access to more knowledgeable colleague will help.
In our experience, C has proven to be a pleasant, expressive and versatile language for a wide
variety of programs. It is easy to learn, and it wears well as on's experience with it grows. We
hope that this book will help you to use it well.
The thoughtful criticisms and suggestions of many friends and colleagues have added greatly to
this book and to our pleasure in writing it. In particular, Mike Bianchi, Jim Blue, Stu Feldman,
Doug McIlroy Bill Roome, Bob Rosin and Larry Rosler all read multiple volumes with care.
We are also indebted to Al Aho, Steve Bourne, Dan Dvorak, Chuck Haley, Debbie Haley,
Marion Harris, Rick Holt, Steve Johnson, John Mashey, Bob Mitze, Ralph Muha, Peter
Nelson, Elliot Pinson, Bill Plauger, Jerry Spivack, Ken Thompson, and Peter Weinberger for
helpful comments at various stages, and to Mile Lesk and Joe Ossanna for invaluable
assistance with typesetting.
Brian W. Kernighan
Dennis M. Ritchie 
9
Chapter 1 - A Tutorial Introduction
Let us begin with a quick introduction in C. Our aim is to show the essential elements of the
language in real programs, but without getting bogged down in details, rules, and exceptions.
At this point, we are not trying to be complete or even precise (save that the examples are
meant to be correct). We want to get you as quickly as possible to the point where you can
write useful programs, and to do that we have to concentrate on the basics: variables and
constants, arithmetic, control flow, functions, and the rudiments of input and output. We are
intentionally leaving out of this chapter features of C that are important for writing bigger
programs. These include pointers, structures, most of C's rich set of operators, several controlflow statements, and the standard library.
This approach and its drawbacks. Most notable is that the complete story on any particular
feature is not found here, and the tutorial, by being brief, may also be misleading. And because
the examples do not use the full power of C, they are not as concise and elegant as they might
be. We have tried to minimize these effects, but be warned. Another drawback is that later
chapters will necessarily repeat some of this chapter. We hope that the repetition will help you
more than it annoys.
In any case, experienced programmers should be able to extrapolate from the material in this
chapter to their own programming needs. Beginners should supplement it by writing small,
similar programs of their own. Both groups can use it as a framework on which to hang the
more detailed descriptions that begin in Chapter 2.
1.1 Getting Started
The only way to learn a new programming language is by writing programs in it. The first
program to write is the same for all languages:
Print the words
hello, world
This is a big hurdle; to leap over it you have to be able to create the program text somewhere,
compile it successfully, load it, run it, and find out where your output went. With these
mechanical details mastered, everything else is comparatively easy.
In C, the program to print ``hello, world'' is
 #include <stdio.h>
 main()
 {
 printf("hello, world\n");
 }
Just how to run this program depends on the system you are using. As a specific example, on
the UNIX operating system you must create the program in a file whose name ends in ``.c'',
such as hello.c, then compile it with the command
 cc hello.c
If you haven't botched anything, such as omitting a character or misspelling something, the
compilation will proceed silently, and make an executable file called a.out. If you run a.out
by typing the command
 a.out
it will print 
10
 hello, world
On other systems, the rules will be different; check with a local expert.
Now, for some explanations about the program itself. A C program, whatever its size, consists
of functions and variables. A function contains statements that specify the computing
operations to be done, and variables store values used during the computation. C functions are
like the subroutines and functions in Fortran or the procedures and functions of Pascal. Our
example is a function named main. Normally you are at liberty to give functions whatever
names you like, but ``main'' is special - your program begins executing at the beginning of
main. This means that every program must have a main somewhere.
main will usually call other functions to help perform its job, some that you wrote, and others
from libraries that are provided for you. The first line of the program,
 #include <stdio.h>
tells the compiler to include information about the standard input/output library; the line
appears at the beginning of many C source files. The standard library is described in Chapter 7
and Appendix B.
One method of communicating data between functions is for the calling function to provide a
list of values, called arguments, to the function it calls. The parentheses after the function name
surround the argument list. In this example, main is defined to be a function that expects no
arguments, which is indicated by the empty list ( ).
#include <stdio.h> include information about standard
library
main() define a function called main
 that received no argument values
{ statements of main are enclosed in braces
 printf("hello, world\n"); main calls library function printf
 to print this sequence of characters
} \n represents the newline character
The first C program
The statements of a function are enclosed in braces { }. The function main contains only one
statement,
 printf("hello, world\n");
A function is called by naming it, followed by a parenthesized list of arguments, so this calls
the function printf with the argument "hello, world\n". printf is a library function that
prints output, in this case the string of characters between the quotes.
A sequence of characters in double quotes, like "hello, world\n", is called a character
string or string constant. For the moment our only use of character strings will be as
arguments for printf and other functions.
The sequence \n in the string is C notation for the newline character, which when printed
advances the output to the left margin on the next line. If you leave out the \n (a worthwhile
experiment), you will find that there is no line advance after the output is printed. You must
use \n to include a newline character in the printf argument; if you try something like
 printf("hello, world
 ");
11
the C compiler will produce an error message.
printf never supplies a newline character automatically, so several calls may be used to build
up an output line in stages. Our first program could just as well have been written
 #include <stdio.h>
 main()
 {
 printf("hello, ");
 printf("world");
 printf("\n");
 }
to produce identical output.
Notice that \n represents only a single character. An escape sequence like \n provides a
general and extensible mechanism for representing hard-to-type or invisible characters. Among
the others that C provides are \t for tab, \b for backspace, \" for the double quote and \\ for
the backslash itself. There is a complete list in Section 2.3.
Exercise 1-1. Run the ``hello, world'' program on your system. Experiment with leaving out
parts of the program, to see what error messages you get.
Exercise 1-2. Experiment to find out what happens when prints's argument string contains
\c, where c is some character not listed above.
1.2 Variables and Arithmetic Expressions
The next program uses the formula oC=(5/9)(oF-32) to print the following table of Fahrenheit
temperatures and their centigrade or Celsius equivalents: 
12
 1 -17
 20 -6
 40 4
 60 15
 80 26
 100 37
 120 48
 140 60
 160 71
 180 82
 200 93
 220 104
 240 115
 260 126
 280 137
 300 148
The program itself still consists of the definition of a single function named main. It is longer
than the one that printed ``hello, world'', but not complicated. It introduces several new
ideas, including comments, declarations, variables, arithmetic expressions, loops , and
formatted output.
 #include <stdio.h>
 /* print Fahrenheit-Celsius table
 for fahr = 0, 20, ..., 300 */
 main()
 {
 int fahr, celsius;
 int lower, upper, step;
 lower = 0; /* lower limit of temperature scale */
 upper = 300; /* upper limit */
 step = 20; /* step size */
 fahr = lower;
 while (fahr <= upper) {
 celsius = 5 * (fahr-32) / 9;
 printf("%d\t%d\n", fahr, celsius);
 fahr = fahr + step;
 }
 }
The two lines
 /* print Fahrenheit-Celsius table
 for fahr = 0, 20, ..., 300 */
are a comment, which in this case explains briefly what the program does. Any characters
between /* and */ are ignored by the compiler; they may be used freely to make a program
easier to understand. Comments may appear anywhere where a blank, tab or newline can.
In C, all variables must be declared before they are used, usually at the beginning of the
function before any executable statements. A declaration announces the properties of
variables; it consists of a name and a list of variables, such as
 int fahr, celsius;
 int lower, upper, step;
The type int means that the variables listed are integers; by contrast with float, which means
floating point, i.e., numbers that may have a fractional part. The range of both int and float
depends on the machine you are using; 16-bits ints, which lie between -32768 and +32767,
are common, as are 32-bit ints. A float number is typically a 32-bit quantity, with at least six
significant digits and magnitude generally between about 10-38 and 1038
.
C provides several other data types besides int and float, including: 
13
char character - a single byte
short short integer
long long integer
double double-precision floating point
The size of these objects is also machine-dependent. There are also arrays, structures and
unions of these basic types, pointers to them, and functions that return them, all of which we
will meet in due course.
Computation in the temperature conversion program begins with the assignment statements
 lower = 0;
 upper = 300;
 step = 20;
which set the variables to their initial values. Individual statements are terminated by
semicolons.
Each line of the table is computed the same way, so we use a loop that repeats once per output
line; this is the purpose of the while loop
 while (fahr <= upper) {
 ...
 }
The while loop operates as follows: The condition in parentheses is tested. If it is true (fahr
is less than or equal to upper), the body of the loop (the three statements enclosed in braces) is
executed. Then the condition is re-tested, and if true, the body is executed again. When the test
becomes false (fahr exceeds upper) the loop ends, and execution continues at the statement
that follows the loop. There are no further statements in this program, so it terminates.
The body of a while can be one or more statements enclosed in braces, as in the temperature
converter, or a single statement without braces, as in
 while (i < j)
 i = 2 * i;
In either case, we will always indent the statements controlled by the while by one tab stop
(which we have shown as four spaces) so you can see at a glance which statements are inside
the loop. The indentation emphasizes the logical structure of the program. Although C
compilers do not care about how a program looks, proper indentation and spacing are critical
in making programs easy for people to read. We recommend writing only one statement per
line, and using blanks around operators to clarify grouping. The position of braces is less
important, although people hold passionate beliefs. We have chosen one of several popular
styles. Pick a style that suits you, then use it consistently.
Most of the work gets done in the body of the loop. The Celsius temperature is computed and
assigned to the variable celsius by the statement
 celsius = 5 * (fahr-32) / 9;
The reason for multiplying by 5 and dividing by 9 instead of just multiplying by 5/9 is that in
C, as in many other languages, integer division truncates: any fractional part is discarded.
Since 5 and 9 are integers. 5/9 would be truncated to zero and so all the Celsius temperatures
would be reported as zero.
This example also shows a bit more of how printf works. printf is a general-purpose
output formatting function, which we will describe in detail in Chapter 7. Its first argument is a
string of characters to be printed, with each % indicating where one of the other (second, third,
14
...) arguments is to be substituted, and in what form it is to be printed. For instance, %d
specifies an integer argument, so the statement
 printf("%d\t%d\n", fahr, celsius);
causes the values of the two integers fahr and celsius to be printed, with a tab (\t) between
them.
Each % construction in the first argument of printf is paired with the corresponding second
argument, third argument, etc.; they must match up properly by number and type, or you will
get wrong answers.
By the way, printf is not part of the C language; there is no input or output defined in C
itself. printf is just a useful function from the standard library of functions that are normally
accessible to C programs. The behaviour of printf is defined in the ANSI standard, however,
so its properties should be the same with any compiler and library that conforms to the
standard.
In order to concentrate on C itself, we don't talk much about input and output until chapter 7.
In particular, we will defer formatted input until then. If you have to input numbers, read the
discussion of the function scanf in Section 7.4. scanf is like printf, except that it reads
input instead of writing output.
There are a couple of problems with the temperature conversion program. The simpler one is
that the output isn't very pretty because the numbers are not right-justified. That's easy to fix; if
we augment each %d in the printf statement with a width, the numbers printed will be rightjustified in their fields. For instance, we might say
 printf("%3d %6d\n", fahr, celsius);
to print the first number of each line in a field three digits wide, and the second in a field six
digits wide, like this:
 0 -17
 20 -6
 40 4
 60 15
 80 26
 100 37
 ...
The more serious problem is that because we have used integer arithmetic, the Celsius
temperatures are not very accurate; for instance, 0
oF is actually about -17.8oC, not -17. To get
more accurate answers, we should use floating-point arithmetic instead of integer. This
requires some changes in the program. Here is the second version:
 #include <stdio.h>
 /* print Fahrenheit-Celsius table
 for fahr = 0, 20, ..., 300; floating-point version */
 main()
 {
 float fahr, celsius;
 float lower, upper, step;
 lower = 0; /* lower limit of temperatuire scale */
 upper = 300; /* upper limit */
 step = 20; /* step size */
 fahr = lower;
 while (fahr <= upper) {
 celsius = (5.0/9.0) * (fahr-32.0);
 printf("%3.0f %6.1f\n", fahr, celsius);
15
 fahr = fahr + step;
 }
 }
This is much the same as before, except that fahr and celsius are declared to be float and
the formula for conversion is written in a more natural way. We were unable to use 5/9 in the
previous version because integer division would truncate it to zero. A decimal point in a
constant indicates that it is floating point, however, so 5.0/9.0 is not truncated because it is
the ratio of two floating-point values.
If an arithmetic operator has integer operands, an integer operation is performed. If an
arithmetic operator has one floating-point operand and one integer operand, however, the
integer will be converted to floating point before the operation is done. If we had written
(fahr-32), the 32 would be automatically converted to floating point. Nevertheless, writing
floating-point constants with explicit decimal points even when they have integral values
emphasizes their floating-point nature for human readers.
The detailed rules for when integers are converted to floating point are in Chapter 2. For now,
notice that the assignment
 fahr = lower;
and the test
 while (fahr <= upper)
also work in the natural way - the int is converted to float before the operation is done.
The printf conversion specification %3.0f says that a floating-point number (here fahr) is to
be printed at least three characters wide, with no decimal point and no fraction digits. %6.1f
describes another number (celsius) that is to be printed at least six characters wide, with 1
digit after the decimal point. The output looks like this:
 0 -17.8
 20 -6.7
 40 4.4
 ...
Width and precision may be omitted from a specification: %6f says that the number is to be at
least six characters wide; %.2f specifies two characters after the decimal point, but the width is
not constrained; and %f merely says to print the number as floating point.
%d print as decimal integer
%6d print as decimal integer, at least 6 characters wide
%f print as floating point
%6f print as floating point, at least 6 characters wide
%.2f print as floating point, 2 characters after decimal point
%6.2f print as floating point, at least 6 wide and 2 after decimal point
Among others, printf also recognizes %o for octal, %x for hexadecimal, %c for character, %s
for character string and %% for itself.
Exercise 1-3. Modify the temperature conversion program to print a heading above the table.
Exercise 1-4. Write a program to print the corresponding Celsius to Fahrenheit table.
1.3 The for statement
There are plenty of different ways to write a program for a particular task. Let's try a variation
on the temperature converter.
 #include <stdio.h>
16
 /* print Fahrenheit-Celsius table */
 main()
 {
 int fahr;
 for (fahr = 0; fahr <= 300; fahr = fahr + 20)
 printf("%3d %6.1f\n", fahr, (5.0/9.0)*(fahr-32));
 }
This produces the same answers, but it certainly looks different. One major change is the
elimination of most of the variables; only fahr remains, and we have made it an int. The
lower and upper limits and the step size appear only as constants in the for statement, itself a
new construction, and the expression that computes the Celsius temperature now appears as
the third argument of printf instead of a separate assignment statement.
This last change is an instance of a general rule - in any context where it is permissible to use
the value of some type, you can use a more complicated expression of that type. Since the third
argument of printf must be a floating-point value to match the %6.1f, any floating-point
expression can occur here.
The for statement is a loop, a generalization of the while. If you compare it to the earlier
while, its operation should be clear. Within the parentheses, there are three parts, separated by
semicolons. The first part, the initialization
 fahr = 0
17
is done once, before the loop proper is entered. The second part is the
test or condition that controls the loop:
 fahr <= 300
This condition is evaluated; if it is true, the body of the loop (here a single ptintf) is
executed. Then the increment step
 fahr = fahr + 20
is executed, and the condition re-evaluated. The loop terminates if the condition has become
false. As with the while, the body of the loop can be a single statement or a group of
statements enclosed in braces. The initialization, condition and increment can be any
expressions.
The choice between while and for is arbitrary, based on which seems clearer. The for is
usually appropriate for loops in which the initialization and increment are single statements and
logically related, since it is more compact than while and it keeps the loop control statements
together in one place.
Exercise 1-5. Modify the temperature conversion program to print the table in reverse order,
that is, from 300 degrees to 0.
1.4 Symbolic Constants
A final observation before we leave temperature conversion forever. It's bad practice to bury
``magic numbers'' like 300 and 20 in a program; they convey little information to someone who
might have to read the program later, and they are hard to change in a systematic way. One
way to deal with magic numbers is to give them meaningful names. A #define line defines a
symbolic name or symbolic constant to be a particular string of characters:
#define name replacement list
Thereafter, any occurrence of name (not in quotes and not part of another name) will be
replaced by the corresponding replacement text. The name has the same form as a variable
name: a sequence of letters and digits that begins with a letter. The replacement text can be
any sequence of characters; it is not limited to numbers.
 #include <stdio.h>
 #define LOWER 0 /* lower limit of table */
 #define UPPER 300 /* upper limit */
 #define STEP 20 /* step size */
 /* print Fahrenheit-Celsius table */
 main()
 {
 int fahr;
 for (fahr = LOWER; fahr <= UPPER; fahr = fahr + STEP)
 printf("%3d %6.1f\n", fahr, (5.0/9.0)*(fahr-32));
 }
The quantities LOWER, UPPER and STEP are symbolic constants, not variables, so they do not
appear in declarations. Symbolic constant names are conventionally written in upper case so
they can ber readily distinguished from lower case variable names. Notice that there is no
semicolon at the end of a #define line.
1.5 Character Input and Output
We are going to consider a family of related programs for processing character data. You will
find that many programs are just expanded versions of the prototypes that we discuss here. 
18
The model of input and output supported by the standard library is very simple. Text input or
output, regardless of where it originates or where it goes to, is dealt with as streams of
characters. A text stream is a sequence of characters divided into lines; each line consists of
zero or more characters followed by a newline character. It is the responsibility of the library to
make each input or output stream confirm this model; the C programmer using the library need
not worry about how lines are represented outside the program.
The standard library provides several functions for reading or writing one character at a time,
of which getchar and putchar are the simplest. Each time it is called, getchar reads the next
input character from a text stream and returns that as its value. That is, after
 c = getchar();
the variable c contains the next character of input. The characters normally come from the
keyboard; input from files is discussed in Chapter 7.
The function putchar prints a character each time it is called:
 putchar(c);
prints the contents of the integer variable c as a character, usually on the screen. Calls to
putchar and printf may be interleaved; the output will appear in the order in which the calls
are made.
1.5.1 File Copying
Given getchar and putchar, you can write a surprising amount of useful code without
knowing anything more about input and output. The simplest example is a program that copies
its input to its output one character at a time:
read a character
 while (charater is not end-of-file indicator)
 output the character just read
 read a character
Converting this into C gives:
 #include <stdio.h>
 /* copy input to output; 1st version */
 main()
 {
 int c;
 c = getchar();
 while (c != EOF) {
 putchar(c);
 c = getchar();
 }
 }
The relational operator != means ``not equal to''.
What appears to be a character on the keyboard or screen is of course, like everything else,
stored internally just as a bit pattern. The type char is specifically meant for storing such
character data, but any integer type can be used. We used int for a subtle but important
reason.
The problem is distinguishing the end of input from valid data. The solution is that getchar
returns a distinctive value when there is no more input, a value that cannot be confused with
any real character. This value is called EOF, for ``end of file''. We must declare c to be a type
big enough to hold any value that getchar returns. We can't use char since c must be big
enough to hold EOF in addition to any possible char. Therefore we use int. 
19
EOF is an integer defined in <stdio.h>, but the specific numeric value doesn't matter as long as
it is not the same as any char value. By using the symbolic constant, we are assured that
nothing in the program depends on the specific numeric value.
The program for copying would be written more concisely by experienced C programmers. In
C, any assignment, such as
 c = getchar();
is an expression and has a value, which is the value of the left hand side after the assignment.
This means that a assignment can appear as part of a larger expression. If the assignment of a
character to c is put inside the test part of a while loop, the copy program can be written this
way:
 #include <stdio.h>
 /* copy input to output; 2nd version */
 main()
 {
 int c;
 while ((c = getchar()) != EOF)
 putchar(c);
 }
The while gets a character, assigns it to c, and then tests whether the character was the endof-file signal. If it was not, the body of the while is executed, printing the character. The
while then repeats. When the end of the input is finally reached, the while terminates and so
does main.
This version centralizes the input - there is now only one reference to getchar - and shrinks
the program. The resulting program is more compact, and, once the idiom is mastered, easier
to read. You'll see this style often. (It's possible to get carried away and create impenetrable
code, however, a tendency that we will try to curb.)
The parentheses around the assignment, within the condition are necessary. The precedence of
!= is higher than that of =, which means that in the absence of parentheses the relational test !=
would be done before the assignment =. So the statement
 c = getchar() != EOF
is equivalent to
 c = (getchar() != EOF)
This has the undesired effect of setting c to 0 or 1, depending on whether or not the call of
getchar returned end of file. (More on this in Chapter 2.)
Exercsise 1-6. Verify that the expression getchar() != EOF is 0 or 1.
Exercise 1-7. Write a program to print the value of EOF.
1.5.2 Character Counting
The next program counts characters; it is similar to the copy program.
 #include <stdio.h>
 /* count characters in input; 1st version */
 main()
 {
 long nc;
 nc = 0;
 while (getchar() != EOF)
20
 ++nc;
 printf("%ld\n", nc);
 }
The statement
 ++nc;
presents a new operator, ++, which means increment by one. You could instead write nc = nc
+ 1 but ++nc is more concise and often more efficient. There is a corresponding operator -- to
decrement by 1. The operators ++ and -- can be either prefix operators (++nc) or postfix
operators (nc++); these two forms have different values in expressions, as will be shown in
Chapter 2, but ++nc and nc++ both increment nc. For the moment we will will stick to the
prefix form.
The character counting program accumulates its count in a long variable instead of an int.
long integers are at least 32 bits. Although on some machines, int and long are the same size,
on others an int is 16 bits, with a maximum value of 32767, and it would take relatively little
input to overflow an int counter. The conversion specification %ld tells printf that the
corresponding argument is a long integer.
It may be possible to cope with even bigger numbers by using a double (double precision
float). We will also use a for statement instead of a while, to illustrate another way to write
the loop.
 #include <stdio.h>
 /* count characters in input; 2nd version */
 main()
 {
 double nc;
 for (nc = 0; gechar() != EOF; ++nc)
 ;
 printf("%.0f\n", nc);
 }
printf uses %f for both float and double; %.0f suppresses the printing of the decimal point
and the fraction part, which is zero.
The body of this for loop is empty, because all the work is done in the test and increment
parts. But the grammatical rules of C require that a for statement have a body. The isolated
semicolon, called a null statement, is there to satisfy that requirement. We put it on a separate
line to make it visible.
Before we leave the character counting program, observe that if the input contains no
characters, the while or for test fails on the very first call to getchar, and the program
produces zero, the right answer. This is important. One of the nice things about while and for
is that they test at the top of the loop, before proceeding with the body. If there is nothing to
do, nothing is done, even if that means never going through the loop body. Programs should
act intelligently when given zero-length input. The while and for statements help ensure that
programs do reasonable things with boundary conditions.
1.5.3 Line Counting
The next program counts input lines. As we mentioned above, the standard library ensures that
an input text stream appears as a sequence of lines, each terminated by a newline. Hence,
counting lines is just counting newlines:
 #include <stdio.h>
 /* count lines in input */
 main()
21
 {
 int c, nl;
 nl = 0;
 while ((c = getchar()) != EOF)
 if (c == '\n')
 ++nl;
 printf("%d\n", nl);
 }
The body of the while now consists of an if, which in turn controls the increment ++nl. The
if statement tests the parenthesized condition, and if the condition is true, executes the
statement (or group of statements in braces) that follows. We have again indented to show
what is controlled by what.
The double equals sign == is the C notation for ``is equal to'' (like Pascal's single = or Fortran's
.EQ.). This symbol is used to distinguish the equality test from the single = that C uses for
assignment. A word of caution: newcomers to C occasionally write = when they mean ==. As
we will see in Chapter 2, the result is usually a legal expression, so you will get no warning.
A character written between single quotes represents an integer value equal to the numerical
value of the character in the machine's character set. This is called a character constant,
although it is just another way to write a small integer. So, for example, 'A' is a character
constant; in the ASCII character set its value is 65, the internal representation of the character
A. Of course, 'A' is to be preferred over 65: its meaning is obvious, and it is independent of a
particular character set.
The escape sequences used in string constants are also legal in character constants, so '\n'
stands for the value of the newline character, which is 10 in ASCII. You should note carefully
that '\n' is a single character, and in expressions is just an integer; on the other hand, '\n' is
a string constant that happens to contain only one character. The topic of strings versus
characters is discussed further in Chapter 2.
Exercise 1-8. Write a program to count blanks, tabs, and newlines.
Exercise 1-9. Write a program to copy its input to its output, replacing each string of one or
more blanks by a single blank.
Exercise 1-10. Write a program to copy its input to its output, replacing each tab by \t, each
backspace by \b, and each backslash by \\. This makes tabs and backspaces visible in an
unambiguous way.
1.5.4 Word Counting
The fourth in our series of useful programs counts lines, words, and characters, with the loose
definition that a word is any sequence of characters that does not contain a blank, tab or
newline. This is a bare-bones version of the UNIX program wc.
 #include <stdio.h>
 #define IN 1 /* inside a word */
 #define OUT 0 /* outside a word */
 /* count lines, words, and characters in input */
 main()
 {
 int c, nl, nw, nc, state;
 state = OUT;
 nl = nw = nc = 0;
 while ((c = getchar()) != EOF) {
22
 ++nc;
 if (c == '\n')
 ++nl;
 if (c == ' ' || c == '\n' || c = '\t')
 state = OUT;
 else if (state == OUT) {
 state = IN;
 ++nw;
 }
 }
 printf("%d %d %d\n", nl, nw, nc);
 }
Every time the program encounters the first character of a word, it counts one more word. The
variable state records whether the program is currently in a word or not; initially it is ``not in
a word'', which is assigned the value OUT. We prefer the symbolic constants IN and OUT to the
literal values 1 and 0 because they make the program more readable. In a program as tiny as
this, it makes little difference, but in larger programs, the increase in clarity is well worth the
modest extra effort to write it this way from the beginning. You'll also find that it's easier to
make extensive changes in programs where magic numbers appear only as symbolic constants. 
23
The line
 nl = nw = nc = 0;
sets all three variables to zero. This is not a special case, but a consequence of the fact that an
assignment is an expression with the value and assignments associated from right to left. It's as
if we had written
 nl = (nw = (nc = 0));
The operator || means OR, so the line
 if (c == ' ' || c == '\n' || c = '\t')
says ``if c is a blank or c is a newline or c is a tab''. (Recall that the escape sequence \t is a
visible representation of the tab character.) There is a corresponding operator && for AND; its
precedence is just higher than ||. Expressions connected by && or || are evaluated left to
right, and it is guaranteed that evaluation will stop as soon as the truth or falsehood is known.
If c is a blank, there is no need to test whether it is a newline or tab, so these tests are not
made. This isn't particularly important here, but is significant in more complicated situations, as
we will soon see.
The example also shows an else, which specifies an alternative action if the condition part of
an if statement is false. The general form is
 if (expression)
 statement1
 else
 statement2
One and only one of the two statements associated with an if-else is performed. If the
expression is true, statement1 is executed; if not, statement2 is executed. Each statement can be
a single statement or several in braces. In the word count program, the one after the else is an
if that controls two statements in braces.
Exercise 1-11. How would you test the word count program? What kinds of input are most
likely to uncover bugs if there are any?
Exercise 1-12. Write a program that prints its input one word per line.
1.6 Arrays
Let is write a program to count the number of occurrences of each digit, of white space
characters (blank, tab, newline), and of all other characters. This is artificial, but it permits us
to illustrate several aspects of C in one program.
There are twelve categories of input, so it is convenient to use an array to hold the number of
occurrences of each digit, rather than ten individual variables. Here is one version of the
program: 
24
 #include <stdio.h>
 /* count digits, white space, others */
 main()
 {
 int c, i, nwhite, nother;
 int ndigit[10];
 nwhite = nother = 0;
 for (i = 0; i < 10; ++i)
 ndigit[i] = 0;
 while ((c = getchar()) != EOF)
 if (c >= '0' && c <= '9')
 ++ndigit[c-'0'];
 else if (c == ' ' || c == '\n' || c == '\t')
 ++nwhite;
 else
 ++nother;
 printf("digits =");
 for (i = 0; i < 10; ++i)
 printf(" %d", ndigit[i]);
 printf(", white space = %d, other = %d\n",
 nwhite, nother);
 }
The output of this program on itself is
 digits = 9 3 0 0 0 0 0 0 0 1, white space = 123, other = 345
The declaration
 int ndigit[10];
declares ndigit to be an array of 10 integers. Array subscripts always start at zero in C, so the
elements are ndigit[0], ndigit[1], ..., ndigit[9]. This is reflected in the for loops
that initialize and print the array.
A subscript can be any integer expression, which includes integer variables like i, and integer
constants.
This particular program relies on the properties of the character representation of the digits.
For example, the test
 if (c >= '0' && c <= '9')
determines whether the character in c is a digit. If it is, the numeric value of that digit is
 c - '0'
This works only if '0', '1', ..., '9' have consecutive increasing values. Fortunately, this
is true for all character sets.
By definition, chars are just small integers, so char variables and constants are identical to
ints in arithmetic expressions. This is natural and convenient; for example c-'0' is an integer
expression with a value between 0 and 9 corresponding to the character '0' to '9' stored in c,
and thus a valid subscript for the array ndigit.
The decision as to whether a character is a digit, white space, or something else is made with
the sequence
 if (c >= '0' && c <= '9')
 ++ndigit[c-'0'];
 else if (c == ' ' || c == '\n' || c == '\t')
 ++nwhite;
25
 else
 ++nother;
The pattern
 if (condition1)
 statement1
 else if (condition2)
 statement2
 ...
 ...
 else
 statementn
occurs frequently in programs as a way to express a multi-way decision. The conditions are
evaluated in order from the top until some condition is satisfied; at that point the
corresponding statement part is executed, and the entire construction is finished. (Any
statement can be several statements enclosed in braces.) If none of the conditions is satisfied,
the statement after the final else is executed if it is present. If the final else and statement are
omitted, as in the word count program, no action takes place. There can be any number of
else if(condition)
statement
groups between the initial if and the final else.
As a matter of style, it is advisable to format this construction as we have shown; if each if
were indented past the previous else, a long sequence of decisions would march off the right
side of the page.
The switch statement, to be discussed in Chapter 4, provides another way to write a multiway branch that is particulary suitable when the condition is whether some integer or character
expression matches one of a set of constants. For contrast, we will present a switch version of
this program in Section 3.4.
Exercise 1-13. Write a program to print a histogram of the lengths of words in its input. It is
easy to draw the histogram with the bars horizontal; a vertical orientation is more challenging.
Exercise 1-14. Write a program to print a histogram of the frequencies of different characters
in its input.
1.7 Functions
In C, a function is equivalent to a subroutine or function in Fortran, or a procedure or function
in Pascal. A function provides a convenient way to encapsulate some computation, which can
then be used without worrying about its implementation. With properly designed functions, it
is possible to ignore how a job is done; knowing what is done is sufficient. C makes the sue of
functions easy, convinient and efficient; you will often see a short function defined and called
only once, just because it clarifies some piece of code.
So far we have used only functions like printf, getchar and putchar that have been
provided for us; now it's time to write a few of our own. Since C has no exponentiation
operator like the ** of Fortran, let us illustrate the mechanics of function definition by writing
a function power(m,n) to raise an integer m to a positive integer power n. That is, the value of
power(2,5) is 32. This function is not a practical exponentiation routine, since it handles only
positive powers of small integers, but it's good enough for illustration.(The standard library
contains a function pow(x,y) that computes x
y
.)
Here is the function power and a main program to exercise it, so you can see the whole
structure at once. 
26
 #include <stdio.h>
 int power(int m, int n);
 /* test power function */
 main()
 {
 int i;
 for (i = 0; i < 10; ++i)
 printf("%d %d %d\n", i, power(2,i), power(-3,i));
 return 0;
 }
 /* power: raise base to n-th power; n >= 0 */
 int power(int base, int n)
 {
 int i, p;
 p = 1;
 for (i = 1; i <= n; ++i)
 p = p * base;
 return p;
 }
A function definition has this form:
return-type function-name(parameter declarations, if any)
{
 declarations
 statements
}
Function definitions can appear in any order, and in one source file or several, although no
function can be split between files. If the source program appears in several files, you may have
to say more to compile and load it than if it all appears in one, but that is an operating system
matter, not a language attribute. For the moment, we will assume that both functions are in the
same file, so whatever you have learned about running C programs will still work.
The function power is called twice by main, in the line
 printf("%d %d %d\n", i, power(2,i), power(-3,i));
Each call passes two arguments to power, which each time returns an integer to be formatted
and printed. In an expression, power(2,i) is an integer just as 2 and i are. (Not all functions
produce an integer value; we will take this up in Chapter 4.)
The first line of power itself,
 int power(int base, int n)
declares the parameter types and names, and the type of the result that the function returns.
The names used by power for its parameters are local to power, and are not visible to any
other function: other routines can use the same names without conflict. This is also true of the
variables i and p: the i in power is unrelated to the i in main.
We will generally use parameter for a variable named in the parenthesized list in a function.
The terms formal argument and actual argument are sometimes used for the same distinction.
The value that power computes is returned to main by the return: statement. Any expression
may follow return:
 return expression;
A function need not return a value; a return statement with no expression causes control, but
no useful value, to be returned to the caller, as does ``falling off the end'' of a function by
27
reaching the terminating right brace. And the calling function can ignore a value returned by a
function.
You may have noticed that there is a return statement at the end of main. Since main is a
function like any other, it may return a value to its caller, which is in effect the environment in
which the program was executed. Typically, a return value of zero implies normal termination;
non-zero values signal unusual or erroneous termination conditions. In the interests of
simplicity, we have omitted return statements from our main functions up to this point, but
we will include them hereafter, as a reminder that programs should return status to their
environment.
The declaration
 int power(int base, int n);
just before main says that power is a function that expects two int arguments and returns an
int. This declaration, which is called a function prototype, has to agree with the definition and
uses of power. It is an error if the definition of a function or any uses of it do not agree with its
prototype.
parameter names need not agree. Indeed, parameter names are optional in a function
prototype, so for the prototype we could have written
 int power(int, int);
Well-chosen names are good documentation however, so we will often use them.
A note of history: the biggest change between ANSI C and earlier versions is how functions
are declared and defined. In the original definition of C, the power function would have been
written like this: 
28
 /* power: raise base to n-th power; n >= 0 */
 /* (old-style version) */
 power(base, n)
 int base, n;
 {
 int i, p;
 p = 1;
 for (i = 1; i <= n; ++i)
 p = p * base;
 return p;
 }
The parameters are named between the parentheses, and their types are declared before
opening the left brace; undeclared parameters are taken as int. (The body of the function is
the same as before.)
The declaration of power at the beginning of the program would have looked like this:
 int power();
No parameter list was permitted, so the compiler could not readily check that power was being
called correctly. Indeed, since by default power would have been assumed to return an int, the
entire declaration might well have been omitted.
The new syntax of function prototypes makes it much easier for a compiler to detect errors in
the number of arguments or their types. The old style of declaration and definition still works
in ANSI C, at least for a transition period, but we strongly recommend that you use the new
form when you have a compiler that supports it.
Exercise 1.15. Rewrite the temperature conversion program of Section 1.2 to use a function
for conversion.
1.8 Arguments - Call by Value
One aspect of C functions may be unfamiliar to programmers who are used to some other
languages, particulary Fortran. In C, all function arguments are passed ``by value.'' This means
that the called function is given the values of its arguments in temporary variables rather than
the originals. This leads to some different properties than are seen with ``call by reference''
languages like Fortran or with var parameters in Pascal, in which the called routine has access
to the original argument, not a local copy.
Call by value is an asset, however, not a liability. It usually leads to more compact programs
with fewer extraneous variables, because parameters can be treated as conveniently initialized
local variables in the called routine. For example, here is a version of power that makes use of
this property.
 /* power: raise base to n-th power; n >= 0; version 2 */
 int power(int base, int n)
 {
 int p;
 for (p = 1; n > 0; --n)
 p = p * base;
 return p;
 }
The parameter n is used as a temporary variable, and is counted down (a for loop that runs
backwards) until it becomes zero; there is no longer a need for the variable i. Whatever is
done to n inside power has no effect on the argument that power was originally called with.
When necessary, it is possible to arrange for a function to modify a variable in a calling routine.
The caller must provide the address of the variable to be set (technically a pointer to the
29
variable), and the called function must declare the parameter to be a pointer and access the
variable indirectly through it. We will cover pointers in Chapter 5.
The story is different for arrays. When the name of an array is used as an argument, the value
passed to the function is the location or address of the beginning of the array - there is no
copying of array elements. By subscripting this value, the function can access and alter any
argument of the array. This is the topic of the next section.
1.9 Character Arrays
The most common type of array in C is the array of characters. To illustrate the use of
character arrays and functions to manipulate them, let's write a program that reads a set of text
lines and prints the longest. The outline is simple enough:
 while (there's another line)
 if (it's longer than the previous longest)
 (save it)
 (save its length)
 print longest line
This outline makes it clear that the program divides naturally into pieces. One piece gets a new
line, another saves it, and the rest controls the process.
Since things divide so nicely, it would be well to write them that way too. Accordingly, let us
first write a separate function getline to fetch the next line of input. We will try to make the
function useful in other contexts. At the minimum, getline has to return a signal about
possible end of file; a more useful design would be to return the length of the line, or zero if
end of file is encountered. Zero is an acceptable end-of-file return because it is never a valid
line length. Every text line has at least one character; even a line containing only a newline has
length 1.
When we find a line that is longer than the previous longest line, it must be saved somewhere.
This suggests a second function, copy, to copy the new line to a safe place.
Finally, we need a main program to control getline and copy. Here is the result. 
30
 #include <stdio.h>
 #define MAXLINE 1000 /* maximum input line length */
 int getline(char line[], int maxline);
 void copy(char to[], char from[]);
 /* print the longest input line */
 main()
 {
 int len; /* current line length */
 int max; /* maximum length seen so far */
 char line[MAXLINE]; /* current input line */
 char longest[MAXLINE]; /* longest line saved here */
 max = 0;
 while ((len = getline(line, MAXLINE)) > 0)
 if (len > max) {
 max = len;
 copy(longest, line);
 }
 if (max > 0) /* there was a line */
 printf("%s", longest);
 return 0;
 }
 /* getline: read a line into s, return length */
 int getline(char s[],int lim)
 {
 int c, i;
 for (i=0; i < lim-1 && (c=getchar())!=EOF && c!='\n'; ++i)
 s[i] = c;
 if (c == '\n') {
 s[i] = c;
 ++i;
 }
 s[i] = '\0';
 return i;
 }
 /* copy: copy 'from' into 'to'; assume to is big enough */
 void copy(char to[], char from[])
 {
 int i;
 i = 0;
 while ((to[i] = from[i]) != '\0')
 ++i;
 }
The functions getline and copy are declared at the beginning of the program, which we
assume is contained in one file.
main and getline communicate through a pair of arguments and a returned value. In
getline, the arguments are declared by the line
 int getline(char s[], int lim);
which specifies that the first argument, s, is an array, and the second, lim, is an integer. The
purpose of supplying the size of an array in a declaration is to set aside storage. The length of
an array s is not necessary in getline since its size is set in main. getline uses return to
send a value back to the caller, just as the function power did. This line also declares that
getline returns an int; since int is the default return type, it could be omitted.
Some functions return a useful value; others, like copy, are used only for their effect and return
no value. The return type of copy is void, which states explicitly that no value is returned. 
31
getline puts the character '\0' (the null character, whose value is zero) at the end of the
array it is creating, to mark the end of the string of characters. This conversion is also used by
the C language: when a string constant like
 "hello\n"
appears in a C program, it is stored as an array of characters containing the characters in the
string and terminated with a '\0' to mark the end.
The %s format specification in printf expects the corresponding argument to be a string
represented in this form. copy also relies on the fact that its input argument is terminated with
a '\0', and copies this character into the output.
It is worth mentioning in passing that even a program as small as this one presents some sticky
design problems. For example, what should main do if it encounters a line which is bigger than
its limit? getline works safely, in that it stops collecting when the array is full, even if no
newline has been seen. By testing the length and the last character returned, main can
determine whether the line was too long, and then cope as it wishes. In the interests of brevity,
we have ignored this issue.
There is no way for a user of getline to know in advance how long an input line might be, so
getline checks for overflow. On the other hand, the user of copy already knows (or can find
out) how big the strings are, so we have chosen not to add error checking to it.
Exercise 1-16. Revise the main routine of the longest-line program so it will correctly print the
length of arbitrary long input lines, and as much as possible of the text.
Exercise 1-17. Write a program to print all input lines that are longer than 80 characters.
Exercise 1-18. Write a program to remove trailing blanks and tabs from each line of input, and
to delete entirely blank lines.
Exercise 1-19. Write a function reverse(s) that reverses the character string s. Use it to
write a program that reverses its input a line at a time.
1.10 External Variables and Scope
The variables in main, such as line, longest, etc., are private or local to main. Because they
are declared within main, no other function can have direct access to them. The same is true of
the variables in other functions; for example, the variable i in getline is unrelated to the i in
copy. Each local variable in a function comes into existence only when the function is called,
and disappears when the function is exited. This is why such variables are usually known as
automatic variables, following terminology in other languages. We will use the term automatic
henceforth to refer to these local variables. (Chapter 4 discusses the static storage class, in
which local variables do retain their values between calls.)
Because automatic variables come and go with function invocation, they do not retain their
values from one call to the next, and must be explicitly set upon each entry. If they are not set,
they will contain garbage.
As an alternative to automatic variables, it is possible to define variables that are external to all
functions, that is, variables that can be accessed by name by any function. (This mechanism is
rather like Fortran COMMON or Pascal variables declared in the outermost block.) Because
32
external variables are globally accessible, they can be used instead of argument lists to
communicate data between functions. Furthermore, because external variables remain in
existence permanently, rather than appearing and disappearing as functions are called and
exited, they retain their values even after the functions that set them have returned.
An external variable must be defined, exactly once, outside of any function; this sets aside
storage for it. The variable must also be declared in each function that wants to access it; this
states the type of the variable. The declaration may be an explicit extern statement or may be
implicit from context. To make the discussion concrete, let us rewrite the longest-line program
with line, longest, and max as external variables. This requires changing the calls,
declarations, and bodies of all three functions.
 #include <stdio.h>
 #define MAXLINE 1000 /* maximum input line size */
 int max; /* maximum length seen so far */
 char line[MAXLINE]; /* current input line */
 char longest[MAXLINE]; /* longest line saved here */
 int getline(void);
 void copy(void);
 /* print longest input line; specialized version */
 main()
 {
 int len;
 extern int max;
 extern char longest[];
 max = 0;
 while ((len = getline()) > 0)
 if (len > max) {
 max = len;
 copy();
 }
 if (max > 0) /* there was a line */
 printf("%s", longest);
 return 0;
 }
33
 /* getline: specialized version */
 int getline(void)
 {
 int c, i;
 extern char line[];
 for (i = 0; i < MAXLINE - 1
 && (c=getchar)) != EOF && c != '\n'; ++i)
 line[i] = c;
 if (c == '\n') {
 line[i] = c;
 ++i;
 }
 line[i] = '\0';
 return i;
 }
 /* copy: specialized version */
 void copy(void)
 {
 int i;
 extern char line[], longest[];
 i = 0;
 while ((longest[i] = line[i]) != '\0')
 ++i;
 }
The external variables in main, getline and copy are defined by the first lines of the example
above, which state their type and cause storage to be allocated for them. Syntactically, external
definitions are just like definitions of local variables, but since they occur outside of functions,
the variables are external. Before a function can use an external variable, the name of the
variable must be made known to the function; the declaration is the same as before except for
the added keyword extern.
In certain circumstances, the extern declaration can be omitted. If the definition of the
external variable occurs in the source file before its use in a particular function, then there is no
need for an extern declaration in the function. The extern declarations in main, getline and
copy are thus redundant. In fact, common practice is to place definitions of all external
variables at the beginning of the source file, and then omit all extern declarations.
If the program is in several source files, and a variable is defined in file1 and used in file2 and
file3, then extern declarations are needed in file2 and file3 to connect the occurrences of the
variable. The usual practice is to collect extern declarations of variables and functions in a
separate file, historically called a header, that is included by #include at the front of each
source file. The suffix .h is conventional for header names. The functions of the standard
library, for example, are declared in headers like <stdio.h>. This topic is discussed at length
in Chapter 4, and the library itself in Chapter 7 and Appendix B.
Since the specialized versions of getline and copy have no arguments, logic would suggest
that their prototypes at the beginning of the file should be getline() and copy(). But for
compatibility with older C programs the standard takes an empty list as an old-style
declaration, and turns off all argument list checking; the word void must be used for an
explicitly empty list. We will discuss this further in Chapter 4.
You should note that we are using the words definition and declaration carefully when we
refer to external variables in this section.``Definition'' refers to the place where the variable is
created or assigned storage; ``declaration'' refers to places where the nature of the variable is
stated but no storage is allocated. 
34
By the way, there is a tendency to make everything in sight an extern variable because it
appears to simplify communications - argument lists are short and variables are always there
when you want them. But external variables are always there even when you don't want them.
Relying too heavily on external variables is fraught with peril since it leads to programs whose
data connections are not all obvious - variables can be changed in unexpected and even
inadvertent ways, and the program is hard to modify. The second version of the longest-line
program is inferior to the first, partly for these reasons, and partly because it destroys the
generality of two useful functions by writing into them the names of the variables they
manipulate.
At this point we have covered what might be called the conventional core of C. With this
handful of building blocks, it's possible to write useful programs of considerable size, and it
would probably be a good idea if you paused long enough to do so. These exercises suggest
programs of somewhat greater complexity than the ones earlier in this chapter.
Exercise 1-20. Write a program detab that replaces tabs in the input with the proper number
of blanks to space to the next tab stop. Assume a fixed set of tab stops, say every n columns.
Should n be a variable or a symbolic parameter?
Exercise 1-21. Write a program entab that replaces strings of blanks by the minimum number
of tabs and blanks to achieve the same spacing. Use the same tab stops as for detab. When
either a tab or a single blank would suffice to reach a tab stop, which should be given
preference?
Exercise 1-22. Write a program to ``fold'' long input lines into two or more shorter lines after
the last non-blank character that occurs before the n-th column of input. Make sure your
program does something intelligent with very long lines, and if there are no blanks or tabs
before the specified column.
Exercise 1-23. Write a program to remove all comments from a C program. Don't forget to
handle quoted strings and character constants properly. C comments don't nest.
Exercise 1-24. Write a program to check a C program for rudimentary syntax errors like
unmatched parentheses, brackets and braces. Don't forget about quotes, both single and
double, escape sequences, and comments. (This program is hard if you do it in full generality.) 
35
Chapter 2 - Types, Operators and
Expressions
Variables and constants are the basic data objects manipulated in a program. Declarations list
the variables to be used, and state what type they have and perhaps what their initial values are.
Operators specify what is to be done to them. Expressions combine variables and constants to
produce new values. The type of an object determines the set of values it can have and what
operations can be performed on it. These building blocks are the topics of this chapter.
The ANSI standard has made many small changes and additions to basic types and expressions.
There are now signed and unsigned forms of all integer types, and notations for unsigned
constants and hexadecimal character constants. Floating-point operations may be done in
single precision; there is also a long double type for extended precision. String constants may
be concatenated at compile time. Enumerations have become part of the language, formalizing
a feature of long standing. Objects may be declared const, which prevents them from being
changed. The rules for automatic coercions among arithmetic types have been augmented to
handle the richer set of types.
2.1 Variable Names
Although we didn't say so in Chapter 1, there are some restrictions on the names of variables
and symbolic constants. Names are made up of letters and digits; the first character must be a
letter. The underscore ``_'' counts as a letter; it is sometimes useful for improving the
readability of long variable names. Don't begin variable names with underscore, however, since
library routines often use such names. Upper and lower case letters are distinct, so x and X are
two different names. Traditional C practice is to use lower case for variable names, and all
upper case for symbolic constants.
At least the first 31 characters of an internal name are significant. For function names and
external variables, the number may be less than 31, because external names may be used by
assemblers and loaders over which the language has no control. For external names, the
standard guarantees uniqueness only for 6 characters and a single case. Keywords like if,
else, int, float, etc., are reserved: you can't use them as variable names. They must be in
lower case.
It's wise to choose variable names that are related to the purpose of the variable, and that are
unlikely to get mixed up typographically. We tend to use short names for local variables,
especially loop indices, and longer names for external variables.
2.2 Data Types and Sizes
There are only a few basic data types in C:
char a single byte, capable of holding one character in the local character set
int an integer, typically reflecting the natural size of integers on the host machine
float single-precision floating point
double double-precision floating point
In addition, there are a number of qualifiers that can be applied to these basic types. short and
long apply to integers:
 short int sh;
 long int counter;
The word int can be omitted in such declarations, and typically it is. 
36
The intent is that short and long should provide different lengths of integers where practical;
int will normally be the natural size for a particular machine. short is often 16 bits long, and
int either 16 or 32 bits. Each compiler is free to choose appropriate sizes for its own
hardware, subject only to the the restriction that shorts and ints are at least 16 bits, longs are
at least 32 bits, and short is no longer than int, which is no longer than long.
The qualifier signed or unsigned may be applied to char or any integer. unsigned numbers
are always positive or zero, and obey the laws of arithmetic modulo 2
n
, where n is the number
of bits in the type. So, for instance, if chars are 8 bits, unsigned char variables have values
between 0 and 255, while signed chars have values between -128 and 127 (in a two's
complement machine.) Whether plain chars are signed or unsigned is machine-dependent, but
printable characters are always positive.
The type long double specifies extended-precision floating point. As with integers, the sizes
of floating-point objects are implementation-defined; float, double and long double could
represent one, two or three distinct sizes.
The standard headers <limits.h> and <float.h> contain symbolic constants for all of these
sizes, along with other properties of the machine and compiler. These are discussed in
Appendix B.
Exercise 2-1. Write a program to determine the ranges of char, short, int, and long
variables, both signed and unsigned, by printing appropriate values from standard headers
and by direct computation. Harder if you compute them: determine the ranges of the various
floating-point types.
2.3 Constants
An integer constant like 1234 is an int. A long constant is written with a terminal l (ell) or L,
as in 123456789L; an integer constant too big to fit into an int will also be taken as a long.
Unsigned constants are written with a terminal u or U, and the suffix ul or UL indicates
unsigned long.
Floating-point constants contain a decimal point (123.4) or an exponent (1e-2) or both; their
type is double, unless suffixed. The suffixes f or F indicate a float constant; l or L indicate a
long double.
The value of an integer can be specified in octal or hexadecimal instead of decimal. A leading 0
(zero) on an integer constant means octal; a leading 0x or 0X means hexadecimal. For example,
decimal 31 can be written as 037 in octal and 0x1f or 0x1F in hex. Octal and hexadecimal
constants may also be followed by L to make them long and U to make them unsigned: 0XFUL
is an unsigned long constant with value 15 decimal.
A character constant is an integer, written as one character within single quotes, such as
'x'. The value of a character constant is the numeric value of the character in the machine's
character set. For example, in the ASCII character set the character constant '0' has the value
48, which is unrelated to the numeric value 0. If we write '0' instead of a numeric value like
48 that depends on the character set, the program is independent of the particular value and
easier to read. Character constants participate in numeric operations just as any other integers,
although they are most often used in comparisons with other characters.
Certain characters can be represented in character and string constants by escape sequences
like \n (newline); these sequences look like two characters, but represent only one. In addition,
an arbitrary byte-sized bit pattern can be specified by
 '\ooo'
37
where ooo is one to three octal digits (0...7) or by
 '\xhh'
where hh is one or more hexadecimal digits (0...9, a...f, A...F). So we might write
 #define VTAB '\013' /* ASCII vertical tab */
 #define BELL '\007' /* ASCII bell character */
or, in hexadecimal,
 #define VTAB '\xb' /* ASCII vertical tab */
 #define BELL '\x7' /* ASCII bell character */
The complete set of escape sequences is
\a alert (bell) character \\ backslash
\b backspace \? question mark
\f formfeed \' single quote
\n newline \" double quote
\r carriage return \ooo octal number
\t horizontal tab \xhh hexadecimal number
\v vertical tab
The character constant '\0' represents the character with value zero, the null character. '\0'
is often written instead of 0 to emphasize the character nature of some expression, but the
numeric value is just 0.
A constant expression is an expression that involves only constants. Such expressions may be
evaluated at during compilation rather than run-time, and accordingly may be used in any place
that a constant can occur, as in
 #define MAXLINE 1000
 char line[MAXLINE+1];
or
 #define LEAP 1 /* in leap years */
 int days[31+28+LEAP+31+30+31+30+31+31+30+31+30+31];
A string constant, or string literal, is a sequence of zero or more characters surrounded by
double quotes, as in
 "I am a string"
or
 "" /* the empty string */
The quotes are not part of the string, but serve only to delimit it. The same escape sequences
used in character constants apply in strings; \" represents the double-quote character. String
constants can be concatenated at compile time:
 "hello, " "world"
is equivalent to
 "hello, world"
This is useful for splitting up long strings across several source lines.
Technically, a string constant is an array of characters. The internal representation of a string
has a null character '\0' at the end, so the physical storage required is one more than the
number of characters written between the quotes. This representation means that there is no
limit to how long a string can be, but programs must scan a string completely to determine its
length. The standard library function strlen(s) returns the length of its character string
argument s, excluding the terminal '\0'. Here is our version: 
38
 /* strlen: return length of s */
 int strlen(char s[])
 {
 int i;
 while (s[i] != '\0')
 ++i;
 return i;
 }
strlen and other string functions are declared in the standard header <string.h>.
Be careful to distinguish between a character constant and a string that contains a single
character: 'x' is not the same as "x". The former is an integer, used to produce the numeric
value of the letter x in the machine's character set. The latter is an array of characters that
contains one character (the letter x) and a '\0'.
There is one other kind of constant, the enumeration constant. An enumeration is a list of
constant integer values, as in
 enum boolean { NO, YES };
The first name in an enum has value 0, the next 1, and so on, unless explicit values are
specified. If not all values are specified, unspecified values continue the progression from the
last specified value, as the second of these examples:
 enum escapes { BELL = '\a', BACKSPACE = '\b', TAB = '\t',
 NEWLINE = '\n', VTAB = '\v', RETURN = '\r' };
 enum months { JAN = 1, FEB, MAR, APR, MAY, JUN,
 JUL, AUG, SEP, OCT, NOV, DEC };
 /* FEB = 2, MAR = 3, etc. */
Names in different enumerations must be distinct. Values need not be distinct in the same
enumeration.
Enumerations provide a convenient way to associate constant values with names, an alternative
to #define with the advantage that the values can be generated for you. Although variables of
enum types may be declared, compilers need not check that what you store in such a variable is
a valid value for the enumeration. Nevertheless, enumeration variables offer the chance of
checking and so are often better than #defines. In addition, a debugger may be able to print
values of enumeration variables in their symbolic form.
2.4 Declarations
All variables must be declared before use, although certain declarations can be made implicitly
by content. A declaration specifies a type, and contains a list of one or more variables of that
type, as in
 int lower, upper, step;
 char c, line[1000];
Variables can be distributed among declarations in any fashion; the lists above could well be
written as
 int lower;
 int upper;
 int step;
 char c;
 char line[1000];
The latter form takes more space, but is convenient for adding a comment to each declaration
for subsequent modifications. 
39
A variable may also be initialized in its declaration. If the name is followed by an equals sign
and an expression, the expression serves as an initializer, as in
 char esc = '\\';
 int i = 0;
 int limit = MAXLINE+1;
 float eps = 1.0e-5;
If the variable in question is not automatic, the initialization is done once only, conceptionally
before the program starts executing, and the initializer must be a constant expression. An
explicitly initialized automatic variable is initialized each time the function or block it is in is
entered; the initializer may be any expression. External and static variables are initialized to
zero by default. Automatic variables for which is no explicit initializer have undefined (i.e.,
garbage) values.
The qualifier const can be applied to the declaration of any variable to specify that its value
will not be changed. For an array, the const qualifier says that the elements will not be altered.
 const double e = 2.71828182845905;
 const char msg[] = "warning: ";
The const declaration can also be used with array arguments, to indicate that the function
does not change that array:
 int strlen(const char[]);
The result is implementation-defined if an attempt is made to change a const.
2.5 Arithmetic Operators
The binary arithmetic operators are +, -, *, /, and the modulus operator %. Integer division
truncates any fractional part. The expression
 x % y
produces the remainder when x is divided by y, and thus is zero when y divides x exactly. For
example, a year is a leap year if it is divisible by 4 but not by 100, except that years divisible by
400 are leap years. Therefore
 if ((year % 4 == 0 && year % 100 != 0) || year % 400 == 0)
 printf("%d is a leap year\n", year);
 else
 printf("%d is not a leap year\n", year);
The % operator cannot be applied to a float or double. The direction of truncation for / and
the sign of the result for % are machine-dependent for negative operands, as is the action taken
on overflow or underflow.
The binary + and - operators have the same precedence, which is lower than the precedence of
*, / and %, which is in turn lower than unary + and -. Arithmetic operators associate left to
right.
Table 2.1 at the end of this chapter summarizes precedence and associativity for all operators.
2.6 Relational and Logical Operators
The relational operators are
 > >= < <=
They all have the same precedence. Just below them in precedence are the equality operators:
 == !=
Relational operators have lower precedence than arithmetic operators, so an expression like i
< lim-1 is taken as i < (lim-1), as would be expected. 
40
More interesting are the logical operators && and ||. Expressions connected by && or || are
evaluated left to right, and evaluation stops as soon as the truth or falsehood of the result is
known. Most C programs rely on these properties. For example, here is a loop from the input
function getline that we wrote in Chapter 1:
 for (i=0; i < lim-1 && (c=getchar()) != '\n' && c != EOF; ++i)
 s[i] = c;
Before reading a new character it is necessary to check that there is room to store it in the
array s, so the test i < lim-1 must be made first. Moreover, if this test fails, we must not go
on and read another character.
Similarly, it would be unfortunate if c were tested against EOF before getchar is called;
therefore the call and assignment must occur before the character in c is tested.
The precedence of && is higher than that of ||, and both are lower than relational and equality
operators, so expressions like
 i < lim-1 && (c=getchar()) != '\n' && c != EOF
need no extra parentheses. But since the precedence of != is higher than assignment,
parentheses are needed in
 (c=getchar()) != '\n'
to achieve the desired result of assignment to c and then comparison with '\n'.
By definition, the numeric value of a relational or logical expression is 1 if the relation is true,
and 0 if the relation is false.
The unary negation operator ! converts a non-zero operand into 0, and a zero operand in 1. A
common use of ! is in constructions like
 if (!valid)
rather than
 if (valid == 0)
It's hard to generalize about which form is better. Constructions like !valid read nicely (``if
not valid''), but more complicated ones can be hard to understand.
Exercise 2-2. Write a loop equivalent to the for loop above without using && or ||.
2.7 Type Conversions
When an operator has operands of different types, they are converted to a common type
according to a small number of rules. In general, the only automatic conversions are those that
convert a ``narrower'' operand into a ``wider'' one without losing information, such as
converting an integer into floating point in an expression like f + i. Expressions that don't
make sense, like using a float as a subscript, are disallowed. Expressions that might lose
information, like assigning a longer integer type to a shorter, or a floating-point type to an
integer, may draw a warning, but they are not illegal.
A char is just a small integer, so chars may be freely used in arithmetic expressions. This
permits considerable flexibility in certain kinds of character transformations. One is exemplified
by this naive implementation of the function atoi, which converts a string of digits into its
numeric equivalent.
 /* atoi: convert s to integer */
 int atoi(char s[])
 {
 int i, n;
41
 n = 0;
 for (i = 0; s[i] >= '0' && s[i] <= '9'; ++i)
 n = 10 * n + (s[i] - '0');
 return n;
 }
As we discussed in Chapter 1, the expression
 s[i] - '0'
gives the numeric value of the character stored in s[i], because the values of '0', '1', etc.,
form a contiguous increasing sequence.
Another example of char to int conversion is the function lower, which maps a single
character to lower case for the ASCII character set. If the character is not an upper case letter,
lower returns it unchanged.
 /* lower: convert c to lower case; ASCII only */
 int lower(int c)
 {
 if (c >= 'A' && c <= 'Z')
 return c + 'a' - 'A';
 else
 return c;
 }
This works for ASCII because corresponding upper case and lower case letters are a fixed
distance apart as numeric values and each alphabet is contiguous -- there is nothing but letters
between A and Z. This latter observation is not true of the EBCDIC character set, however, so
this code would convert more than just letters in EBCDIC.
The standard header <ctype.h>, described in Appendix B, defines a family of functions that
provide tests and conversions that are independent of character set. For example, the function
tolower is a portable replacement for the function lower shown above. Similarly, the test
 c >= '0' && c <= '9'
can be replaced by
 isdigit(c)
We will use the <ctype.h> functions from now on.
There is one subtle point about the conversion of characters to integers. The language does not
specify whether variables of type char are signed or unsigned quantities. When a char is
converted to an int, can it ever produce a negative integer? The answer varies from machine
to machine, reflecting differences in architecture. On some machines a char whose leftmost bit
is 1 will be converted to a negative integer (``sign extension''). On others, a char is promoted
to an int by adding zeros at the left end, and thus is always positive.
The definition of C guarantees that any character in the machine's standard printing character
set will never be negative, so these characters will always be positive quantities in expressions.
But arbitrary bit patterns stored in character variables may appear to be negative on some
machines, yet positive on others. For portability, specify signed or unsigned if non-character
data is to be stored in char variables.
Relational expressions like i > j and logical expressions connected by && and || are defined
to have value 1 if true, and 0 if false. Thus the assignment
 d = c >= '0' && c <= '9'
sets d to 1 if c is a digit, and 0 if not. However, functions like isdigit may return any nonzero value for true. In the test part of if, while, for, etc., ``true'' just means ``non-zero'', so
this makes no difference. 
42
Implicit arithmetic conversions work much as expected. In general, if an operator like + or *
that takes two operands (a binary operator) has operands of different types, the ``lower'' type is
promoted to the ``higher'' type before the operation proceeds. The result is of the integer type.
Section 6 of Appendix A states the conversion rules precisely. If there are no unsigned
operands, however, the following informal set of rules will suffice:
• If either operand is long double, convert the other to long double.
• Otherwise, if either operand is double, convert the other to double.
• Otherwise, if either operand is float, convert the other to float.
• Otherwise, convert char and short to int.
• Then, if either operand is long, convert the other to long.
Notice that floats in an expression are not automatically converted to double; this is a
change from the original definition. In general, mathematical functions like those in <math.h>
will use double precision. The main reason for using float is to save storage in large arrays,
or, less often, to save time on machines where double-precision arithmetic is particularly
expensive.
Conversion rules are more complicated when unsigned operands are involved. The problem is
that comparisons between signed and unsigned values are machine-dependent, because they
depend on the sizes of the various integer types. For example, suppose that int is 16 bits and
long is 32 bits. Then -1L < 1U, because 1U, which is an unsigned int, is promoted to a
signed long. But -1L > 1UL because -1L is promoted to unsigned long and thus appears
to be a large positive number.
Conversions take place across assignments; the value of the right side is converted to the type
of the left, which is the type of the result.
A character is converted to an integer, either by sign extension or not, as described above.
Longer integers are converted to shorter ones or to chars by dropping the excess high-order
bits. Thus in
 int i;
 char c;
 i = c;
 c = i;
the value of c is unchanged. This is true whether or not sign extension is involved. Reversing
the order of assignments might lose information, however.
If x is float and i is int, then x = i and i = x both cause conversions; float to int causes
truncation of any fractional part. When a double is converted to float, whether the value is
rounded or truncated is implementation dependent.
Since an argument of a function call is an expression, type conversion also takes place when
arguments are passed to functions. In the absence of a function prototype, char and short
become int, and float becomes double. This is why we have declared function arguments to
be int and double even when the function is called with char and float.
Finally, explicit type conversions can be forced (``coerced'') in any expression, with a unary
operator called a cast. In the construction
(type name) expression
43
the expression is converted to the named type by the conversion rules above. The precise
meaning of a cast is as if the expression were assigned to a variable of the specified type, which
is then used in place of the whole construction. For example, the library routine sqrt expects a
double argument, and will produce nonsense if inadvertently handled something else. (sqrt is
declared in <math.h>.) So if n is an integer, we can use
 sqrt((double) n)
to convert the value of n to double before passing it to sqrt. Note that the cast produces the
value of n in the proper type; n itself is not altered. The cast operator has the same high
precedence as other unary operators, as summarized in the table at the end of this chapter.
If arguments are declared by a function prototype, as the normally should be, the declaration
causes automatic coercion of any arguments when the function is called. Thus, given a function
prototype for sqrt:
 double sqrt(double)
the call
 root2 = sqrt(2)
coerces the integer 2 into the double value 2.0 without any need for a cast.
The standard library includes a portable implementation of a pseudo-random number generator
and a function for initializing the seed; the former illustrates a cast:
 unsigned long int next = 1;
 /* rand: return pseudo-random integer on 0..32767 */
 int rand(void)
 {
 next = next * 1103515245 + 12345;
 return (unsigned int)(next/65536) % 32768;
 }
 /* srand: set seed for rand() */
 void srand(unsigned int seed)
 {
 next = seed;
 }
Exercise 2-3. Write a function htoi(s), which converts a string of hexadecimal digits
(including an optional 0x or 0X) into its equivalent integer value. The allowable digits are 0
through 9, a through f, and A through F.
2.8 Increment and Decrement Operators
C provides two unusual operators for incrementing and decrementing variables. The increment
operator ++ adds 1 to its operand, while the decrement operator -- subtracts 1. We have
frequently used ++ to increment variables, as in
 if (c == '\n')
 ++nl;
The unusual aspect is that ++ and -- may be used either as prefix operators (before the
variable, as in ++n), or postfix operators (after the variable: n++). In both cases, the effect is to
increment n. But the expression ++n increments n before its value is used, while n++
increments n after its value has been used. This means that in a context where the value is
being used, not just the effect, ++n and n++ are different. If n is 5, then
 x = n++;
sets x to 5, but
 x = ++n;
44
sets x to 6. In both cases, n becomes 6. The increment and decrement operators can only be
applied to variables; an expression like (i+j)++ is illegal.
In a context where no value is wanted, just the incrementing effect, as in
 if (c == '\n')
 nl++;
prefix and postfix are the same. But there are situations where one or the other is specifically
called for. For instance, consider the function squeeze(s,c), which removes all occurrences
of the character c from the string s.
 /* squeeze: delete all c from s */
 void squeeze(char s[], int c)
 {
 int i, j;
 for (i = j = 0; s[i] != '\0'; i++)
 if (s[i] != c)
 s[j++] = s[i];
 s[j] = '\0';
 }
Each time a non-c occurs, it is copied into the current j position, and only then is j
incremented to be ready for the next character. This is exactly equivalent to
 if (s[i] != c) {
 s[j] = s[i];
 j++;
 }
Another example of a similar construction comes from the getline function that we wrote in
Chapter 1, where we can replace
 if (c == '\n') {
 s[i] = c;
 ++i;
 }
by the more compact
 if (c == '\n')
 s[i++] = c;
As a third example, consider the standard function strcat(s,t), which concatenates the
string t to the end of string s. strcat assumes that there is enough space in s to hold the
combination. As we have written it, strcat returns no value; the standard library version
returns a pointer to the resulting string.
 /* strcat: concatenate t to end of s; s must be big enough */
 void strcat(char s[], char t[])
 {
 int i, j;
 i = j = 0;
 while (s[i] != '\0') /* find end of s */
 i++;
 while ((s[i++] = t[j++]) != '\0') /* copy t */
 ;
 }
As each member is copied from t to s, the postfix ++ is applied to both i and j to make sure
that they are in position for the next pass through the loop.
Exercise 2-4. Write an alternative version of squeeze(s1,s2) that deletes each character in
s1 that matches any character in the string s2. 
45
Exercise 2-5. Write the function any(s1,s2), which returns the first location in a string s1
where any character from the string s2 occurs, or -1 if s1 contains no characters from s2.
(The standard library function strpbrk does the same job but returns a pointer to the
location.)
2.9 Bitwise Operators
C provides six operators for bit manipulation; these may only be applied to integral operands,
that is, char, short, int, and long, whether signed or unsigned.
& bitwise AND
| bitwise inclusive OR
^ bitwise exclusive OR
<< left shift
>> right shift
~ one's complement (unary)
The bitwise AND operator & is often used to mask off some set of bits, for example
 n = n & 0177;
sets to zero all but the low-order 7 bits of n.
The bitwise OR operator | is used to turn bits on:
 x = x | SET_ON;
sets to one in x the bits that are set to one in SET_ON.
The bitwise exclusive OR operator ^ sets a one in each bit position where its operands have
different bits, and zero where they are the same.
One must distinguish the bitwise operators & and | from the logical operators && and ||, which
imply left-to-right evaluation of a truth value. For example, if x is 1 and y is 2, then x & y is
zero while x && y is one.
The shift operators << and >> perform left and right shifts of their left operand by the number
of bit positions given by the right operand, which must be non-negative. Thus x << 2 shifts
the value of x by two positions, filling vacated bits with zero; this is equivalent to
multiplication by 4. Right shifting an unsigned quantity always fits the vacated bits with zero.
Right shifting a signed quantity will fill with bit signs (``arithmetic shift'') on some machines
and with 0-bits (``logical shift'') on others.
The unary operator ~ yields the one's complement of an integer; that is, it converts each 1-bit
into a 0-bit and vice versa. For example
 x = x & ~077
sets the last six bits of x to zero. Note that x & ~077 is independent of word length, and is
thus preferable to, for example, x & 0177700, which assumes that x is a 16-bit quantity. The
portable form involves no extra cost, since ~077 is a constant expression that can be evaluated
at compile time.
As an illustration of some of the bit operators, consider the function getbits(x,p,n) that
returns the (right adjusted) n-bit field of x that begins at position p. We assume that bit
position 0 is at the right end and that n and p are sensible positive values. For example,
getbits(x,4,3) returns the three bits in positions 4, 3 and 2, right-adjusted.
 /* getbits: get n bits from position p */
 unsigned getbits(unsigned x, int p, int n)
 {
46
 return (x >> (p+1-n)) & ~(~0 << n);
 }
The expression x >> (p+1-n) moves the desired field to the right end of the word. ~0 is all 1-
bits; shifting it left n positions with ~0<<n places zeros in the rightmost n bits; complementing
that with ~ makes a mask with ones in the rightmost n bits.
Exercise 2-6. Write a function setbits(x,p,n,y) that returns x with the n bits that begin at
position p set to the rightmost n bits of y, leaving the other bits unchanged.
Exercise 2-7. Write a function invert(x,p,n) that returns x with the n bits that begin at
position p inverted (i.e., 1 changed into 0 and vice versa), leaving the others unchanged.
Exercise 2-8. Write a function rightrot(x,n) that returns the value of the integer x rotated
to the right by n positions.
2.10 Assignment Operators and Expressions
An expression such as
 i = i + 2
in which the variable on the left side is repeated immediately on the right, can be written in the
compressed form
 i += 2
The operator += is called an assignment operator.
Most binary operators (operators like + that have a left and right operand) have a
corresponding assignment operator op=, where op is one of
 + - * / % << >> & ^ |
If expr1 and expr2 are expressions, then
 expr1 op= expr2
is equivalent to
 expr1 = (expr1) op (expr2)
except that expr1 is computed only once. Notice the parentheses around expr2:
 x *= y + 1
means
 x = x * (y + 1)
rather than
 x = x * y + 1
As an example, the function bitcount counts the number of 1-bits in its integer argument.
 /* bitcount: count 1 bits in x */
 int bitcount(unsigned x)
 {
 int b;
 for (b = 0; x != 0; x >>= 1)
 if (x & 01)
 b++;
 return b;
 }
Declaring the argument x to be an unsigned ensures that when it is right-shifted, vacated bits
will be filled with zeros, not sign bits, regardless of the machine the program is run on. 
47
Quite apart from conciseness, assignment operators have the advantage that they correspond
better to the way people think. We say ``add 2 to i'' or ``increment i by 2'', not ``take i, add 2,
then put the result back in i''. Thus the expression i += 2 is preferable to i = i+2. In
addition, for a complicated expression like
 yyval[yypv[p3+p4] + yypv[p1]] += 2
the assignment operator makes the code easier to understand, since the reader doesn't have to
check painstakingly that two long expressions are indeed the same, or to wonder why they're
not. And an assignment operator may even help a compiler to produce efficient code.
We have already seen that the assignment statement has a value and can occur in expressions;
the most common example is
 while ((c = getchar()) != EOF)
 ...
The other assignment operators (+=, -=, etc.) can also occur in expressions, although this is
less frequent.
In all such expressions, the type of an assignment expression is the type of its left operand, and
the value is the value after the assignment.
Exercise 2-9. In a two's complement number system, x &= (x-1) deletes the rightmost 1-bit
in x. Explain why. Use this observation to write a faster version of bitcount.
2.11 Conditional Expressions
The statements
 if (a > b)
 z = a;
 else
 z = b;
compute in z the maximum of a and b. The conditional expression, written with the ternary
operator ``?:'', provides an alternate way to write this and similar constructions. In the
expression
 expr1 ? expr2 : expr3
the expression expr1 is evaluated first. If it is non-zero (true), then the expression expr2 is
evaluated, and that is the value of the conditional expression. Otherwise expr3 is evaluated, and
that is the value. Only one of expr2 and expr3 is evaluated. Thus to set z to the maximum of a
and b,
 z = (a > b) ? a : b; /* z = max(a, b) */
It should be noted that the conditional expression is indeed an expression, and it can be used
wherever any other expression can be. If expr2 and expr3 are of different types, the type of the
result is determined by the conversion rules discussed earlier in this chapter. For example, if f
is a float and n an int, then the expression
 (n > 0) ? f : n
is of type float regardless of whether n is positive.
Parentheses are not necessary around the first expression of a conditional expression, since the
precedence of ?: is very low, just above assignment. They are advisable anyway, however,
since they make the condition part of the expression easier to see.
The conditional expression often leads to succinct code. For example, this loop prints n
elements of an array, 10 per line, with each column separated by one blank, and with each line
(including the last) terminated by a newline. 
48
 for (i = 0; i < n; i++)
 printf("%6d%c", a[i], (i%10==9 || i==n-1) ? '\n' : ' ');
A newline is printed after every tenth element, and after the n-th. All other elements are
followed by one blank. This might look tricky, but it's more compact than the equivalent ifelse. Another good example is
 printf("You have %d items%s.\n", n, n==1 ? "" : "s");
Exercise 2-10. Rewrite the function lower, which converts upper case letters to lower case,
with a conditional expression instead of if-else.
2.12 Precedence and Order of Evaluation
Table 2.1 summarizes the rules for precedence and associativity of all operators, including
those that we have not yet discussed. Operators on the same line have the same precedence;
rows are in order of decreasing precedence, so, for example, *, /, and % all have the same
precedence, which is higher than that of binary + and -. The ``operator'' () refers to function
call. The operators -> and . are used to access members of structures; they will be covered in
Chapter 6, along with sizeof (size of an object). Chapter 5 discusses * (indirection through a
pointer) and & (address of an object), and Chapter 3 discusses the comma operator.
Operators Associativity
() [] -> . left to right
! ~ ++ -- + - * (type) sizeof right to left
* / % left to right
+ - left to right
<< >> left to right
< <= > >= left to right
== != left to right
& left to right
^ left to right
| left to right
&& left to right
|| left to right
?: right to left
= += -= *= /= %= &= ^= |= <<= >>= right to left
, left to right
Unary & +, -, and * have higher precedence than the binary forms.
Table 2.1: Precedence and Associativity of Operators
Note that the precedence of the bitwise operators &, ^, and | falls below == and !=. This
implies that bit-testing expressions like
 if ((x & MASK) == 0) ...
must be fully parenthesized to give proper results.
C, like most languages, does not specify the order in which the operands of an operator are
evaluated. (The exceptions are &&, ||, ?:, and `,'.) For example, in a statement like
 x = f() + g();
49
f may be evaluated before g or vice versa; thus if either f or g alters a variable on which the
other depends, x can depend on the order of evaluation. Intermediate results can be stored in
temporary variables to ensure a particular sequence.
Similarly, the order in which function arguments are evaluated is not specified, so the
statement
 printf("%d %d\n", ++n, power(2, n)); /* WRONG */
can produce different results with different compilers, depending on whether n is incremented
before power is called. The solution, of course, is to write
 ++n;
 printf("%d %d\n", n, power(2, n));
Function calls, nested assignment statements, and increment and decrement operators cause
``side effects'' - some variable is changed as a by-product of the evaluation of an expression. In
any expression involving side effects, there can be subtle dependencies on the order in which
variables taking part in the expression are updated. One unhappy situation is typified by the
statement
 a[i] = i++;
The question is whether the subscript is the old value of i or the new. Compilers can interpret
this in different ways, and generate different answers depending on their interpretation. The
standard intentionally leaves most such matters unspecified. When side effects (assignment to
variables) take place within an expression is left to the discretion of the compiler, since the best
order depends strongly on machine architecture. (The standard does specify that all side effects
on arguments take effect before a function is called, but that would not help in the call to
printf above.)
The moral is that writing code that depends on order of evaluation is a bad programming
practice in any language. Naturally, it is necessary to know what things to avoid, but if you
don't know how they are done on various machines, you won't be tempted to take advantage of
a particular implementation. 
50
Chapter 3 - Control Flow
The control-flow of a language specify the order in which computations are performed. We
have already met the most common control-flow constructions in earlier examples; here we
will complete the set, and be more precise about the ones discussed before.
3.1 Statements and Blocks
An expression such as x = 0 or i++ or printf(...) becomes a statement when it is followed
by a semicolon, as in
 x = 0;
 i++;
 printf(...);
In C, the semicolon is a statement terminator, rather than a separator as it is in languages like
Pascal.
Braces { and } are used to group declarations and statements together into a compound
statement, or block, so that they are syntactically equivalent to a single statement. The braces
that surround the statements of a function are one obvious example; braces around multiple
statements after an if, else, while, or for are another. (Variables can be declared inside any
block; we will talk about this in Chapter 4.) There is no semicolon after the right brace that
ends a block.
3.2 If-Else
The if-else statement is used to express decisions. Formally the syntax is
 if (expression)
 statement1
 else
 statement2
where the else part is optional. The expression is evaluated; if it is true (that is, if expression
has a non-zero value), statement1 is executed. If it is false (expression is zero) and if there is an
else part, statement2 is executed instead.
Since an if tests the numeric value of an expression, certain coding shortcuts are possible. The
most obvious is writing
 if (expression)
instead of
 if (expression != 0)
Sometimes this is natural and clear; at other times it can be cryptic.
Because the else part of an if-else is optional,there is an ambiguity when an else if omitted
from a nested if sequence. This is resolved by associating the else with the closest previous
else-less if. For example, in
 if (n > 0)
 if (a > b)
 z = a;
 else
 z = b;
the else goes to the inner if, as we have shown by indentation. If that isn't what you want,
braces must be used to force the proper association: 
51
 if (n > 0) {
 if (a > b)
 z = a;
 }
 else
 z = b;
The ambiguity is especially pernicious in situations like this:
 if (n > 0)
 for (i = 0; i < n; i++)
 if (s[i] > 0) {
 printf("...");
 return i;
 }
 else /* WRONG */
 printf("error -- n is negative\n");
The indentation shows unequivocally what you want, but the compiler doesn't get the message,
and associates the else with the inner if. This kind of bug can be hard to find; it's a good idea
to use braces when there are nested ifs.
By the way, notice that there is a semicolon after z = a in
 if (a > b)
 z = a;
 else
 z = b;
This is because grammatically, a statement follows the if, and an expression statement like ``z
= a;'' is always terminated by a semicolon.
3.3 Else-If
The construction
 if (expression)
 statement
 else if (expression)
 statement
 else if (expression)
 statement
 else if (expression)
 statement
 else
 statement
occurs so often that it is worth a brief separate discussion. This sequence of if statements is
the most general way of writing a multi-way decision. The expressions are evaluated in order;
if an expression is true, the statement associated with it is executed, and this terminates the
whole chain. As always, the code for each statement is either a single statement, or a group of
them in braces.
The last else part handles the ``none of the above'' or default case where none of the other
conditions is satisfied. Sometimes there is no explicit action for the default; in that case the
trailing
 else
 statement
can be omitted, or it may be used for error checking to catch an ``impossible'' condition.
To illustrate a three-way decision, here is a binary search function that decides if a particular
value x occurs in the sorted array v. The elements of v must be in increasing order. The
function returns the position (a number between 0 and n-1) if x occurs in v, and -1 if not. 
52
Binary search first compares the input value x to the middle element of the array v. If x is less
than the middle value, searching focuses on the lower half of the table, otherwise on the upper
half. In either case, the next step is to compare x to the middle element of the selected half.
This process of dividing the range in two continues until the value is found or the range is
empty.
 /* binsearch: find x in v[0] <= v[1] <= ... <= v[n-1] */
 int binsearch(int x, int v[], int n)
 {
 int low, high, mid;
 low = 0;
 high = n - 1;
 while (low <= high) {
 mid = (low+high)/2;
 if (x < v[mid])
 high = mid + 1;
 else if (x > v[mid])
 low = mid + 1;
 else /* found match */
 return mid;
 }
 return -1; /* no match */
 }
The fundamental decision is whether x is less than, greater than, or equal to the middle element
v[mid] at each step; this is a natural for else-if.
Exercise 3-1. Our binary search makes two tests inside the loop, when one would suffice (at
the price of more tests outside.) Write a version with only one test inside the loop and measure
the difference in run-time.
3.4 Switch
The switch statement is a multi-way decision that tests whether an expression matches one of
a number of constant integer values, and branches accordingly.
 switch (expression) {
 case const-expr: statements
 case const-expr: statements
 default: statements
 }
Each case is labeled by one or more integer-valued constants or constant expressions. If a case
matches the expression value, execution starts at that case. All case expressions must be
different. The case labeled default is executed if none of the other cases are satisfied. A
default is optional; if it isn't there and if none of the cases match, no action at all takes place.
Cases and the default clause can occur in any order.
In Chapter 1 we wrote a program to count the occurrences of each digit, white space, and all
other characters, using a sequence of if ... else if ... else. Here is the same program
with a switch:
 #include <stdio.h>
 main() /* count digits, white space, others */
 {
 int c, i, nwhite, nother, ndigit[10];
 nwhite = nother = 0;
 for (i = 0; i < 10; i++)
 ndigit[i] = 0;
 while ((c = getchar()) != EOF) {
53
 switch (c) {
 case '0': case '1': case '2': case '3': case '4':
 case '5': case '6': case '7': case '8': case '9':
 ndigit[c-'0']++;
 break;
 case ' ':
 case '\n':
 case '\t':
 nwhite++;
 break;
 default:
 nother++;
 break;
 }
 }
 printf("digits =");
 for (i = 0; i < 10; i++)
 printf(" %d", ndigit[i]);
 printf(", white space = %d, other = %d\n",
 nwhite, nother);
 return 0;
 }
The break statement causes an immediate exit from the switch. Because cases serve just as
labels, after the code for one case is done, execution falls through to the next unless you take
explicit action to escape. break and return are the most common ways to leave a switch. A
break statement can also be used to force an immediate exit from while, for, and do loops,
as will be discussed later in this chapter.
Falling through cases is a mixed blessing. On the positive side, it allows several cases to be
attached to a single action, as with the digits in this example. But it also implies that normally
each case must end with a break to prevent falling through to the next. Falling through from
one case to another is not robust, being prone to disintegration when the program is modified.
With the exception of multiple labels for a single computation, fall-throughs should be used
sparingly, and commented.
As a matter of good form, put a break after the last case (the default here) even though it's
logically unnecessary. Some day when another case gets added at the end, this bit of defensive
programming will save you.
Exercise 3-2. Write a function escape(s,t) that converts characters like newline and tab into
visible escape sequences like \n and \t as it copies the string t to s. Use a switch. Write a
function for the other direction as well, converting escape sequences into the real characters.
3.5 Loops - While and For
We have already encountered the while and for loops. In
 while (expression)
 statement
the expression is evaluated. If it is non-zero, statement is executed and expression is reevaluated. This cycle continues until expression becomes zero, at which point execution
resumes after statement.
The for statement
 for (expr1; expr2; expr3)
 statement
is equivalent to
 expr1;
 while (expr2) {
54
 statement
 expr3;
 }
except for the behaviour of continue, which is described in Section 3.7.
Grammatically, the three components of a for loop are expressions. Most commonly, expr1
and expr3 are assignments or function calls and expr2 is a relational expression. Any of the
three parts can be omitted, although the semicolons must remain. If expr1 or expr3 is omitted, it
is simply dropped from the expansion. If the test, expr2, is not present, it is taken as
permanently true, so
 for (;;) {
 ...
 }
is an ``infinite'' loop, presumably to be broken by other means, such as a break or return.
Whether to use while or for is largely a matter of personal preference. For example, in
 while ((c = getchar()) == ' ' || c == '\n' || c = '\t')
 ; /* skip white space characters */
there is no initialization or re-initialization, so the while is most natural.
The for is preferable when there is a simple initialization and increment since it keeps the loop
control statements close together and visible at the top of the loop. This is most obvious in
 for (i = 0; i < n; i++)
 ...
which is the C idiom for processing the first n elements of an array, the analog of the Fortran
DO loop or the Pascal for. The analogy is not perfect, however, since the index variable i
retains its value when the loop terminates for any reason. Because the components of the for
are arbitrary expressions, for loops are not restricted to arithmetic progressions. Nonetheless,
it is bad style to force unrelated computations into the initialization and increment of a for,
which are better reserved for loop control operations.
As a larger example, here is another version of atoi for converting a string to its numeric
equivalent. This one is slightly more general than the one in Chapter 2; it copes with optional
leading white space and an optional + or - sign. (Chapter 4 shows atof, which does the same
conversion for floating-point numbers.)
The structure of the program reflects the form of the input:
skip white space, if any
get sign, if any
get integer part and convert it
Each step does its part, and leaves things in a clean state for the next. The whole process
terminates on the first character that could not be part of a number.
 #include <ctype.h>
 /* atoi: convert s to integer; version 2 */
 int atoi(char s[])
 {
 int i, n, sign;
 for (i = 0; isspace(s[i]); i++) /* skip white space */
 ;
 sign = (s[i] == '-') ? -1 : 1;
 if (s[i] == '+' || s[i] == '-') /* skip sign */
 i++;
55
 for (n = 0; isdigit(s[i]); i++)
 n = 10 * n + (s[i] - '0');
 return sign * n;
 }
The standard library provides a more elaborate function strtol for conversion of strings to
long integers; see Section 5 of Appendix B.
The advantages of keeping loop control centralized are even more obvious when there are
several nested loops. The following function is a Shell sort for sorting an array of integers. The
basic idea of this sorting algorithm, which was invented in 1959 by D. L. Shell, is that in early
stages, far-apart elements are compared, rather than adjacent ones as in simpler interchange
sorts. This tends to eliminate large amounts of disorder quickly, so later stages have less work
to do. The interval between compared elements is gradually decreased to one, at which point
the sort effectively becomes an adjacent interchange method.
 /* shellsort: sort v[0]...v[n-1] into increasing order */
 void shellsort(int v[], int n)
 {
 int gap, i, j, temp;
 for (gap = n/2; gap > 0; gap /= 2)
 for (i = gap; i < n; i++)
 for (j=i-gap; j>=0 && v[j]>v[j+gap]; j-=gap) {
 temp = v[j];
 v[j] = v[j+gap];
 v[j+gap] = temp;
 }
 }
There are three nested loops. The outermost controls the gap between compared elements,
shrinking it from n/2 by a factor of two each pass until it becomes zero. The middle loop steps
along the elements. The innermost loop compares each pair of elements that is separated by
gap and reverses any that are out of order. Since gap is eventually reduced to one, all elements
are eventually ordered correctly. Notice how the generality of the for makes the outer loop fit
in the same form as the others, even though it is not an arithmetic progression.
One final C operator is the comma ``,'', which most often finds use in the for statement. A
pair of expressions separated by a comma is evaluated left to right, and the type and value of
the result are the type and value of the right operand. Thus in a for statement, it is possible to
place multiple expressions in the various parts, for example to process two indices in parallel.
This is illustrated in the function reverse(s), which reverses the string s in place.
 #include <string.h>
 /* reverse: reverse string s in place */
 void reverse(char s[])
 {
 int c, i, j;
 for (i = 0, j = strlen(s)-1; i < j; i++, j--) {
 c = s[i];
 s[i] = s[j];
 s[j] = c;
 }
 }
The commas that separate function arguments, variables in declarations, etc., are not comma
operators, and do not guarantee left to right evaluation.
Comma operators should be used sparingly. The most suitable uses are for constructs strongly
related to each other, as in the for loop in reverse, and in macros where a multistep
computation has to be a single expression. A comma expression might also be appropriate for
56
the exchange of elements in reverse, where the exchange can be thought of a single
operation:
 for (i = 0, j = strlen(s)-1; i < j; i++, j--)
 c = s[i], s[i] = s[j], s[j] = c;
Exercise 3-3. Write a function expand(s1,s2) that expands shorthand notations like a-z in
the string s1 into the equivalent complete list abc...xyz in s2. Allow for letters of either case
and digits, and be prepared to handle cases like a-b-c and a-z0-9 and -a-z. Arrange that a
leading or trailing - is taken literally.
3.6 Loops - Do-While
As we discussed in Chapter 1, the while and for loops test the termination condition at the
top. By contrast, the third loop in C, the do-while, tests at the bottom after making each pass
through the loop body; the body is always executed at least once.
The syntax of the do is
 do
 statement
 while (expression);
The statement is executed, then expression is evaluated. If it is true, statement is evaluated
again, and so on. When the expression becomes false, the loop terminates. Except for the sense
of the test, do-while is equivalent to the Pascal repeat-until statement.
Experience shows that do-while is much less used than while and for. Nonetheless, from
time to time it is valuable, as in the following function itoa, which converts a number to a
character string (the inverse of atoi). The job is slightly more complicated than might be
thought at first, because the easy methods of generating the digits generate them in the wrong
order. We have chosen to generate the string backwards, then reverse it.
 /* itoa: convert n to characters in s */
 void itoa(int n, char s[])
 {
 int i, sign;
 if ((sign = n) < 0) /* record sign */
 n = -n; /* make n positive */
 i = 0;
 do { /* generate digits in reverse order */
 s[i++] = n % 10 + '0'; /* get next digit */
 } while ((n /= 10) > 0); /* delete it */
 if (sign < 0)
 s[i++] = '-';
 s[i] = '\0';
 reverse(s);
 }
The do-while is necessary, or at least convenient, since at least one character must be
installed in the array s, even if n is zero. We also used braces around the single statement that
makes up the body of the do-while, even though they are unnecessary, so the hasty reader
will not mistake the while part for the beginning of a while loop.
Exercise 3-4. In a two's complement number representation, our version of itoa does not
handle the largest negative number, that is, the value of n equal to -(2wordsize-1). Explain why not.
Modify it to print that value correctly, regardless of the machine on which it runs.
Exercise 3-5. Write the function itob(n,s,b) that converts the integer n into a base b
character representation in the string s. In particular, itob(n,s,16) formats s as a
hexadecimal integer in s. 
57
Exercise 3-6. Write a version of itoa that accepts three arguments instead of two. The third
argument is a minimum field width; the converted number must be padded with blanks on the
left if necessary to make it wide enough.
3.7 Break and Continue
It is sometimes convenient to be able to exit from a loop other than by testing at the top or
bottom. The break statement provides an early exit from for, while, and do, just as from
switch. A break causes the innermost enclosing loop or switch to be exited immediately.
The following function, trim, removes trailing blanks, tabs and newlines from the end of a
string, using a break to exit from a loop when the rightmost non-blank, non-tab, non-newline
is found.
 /* trim: remove trailing blanks, tabs, newlines */
 int trim(char s[])
 {
 int n;
 for (n = strlen(s)-1; n >= 0; n--)
 if (s[n] != ' ' && s[n] != '\t' && s[n] != '\n')
 break;
 s[n+1] = '\0';
 return n;
 }
strlen returns the length of the string. The for loop starts at the end and scans backwards
looking for the first character that is not a blank or tab or newline. The loop is broken when
one is found, or when n becomes negative (that is, when the entire string has been scanned).
You should verify that this is correct behavior even when the string is empty or contains only
white space characters.
The continue statement is related to break, but less often used; it causes the next iteration of
the enclosing for, while, or do loop to begin. In the while and do, this means that the test
part is executed immediately; in the for, control passes to the increment step. The continue
statement applies only to loops, not to switch. A continue inside a switch inside a loop
causes the next loop iteration.
As an example, this fragment processes only the non-negative elements in the array a; negative
values are skipped.
 for (i = 0; i < n; i++)
 if (a[i] < 0) /* skip negative elements */
 continue;
 ... /* do positive elements */
The continue statement is often used when the part of the loop that follows is complicated, so
that reversing a test and indenting another level would nest the program too deeply.
3.8 Goto and labels
C provides the infinitely-abusable goto statement, and labels to branch to. Formally, the goto
statement is never necessary, and in practice it is almost always easy to write code without it.
We have not used goto in this book.
Nevertheless, there are a few situations where gotos may find a place. The most common is to
abandon processing in some deeply nested structure, such as breaking out of two or more
loops at once. The break statement cannot be used directly since it only exits from the
innermost loop. Thus:
 for ( ... )
58
 for ( ... ) {
 ...
 if (disaster)
 goto error;
 }
 ...
 error:
 /* clean up the mess */
This organization is handy if the error-handling code is non-trivial, and if errors can occur in
several places.
A label has the same form as a variable name, and is followed by a colon. It can be attached to
any statement in the same function as the goto. The scope of a label is the entire function.
As another example, consider the problem of determining whether two arrays a and b have an
element in common. One possibility is
 for (i = 0; i < n; i++)
 for (j = 0; j < m; j++)
 if (a[i] == b[j])
 goto found;
 /* didn't find any common element */
 ...
 found:
 /* got one: a[i] == b[j] */
 ...
Code involving a goto can always be written without one, though perhaps at the price of some
repeated tests or an extra variable. For example, the array search becomes
 found = 0;
 for (i = 0; i < n && !found; i++)
 for (j = 0; j < m && !found; j++)
 if (a[i] == b[j])
 found = 1;
 if (found)
 /* got one: a[i-1] == b[j-1] */
 ...
 else
 /* didn't find any common element */
 ...
With a few exceptions like those cited here, code that relies on goto statements is generally
harder to understand and to maintain than code without gotos. Although we are not dogmatic
about the matter, it does seem that goto statements should be used rarely, if at all. 
59
Chapter 4 - Functions and Program
Structure
Functions break large computing tasks into smaller ones, and enable people to build on what
others have done instead of starting over from scratch. Appropriate functions hide details of
operation from parts of the program that don't need to know about them, thus clarifying the
whole, and easing the pain of making changes.
C has been designed to make functions efficient and easy to use; C programs generally consist
of many small functions rather than a few big ones. A program may reside in one or more
source files. Source files may be compiled separately and loaded together, along with
previously compiled functions from libraries. We will not go into that process here, however,
since the details vary from system to system.
Function declaration and definition is the area where the ANSI standard has made the most
changes to C. As we saw first in Chapter 1, it is now possible to declare the type of arguments
when a function is declared. The syntax of function declaration also changes, so that
declarations and definitions match. This makes it possible for a compiler to detect many more
errors than it could before. Furthermore, when arguments are properly declared, appropriate
type coercions are performed automatically.
The standard clarifies the rules on the scope of names; in particular, it requires that there be
only one definition of each external object. Initialization is more general: automatic arrays and
structures may now be initialized.
The C preprocessor has also been enhanced. New preprocessor facilities include a more
complete set of conditional compilation directives, a way to create quoted strings from macro
arguments, and better control over the macro expansion process.
4.1 Basics of Functions
To begin with, let us design and write a program to print each line of its input that contains a
particular ``pattern'' or string of characters. (This is a special case of the UNIX program grep.)
For example, searching for the pattern of letters ``ould'' in the set of lines
 Ah Love! could you and I with Fate conspire
 To grasp this sorry Scheme of Things entire,
 Would not we shatter it to bits -- and then
 Re-mould it nearer to the Heart's Desire!
will produce the output
 Ah Love! could you and I with Fate conspire
 Would not we shatter it to bits -- and then
 Re-mould it nearer to the Heart's Desire!
The job falls neatly into three pieces:
while (there's another line)
 if (the line contains the pattern)
 print it
Although it's certainly possible to put the code for all of this in main, a better way is to use the
structure to advantage by making each part a separate function. Three small pieces are better
to deal with than one big one, because irrelevant details can be buried in the functions, and the
chance of unwanted interactions is minimized. And the pieces may even be useful in other
programs. 
60
``While there's another line'' is getline, a function that we wrote in Chapter 1, and ``print it'' is
printf, which someone has already provided for us. This means we need only write a routine
to decide whether the line contains an occurrence of the pattern.
We can solve that problem by writing a function strindex(s,t) that returns the position or
index in the string s where the string t begins, or -1 if s does not contain t. Because C arrays
begin at position zero, indexes will be zero or positive, and so a negative value like -1 is
convenient for signaling failure. When we later need more sophisticated pattern matching, we
only have to replace strindex; the rest of the code can remain the same. (The standard library
provides a function strstr that is similar to strindex, except that it returns a pointer instead
of an index.)
Given this much design, filling in the details of the program is straightforward. Here is the
whole thing, so you can see how the pieces fit together. For now, the pattern to be searched
for is a literal string, which is not the most general of mechanisms. We will return shortly to a
discussion of how to initialize character arrays, and in Chapter 5 will show how to make the
pattern a parameter that is set when the program is run. There is also a slightly different
version of getline; you might find it instructive to compare it to the one in Chapter 1.
 #include <stdio.h>
 #define MAXLINE 1000 /* maximum input line length */
 int getline(char line[], int max)
 int strindex(char source[], char searchfor[]);
 char pattern[] = "ould"; /* pattern to search for */
 /* find all lines matching pattern */
 main()
 {
 char line[MAXLINE];
 int found = 0;
 while (getline(line, MAXLINE) > 0)
 if (strindex(line, pattern) >= 0) {
 printf("%s", line);
 found++;
 }
 return found;
 }
 /* getline: get line into s, return length */
 int getline(char s[], int lim)
 {
 int c, i;
 i = 0;
 while (--lim > 0 && (c=getchar()) != EOF && c != '\n')
 s[i++] = c;
 if (c == '\n')
 s[i++] = c;
 s[i] = '\0';
 return i;
 }
 /* strindex: return index of t in s, -1 if none */
 int strindex(char s[], char t[])
 {
 int i, j, k;
 for (i = 0; s[i] != '\0'; i++) {
 for (j=i, k=0; t[k]!='\0' && s[j]==t[k]; j++, k++)
61
 ;
 if (k > 0 && t[k] == '\0')
 return i;
 }
 return -1;
 }
Each function definition has the form
return-type function-name(argument declarations)
{
 declarations and statements
}
Various parts may be absent; a minimal function is
 dummy() {}
which does nothing and returns nothing. A do-nothing function like this is sometimes useful as
a place holder during program development. If the return type is omitted, int is assumed.
A program is just a set of definitions of variables and functions. Communication between the
functions is by arguments and values returned by the functions, and through external variables.
The functions can occur in any order in the source file, and the source program can be split
into multiple files, so long as no function is split.
The return statement is the mechanism for returning a value from the called function to its
caller. Any expression can follow return:
 return expression;
The expression will be converted to the return type of the function if necessary. Parentheses
are often used around the expression, but they are optional.
The calling function is free to ignore the returned value. Furthermore, there need to be no
expression after return; in that case, no value is returned to the caller. Control also returns to
the caller with no value when execution ``falls off the end'' of the function by reaching the
closing right brace. It is not illegal, but probably a sign of trouble, if a function returns a value
from one place and no value from another. In any case, if a function fails to return a value, its
``value'' is certain to be garbage.
The pattern-searching program returns a status from main, the number of matches found. This
value is available for use by the environment that called the program
The mechanics of how to compile and load a C program that resides on multiple source files
vary from one system to the next. On the UNIX system, for example, the cc command
mentioned in Chapter 1 does the job. Suppose that the three functions are stored in three files
called main.c, getline.c, and strindex.c. Then the command
 cc main.c getline.c strindex.c
compiles the three files, placing the resulting object code in files main.o, getline.o, and
strindex.o, then loads them all into an executable file called a.out. If there is an error, say in
main.c, the file can be recompiled by itself and the result loaded with the previous object files,
with the command
 cc main.c getline.o strindex.o
The cc command uses the ``.c'' versus ``.o'' naming convention to distinguish source files
from object files.
Exercise 4-1. Write the function strindex(s,t) which returns the position of the rightmost
occurrence of t in s, or -1 if there is none. 
62
4.2 Functions Returning Non-integers
So far our examples of functions have returned either no value (void) or an int. What if a
function must return some other type? many numerical functions like sqrt, sin, and cos
return double; other specialized functions return other types. To illustrate how to deal with
this, let us write and use the function atof(s), which converts the string s to its doubleprecision floating-point equivalent. atof if an extension of atoi, which we showed versions of
in Chapters 2 and 3. It handles an optional sign and decimal point, and the presence or absence
of either part or fractional part. Our version is not a high-quality input conversion routine; that
would take more space than we care to use. The standard library includes an atof; the header
<stdlib.h> declares it.
First, atof itself must declare the type of value it returns, since it is not int. The type name
precedes the function name:
 #include <ctype.h>
 /* atof: convert string s to double */
 double atof(char s[])
 {
 double val, power;
 int i, sign;
 for (i = 0; isspace(s[i]); i++) /* skip white space */
 ;
 sign = (s[i] == '-') ? -1 : 1;
 if (s[i] == '+' || s[i] == '-')
 i++;
 for (val = 0.0; isdigit(s[i]); i++)
 val = 10.0 * val + (s[i] - '0');
 if (s[i] == '.')
 i++;
 for (power = 1.0; isdigit(s[i]); i++) {
 val = 10.0 * val + (s[i] - '0');
 power *= 10;
 }
 return sign * val / power;
 }
Second, and just as important, the calling routine must know that atof returns a non-int value.
One way to ensure this is to declare atof explicitly in the calling routine. The declaration is
shown in this primitive calculator (barely adequate for check-book balancing), which reads one
number per line, optionally preceded with a sign, and adds them up, printing the running sum
after each input:
 #include <stdio.h>
 #define MAXLINE 100
 /* rudimentary calculator */
 main()
 {
 double sum, atof(char []);
 char line[MAXLINE];
 int getline(char line[], int max);
 sum = 0;
 while (getline(line, MAXLINE) > 0)
 printf("\t%g\n", sum += atof(line));
 return 0;
 }
The declaration 
63
 double sum, atof(char []);
says that sum is a double variable, and that atof is a function that takes one char[] argument
and returns a double.
The function atof must be declared and defined consistently. If atof itself and the call to it in
main have inconsistent types in the same source file, the error will be detected by the compiler.
But if (as is more likely) atof were compiled separately, the mismatch would not be detected,
atof would return a double that main would treat as an int, and meaningless answers would
result.
In the light of what we have said about how declarations must match definitions, this might
seem surprising. The reason a mismatch can happen is that if there is no function prototype, a
function is implicitly declared by its first appearance in an expression, such as
 sum += atof(line)
If a name that has not been previously declared occurs in an expression and is followed by a
left parentheses, it is declared by context to be a function name, the function is assumed to
return an int, and nothing is assumed about its arguments. Furthermore, if a function
declaration does not include arguments, as in
 double atof();
that too is taken to mean that nothing is to be assumed about the arguments of atof; all
parameter checking is turned off. This special meaning of the empty argument list is intended
to permit older C programs to compile with new compilers. But it's a bad idea to use it with
new C programs. If the function takes arguments, declare them; if it takes no arguments, use
void.
Given atof, properly declared, we could write atoi (convert a string to int) in terms of it:
 /* atoi: convert string s to integer using atof */
 int atoi(char s[])
 {
 double atof(char s[]);
 return (int) atof(s);
 }
Notice the structure of the declarations and the return statement. The value of the expression
in
 return expression;
is converted to the type of the function before the return is taken. Therefore, the value of atof,
a double, is converted automatically to int when it appears in this return, since the function
atoi returns an int. This operation does potentionally discard information, however, so some
compilers warn of it. The cast states explicitly that the operation is intended, and suppresses
any warning.
Exercise 4-2. Extend atof to handle scientific notation of the form
 123.45e-6
where a floating-point number may be followed by e or E and an optionally signed exponent.
4.3 External Variables
A C program consists of a set of external objects, which are either variables or functions. The
adjective ``external'' is used in contrast to ``internal'', which describes the arguments and
variables defined inside functions. External variables are defined outside of any function, and
are thus potentionally available to many functions. Functions themselves are always external,
because C does not allow functions to be defined inside other functions. By default, external
64
variables and functions have the property that all references to them by the same name, even
from functions compiled separately, are references to the same thing. (The standard calls this
property external linkage.) In this sense, external variables are analogous to Fortran
COMMON blocks or variables in the outermost block in Pascal. We will see later how to
define external variables and functions that are visible only within a single source file. Because
external variables are globally accessible, they provide an alternative to function arguments and
return values for communicating data between functions. Any function may access an external
variable by referring to it by name, if the name has been declared somehow.
If a large number of variables must be shared among functions, external variables are more
convenient and efficient than long argument lists. As pointed out in Chapter 1, however, this
reasoning should be applied with some caution, for it can have a bad effect on program
structure, and lead to programs with too many data connections between functions.
External variables are also useful because of their greater scope and lifetime. Automatic
variables are internal to a function; they come into existence when the function is entered, and
disappear when it is left. External variables, on the other hand, are permanent, so they can
retain values from one function invocation to the next. Thus if two functions must share some
data, yet neither calls the other, it is often most convenient if the shared data is kept in external
variables rather than being passed in and out via arguments.
Let us examine this issue with a larger example. The problem is to write a calculator program
that provides the operators +, -, * and /. Because it is easier to implement, the calculator will
use reverse Polish notation instead of infix. (Reverse Polish notation is used by some pocket
calculators, and in languages like Forth and Postscript.)
In reverse Polish notation, each operator follows its operands; an infix expression like
 (1 - 2) * (4 + 5)
is entered as
 1 2 - 4 5 + *
Parentheses are not needed; the notation is unambiguous as long as we know how many
operands each operator expects.
The implementation is simple. Each operand is pushed onto a stack; when an operator arrives,
the proper number of operands (two for binary operators) is popped, the operator is applied to
them, and the result is pushed back onto the stack. In the example above, for instance, 1 and 2
are pushed, then replaced by their difference, -1. Next, 4 and 5 are pushed and then replaced
by their sum, 9. The product of -1 and 9, which is -9, replaces them on the stack. The value on
the top of the stack is popped and printed when the end of the input line is encountered.
The structure of the program is thus a loop that performs the proper operation on each
operator and operand as it appears:
 while (next operator or operand is not end-of-file indicator)
 if (number)
 push it
 else if (operator)
 pop operands
 do operation
 push result
 else if (newline)
 pop and print top of stack
 else
 error
65
The operation of pushing and popping a stack are trivial, but by the time error detection and
recovery are added, they are long enough that it is better to put each in a separate function
than to repeat the code throughout the whole program. And there should be a separate
function for fetching the next input operator or operand.
The main design decision that has not yet been discussed is where the stack is, that is, which
routines access it directly. On possibility is to keep it in main, and pass the stack and the
current stack position to the routines that push and pop it. But main doesn't need to know
about the variables that control the stack; it only does push and pop operations. So we have
decided to store the stack and its associated information in external variables accessible to the
push and pop functions but not to main.
Translating this outline into code is easy enough. If for now we think of the program as
existing in one source file, it will look like this:
#includes
#defines
function declarations for main
main() { ... }
external variables for push and pop
 void push( double f) { ... }
 double pop(void) { ... }
 int getop(char s[]) { ... }
routines called by getop
Later we will discuss how this might be split into two or more source files.
The function main is a loop containing a big switch on the type of operator or operand; this is
a more typical use of switch than the one shown in Section 3.4.
 #include <stdio.h>
 #include <stdlib.h> /* for atof() */
 #define MAXOP 100 /* max size of operand or operator */
 #define NUMBER '0' /* signal that a number was found */
 int getop(char []);
 void push(double);
 double pop(void);
 /* reverse Polish calculator */
 main()
 {
 int type;
 double op2;
 char s[MAXOP];
 while ((type = getop(s)) != EOF) {
 switch (type) {
 case NUMBER:
 push(atof(s));
 break;
 case '+':
 push(pop() + pop());
 break;
 case '*':
 push(pop() * pop());
66
 break;
 case '-':
 op2 = pop();
 push(pop() - op2);
 break;
 case '/':
 op2 = pop();
 if (op2 != 0.0)
 push(pop() / op2);
 else
 printf("error: zero divisor\n");
 break;
 case '\n':
 printf("\t%.8g\n", pop());
 break;
 default:
 printf("error: unknown command %s\n", s);
 break;
 }
 }
 return 0;
 }
Because + and * are commutative operators, the order in which the popped operands are
combined is irrelevant, but for - and / the left and right operand must be distinguished. In
 push(pop() - pop()); /* WRONG */
the order in which the two calls of pop are evaluated is not defined. To guarantee the right
order, it is necessary to pop the first value into a temporary variable as we did in main.
 #define MAXVAL 100 /* maximum depth of val stack */
 int sp = 0; /* next free stack position */
 double val[MAXVAL]; /* value stack */
 /* push: push f onto value stack */
 void push(double f)
 {
 if (sp < MAXVAL)
 val[sp++] = f;
 else
 printf("error: stack full, can't push %g\n", f);
 }
 /* pop: pop and return top value from stack */
 double pop(void)
 {
 if (sp > 0)
 return val[--sp];
 else {
 printf("error: stack empty\n");
 return 0.0;
 }
 }
A variable is external if it is defined outside of any function. Thus the stack and stack index
that must be shared by push and pop are defined outside these functions. But main itself does
not refer to the stack or stack position - the representation can be hidden.
Let us now turn to the implementation of getop, the function that fetches the next operator or
operand. The task is easy. Skip blanks and tabs. If the next character is not a digit or a
hexadecimal point, return it. Otherwise, collect a string of digits (which might include a
decimal point), and return NUMBER, the signal that a number has been collected.
 #include <ctype.h>
67
 int getch(void);
 void ungetch(int);
 /* getop: get next character or numeric operand */
 int getop(char s[])
 {
 int i, c;
 while ((s[0] = c = getch()) == ' ' || c == '\t')
 ;
 s[1] = '\0';
 if (!isdigit(c) && c != '.')
 return c; /* not a number */
 i = 0;
 if (isdigit(c)) /* collect integer part */
 while (isdigit(s[++i] = c = getch()))
 ;
 if (c == '.') /* collect fraction part */
 while (isdigit(s[++i] = c = getch()))
 ;
 s[i] = '\0';
 if (c != EOF)
 ungetch(c);
 return NUMBER;
 }
What are getch and ungetch? It is often the case that a program cannot determine that it has
read enough input until it has read too much. One instance is collecting characters that make
up a number: until the first non-digit is seen, the number is not complete. But then the program
has read one character too far, a character that it is not prepared for.
The problem would be solved if it were possible to ``un-read'' the unwanted character. Then,
every time the program reads one character too many, it could push it back on the input, so the
rest of the code could behave as if it had never been read. Fortunately, it's easy to simulate ungetting a character, by writing a pair of cooperating functions. getch delivers the next input
character to be considered; ungetch will return them before reading new input.
How they work together is simple. ungetch puts the pushed-back characters into a shared
buffer -- a character array. getch reads from the buffer if there is anything else, and calls
getchar if the buffer is empty. There must also be an index variable that records the position
of the current character in the buffer.
Since the buffer and the index are shared by getch and ungetch and must retain their values
between calls, they must be external to both routines. Thus we can write getch, ungetch, and
their shared variables as:
 #define BUFSIZE 100
 char buf[BUFSIZE]; /* buffer for ungetch */
 int bufp = 0; /* next free position in buf */
 int getch(void) /* get a (possibly pushed-back) character */
 {
 return (bufp > 0) ? buf[--bufp] : getchar();
 }
 void ungetch(int c) /* push character back on input */
 {
 if (bufp >= BUFSIZE)
 printf("ungetch: too many characters\n");
 else
 buf[bufp++] = c;
 }
68
The standard library includes a function ungetch that provides one character of pushback; we
will discuss it in Chapter 7. We have used an array for the pushback, rather than a single
character, to illustrate a more general approach.
Exercise 4-3. Given the basic framework, it's straightforward to extend the calculator. Add the
modulus (%) operator and provisions for negative numbers.
Exercise 4-4. Add the commands to print the top elements of the stack without popping, to
duplicate it, and to swap the top two elements. Add a command to clear the stack.
Exercise 4-5. Add access to library functions like sin, exp, and pow. See <math.h> in
Appendix B, Section 4.
Exercise 4-6. Add commands for handling variables. (It's easy to provide twenty-six variables
with single-letter names.) Add a variable for the most recently printed value.
Exercise 4-7. Write a routine ungets(s) that will push back an entire string onto the input.
Should ungets know about buf and bufp, or should it just use ungetch?
Exercise 4-8. Suppose that there will never be more than one character of pushback. Modify
getch and ungetch accordingly.
Exercise 4-9. Our getch and ungetch do not handle a pushed-back EOF correctly. Decide
what their properties ought to be if an EOF is pushed back, then implement your design.
Exercise 4-10. An alternate organization uses getline to read an entire input line; this makes
getch and ungetch unnecessary. Revise the calculator to use this approach.
4.4 Scope Rules
The functions and external variables that make up a C program need not all be compiled at the
same time; the source text of the program may be kept in several files, and previously compiled
routines may be loaded from libraries. Among the questions of interest are
• How are declarations written so that variables are properly declared during
compilation?
• How are declarations arranged so that all the pieces will be properly connected when
the program is loaded?
• How are declarations organized so there is only one copy?
• How are external variables initialized?
Let us discuss these topics by reorganizing the calculator program into several files. As a
practical matter, the calculator is too small to be worth splitting, but it is a fine illustration of
the issues that arise in larger programs.
The scope of a name is the part of the program within which the name can be used. For an
automatic variable declared at the beginning of a function, the scope is the function in which
the name is declared. Local variables of the same name in different functions are unrelated. The
same is true of the parameters of the function, which are in effect local variables.
The scope of an external variable or a function lasts from the point at which it is declared to
the end of the file being compiled. For example, if main, sp, val, push, and pop are defined in
one file, in the order shown above, that is,
 main() { ... }
69
 int sp = 0;
 double val[MAXVAL];
 void push(double f) { ... }
 double pop(void) { ... }
then the variables sp and val may be used in push and pop simply by naming them; no further
declarations are needed. But these names are not visible in main, nor are push and pop
themselves.
On the other hand, if an external variable is to be referred to before it is defined, or if it is
defined in a different source file from the one where it is being used, then an extern
declaration is mandatory.
It is important to distinguish between the declaration of an external variable and its definition.
A declaration announces the properties of a variable (primarily its type); a definition also
causes storage to be set aside. If the lines
 int sp;
 double val[MAXVAL];
appear outside of any function, they define the external variables sp and val, cause storage to
be set aside, and also serve as the declarations for the rest of that source file. On the other
hand, the lines
 extern int sp;
 extern double val[];
declare for the rest of the source file that sp is an int and that val is a double array (whose
size is determined elsewhere), but they do not create the variables or reserve storage for them.
There must be only one definition of an external variable among all the files that make up the
source program; other files may contain extern declarations to access it. (There may also be
extern declarations in the file containing the definition.) Array sizes must be specified with the
definition, but are optional with an extern declaration.
Initialization of an external variable goes only with the definition.
Although it is not a likely organization for this program, the functions push and pop could be
defined in one file, and the variables val and sp defined and initialized in another. Then these
definitions and declarations would be necessary to tie them together:
in file1:
 extern int sp;
 extern double val[];
 void push(double f) { ... }
 double pop(void) { ... }
in file2:
 int sp = 0;
 double val[MAXVAL];
Because the extern declarations in file1 lie ahead of and outside the function definitions, they
apply to all functions; one set of declarations suffices for all of file1. This same organization
would also bee needed if the definition of sp and val followed their use in one file.
4.5 Header Files
Let is now consider dividing the calculator program into several source files, as it might be is
each of the components were substantially bigger. The main function would go in one file,
70
which we will call main.c; push, pop, and their variables go into a second file, stack.c;
getop goes into a third, getop.c. Finally, getch and ungetch go into a fourth file, getch.c;
we separate them from the others because they would come from a separately-compiled library
in a realistic program.
There is one more thing to worry about - the definitions and declarations shared among files.
As much as possible, we want to centralize this, so that there is only one copy to get and keep
right as the program evolves. Accordingly, we will place this common material in a header file,
calc.h, which will be included as necessary. (The #include line is described in Section 4.11.)
The resulting program then looks like this:
There is a tradeoff between the desire that each file have access only to the information it
needs for its job and the practical reality that it is harder to maintain more header files. Up to
some moderate program size, it is probably best to have one header file that contains
everything that is to be shared between any two parts of the program; that is the decision we
made here. For a much larger program, more organization and more headers would be needed.
4.6 Static Variables
71
The variables sp and val in stack.c, and buf and bufp in getch.c, are for the private use of
the functions in their respective source files, and are not meant to be accessed by anything else.
The static declaration, applied to an external variable or function, limits the scope of that
object to the rest of the source file being compiled. External static thus provides a way to
hide names like buf and bufp in the getch-ungetch combination, which must be external so
they can be shared, yet which should not be visible to users of getch and ungetch.
Static storage is specified by prefixing the normal declaration with the word static. If the two
routines and the two variables are compiled in one file, as in
 static char buf[BUFSIZE]; /* buffer for ungetch */
 static int bufp = 0; /* next free position in buf */
 int getch(void) { ... }
 void ungetch(int c) { ... }
then no other routine will be able to access buf and bufp, and those names will not conflict
with the same names in other files of the same program. In the same way, the variables that
push and pop use for stack manipulation can be hidden, by declaring sp and val to be static.
The external static declaration is most often used for variables, but it can be applied to
functions as well. Normally, function names are global, visible to any part of the entire
program. If a function is declared static, however, its name is invisible outside of the file in
which it is declared.
The static declaration can also be applied to internal variables. Internal static variables are
local to a particular function just as automatic variables are, but unlike automatics, they remain
in existence rather than coming and going each time the function is activated. This means that
internal static variables provide private, permanent storage within a single function.
Exercise 4-11. Modify getop so that it doesn't need to use ungetch. Hint: use an internal
static variable.
4.7 Register Variables
A register declaration advises the compiler that the variable in question will be heavily used.
The idea is that register variables are to be placed in machine registers, which may result in
smaller and faster programs. But compilers are free to ignore the advice.
The register declaration looks like
 register int x;
 register char c;
and so on. The register declaration can only be applied to automatic variables and to the
formal parameters of a function. In this later case, it looks like
 f(register unsigned m, register long n)
 {
 register int i;
 ...
 }
In practice, there are restrictions on register variables, reflecting the realities of underlying
hardware. Only a few variables in each function may be kept in registers, and only certain types
are allowed. Excess register declarations are harmless, however, since the word register is
ignored for excess or disallowed declarations. And it is not possible to take the address of a
register variable (a topic covered in Chapter 5), regardless of whether the variable is actually
placed in a register. The specific restrictions on number and types of register variables vary
from machine to machine. 
72
4.8 Block Structure
C is not a block-structured language in the sense of Pascal or similar languages, because
functions may not be defined within other functions. On the other hand, variables can be
defined in a block-structured fashion within a function. Declarations of variables (including
initializations) may follow the left brace that introduces any compound statement, not just the
one that begins a function. Variables declared in this way hide any identically named variables
in outer blocks, and remain in existence until the matching right brace. For example, in
 if (n > 0) {
 int i; /* declare a new i */
 for (i = 0; i < n; i++)
 ...
 }
the scope of the variable i is the ``true'' branch of the if; this i is unrelated to any i outside
the block. An automatic variable declared and initialized in a block is initialized each time the
block is entered.
Automatic variables, including formal parameters, also hide external variables and functions of
the same name. Given the declarations
 int x;
 int y;
 f(double x)
 {
 double y;
 }
then within the function f, occurrences of x refer to the parameter, which is a double; outside
f, they refer to the external int. The same is true of the variable y.
As a matter of style, it's best to avoid variable names that conceal names in an outer scope; the
potential for confusion and error is too great.
4.9 Initialization
Initialization has been mentioned in passing many times so far, but always peripherally to some
other topic. This section summarizes some of the rules, now that we have discussed the
various storage classes.
In the absence of explicit initialization, external and static variables are guaranteed to be
initialized to zero; automatic and register variables have undefined (i.e., garbage) initial values.
Scalar variables may be initialized when they are defined, by following the name with an equals
sign and an expression:
 int x = 1;
 char squota = '\'';
 long day = 1000L * 60L * 60L * 24L; /* milliseconds/day */
For external and static variables, the initializer must be a constant expression; the initialization
is done once, conceptionally before the program begins execution. For automatic and register
variables, the initializer is not restricted to being a constant: it may be any expression involving
previously defined values, even function calls. For example, the initialization of the binary
search program in Section 3.3 could be written as
 int binsearch(int x, int v[], int n)
 {
 int low = 0;
 int high = n - 1;
73
 int mid;
 ...
 }
instead of
 int low, high, mid;
 low = 0;
 high = n - 1;
In effect, initialization of automatic variables are just shorthand for assignment statements.
Which form to prefer is largely a matter of taste. We have generally used explicit assignments,
because initializers in declarations are harder to see and further away from the point of use.
An array may be initialized by following its declaration with a list of initializers enclosed in
braces and separated by commas. For example, to initialize an array days with the number of
days in each month:
 int days[] = { 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31 }
When the size of the array is omitted, the compiler will compute the length by counting the
initializers, of which there are 12 in this case.
If there are fewer initializers for an array than the specified size, the others will be zero for
external, static and automatic variables. It is an error to have too many initializers. There is no
way to specify repetition of an initializer, nor to initialize an element in the middle of an array
without supplying all the preceding values as well.
Character arrays are a special case of initialization; a string may be used instead of the braces
and commas notation:
 char pattern = "ould";
is a shorthand for the longer but equivalent
 char pattern[] = { 'o', 'u', 'l', 'd', '\0' };
In this case, the array size is five (four characters plus the terminating '\0').
4.10 Recursion
C functions may be used recursively; that is, a function may call itself either directly or
indirectly. Consider printing a number as a character string. As we mentioned before, the digits
are generated in the wrong order: low-order digits are available before high-order digits, but
they have to be printed the other way around.
There are two solutions to this problem. On is to store the digits in an array as they are
generated, then print them in the reverse order, as we did with itoa in section 3.6. The
alternative is a recursive solution, in which printd first calls itself to cope with any leading
digits, then prints the trailing digit. Again, this version can fail on the largest negative number.
 #include <stdio.h>
 /* printd: print n in decimal */
 void printd(int n)
 {
 if (n < 0) {
 putchar('-');
 n = -n;
 }
 if (n / 10)
 printd(n / 10);
 putchar(n % 10 + '0');
 }
74
When a function calls itself recursively, each invocation gets a fresh set of all the automatic
variables, independent of the previous set. This in printd(123) the first printd receives the
argument n = 123. It passes 12 to a second printd, which in turn passes 1 to a third. The
third-level printd prints 1, then returns to the second level. That printd prints 2, then returns
to the first level. That one prints 3 and terminates.
Another good example of recursion is quicksort, a sorting algorithm developed by C.A.R.
Hoare in 1962. Given an array, one element is chosen and the others partitioned in two subsets
- those less than the partition element and those greater than or equal to it. The same process is
then applied recursively to the two subsets. When a subset has fewer than two elements, it
doesn't need any sorting; this stops the recursion.
Our version of quicksort is not the fastest possible, but it's one of the simplest. We use the
middle element of each subarray for partitioning.
 /* qsort: sort v[left]...v[right] into increasing order */
 void qsort(int v[], int left, int right)
 {
 int i, last;
 void swap(int v[], int i, int j);
 if (left >= right) /* do nothing if array contains */
 return; /* fewer than two elements */
 swap(v, left, (left + right)/2); /* move partition elem */
 last = left; /* to v[0] */
 for (i = left + 1; i <= right; i++) /* partition */
 if (v[i] < v[left])
 swap(v, ++last, i);
 swap(v, left, last); /* restore partition elem */
 qsort(v, left, last-1);
 qsort(v, last+1, right);
 }
We moved the swapping operation into a separate function swap because it occurs three times
in qsort.
 /* swap: interchange v[i] and v[j] */
 void swap(int v[], int i, int j)
 {
 int temp;
 temp = v[i];
 v[i] = v[j];
 v[j] = temp;
 }
The standard library includes a version of qsort that can sort objects of any type.
Recursion may provide no saving in storage, since somewhere a stack of the values being
processed must be maintained. Nor will it be faster. But recursive code is more compact, and
often much easier to write and understand than the non-recursive equivalent. Recursion is
especially convenient for recursively defined data structures like trees, we will see a nice
example in Section 6.6.
Exercise 4-12. Adapt the ideas of printd to write a recursive version of itoa; that is, convert
an integer into a string by calling a recursive routine.
Exercise 4-13. Write a recursive version of the function reverse(s), which reverses the
string s in place.
4.11 The C Preprocessor
75
C provides certain language facilities by means of a preprocessor, which is conceptionally a
separate first step in compilation. The two most frequently used features are #include, to
include the contents of a file during compilation, and #define, to replace a token by an
arbitrary sequence of characters. Other features described in this section include conditional
compilation and macros with arguments.
4.11.1 File Inclusion
File inclusion makes it easy to handle collections of #defines and declarations (among other
things). Any source line of the form
 #include "filename"
or
 #include <filename>
is replaced by the contents of the file filename. If the filename is quoted, searching for the file
typically begins where the source program was found; if it is not found there, or if the name is
enclosed in < and >, searching follows an implementation-defined rule to find the file. An
included file may itself contain #include lines.
There are often several #include lines at the beginning of a source file, to include common
#define statements and extern declarations, or to access the function prototype declarations
for library functions from headers like <stdio.h>. (Strictly speaking, these need not be files;
the details of how headers are accessed are implementation-dependent.)
#include is the preferred way to tie the declarations together for a large program. It
guarantees that all the source files will be supplied with the same definitions and variable
declarations, and thus eliminates a particularly nasty kind of bug. Naturally, when an included
file is changed, all files that depend on it must be recompiled.
4.11.2 Macro Substitution
A definition has the form
 #define name replacement text
It calls for a macro substitution of the simplest kind - subsequent occurrences of the token
name will be replaced by the replacement text. The name in a #define has the same form as a
variable name; the replacement text is arbitrary. Normally the replacement text is the rest of the
line, but a long definition may be continued onto several lines by placing a \ at the end of each
line to be continued. The scope of a name defined with #define is from its point of definition
to the end of the source file being compiled. A definition may use previous definitions.
Substitutions are made only for tokens, and do not take place within quoted strings. For
example, if YES is a defined name, there would be no substitution in printf("YES") or in
YESMAN.
Any name may be defined with any replacement text. For example
 #define forever for (;;) /* infinite loop */
defines a new word, forever, for an infinite loop.
It is also possible to define macros with arguments, so the replacement text can be different for
different calls of the macro. As an example, define a macro called max:
 #define max(A, B) ((A) > (B) ? (A) : (B))
Although it looks like a function call, a use of max expands into in-line code. Each occurrence
of a formal parameter (here A or B) will be replaced by the corresponding actual argument.
Thus the line 
76
 x = max(p+q, r+s);
will be replaced by the line
 x = ((p+q) > (r+s) ? (p+q) : (r+s));
So long as the arguments are treated consistently, this macro will serve for any data type; there
is no need for different kinds of max for different data types, as there would be with functions.
If you examine the expansion of max, you will notice some pitfalls. The expressions are
evaluated twice; this is bad if they involve side effects like increment operators or input and
output. For instance
 max(i++, j++) /* WRONG */
will increment the larger twice. Some care also has to be taken with parentheses to make sure
the order of evaluation is preserved; consider what happens when the macro
 #define square(x) x * x /* WRONG */
is invoked as square(z+1).
Nonetheless, macros are valuable. One practical example comes from <stdio.h>, in which
getchar and putchar are often defined as macros to avoid the run-time overhead of a
function call per character processed. The functions in <ctype.h> are also usually
implemented as macros.
Names may be undefined with #undef, usually to ensure that a routine is really a function, not
a macro:
 #undef getchar
 int getchar(void) { ... }
Formal parameters are not replaced within quoted strings. If, however, a parameter name is
preceded by a # in the replacement text, the combination will be expanded into a quoted string
with the parameter replaced by the actual argument. This can be combined with string
concatenation to make, for example, a debugging print macro:
 #define dprint(expr) printf(#expr " = %g\n", expr)
When this is invoked, as in
 dprint(x/y)
the macro is expanded into
 printf("x/y" " = &g\n", x/y);
and the strings are concatenated, so the effect is
 printf("x/y = &g\n", x/y);
Within the actual argument, each " is replaced by \" and each \ by \\, so the result is a legal
string constant.
The preprocessor operator ## provides a way to concatenate actual arguments during macro
expansion. If a parameter in the replacement text is adjacent to a ##, the parameter is replaced
by the actual argument, the ## and surrounding white space are removed, and the result is rescanned. For example, the macro paste concatenates its two arguments:
 #define paste(front, back) front ## back
so paste(name, 1) creates the token name1.
The rules for nested uses of ## are arcane; further details may be found in Appendix A.
Exercise 4-14. Define a macro swap(t,x,y) that interchanges two arguments of type t.
(Block structure will help.) 
77
4.11.3 Conditional Inclusion
It is possible to control preprocessing itself with conditional statements that are evaluated
during preprocessing. This provides a way to include code selectively, depending on the value
of conditions evaluated during compilation.
The #if line evaluates a constant integer expression (which may not include sizeof, casts, or
enum constants). If the expression is non-zero, subsequent lines until an #endif or #elif or
#else are included. (The preprocessor statement #elif is like else-if.) The expression
defined(name) in a #if is 1 if the name has been defined, and 0 otherwise.
For example, to make sure that the contents of a file hdr.h are included only once, the
contents of the file are surrounded with a conditional like this:
 #if !defined(HDR)
 #define HDR
 /* contents of hdr.h go here */
 #endif
The first inclusion of hdr.h defines the name HDR; subsequent inclusions will find the name
defined and skip down to the #endif. A similar style can be used to avoid including files
multiple times. If this style is used consistently, then each header can itself include any other
headers on which it depends, without the user of the header having to deal with the
interdependence.
This sequence tests the name SYSTEM to decide which version of a header to include:
 #if SYSTEM == SYSV
 #define HDR "sysv.h"
 #elif SYSTEM == BSD
 #define HDR "bsd.h"
 #elif SYSTEM == MSDOS
 #define HDR "msdos.h"
 #else
 #define HDR "default.h"
 #endif
 #include HDR
The #ifdef and #ifndef lines are specialized forms that test whether a name is defined. The
first example of #if above could have been written
 #ifndef HDR
 #define HDR
 /* contents of hdr.h go here */
 #endif
78
Chapter 5 - Pointers and Arrays
A pointer is a variable that contains the address of a variable. Pointers are much used in C,
partly because they are sometimes the only way to express a computation, and partly because
they usually lead to more compact and efficient code than can be obtained in other ways.
Pointers and arrays are closely related; this chapter also explores this relationship and shows
how to exploit it.
Pointers have been lumped with the goto statement as a marvelous way to create impossibleto-understand programs. This is certainly true when they are used carelessly, and it is easy to
create pointers that point somewhere unexpected. With discipline, however, pointers can also
be used to achieve clarity and simplicity. This is the aspect that we will try to illustrate.
The main change in ANSI C is to make explicit the rules about how pointers can be
manipulated, in effect mandating what good programmers already practice and good compilers
already enforce. In addition, the type void * (pointer to void) replaces char * as the proper
type for a generic pointer.
5.1 Pointers and Addresses
Let us begin with a simplified picture of how memory is organized. A typical machine has an
array of consecutively numbered or addressed memory cells that may be manipulated
individually or in contiguous groups. One common situation is that any byte can be a char, a
pair of one-byte cells can be treated as a short integer, and four adjacent bytes form a long. A
pointer is a group of cells (often two or four) that can hold an address. So if c is a char and p
is a pointer that points to it, we could represent the situation this way:
The unary operator & gives the address of an object, so the statement
 p = &c;
assigns the address of c to the variable p, and p is said to ``point to'' c. The & operator only
applies to objects in memory: variables and array elements. It cannot be applied to expressions,
constants, or register variables.
The unary operator * is the indirection or dereferencing operator; when applied to a pointer, it
accesses the object the pointer points to. Suppose that x and y are integers and ip is a pointer
to int. This artificial sequence shows how to declare a pointer and how to use & and *:
 int x = 1, y = 2, z[10];
 int *ip; /* ip is a pointer to int */
 ip = &x; /* ip now points to x */
 y = *ip; /* y is now 1 */
 *ip = 0; /* x is now 0 */
 ip = &z[0]; /* ip now points to z[0] */
The declaration of x, y, and z are what we've seen all along. The declaration of the pointer ip,
 int *ip;
79
is intended as a mnemonic; it says that the expression *ip is an int. The syntax of the
declaration for a variable mimics the syntax of expressions in which the variable might appear.
This reasoning applies to function declarations as well. For example,
 double *dp, atof(char *);
says that in an expression *dp and atof(s) have values of double, and that the argument of
atof is a pointer to char.
You should also note the implication that a pointer is constrained to point to a particular kind
of object: every pointer points to a specific data type. (There is one exception: a ``pointer to
void'' is used to hold any type of pointer but cannot be dereferenced itself. We'll come back to
it in Section 5.11.)
If ip points to the integer x, then *ip can occur in any context where x could, so
 *ip = *ip + 10;
increments *ip by 10.
The unary operators * and & bind more tightly than arithmetic operators, so the assignment
 y = *ip + 1
takes whatever ip points at, adds 1, and assigns the result to y, while
 *ip += 1
increments what ip points to, as do
 ++*ip
and
 (*ip)++
The parentheses are necessary in this last example; without them, the expression would
increment ip instead of what it points to, because unary operators like * and ++ associate right
to left.
Finally, since pointers are variables, they can be used without dereferencing. For example, if iq
is another pointer to int,
 iq = ip
copies the contents of ip into iq, thus making iq point to whatever ip pointed to.
5.2 Pointers and Function Arguments
Since C passes arguments to functions by value, there is no direct way for the called function
to alter a variable in the calling function. For instance, a sorting routine might exchange two
out-of-order arguments with a function called swap. It is not enough to write
 swap(a, b);
where the swap function is defined as
 void swap(int x, int y) /* WRONG */
 {
 int temp;
 temp = x;
 x = y;
 y = temp;
 }
Because of call by value, swap can't affect the arguments a and b in the routine that called it.
The function above swaps copies of a and b. 
80
The way to obtain the desired effect is for the calling program to pass pointers to the values to
be changed:
 swap(&a, &b);
Since the operator & produces the address of a variable, &a is a pointer to a. In swap itself, the
parameters are declared as pointers, and the operands are accessed indirectly through them.
 void swap(int *px, int *py) /* interchange *px and *py */
 {
 int temp;
 temp = *px;
 *px = *py;
 *py = temp;
 }
Pictorially:
Pointer arguments enable a function to access and change objects in the function that called it.
As an example, consider a function getint that performs free-format input conversion by
breaking a stream of characters into integer values, one integer per call. getint has to return
the value it found and also signal end of file when there is no more input. These values have to
be passed back by separate paths, for no matter what value is used for EOF, that could also be
the value of an input integer.
One solution is to have getint return the end of file status as its function value, while using a
pointer argument to store the converted integer back in the calling function. This is the scheme
used by scanf as well; see Section 7.4.
The following loop fills an array with integers by calls to getint: 
81
 int n, array[SIZE], getint(int *);
 for (n = 0; n < SIZE && getint(&array[n]) != EOF; n++)
 ;
Each call sets array[n] to the next integer found in the input and increments n. Notice that it
is essential to pass the address of array[n] to getint. Otherwise there is no way for getint
to communicate the converted integer back to the caller.
Our version of getint returns EOF for end of file, zero if the next input is not a number, and a
positive value if the input contains a valid number.
 #include <ctype.h>
 int getch(void);
 void ungetch(int);
 /* getint: get next integer from input into *pn */
 int getint(int *pn)
 {
 int c, sign;
 while (isspace(c = getch())) /* skip white space */
 ;
 if (!isdigit(c) && c != EOF && c != '+' && c != '-') {
 ungetch(c); /* it is not a number */
 return 0;
 }
 sign = (c == '-') ? -1 : 1;
 if (c == '+' || c == '-')
 c = getch();
 for (*pn = 0; isdigit(c), c = getch())
 *pn = 10 * *pn + (c - '0');
 *pn *= sign;
 if (c != EOF)
 ungetch(c);
 return c;
 }
Throughout getint, *pn is used as an ordinary int variable. We have also used getch and
ungetch (described in Section 4.3) so the one extra character that must be read can be pushed
back onto the input.
Exercise 5-1. As written, getint treats a + or - not followed by a digit as a valid
representation of zero. Fix it to push such a character back on the input.
Exercise 5-2. Write getfloat, the floating-point analog of getint. What type does getfloat
return as its function value?
5.3 Pointers and Arrays
In C, there is a strong relationship between pointers and arrays, strong enough that pointers
and arrays should be discussed simultaneously. Any operation that can be achieved by array
subscripting can also be done with pointers. The pointer version will in general be faster but, at
least to the uninitiated, somewhat harder to understand.
The declaration
 int a[10];
defines an array of size 10, that is, a block of 10 consecutive objects named a[0], a[1],
...,a[9]. 
82
The notation a[i] refers to the i-th element of the array. If pa is a pointer to an integer,
declared as
 int *pa;
then the assignment
 pa = &a[0];
sets pa to point to element zero of a; that is, pa contains the address of a[0].
Now the assignment
 x = *pa;
will copy the contents of a[0] into x.
If pa points to a particular element of an array, then by definition pa+1 points to the next
element, pa+i points i elements after pa, and pa-i points i elements before. Thus, if pa points
to a[0],
 *(pa+1)
refers to the contents of a[1], pa+i is the address of a[i], and *(pa+i) is the contents of
a[i]. 
83
These remarks are true regardless of the type or size of the variables in the array a. The
meaning of ``adding 1 to a pointer,'' and by extension, all pointer arithmetic, is that pa+1 points
to the next object, and pa+i points to the i-th object beyond pa.
The correspondence between indexing and pointer arithmetic is very close. By definition, the
value of a variable or expression of type array is the address of element zero of the array. Thus
after the assignment
 pa = &a[0];
pa and a have identical values. Since the name of an array is a synonym for the location of the
initial element, the assignment pa=&a[0] can also be written as
 pa = a;
Rather more surprising, at first sight, is the fact that a reference to a[i] can also be written as
*(a+i). In evaluating a[i], C converts it to *(a+i) immediately; the two forms are
equivalent. Applying the operator & to both parts of this equivalence, it follows that &a[i] and
a+i are also identical: a+i is the address of the i-th element beyond a. As the other side of this
coin, if pa is a pointer, expressions might use it with a subscript; pa[i] is identical to *(pa+i).
In short, an array-and-index expression is equivalent to one written as a pointer and offset.
There is one difference between an array name and a pointer that must be kept in mind. A
pointer is a variable, so pa=a and pa++ are legal. But an array name is not a variable;
constructions like a=pa and a++ are illegal.
When an array name is passed to a function, what is passed is the location of the initial
element. Within the called function, this argument is a local variable, and so an array name
parameter is a pointer, that is, a variable containing an address. We can use this fact to write
another version of strlen, which computes the length of a string.
 /* strlen: return length of string s */
 int strlen(char *s)
 {
 int n;
 for (n = 0; *s != '\0', s++)
 n++;
 return n;
 }
Since s is a pointer, incrementing it is perfectly legal; s++ has no effect on the character string
in the function that called strlen, but merely increments strlen's private copy of the pointer.
That means that calls like
 strlen("hello, world"); /* string constant */
 strlen(array); /* char array[100]; */
 strlen(ptr); /* char *ptr; */
all work.
As formal parameters in a function definition,
 char s[];
and
 char *s;
are equivalent; we prefer the latter because it says more explicitly that the variable is a pointer.
When an array name is passed to a function, the function can at its convenience believe that it
has been handed either an array or a pointer, and manipulate it accordingly. It can even use
both notations if it seems appropriate and clear. 
84
It is possible to pass part of an array to a function, by passing a pointer to the beginning of the
subarray. For example, if a is an array,
 f(&a[2])
and
 f(a+2)
both pass to the function f the address of the subarray that starts at a[2]. Within f, the
parameter declaration can read
 f(int arr[]) { ... }
or
 f(int *arr) { ... }
So as far as f is concerned, the fact that the parameter refers to part of a larger array is of no
consequence.
If one is sure that the elements exist, it is also possible to index backwards in an array; p[-1],
p[-2], and so on are syntactically legal, and refer to the elements that immediately precede
p[0]. Of course, it is illegal to refer to objects that are not within the array bounds.
5.4 Address Arithmetic
If p is a pointer to some element of an array, then p++ increments p to point to the next
element, and p+=i increments it to point i elements beyond where it currently does. These and
similar constructions are the simples forms of pointer or address arithmetic.
C is consistent and regular in its approach to address arithmetic; its integration of pointers,
arrays, and address arithmetic is one of the strengths of the language. Let us illustrate by
writing a rudimentary storage allocator. There are two routines. The first, alloc(n), returns a
pointer to n consecutive character positions, which can be used by the caller of alloc for
storing characters. The second, afree(p), releases the storage thus acquired so it can be reused later. The routines are ``rudimentary'' because the calls to afree must be made in the
opposite order to the calls made on alloc. That is, the storage managed by alloc and afree
is a stack, or last-in, first-out. The standard library provides analogous functions called malloc
and free that have no such restrictions; in Section 8.7 we will show how they can be
implemented.
The easiest implementation is to have alloc hand out pieces of a large character array that we
will call allocbuf. This array is private to alloc and afree. Since they deal in pointers, not
array indices, no other routine need know the name of the array, which can be declared static
in the source file containing alloc and afree, and thus be invisible outside it. In practical
implementations, the array may well not even have a name; it might instead be obtained by
calling malloc or by asking the operating system for a pointer to some unnamed block of
storage.
The other information needed is how much of allocbuf has been used. We use a pointer,
called allocp, that points to the next free element. When alloc is asked for n characters, it
checks to see if there is enough room left in allocbuf. If so, alloc returns the current value
of allocp (i.e., the beginning of the free block), then increments it by n to point to the next
free area. If there is no room, alloc returns zero. afree(p) merely sets allocp to p if p is
inside allocbuf. 
85
 #define ALLOCSIZE 10000 /* size of available space */
 static char allocbuf[ALLOCSIZE]; /* storage for alloc */
 static char *allocp = allocbuf; /* next free position */
 char *alloc(int n) /* return pointer to n characters */
 {
 if (allocbuf + ALLOCSIZE - allocp >= n) { /* it fits */
 allocp += n;
 return allocp - n; /* old p */
 } else /* not enough room */
 return 0;
 }
 void afree(char *p) /* free storage pointed to by p */
 {
 if (p >= allocbuf && p < allocbuf + ALLOCSIZE)
 allocp = p;
 }
In general a pointer can be initialized just as any other variable can, though normally the only
meaningful values are zero or an expression involving the address of previously defined data of
appropriate type. The declaration
 static char *allocp = allocbuf;
defines allocp to be a character pointer and initializes it to point to the beginning of
allocbuf, which is the next free position when the program starts. This could also have been
written
 static char *allocp = &allocbuf[0];
since the array name is the address of the zeroth element.
The test
 if (allocbuf + ALLOCSIZE - allocp >= n) { /* it fits */
checks if there's enough room to satisfy a request for n characters. If there is, the new value of
allocp would be at most one beyond the end of allocbuf. If the request can be satisfied,
alloc returns a pointer to the beginning of a block of characters (notice the declaration of the
function itself). If not, alloc must return some signal that there is no space left. C guarantees
that zero is never a valid address for data, so a return value of zero can be used to signal an
abnormal event, in this case no space.
Pointers and integers are not interchangeable. Zero is the sole exception: the constant zero may
be assigned to a pointer, and a pointer may be compared with the constant zero. The symbolic
86
constant NULL is often used in place of zero, as a mnemonic to indicate more clearly that this is
a special value for a pointer. NULL is defined in <stdio.h>. We will use NULL henceforth.
Tests like
 if (allocbuf + ALLOCSIZE - allocp >= n) { /* it fits */
and
 if (p >= allocbuf && p < allocbuf + ALLOCSIZE)
show several important facets of pointer arithmetic. First, pointers may be compared under
certain circumstances. If p and q point to members of the same array, then relations like ==, !=,
<, >=, etc., work properly. For example,
 p < q
is true if p points to an earlier element of the array than q does. Any pointer can be
meaningfully compared for equality or inequality with zero. But the behavior is undefined for
arithmetic or comparisons with pointers that do not point to members of the same array.
(There is one exception: the address of the first element past the end of an array can be used in
pointer arithmetic.)
Second, we have already observed that a pointer and an integer may be added or subtracted.
The construction
 p + n
means the address of the n-th object beyond the one p currently points to. This is true
regardless of the kind of object p points to; n is scaled according to the size of the objects p
points to, which is determined by the declaration of p. If an int is four bytes, for example, the
int will be scaled by four.
Pointer subtraction is also valid: if p and q point to elements of the same array, and p<q, then
q-p+1 is the number of elements from p to q inclusive. This fact can be used to write yet
another version of strlen:
 /* strlen: return length of string s */
 int strlen(char *s)
 {
 char *p = s;
 while (*p != '\0')
 p++;
 return p - s;
 }
In its declaration, p is initialized to s, that is, to point to the first character of the string. In the
while loop, each character in turn is examined until the '\0' at the end is seen. Because p
points to characters, p++ advances p to the next character each time, and p-s gives the number
of characters advanced over, that is, the string length. (The number of characters in the string
could be too large to store in an int. The header <stddef.h> defines a type ptrdiff_t that is
large enough to hold the signed difference of two pointer values. If we were being cautious,
however, we would use size_t for the return value of strlen, to match the standard library
version. size_t is the unsigned integer type returned by the sizeof operator.
Pointer arithmetic is consistent: if we had been dealing with floats, which occupy more
storage that chars, and if p were a pointer to float, p++ would advance to the next float.
Thus we could write another version of alloc that maintains floats instead of chars, merely
by changing char to float throughout alloc and afree. All the pointer manipulations
automatically take into account the size of the objects pointed to. 
87
The valid pointer operations are assignment of pointers of the same type, adding or subtracting
a pointer and an integer, subtracting or comparing two pointers to members of the same array,
and assigning or comparing to zero. All other pointer arithmetic is illegal. It is not legal to add
two pointers, or to multiply or divide or shift or mask them, or to add float or double to
them, or even, except for void *, to assign a pointer of one type to a pointer of another type
without a cast.
5.5 Character Pointers and Functions
A string constant, written as
 "I am a string"
is an array of characters. In the internal representation, the array is terminated with the null
character '\0' so that programs can find the end. The length in storage is thus one more than
the number of characters between the double quotes.
Perhaps the most common occurrence of string constants is as arguments to functions, as in
 printf("hello, world\n");
When a character string like this appears in a program, access to it is through a character
pointer; printf receives a pointer to the beginning of the character array. That is, a string
constant is accessed by a pointer to its first element.
String constants need not be function arguments. If pmessage is declared as
 char *pmessage;
then the statement
 pmessage = "now is the time";
assigns to pmessage a pointer to the character array. This is not a string copy; only pointers
are involved. C does not provide any operators for processing an entire string of characters as
a unit.
There is an important difference between these definitions:
 char amessage[] = "now is the time"; /* an array */
 char *pmessage = "now is the time"; /* a pointer */
amessage is an array, just big enough to hold the sequence of characters and '\0' that
initializes it. Individual characters within the array may be changed but amessage will always
refer to the same storage. On the other hand, pmessage is a pointer, initialized to point to a
string constant; the pointer may subsequently be modified to point elsewhere, but the result is
undefined if you try to modify the string contents.
We will illustrate more aspects of pointers and arrays by studying versions of two useful
functions adapted from the standard library. The first function is strcpy(s,t), which copies
the string t to the string s. It would be nice just to say s=t but this copies the pointer, not the
characters. To copy the characters, we need a loop. The array version first: 
88
 /* strcpy: copy t to s; array subscript version */
 void strcpy(char *s, char *t)
 {
 int i;
 i = 0;
 while ((s[i] = t[i]) != '\0')
 i++;
 }
For contrast, here is a version of strcpy with pointers:
 /* strcpy: copy t to s; pointer version */
 void strcpy(char *s, char *t)
 {
 int i;
 i = 0;
 while ((*s = *t) != '\0') {
 s++;
 t++;
 }
 }
Because arguments are passed by value, strcpy can use the parameters s and t in any way it
pleases. Here they are conveniently initialized pointers, which are marched along the arrays a
character at a time, until the '\0' that terminates t has been copied into s.
In practice, strcpy would not be written as we showed it above. Experienced C programmers
would prefer
 /* strcpy: copy t to s; pointer version 2 */
 void strcpy(char *s, char *t)
 {
 while ((*s++ = *t++) != '\0')
 ;
 }
This moves the increment of s and t into the test part of the loop. The value of *t++ is the
character that t pointed to before t was incremented; the postfix ++ doesn't change t until
after this character has been fetched. In the same way, the character is stored into the old s
position before s is incremented. This character is also the value that is compared against '\0'
to control the loop. The net effect is that characters are copied from t to s, up and including
the terminating '\0'.
As the final abbreviation, observe that a comparison against '\0' is redundant, since the
question is merely whether the expression is zero. So the function would likely be written as
 /* strcpy: copy t to s; pointer version 3 */
 void strcpy(char *s, char *t)
 {
 while (*s++ = *t++)
 ;
 }
Although this may seem cryptic at first sight, the notational convenience is considerable, and
the idiom should be mastered, because you will see it frequently in C programs.
The strcpy in the standard library (<string.h>) returns the target string as its function value.
The second routine that we will examine is strcmp(s,t), which compares the character
strings s and t, and returns negative, zero or positive if s is lexicographically less than, equal
to, or greater than t. The value is obtained by subtracting the characters at the first position
where s and t disagree.
 /* strcmp: return <0 if s<t, 0 if s==t, >0 if s>t */
89
 int strcmp(char *s, char *t)
 {
 int i;
 for (i = 0; s[i] == t[i]; i++)
 if (s[i] == '\0')
 return 0;
 return s[i] - t[i];
 }
The pointer version of strcmp:
 /* strcmp: return <0 if s<t, 0 if s==t, >0 if s>t */
 int strcmp(char *s, char *t)
 {
 for ( ; *s == *t; s++, t++)
 if (*s == '\0')
 return 0;
 return *s - *t;
 }
Since ++ and -- are either prefix or postfix operators, other combinations of * and ++ and --
occur, although less frequently. For example,
 *--p
decrements p before fetching the character that p points to. In fact, the pair of expressions
 *p++ = val; /* push val onto stack */
 val = *--p; /* pop top of stack into val */
are the standard idiom for pushing and popping a stack; see Section 4.3.
The header <string.h> contains declarations for the functions mentioned in this section, plus
a variety of other string-handling functions from the standard library.
Exercise 5-3. Write a pointer version of the function strcat that we showed in Chapter 2:
strcat(s,t) copies the string t to the end of s.
Exercise 5-4. Write the function strend(s,t), which returns 1 if the string t occurs at the
end of the string s, and zero otherwise.
Exercise 5-5. Write versions of the library functions strncpy, strncat, and strncmp, which
operate on at most the first n characters of their argument strings. For example,
strncpy(s,t,n) copies at most n characters of t to s. Full descriptions are in Appendix B.
Exercise 5-6. Rewrite appropriate programs from earlier chapters and exercises with pointers
instead of array indexing. Good possibilities include getline (Chapters 1 and 4), atoi, itoa,
and their variants (Chapters 2, 3, and 4), reverse (Chapter 3), and strindex and getop
(Chapter 4).
5.6 Pointer Arrays; Pointers to Pointers
Since pointers are variables themselves, they can be stored in arrays just as other variables can.
Let us illustrate by writing a program that will sort a set of text lines into alphabetic order, a
stripped-down version of the UNIX program sort.
In Chapter 3, we presented a Shell sort function that would sort an array of integers, and in
Chapter 4 we improved on it with a quicksort. The same algorithms will work, except that now
we have to deal with lines of text, which are of different lengths, and which, unlike integers,
can't be compared or moved in a single operation. We need a data representation that will cope
efficiently and conveniently with variable-length text lines.
This is where the array of pointers enters. If the lines to be sorted are stored end-to-end in one
long character array, then each line can be accessed by a pointer to its first character. The
90
pointers themselves can bee stored in an array. Two lines can be compared by passing their
pointers to strcmp. When two out-of-order lines have to be exchanged, the pointers in the
pointer array are exchanged, not the text lines themselves.
This eliminates the twin problems of complicated storage management and high overhead that
would go with moving the lines themselves.
The sorting process has three steps:
read all the lines of input
sort them
print them in order
As usual, it's best to divide the program into functions that match this natural division, with the
main routine controlling the other functions. Let us defer the sorting step for a moment, and
concentrate on the data structure and the input and output.
The input routine has to collect and save the characters of each line, and build an array of
pointers to the lines. It will also have to count the number of input lines, since that information
is needed for sorting and printing. Since the input function can only cope with a finite number
of input lines, it can return some illegal count like -1 if too much input is presented.
The output routine only has to print the lines in the order in which they appear in the array of
pointers.
 #include <stdio.h>
 #include <string.h>
 #define MAXLINES 5000 /* max #lines to be sorted */
 char *lineptr[MAXLINES]; /* pointers to text lines */
 int readlines(char *lineptr[], int nlines);
 void writelines(char *lineptr[], int nlines);
 void qsort(char *lineptr[], int left, int right);
 /* sort input lines */
 main()
 {
 int nlines; /* number of input lines read */
 if ((nlines = readlines(lineptr, MAXLINES)) >= 0) {
 qsort(lineptr, 0, nlines-1);
 writelines(lineptr, nlines);
 return 0;
 } else {
 printf("error: input too big to sort\n");
 return 1;
 }
 }
 #define MAXLEN 1000 /* max length of any input line */
 int getline(char *, int);
91
 char *alloc(int);
 /* readlines: read input lines */
 int readlines(char *lineptr[], int maxlines)
 {
 int len, nlines;
 char *p, line[MAXLEN];
 nlines = 0;
 while ((len = getline(line, MAXLEN)) > 0)
 if (nlines >= maxlines || p = alloc(len) == NULL)
 return -1;
 else {
 line[len-1] = '\0'; /* delete newline */
 strcpy(p, line);
 lineptr[nlines++] = p;
 }
 return nlines;
 }
 /* writelines: write output lines */
 void writelines(char *lineptr[], int nlines)
 {
 int i;
 for (i = 0; i < nlines; i++)
 printf("%s\n", lineptr[i]);
 }
The function getline is from Section 1.9.
The main new thing is the declaration for lineptr:
 char *lineptr[MAXLINES]
says that lineptr is an array of MAXLINES elements, each element of which is a pointer to a
char. That is, lineptr[i] is a character pointer, and *lineptr[i] is the character it points
to, the first character of the i-th saved text line.
Since lineptr is itself the name of an array, it can be treated as a pointer in the same manner
as in our earlier examples, and writelines can be written instead as
 /* writelines: write output lines */
 void writelines(char *lineptr[], int nlines)
 {
 while (nlines-- > 0)
 printf("%s\n", *lineptr++);
 }
Initially, *lineptr points to the first line; each element advances it to the next line pointer
while nlines is counted down.
With input and output under control, we can proceed to sorting. The quicksort from Chapter 4
needs minor changes: the declarations have to be modified, and the comparison operation must
be done by calling strcmp. The algorithm remains the same, which gives us some confidence
that it will still work.
 /* qsort: sort v[left]...v[right] into increasing order */
 void qsort(char *v[], int left, int right)
 {
 int i, last;
 void swap(char *v[], int i, int j);
 if (left >= right) /* do nothing if array contains */
 return; /* fewer than two elements */
 swap(v, left, (left + right)/2);
 last = left;
92
 for (i = left+1; i <= right; i++)
 if (strcmp(v[i], v[left]) < 0)
 swap(v, ++last, i);
 swap(v, left, last);
 qsort(v, left, last-1);
 qsort(v, last+1, right);
 }
Similarly, the swap routine needs only trivial changes:
 /* swap: interchange v[i] and v[j] */
 void swap(char *v[], int i, int j)
 {
 char *temp;
 temp = v[i];
 v[i] = v[j];
 v[j] = temp;
 }
Since any individual element of v (alias lineptr) is a character pointer, temp must be also, so
one can be copied to the other.
Exercise 5-7. Rewrite readlines to store lines in an array supplied by main, rather than
calling alloc to maintain storage. How much faster is the program?
5.7 Multi-dimensional Arrays
C provides rectangular multi-dimensional arrays, although in practice they are much less used
than arrays of pointers. In this section, we will show some of their properties.
Consider the problem of date conversion, from day of the month to day of the year and vice
versa. For example, March 1 is the 60th day of a non-leap year, and the 61st day of a leap year.
Let us define two functions to do the conversions: day_of_year converts the month and day
into the day of the year, and month_day converts the day of the year into the month and day.
Since this latter function computes two values, the month and day arguments will be pointers:
 month_day(1988, 60, &m, &d)
sets m to 2 and d to 29 (February 29th).
These functions both need the same information, a table of the number of days in each month
(``thirty days hath September ...''). Since the number of days per month differs for leap years
and non-leap years, it's easier to separate them into two rows of a two-dimensional array than
to keep track of what happens to February during computation. The array and the functions for
performing the transformations are as follows:
 static char daytab[2][13] = {
 {0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31},
 {0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31}
 };
 /* day_of_year: set day of year from month & day */
 int day_of_year(int year, int month, int day)
 {
 int i, leap;
 leap = year%4 == 0 && year%100 != 0 || year%400 == 0;
 for (i = 1; i < month; i++)
 day += daytab[leap][i];
 return day;
 }
 /* month_day: set month, day from day of year */
 void month_day(int year, int yearday, int *pmonth, int *pday)
 {
 int i, leap;
93
 leap = year%4 == 0 && year%100 != 0 || year%400 == 0;
 for (i = 1; yearday > daytab[leap][i]; i++)
 yearday -= daytab[leap][i];
 *pmonth = i;
 *pday = yearday;
 }
Recall that the arithmetic value of a logical expression, such as the one for leap, is either zero
(false) or one (true), so it can be used as a subscript of the array daytab.
The array daytab has to be external to both day_of_year and month_day, so they can both
use it. We made it char to illustrate a legitimate use of char for storing small non-character
integers.
daytab is the first two-dimensional array we have dealt with. In C, a two-dimensional array is
really a one-dimensional array, each of whose elements is an array. Hence subscripts are
written as
 daytab[i][j] /* [row][col] */
rather than
 daytab[i,j] /* WRONG */
Other than this notational distinction, a two-dimensional array can be treated in much the same
way as in other languages. Elements are stored by rows, so the rightmost subscript, or column,
varies fastest as elements are accessed in storage order.
An array is initialized by a list of initializers in braces; each row of a two-dimensional array is
initialized by a corresponding sub-list. We started the array daytab with a column of zero so
that month numbers can run from the natural 1 to 12 instead of 0 to 11. Since space is not at a
premium here, this is clearer than adjusting the indices.
If a two-dimensional array is to be passed to a function, the parameter declaration in the
function must include the number of columns; the number of rows is irrelevant, since what is
passed is, as before, a pointer to an array of rows, where each row is an array of 13 ints. In
this particular case, it is a pointer to objects that are arrays of 13 ints. Thus if the array
daytab is to be passed to a function f, the declaration of f would be:
 f(int daytab[2][13]) { ... }
It could also be
 f(int daytab[][13]) { ... }
since the number of rows is irrelevant, or it could be
 f(int (*daytab)[13]) { ... }
which says that the parameter is a pointer to an array of 13 integers. The parentheses are
necessary since brackets [] have higher precedence than *. Without parentheses, the
declaration
 int *daytab[13]
is an array of 13 pointers to integers. More generally, only the first dimension (subscript) of an
array is free; all the others have to be specified.
Section 5.12 has a further discussion of complicated declarations.
Exercise 5-8. There is no error checking in day_of_year or month_day. Remedy this defect.
5.8 Initialization of Pointer Arrays
94
Consider the problem of writing a function month_name(n), which returns a pointer to a
character string containing the name of the n-th month. This is an ideal application for an
internal static array. month_name contains a private array of character strings, and returns a
pointer to the proper one when called. This section shows how that array of names is
initialized.
The syntax is similar to previous initializations:
 /* month_name: return name of n-th month */
 char *month_name(int n)
 {
 static char *name[] = {
 "Illegal month",
 "January", "February", "March",
 "April", "May", "June",
 "July", "August", "September",
 "October", "November", "December"
 };
 return (n < 1 || n > 12) ? name[0] : name[n];
 }
The declaration of name, which is an array of character pointers, is the same as lineptr in the
sorting example. The initializer is a list of character strings; each is assigned to the
corresponding position in the array. The characters of the i-th string are placed somewhere,
and a pointer to them is stored in name[i]. Since the size of the array name is not specified,
the compiler counts the initializers and fills in the correct number.
5.9 Pointers vs. Multi-dimensional Arrays
Newcomers to C are sometimes confused about the difference between a two-dimensional
array and an array of pointers, such as name in the example above. Given the definitions
 int a[10][20];
 int *b[10];
then a[3][4] and b[3][4] are both syntactically legal references to a single int. But a is a
true two-dimensional array: 200 int-sized locations have been set aside, and the conventional
rectangular subscript calculation 20 * row +col is used to find the element a[row,col]. For b,
however, the definition only allocates 10 pointers and does not initialize them; initialization
must be done explicitly, either statically or with code. Assuming that each element of b does
point to a twenty-element array, then there will be 200 ints set aside, plus ten cells for the
pointers. The important advantage of the pointer array is that the rows of the array may be of
different lengths. That is, each element of b need not point to a twenty-element vector; some
may point to two elements, some to fifty, and some to none at all.
Although we have phrased this discussion in terms of integers, by far the most frequent use of
arrays of pointers is to store character strings of diverse lengths, as in the function
month_name. Compare the declaration and picture for an array of pointers:
 char *name[] = { "Illegal month", "Jan", "Feb", "Mar" };
95
with those for a two-dimensional array:
 char aname[][15] = { "Illegal month", "Jan", "Feb", "Mar" };
Exercise 5-9. Rewrite the routines day_of_year and month_day with pointers instead of
indexing.
5.10 Command-line Arguments
In environments that support C, there is a way to pass command-line arguments or parameters
to a program when it begins executing. When main is called, it is called with two arguments.
The first (conventionally called argc, for argument count) is the number of command-line
arguments the program was invoked with; the second (argv, for argument vector) is a pointer
to an array of character strings that contain the arguments, one per string. We customarily use
multiple levels of pointers to manipulate these character strings.
The simplest illustration is the program echo, which echoes its command-line arguments on a
single line, separated by blanks. That is, the command
 echo hello, world
prints the output
 hello, world
By convention, argv[0] is the name by which the program was invoked, so argc is at least 1.
If argc is 1, there are no command-line arguments after the program name. In the example
above, argc is 3, and argv[0], argv[1], and argv[2] are "echo", "hello,", and "world"
respectively. The first optional argument is argv[1] and the last is argv[argc-1];
additionally, the standard requires that argv[argc] be a null pointer.
The first version of echo treats argv as an array of character pointers:
 #include <stdio.h>
 /* echo command-line arguments; 1st version */
 main(int argc, char *argv[])
 {
 int i;
 for (i = 1; i < argc; i++)
 printf("%s%s", argv[i], (i < argc-1) ? " " : "");
 printf("\n");
 return 0;
96
 }
Since argv is a pointer to an array of pointers, we can manipulate the pointer rather than index
the array. This next variant is based on incrementing argv, which is a pointer to pointer to
char, while argc is counted down:
 #include <stdio.h>
 /* echo command-line arguments; 2nd version */
 main(int argc, char *argv[])
 {
 while (--argc > 0)
 printf("%s%s", *++argv, (argc > 1) ? " " : "");
 printf("\n");
 return 0;
 }
Since argv is a pointer to the beginning of the array of argument strings, incrementing it by 1
(++argv) makes it point at the original argv[1] instead of argv[0]. Each successive
increment moves it along to the next argument; *argv is then the pointer to that argument. At
the same time, argc is decremented; when it becomes zero, there are no arguments left to
print.
Alternatively, we could write the printf statement as
 printf((argc > 1) ? "%s " : "%s", *++argv);
This shows that the format argument of printf can be an expression too.
As a second example, let us make some enhancements to the pattern-finding program from
Section 4.1. If you recall, we wired the search pattern deep into the program, an obviously
unsatisfactory arrangement. Following the lead of the UNIX program grep, let us enhance the
program so the pattern to be matched is specified by the first argument on the command line.
 #include <stdio.h>
 #include <string.h>
 #define MAXLINE 1000
 int getline(char *line, int max);
 /* find: print lines that match pattern from 1st arg */
 main(int argc, char *argv[])
 {
 char line[MAXLINE];
 int found = 0;
 if (argc != 2)
 printf("Usage: find pattern\n");
 else
 while (getline(line, MAXLINE) > 0)
 if (strstr(line, argv[1]) != NULL) {
 printf("%s", line);
 found++;
 }
 return found;
 }
The standard library function strstr(s,t) returns a pointer to the first occurrence of the
string t in the string s, or NULL if there is none. It is declared in <string.h>.
The model can now be elaborated to illustrate further pointer constructions. Suppose we want
to allow two optional arguments. One says ``print all the lines except those that match the
pattern;'' the second says ``precede each printed line by its line number.'' 
97
A common convention for C programs on UNIX systems is that an argument that begins with
a minus sign introduces an optional flag or parameter. If we choose -x (for ``except'') to signal
the inversion, and -n (``number'') to request line numbering, then the command
 find -x -npattern
will print each line that doesn't match the pattern, preceded by its line number.
Optional arguments should be permitted in any order, and the rest of the program should be
independent of the number of arguments that we present. Furthermore, it is convenient for
users if option arguments can be combined, as in
 find -nx pattern
Here is the program:
 #include <stdio.h>
 #include <string.h>
 #define MAXLINE 1000
 int getline(char *line, int max);
 /* find: print lines that match pattern from 1st arg */
 main(int argc, char *argv[])
 {
 char line[MAXLINE];
 long lineno = 0;
 int c, except = 0, number = 0, found = 0;
 while (--argc > 0 && (*++argv)[0] == '-')
 while (c = *++argv[0])
 switch (c) {
 case 'x':
 except = 1;
 break;
 case 'n':
 number = 1;
 break;
 default:
 printf("find: illegal option %c\n", c);
 argc = 0;
 found = -1;
 break;
 }
 if (argc != 1)
 printf("Usage: find -x -n pattern\n");
 else
 while (getline(line, MAXLINE) > 0) {
 lineno++;
 if ((strstr(line, *argv) != NULL) != except) {
 if (number)
 printf("%ld:", lineno);
 printf("%s", line);
 found++;
 }
 }
 return found;
 }
argc is decremented and argv is incremented before each optional argument. At the end of the
loop, if there are no errors, argc tells how many arguments remain unprocessed and argv
points to the first of these. Thus argc should be 1 and *argv should point at the pattern.
Notice that *++argv is a pointer to an argument string, so (*++argv)[0] is its first character.
(An alternate valid form would be **++argv.) Because [] binds tighter than * and ++, the
parentheses are necessary; without them the expression would be taken as *++(argv[0]). In
98
fact, that is what we have used in the inner loop, where the task is to walk along a specific
argument string. In the inner loop, the expression *++argv[0] increments the pointer
argv[0]!
It is rare that one uses pointer expressions more complicated than these; in such cases,
breaking them into two or three steps will be more intuitive.
Exercise 5-10. Write the program expr, which evaluates a reverse Polish expression from the
command line, where each operator or operand is a separate argument. For example,
 expr 2 3 4 + *
evaluates 2 * (3+4).
Exercise 5-11. Modify the program entab and detab (written as exercises in Chapter 1) to
accept a list of tab stops as arguments. Use the default tab settings if there are no arguments.
Exercise 5-12. Extend entab and detab to accept the shorthand
 entab -m +n
to mean tab stops every n columns, starting at column m. Choose convenient (for the user)
default behavior.
Exercise 5-13. Write the program tail, which prints the last n lines of its input. By default, n
is set to 10, let us say, but it can be changed by an optional argument so that
 tail -n
prints the last n lines. The program should behave rationally no matter how unreasonable the
input or the value of n. Write the program so it makes the best use of available storage; lines
should be stored as in the sorting program of Section 5.6, not in a two-dimensional array of
fixed size.
5.11 Pointers to Functions
In C, a function itself is not a variable, but it is possible to define pointers to functions, which
can be assigned, placed in arrays, passed to functions, returned by functions, and so on. We
will illustrate this by modifying the sorting procedure written earlier in this chapter so that if
the optional argument -n is given, it will sort the input lines numerically instead of
lexicographically.
A sort often consists of three parts - a comparison that determines the ordering of any pair of
objects, an exchange that reverses their order, and a sorting algorithm that makes comparisons
and exchanges until the objects are in order. The sorting algorithm is independent of the
comparison and exchange operations, so by passing different comparison and exchange
functions to it, we can arrange to sort by different criteria. This is the approach taken in our
new sort.
Lexicographic comparison of two lines is done by strcmp, as before; we will also need a
routine numcmp that compares two lines on the basis of numeric value and returns the same
kind of condition indication as strcmp does. These functions are declared ahead of main and a
pointer to the appropriate one is passed to qsort. We have skimped on error processing for
arguments, so as to concentrate on the main issues.
 #include <stdio.h>
 #include <string.h>
 #define MAXLINES 5000 /* max #lines to be sorted */
 char *lineptr[MAXLINES]; /* pointers to text lines */
99
 int readlines(char *lineptr[], int nlines);
 void writelines(char *lineptr[], int nlines);
 void qsort(void *lineptr[], int left, int right,
 int (*comp)(void *, void *));
 int numcmp(char *, char *);
 /* sort input lines */
 main(int argc, char *argv[])
 {
 int nlines; /* number of input lines read */
 int numeric = 0; /* 1 if numeric sort */
 if (argc > 1 && strcmp(argv[1], "-n") == 0)
 numeric = 1;
 if ((nlines = readlines(lineptr, MAXLINES)) >= 0) {
 qsort((void**) lineptr, 0, nlines-1,
 (int (*)(void*,void*))(numeric ? numcmp : strcmp));
 writelines(lineptr, nlines);
 return 0;
 } else {
 printf("input too big to sort\n");
 return 1;
 }
 }
In the call to qsort, strcmp and numcmp are addresses of functions. Since they are known to
be functions, the & is not necessary, in the same way that it is not needed before an array name.
We have written qsort so it can process any data type, not just character strings. As indicated
by the function prototype, qsort expects an array of pointers, two integers, and a function
with two pointer arguments. The generic pointer type void * is used for the pointer
arguments. Any pointer can be cast to void * and back again without loss of information, so
we can call qsort by casting arguments to void *. The elaborate cast of the function
argument casts the arguments of the comparison function. These will generally have no effect
on actual representation, but assure the compiler that all is well.
 /* qsort: sort v[left]...v[right] into increasing order */
 void qsort(void *v[], int left, int right,
 int (*comp)(void *, void *))
 {
 int i, last;
 void swap(void *v[], int, int);
 if (left >= right) /* do nothing if array contains */
 return; /* fewer than two elements */
 swap(v, left, (left + right)/2);
 last = left;
 for (i = left+1; i <= right; i++)
 if ((*comp)(v[i], v[left]) < 0)
 swap(v, ++last, i);
 swap(v, left, last);
 qsort(v, left, last-1, comp);
 qsort(v, last+1, right, comp);
 }
The declarations should be studied with some care. The fourth parameter of qsort is
 int (*comp)(void *, void *)
which says that comp is a pointer to a function that has two void * arguments and returns an
int.
The use of comp in the line
 if ((*comp)(v[i], v[left]) < 0)
100
is consistent with the declaration: comp is a pointer to a function, *comp is the function, and
 (*comp)(v[i], v[left])
is the call to it. The parentheses are needed so the components are correctly associated;
without them,
 int *comp(void *, void *) /* WRONG */
says that comp is a function returning a pointer to an int, which is very different.
We have already shown strcmp, which compares two strings. Here is numcmp, which
compares two strings on a leading numeric value, computed by calling atof:
 #include <stdlib.h>
 /* numcmp: compare s1 and s2 numerically */
 int numcmp(char *s1, char *s2)
 {
 double v1, v2;
 v1 = atof(s1);
 v2 = atof(s2);
 if (v1 < v2)
 return -1;
 else if (v1 > v2)
 return 1;
 else
 return 0;
 }
The swap function, which exchanges two pointers, is identical to what we presented earlier in
the chapter, except that the declarations are changed to void *.
 void swap(void *v[], int i, int j;)
 {
 void *temp;
 temp = v[i];
 v[i] = v[j];
 v[j] = temp;
 }
A variety of other options can be added to the sorting program; some make challenging
exercises.
Exercise 5-14. Modify the sort program to handle a -r flag, which indicates sorting in reverse
(decreasing) order. Be sure that -r works with -n.
Exercise 5-15. Add the option -f to fold upper and lower case together, so that case
distinctions are not made during sorting; for example, a and A compare equal.
Exercise 5-16. Add the -d (``directory order'') option, which makes comparisons only on
letters, numbers and blanks. Make sure it works in conjunction with -f.
Exercise 5-17. Add a field-searching capability, so sorting may bee done on fields within lines,
each field sorted according to an independent set of options. (The index for this book was
sorted with -df for the index category and -n for the page numbers.)
5.12 Complicated Declarations
C is sometimes castigated for the syntax of its declarations, particularly ones that involve
pointers to functions. The syntax is an attempt to make the declaration and the use agree; it
works well for simple cases, but it can be confusing for the harder ones, because declarations
cannot be read left to right, and because parentheses are over-used. The difference between 
101
 int *f(); /* f: function returning pointer to int */
and
 int (*pf)(); /* pf: pointer to function returning int */
illustrates the problem: * is a prefix operator and it has lower precedence than (), so
parentheses are necessary to force the proper association.
Although truly complicated declarations rarely arise in practice, it is important to know how to
understand them, and, if necessary, how to create them. One good way to synthesize
declarations is in small steps with typedef, which is discussed in Section 6.7. As an
alternative, in this section we will present a pair of programs that convert from valid C to a
word description and back again. The word description reads left to right.
The first, dcl, is the more complex. It converts a C declaration into a word description, as in
these examples:
char **argv
 argv: pointer to char
int (*daytab)[13]
 daytab: pointer to array[13] of int
int *daytab[13]
 daytab: array[13] of pointer to int
void *comp()
 comp: function returning pointer to void
void (*comp)()
 comp: pointer to function returning void
char (*(*x())[])()
 x: function returning pointer to array[] of
 pointer to function returning char
char (*(*x[3])())[5]
 x: array[3] of pointer to function returning
 pointer to array[5] of char
dcl is based on the grammar that specifies a declarator, which is spelled out precisely in
Appendix A, Section 8.5; this is a simplified form:
dcl: optional *'s direct-dcl
direct-dcl name
 (dcl)
 direct-dcl()
 direct-dcl[optional size]
In words, a dcl is a direct-dcl, perhaps preceded by *'s. A direct-dcl is a name, or a
parenthesized dcl, or a direct-dcl followed by parentheses, or a direct-dcl followed by brackets
with an optional size.
This grammar can be used to parse functions. For instance, consider this declarator:
 (*pfa[])()
pfa will be identified as a name and thus as a direct-dcl. Then pfa[] is also a direct-dcl. Then
*pfa[] is recognized as a dcl, so (*pfa[]) is a direct-dcl. Then (*pfa[])() is a direct-dcl
and thus a dcl. We can also illustrate the parse with a tree like this (where direct-dcl has been
abbreviated to dir-dcl): 
102
The heart of the dcl program is a pair of functions, dcl and dirdcl, that parse a declaration
according to this grammar. Because the grammar is recursively defined, the functions call each
other recursively as they recognize pieces of a declaration; the program is called a recursivedescent parser.
 /* dcl: parse a declarator */
 void dcl(void)
 {
 int ns;
 for (ns = 0; gettoken() == '*'; ) /* count *'s */
 ns++;
 dirdcl();
 while (ns-- > 0)
 strcat(out, " pointer to");
 }
 /* dirdcl: parse a direct declarator */
 void dirdcl(void)
 {
 int type;
 if (tokentype == '(') { /* ( dcl ) */
 dcl();
 if (tokentype != ')')
 printf("error: missing )\n");
 } else if (tokentype == NAME) /* variable name */
 strcpy(name, token);
 else
 printf("error: expected name or (dcl)\n");
 while ((type=gettoken()) == PARENS || type == BRACKETS)
103
 if (type == PARENS)
 strcat(out, " function returning");
 else {
 strcat(out, " array");
 strcat(out, token);
 strcat(out, " of");
 }
 }
Since the programs are intended to be illustrative, not bullet-proof, there are significant
restrictions on dcl. It can only handle a simple data type line char or int. It does not handle
argument types in functions, or qualifiers like const. Spurious blanks confuse it. It doesn't do
much error recovery, so invalid declarations will also confuse it. These improvements are left
as exercises.
Here are the global variables and the main routine:
 #include <stdio.h>
 #include <string.h>
 #include <ctype.h>
 #define MAXTOKEN 100
 enum { NAME, PARENS, BRACKETS };
 void dcl(void);
 void dirdcl(void);
 int gettoken(void);
 int tokentype; /* type of last token */
 char token[MAXTOKEN]; /* last token string */
 char name[MAXTOKEN]; /* identifier name */
 char datatype[MAXTOKEN]; /* data type = char, int, etc. */
 char out[1000];
 main() /* convert declaration to words */
 {
 while (gettoken() != EOF) { /* 1st token on line */
 strcpy(datatype, token); /* is the datatype */
 out[0] = '\0';
 dcl(); /* parse rest of line */
 if (tokentype != '\n')
 printf("syntax error\n");
 printf("%s: %s %s\n", name, out, datatype);
 }
 return 0;
 }
The function gettoken skips blanks and tabs, then finds the next token in the input; a ``token''
is a name, a pair of parentheses, a pair of brackets perhaps including a number, or any other
single character.
 int gettoken(void) /* return next token */
 {
 int c, getch(void);
 void ungetch(int);
 char *p = token;
 while ((c = getch()) == ' ' || c == '\t')
 ;
 if (c == '(') {
 if ((c = getch()) == ')') {
 strcpy(token, "()");
 return tokentype = PARENS;
 } else {
 ungetch(c);
 return tokentype = '(';
104
 }
 } else if (c == '[') {
 for (*p++ = c; (*p++ = getch()) != ']'; )
 ;
 *p = '\0';
 return tokentype = BRACKETS;
 } else if (isalpha(c)) {
 for (*p++ = c; isalnum(c = getch()); )
 *p++ = c;
 *p = '\0';
 ungetch(c);
 return tokentype = NAME;
 } else
 return tokentype = c;
 }
getch and ungetch are discussed in Chapter 4.
Going in the other direction is easier, especially if we do not worry about generating redundant
parentheses. The program undcl converts a word description like ``x is a function returning a
pointer to an array of pointers to functions returning char,'' which we will express as
 x () * [] * () char
to
 char (*(*x())[])()
The abbreviated input syntax lets us reuse the gettoken function. undcl also uses the same
external variables as dcl does.
 /* undcl: convert word descriptions to declarations */
 main()
 {
 int type;
 char temp[MAXTOKEN];
 while (gettoken() != EOF) {
 strcpy(out, token);
 while ((type = gettoken()) != '\n')
 if (type == PARENS || type == BRACKETS)
 strcat(out, token);
 else if (type == '*') {
 sprintf(temp, "(*%s)", out);
 strcpy(out, temp);
 } else if (type == NAME) {
 sprintf(temp, "%s %s", token, out);
 strcpy(out, temp);
 } else
 printf("invalid input at %s\n", token);
 }
 return 0;
 }
Exercise 5-18. Make dcl recover from input errors.
Exercise 5-19. Modify undcl so that it does not add redundant parentheses to declarations.
Exercise 5-20. Expand dcl to handle declarations with function argument types, qualifiers like
const, and so on. 
105
Chapter 6 - Structures
A structure is a collection of one or more variables, possibly of different types, grouped
together under a single name for convenient handling. (Structures are called ``records'' in some
languages, notably Pascal.) Structures help to organize complicated data, particularly in large
programs, because they permit a group of related variables to be treated as a unit instead of as
separate entities.
One traditional example of a structure is the payroll record: an employee is described by a set
of attributes such as name, address, social security number, salary, etc. Some of these in turn
could be structures: a name has several components, as does an address and even a salary.
Another example, more typical for C, comes from graphics: a point is a pair of coordinate, a
rectangle is a pair of points, and so on.
The main change made by the ANSI standard is to define structure assignment - structures may
be copied and assigned to, passed to functions, and returned by functions. This has been
supported by most compilers for many years, but the properties are now precisely defined.
Automatic structures and arrays may now also be initialized.
6.1 Basics of Structures
Let us create a few structures suitable for graphics. The basic object is a point, which we will
assume has an x coordinate and a y coordinate, both integers.
The two components can be placed in a structure declared like this:
 struct point {
 int x;
 int y;
 };
The keyword struct introduces a structure declaration, which is a list of declarations enclosed
in braces. An optional name called a structure tag may follow the word struct (as with point
here). The tag names this kind of structure, and can be used subsequently as a shorthand for
the part of the declaration in braces.
The variables named in a structure are called members. A structure member or tag and an
ordinary (i.e., non-member) variable can have the same name without conflict, since they can
always be distinguished by context. Furthermore, the same member names may occur in
different structures, although as a matter of style one would normally use the same names only
for closely related objects.
A struct declaration defines a type. The right brace that terminates the list of members may
be followed by a list of variables, just as for any basic type. That is, 
106
 struct { ... } x, y, z;
is syntactically analogous to
 int x, y, z;
in the sense that each statement declares x, y and z to be variables of the named type and
causes space to be set aside for them.
A structure declaration that is not followed by a list of variables reserves no storage; it merely
describes a template or shape of a structure. If the declaration is tagged, however, the tag can
be used later in definitions of instances of the structure. For example, given the declaration of
point above,
 struct point pt;
defines a variable pt which is a structure of type struct point. A structure can be initialized
by following its definition with a list of initializers, each a constant expression, for the
members:
 struct maxpt = { 320, 200 };
An automatic structure may also be initialized by assignment or by calling a function that
returns a structure of the right type.
A member of a particular structure is referred to in an expression by a construction of the form
structure-name.member
The structure member operator ``.'' connects the structure name and the member name. To
print the coordinates of the point pt, for instance,
 printf("%d,%d", pt.x, pt.y);
or to compute the distance from the origin (0,0) to pt,
 double dist, sqrt(double);
 dist = sqrt((double)pt.x * pt.x + (double)pt.y * pt.y);
Structures can be nested. One representation of a rectangle is a pair of points that denote the
diagonally opposite corners:
 struct rect {
 struct point pt1;
 struct point pt2;
 };
The rect structure contains two point structures. If we declare screen as
 struct rect screen;
then
 screen.pt1.x
107
refers to the x coordinate of the pt1 member of screen.
6.2 Structures and Functions
The only legal operations on a structure are copying it or assigning to it as a unit, taking its
address with &, and accessing its members. Copy and assignment include passing arguments to
functions and returning values from functions as well. Structures may not be compared. A
structure may be initialized by a list of constant member values; an automatic structure may
also be initialized by an assignment.
Let us investigate structures by writing some functions to manipulate points and rectangles.
There are at least three possible approaches: pass components separately, pass an entire
structure, or pass a pointer to it. Each has its good points and bad points.
The first function, makepoint, will take two integers and return a point structure:
 /* makepoint: make a point from x and y components */
 struct point makepoint(int x, int y)
 {
 struct point temp;
 temp.x = x;
 temp.y = y;
 return temp;
 }
Notice that there is no conflict between the argument name and the member with the same
name; indeed the re-use of the names stresses the relationship.
makepoint can now be used to initialize any structure dynamically, or to provide structure
arguments to a function:
 struct rect screen;
 struct point middle;
 struct point makepoint(int, int);
 screen.pt1 = makepoint(0,0);
 screen.pt2 = makepoint(XMAX, YMAX);
 middle = makepoint((screen.pt1.x + screen.pt2.x)/2,
 (screen.pt1.y + screen.pt2.y)/2);
The next step is a set of functions to do arithmetic on points. For instance,
 /* addpoints: add two points */
 struct addpoint(struct point p1, struct point p2)
 {
 p1.x += p2.x;
 p1.y += p2.y;
 return p1;
 }
Here both the arguments and the return value are structures. We incremented the components
in p1 rather than using an explicit temporary variable to emphasize that structure parameters
are passed by value like any others.
As another example, the function ptinrect tests whether a point is inside a rectangle, where
we have adopted the convention that a rectangle includes its left and bottom sides but not its
top and right sides:
 /* ptinrect: return 1 if p in r, 0 if not */
 int ptinrect(struct point p, struct rect r)
 {
 return p.x >= r.pt1.x && p.x < r.pt2.x
 && p.y >= r.pt1.y && p.y < r.pt2.y;
 }
108
This assumes that the rectangle is presented in a standard form where the pt1 coordinates are
less than the pt2 coordinates. The following function returns a rectangle guaranteed to be in
canonical form:
 #define min(a, b) ((a) < (b) ? (a) : (b))
 #define max(a, b) ((a) > (b) ? (a) : (b))
 /* canonrect: canonicalize coordinates of rectangle */
 struct rect canonrect(struct rect r)
 {
 struct rect temp;
 temp.pt1.x = min(r.pt1.x, r.pt2.x);
 temp.pt1.y = min(r.pt1.y, r.pt2.y);
 temp.pt2.x = max(r.pt1.x, r.pt2.x);
 temp.pt2.y = max(r.pt1.y, r.pt2.y);
 return temp;
 }
If a large structure is to be passed to a function, it is generally more efficient to pass a pointer
than to copy the whole structure. Structure pointers are just like pointers to ordinary variables.
The declaration
 struct point *pp;
says that pp is a pointer to a structure of type struct point. If pp points to a point
structure, *pp is the structure, and (*pp).x and (*pp).y are the members. To use pp, we
might write, for example,
 struct point origin, *pp;
 pp = &origin;
 printf("origin is (%d,%d)\n", (*pp).x, (*pp).y);
The parentheses are necessary in (*pp).x because the precedence of the structure member
operator . is higher then *. The expression *pp.x means *(pp.x), which is illegal here
because x is not a pointer.
Pointers to structures are so frequently used that an alternative notation is provided as a
shorthand. If p is a pointer to a structure, then
 p->member-of-structure
refers to the particular member. So we could write instead
 printf("origin is (%d,%d)\n", pp->x, pp->y);
Both . and -> associate from left to right, so if we have
 struct rect r, *rp = &r;
then these four expressions are equivalent:
 r.pt1.x
 rp->pt1.x
 (r.pt1).x
 (rp->pt1).x
The structure operators . and ->, together with () for function calls and [] for subscripts, are
at the top of the precedence hierarchy and thus bind very tightly. For example, given the
declaration
 struct {
 int len;
 char *str;
 } *p;
then 
109
 ++p->len
increments len, not p, because the implied parenthesization is ++(p->len). Parentheses can be
used to alter binding: (++p)->len increments p before accessing len, and (p++)->len
increments p afterward. (This last set of parentheses is unnecessary.)
In the same way, *p->str fetches whatever str points to; *p->str++ increments str after
accessing whatever it points to (just like *s++); (*p->str)++ increments whatever str points
to; and *p++->str increments p after accessing whatever str points to.
6.3 Arrays of Structures
Consider writing a program to count the occurrences of each C keyword. We need an array of
character strings to hold the names, and an array of integers for the counts. One possibility is
to use two parallel arrays, keyword and keycount, as in
 char *keyword[NKEYS];
 int keycount[NKEYS];
But the very fact that the arrays are parallel suggests a different organization, an array of
structures. Each keyword is a pair:
 char *word;
 int cout;
and there is an array of pairs. The structure declaration
 struct key {
 char *word;
 int count;
 } keytab[NKEYS];
declares a structure type key, defines an array keytab of structures of this type, and sets aside
storage for them. Each element of the array is a structure. This could also be written
 struct key {
 char *word;
 int count;
 };
 struct key keytab[NKEYS];
Since the structure keytab contains a constant set of names, it is easiest to make it an external
variable and initialize it once and for all when it is defined. The structure initialization is
analogous to earlier ones - the definition is followed by a list of initializers enclosed in braces:
 struct key {
 char *word;
 int count;
 } keytab[] = {
 "auto", 0,
 "break", 0,
 "case", 0,
 "char", 0,
 "const", 0,
 "continue", 0,
 "default", 0,
 /* ... */
 "unsigned", 0,
 "void", 0,
 "volatile", 0,
 "while", 0
 };
The initializers are listed in pairs corresponding to the structure members. It would be more
precise to enclose the initializers for each "row" or structure in braces, as in
 { "auto", 0 },
110
 { "break", 0 },
 { "case", 0 },
 ...
but inner braces are not necessary when the initializers are simple variables or character strings,
and when all are present. As usual, the number of entries in the array keytab will be computed
if the initializers are present and the [] is left empty.
The keyword counting program begins with the definition of keytab. The main routine reads
the input by repeatedly calling a function getword that fetches one word at a time. Each word
is looked up in keytab with a version of the binary search function that we wrote in Chapter 3.
The list of keywords must be sorted in increasing order in the table.
 #include <stdio.h>
 #include <ctype.h>
 #include <string.h>
 #define MAXWORD 100
 int getword(char *, int);
 int binsearch(char *, struct key *, int);
 /* count C keywords */
 main()
 {
 int n;
 char word[MAXWORD];
 while (getword(word, MAXWORD) != EOF)
 if (isalpha(word[0]))
 if ((n = binsearch(word, keytab, NKEYS)) >= 0)
 keytab[n].count++;
 for (n = 0; n < NKEYS; n++)
 if (keytab[n].count > 0)
 printf("%4d %s\n",
 keytab[n].count, keytab[n].word);
 return 0;
 }
 /* binsearch: find word in tab[0]...tab[n-1] */
 int binsearch(char *word, struct key tab[], int n)
 {
 int cond;
 int low, high, mid;
 low = 0;
 high = n - 1;
 while (low <= high) {
 mid = (low+high) / 2;
 if ((cond = strcmp(word, tab[mid].word)) < 0)
 high = mid - 1;
 else if (cond > 0)
 low = mid + 1;
 else
 return mid;
 }
 return -1;
 }
We will show the function getword in a moment; for now it suffices to say that each call to
getword finds a word, which is copied into the array named as its first argument.
The quantity NKEYS is the number of keywords in keytab. Although we could count this by
hand, it's a lot easier and safer to do it by machine, especially if the list is subject to change.
One possibility would be to terminate the list of initializers with a null pointer, then loop along
keytab until the end is found. 
111
But this is more than is needed, since the size of the array is completely determined at compile
time. The size of the array is the size of one entry times the number of entries, so the number
of entries is just
size of keytab / size of struct key
C provides a compile-time unary operator called sizeof that can be used to compute the size
of any object. The expressions
 sizeof object
and
 sizeof (type name)
yield an integer equal to the size of the specified object or type in bytes. (Strictly, sizeof
produces an unsigned integer value whose type, size_t, is defined in the header <stddef.h>.)
An object can be a variable or array or structure. A type name can be the name of a basic type
like int or double, or a derived type like a structure or a pointer.
In our case, the number of keywords is the size of the array divided by the size of one element.
This computation is used in a #define statement to set the value of NKEYS:
 #define NKEYS (sizeof keytab / sizeof(struct key))
Another way to write this is to divide the array size by the size of a specific element:
 #define NKEYS (sizeof keytab / sizeof(keytab[0]))
This has the advantage that it does not need to be changed if the type changes.
A sizeof can not be used in a #if line, because the preprocessor does not parse type names.
But the expression in the #define is not evaluated by the preprocessor, so the code here is
legal.
Now for the function getword. We have written a more general getword than is necessary for
this program, but it is not complicated. getword fetches the next ``word'' from the input,
where a word is either a string of letters and digits beginning with a letter, or a single nonwhite space character. The function value is the first character of the word, or EOF for end of
file, or the character itself if it is not alphabetic.
 /* getword: get next word or character from input */
 int getword(char *word, int lim)
 {
 int c, getch(void);
 void ungetch(int);
 char *w = word;
 while (isspace(c = getch()))
 ;
 if (c != EOF)
 *w++ = c;
 if (!isalpha(c)) {
 *w = '\0';
 return c;
 }
 for ( ; --lim > 0; w++)
 if (!isalnum(*w = getch())) {
 ungetch(*w);
 break;
 }
 *w = '\0';
 return word[0];
 }
112
getword uses the getch and ungetch that we wrote in Chapter 4. When the collection of an
alphanumeric token stops, getword has gone one character too far. The call to ungetch
pushes that character back on the input for the next call. getword also uses isspace to skip
whitespace, isalpha to identify letters, and isalnum to identify letters and digits; all are from
the standard header <ctype.h>.
Exercise 6-1. Our version of getword does not properly handle underscores, string constants,
comments, or preprocessor control lines. Write a better version.
6.4 Pointers to Structures
To illustrate some of the considerations involved with pointers to and arrays of structures, let
us write the keyword-counting program again, this time using pointers instead of array indices.
The external declaration of keytab need not change, but main and binsearch do need
modification.
 #include <stdio.h>
 #include <ctype.h>
 #include <string.h>
 #define MAXWORD 100
 int getword(char *, int);
 struct key *binsearch(char *, struct key *, int);
 /* count C keywords; pointer version */
 main()
 {
 char word[MAXWORD];
 struct key *p;
 while (getword(word, MAXWORD) != EOF)
 if (isalpha(word[0]))
 if ((p=binsearch(word, keytab, NKEYS)) != NULL)
 p->count++;
 for (p = keytab; p < keytab + NKEYS; p++)
 if (p->count > 0)
 printf("%4d %s\n", p->count, p->word);
 return 0;
 }
 /* binsearch: find word in tab[0]...tab[n-1] */
 struct key *binsearch(char *word, struck key *tab, int n)
 {
 int cond;
 struct key *low = &tab[0];
 struct key *high = &tab[n];
 struct key *mid;
 while (low < high) {
 mid = low + (high-low) / 2;
 if ((cond = strcmp(word, mid->word)) < 0)
 high = mid;
 else if (cond > 0)
 low = mid + 1;
 else
 return mid;
 }
 return NULL;
 }
There are several things worthy of note here. First, the declaration of binsearch must indicate
that it returns a pointer to struct key instead of an integer; this is declared both in the
113
function prototype and in binsearch. If binsearch finds the word, it returns a pointer to it; if
it fails, it returns NULL.
Second, the elements of keytab are now accessed by pointers. This requires significant
changes in binsearch.
The initializers for low and high are now pointers to the beginning and just past the end of the
table.
The computation of the middle element can no longer be simply
 mid = (low+high) / 2 /* WRONG */
because the addition of pointers is illegal. Subtraction is legal, however, so high-low is the
number of elements, and thus
 mid = low + (high-low) / 2
sets mid to the element halfway between low and high.
The most important change is to adjust the algorithm to make sure that it does not generate an
illegal pointer or attempt to access an element outside the array. The problem is that &tab[-1]
and &tab[n] are both outside the limits of the array tab. The former is strictly illegal, and it is
illegal to dereference the latter. The language definition does guarantee, however, that pointer
arithmetic that involves the first element beyond the end of an array (that is, &tab[n]) will
work correctly.
In main we wrote
 for (p = keytab; p < keytab + NKEYS; p++)
If p is a pointer to a structure, arithmetic on p takes into account the size of the structure, so
p++ increments p by the correct amount to get the next element of the array of structures, and
the test stops the loop at the right time.
Don't assume, however, that the size of a structure is the sum of the sizes of its members.
Because of alignment requirements for different objects, there may be unnamed ``holes'' in a
structure. Thus, for instance, if a char is one byte and an int four bytes, the structure
 struct {
 char c;
 int i;
 };
might well require eight bytes, not five. The sizeof operator returns the proper value.
Finally, an aside on program format: when a function returns a complicated type like a
structure pointer, as in
 struct key *binsearch(char *word, struct key *tab, int n)
the function name can be hard to see, and to find with a text editor. Accordingly an alternate
style is sometimes used:
 struct key *
 binsearch(char *word, struct key *tab, int n)
This is a matter of personal taste; pick the form you like and hold to it.
6.5 Self-referential Structures
Suppose we want to handle the more general problem of counting the occurrences of all the
words in some input. Since the list of words isn't known in advance, we can't conveniently sort
it and use a binary search. Yet we can't do a linear search for each word as it arrives, to see if
it's already been seen; the program would take too long. (More precisely, its running time is
114
likely to grow quadratically with the number of input words.) How can we organize the data to
copy efficiently with a list or arbitrary words?
One solution is to keep the set of words seen so far sorted at all times, by placing each word
into its proper position in the order as it arrives. This shouldn't be done by shifting words in a
linear array, though - that also takes too long. Instead we will use a data structure called a
binary tree.
The tree contains one ``node'' per distinct word; each node contains
• A pointer to the text of the word,
• A count of the number of occurrences,
• A pointer to the left child node,
• A pointer to the right child node.
No node may have more than two children; it might have only zero or one.
The nodes are maintained so that at any node the left subtree contains only words that are
lexicographically less than the word at the node, and the right subtree contains only words that
are greater. This is the tree for the sentence ``now is the time for all good men to come to the
aid of their party'', as built by inserting each word as it is encountered:
To find out whether a new word is already in the tree, start at the root and compare the new
word to the word stored at that node. If they match, the question is answered affirmatively. If
the new record is less than the tree word, continue searching at the left child, otherwise at the
right child. If there is no child in the required direction, the new word is not in the tree, and in
fact the empty slot is the proper place to add the new word. This process is recursive, since the
search from any node uses a search from one of its children. Accordingly, recursive routines
for insertion and printing will be most natural.
Going back to the description of a node, it is most conveniently represented as a structure with
four components:
 struct tnode { /* the tree node: */
 char *word; /* points to the text */
 int count; /* number of occurrences */
 struct tnode *left; /* left child */
 struct tnode *right; /* right child */
 };
This recursive declaration of a node might look chancy, but it's correct. It is illegal for a
structure to contain an instance of itself, but 
115
 struct tnode *left;
declares left to be a pointer to a tnode, not a tnode itself.
Occasionally, one needs a variation of self-referential structures: two structures that refer to
each other. The way to handle this is:
 struct t {
 ...
 struct s *p; /* p points to an s */
 };
 struct s {
 ...
 struct t *q; /* q points to a t */
 };
The code for the whole program is surprisingly small, given a handful of supporting routines
like getword that we have already written. The main routine reads words with getword and
installs them in the tree with addtree.
 #include <stdio.h>
 #include <ctype.h>
 #include <string.h>
 #define MAXWORD 100
 struct tnode *addtree(struct tnode *, char *);
 void treeprint(struct tnode *);
 int getword(char *, int);
 /* word frequency count */
 main()
 {
 struct tnode *root;
 char word[MAXWORD];
 root = NULL;
 while (getword(word, MAXWORD) != EOF)
 if (isalpha(word[0]))
 root = addtree(root, word);
 treeprint(root);
 return 0;
 }
The function addtree is recursive. A word is presented by main to the top level (the root) of
the tree. At each stage, that word is compared to the word already stored at the node, and is
percolated down to either the left or right subtree by a recursive call to adtree. Eventually,
the word either matches something already in the tree (in which case the count is incremented),
or a null pointer is encountered, indicating that a node must be created and added to the tree. If
a new node is created, addtree returns a pointer to it, which is installed in the parent node.
 struct tnode *talloc(void);
 char *strdup(char *);
 /* addtree: add a node with w, at or below p */
 struct treenode *addtree(struct tnode *p, char *w)
 {
 int cond;
 if (p == NULL) { /* a new word has arrived */
 p = talloc(); /* make a new node */
 p->word = strdup(w);
 p->count = 1;
 p->left = p->right = NULL;
 } else if ((cond = strcmp(w, p->word)) == 0)
 p->count++; /* repeated word */
 else if (cond < 0) /* less than into left subtree */
 p->left = addtree(p->left, w);
116
 else /* greater than into right subtree */
 p->right = addtree(p->right, w);
 return p;
 }
Storage for the new node is fetched by a routine talloc, which returns a pointer to a free
space suitable for holding a tree node, and the new word is copied into a hidden space by
strdup. (We will discuss these routines in a moment.) The count is initialized, and the two
children are made null. This part of the code is executed only at the leaves of the tree, when a
new node is being added. We have (unwisely) omitted error checking on the values returned by
strdup and talloc.
treeprint prints the tree in sorted order; at each node, it prints the left subtree (all the words
less than this word), then the word itself, then the right subtree (all the words greater). If you
feel shaky about how recursion works, simulate treeprint as it operates on the tree shown
above.
 /* treeprint: in-order print of tree p */
 void treeprint(struct tnode *p)
 {
 if (p != NULL) {
 treeprint(p->left);
 printf("%4d %s\n", p->count, p->word);
 treeprint(p->right);
 }
 }
A practical note: if the tree becomes ``unbalanced'' because the words don't arrive in random
order, the running time of the program can grow too much. As a worst case, if the words are
already in order, this program does an expensive simulation of linear search. There are
generalizations of the binary tree that do not suffer from this worst-case behavior, but we will
not describe them here.
Before leaving this example, it is also worth a brief digression on a problem related to storage
allocators. Clearly it's desirable that there be only one storage allocator in a program, even
though it allocates different kinds of objects. But if one allocator is to process requests for,
say, pointers to chars and pointers to struct tnodes, two questions arise. First, how does it
meet the requirement of most real machines that objects of certain types must satisfy alignment
restrictions (for example, integers often must be located at even addresses)? Second, what
declarations can cope with the fact that an allocator must necessarily return different kinds of
pointers?
Alignment requirements can generally be satisfied easily, at the cost of some wasted space, by
ensuring that the allocator always returns a pointer that meets all alignment restrictions. The
alloc of Chapter 5 does not guarantee any particular alignment, so we will use the standard
library function malloc, which does. In Chapter 8 we will show one way to implement
malloc.
The question of the type declaration for a function like malloc is a vexing one for any
language that takes its type-checking seriously. In C, the proper method is to declare that
malloc returns a pointer to void, then explicitly coerce the pointer into the desired type with a
cast. malloc and related routines are declared in the standard header <stdlib.h>. Thus
talloc can be written as
 #include <stdlib.h>
 /* talloc: make a tnode */
 struct tnode *talloc(void)
 {
 return (struct tnode *) malloc(sizeof(struct tnode));
117
 }
strdup merely copies the string given by its argument into a safe place, obtained by a call on
malloc:
 char *strdup(char *s) /* make a duplicate of s */
 {
 char *p;
 p = (char *) malloc(strlen(s)+1); /* +1 for '\0' */
 if (p != NULL)
 strcpy(p, s);
 return p;
 }
malloc returns NULL if no space is available; strdup passes that value on, leaving errorhandling to its caller.
Storage obtained by calling malloc may be freed for re-use by calling free; see Chapters 8
and 7.
Exercise 6-2. Write a program that reads a C program and prints in alphabetical order each
group of variable names that are identical in the first 6 characters, but different somewhere
thereafter. Don't count words within strings and comments. Make 6 a parameter that can be set
from the command line.
Exercise 6-3. Write a cross-referencer that prints a list of all words in a document, and for
each word, a list of the line numbers on which it occurs. Remove noise words like ``the,''
``and,'' and so on.
Exercise 6-4. Write a program that prints the distinct words in its input sorted into decreasing
order of frequency of occurrence. Precede each word by its count.
6.6 Table Lookup
In this section we will write the innards of a table-lookup package, to illustrate more aspects of
structures. This code is typical of what might be found in the symbol table management
routines of a macro processor or a compiler. For example, consider the #define statement.
When a line like
 #define IN 1
is encountered, the name IN and the replacement text 1 are stored in a table. Later, when the
name IN appears in a statement like
 state = IN;
it must be replaced by 1.
There are two routines that manipulate the names and replacement texts. install(s,t)
records the name s and the replacement text t in a table; s and t are just character strings.
lookup(s) searches for s in the table, and returns a pointer to the place where it was found, or
NULL if it wasn't there.
The algorithm is a hash-search - the incoming name is converted into a small non-negative
integer, which is then used to index into an array of pointers. An array element points to the
beginning of a linked list of blocks describing names that have that hash value. It is NULL if no
names have hashed to that value. 
118
A block in the list is a structure containing pointers to the name, the replacement text, and the
next block in the list. A null next-pointer marks the end of the list.
 struct nlist { /* table entry: */
 struct nlist *next; /* next entry in chain */
 char *name; /* defined name */
 char *defn; /* replacement text */
 };
The pointer array is just
 #define HASHSIZE 101
 static struct nlist *hashtab[HASHSIZE]; /* pointer table */
The hashing function, which is used by both lookup and install, adds each character value in
the string to a scrambled combination of the previous ones and returns the remainder modulo
the array size. This is not the best possible hash function, but it is short and effective.
 /* hash: form hash value for string s */
 unsigned hash(char *s)
 {
 unsigned hashval;
 for (hashval = 0; *s != '\0'; s++)
 hashval = *s + 31 * hashval;
 return hashval % HASHSIZE;
 }
Unsigned arithmetic ensures that the hash value is non-negative.
The hashing process produces a starting index in the array hashtab; if the string is to be found
anywhere, it will be in the list of blocks beginning there. The search is performed by lookup. If
lookup finds the entry already present, it returns a pointer to it; if not, it returns NULL.
 /* lookup: look for s in hashtab */
 struct nlist *lookup(char *s)
 {
 struct nlist *np;
 for (np = hashtab[hash(s)]; np != NULL; np = np->next)
 if (strcmp(s, np->name) == 0)
 return np; /* found */
 return NULL; /* not found */
 }
The for loop in lookup is the standard idiom for walking along a linked list:
 for (ptr = head; ptr != NULL; ptr = ptr->next)
 ...
install uses lookup to determine whether the name being installed is already present; if so,
the new definition will supersede the old one. Otherwise, a new entry is created. install
returns NULL if for any reason there is no room for a new entry. 
119
 struct nlist *lookup(char *);
 char *strdup(char *);
 /* install: put (name, defn) in hashtab */
 struct nlist *install(char *name, char *defn)
 {
 struct nlist *np;
 unsigned hashval;
 if ((np = lookup(name)) == NULL) { /* not found */
 np = (struct nlist *) malloc(sizeof(*np));
 if (np == NULL || (np->name = strdup(name)) == NULL)
 return NULL;
 hashval = hash(name);
 np->next = hashtab[hashval];
 hashtab[hashval] = np;
 } else /* already there */
 free((void *) np->defn); /*free previous defn */
 if ((np->defn = strdup(defn)) == NULL)
 return NULL;
 return np;
 }
Exercise 6-5. Write a function undef that will remove a name and definition from the table
maintained by lookup and install.
Exercise 6-6. Implement a simple version of the #define processor (i.e., no arguments)
suitable for use with C programs, based on the routines of this section. You may also find
getch and ungetch helpful.
6.7 Typedef
C provides a facility called typedef for creating new data type names. For example, the
declaration
 typedef int Length;
makes the name Length a synonym for int. The type Length can be used in declarations,
casts, etc., in exactly the same ways that the int type can be:
 Length len, maxlen;
 Length *lengths[];
Similarly, the declaration
 typedef char *String;
makes String a synonym for char * or character pointer, which may then be used in
declarations and casts:
 String p, lineptr[MAXLINES], alloc(int);
 int strcmp(String, String);
 p = (String) malloc(100);
Notice that the type being declared in a typedef appears in the position of a variable name,
not right after the word typedef. Syntactically, typedef is like the storage classes extern,
static, etc. We have used capitalized names for typedefs, to make them stand out.
As a more complicated example, we could make typedefs for the tree nodes shown earlier in
this chapter:
 typedef struct tnode *Treeptr;
 typedef struct tnode { /* the tree node: */
 char *word; /* points to the text */
 int count; /* number of occurrences */
 struct tnode *left; /* left child */
120
 struct tnode *right; /* right child */
 } Treenode;
This creates two new type keywords called Treenode (a structure) and Treeptr (a pointer to
the structure). Then the routine talloc could become
 Treeptr talloc(void)
 {
 return (Treeptr) malloc(sizeof(Treenode));
 }
It must be emphasized that a typedef declaration does not create a new type in any sense; it
merely adds a new name for some existing type. Nor are there any new semantics: variables
declared this way have exactly the same properties as variables whose declarations are spelled
out explicitly. In effect, typedef is like #define, except that since it is interpreted by the
compiler, it can cope with textual substitutions that are beyond the capabilities of the
preprocessor. For example,
 typedef int (*PFI)(char *, char *);
creates the type PFI, for ``pointer to function (of two char * arguments) returning int,''
which can be used in contexts like
 PFI strcmp, numcmp;
in the sort program of Chapter 5.
Besides purely aesthetic issues, there are two main reasons for using typedefs. The first is to
parameterize a program against portability problems. If typedefs are used for data types that
may be machine-dependent, only the typedefs need change when the program is moved. One
common situation is to use typedef names for various integer quantities, then make an
appropriate set of choices of short, int, and long for each host machine. Types like size_t
and ptrdiff_t from the standard library are examples.
The second purpose of typedefs is to provide better documentation for a program - a type
called Treeptr may be easier to understand than one declared only as a pointer to a
complicated structure.
6.8 Unions
A union is a variable that may hold (at different times) objects of different types and sizes, with
the compiler keeping track of size and alignment requirements. Unions provide a way to
manipulate different kinds of data in a single area of storage, without embedding any machinedependent information in the program. They are analogous to variant records in pascal.
As an example such as might be found in a compiler symbol table manager, suppose that a
constant may be an int, a float, or a character pointer. The value of a particular constant
must be stored in a variable of the proper type, yet it is most convenient for table management
if the value occupies the same amount of storage and is stored in the same place regardless of
its type. This is the purpose of a union - a single variable that can legitimately hold any of one
of several types. The syntax is based on structures:
 union u_tag {
 int ival;
 float fval;
 char *sval;
 } u;
The variable u will be large enough to hold the largest of the three types; the specific size is
implementation-dependent. Any of these types may be assigned to u and then used in
expressions, so long as the usage is consistent: the type retrieved must be the type most
recently stored. It is the programmer's responsibility to keep track of which type is currently
121
stored in a union; the results are implementation-dependent if something is stored as one type
and extracted as another.
Syntactically, members of a union are accessed as
union-name.member
or
union-pointer->member
just as for structures. If the variable utype is used to keep track of the current type stored in u,
then one might see code such as
 if (utype == INT)
 printf("%d\n", u.ival);
 if (utype == FLOAT)
 printf("%f\n", u.fval);
 if (utype == STRING)
 printf("%s\n", u.sval);
 else
 printf("bad type %d in utype\n", utype);
Unions may occur within structures and arrays, and vice versa. The notation for accessing a
member of a union in a structure (or vice versa) is identical to that for nested structures. For
example, in the structure array defined by
 struct {
 char *name;
 int flags;
 int utype;
 union {
 int ival;
 float fval;
 char *sval;
 } u;
 } symtab[NSYM];
the member ival is referred to as
 symtab[i].u.ival
and the first character of the string sval by either of
 *symtab[i].u.sval
 symtab[i].u.sval[0]
In effect, a union is a structure in which all members have offset zero from the base, the
structure is big enough to hold the ``widest'' member, and the alignment is appropriate for all
of the types in the union. The same operations are permitted on unions as on structures:
assignment to or copying as a unit, taking the address, and accessing a member.
A union may only be initialized with a value of the type of its first member; thus union u
described above can only be initialized with an integer value.
The storage allocator in Chapter 8 shows how a union can be used to force a variable to be
aligned on a particular kind of storage boundary.
6.9 Bit-fields
When storage space is at a premium, it may be necessary to pack several objects into a single
machine word; one common use is a set of single-bit flags in applications like compiler symbol
tables. Externally-imposed data formats, such as interfaces to hardware devices, also often
require the ability to get at pieces of a word. 
122
Imagine a fragment of a compiler that manipulates a symbol table. Each identifier in a program
has certain information associated with it, for example, whether or not it is a keyword, whether
or not it is external and/or static, and so on. The most compact way to encode such
information is a set of one-bit flags in a single char or int.
The usual way this is done is to define a set of ``masks'' corresponding to the relevant bit
positions, as in
 #define KEYWORD 01
 #define EXTRENAL 02
 #define STATIC 04
or
 enum { KEYWORD = 01, EXTERNAL = 02, STATIC = 04 };
The numbers must be powers of two. Then accessing the bits becomes a matter of ``bitfiddling'' with the shifting, masking, and complementing operators that were described in
Chapter 2.
Certain idioms appear frequently:
 flags |= EXTERNAL | STATIC;
turns on the EXTERNAL and STATIC bits in flags, while
 flags &= ~(EXTERNAL | STATIC);
turns them off, and
 if ((flags & (EXTERNAL | STATIC)) == 0) ...
is true if both bits are off.
Although these idioms are readily mastered, as an alternative C offers the capability of defining
and accessing fields within a word directly rather than by bitwise logical operators. A bit-field,
or field for short, is a set of adjacent bits within a single implementation-defined storage unit
that we will call a ``word.'' For example, the symbol table #defines above could be replaced
by the definition of three fields:
 struct {
 unsigned int is_keyword : 1;
 unsigned int is_extern : 1;
 unsigned int is_static : 1;
 } flags;
This defines a variable table called flags that contains three 1-bit fields. The number following
the colon represents the field width in bits. The fields are declared unsigned int to ensure
that they are unsigned quantities.
Individual fields are referenced in the same way as other structure members:
flags.is_keyword, flags.is_extern, etc. Fields behave like small integers, and may
participate in arithmetic expressions just like other integers. Thus the previous examples may
be written more naturally as
 flags.is_extern = flags.is_static = 1;
to turn the bits on;
 flags.is_extern = flags.is_static = 0;
to turn them off; and
 if (flags.is_extern == 0 && flags.is_static == 0)
 ...
to test them. 
123
Almost everything about fields is implementation-dependent. Whether a field may overlap a
word boundary is implementation-defined. Fields need not be names; unnamed fields (a colon
and width only) are used for padding. The special width 0 may be used to force alignment at
the next word boundary.
Fields are assigned left to right on some machines and right to left on others. This means that
although fields are useful for maintaining internally-defined data structures, the question of
which end comes first has to be carefully considered when picking apart externally-defined
data; programs that depend on such things are not portable. Fields may be declared only as
ints; for portability, specify signed or unsigned explicitly. They are not arrays and they do
not have addresses, so the & operator cannot be applied on them. 
124
Chapter 7 - Input and Output
Input and output are not part of the C language itself, so we have not emphasized them in our
presentation thus far. Nonetheless, programs interact with their environment in much more
complicated ways than those we have shown before. In this chapter we will describe the
standard library, a set of functions that provide input and output, string handling, storage
management, mathematical routines, and a variety of other services for C programs. We will
concentrate on input and output
The ANSI standard defines these library functions precisely, so that they can exist in
compatible form on any system where C exists. Programs that confine their system interactions
to facilities provided by the standard library can be moved from one system to another without
change.
The properties of library functions are specified in more than a dozen headers; we have already
seen several of these, including <stdio.h>, <string.h>, and <ctype.h>. We will not present
the entire library here, since we are more interested in writing C programs that use it. The
library is described in detail in Appendix B.
7.1 Standard Input and Output
As we said in Chapter 1, the library implements a simple model of text input and output. A text
stream consists of a sequence of lines; each line ends with a newline character. If the system
doesn't operate that way, the library does whatever necessary to make it appear as if it does.
For instance, the library might convert carriage return and linefeed to newline on input and
back again on output.
The simplest input mechanism is to read one character at a time from the standard input,
normally the keyboard, with getchar:
 int getchar(void)
getchar returns the next input character each time it is called, or EOF when it encounters end
of file. The symbolic constant EOF is defined in <stdio.h>. The value is typically -1, bus tests
should be written in terms of EOF so as to be independent of the specific value.
In many environments, a file may be substituted for the keyboard by using the < convention for
input redirection: if a program prog uses getchar, then the command line
 prog <infile
causes prog to read characters from infile instead. The switching of the input is done in such
a way that prog itself is oblivious to the change; in particular, the string ``<infile'' is not
included in the command-line arguments in argv. Input switching is also invisible if the input
comes from another program via a pipe mechanism: on some systems, the command line
 otherprog | prog
runs the two programs otherprog and prog, and pipes the standard output of otherprog into
the standard input for prog.
The function
 int putchar(int)
is used for output: putchar(c) puts the character c on the standard output, which is by
default the screen. putchar returns the character written, or EOF is an error occurs. Again,
output can usually be directed to a file with >filename: if prog uses putchar, 
125
 prog >outfile
will write the standard output to outfile instead. If pipes are supported,
 prog | anotherprog
puts the standard output of prog into the standard input of anotherprog.
Output produced by printf also finds its way to the standard output. Calls to putchar and
printf may be interleaved - output happens in the order in which the calls are made.
Each source file that refers to an input/output library function must contain the line
 #include <stdio.h>
before the first reference. When the name is bracketed by < and > a search is made for the
header in a standard set of places (for example, on UNIX systems, typically in the directory
/usr/include).
Many programs read only one input stream and write only one output stream; for such
programs, input and output with getchar, putchar, and printf may be entirely adequate,
and is certainly enough to get started. This is particularly true if redirection is used to connect
the output of one program to the input of the next. For example, consider the program lower,
which converts its input to lower case:
 #include <stdio.h>
 #include <ctype.h>
 main() /* lower: convert input to lower case*/
 {
 int c
 while ((c = getchar()) != EOF)
 putchar(tolower(c));
 return 0;
 }
The function tolower is defined in <ctype.h>; it converts an upper case letter to lower case,
and returns other characters untouched. As we mentioned earlier, ``functions'' like getchar
and putchar in <stdio.h> and tolower in <ctype.h> are often macros, thus avoiding the
overhead of a function call per character. We will show how this is done in Section 8.5.
Regardless of how the <ctype.h> functions are implemented on a given machine, programs
that use them are shielded from knowledge of the character set.
Exercise 7-1. Write a program that converts upper case to lower or lower case to upper,
depending on the name it is invoked with, as found in argv[0].
7.2 Formatted Output - printf
The output function printf translates internal values to characters. We have used printf
informally in previous chapters. The description here covers most typical uses but is not
complete; for the full story, see Appendix B.
 int printf(char *format, arg1, arg2, ...);
printf converts, formats, and prints its arguments on the standard output under control of the
format. It returns the number of characters printed.
The format string contains two types of objects: ordinary characters, which are copied to the
output stream, and conversion specifications, each of which causes conversion and printing of
the next successive argument to printf. Each conversion specification begins with a % and
ends with a conversion character. Between the % and the conversion character there may be,
in order: 
126
• A minus sign, which specifies left adjustment of the converted argument.
• A number that specifies the minimum field width. The converted argument will be
printed in a field at least this wide. If necessary it will be padded on the left (or right, if
left adjustment is called for) to make up the field width.
• A period, which separates the field width from the precision.
• A number, the precision, that specifies the maximum number of characters to be printed
from a string, or the number of digits after the decimal point of a floating-point value,
or the minimum number of digits for an integer.
• An h if the integer is to be printed as a short, or l (letter ell) if as a long.
Conversion characters are shown in Table 7.1. If the character after the % is not a conversion
specification, the behavior is undefined.
Table 7.1 Basic Printf Conversions
Character Argument type; Printed As
d,i int; decimal number
o int; unsigned octal number (without a leading zero)
x,X int; unsigned hexadecimal number (without a leading 0x or 0X), using abcdef or
ABCDEF for 10, ...,15.
u int; unsigned decimal number
c int; single character
s
char *; print characters from the string until a '\0' or the number of characters
given by the precision.
f
double; [-]m.dddddd, where the number of d's is given by the precision (default
6).
e,E double; [-]m.dddddde+/-xx or [-]m.ddddddE+/-xx, where the number of d's
is given by the precision (default 6).
g,G
double; use %e or %E if the exponent is less than -4 or greater than or equal to the
precision; otherwise use %f. Trailing zeros and a trailing decimal point are not
printed.
p void *; pointer (implementation-dependent representation).
% no argument is converted; print a %
A width or precision may be specified as *, in which case the value is computed by converting
the next argument (which must be an int). For example, to print at most max characters from
a string s,
 printf("%.*s", max, s);
Most of the format conversions have been illustrated in earlier chapters. One exception is the
precision as it relates to strings. The following table shows the effect of a variety of
specifications in printing ``hello, world'' (12 characters). We have put colons around each field
so you can see it extent.
 :%s: :hello, world:
 :%10s: :hello, world:
 :%.10s: :hello, wor:
 :%-10s: :hello, world:
 :%.15s: :hello, world:
 :%-15s: :hello, world :
 :%15.10s: : hello, wor:
 :%-15.10s: :hello, wor :
127
A warning: printf uses its first argument to decide how many arguments follow and what
their type is. It will get confused, and you will get wrong answers, if there are not enough
arguments of if they are the wrong type. You should also be aware of the difference between
these two calls:
 printf(s); /* FAILS if s contains % */
 printf("%s", s); /* SAFE */
The function sprintf does the same conversions as printf does, but stores the output in a
string:
 int sprintf(char *string, char *format, arg1, arg2, ...);
sprintf formats the arguments in arg1, arg2, etc., according to format as before, but places
the result in string instead of the standard output; string must be big enough to receive the
result.
Exercise 7-2. Write a program that will print arbitrary input in a sensible way. As a minimum,
it should print non-graphic characters in octal or hexadecimal according to local custom, and
break long text lines.
7.3 Variable-length Argument Lists
This section contains an implementation of a minimal version of printf, to show how to write
a function that processes a variable-length argument list in a portable way. Since we are mainly
interested in the argument processing, minprintf will process the format string and arguments
but will call the real printf to do the format conversions.
The proper declaration for printf is
 int printf(char *fmt, ...)
where the declaration ... means that the number and types of these arguments may vary. The
declaration ... can only appear at the end of an argument list. Our minprintf is declared as
 void minprintf(char *fmt, ...)
since we will not return the character count that printf does.
The tricky bit is how minprintf walks along the argument list when the list doesn't even have
a name. The standard header <stdarg.h> contains a set of macro definitions that define how
to step through an argument list. The implementation of this header will vary from machine to
machine, but the interface it presents is uniform.
The type va_list is used to declare a variable that will refer to each argument in turn; in
minprintf, this variable is called ap, for ``argument pointer.'' The macro va_start initializes
ap to point to the first unnamed argument. It must be called once before ap is used. There
must be at least one named argument; the final named argument is used by va_start to get
started.
Each call of va_arg returns one argument and steps ap to the next; va_arg uses a type name
to determine what type to return and how big a step to take. Finally, va_end does whatever
cleanup is necessary. It must be called before the program returns.
These properties form the basis of our simplified printf:
 #include <stdarg.h>
 /* minprintf: minimal printf with variable argument list */
 void minprintf(char *fmt, ...)
 {
 va_list ap; /* points to each unnamed arg in turn */
 char *p, *sval;
128
 int ival;
 double dval;
 va_start(ap, fmt); /* make ap point to 1st unnamed arg */
 for (p = fmt; *p; p++) {
 if (*p != '%') {
 putchar(*p);
 continue;
 }
 switch (*++p) {
 case 'd':
 ival = va_arg(ap, int);
 printf("%d", ival);
 break;
 case 'f':
 dval = va_arg(ap, double);
 printf("%f", dval);
 break;
 case 's':
 for (sval = va_arg(ap, char *); *sval; sval++)
 putchar(*sval);
 break;
 default:
 putchar(*p);
 break;
 }
 }
 va_end(ap); /* clean up when done */
 }
Exercise 7-3. Revise minprintf to handle more of the other facilities of printf.
7.4 Formatted Input - Scanf
The function scanf is the input analog of printf, providing many of the same conversion
facilities in the opposite direction.
 int scanf(char *format, ...)
scanf reads characters from the standard input, interprets them according to the specification
in format, and stores the results through the remaining arguments. The format argument is
described below; the other arguments, each of which must be a pointer, indicate where the
corresponding converted input should be stored. As with printf, this section is a summary of
the most useful features, not an exhaustive list.
scanf stops when it exhausts its format string, or when some input fails to match the control
specification. It returns as its value the number of successfully matched and assigned input
items. This can be used to decide how many items were found. On the end of file, EOF is
returned; note that this is different from 0, which means that the next input character does not
match the first specification in the format string. The next call to scanf resumes searching
immediately after the last character already converted.
There is also a function sscanf that reads from a string instead of the standard input:
 int sscanf(char *string, char *format, arg1, arg2, ...)
It scans the string according to the format in format and stores the resulting values through
arg1, arg2, etc. These arguments must be pointers.
The format string usually contains conversion specifications, which are used to control
conversion of input. The format string may contain:
• Blanks or tabs, which are not ignored. 
129
• Ordinary characters (not %), which are expected to match the next non-white space
character of the input stream.
• Conversion specifications, consisting of the character %, an optional assignment
suppression character *, an optional number specifying a maximum field width, an
optional h, l or L indicating the width of the target, and a conversion character.
A conversion specification directs the conversion of the next input field. Normally the result is
places in the variable pointed to by the corresponding argument. If assignment suppression is
indicated by the * character, however, the input field is skipped; no assignment is made. An
input field is defined as a string of non-white space characters; it extends either to the next
white space character or until the field width, is specified, is exhausted. This implies that scanf
will read across boundaries to find its input, since newlines are white space. (White space
characters are blank, tab, newline, carriage return, vertical tab, and formfeed.)
The conversion character indicates the interpretation of the input field. The corresponding
argument must be a pointer, as required by the call-by-value semantics of C. Conversion
characters are shown in Table 7.2.
Table 7.2: Basic Scanf Conversions
Character Input Data; Argument type
d decimal integer; int *
i
integer; int *. The integer may be in octal (leading 0) or hexadecimal (leading
0x or 0X).
o octal integer (with or without leading zero); int *
u unsigned decimal integer; unsigned int *
x hexadecimal integer (with or without leading 0x or 0X); int *
c
characters; char *. The next input characters (default 1) are placed at the
indicated spot. The normal skip-over white space is suppressed; to read the next
non-white space character, use %1s
s
character string (not quoted); char *, pointing to an array of characters long
enough for the string and a terminating '\0' that will be added.
e,f,g floating-point number with optional sign, optional decimal point and optional
exponent; float *
% literal %; no assignment is made.
The conversion characters d, i, o, u, and x may be preceded by h to indicate that a pointer to
short rather than int appears in the argument list, or by l (letter ell) to indicate that a pointer
to long appears in the argument list.
As a first example, the rudimentary calculator of Chapter 4 can be written with scanf to do
the input conversion:
 #include <stdio.h>
 main() /* rudimentary calculator */
 {
 double sum, v;
 sum = 0;
 while (scanf("%lf", &v) == 1)
 printf("\t%.2f\n", sum += v);
 return 0;
 }
Suppose we want to read input lines that contain dates of the form 
130
 25 Dec 1988
The scanf statement is
 int day, year;
 char monthname[20];
 scanf("%d %s %d", &day, monthname, &year);
No & is used with monthname, since an array name is a pointer.
Literal characters can appear in the scanf format string; they must match the same characters
in the input. So we could read dates of the form mm/dd/yy with the scanf statement:
 int day, month, year;
 scanf("%d/%d/%d", &month, &day, &year);
scanf ignores blanks and tabs in its format string. Furthermore, it skips over white space
(blanks, tabs, newlines, etc.) as it looks for input values. To read input whose format is not
fixed, it is often best to read a line at a time, then pick it apart with scanf. For example,
suppose we want to read lines that might contain a date in either of the forms above. Then we
could write
 while (getline(line, sizeof(line)) > 0) {
 if (sscanf(line, "%d %s %d", &day, monthname, &year) == 3)
 printf("valid: %s\n", line); /* 25 Dec 1988 form */
 else if (sscanf(line, "%d/%d/%d", &month, &day, &year) == 3)
 printf("valid: %s\n", line); /* mm/dd/yy form */
 else
 printf("invalid: %s\n", line); /* invalid form */
 }
Calls to scanf can be mixed with calls to other input functions. The next call to any input
function will begin by reading the first character not read by scanf.
A final warning: the arguments to scanf and sscanf must be pointers. By far the most
common error is writing
 scanf("%d", n);
instead of
 scanf("%d", &n);
This error is not generally detected at compile time.
Exercise 7-4. Write a private version of scanf analogous to minprintf from the previous
section.
Exercise 5-5. Rewrite the postfix calculator of Chapter 4 to use scanf and/or sscanf to do
the input and number conversion.
7.5 File Access
The examples so far have all read the standard input and written the standard output, which are
automatically defined for a program by the local operating system.
The next step is to write a program that accesses a file that is not already connected to the
program. One program that illustrates the need for such operations is cat, which concatenates
a set of named files into the standard output. cat is used for printing files on the screen, and as
a general-purpose input collector for programs that do not have the capability of accessing files
by name. For example, the command
 cat x.c y.c
131
prints the contents of the files x.c and y.c (and nothing else) on the standard output.
The question is how to arrange for the named files to be read - that is, how to connect the
external names that a user thinks of to the statements that read the data.
The rules are simple. Before it can be read or written, a file has to be opened by the library
function fopen. fopen takes an external name like x.c or y.c, does some housekeeping and
negotiation with the operating system (details of which needn't concern us), and returns a
pointer to be used in subsequent reads or writes of the file.
This pointer, called the file pointer, points to a structure that contains information about the
file, such as the location of a buffer, the current character position in the buffer, whether the
file is being read or written, and whether errors or end of file have occurred. Users don't need
to know the details, because the definitions obtained from <stdio.h> include a structure
declaration called FILE. The only declaration needed for a file pointer is exemplified by
 FILE *fp;
 FILE *fopen(char *name, char *mode);
This says that fp is a pointer to a FILE, and fopen returns a pointer to a FILE. Notice that
FILE is a type name, like int, not a structure tag; it is defined with a typedef. (Details of how
fopen can be implemented on the UNIX system are given in Section 8.5.)
The call to fopen in a program is
 fp = fopen(name, mode);
The first argument of fopen is a character string containing the name of the file. The second
argument is the mode, also a character string, which indicates how one intends to use the file.
Allowable modes include read ("r"), write ("w"), and append ("a"). Some systems distinguish
between text and binary files; for the latter, a "b" must be appended to the mode string.
If a file that does not exist is opened for writing or appending, it is created if possible. Opening
an existing file for writing causes the old contents to be discarded, while opening for appending
preserves them. Trying to read a file that does not exist is an error, and there may be other
causes of error as well, like trying to read a file when you don't have permission. If there is any
error, fopen will return NULL. (The error can be identified more precisely; see the discussion of
error-handling functions at the end of Section 1 in Appendix B.)
The next thing needed is a way to read or write the file once it is open. getc returns the next
character from a file; it needs the file pointer to tell it which file.
 int getc(FILE *fp)
getc returns the next character from the stream referred to by fp; it returns EOF for end of file
or error.
putc is an output function:
 int putc(int c, FILE *fp)
putc writes the character c to the file fp and returns the character written, or EOF if an error
occurs. Like getchar and putchar, getc and putc may be macros instead of functions.
When a C program is started, the operating system environment is responsible for opening
three files and providing pointers for them. These files are the standard input, the standard
output, and the standard error; the corresponding file pointers are called stdin, stdout, and
stderr, and are declared in <stdio.h>. Normally stdin is connected to the keyboard and
stdout and stderr are connected to the screen, but stdin and stdout may be redirected to
files or pipes as described in Section 7.1. 
132
getchar and putchar can be defined in terms of getc, putc, stdin, and stdout as follows:
 #define getchar() getc(stdin)
 #define putchar(c) putc((c), stdout)
For formatted input or output of files, the functions fscanf and fprintf may be used. These
are identical to scanf and printf, except that the first argument is a file pointer that specifies
the file to be read or written; the format string is the second argument.
 int fscanf(FILE *fp, char *format, ...)
 int fprintf(FILE *fp, char *format, ...)
With these preliminaries out of the way, we are now in a position to write the program cat to
concatenate files. The design is one that has been found convenient for many programs. If
there are command-line arguments, they are interpreted as filenames, and processed in order. If
there are no arguments, the standard input is processed.
 #include <stdio.h>
 /* cat: concatenate files, version 1 */
 main(int argc, char *argv[])
 {
 FILE *fp;
 void filecopy(FILE *, FILE *)
 if (argc == 1) /* no args; copy standard input */
 filecopy(stdin, stdout);
 else
 while(--argc > 0)
 if ((fp = fopen(*++argv, "r")) == NULL) {
 printf("cat: can't open %s\n, *argv);
 return 1;
 } else {
 filecopy(fp, stdout);
 fclose(fp);
 }
 return 0;
 }
 /* filecopy: copy file ifp to file ofp */
 void filecopy(FILE *ifp, FILE *ofp)
 {
 int c;
 while ((c = getc(ifp)) != EOF)
 putc(c, ofp);
 }
The file pointers stdin and stdout are objects of type FILE *. They are constants, however,
not variables, so it is not possible to assign to them.
The function
 int fclose(FILE *fp)
is the inverse of fopen, it breaks the connection between the file pointer and the external name
that was established by fopen, freeing the file pointer for another file. Since most operating
systems have some limit on the number of files that a program may have open simultaneously,
it's a good idea to free the file pointers when they are no longer needed, as we did in cat.
There is also another reason for fclose on an output file - it flushes the buffer in which putc
is collecting output. fclose is called automatically for each open file when a program
terminates normally. (You can close stdin and stdout if they are not needed. They can also
be reassigned by the library function freopen.)
7.6 Error Handling - Stderr and Exit
133
The treatment of errors in cat is not ideal. The trouble is that if one of the files can't be
accessed for some reason, the diagnostic is printed at the end of the concatenated output. That
might be acceptable if the output is going to a screen, but not if it's going into a file or into
another program via a pipeline.
To handle this situation better, a second output stream, called stderr, is assigned to a
program in the same way that stdin and stdout are. Output written on stderr normally
appears on the screen even if the standard output is redirected.
Let us revise cat to write its error messages on the standard error.
 #include <stdio.h>
 /* cat: concatenate files, version 2 */
 main(int argc, char *argv[])
 {
 FILE *fp;
 void filecopy(FILE *, FILE *);
 char *prog = argv[0]; /* program name for errors */
 if (argc == 1 ) /* no args; copy standard input */
 filecopy(stdin, stdout);
 else
 while (--argc > 0)
 if ((fp = fopen(*++argv, "r")) == NULL) {
 fprintf(stderr, "%s: can't open %s\n",
 prog, *argv);
 exit(1);
 } else {
 filecopy(fp, stdout);
 fclose(fp);
 }
 if (ferror(stdout)) {
 fprintf(stderr, "%s: error writing stdout\n", prog);
 exit(2);
 }
 exit(0);
 }
The program signals errors in two ways. First, the diagnostic output produced by fprintf
goes to stderr, so it finds its way to the screen instead of disappearing down a pipeline or into
an output file. We included the program name, from argv[0], in the message, so if this
program is used with others, the source of an error is identified.
Second, the program uses the standard library function exit, which terminates program
execution when it is called. The argument of exit is available to whatever process called this
one, so the success or failure of the program can be tested by another program that uses this
one as a sub-process. Conventionally, a return value of 0 signals that all is well; non-zero
values usually signal abnormal situations. exit calls fclose for each open output file, to flush
out any buffered output.
Within main, return expr is equivalent to exit(expr). exit has the advantage that it can be
called from other functions, and that calls to it can be found with a pattern-searching program
like those in Chapter 5.
The function ferror returns non-zero if an error occurred on the stream fp.
 int ferror(FILE *fp)
Although output errors are rare, they do occur (for example, if a disk fills up), so a production
program should check this as well. 
134
The function feof(FILE *) is analogous to ferror; it returns non-zero if end of file has
occurred on the specified file.
 int feof(FILE *fp)
We have generally not worried about exit status in our small illustrative programs, but any
serious program should take care to return sensible, useful status values.
7.7 Line Input and Output
The standard library provides an input and output routine fgets that is similar to the getline
function that we have used in earlier chapters:
 char *fgets(char *line, int maxline, FILE *fp)
fgets reads the next input line (including the newline) from file fp into the character array
line; at most maxline-1 characters will be read. The resulting line is terminated with '\0'.
Normally fgets returns line; on end of file or error it returns NULL. (Our getline returns the
line length, which is a more useful value; zero means end of file.)
For output, the function fputs writes a string (which need not contain a newline) to a file:
 int fputs(char *line, FILE *fp)
It returns EOF if an error occurs, and non-negative otherwise.
The library functions gets and puts are similar to fgets and fputs, but operate on stdin
and stdout. Confusingly, gets deletes the terminating '\n', and puts adds it.
To show that there is nothing special about functions like fgets and fputs, here they are,
copied from the standard library on our system:
 /* fgets: get at most n chars from iop */
 char *fgets(char *s, int n, FILE *iop)
 {
 register int c;
 register char *cs;
 cs = s;
 while (--n > 0 && (c = getc(iop)) != EOF)
 if ((*cs++ = c) == '\n')
 break;
 *cs = '\0';
 return (c == EOF && cs == s) ? NULL : s;
 }
 /* fputs: put string s on file iop */
 int fputs(char *s, FILE *iop)
 {
 int c;
 while (c = *s++)
 putc(c, iop);
 return ferror(iop) ? EOF : 0;
 }
For no obvious reason, the standard specifies different return values for ferror and fputs.
It is easy to implement our getline from fgets:
 /* getline: read a line, return length */
 int getline(char *line, int max)
 {
 if (fgets(line, max, stdin) == NULL)
 return 0;
 else
135
 return strlen(line);
 }
Exercise 7-6. Write a program to compare two files, printing the first line where they differ.
Exercise 7-7. Modify the pattern finding program of Chapter 5 to take its input from a set of
named files or, if no files are named as arguments, from the standard input. Should the file
name be printed when a matching line is found?
Exercise 7-8. Write a program to print a set of files, starting each new one on a new page,
with a title and a running page count for each file.
7.8 Miscellaneous Functions
The standard library provides a wide variety of functions. This section is a brief synopsis of the
most useful. More details and many other functions can be found in Appendix B.
7.8.1 String Operations
We have already mentioned the string functions strlen, strcpy, strcat, and strcmp, found
in <string.h>. In the following, s and t are char *'s, and c and n are ints.
strcat(s,t) concatenate t to end of s
strncat(s,t,n) concatenate n characters of t to end of s
strcmp(s,t) return negative, zero, or positive for s < t, s == t, s > t
strncmp(s,t,n) same as strcmp but only in first n characters
strcpy(s,t) copy t to s
strncpy(s,t,n) copy at most n characters of t to s
strlen(s) return length of s
strchr(s,c) return pointer to first c in s, or NULL if not present
strrchr(s,c) return pointer to last c in s, or NULL if not present
7.8.2 Character Class Testing and Conversion
Several functions from <ctype.h> perform character tests and conversions. In the following, c
is an int that can be represented as an unsigned char or EOF. The function returns int.
isalpha(c) non-zero if c is alphabetic, 0 if not
isupper(c) non-zero if c is upper case, 0 if not
islower(c) non-zero if c is lower case, 0 if not
isdigit(c) non-zero if c is digit, 0 if not
isalnum(c) non-zero if isalpha(c) or isdigit(c), 0 if not
isspace(c) non-zero if c is blank, tab, newline, return, formfeed, vertical tab
toupper(c) return c converted to upper case
tolower(c) return c converted to lower case
7.8.3 Ungetc
The standard library provides a rather restricted version of the function ungetch that we wrote
in Chapter 4; it is called ungetc.
 int ungetc(int c, FILE *fp)
pushes the character c back onto file fp, and returns either c, or EOF for an error. Only one
character of pushback is guaranteed per file. ungetc may be used with any of the input
functions like scanf, getc, or getchar.
7.8.4 Command Execution
The function system(char *s) executes the command contained in the character string s,
then resumes execution of the current program. The contents of s depend strongly on the local
operating system. As a trivial example, on UNIX systems, the statement
 system("date");
136
causes the program date to be run; it prints the date and time of day on the standard output.
system returns a system-dependent integer status from the command executed. In the UNIX
system, the status return is the value returned by exit.
7.8.5 Storage Management
The functions malloc and calloc obtain blocks of memory dynamically.
 void *malloc(size_t n)
returns a pointer to n bytes of uninitialized storage, or NULL if the request cannot be satisfied.
 void *calloc(size_t n, size_t size)
returns a pointer to enough free space for an array of n objects of the specified size, or NULL if
the request cannot be satisfied. The storage is initialized to zero.
The pointer returned by malloc or calloc has the proper alignment for the object in question,
but it must be cast into the appropriate type, as in
 int *ip;
 ip = (int *) calloc(n, sizeof(int));
free(p) frees the space pointed to by p, where p was originally obtained by a call to malloc
or calloc. There are no restrictions on the order in which space is freed, but it is a ghastly
error to free something not obtained by calling malloc or calloc.
It is also an error to use something after it has been freed. A typical but incorrect piece of code
is this loop that frees items from a list:
 for (p = head; p != NULL; p = p->next) /* WRONG */
 free(p);
The right way is to save whatever is needed before freeing:
 for (p = head; p != NULL; p = q) {
 q = p->next;
 free(p);
 }
Section 8.7 shows the implementation of a storage allocator like malloc, in which allocated
blocks may be freed in any order.
7.8.6 Mathematical Functions
There are more than twenty mathematical functions declared in <math.h>; here are some of
the more frequently used. Each takes one or two double arguments and returns a double.
sin(x) sine of x, x in radians
cos(x) cosine of x, x in radians
atan2(y,x) arctangent of y/x, in radians
exp(x) exponential function e
x
log(x) natural (base e) logarithm of x (x>0)
log10(x) common (base 10) logarithm of x (x>0)
pow(x,y) x
y
sqrt(x) square root of x (x>0)
fabs(x) absolute value of x
7.8.7 Random Number generation
The function rand() computes a sequence of pseudo-random integers in the range zero to
RAND_MAX, which is defined in <stdlib.h>. One way to produce random floating-point
numbers greater than or equal to zero but less than one is
 #define frand() ((double) rand() / (RAND_MAX+1.0))
137
(If your library already provides a function for floating-point random numbers, it is likely to
have better statistical properties than this one.)
The function srand(unsigned) sets the seed for rand. The portable implementation of rand
and srand suggested by the standard appears in Section 2.7.
Exercise 7-9. Functions like isupper can be implemented to save space or to save time.
Explore both possibilities. 
138
Chapter 8 - The UNIX System Interface
The UNIX operating system provides its services through a set of system calls, which are in
effect functions within the operating system that may be called by user programs. This chapter
describes how to use some of the most important system calls from C programs. If you use
UNIX, this should be directly helpful, for it is sometimes necessary to employ system calls for
maximum efficiency, or to access some facility that is not in the library. Even if you use C on a
different operating system, however, you should be able to glean insight into C programming
from studying these examples; although details vary, similar code will be found on any system.
Since the ANSI C library is in many cases modeled on UNIX facilities, this code may help your
understanding of the library as well.
This chapter is divided into three major parts: input/output, file system, and storage allocation.
The first two parts assume a modest familiarity with the external characteristics of UNIX
systems.
Chapter 7 was concerned with an input/output interface that is uniform across operating
systems. On any particular system the routines of the standard library have to be written in
terms of the facilities provided by the host system. In the next few sections we will describe the
UNIX system calls for input and output, and show how parts of the standard library can be
implemented with them.
8.1 File Descriptors
In the UNIX operating system, all input and output is done by reading or writing files, because
all peripheral devices, even keyboard and screen, are files in the file system. This means that a
single homogeneous interface handles all communication between a program and peripheral
devices.
In the most general case, before you read and write a file, you must inform the system of your
intent to do so, a process called opening the file. If you are going to write on a file it may also
be necessary to create it or to discard its previous contents. The system checks your right to do
so (Does the file exist? Do you have permission to access it?) and if all is well, returns to the
program a small non-negative integer called a file descriptor. Whenever input or output is to
be done on the file, the file descriptor is used instead of the name to identify the file. (A file
descriptor is analogous to the file pointer used by the standard library, or to the file handle of
MS-DOS.) All information about an open file is maintained by the system; the user program
refers to the file only by the file descriptor.
Since input and output involving keyboard and screen is so common, special arrangements
exist to make this convenient. When the command interpreter (the ``shell'') runs a program,
three files are open, with file descriptors 0, 1, and 2, called the standard input, the standard
output, and the standard error. If a program reads 0 and writes 1 and 2, it can do input and
output without worrying about opening files.
The user of a program can redirect I/O to and from files with < and >:
 prog <infile >outfile
In this case, the shell changes the default assignments for the file descriptors 0 and 1 to the
named files. Normally file descriptor 2 remains attached to the screen, so error messages can
go there. Similar observations hold for input or output associated with a pipe. In all cases, the
file assignments are changed by the shell, not by the program. The program does not know
where its input comes from nor where its output goes, so long as it uses file 0 for input and 1
and 2 for output. 
139
8.2 Low Level I/O - Read and Write
Input and output uses the read and write system calls, which are accessed from C programs
through two functions called read and write. For both, the first argument is a file descriptor.
The second argument is a character array in your program where the data is to go to or to
come from. The third argument is the number is the number of bytes to be transferred.
 int n_read = read(int fd, char *buf, int n);
 int n_written = write(int fd, char *buf, int n);
Each call returns a count of the number of bytes transferred. On reading, the number of bytes
returned may be less than the number requested. A return value of zero bytes implies end of
file, and -1 indicates an error of some sort. For writing, the return value is the number of bytes
written; an error has occurred if this isn't equal to the number requested.
Any number of bytes can be read or written in one call. The most common values are 1, which
means one character at a time (``unbuffered''), and a number like 1024 or 4096 that
corresponds to a physical block size on a peripheral device. Larger sizes will be more efficient
because fewer system calls will be made.
Putting these facts together, we can write a simple program to copy its input to its output, the
equivalent of the file copying program written for Chapter 1. This program will copy anything
to anything, since the input and output can be redirected to any file or device.
 #include "syscalls.h"
 main() /* copy input to output */
 {
 char buf[BUFSIZ];
 int n;
 while ((n = read(0, buf, BUFSIZ)) > 0)
 write(1, buf, n);
 return 0;
 }
We have collected function prototypes for the system calls into a file called syscalls.h so we
can include it in the programs of this chapter. This name is not standard, however.
The parameter BUFSIZ is also defined in syscalls.h; its value is a good size for the local
system. If the file size is not a multiple of BUFSIZ, some read will return a smaller number of
bytes to be written by write; the next call to read after that will return zero.
It is instructive to see how read and write can be used to construct higher-level routines like
getchar, putchar, etc. For example, here is a version of getchar that does unbuffered input,
by reading the standard input one character at a time.
 #include "syscalls.h"
 /* getchar: unbuffered single character input */
 int getchar(void)
 {
 char c;
 return (read(0, &c, 1) == 1) ? (unsigned char) c : EOF;
 }
c must be a char, because read needs a character pointer. Casting c to unsigned char in the
return statement eliminates any problem of sign extension.
The second version of getchar does input in big chunks, and hands out the characters one at a
time. 
140
 #include "syscalls.h"
 /* getchar: simple buffered version */
 int getchar(void)
 {
 static char buf[BUFSIZ];
 static char *bufp = buf;
 static int n = 0;
 if (n == 0) { /* buffer is empty */
 n = read(0, buf, sizeof buf);
 bufp = buf;
 }
 return (--n >= 0) ? (unsigned char) *bufp++ : EOF;
 }
If these versions of getchar were to be compiled with <stdio.h> included, it would be
necessary to #undef the name getchar in case it is implemented as a macro.
8.3 Open, Creat, Close, Unlink
Other than the default standard input, output and error, you must explicitly open files in order
to read or write them. There are two system calls for this, open and creat [sic].
open is rather like the fopen discussed in Chapter 7, except that instead of returning a file
pointer, it returns a file descriptor, which is just an int. open returns -1 if any error occurs.
 #include <fcntl.h>
 int fd;
 int open(char *name, int flags, int perms);
 fd = open(name, flags, perms);
As with fopen, the name argument is a character string containing the filename. The second
argument, flags, is an int that specifies how the file is to be opened; the main values are
O_RDONLY open for reading only
O_WRONLY open for writing only
O_RDWR open for both reading and writing
These constants are defined in <fcntl.h> on System V UNIX systems, and in <sys/file.h>
on Berkeley (BSD) versions.
To open an existing file for reading,
 fd = open(name, O_RDONLY,0);
The perms argument is always zero for the uses of open that we will discuss.
It is an error to try to open a file that does not exist. The system call creat is provided to
create new files, or to re-write old ones.
 int creat(char *name, int perms);
 fd = creat(name, perms);
returns a file descriptor if it was able to create the file, and -1 if not. If the file already exists,
creat will truncate it to zero length, thereby discarding its previous contents; it is not an error
to creat a file that already exists.
If the file does not already exist, creat creates it with the permissions specified by the perms
argument. In the UNIX file system, there are nine bits of permission information associated
with a file that control read, write and execute access for the owner of the file, for the owner's
group, and for all others. Thus a three-digit octal number is convenient for specifying the
141
permissions. For example, 0775 specifies read, write and execute permission for the owner,
and read and execute permission for the group and everyone else.
To illustrate, here is a simplified version of the UNIX program cp, which copies one file to
another. Our version copies only one file, it does not permit the second argument to be a
directory, and it invents permissions instead of copying them.
 #include <stdio.h>
 #include <fcntl.h>
 #include "syscalls.h"
 #define PERMS 0666 /* RW for owner, group, others */
 void error(char *, ...);
 /* cp: copy f1 to f2 */
 main(int argc, char *argv[])
 {
 int f1, f2, n;
 char buf[BUFSIZ];
 if (argc != 3)
 error("Usage: cp from to");
 if ((f1 = open(argv[1], O_RDONLY, 0)) == -1)
 error("cp: can't open %s", argv[1]);
 if ((f2 = creat(argv[2], PERMS)) == -1)
 error("cp: can't create %s, mode %03o",
 argv[2], PERMS);
 while ((n = read(f1, buf, BUFSIZ)) > 0)
 if (write(f2, buf, n) != n)
 error("cp: write error on file %s", argv[2]);
 return 0;
 }
This program creates the output file with fixed permissions of 0666. With the stat system call,
described in Section 8.6, we can determine the mode of an existing file and thus give the same
mode to the copy.
Notice that the function error is called with variable argument lists much like printf. The
implementation of error illustrates how to use another member of the printf family. The
standard library function vprintf is like printf except that the variable argument list is
replaced by a single argument that has been initialized by calling the va_start macro.
Similarly, vfprintf and vsprintf match fprintf and sprintf.
 #include <stdio.h>
 #include <stdarg.h>
 /* error: print an error message and die */
 void error(char *fmt, ...)
 {
 va_list args;
 va_start(args, fmt);
 fprintf(stderr, "error: ");
 vprintf(stderr, fmt, args);
 fprintf(stderr, "\n");
 va_end(args);
 exit(1);
 }
There is a limit (often about 20) on the number of files that a program may open
simultaneously. Accordingly, any program that intends to process many files must be prepared
to re-use file descriptors. The function close(int fd) breaks the connection between a file
descriptor and an open file, and frees the file descriptor for use with some other file; it
142
corresponds to fclose in the standard library except that there is no buffer to flush.
Termination of a program via exit or return from the main program closes all open files.
The function unlink(char *name) removes the file name from the file system. It corresponds
to the standard library function remove.
Exercise 8-1. Rewrite the program cat from Chapter 7 using read, write, open, and close
instead of their standard library equivalents. Perform experiments to determine the relative
speeds of the two versions.
8.4 Random Access - Lseek
Input and output are normally sequential: each read or write takes place at a position in the
file right after the previous one. When necessary, however, a file can be read or written in any
arbitrary order. The system call lseek provides a way to move around in a file without reading
or writing any data:
 long lseek(int fd, long offset, int origin);
sets the current position in the file whose descriptor is fd to offset, which is taken relative to
the location specified by origin. Subsequent reading or writing will begin at that position.
origin can be 0, 1, or 2 to specify that offset is to be measured from the beginning, from the
current position, or from the end of the file respectively. For example, to append to a file (the
redirection >> in the UNIX shell, or "a" for fopen), seek to the end before writing:
 lseek(fd, 0L, 2);
To get back to the beginning (``rewind''),
 lseek(fd, 0L, 0);
Notice the 0L argument; it could also be written as (long) 0 or just as 0 if lseek is properly
declared.
With lseek, it is possible to treat files more or less like arrays, at the price of slower access.
For example, the following function reads any number of bytes from any arbitrary place in a
file. It returns the number read, or -1 on error.
 #include "syscalls.h"
 /*get: read n bytes from position pos */
 int get(int fd, long pos, char *buf, int n)
 {
 if (lseek(fd, pos, 0) >= 0) /* get to pos */
 return read(fd, buf, n);
 else
 return -1;
 }
The return value from lseek is a long that gives the new position in the file, or -1 if an error
occurs. The standard library function fseek is similar to lseek except that the first argument
is a FILE * and the return is non-zero if an error occurred.
8.5 Example - An implementation of Fopen and Getc
Let us illustrate how some of these pieces fit together by showing an implementation of the
standard library routines fopen and getc.
Recall that files in the standard library are described by file pointers rather than file descriptors.
A file pointer is a pointer to a structure that contains several pieces of information about the
file: a pointer to a buffer, so the file can be read in large chunks; a count of the number of
characters left in the buffer; a pointer to the next character position in the buffer; the file
descriptor; and flags describing read/write mode, error status, etc. 
143
The data structure that describes a file is contained in <stdio.h>, which must be included (by
#include) in any source file that uses routines from the standard input/output library. It is also
included by functions in that library. In the following excerpt from a typical <stdio.h>, names
that are intended for use only by functions of the library begin with an underscore so they are
less likely to collide with names in a user's program. This convention is used by all standard
library routines.
 #define NULL 0
 #define EOF (-1)
 #define BUFSIZ 1024
 #define OPEN_MAX 20 /* max #files open at once */
 typedef struct _iobuf {
 int cnt; /* characters left */
 char *ptr; /* next character position */
 char *base; /* location of buffer */
 int flag; /* mode of file access */
 int fd; /* file descriptor */
 } FILE;
 extern FILE _iob[OPEN_MAX];
 #define stdin (&_iob[0])
 #define stdout (&_iob[1])
 #define stderr (&_iob[2])
 enum _flags {
 _READ = 01, /* file open for reading */
 _WRITE = 02, /* file open for writing */
 _UNBUF = 04, /* file is unbuffered */
 _EOF = 010, /* EOF has occurred on this file */
 _ERR = 020 /* error occurred on this file */
 };
 int _fillbuf(FILE *);
 int _flushbuf(int, FILE *);
 #define feof(p) ((p)->flag & _EOF) != 0)
 #define ferror(p) ((p)->flag & _ERR) != 0)
 #define fileno(p) ((p)->fd)
 #define getc(p) (--(p)->cnt >= 0 \
 ? (unsigned char) *(p)->ptr++ : _fillbuf(p))
 #define putc(x,p) (--(p)->cnt >= 0 \
 ? *(p)->ptr++ = (x) : _flushbuf((x),p))
 #define getchar() getc(stdin)
 #define putcher(x) putc((x), stdout)
The getc macro normally decrements the count, advances the pointer, and returns the
character. (Recall that a long #define is continued with a backslash.) If the count goes
negative, however, getc calls the function _fillbuf to replenish the buffer, re-initialize the
structure contents, and return a character. The characters are returned unsigned, which
ensures that all characters will be positive.
Although we will not discuss any details, we have included the definition of putc to show that
it operates in much the same way as getc, calling a function _flushbuf when its buffer is full.
We have also included macros for accessing the error and end-of-file status and the file
descriptor.
The function fopen can now be written. Most of fopen is concerned with getting the file
opened and positioned at the right place, and setting the flag bits to indicate the proper state.
fopen does not allocate any buffer space; this is done by _fillbuf when the file is first read. 
144
 #include <fcntl.h>
 #include "syscalls.h"
 #define PERMS 0666 /* RW for owner, group, others */
 FILE *fopen(char *name, char *mode)
 {
 int fd;
 FILE *fp;
 if (*mode != 'r' && *mode != 'w' && *mode != 'a')
 return NULL;
 for (fp = _iob; fp < _iob + OPEN_MAX; fp++)
 if ((fp->flag & (_READ | _WRITE)) == 0)
 break; /* found free slot */
 if (fp >= _iob + OPEN_MAX) /* no free slots */
 return NULL;
 if (*mode == 'w')
 fd = creat(name, PERMS);
 else if (*mode == 'a') {
 if ((fd = open(name, O_WRONLY, 0)) == -1)
 fd = creat(name, PERMS);
 lseek(fd, 0L, 2);
 } else
 fd = open(name, O_RDONLY, 0);
 if (fd == -1) /* couldn't access name */
 return NULL;
 fp->fd = fd;
 fp->cnt = 0;
 fp->base = NULL;
 fp->flag = (*mode == 'r') ? _READ : _WRITE;
 return fp;
 }
This version of fopen does not handle all of the access mode possibilities of the standard,
though adding them would not take much code. In particular, our fopen does not recognize
the ``b'' that signals binary access, since that is meaningless on UNIX systems, nor the ``+'' that
permits both reading and writing.
The first call to getc for a particular file finds a count of zero, which forces a call of
_fillbuf. If _fillbuf finds that the file is not open for reading, it returns EOF immediately.
Otherwise, it tries to allocate a buffer (if reading is to be buffered).
Once the buffer is established, _fillbuf calls read to fill it, sets the count and pointers, and
returns the character at the beginning of the buffer. Subsequent calls to _fillbuf will find a
buffer allocated.
 #include "syscalls.h"
 /* _fillbuf: allocate and fill input buffer */
 int _fillbuf(FILE *fp)
 {
 int bufsize;
 if ((fp->flag&(_READ|_EOF_ERR)) != _READ)
 return EOF;
 bufsize = (fp->flag & _UNBUF) ? 1 : BUFSIZ;
 if (fp->base == NULL) /* no buffer yet */
 if ((fp->base = (char *) malloc(bufsize)) == NULL)
 return EOF; /* can't get buffer */
 fp->ptr = fp->base;
 fp->cnt = read(fp->fd, fp->ptr, bufsize);
 if (--fp->cnt < 0) {
 if (fp->cnt == -1)
 fp->flag |= _EOF;
145
 else
 fp->flag |= _ERR;
 fp->cnt = 0;
 return EOF;
 }
 return (unsigned char) *fp->ptr++;
 }
The only remaining loose end is how everything gets started. The array _iob must be defined
and initialized for stdin, stdout and stderr:
 FILE _iob[OPEN_MAX] = { /* stdin, stdout, stderr */
 { 0, (char *) 0, (char *) 0, _READ, 0 },
 { 0, (char *) 0, (char *) 0, _WRITE, 1 },
 { 0, (char *) 0, (char *) 0, _WRITE, | _UNBUF, 2 }
 };
The initialization of the flag part of the structure shows that stdin is to be read, stdout is to
be written, and stderr is to be written unbuffered.
Exercise 8-2. Rewrite fopen and _fillbuf with fields instead of explicit bit operations.
Compare code size and execution speed.
Exercise 8-3. Design and write _flushbuf, fflush, and fclose.
Exercise 8-4. The standard library function
 int fseek(FILE *fp, long offset, int origin)
is identical to lseek except that fp is a file pointer instead of a file descriptor and return value
is an int status, not a position. Write fseek. Make sure that your fseek coordinates properly
with the buffering done for the other functions of the library.
8.6 Example - Listing Directories
A different kind of file system interaction is sometimes called for - determining information
about a file, not what it contains. A directory-listing program such as the UNIX command ls
is an example - it prints the names of files in a directory, and, optionally, other information,
such as sizes, permissions, and so on. The MS-DOS dir command is analogous.
Since a UNIX directory is just a file, ls need only read it to retrieve the filenames. But is is
necessary to use a system call to access other information about a file, such as its size. On
other systems, a system call may be needed even to access filenames; this is the case on MSDOS for instance. What we want is provide access to the information in a relatively systemindependent way, even though the implementation may be highly system-dependent.
We will illustrate some of this by writing a program called fsize. fsize is a special form of
ls that prints the sizes of all files named in its commandline argument list. If one of the files is
a directory, fsize applies itself recursively to that directory. If there are no arguments at all, it
processes the current directory.
Let us begin with a short review of UNIX file system structure. A directory is a file that
contains a list of filenames and some indication of where they are located. The ``location'' is an
index into another table called the ``inode list.'' The inode for a file is where all information
about the file except its name is kept. A directory entry generally consists of only two items,
the filename and an inode number.
Regrettably, the format and precise contents of a directory are not the same on all versions of
the system. So we will divide the task into two pieces to try to isolate the non-portable parts.
The outer level defines a structure called a Dirent and three routines opendir, readdir, and
closedir to provide system-independent access to the name and inode number in a directory
entry. We will write fsize with this interface. Then we will show how to implement these on
146
systems that use the same directory structure as Version 7 and System V UNIX; variants are
left as exercises.
The Dirent structure contains the inode number and the name. The maximum length of a
filename component is NAME_MAX, which is a system-dependent value. opendir returns a
pointer to a structure called DIR, analogous to FILE, which is used by readdir and closedir.
This information is collected into a file called dirent.h.
 #define NAME_MAX 14 /* longest filename component; */
 /* system-dependent */
 typedef struct { /* portable directory entry */
 long ino; /* inode number */
 char name[NAME_MAX+1]; /* name + '\0' terminator */
 } Dirent;
 typedef struct { /* minimal DIR: no buffering, etc. */
 int fd; /* file descriptor for the directory */
 Dirent d; /* the directory entry */
 } DIR;
 DIR *opendir(char *dirname);
 Dirent *readdir(DIR *dfd);
 void closedir(DIR *dfd);
The system call stat takes a filename and returns all of the information in the inode for that
file, or -1 if there is an error. That is,
 char *name;
 struct stat stbuf;
 int stat(char *, struct stat *);
 stat(name, &stbuf);
fills the structure stbuf with the inode information for the file name. The structure describing
the value returned by stat is in <sys/stat.h>, and typically looks like this:
 struct stat /* inode information returned by stat */
 {
 dev_t st_dev; /* device of inode */
 ino_t st_ino; /* inode number */
 short st_mode; /* mode bits */
 short st_nlink; /* number of links to file */
 short st_uid; /* owners user id */
 short st_gid; /* owners group id */
 dev_t st_rdev; /* for special files */
 off_t st_size; /* file size in characters */
 time_t st_atime; /* time last accessed */
 time_t st_mtime; /* time last modified */
 time_t st_ctime; /* time originally created */
 };
Most of these values are explained by the comment fields. The types like dev_t and ino_t are
defined in <sys/types.h>, which must be included too.
The st_mode entry contains a set of flags describing the file. The flag definitions are also
included in <sys/types.h>; we need only the part that deals with file type:
 #define S_IFMT 0160000 /* type of file: */
 #define S_IFDIR 0040000 /* directory */
 #define S_IFCHR 0020000 /* character special */
 #define S_IFBLK 0060000 /* block special */
 #define S_IFREG 0010000 /* regular */
 /* ... */
147
Now we are ready to write the program fsize. If the mode obtained from stat indicates that
a file is not a directory, then the size is at hand and can be printed directly. If the name is a
directory, however, then we have to process that directory one file at a time; it may in turn
contain sub-directories, so the process is recursive.
The main routine deals with command-line arguments; it hands each argument to the function
fsize.
 #include <stdio.h>
 #include <string.h>
 #include "syscalls.h"
 #include <fcntl.h> /* flags for read and write */
 #include <sys/types.h> /* typedefs */
 #include <sys/stat.h> /* structure returned by stat */
 #include "dirent.h"
 void fsize(char *)
 /* print file name */
 main(int argc, char **argv)
 {
 if (argc == 1) /* default: current directory */
 fsize(".");
 else
 while (--argc > 0)
 fsize(*++argv);
 return 0;
 }
The function fsize prints the size of the file. If the file is a directory, however, fsize first
calls dirwalk to handle all the files in it. Note how the flag names S_IFMT and S_IFDIR are
used to decide if the file is a directory. Parenthesization matters, because the precedence of & is
lower than that of ==.
 int stat(char *, struct stat *);
 void dirwalk(char *, void (*fcn)(char *));
 /* fsize: print the name of file "name" */
 void fsize(char *name)
 {
 struct stat stbuf;
 if (stat(name, &stbuf) == -1) {
 fprintf(stderr, "fsize: can't access %s\n", name);
 return;
 }
 if ((stbuf.st_mode & S_IFMT) == S_IFDIR)
 dirwalk(name, fsize);
 printf("%8ld %s\n", stbuf.st_size, name);
 }
The function dirwalk is a general routine that applies a function to each file in a directory. It
opens the directory, loops through the files in it, calling the function on each, then closes the
directory and returns. Since fsize calls dirwalk on each directory, the two functions call each
other recursively.
 #define MAX_PATH 1024
 /* dirwalk: apply fcn to all files in dir */
 void dirwalk(char *dir, void (*fcn)(char *))
 {
 char name[MAX_PATH];
 Dirent *dp;
 DIR *dfd;
148
 if ((dfd = opendir(dir)) == NULL) {
 fprintf(stderr, "dirwalk: can't open %s\n", dir);
 return;
 }
 while ((dp = readdir(dfd)) != NULL) {
 if (strcmp(dp->name, ".") == 0
 || strcmp(dp->name, ".."))
 continue; /* skip self and parent */
 if (strlen(dir)+strlen(dp->name)+2 > sizeof(name))
 fprintf(stderr, "dirwalk: name %s %s too long\n",
 dir, dp->name);
 else {
 sprintf(name, "%s/%s", dir, dp->name);
 (*fcn)(name);
 }
 }
 closedir(dfd);
 }
Each call to readdir returns a pointer to information for the next file, or NULL when there are
no files left. Each directory always contains entries for itself, called ".", and its parent, "..";
these must be skipped, or the program will loop forever.
Down to this last level, the code is independent of how directories are formatted. The next step
is to present minimal versions of opendir, readdir, and closedir for a specific system. The
following routines are for Version 7 and System V UNIX systems; they use the directory
information in the header <sys/dir.h>, which looks like this:
 #ifndef DIRSIZ
 #define DIRSIZ 14
 #endif
 struct direct { /* directory entry */
 ino_t d_ino; /* inode number */
 char d_name[DIRSIZ]; /* long name does not have '\0' */
 };
Some versions of the system permit much longer names and have a more complicated directory
structure.
The type ino_t is a typedef that describes the index into the inode list. It happens to be
unsigned short on the systems we use regularly, but this is not the sort of information to
embed in a program; it might be different on a different system, so the typedef is better. A
complete set of ``system'' types is found in <sys/types.h>.
opendir opens the directory, verifies that the file is a directory (this time by the system call
fstat, which is like stat except that it applies to a file descriptor), allocates a directory
structure, and records the information:
 int fstat(int fd, struct stat *);
 /* opendir: open a directory for readdir calls */
 DIR *opendir(char *dirname)
 {
 int fd;
 struct stat stbuf;
 DIR *dp;
 if ((fd = open(dirname, O_RDONLY, 0)) == -1
 || fstat(fd, &stbuf) == -1
 || (stbuf.st_mode & S_IFMT) != S_IFDIR
 || (dp = (DIR *) malloc(sizeof(DIR))) == NULL)
 return NULL;
 dp->fd = fd;
 return dp;
 }
149
closedir closes the directory file and frees the space:
 /* closedir: close directory opened by opendir */
 void closedir(DIR *dp)
 {
 if (dp) {
 close(dp->fd);
 free(dp);
 }
 }
Finally, readdir uses read to read each directory entry. If a directory slot is not currently in
use (because a file has been removed), the inode number is zero, and this position is skipped.
Otherwise, the inode number and name are placed in a static structure and a pointer to that is
returned to the user. Each call overwrites the information from the previous one.
 #include <sys/dir.h> /* local directory structure */
 /* readdir: read directory entries in sequence */
 Dirent *readdir(DIR *dp)
 {
 struct direct dirbuf; /* local directory structure */
 static Dirent d; /* return: portable structure */
 while (read(dp->fd, (char *) &dirbuf, sizeof(dirbuf))
 == sizeof(dirbuf)) {
 if (dirbuf.d_ino == 0) /* slot not in use */
 continue;
 d.ino = dirbuf.d_ino;
 strncpy(d.name, dirbuf.d_name, DIRSIZ);
 d.name[DIRSIZ] = '\0'; /* ensure termination */
 return &d;
 }
 return NULL;
 }
Although the fsize program is rather specialized, it does illustrate a couple of important
ideas. First, many programs are not ``system programs''; they merely use information that is
maintained by the operating system. For such programs, it is crucial that the representation of
the information appear only in standard headers, and that programs include those headers
instead of embedding the declarations in themselves. The second observation is that with care
it is possible to create an interface to system-dependent objects that is itself relatively systemindependent. The functions of the standard library are good examples.
Exercise 8-5. Modify the fsize program to print the other information contained in the inode
entry.
8.7 Example - A Storage Allocator
In Chapter 5, we presented a vary limited stack-oriented storage allocator. The version that we
will now write is unrestricted. Calls to malloc and free may occur in any order; malloc calls
upon the operating system to obtain more memory as necessary. These routines illustrate some
of the considerations involved in writing machine-dependent code in a relatively machineindependent way, and also show a real-life application of structures, unions and typedef.
Rather than allocating from a compiled-in fixed-size array, malloc will request space from the
operating system as needed. Since other activities in the program may also request space
without calling this allocator, the space that malloc manages may not be contiguous. Thus its
free storage is kept as a list of free blocks. Each block contains a size, a pointer to the next
block, and the space itself. The blocks are kept in order of increasing storage address, and the
last block (highest address) points to the first. 
150
When a request is made, the free list is scanned until a big-enough block is found. This
algorithm is called ``first fit,'' by contrast with ``best fit,'' which looks for the smallest block
that will satisfy the request. If the block is exactly the size requested it is unlinked from the list
and returned to the user. If the block is too big, it is split, and the proper amount is returned to
the user while the residue remains on the free list. If no big-enough block is found, another
large chunk is obtained by the operating system and linked into the free list.
Freeing also causes a search of the free list, to find the proper place to insert the block being
freed. If the block being freed is adjacent to a free block on either side, it is coalesced with it
into a single bigger block, so storage does not become too fragmented. Determining the
adjacency is easy because the free list is maintained in order of decreasing address.
One problem, which we alluded to in Chapter 5, is to ensure that the storage returned by
malloc is aligned properly for the objects that will be stored in it. Although machines vary, for
each machine there is a most restrictive type: if the most restrictive type can be stored at a
particular address, all other types may be also. On some machines, the most restrictive type is a
double; on others, int or long suffices.
A free block contains a pointer to the next block in the chain, a record of the size of the block,
and then the free space itself; the control information at the beginning is called the ``header.''
To simplify alignment, all blocks are multiples of the header size, and the header is aligned
properly. This is achieved by a union that contains the desired header structure and an instance
of the most restrictive alignment type, which we have arbitrarily made a long:
 typedef long Align; /* for alignment to long boundary */
 union header { /* block header */
 struct {
 union header *ptr; /* next block if on free list */
 unsigned size; /* size of this block */
 } s;
 Align x; /* force alignment of blocks */
 };
 typedef union header Header;
The Align field is never used; it just forces each header to be aligned on a worst-case
boundary.
In malloc, the requested size in characters is rounded up to the proper number of header-sized
units; the block that will be allocated contains one more unit, for the header itself, and this is
the value recorded in the size field of the header. The pointer returned by malloc points at
the free space, not at the header itself. The user can do anything with the space requested, but
if anything is written outside of the allocated space the list is likely to be scrambled. 
151
The size field is necessary because the blocks controlled by malloc need not be contiguous - it
is not possible to compute sizes by pointer arithmetic.
The variable base is used to get started. If freep is NULL, as it is at the first call of malloc,
then a degenerate free list is created; it contains one block of size zero, and points to itself. In
any case, the free list is then searched. The search for a free block of adequate size begins at
the point (freep) where the last block was found; this strategy helps keep the list
homogeneous. If a too-big block is found, the tail end is returned to the user; in this way the
header of the original needs only to have its size adjusted. In all cases, the pointer returned to
the user points to the free space within the block, which begins one unit beyond the header.
 static Header base; /* empty list to get started */
 static Header *freep = NULL; /* start of free list */
 /* malloc: general-purpose storage allocator */
 void *malloc(unsigned nbytes)
 {
 Header *p, *prevp;
 Header *moreroce(unsigned);
 unsigned nunits;
 nunits = (nbytes+sizeof(Header)-1)/sizeof(header) + 1;
 if ((prevp = freep) == NULL) { /* no free list yet */
 base.s.ptr = freeptr = prevptr = &base;
 base.s.size = 0;
 }
 for (p = prevp->s.ptr; ; prevp = p, p = p->s.ptr) {
 if (p->s.size >= nunits) { /* big enough */
 if (p->s.size == nunits) /* exactly */
 prevp->s.ptr = p->s.ptr;
 else { /* allocate tail end */
 p->s.size -= nunits;
 p += p->s.size;
 p->s.size = nunits;
 }
 freep = prevp;
 return (void *)(p+1);
 }
 if (p == freep) /* wrapped around free list */
 if ((p = morecore(nunits)) == NULL)
 return NULL; /* none left */
 }
 }
The function morecore obtains storage from the operating system. The details of how it does
this vary from system to system. Since asking the system for memory is a comparatively
expensive operation. we don't want to do that on every call to malloc, so morecore requests
al least NALLOC units; this larger block will be chopped up as needed. After setting the size
field, morecore inserts the additional memory into the arena by calling free. 
152
The UNIX system call sbrk(n) returns a pointer to n more bytes of storage. sbrk returns -1
if there was no space, even though NULL could have been a better design. The -1 must be cast
to char * so it can be compared with the return value. Again, casts make the function
relatively immune to the details of pointer representation on different machines. There is still
one assumption, however, that pointers to different blocks returned by sbrk can be
meaningfully compared. This is not guaranteed by the standard, which permits pointer
comparisons only within an array. Thus this version of malloc is portable only among
machines for which general pointer comparison is meaningful.
 #define NALLOC 1024 /* minimum #units to request */
 /* morecore: ask system for more memory */
 static Header *morecore(unsigned nu)
 {
 char *cp, *sbrk(int);
 Header *up;
 if (nu < NALLOC)
 nu = NALLOC;
 cp = sbrk(nu * sizeof(Header));
 if (cp == (char *) -1) /* no space at all */
 return NULL;
 up = (Header *) cp;
 up->s.size = nu;
 free((void *)(up+1));
 return freep;
 }
free itself is the last thing. It scans the free list, starting at freep, looking for the place to
insert the free block. This is either between two existing blocks or at the end of the list. In any
case, if the block being freed is adjacent to either neighbor, the adjacent blocks are combined.
The only troubles are keeping the pointers pointing to the right things and the sizes correct.
 /* free: put block ap in free list */
 void free(void *ap)
 {
 Header *bp, *p;
 bp = (Header *)ap - 1; /* point to block header */
 for (p = freep; !(bp > p && bp < p->s.ptr); p = p->s.ptr)
 if (p >= p->s.ptr && (bp > p || bp < p->s.ptr))
 break; /* freed block at start or end of arena */
 if (bp + bp->size == p->s.ptr) { /* join to upper nbr */
 bp->s.size += p->s.ptr->s.size;
 bp->s.ptr = p->s.ptr->s.ptr;
 } else
 bp->s.ptr = p->s.ptr;
 if (p + p->size == bp) { /* join to lower nbr */
 p->s.size += bp->s.size;
 p->s.ptr = bp->s.ptr;
 } else
 p->s.ptr = bp;
 freep = p;
 }
Although storage allocation is intrinsically machine-dependent, the code above illustrates how
the machine dependencies can be controlled and confined to a very small part of the program.
The use of typedef and union handles alignment (given that sbrk supplies an appropriate
pointer). Casts arrange that pointer conversions are made explicit, and even cope with a badlydesigned system interface. Even though the details here are related to storage allocation, the
general approach is applicable to other situations as well. 
153
Exercise 8-6. The standard library function calloc(n,size) returns a pointer to n objects of
size size, with the storage initialized to zero. Write calloc, by calling malloc or by
modifying it.
Exercise 8-7. malloc accepts a size request without checking its plausibility; free believes
that the block it is asked to free contains a valid size field. Improve these routines so they make
more pains with error checking.
Exercise 8-8. Write a routine bfree(p,n) that will free any arbitrary block p of n characters
into the free list maintained by malloc and free. By using bfree, a user can add a static or
external array to the free list at any time. 
154
Appendix A - Reference Manual
A.1 Introduction
This manual describes the C language specified by the draft submitted to ANSI on 31 October,
1988, for approval as ``American Standard for Information Systems - programming Language
C, X3.159-1989.'' The manual is an interpretation of the proposed standard, not the standard
itself, although care has been taken to make it a reliable guide to the language.
For the most part, this document follows the broad outline of the standard, which in turn
follows that of the first edition of this book, although the organization differs in detail. Except
for renaming a few productions, and not formalizing the definitions of the lexical tokens or the
preprocessor, the grammar given here for the language proper is equivalent to that of the
standard.
Throughout this manual, commentary material is indented and written in smaller type, as this is. Most
often these comments highlight ways in which ANSI Standard C differs from the language defined by
the first edition of this book, or from refinements subsequently introduced in various compilers.
A.2 Lexical Conventions
A program consists of one or more translation units stored in files. It is translated in several
phases, which are described in Par.A.12. The first phases do low-level lexical transformations,
carry out directives introduced by the lines beginning with the # character, and perform macro
definition and expansion. When the preprocessing of Par.A.12 is complete, the program has
been reduced to a sequence of tokens.
A.2.1 Tokens
There are six classes of tokens: identifiers, keywords, constants, string literals, operators, and
other separators. Blanks, horizontal and vertical tabs, newlines, formfeeds and comments as
described below (collectively, ``white space'') are ignored except as they separate tokens.
Some white space is required to separate otherwise adjacent identifiers, keywords, and
constants.
If the input stream has been separated into tokens up to a given character, the next token is the
longest string of characters that could constitute a token.
A.2.2 Comments
The characters /* introduce a comment, which terminates with the characters */. Comments
do not nest, and they do not occur within a string or character literals.
A.2.3 Identifiers
An identifier is a sequence of letters and digits. The first character must be a letter; the
underscore _ counts as a letter. Upper and lower case letters are different. Identifiers may have
any length, and for internal identifiers, at least the first 31 characters are significant; some
implementations may take more characters significant. Internal identifiers include preprocessor
macro names and all other names that do not have external linkage (Par.A.11.2). Identifiers
with external linkage are more restricted: implementations may make as few as the first six
characters significant, and may ignore case distinctions.
A.2.4 Keywords
The following identifiers are reserved for the use as keywords, and may not be used otherwise:
 auto double int struct
 break else long switch
155
 case enum register typedef
 char extern return union
 const float short unsigned
 continue for signed void
 default goto sizeof volatile
 do if static while
Some implementations also reserve the words fortran and asm.
The keywords const, signed, and volatile are new with the ANSI standard; enum and void
are new since the first edition, but in common use; entry, formerly reserved but never used, is no
longer reserved.
A.2.5 Constants
There are several kinds of constants. Each has a data type; Par.A.4.2 discusses the basic types:
constant:
integer-constant
character-constant
floating-constant
enumeration-constant
A.2.5.1 Integer Constants
An integer constant consisting of a sequence of digits is taken to be octal if it begins with 0
(digit zero), decimal otherwise. Octal constants do not contain the digits 8 or 9. A sequence of
digits preceded by 0x or 0X (digit zero) is taken to be a hexadecimal integer. The hexadecimal
digits include a or A through f or F with values 10 through 15.
An integer constant may be suffixed by the letter u or U, to specify that it is unsigned. It may
also be suffixed by the letter l or L to specify that it is long.
The type of an integer constant depends on its form, value and suffix. (See Par.A.4 for a
discussion of types). If it is unsuffixed and decimal, it has the first of these types in which its
value can be represented: int, long int, unsigned long int. If it is unsuffixed, octal or
hexadecimal, it has the first possible of these types: int, unsigned int, long int, unsigned
long int. If it is suffixed by u or U, then unsigned int, unsigned long int. If it is
suffixed by l or L, then long int, unsigned long int. If an integer constant is suffixed by
UL, it is unsigned long.
The elaboration of the types of integer constants goes considerably beyond the first edition, which
merely caused large integer constants to be long. The U suffixes are new.
A.2.5.2 Character Constants
A character constant is a sequence of one or more characters enclosed in single quotes as in
'x'. The value of a character constant with only one character is the numeric value of the
character in the machine's character set at execution time. The value of a multi-character
constant is implementation-defined.
Character constants do not contain the ' character or newlines; in order to represent them, and
certain other characters, the following escape sequences may be used:
newline NL (LF) \n backslash \ \\
horizontal tab HT \t question mark ? \?
vertical tab VT \v single quote ' \'
backspace BS \b double quote " \"
carriage return CR \r octal number ooo \ooo
formfeed FF \f hex number hh \xhh
audible alert BEL \a
156
The escape \ooo consists of the backslash followed by 1, 2, or 3 octal digits, which are taken
to specify the value of the desired character. A common example of this construction is \0 (not
followed by a digit), which specifies the character NUL. The escape \xhh consists of the
backslash, followed by x, followed by hexadecimal digits, which are taken to specify the value
of the desired character. There is no limit on the number of digits, but the behavior is undefined
if the resulting character value exceeds that of the largest character. For either octal or
hexadecimal escape characters, if the implementation treats the char type as signed, the value
is sign-extended as if cast to char type. If the character following the \ is not one of those
specified, the behavior is undefined.
In some implementations, there is an extended set of characters that cannot be represented in
the char type. A constant in this extended set is written with a preceding L, for example L'x',
and is called a wide character constant. Such a constant has type wchar_t, an integral type
defined in the standard header <stddef.h>. As with ordinary character constants, hexadecimal
escapes may be used; the effect is undefined if the specified value exceeds that representable
with wchar_t.
Some of these escape sequences are new, in particular the hexadecimal character representation.
Extended characters are also new. The character sets commonly used in the Americas and western
Europe can be encoded to fit in the char type; the main intent in adding wchar_t was to
accommodate Asian languages.
A.2.5.3 Floating Constants
A floating constant consists of an integer part, a decimal part, a fraction part, an e or E, an
optionally signed integer exponent and an optional type suffix, one of f, F, l, or L. The integer
and fraction parts both consist of a sequence of digits. Either the integer part, or the fraction
part (not both) may be missing; either the decimal point or the e and the exponent (not both)
may be missing. The type is determined by the suffix; F or f makes it float, L or l makes it
long double, otherwise it is double.
A2.5.4 Enumeration Constants
Identifiers declared as enumerators (see Par.A.8.4) are constants of type int.
A.2.6 String Literals
A string literal, also called a string constant, is a sequence of characters surrounded by double
quotes as in "...". A string has type ``array of characters'' and storage class static (see
Par.A.3 below) and is initialized with the given characters. Whether identical string literals are
distinct is implementation-defined, and the behavior of a program that attempts to alter a string
literal is undefined.
Adjacent string literals are concatenated into a single string. After any concatenation, a null
byte \0 is appended to the string so that programs that scan the string can find its end. String
literals do not contain newline or double-quote characters; in order to represent them, the same
escape sequences as for character constants are available.
As with character constants, string literals in an extended character set are written with a
preceding L, as in L"...". Wide-character string literals have type ``array of wchar_t.''
Concatenation of ordinary and wide string literals is undefined.
The specification that string literals need not be distinct, and the prohibition against modifying them,
are new in the ANSI standard, as is the concatenation of adjacent string literals. Wide-character string
literals are new.
A.3 Syntax Notation
In the syntax notation used in this manual, syntactic categories are indicated by italic type, and
literal words and characters in typewriter style. Alternative categories are usually listed on
separate lines; in a few cases, a long set of narrow alternatives is presented on one line, marked
157
by the phrase ``one of.'' An optional terminal or nonterminal symbol carries the subscript ``opt,''
so that, for example,
{ expressionopt }
means an optional expression, enclosed in braces. The syntax is summarized in Par.A.13.
Unlike the grammar given in the first edition of this book, the one given here makes precedence and
associativity of expression operators explicit.
A.4 Meaning of Identifiers
Identifiers, or names, refer to a variety of things: functions; tags of structures, unions, and
enumerations; members of structures or unions; enumeration constants; typedef names; and
objects. An object, sometimes called a variable, is a location in storage, and its interpretation
depends on two main attributes: its storage class and its type. The storage class determines the
lifetime of the storage associated with the identified object; the type determines the meaning of
the values found in the identified object. A name also has a scope, which is the region of the
program in which it is known, and a linkage, which determines whether the same name in
another scope refers to the same object or function. Scope and linkage are discussed in
Par.A.11.
A.4.1 Storage Class
There are two storage classes: automatic and static. Several keywords, together with the
context of an object's declaration, specify its storage class. Automatic objects are local to a
block (Par.9.3), and are discarded on exit from the block. Declarations within a block create
automatic objects if no storage class specification is mentioned, or if the auto specifier is used.
Objects declared register are automatic, and are (if possible) stored in fast registers of the
machine.
Static objects may be local to a block or external to all blocks, but in either case retain their
values across exit from and reentry to functions and blocks. Within a block, including a block
that provides the code for a function, static objects are declared with the keyword static. The
objects declared outside all blocks, at the same level as function definitions, are always static.
They may be made local to a particular translation unit by use of the static keyword; this
gives them internal linkage. They become global to an entire program by omitting an explicit
storage class, or by using the keyword extern; this gives them external linkage.
A.4.2 Basic Types
There are several fundamental types. The standard header <limits.h> described in Appendix
B defines the largest and smallest values of each type in the local implementation. The numbers
given in Appendix B show the smallest acceptable magnitudes.
Objects declared as characters (char) are large enough to store any member of the execution
character set. If a genuine character from that set is stored in a char object, its value is
equivalent to the integer code for the character, and is non-negative. Other quantities may be
stored into char variables, but the available range of values, and especially whether the value is
signed, is implementation-dependent.
Unsigned characters declared unsigned char consume the same amount of space as plain
characters, but always appear non-negative; explicitly signed characters declared signed char
likewise take the same space as plain characters.
unsigned char type does not appear in the first edition of this book, but is in common use. signed
char is new.
Besides the char types, up to three sizes of integer, declared short int, int, and long int,
are available. Plain int objects have the natural size suggested by the host machine
158
architecture; the other sizes are provided to meet special needs. Longer integers provide at
least as much storage as shorter ones, but the implementation may make plain integers
equivalent to either short integers, or long integers. The int types all represent signed values
unless specified otherwise.
Unsigned integers, declared using the keyword unsigned, obey the laws of arithmetic modulo
2
n where n is the number of bits in the representation, and thus arithmetic on unsigned
quantities can never overflow. The set of non-negative values that can be stored in a signed
object is a subset of the values that can be stored in the corresponding unsigned object, and the
representation for the overlapping values is the same.
Any of single precision floating point (float), double precision floating point (double), and
extra precision floating point (long double) may be synonymous, but the ones later in the list
are at least as precise as those before.
long double is new. The first edition made long float equivalent to double; the locution has
been withdrawn.
Enumerations are unique types that have integral values; associated with each enumeration is a
set of named constants (Par.A.8.4). Enumerations behave like integers, but it is common for a
compiler to issue a warning when an object of a particular enumeration is assigned something
other than one of its constants, or an expression of its type.
Because objects of these types can be interpreted as numbers, they will be referred to as
arithmetic types. Types char, and int of all sizes, each with or without sign, and also
enumeration types, will collectively be called integral types. The types float, double, and
long double will be called floating types.
The void type specifies an empty set of values. It is used as the type returned by functions that
generate no value.
A.4.3 Derived types
Beside the basic types, there is a conceptually infinite class of derived types constructed from
the fundamental types in the following ways:
arrays of objects of a given type;
functions returning objects of a given type;
pointers to objects of a given type;
structures containing a sequence of objects of various types;
unions capable of containing any of one of several objects of various types.
In general these methods of constructing objects can be applied recursively.
A.4.4 Type Qualifiers
An object's type may have additional qualifiers. Declaring an object const announces that its
value will not be changed; declaring it volatile announces that it has special properties
relevant to optimization. Neither qualifier affects the range of values or arithmetic properties of
the object. Qualifiers are discussed in Par.A.8.2.
A.5 Objects and Lvalues
An Object is a named region of storage; an lvalue is an expression referring to an object. An
obvious example of an lvalue expression is an identifier with suitable type and storage class.
There are operators that yield lvalues, if E is an expression of pointer type, then *E is an lvalue
expression referring to the object to which E points. The name ``lvalue'' comes from the
assignment expression E1 = E2 in which the left operand E1 must be an lvalue expression. The
159
discussion of each operator specifies whether it expects lvalue operands and whether it yields
an lvalue.
A.6 Conversions
Some operators may, depending on their operands, cause conversion of the value of an
operand from one type to another. This section explains the result to be expected from such
conversions. Par.6.5 summarizes the conversions demanded by most ordinary operators; it will
be supplemented as required by the discussion of each operator.
A.6.1 Integral Promotion
A character, a short integer, or an integer bit-field, all either signed or not, or an object of
enumeration type, may be used in an expression wherever an integer may be used. If an int
can represent all the values of the original type, then the value is converted to int; otherwise
the value is converted to unsigned int. This process is called integral promotion.
A.6.2 Integral Conversions
Any integer is converted to a given unsigned type by finding the smallest non-negative value
that is congruent to that integer, modulo one more than the largest value that can be
represented in the unsigned type. In a two's complement representation, this is equivalent to
left-truncation if the bit pattern of the unsigned type is narrower, and to zero-filling unsigned
values and sign-extending signed values if the unsigned type is wider.
When any integer is converted to a signed type, the value is unchanged if it can be represented
in the new type and is implementation-defined otherwise.
A.6.3 Integer and Floating
When a value of floating type is converted to integral type, the fractional part is discarded; if
the resulting value cannot be represented in the integral type, the behavior is undefined. In
particular, the result of converting negative floating values to unsigned integral types is not
specified.
When a value of integral type is converted to floating, and the value is in the representable
range but is not exactly representable, then the result may be either the next higher or next
lower representable value. If the result is out of range, the behavior is undefined.
A.6.4 Floating Types
When a less precise floating value is converted to an equally or more precise floating type, the
value is unchanged. When a more precise floating value is converted to a less precise floating
type, and the value is within representable range, the result may be either the next higher or the
next lower representable value. If the result is out of range, the behavior is undefined.
A.6.5 Arithmetic Conversions
Many operators cause conversions and yield result types in a similar way. The effect is to bring
operands into a common type, which is also the type of the result. This pattern is called the
usual arithmetic conversions.
• First, if either operand is long double, the other is converted to long double.
• Otherwise, if either operand is double, the other is converted to double.
• Otherwise, if either operand is float, the other is converted to float.
• Otherwise, the integral promotions are performed on both operands; then, if either
operand is unsigned long int, the other is converted to unsigned long int. 
160
• Otherwise, if one operand is long int and the other is unsigned int, the effect
depends on whether a long int can represent all values of an unsigned int; if so,
the unsigned int operand is converted to long int; if not, both are converted to
unsigned long int.
• Otherwise, if one operand is long int, the other is converted to long int.
• Otherwise, if either operand is unsigned int, the other is converted to unsigned
int.
• Otherwise, both operands have type int.
There are two changes here. First, arithmetic on float operands may be done in single precision,
rather than double; the first edition specified that all floating arithmetic was double precision. Second,
shorter unsigned types, when combined with a larger signed type, do not propagate the unsigned
property to the result type; in the first edition, the unsigned always dominated. The new rules are
slightly more complicated, but reduce somewhat the surprises that may occur when an unsigned
quantity meets signed. Unexpected results may still occur when an unsigned expression is compared to
a signed expression of the same size.
A.6.6 Pointers and Integers
An expression of integral type may be added to or subtracted from a pointer; in such a case the
integral expression is converted as specified in the discussion of the addition operator
(Par.A.7.7).
Two pointers to objects of the same type, in the same array, may be subtracted; the result is
converted to an integer as specified in the discussion of the subtraction operator (Par.A.7.7).
An integral constant expression with value 0, or such an expression cast to type void *, may
be converted, by a cast, by assignment, or by comparison, to a pointer of any type. This
produces a null pointer that is equal to another null pointer of the same type, but unequal to
any pointer to a function or object.
Certain other conversions involving pointers are permitted, but have implementation-defined
aspects. They must be specified by an explicit type-conversion operator, or cast (Pars.A.7.5
and A.8.8).
A pointer may be converted to an integral type large enough to hold it; the required size is
implementation-dependent. The mapping function is also implementation-dependent.
A pointer to one type may be converted to a pointer to another type. The resulting pointer may
cause addressing exceptions if the subject pointer does not refer to an object suitably aligned in
storage. It is guaranteed that a pointer to an object may be converted to a pointer to an object
whose type requires less or equally strict storage alignment and back again without change; the
notion of ``alignment'' is implementation-dependent, but objects of the char types have least
strict alignment requirements. As described in Par.A.6.8, a pointer may also be converted to
type void * and back again without change.
A pointer may be converted to another pointer whose type is the same except for the addition
or removal of qualifiers (Pars.A.4.4, A.8.2) of the object type to which the pointer refers. If
qualifiers are added, the new pointer is equivalent to the old except for restrictions implied by
the new qualifiers. If qualifiers are removed, operations on the underlying object remain subject
to the qualifiers in its actual declaration.
Finally, a pointer to a function may be converted to a pointer to another function type. Calling
the function specified by the converted pointer is implementation-dependent; however, if the
converted pointer is reconverted to its original type, the result is identical to the original
pointer. 
161
A.6.7 Void
The (nonexistent) value of a void object may not be used in any way, and neither explicit nor
implicit conversion to any non-void type may be applied. Because a void expression denotes a
nonexistent value, such an expression may be used only where the value is not required, for
example as an expression statement (Par.A.9.2) or as the left operand of a comma operator
(Par.A.7.18).
An expression may be converted to type void by a cast. For example, a void cast documents
the discarding of the value of a function call used as an expression statement.
void did not appear in the first edition of this book, but has become common since.
A.6.8 Pointers to Void
Any pointer to an object may be converted to type void * without loss of information. If the
result is converted back to the original pointer type, the original pointer is recovered. Unlike
the pointer-to-pointer conversions discussed in Par.A.6.6, which generally require an explicit
cast, pointers may be assigned to and from pointers of type void *, and may be compared
with them.
This interpretation of void * pointers is new; previously, char * pointers played the role of
generic pointer. The ANSI standard specifically blesses the meeting of void * pointers with object
pointers in assignments and relationals, while requiring explicit casts for other pointer mixtures.
A.7 Expressions
The precedence of expression operators is the same as the order of the major subsections of
this section, highest precedence first. Thus, for example, the expressions referred to as the
operands of + (Par.A.7.7) are those expressions defined in Pars.A.7.1-A.7.6. Within each
subsection, the operators have the same precedence. Left- or right-associativity is specified in
each subsection for the operators discussed therein. The grammar given in Par.13 incorporates
the precedence and associativity of the operators.
The precedence and associativity of operators is fully specified, but the order of evaluation of
expressions is, with certain exceptions, undefined, even if the subexpressions involve side
effects. That is, unless the definition of the operator guarantees that its operands are evaluated
in a particular order, the implementation is free to evaluate operands in any order, or even to
interleave their evaluation. However, each operator combines the values produced by its
operands in a way compatible with the parsing of the expression in which it appears.
This rule revokes the previous freedom to reorder expressions with operators that are mathematically
commutative and associative, but can fail to be computationally associative. The change affects only
floating-point computations near the limits of their accuracy, and situations where overflow is
possible.
The handling of overflow, divide check, and other exceptions in expression evaluation is not
defined by the language. Most existing implementations of C ignore overflow in evaluation of
signed integral expressions and assignments, but this behavior is not guaranteed. Treatment of
division by 0, and all floating-point exceptions, varies among implementations; sometimes it is
adjustable by a non-standard library function.
A.7.1 Pointer Conversion
If the type of an expression or subexpression is ``array of T,'' for some type T, then the value of
the expression is a pointer to the first object in the array, and the type of the expression is
altered to ``pointer to T.'' This conversion does not take place if the expression is in the
operand of the unary & operator, or of ++, --, sizeof, or as the left operand of an assignment
operator or the . operator. Similarly, an expression of type ``function returning T,'' except
when used as the operand of the & operator, is converted to ``pointer to function returning T.''
A.7.2 Primary Expressions
162
Primary expressions are identifiers, constants, strings, or expressions in parentheses.
primary-expression
identifier
constant
string
(expression)
An identifier is a primary expression, provided it has been suitably declared as discussed below.
Its type is specified by its declaration. An identifier is an lvalue if it refers to an object
(Par.A.5) and if its type is arithmetic, structure, union, or pointer.
A constant is a primary expression. Its type depends on its form as discussed in Par.A.2.5.
A string literal is a primary expression. Its type is originally ``array of char'' (for wide-char
strings, ``array of wchar_t''), but following the rule given in Par.A.7.1, this is usually modified
to ``pointer to char'' (wchar_t) and the result is a pointer to the first character in the string.
The conversion also does not occur in certain initializers; see Par.A.8.7.
A parenthesized expression is a primary expression whose type and value are identical to those
of the unadorned expression. The precedence of parentheses does not affect whether the
expression is an lvalue.
A.7.3 Postfix Expressions
The operators in postfix expressions group left to right.
postfix-expression:
primary-expression
postfix-expression[expression]
postfix-expression(argument-expression-listopt)
postfix-expression.identifier
postfix-expression->identifier
postfix-expression++
postfix-expression--
argument-expression-list:
assignment-expression
assignment-expression-list , assignment-expression
A.7.3.1 Array References
A postfix expression followed by an expression in square brackets is a postfix expression
denoting a subscripted array reference. One of the two expressions must have type ``pointer to
T'', where T is some type, and the other must have integral type; the type of the subscript
expression is T. The expression E1[E2] is identical (by definition) to *((E1)+(E2)). See
Par.A.8.6.2 for further discussion.
A.7.3.2 Function Calls
A function call is a postfix expression, called the function designator, followed by parentheses
containing a possibly empty, comma-separated list of assignment expressions (Par.A7.17),
which constitute the arguments to the function. If the postfix expression consists of an
identifier for which no declaration exists in the current scope, the identifier is implicitly
declared as if the declaration
extern int identifier();
163
had been given in the innermost block containing the function call. The postfix expression
(after possible explicit declaration and pointer generation, Par.A7.1) must be of type ``pointer
to function returning T,'' for some type T, and the value of the function call has type T.
In the first edition, the type was restricted to ``function,'' and an explicit * operator was required to
call through pointers to functions. The ANSI standard blesses the practice of some existing compilers
by permitting the same syntax for calls to functions and to functions specified by pointers. The older
syntax is still usable.
The term argument is used for an expression passed by a function call; the term parameter is
used for an input object (or its identifier) received by a function definition, or described in a
function declaration. The terms ``actual argument (parameter)'' and ``formal argument
(parameter)'' respectively are sometimes used for the same distinction.
In preparing for the call to a function, a copy is made of each argument; all argument-passing
is strictly by value. A function may change the values of its parameter objects, which are copies
of the argument expressions, but these changes cannot affect the values of the arguments.
However, it is possible to pass a pointer on the understanding that the function may change the
value of the object to which the pointer points.
There are two styles in which functions may be declared. In the new style, the types of
parameters are explicit and are part of the type of the function; such a declaration os also
called a function prototype. In the old style, parameter types are not specified. Function
declaration is issued in Pars.A.8.6.3 and A.10.1.
If the function declaration in scope for a call is old-style, then default argument promotion is
applied to each argument as follows: integral promotion (Par.A.6.1) is performed on each
argument of integral type, and each float argument is converted to double. The effect of the
call is undefined if the number of arguments disagrees with the number of parameters in the
definition of the function, or if the type of an argument after promotion disagrees with that of
the corresponding parameter. Type agreement depends on whether the function's definition is
new-style or old-style. If it is old-style, then the comparison is between the promoted type of
the arguments of the call, and the promoted type of the parameter, if the definition is newstyle, the promoted type of the argument must be that of the parameter itself, without
promotion.
If the function declaration in scope for a call is new-style, then the arguments are converted, as
if by assignment, to the types of the corresponding parameters of the function's prototype. The
number of arguments must be the same as the number of explicitly described parameters,
unless the declaration's parameter list ends with the ellipsis notation (, ...). In that case, the
number of arguments must equal or exceed the number of parameters; trailing arguments
beyond the explicitly typed parameters suffer default argument promotion as described in the
preceding paragraph. If the definition of the function is old-style, then the type of each
parameter in the definition, after the definition parameter's type has undergone argument
promotion.
These rules are especially complicated because they must cater to a mixture of old- and new-style
functions. Mixtures are to be avoided if possible.
The order of evaluation of arguments is unspecified; take note that various compilers differ.
However, the arguments and the function designator are completely evaluated, including all
side effects, before the function is entered. Recursive calls to any function are permitted.
A.7.3.3 Structure References
A postfix expression followed by a dot followed by an identifier is a postfix expression. The
first operand expression must be a structure or a union, and the identifier must name a member
of the structure or union. The value is the named member of the structure or union, and its
164
type is the type of the member. The expression is an lvalue if the first expression is an lvalue,
and if the type of the second expression is not an array type.
A postfix expression followed by an arrow (built from - and >) followed by an identifier is a
postfix expression. The first operand expression must be a pointer to a structure or union, and
the identifier must name a member of the structure or union. The result refers to the named
member of the structure or union to which the pointer expression points, and the type is the
type of the member; the result is an lvalue if the type is not an array type.
Thus the expression E1->MOS is the same as (*E1).MOS. Structures and unions are discussed in
Par.A.8.3.
In the first edition of this book, it was already the rule that a member name in such an expression had
to belong to the structure or union mentioned in the postfix expression; however, a note admitted that
this rule was not firmly enforced. Recent compilers, and ANSI, do enforce it.
A.7.3.4 Postfix Incrementation
A postfix expression followed by a ++ or -- operator is a postfix expression. The value of the
expression is the value of the operand. After the value is noted, the operand is incremented ++
or decremented -- by 1. The operand must be an lvalue; see the discussion of additive
operators (Par.A.7.7) and assignment (Par.A.7.17) for further constraints on the operand and
details of the operation. The result is not an lvalue.
A.7.4 Unary Operators
Expressions with unary operators group right-to-left.
unary-expression:
postfix expression
++unary expression
--unary expression
unary-operator cast-expression
sizeof unary-expression
sizeof(type-name)
unary operator: one of
& * + - ~ !
A.7.4.1 Prefix Incrementation Operators
A unary expression followed by a ++ or -- operator is a unary expression. The operand is
incremented ++ or decremented -- by 1. The value of the expression is the value after the
incrementation (decrementation). The operand must be an lvalue; see the discussion of additive
operators (Par.A.7.7) and assignment (Par.A.7.17) for further constraints on the operands and
details of the operation. The result is not an lvalue.
A.7.4.2 Address Operator
The unary operator & takes the address of its operand. The operand must be an lvalue referring
neither to a bit-field nor to an object declared as register, or must be of function type. The
result is a pointer to the object or function referred to by the lvalue. If the type of the operand
is T, the type of the result is ``pointer to T.''
A.7.4.3 Indirection Operator
The unary * operator denotes indirection, and returns the object or function to which its
operand points. It is an lvalue if the operand is a pointer to an object of arithmetic, structure,
union, or pointer type. If the type of the expression is ``pointer to T,'' the type of the result is T.
A.7.4.4 Unary Plus Operator
165
The operand of the unary + operator must have arithmetic type, and the result is the value of
the operand. An integral operand undergoes integral promotion. The type of the result is the
type of the promoted operand.
The unary + is new with the ANSI standard. It was added for symmetry with the unary -.
A.7.4.5 Unary Minus Operator
The operand of the unary - operator must have arithmetic type, and the result is the negative
of its operand. An integral operand undergoes integral promotion. The negative of an unsigned
quantity is computed by subtracting the promoted value from the largest value of the promoted
type and adding one; but negative zero is zero. The type of the result is the type of the
promoted operand.
A.7.4.6 One's Complement Operator
The operand of the ~ operator must have integral type, and the result is the one's complement
of its operand. The integral promotions are performed. If the operand is unsigned, the result is
computed by subtracting the value from the largest value of the promoted type. If the operand
is signed, the result is computed by converting the promoted operand to the corresponding
unsigned type, applying ~, and converting back to the signed type. The type of the result is the
type of the promoted operand.
A.7.4.7 Logical Negation Operator
The operand of the ! operator must have arithmetic type or be a pointer, and the result is 1 if
the value of its operand compares equal to 0, and 0 otherwise. The type of the result is int.
A.7.4.8 Sizeof Operator
The sizeof operator yields the number of bytes required to store an object of the type of its
operand. The operand is either an expression, which is not evaluated, or a parenthesized type
name. When sizeof is applied to a char, the result is 1; when applied to an array, the result is
the total number of bytes in the array. When applied to a structure or union, the result is the
number of bytes in the object, including any padding required to make the object tile an array:
the size of an array of n elements is n times the size of one element. The operator may not be
applied to an operand of function type, or of incomplete type, or to a bit-field. The result is an
unsigned integral constant; the particular type is implementation-defined. The standard header
<stddef.h> (See appendix B) defines this type as size_t.
A.7.5 Casts
A unary expression preceded by the parenthesized name of a type causes conversion of the
value of the expression to the named type.
cast-expression:
unary expression
(type-name) cast-expression
This construction is called a cast. The names are described in Par.A.8.8. The effects of
conversions are described in Par.A.6. An expression with a cast is not an lvalue.
A.7.6 Multiplicative Operators
The multiplicative operators *, /, and % group left-to-right.
multiplicative-expression:
multiplicative-expression * cast-expression
multiplicative-expression / cast-expression
multiplicative-expression % cast-expression
166
The operands of * and / must have arithmetic type; the operands of % must have integral type.
The usual arithmetic conversions are performed on the operands, and predict the type of the
result.
The binary * operator denotes multiplication.
The binary / operator yields the quotient, and the % operator the remainder, of the division of
the first operand by the second; if the second operand is 0, the result is undefined. Otherwise, it
is always true that (a/b)*b + a%b is equal to a. If both operands are non-negative, then the
remainder is non-negative and smaller than the divisor, if not, it is guaranteed only that the
absolute value of the remainder is smaller than the absolute value of the divisor.
A.7.7 Additive Operators
The additive operators + and - group left-to-right. If the operands have arithmetic type, the
usual arithmetic conversions are performed. There are some additional type possibilities for
each operator.
additive-expression:
multiplicative-expression
additive-expression + multiplicative-expression
additive-expression - multiplicative-expression
The result of the + operator is the sum of the operands. A pointer to an object in an array and a
value of any integral type may be added. The latter is converted to an address offset by
multiplying it by the size of the object to which the pointer points. The sum is a pointer of the
same type as the original pointer, and points to another object in the same array, appropriately
offset from the original object. Thus if P is a pointer to an object in an array, the expression
P+1 is a pointer to the next object in the array. If the sum pointer points outside the bounds of
the array, except at the first location beyond the high end, the result is undefined.
The provision for pointers just beyond the end of an array is new. It legitimizes a common idiom for
looping over the elements of an array.
The result of the - operator is the difference of the operands. A value of any integral type may
be subtracted from a pointer, and then the same conversions and conditions as for addition
apply.
If two pointers to objects of the same type are subtracted, the result is a signed integral value
representing the displacement between the pointed-to objects; pointers to successive objects
differ by 1. The type of the result is defined as ptrdiff_t in the standard header <stddef.h>.
The value is undefined unless the pointers point to objects within the same array; however, if P
points to the last member of an array, then (P+1)-P has value 1.
A.7.8 Shift Operators
The shift operators << and >> group left-to-right. For both operators, each operand must be
integral, and is subject to integral the promotions. The type of the result is that of the
promoted left operand. The result is undefined if the right operand is negative, or greater than
or equal to the number of bits in the left expression's type.
shift-expression:
additive-expression
shift-expression << additive-expression
shift-expression >> additive-expression
The value of E1<<E2 is E1 (interpreted as a bit pattern) left-shifted E2 bits; in the absence of
overflow, this is equivalent to multiplication by 2
E2
. The value of E1>>E2 is E1 right-shifted E2
167
bit positions. The right shift is equivalent to division by 2
E2 if E1 is unsigned or it has a nonnegative value; otherwise the result is implementation-defined.
A.7.9 Relational Operators
The relational operators group left-to-right, but this fact is not useful; a<b<c is parsed as
(a<b)<c, and evaluates to either 0 or 1.
relational-expression:
shift-expression
relational-expression < shift-expression
relational-expression > shift-expression
relational-expression <= shift-expression
relational-expression >= shift-expression
The operators < (less), > (greater), <= (less or equal) and >= (greater or equal) all yield 0 if the
specified relation is false and 1 if it is true. The type of the result is int. The usual arithmetic
conversions are performed on arithmetic operands. Pointers to objects of the same type
(ignoring any qualifiers) may be compared; the result depends on the relative locations in the
address space of the pointed-to objects. Pointer comparison is defined only for parts of the
same object; if two pointers point to the same simple object, they compare equal; if the
pointers are to members of the same structure, pointers to objects declared later in the
structure compare higher; if the pointers refer to members of an array, the comparison is
equivalent to comparison of the the corresponding subscripts. If P points to the last member of
an array, then P+1 compares higher than P, even though P+1 points outside the array.
Otherwise, pointer comparison is undefined.
These rules slightly liberalize the restrictions stated in the first edition, by permitting comparison of
pointers to different members of a structure or union. They also legalize comparison with a pointer just
off the end of an array.
A.7.10 Equality Operators
equality-expression:
relational-expression
equality-expression == relational-expression
equality-expression != relational-expression
The == (equal to) and the != (not equal to) operators are analogous to the relational operators
except for their lower precedence. (Thus a<b == c<d is 1 whenever a<b and c<d have the
same truth-value.)
The equality operators follow the same rules as the relational operators, but permit additional
possibilities: a pointer may be compared to a constant integral expression with value 0, or to a
pointer to void. See Par.A.6.6.
A.7.11 Bitwise AND Operator
AND-expression:
equality-expression
AND-expression & equality-expression
The usual arithmetic conversions are performed; the result is the bitwise AND function of the
operands. The operator applies only to integral operands.
A.7.12 Bitwise Exclusive OR Operator
exclusive-OR-expression:
AND-expression
exclusive-OR-expression ^ AND-expression
168
The usual arithmetic conversions are performed; the result is the bitwise exclusive OR function
of the operands. The operator applies only to integral operands.
A.7.13 Bitwise Inclusive OR Operator
inclusive-OR-expression:
exclusive-OR-expression
inclusive-OR-expression | exclusive-OR-expression
The usual arithmetic conversions are performed; the result is the bitwise inclusive OR function
of the operands. The operator applies only to integral operands.
A.7.14 Logical AND Operator
logical-AND-expression:
inclusive-OR-expression
logical-AND-expression && inclusive-OR-expression
The && operator groups left-to-right. It returns 1 if both its operands compare unequal to zero,
0 otherwise. Unlike &, && guarantees left-to-right evaluation: the first operand is evaluated,
including all side effects; if it is equal to 0, the value of the expression is 0. Otherwise, the right
operand is evaluated, and if it is equal to 0, the expression's value is 0, otherwise 1.
The operands need not have the same type, but each must have arithmetic type or be a pointer.
The result is int.
A.7.15 Logical OR Operator
logical-OR-expression:
logical-AND-expression
logical-OR-expression || logical-AND-expression
The || operator groups left-to-right. It returns 1 if either of its operands compare unequal to
zero, and 0 otherwise. Unlike |, || guarantees left-to-right evaluation: the first operand is
evaluated, including all side effects; if it is unequal to 0, the value of the expression is 1.
Otherwise, the right operand is evaluated, and if it is unequal to 0, the expression's value is 1,
otherwise 0.
The operands need not have the same type, but each must have arithmetic type or be a pointer.
The result is int.
A.7.16 Conditional Operator
conditional-expression:
logical-OR-expression
logical-OR-expression ? expression : conditional-expression
The first expression is evaluated, including all side effects; if it compares unequal to 0, the
result is the value of the second expression, otherwise that of the third expression. Only one of
the second and third operands is evaluated. If the second and third operands are arithmetic, the
usual arithmetic conversions are performed to bring them to a common type, and that type is
the type of the result. If both are void, or structures or unions of the same type, or pointers to
objects of the same type, the result has the common type. If one is a pointer and the other the
constant 0, the 0 is converted to the pointer type, and the result has that type. If one is a
pointer to void and the other is another pointer, the other pointer is converted to a pointer to
void, and that is the type of the result. 
169
In the type comparison for pointers, any type qualifiers (Par.A.8.2) in the type to which the
pointer points are insignificant, but the result type inherits qualifiers from both arms of the
conditional.
A.7.17 Assignment Expressions
There are several assignment operators; all group right-to-left.
assignment-expression:
conditional-expression
unary-expression assignment-operator assignment-expression
assignment-operator: one of
= *= /= %= += -= <<= >>= &= ^= |=
All require an lvalue as left operand, and the lvalue must be modifiable: it must not be an array,
and must not have an incomplete type, or be a function. Also, its type must not be qualified
with const; if it is a structure or union, it must not have any member or, recursively,
submember qualified with const. The type of an assignment expression is that of its left
operand, and the value is the value stored in the left operand after the assignment has taken
place.
In the simple assignment with =, the value of the expression replaces that of the object referred
to by the lvalue. One of the following must be true: both operands have arithmetic type, in
which case the right operand is converted to the type of the left by the assignment; or both
operands are structures or unions of the same type; or one operand is a pointer and the other is
a pointer to void, or the left operand is a pointer and the right operand is a constant
expression with value 0; or both operands are pointers to functions or objects whose types are
the same except for the possible absence of const or volatile in the right operand.
An expression of the form E1 op= E2 is equivalent to E1 = E1 op (E2) except that E1 is
evaluated only once.
A.7.18 Comma Operator
expression:
assignment-expression
expression , assignment-expression
A pair of expressions separated by a comma is evaluated left-to-right, and the value of the left
expression is discarded. The type and value of the result are the type and value of the right
operand. All side effects from the evaluation of the left-operand are completed before
beginning the evaluation of the right operand. In contexts where comma is given a special
meaning, for example in lists of function arguments (Par.A.7.3.2) and lists of initializers
(Par.A.8.7), the required syntactic unit is an assignment expression, so the comma operator
appears only in a parenthetical grouping, for example,
 f(a, (t=3, t+2), c)
has three arguments, the second of which has the value 5.
A.7.19 Constant Expressions
Syntactically, a constant expression is an expression restricted to a subset of operators:
constant-expression:
conditional-expression
170
Expressions that evaluate to a constant are required in several contexts: after case, as array
bounds and bit-field lengths, as the value of an enumeration constant, in initializers, and in
certain preprocessor expressions.
Constant expressions may not contain assignments, increment or decrement operators, function
calls, or comma operators; except in an operand of sizeof. If the constant expression is
required to be integral, its operands must consist of integer, enumeration, character, and
floating constants; casts must specify an integral type, and any floating constants must be cast
to integer. This necessarily rules out arrays, indirection, address-of, and structure member
operations. (However, any operand is permitted for sizeof.)
More latitude is permitted for the constant expressions of initializers; the operands may be any
type of constant, and the unary & operator may be applied to external or static objects, and to
external and static arrays subscripted with a constant expression. The unary & operator can
also be applied implicitly by appearance of unsubscripted arrays and functions. Initializers must
evaluate either to a constant or to the address of a previously declared external or static object
plus or minus a constant.
Less latitude is allowed for the integral constant expressions after #if; sizeof expressions,
enumeration constants, and casts are not permitted. See Par.A.12.5.
A.8 Declarations
Declarations specify the interpretation given to each identifier; they do not necessarily reserve
storage associated with the identifier. Declarations that reserve storage are called definitions.
Declarations have the form
declaration:
declaration-specifiers init-declarator-listopt;
The declarators in the init-declarator list contain the identifiers being declared; the declarationspecifiers consist of a sequence of type and storage class specifiers.
declaration-specifiers:
storage-class-specifier declaration-specifiersopt
type-specifier declaration-specifiersopt
type-qualifier declaration-specifiersopt
init-declarator-list:
init-declarator
init-declarator-list , init-declarator
init-declarator:
declarator
declarator = initializer
Declarators will be discussed later (Par.A.8.5); they contain the names being declared. A
declaration must have at least one declarator, or its type specifier must declare a structure tag,
a union tag, or the members of an enumeration; empty declarations are not permitted.
A.8.1 Storage Class Specifiers
The storage class specifiers are:
storage-class specifier:
auto
register
171
static
extern
typedef
The meaning of the storage classes were discussed in Par.A.4.4.
The auto and register specifiers give the declared objects automatic storage class, and may
be used only within functions. Such declarations also serve as definitions and cause storage to
be reserved. A register declaration is equivalent to an auto declaration, but hints that the
declared objects will be accessed frequently. Only a few objects are actually placed into
registers, and only certain types are eligible; the restrictions are implementation-dependent.
However, if an object is declared register, the unary & operator may not be applied to it,
explicitly or implicitly.
The rule that it is illegal to calculate the address of an object declared register, but actually taken
to be auto, is new.
The static specifier gives the declared objects static storage class, and may be used either
inside or outside functions. Inside a function, this specifier causes storage to be allocated, and
serves as a definition; for its effect outside a function, see Par.A.11.2.
A declaration with extern, used inside a function, specifies that the storage for the declared
objects is defined elsewhere; for its effects outside a function, see Par.A.11.2.
The typedef specifier does not reserve storage and is called a storage class specifier only for
syntactic convenience; it is discussed in Par.A.8.9.
At most one storage class specifier may be given in a declaration. If none is given, these rules
are used: objects declared inside a function are taken to be auto; functions declared within a
function are taken to be extern; objects and functions declared outside a function are taken to
be static, with external linkage. See Pars. A.10-A.11.
A.8.2 Type Specifiers
The type-specifiers are
type specifier:
void
char
short
int
long
float
double
signed
unsigned
struct-or-union-specifier
enum-specifier
typedef-name
At most one of the words long or short may be specified together with int; the meaning is
the same if int is not mentioned. The word long may be specified together with double. At
most one of signed or unsigned may be specified together with int or any of its short or
long varieties, or with char. Either may appear alone in which case int is understood. The
signed specifier is useful for forcing char objects to carry a sign; it is permissible but
redundant with other integral types. 
172
Otherwise, at most one type-specifier may be given in a declaration. If the type-specifier is
missing from a declaration, it is taken to be int.
Types may also be qualified, to indicate special properties of the objects being declared.
type-qualifier:
const
volatile
Type qualifiers may appear with any type specifier. A const object may be initialized, but not
thereafter assigned to. There are no implementation-dependent semantics for volatile
objects.
The const and volatile properties are new with the ANSI standard. The purpose of const is to
announce objects that may be placed in read-only memory, and perhaps to increase opportunities for
optimization. The purpose of volatile is to force an implementation to suppress optimization that
could otherwise occur. For example, for a machine with memory-mapped input/output, a pointer to a
device register might be declared as a pointer to volatile, in order to prevent the compiler from
removing apparently redundant references through the pointer. Except that it should diagnose explicit
attempts to change const objects, a compiler may ignore these qualifiers.
A.8.3 Structure and Union Declarations
A structure is an object consisting of a sequence of named members of various types. A union
is an object that contains, at different times, any of several members of various types. Structure
and union specifiers have the same form.
struct-or-union-specifier:
struct-or-union identifieropt{ struct-declaration-list }
struct-or-union identifier
struct-or-union:
struct
union
A struct-declaration-list is a sequence of declarations for the members of the structure or
union:
struct-declaration-list:
struct declaration
struct-declaration-list struct declaration
struct-declaration: specifier-qualifier-list struct-declarator-list;
specifier-qualifier-list:
type-specifier specifier-qualifier-listopt
type-qualifier specifier-qualifier-listopt
struct-declarator-list:
struct-declarator
struct-declarator-list , struct-declarator
Usually, a struct-declarator is just a declarator for a member of a structure or union. A
structure member may also consist of a specified number of bits. Such a member is also called
a bit-field; its length is set off from the declarator for the field name by a colon.
struct-declarator:
declarator declaratoropt : constant-expression
173
A type specifier of the form
struct-or-union identifier { struct-declaration-list }
declares the identifier to be the tag of the structure or union specified by the list. A subsequent
declaration in the same or an inner scope may refer to the same type by using the tag in a
specifier without the list:
struct-or-union identifier
If a specifier with a tag but without a list appears when the tag is not declared, an incomplete
type is specified. Objects with an incomplete structure or union type may be mentioned in
contexts where their size is not needed, for example in declarations (not definitions), for
specifying a pointer, or for creating a typedef, but not otherwise. The type becomes complete
on occurrence of a subsequent specifier with that tag, and containing a declaration list. Even in
specifiers with a list, the structure or union type being declared is incomplete within the list,
and becomes complete only at the } terminating the specifier.
A structure may not contain a member of incomplete type. Therefore, it is impossible to
declare a structure or union containing an instance of itself. However, besides giving a name to
the structure or union type, tags allow definition of self-referential structures; a structure or
union may contain a pointer to an instance of itself, because pointers to incomplete types may
be declared.
A very special rule applies to declarations of the form
struct-or-union identifier;
that declare a structure or union, but have no declaration list and no declarators. Even if the
identifier is a structure or union tag already declared in an outer scope (Par.A.11.1), this
declaration makes the identifier the tag of a new, incompletely-typed structure or union in the
current scope.
This recondite is new with ANSI. It is intended to deal with mutually-recursive structures declared in
an inner scope, but whose tags might already be declared in the outer scope.
A structure or union specifier with a list but no tag creates a unique type; it can be referred to
directly only in the declaration of which it is a part.
The names of members and tags do not conflict with each other or with ordinary variables. A
member name may not appear twice in the same structure or union, but the same member name
may be used in different structures or unions.
In the first edition of this book, the names of structure and union members were not associated with
their parent. However, this association became common in compilers well before the ANSI standard.
A non-field member of a structure or union may have any object type. A field member (which
need not have a declarator and thus may be unnamed) has type int, unsigned int, or signed
int, and is interpreted as an object of integral type of the specified length in bits; whether an
int field is treated as signed is implementation-dependent. Adjacent field members of
structures are packed into implementation-dependent storage units in an implementationdependent direction. When a field following another field will not fit into a partially-filled
storage unit, it may be split between units, or the unit may be padded. An unnamed field with
width 0 forces this padding, so that the next field will begin at the edge of the next allocation
unit.
The ANSI standard makes fields even more implementation-dependent than did the first edition. It is
advisable to read the language rules for storing bit-fields as ``implementation-dependent'' without
qualification. Structures with bit-fields may be used as a portable way of attempting to reduce the
storage required for a structure (with the probable cost of increasing the instruction space, and time,
174
needed to access the fields), or as a non-portable way to describe a storage layout known at the bitlevel. In the second case, it is necessary to understand the rules of the local implementation.
The members of a structure have addresses increasing in the order of their declarations. A nonfield member of a structure is aligned at an addressing boundary depending on its type;
therefore, there may be unnamed holes in a structure. If a pointer to a structure is cast to the
type of a pointer to its first member, the result refers to the first member.
A union may be thought of as a structure all of whose members begin at offset 0 and whose
size is sufficient to contain any of its members. At most one of the members can be stored in a
union at any time. If a pointr to a union is cast to the type of a pointer to a member, the result
refers to that member.
A simple example of a structure declaration is
 struct tnode {
 char tword[20];
 int count;
 struct tnode *left;
 struct tnode *right;
 }
which contains an array of 20 characters, an integer, and two pointers to similar structures.
Once this declaration has bene given, the declaration
 struct tnode s, *sp;
declares s to be a structure of the given sort, and sp to be a pointer to a structure of the given
sort. With these declarations, the expression
 sp->count
refers to the count field of the structure to which sp points;
 s.left
refers to the left subtree pointer of the structure s, and
 s.right->tword[0]
refers to the first character of the tword member of the right subtree of s.
In general, a member of a union may not be inspected unless the value of the union has been
assigned using the same member. However, one special guarantee simplifies the use of unions:
if a union contains several structures that share a common initial sequence, and the union
currently contains one of these structures, it is permitted to refer to the common initial part of
any of the contained structures. For example, the following is a legal fragment:
 union {
 struct {
 int type;
 } n;
 struct {
 int type;
 int intnode;
 } ni;
 struct {
 int type;
 float floatnode;
 } nf;
 } u;
 ...
 u.nf.type = FLOAT;
 u.nf.floatnode = 3.14;
 ...
 if (u.n.type == FLOAT)
175
 ... sin(u.nf.floatnode) ...
A.8.4 Enumerations
Enumerations are unique types with values ranging over a set of named constants called
enumerators. The form of an enumeration specifier borrows from that of structures and unions.
enum-specifier:
enum identifieropt { enumerator-list }
enum identifier
enumerator-list:
enumerator
enumerator-list , enumerator
enumerator:
identifier
identifier = constant-expression
The identifiers in an enumerator list are declared as constants of type int, and may appear
wherever constants are required. If no enumerations with = appear, then the values of the
corresponding constants begin at 0 and increase by 1 as the declaration is read from left to
right. An enumerator with = gives the associated identifier the value specified; subsequent
identifiers continue the progression from the assigned value.
Enumerator names in the same scope must all be distinct from each other and from ordinary
variable names, but the values need not be distinct.
The role of the identifier in the enum-specifier is analogous to that of the structure tag in a
struct-specifier; it names a particular enumeration. The rules for enum-specifiers with and
without tags and lists are the same as those for structure or union specifiers, except that
incomplete enumeration types do not exist; the tag of an enum-specifier without an enumerator
list must refer to an in-scope specifier with a list.
Enumerations are new since the first edition of this book, but have been part of the language for some
years.
A.8.5 Declarators
Declarators have the syntax:
declarator:
pointeropt direct-declarator
direct-declarator:
identifier
(declarator)
direct-declarator [ constant-expressionopt ]
direct-declarator ( parameter-type-list )
direct-declarator ( identifier-listopt )
pointer:
* type-qualifier-listopt
* type-qualifier-listopt pointer
type-qualifier-list:
type-qualifier
type-qualifier-list type-qualifier
176
The structure of declarators resembles that of indirection, function, and array expressions; the
grouping is the same.
A.8.6 Meaning of Declarators
A list of declarators appears after a sequence of type and storage class specifiers. Each
declarator declares a unique main identifier, the one that appears as the first alternative of the
production for direct-declarator. The storage class specifiers apply directly to this identifier,
but its type depends on the form of its declarator. A declarator is read as an assertion that
when its identifier appears in an expression of the same form as the declarator, it yields an
object of the specified type.
Considering only the type parts of the declaration specifiers (Par. A.8.2) and a particular
declarator, a declaration has the form ``T D,'' where T is a type and D is a declarator. The type
attributed to the identifier in the various forms of declarator is described inductively using this
notation.
In a declaration T D where D is an unadored identifier, the type of the identifier is T.
In a declaration T D where D has the form
 ( D1 )
then the type of the identifier in D1 is the same as that of D. The parentheses do not alter the
type, but may change the binding of complex declarators.
A.8.6.1 Pointer Declarators
In a declaration T D where D has the form
* type-qualifier-listopt D1
and the type of the identifier in the declaration T D1 is ``type-modifier T,'' the type of the
identifier of D is ``type-modifier type-qualifier-list pointer to T.'' Qualifiers following * apply to
pointer itself, rather than to the object to which the pointer points.
For example, consider the declaration
 int *ap[];
Here, ap[] plays the role of D1; a declaration ``int ap[]'' (below) would give ap the type
``array of int,'' the type-qualifier list is empty, and the type-modifier is ``array of.'' Hence the
actual declaration gives ap the type ``array to pointers to int.''
As other examples, the declarations
 int i, *pi, *const cpi = &i;
 const int ci = 3, *pci;
declare an integer i and a pointer to an integer pi. The value of the constant pointer cpi may
not be changed; it will always point to the same location, although the value to which it refers
may be altered. The integer ci is constant, and may not be changed (though it may be
initialized, as here.) The type of pci is ``pointer to const int,'' and pci itself may be changed
to point to another place, but the value to which it points may not be altered by assigning
through pci.
A.8.6.2 Array Declarators
In a declaration T D where D has the form
D1 [constant-expressionopt]
177
and the type of the identifier in the declaration T D1 is ``type-modifier T,'' the type of the
identifier of D is ``type-modifier array of T.'' If the constant-expression is present, it must have
integral type, and value greater than 0. If the constant expression specifying the bound is
missing, the array has an incomplete type.
An array may be constructed from an arithmetic type, from a pointer, from a structure or
union, or from another array (to generate a multi-dimensional array). Any type from which an
array is constructed must be complete; it must not be an array of structure of incomplete type.
This implies that for a multi-dimensional array, only the first dimension may be missing. The
type of an object of incomplete aray type is completed by another, complete, declaration for
the object (Par.A.10.2), or by initializing it (Par.A.8.7). For example,
 float fa[17], *afp[17];
declares an array of float numbers and an array of pointers to float numbers. Also,
 static int x3d[3][5][7];
declares a static three-dimensional array of integers, with rank 3 X 5 X 7. In complete detail,
x3d is an array of three items: each item is an array of five arrays; each of the latter arrays is an
array of seven integers. Any of the expressions x3d, x3d[i], x3d[i][j], x3d[i][j][k] may
reasonably appear in an expression. The first three have type ``array,'', the last has type int.
More specifically, x3d[i][j] is an array of 7 integers, and x3d[i] is an array of 5 arrays of 7
integers.
The array subscripting operation is defined so that E1[E2] is identical to *(E1+E2). Therefore,
despite its asymmetric appearance, subscripting is a commutative operation. Because of the
conversion rules that apply to + and to arrays (Pars.A6.6, A.7.1, A.7.7), if E1 is an array and
E2 an integer, then E1[E2] refers to the E2-th member of E1.
In the example, x3d[i][j][k] is equivalent to *(x3d[i][j] + k). The first subexpression
x3d[i][j] is converted by Par.A.7.1 to type ``pointer to array of integers,'' by Par.A.7.7, the
addition involves multiplication by the size of an integer. It follows from the rules that arrays
are stored by rows (last subscript varies fastest) and that the first subscript in the declaration
helps determine the amount of storage consumed by an array, but plays no other part in
subscript calculations.
A.8.6.3 Function Declarators
In a new-style function declaration T D where D has the form
D1 (parameter-type-list)
and the type of the identifier in the declaration T D1 is ``type-modifier T,'' the type of the
identifier of D is ``type-modifier function with arguments parameter-type-list returning T.''
The syntax of the parameters is
parameter-type-list:
parameter-list
parameter-list , ...
parameter-list:
parameter-declaration
parameter-list , parameter-declaration
parameter-declaration:
declaration-specifiers declarator
declaration-specifiers abstract-declaratoropt
178
In the new-style declaration, the parameter list specifies the types of the parameters. As a
special case, the declarator for a new-style function with no parameters has a parameter list
consisting soley of the keyword void. If the parameter list ends with an ellipsis ``, ...'', then
the function may accept more arguments than the number of parameters explicitly described,
see Par.A.7.3.2.
The types of parameters that are arrays or functions are altered to pointers, in accordance with
the rules for parameter conversions; see Par.A.10.1. The only storage class specifier permitted
in a parameter's declaration is register, and this specifier is ignored unless the function
declarator heads a function definition. Similarly, if the declarators in the parameter declarations
contain identifiers and the function declarator does not head a function definition, the
identifiers go out of scope immediately. Abstract declarators, which do not mention the
identifiers, are discussed in Par.A.8.8.
In an old-style function declaration T D where D has the form
D1(identifier-listopt)
and the type of the identifier in the declaration T D1 is ``type-modifier T,'' the type of the
identifier of D is ``type-modifier function of unspecified arguments returning T.'' The
parameters (if present) have the form
identifier-list:
identifier
identifier-list , identifier
In the old-style declarator, the identifier list must be absent unless the declarator is used in the
head of a function definition (Par.A.10.1). No information about the types of the parameters is
supplied by the declaration.
For example, the declaration
 int f(), *fpi(), (*pfi)();
declares a function f returning an integer, a function fpi returning a pointer to an integer, and
a pointer pfi to a function returning an integer. In none of these are the parameter types
specified; they are old-style.
In the new-style declaration
 int strcpy(char *dest, const char *source), rand(void);
strcpy is a function returning int, with two arguments, the first a character pointer, and the
second a pointer to constant characters. The parameter names are effectively comments. The
second function rand takes no arguments and returns int.
Function declarators with parameter prototypes are, by far, the most important language change
introduced by the ANSI standard. They offer an advantage over the ``old-style'' declarators of the first
edition by providing error-detection and coercion of arguments across function calls, but at a cost:
turmoil and confusion during their introduction, and the necessity of accomodating both forms. Some
syntactic ugliness was required for the sake of compatibility, namely void as an explicit marker of
new-style functions without parameters.
The ellipsis notation ``, ...'' for variadic functions is also new, and, together with the macros in the
standard header <stdarg.h>, formalizes a mechanism that was officially forbidden but unofficially
condoned in the first edition.
These notations were adapted from the C++ language.
A.8.7 Initialization
179
When an object is declared, its init-declarator may specify an initial value for the identifier
being declared. The initializer is preceded by =, and is either an expression, or a list of
initializers nested in braces. A list may end with a comma, a nicety for neat formatting.
initializer:
assignment-expression
{ initializer-list }
{ initializer-list , }
initializer-list:
initializer
initializer-list , initializer
All the expressions in the initializer for a static object or array must be constant expressions as
described in Par.A.7.19. The expressions in the initializer for an auto or register object or
array must likewise be constant expressions if the initializer is a brace-enclosed list. However,
if the initializer for an automatic object is a single expression, it need not be a constant
expression, but must merely have appropriate type for assignment to the object.
The first edition did not countenance initialization of automatic structures, unions, or arrays. The
ANSI standard allows it, but only by constant constructions unless the initializer can be expressed by a
simple expression.
A static object not explicitly initialized is initialized as if it (or its members) were assigned the
constant 0. The initial value of an automatic object not explicitly intialized is undefined.
The initializer for a pointer or an object of arithmetic type is a single expression, perhaps in
braces. The expression is assigned to the object.
The initializer for a structure is either an expression of the same type, or a brace-enclosed list
of initializers for its members in order. Unnamed bit-field members are ignored, and are not
initialized. If there are fewer initializers in the list than members of the structure, the trailing
members are initialized with 0. There may not be more initializers than members. Unnamed bitfield members are ignored,and are not initialized.
The initializer for an array is a brace-enclosed list of initializers for its members. If the array has
unknown size, the number of initializers determines the size of the array, and its type becomes
complete. If the array has fixed size, the number of initializers may not exceed the number of
members of the array; if there are fewer, the trailing members are initialized with 0.
As a special case, a character array may be initialized by a string literal; successive characters
of the string initialize successive members of the array. Similarly, a wide character literal
(Par.A.2.6) may initialize an array of type wchar_t. If the array has unknown size, the number
of characters in the string, including the terminating null character, determines its size; if its
size is fixed, the number of characters in the string, not counting the terminating null character,
must not exceed the size of the array.
The initializer for a union is either a single expression of the same type, or a brace-enclosed
initializer for the first member of the union.
The first edition did not allow initialization of unions. The ``first-member'' rule is clumsy, but is hard
to generalize without new syntax. Besides allowing unions to be explicitly initialized in at least a
primitive way, this ANSI rule makes definite the semantics of static unions not explicitly initialized.
An aggregate is a structure or array. If an aggregate contains members of aggregate type, the
initialization rules apply recursively. Braces may be elided in the initialization as follows: if the
initializer for an aggregate's member that itself is an aggregate begins with a left brace, then the
succeding comma-separated list of initializers initializes the members of the subaggregate; it is
erroneous for there to be more initializers than members. If, however, the initializer for a
180
subaggregate does not begin with a left brace, then only enough elements from the list are
taken into account for the members of the subaggregate; any remaining members are left to
initialize the next member of the aggregate of which the subaggregate is a part.
For example,
 int x[] = { 1, 3, 5 };
declares and initializes x as a 1-dimensional array with three members, since no size was
specified and there are three initializers.
 float y[4][3] = {
 { 1, 3, 5 },
 { 2, 4, 6 },
 { 3, 5, 7 },
 };
is a completely-bracketed initialization: 1, 3 and 5 initialize the first row of the array y[0],
namely y[0][0], y[0][1], and y[0][2]. Likewise the next two lines initialize y[1] and y[2].
The initializer ends early, and therefore the elements of y[3] are initialized with 0. Precisely
the same effect could have been achieved by
 float y[4][3] = {
 1, 3, 5, 2, 4, 6, 3, 5, 7
 };
The initializer for y begins with a left brace, but that for y[0] does not; therefore three
elements from the list are used. Likewise the next three are taken successively for y[1] and for
y[2]. Also,
 float y[4][3] = {
 { 1 }, { 2 }, { 3 }, { 4 }
 };
initializes the first column of y (regarded as a two-dimensional array) and leaves the rest 0.
Finally,
 char msg[] = "Syntax error on line %s\n";
shows a character array whose members are initialized with a string; its size includes the
terminating null character.
A.8.8 Type names
In several contexts (to specify type conversions explicitly with a cast, to declare parameter
types in function declarators, and as argument of sizeof) it is necessary to supply the name of
a data type. This is accomplished using a type name, which is syntactically a declaration for an
object of that type omitting the name of the object.
type-name:
specifier-qualifier-list abstract-declaratoropt
abstract-declarator:
pointer
pointeropt direct-abstract-declarator
direct-abstract-declarator:
( abstract-declarator )
direct-abstract-declaratoropt [constant-expressionopt]
direct-abstract-declaratoropt (parameter-type-listopt) 
181
It is possible to identify uniquely the location in the abstract-declarator where the identifier
would appear if the construction were a declarator in a declaration. The named type is then the
same as the type of the hypothetical identifier. For example,
 int
 int *
 int *[3]
 int (*)[]
 int *()
 int (*[])(void)
name respectively the types ``integer,'' ``pointer to integer,'' ``array of 3 pointers to integers,''
``pointer to an unspecified number of integers,'' ``function of unspecified parameters returning
pointer to integer,'' and ``array, of unspecified size, of pointers to functions with no parameters
each returning an integer.''
A.8.9 Typedef
Declarations whose storage class specifier is typedef do not declare objects; instead they
define identifiers that name types. These identifiers are called typedef names.
typedef-name:
identifier
A typedef declaration attributes a type to each name among its declarators in the usual way
(see Par.A.8.6). Thereafter, each such typedef name is syntactically equivalent to a type
specifier keyword for the associated type.
For example, after
 typedef long Blockno, *Blockptr;
 typedef struct { double r, theta; } Complex;
the constructions
 Blockno b;
 extern Blockptr bp;
 Complex z, *zp;
are legal declarations. The type of b is long, that of bp is ``pointer to long,'' and that of z is
the specified structure; zp is a pointer to such a structure.
typedef does not introduce new types, only synonyms for types that could be specified in
another way. In the example, b has the same type as any long object.
Typedef names may be redeclared in an inner scope, but a non-empty set of type specifiers
must be given. For example,
 extern Blockno;
does not redeclare Blockno, but
 extern int Blockno;
does.
A.8.10 Type Equivalence
Two type specifier lists are equivalent if they contain the same set of type specifiers, taking
into account that some specifiers can be implied by others (for example, long alone implies
long int). Structures, unions, and enumerations with different tags are distinct, and a tagless
union, structure, or enumeration specifies a unique type. 
182
Two types are the same if their abstract declarators (Par.A.8.8), after expanding any typedef
types, and deleting any function parameter specifiers, are the same up to the equivalence of
type specifier lists. Array sizes and function parameter types are significant.
A.9 Statements
Except as described, statements are executed in sequence. Statements are executed for their
effect, and do not have values. They fall into several groups.
statement:
labeled-statement
expression-statement
compound-statement
selection-statement
iteration-statement
jump-statement
A.9.1 Labeled Statements
Statements may carry label prefixes.
labeled-statement:
identifier : statement
case constant-expression : statement
default : statement
A label consisting of an identifier declares the identifier. The only use of an identifier label is as
a target of goto. The scope of the identifier is the current function. Because labels have their
own name space, they do not interfere with other identifiers and cannot be redeclared. See
Par.A.11.1.
Case labels and default labels are used with the switch statement (Par.A.9.4). The constant
expression of case must have integral type.
Labels themselves do not alter the flow of control.
A.9.2 Expression Statement
Most statements are expression statements, which have the form
expression-statement:
expressionopt;
Most expression statements are assignments or function calls. All side effects from the
expression are completed before the next statement is executed. If the expression is missing,
the construction is called a null statement; it is often used to supply an empty body to an
iteration statement to place a label.
A.9.3 Compound Statement
So that several statements can be used where one is expected, the compound statement (also
called ``block'') is provided. The body of a function definition is a compound statement.
compound-statement:
{ declaration-listopt statement-listopt }
declaration-list:
declaration
declaration-list declaration
183
statement-list:
statement
statement-list statement
If an identifier in the declaration-list was in scope outside the block, the outer declaration is
suspended within the block (see Par.A.11.1), after which it resumes its force. An identifier may
be declared only once in the same block. These rules apply to identifiers in the same name
space (Par.A.11); identifiers in different name spaces are treated as distinct.
Initialization of automatic objects is performed each time the block is entered at the top, and
proceeds in the order of the declarators. If a jump into the block is executed, these
initializations are not performed. Initialization of static objects are performed only once,
before the program begins execution.
A.9.4 Selection Statements
Selection statements choose one of several flows of control.
selection-statement:
if (expression) statement
if (expression) statement else statement
switch (expression) statement
In both forms of the if statement, the expression, which must have arithmetic or pointer type,
is evaluated, including all side effects, and if it compares unequal to 0, the first substatement is
executed. In the second form, the second substatement is executed if the expression is 0. The
else ambiguity is resolved by connecting an else with the last encountered else-less if at
the same block nesting level.
The switch statement causes control to be transferred to one of several statements depending
on the value of an expression, which must have integral type. The substatement controlled by a
switch is typically compound. Any statement within the substatement may be labeled with one
or more case labels (Par.A.9.1). The controlling expression undergoes integral promotion
(Par.A.6.1), and the case constants are converted to the promoted type. No two of these case
constants associated with the same switch may have the same value after conversion. There
may also be at most one default label associated with a switch. Switches may be nested; a
case or default label is associated with the smallest switch that contains it.
When the switch statement is executed, its expression is evaluated, including all side effects,
and compared with each case constant. If one of the case constants is equal to the value of the
expression, control passes to the statement of the matched case label. If no case constant
matches the expression, and if there is a default label, control passes to the labeled statement.
If no case matches, and if there is no default, then none of the substatements of the swtich is
executed.
In the first edition of this book, the controlling expression of switch, and the case constants, were
required to have int type.
A.9.5 Iteration Statements
Iteration statements specify looping.
iteration-statement:
while (expression) statement
do statement while (expression);
for (expressionopt; expressionopt; expressionopt) statement
184
In the while and do statements, the substatement is executed repeatedly so long as the value
of the expression remains unequal to 0; the expression must have arithmetic or pointer type.
With while, the test, including all side effects from the expression, occurs before each
execution of the statement; with do, the test follows each iteration.
In the for statement, the first expression is evaluated once, and thus specifies initialization for
the loop. There is no restriction on its type. The second expression must have arithmetic or
pointer type; it is evaluated before each iteration, and if it becomes equal to 0, the for is
terminated. The third expression is evaluated after each iteration, and thus specifies a reinitialization for the loop. There is no restriction on its type. Side-effects from each expression
are completed immediately after its evaluation. If the substatement does not contain continue,
a statement
for (expression1; expression2; expression3) statement
is equivalent to
expression1;
while (expression2) {
 statement
 expression3;
}
Any of the three expressions may be dropped. A missing second expression makes the implied
test equivalent to testing a non-zero element.
A.9.6 Jump statements
Jump statements transfer control unconditionally.
jump-statement:
goto identifier;
continue;
break;
return expressionopt;
In the goto statement, the identifier must be a label (Par.A.9.1) located in the current function.
Control transfers to the labeled statement.
A continue statement may appear only within an iteration statement. It causes control to pass
to the loop-continuation portion of the smallest enclosing such statement. More precisely,
within each of the statements
 while (...) { do { for (...) {
 ... ... ...
 contin: ; contin: ; contin: ;
 } } while (...); }
a continue not contained in a smaller iteration statement is the same as goto contin.
A break statement may appear only in an iteration statement or a switch statement, and
terminates execution of the smallest enclosing such statement; control passes to the statement
following the terminated statement.
A function returns to its caller by the return statement. When return is followed by an
expression, the value is returned to the caller of the function. The expression is converted, as
by assignment, to the type returned by the function in which it appears.
Flowing off the end of a function is equivalent to a return with no expression. In either case,
the returned value is undefined. 
185
A.10 External Declarations
The unit of input provided to the C compiler is called a translation unit; it consists of a
sequence of external declarations, which are either declarations or function definitions.
translation-unit:
external-declaration
translation-unit external-declaration
external-declaration:
function-definition
declaration
The scope of external declarations persists to the end of the translation unit in which they are
declared, just as the effect of declarations within the blocks persists to the end of the block.
The syntax of external declarations is the same as that of all declarations, except that only at
this level may the code for functions be given.
A.10.1 Function Definitions
Function definitions have the form
function-definition:
declaration-specifiersopt declarator declaration-listopt compound-statement
The only storage-class specifiers allowed among the declaration specifiers are extern or
static; see Par.A.11.2 for the distinction between them.
A function may return an arithmetic type, a structure, a union, a pointer, or void, but not a
function or an array. The declarator in a function declaration must specify explicitly that the
declared identifier has function type; that is, it must contain one of the forms (see Par.A.8.6.3).
direct-declarator ( parameter-type-list )
direct-declarator ( identifier-listopt )
where the direct-declarator is an identifier or a parenthesized identifier. In particular, it must
not achieve function type by means of a typedef.
In the first form, the definition is a new-style function, and its parameters, together with their
types, are declared in its parameter type list; the declaration-list following the function's
declarator must be absent. Unless the parameter type list consists solely of void, showing that
the function takes no parameters, each declarator in the parameter type list must contain an
identifier. If the parameter type list ends with ``, ...'' then the function may be called with
more arguments than parameters; the va_arg macro mechanism defined in the standard header
<stdarg.h> and described in Appendix B must be used to refer to the extra arguments.
Variadic functions must have at least one named parameter.
In the second form, the definition is old-style: the identifier list names the parameters, while the
declaration list attributes types to them. If no declaration is given for a parameter, its type is
taken to be int. The declaration list must declare only parameters named in the list,
initialization is not permitted, and the only storage-class specifier possible is register.
In both styles of function definition, the parameters are understood to be declared just after the
beginning of the compound statement constituting the function's body, and thus the same
identifiers must not be redeclared there (although they may, like other identifiers, be redeclared
in inner blocks). If a parameter is declared to have type ``array of type,'' the declaration is
adjusted to read ``pointer to type;'' similarly, if a parameter is declared to have type ``function
186
returning type,'' the declaration is adjusted to read ``pointer to function returning type.'' During
the call to a function, the arguments are converted as necessary and assigned to the
parameters; see Par.A.7.3.2.
New-style function definitions are new with the ANSI standard. There is also a small change in the
details of promotion; the first edition specified that the declarations of float parameters were
adjusted to read double. The difference becomes noticable when a pointer to a parameter is
generated within a function.
A complete example of a new-style function definition is
 int max(int a, int b, int c)
 {
 int m;
 m = (a > b) ? a : b;
 return (m > c) ? m : c;
 }
Here int is the declaration specifier; max(int a, int b, int c) is the function's declarator,
and { ... } is the block giving the code for the function. The corresponding old-style
definition would be
 int max(a, b, c)
 int a, b, c;
 {
 /* ... */
 }
where now int max(a, b, c) is the declarator, and int a, b, c; is the declaration list for
the parameters.
A.10.2 External Declarations
External declarations specify the characteristics of objects, functions and other identifiers. The
term ``external'' refers to their location outside functions, and is not directly connected with the
extern keyword; the storage class for an externally-declared object may be left empty, or it
may be specified as extern or static.
Several external declarations for the same identifier may exist within the same translation unit if
they agree in type and linkage, and if there is at most one definition for the identifier.
Two declarations for an object or function are deemed to agree in type under the rule
discussed in Par.A.8.10. In addition, if the declarations differ because one type is an incomplete
structure, union, or enumeration type (Par.A.8.3) and the other is the corresponding completed
type with the same tag, the types are taken to agree. Moreover, if one type is an incomplete
array type (Par.A.8.6.2) and the other is a completed array type, the types, if otherwise
identical, are also taken to agree. Finally, if one type specifies an old-style function, and the
other an otherwise identical new-style function, with parameter declarations, the types are
taken to agree.
If the first external declarator for a function or object includes the static specifier, the
identifier has internal linkage; otherwise it has external linkage. Linkage is discussed in
Par.11.2.
An external declaration for an object is a definition if it has an initializer. An external object
declaration that does not have an initializer, and does not contain the extern specifier, is a
tentative definition. If a definition for an object appears in a translation unit, any tentative
definitions are treated merely as redundant declarations. If no definition for the object appears
in the translation unit, all its tentative definitions become a single definition with initializer 0. 
187
Each object must have exactly one definition. For objects with internal linkage, this rule applies
separately to each translation unit, because internally-linked objects are unique to a translation
unit. For objects with external linkage, it applies to the entire program.
Although the one-definition rule is formulated somewhat differently in the first edition of this book, it
is in effect identical to the one stated here. Some implementations relax it by generalizing the notion
of tentative definition. In the alternate formulation, which is usual in UNIX systems and recognized as
a common extension by the Standard, all the tentative definitions for an externally linked object,
throughout all the translation units of the program, are considered together instead of in each
translation unit separately. If a definition occurs somewhere in the program, then the tentative
definitions become merely declarations, but if no definition appears, then all its tentative definitions
become a definition with initializer 0.
A.11 Scope and Linkage
A program need not all be compiled at one time: the source text may be kept in several files
containing translation units, and precompiled routines may be loaded from libraries.
Communication among the functions of a program may be carried out both through calls and
through manipulation of external data.
Therefore, there are two kinds of scope to consider: first, the lexical scope of an identifier
which is the region of the program text within which the identifier's characteristics are
understood; and second, the scope associated with objects and functions with external linkage,
which determines the connections between identifiers in separately compiled translation units.
A.11.1 Lexical Scope
Identifiers fall into several name spaces that do not interfere with one another; the same
identifier may be used for different purposes, even in the same scope, if the uses are in different
name spaces. These classes are: objects, functions, typedef names, and enum constants; labels;
tags of structures or unions, and enumerations; and members of each structure or union
individually.
These rules differ in several ways from those described in the first edition of this manual. Labels did
not previously have their own name space; tags of structures and unions each had a separate space,
and in some implementations enumerations tags did as well; putting different kinds of tags into the
same space is a new restriction. The most important departure from the first edition is that each
structure or union creates a separate name space for its members, so that the same name may appear in
several different structures. This rule has been common practice for several years.
The lexical scope of an object or function identifier in an external declaration begins at the end
of its declarator and persists to the end of the translation unit in which it appears. The scope of
a parameter of a function definition begins at the start of the block defining the function, and
persists through the function; the scope of a parameter in a function declaration ends at the end
of the declarator. The scope of an identifier declared at the head of a block begins at the end of
its declarator, and persists to the end of the block. The scope of a label is the whole of the
function in which it appears. The scope of a structure, union, or enumeration tag, or an
enumeration constant, begins at its appearance in a type specifier, and persists to the end of a
translation unit (for declarations at the external level) or to the end of the block (for
declarations within a function).
If an identifier is explicitly declared at the head of a block, including the block constituting a
function, any declaration of the identifier outside the block is suspended until the end of the
block.
A.11.2 Linkage
Within a translation unit, all declarations of the same object or function identifier with internal
linkage refer to the same thing, and the object or function is unique to that translation unit. All
188
declarations for the same object or function identifier with external linkage refer to the same
thing, and the object or function is shared by the entire program.
As discussed in Par.A.10.2, the first external declaration for an identifier gives the identifier
internal linkage if the static specifier is used, external linkage otherwise. If a declaration for
an identifier within a block does not include the extern specifier, then the identifier has no
linkage and is unique to the function. If it does include extern, and an external declaration for
is active in the scope surrounding the block, then the identifier has the same linkage as the
external declaration, and refers to the same object or function; but if no external declaration is
visible, its linkage is external.
A.12 Preprocessing
A preprocessor performs macro substitution, conditional compilation, and inclusion of named
files. Lines beginning with #, perhaps preceded by white space, communicate with this
preprocessor. The syntax of these lines is independent of the rest of the language; they may
appear anywhere and have effect that lasts (independent of scope) until the end of the
translation unit. Line boundaries are significant; each line is analyzed individually (bus see
Par.A.12.2 for how to adjoin lines). To the preprocessor, a token is any language token, or a
character sequence giving a file name as in the #include directive (Par.A.12.4); in addition,
any character not otherwise defined is taken as a token. However, the effect of white spaces
other than space and horizontal tab is undefined within preprocessor lines.
Preprocessing itself takes place in several logically successive phases that may, in a particular
implementation, be condensed.
1. First, trigraph sequences as described in Par.A.12.1 are replaced by their equivalents.
Should the operating system environment require it, newline characters are introduced
between the lines of the source file.
2. Each occurrence of a backslash character \ followed by a newline is deleted, this
splicing lines (Par.A.12.2).
3. The program is split into tokens separated by white-space characters; comments are
replaced by a single space. Then preprocessing directives are obeyed, and macros
(Pars.A.12.3-A.12.10) are expanded.
4. Escape sequences in character constants and string literals (Pars. A.2.5.2, A.2.6) are
replaced by their equivalents; then adjacent string literals are concatenated.
5. The result is translated, then linked together with other programs and libraries, by
collecting the necessary programs and data, and connecting external functions and
object references to their definitions.
A.12.1 Trigraph Sequences
The character set of C source programs is contained within seven-bit ASCII, but is a superset
of the ISO 646-1983 Invariant Code Set. In order to enable programs to be represented in the
reduced set, all occurrences of the following trigraph sequences are replaced by the
corresponding single character. This replacement occurs before any other processing.
 ??= # ??( [ ??< {
 ??/ \ ??) ] ??> }
 ??' ^ ??! | ??- ~
No other such replacements occur.
Trigraph sequences are new with the ANSI standard.
A.12.2 Line Splicing
189
Lines that end with the backslash character \ are folded by deleting the backslash and the
following newline character. This occurs before division into tokens.
A.12.3 Macro Definition and Expansion
A control line of the form
# define identifier token-sequence
causes the preprocessor to replace subsequent instances of the identifier with the given
sequence of tokens; leading and trailing white space around the token sequence is discarded. A
second #define for the same identifier is erroneous unless the second token sequence is
identical to the first, where all white space separations are taken to be equivalent.
A line of the form
# define identifier (identifier-list) token-sequence
where there is no space between the first identifier and the (, is a macro definition with
parameters given by the identifier list. As with the first form, leading and trailing white space
arround the token sequence is discarded, and the macro may be redefined only with a definition
in which the number and spelling of parameters, and the token sequence, is identical.
A control line of the form
# undef identifier
causes the identifier's preprocessor definition to be forgotten. It is not erroneous to apply
#undef to an unknown identifier.
When a macro has been defined in the second form, subsequent textual instances of the macro
identifier followed by optional white space, and then by (, a sequence of tokens separated by
commas, and a ) constitute a call of the macro. The arguments of the call are the commaseparated token sequences; commas that are quoted or protected by nested parentheses do not
separate arguments. During collection, arguments are not macro-expanded. The number of
arguments in the call must match the number of parameters in the definition. After the
arguments are isolated, leading and trailing white space is removed from them. Then the token
sequence resulting from each argument is substituted for each unquoted occurrence of the
corresponding parameter's identifier in the replacement token sequence of the macro. Unless
the parameter in the replacement sequence is preceded by #, or preceded or followed by ##,
the argument tokens are examined for macro calls, and expanded as necessary, just before
insertion.
Two special operators influence the replacement process. First, if an occurrence of a parameter
in the replacement token sequence is immediately preceded by #, string quotes (") are placed
around the corresponding parameter, and then both the # and the parameter identifier are
replaced by the quoted argument. A \ character is inserted before each " or \ character that
appears surrounding, or inside, a string literal or character constant in the argument.
Second, if the definition token sequence for either kind of macro contains a ## operator, then
just after replacement of the parameters, each ## is deleted, together with any white space on
either side, so as to concatenate the adjacent tokens and form a new token. The effect is
undefined if invalid tokens are produced, or if the result depends on the order of processing of
the ## operators. Also, ## may not appear at the beginning or end of a replacement token
sequence. 
190
In both kinds of macro, the replacement token sequence is repeatedly rescanned for more
defined identifiers. However, once a given identifier has been replaced in a given expansion, it
is not replaced if it turns up again during rescanning; instead it is left unchanged.
Even if the final value of a macro expansion begins with with #, it is not taken to be a
preprocessing directive.
The details of the macro-expansion process are described more precisely in the ANSI standard than in
the first edition. The most important change is the addition of the # and ## operators, which make
quotation and concatenation admissible. Some of the new rules, especially those involving
concatenation, are bizarre. (See example below.)
For example, this facility may be used for ``manifest-constants,'' as in
 #define TABSIZE 100
 int table[TABSIZE];
The definition
 #define ABSDIFF(a, b) ((a)>(b) ? (a)-(b) : (b)-(a))
defines a macro to return the absolute value of the difference between its arguments. Unlike a
function to do the same thing, the arguments and returned value may have any arithmetic type
or even be pointers. Also, the arguments, which might have side effects, are evaluated twice,
once for the test and once to produce the value.
Given the definition
 #define tempfile(dir) #dir "%s"
the macro call tempfile(/usr/tmp) yields
 "/usr/tmp" "%s"
which will subsequently be catenated into a single string. After
 #define cat(x, y) x ## y
the call cat(var, 123) yields var123. However, the call cat(cat(1,2),3) is undefined: the
presence of ## prevents the arguments of the outer call from being expanded. Thus it produces
the token string
 cat ( 1 , 2 )3
and )3 (the catenation of the last token of the first argument with the first token of the second)
is not a legal token. If a second level of macro definition is introduced,
 #define xcat(x, y) cat(x,y)
things work more smoothly; xcat(xcat(1, 2), 3) does produce 123, because the expansion
of xcat itself does not involve the ## operator.
Likewise, ABSDIFF(ABSDIFF(a,b),c) produces the expected, fully-expanded result.
A.12.4 File Inclusion
A control line of the form
# include <filename>
causes the replacement of that line by the entire contents of the file filename. The characters in
the name filename must not include > or newline, and the effect is undefined if it contains any
of ", ', \, or /*. The named file is searched for in a sequence of implementation-defined
places.
Similarly, a control line of the form
# include "filename"
191
searches first in association with the original source file (a deliberately implementationdependent phrase), and if that search fails, then as in the first form. The effect of using ', \, or
/* in the filename remains undefined, but > is permitted.
Finally, a directive of the form
# include token-sequence
not matching one of the previous forms is interpreted by expanding the token sequence as for
normal text; one of the two forms with <...> or "..." must result, and is then treated as
previously described.
#include files may be nested.
A.12.5 Conditional Compilation
Parts of a program may be compiled conditionally, according to the following schematic
syntax.
preprocessor-conditional:
if-line text elif-parts else-partopt #endif
if-line:
# if constant-expression
# ifdef identifier
# ifndef identifier
elif-parts:
elif-line text
elif-partsopt
elif-line:
# elif constant-expression
else-part:
else-line text
else-line:
#else
Each of the directives (if-line, elif-line, else-line, and #endif) appears alone on a line. The
constant expressions in #if and subsequent #elif lines are evaluated in order until an
expression with a non-zero value is found; text following a line with a zero value is discarded.
The text following the successful directive line is treated normally. ``Text'' here refers to any
material, including preprocessor lines, that is not part of the conditional structure; it may be
empty. Once a successful #if or #elif line has been found and its text processed, succeeding
#elif and #else lines, together with their text, are discarded. If all the expressions are zero,
and there is an #else, the text following the #else is treated normally. Text controlled by
inactive arms of the conditional is ignored except for checking the nesting of conditionals.
The constant expression in #if and #elif is subject to ordinary macro replacement.
Moreover, any expressions of the form
defined identifier
or
defined (identifier) 
192
are replaced, before scanning for macros, by 1L if the identifier is defined in the preprocessor,
and by 0L if not. Any identifiers remaining after macro expansion are replaced by 0L. Finally,
each integer constant is considered to be suffixed with L, so that all arithmetic is taken to be
long or unsigned long.
The resulting constant expression (Par.A.7.19) is restricted: it must be integral, and may not
contain sizeof, a cast, or an enumeration constant.
The control lines
#ifdef identifier
#ifndef identifier
are equivalent to
# if defined identifier
# if ! defined identifier
respectively.
#elif is new since the first edition, although it has been available is some preprocessors. The
defined preprocessor operator is also new.
A.12.6 Line Control
For the benefit of other preprocessors that generate C programs, a line in one of the forms
# line constant "filename"
# line constant
causes the compiler to believe, for purposes of error diagnostics, that the line number of the
next source line is given by the decimal integer constant and the current input file is named by
the identifier. If the quoted filename is absent, the remembered name does not change. Macros
in the line are expanded before it is interpreted.
A.12.7 Error Generation
A preprocessor line of the form
# error token-sequenceopt
causes the preprocessor to write a diagnostic message that includes the token sequence.
A.12.8 Pragmas
A control line of the form
# pragma token-sequenceopt
causes the preprocessor to perform an implementation-dependent action. An unrecognized
pragma is ignored.
A.12.9 Null directive
A control line of the form
#
has no effect.
A.12.10 Predefined names
193
Several identifiers are predefined, and expand to produce special information. They, and also
the preprocessor expansion operator defined, may not be undefined or redefined.
__LINE__ A decimal constant containing the current source line number.
__FILE__ A string literal containing the name of the file being compiled.
__DATE__ A string literal containing the date of compilation, in the form "Mmmm dd yyyy"
__TIME__ A string literal containing the time of compilation, in the form "hh:mm:ss"
__STDC__ The constant 1. It is intended that this identifier be defined to be 1 only in standardconforming implementations.
#error and #pragma are new with the ANSI standard; the predefined preprocessor macros are
new, but some of them have been available in some implementations.
A.13 Grammar
Below is a recapitulation of the grammar that was given throughout the earlier part of this
appendix. It has exactly the same content, but is in different order.
The grammar has undefined terminal symbols integer-constant, character-constant, floatingconstant, identifier, string, and enumeration-constant; the typewriter style words and
symbols are terminals given literally. This grammar can be transformed mechanically into input
acceptable for an automatic parser-generator. Besides adding whatever syntactic marking is
used to indicate alternatives in productions, it is necessary to expand the ``one of''
constructions, and (depending on the rules of the parser-generator) to duplicate each
production with an opt symbol, once with the symbol and once without. With one further
change, namely deleting the production typedef-name: identifier and making typedef-name a
terminal symbol, this grammar is acceptable to the YACC parser-generator. It has only one
conflict, generated by the if-else ambiguity.
translation-unit:
external-declaration
translation-unit external-declaration
external-declaration:
function-definition
declaration
function-definition:
declaration-specifiersopt declarator declaration-listopt compound-statement
declaration:
declaration-specifiers init-declarator-listopt;
declaration-list:
declaration
declaration-list declaration
declaration-specifiers:
storage-class-specifier declaration-specifiersopt
type-specifier declaration-specifiersopt
type-qualifier declaration-specifiersopt
storage-class specifier: one of
auto register static extern typedef
type specifier: one of
void char short int long float double signed
unsigned struct-or-union-specifier enum-specifier typedef-name
194
type-qualifier: one of
const volatile
struct-or-union-specifier:
struct-or-union identifieropt { struct-declaration-list }
struct-or-union identifier
struct-or-union: one of
struct union
struct-declaration-list:
struct declaration
struct-declaration-list struct declaration
init-declarator-list:
init-declarator
init-declarator-list, init-declarator
init-declarator:
declarator
declarator = initializer
struct-declaration:
specifier-qualifier-list struct-declarator-list;
specifier-qualifier-list:
type-specifier specifier-qualifier-listopt
type-qualifier specifier-qualifier-listopt
struct-declarator-list:
struct-declarator
struct-declarator-list , struct-declarator
struct-declarator:
declarator
declaratoropt : constant-expression
enum-specifier:
enum identifieropt { enumerator-list }
enum identifier
enumerator-list:
enumerator
enumerator-list , enumerator
enumerator:
identifier
identifier = constant-expression
declarator:
pointeropt direct-declarator
direct-declarator:
identifier
(declarator)
direct-declarator [ constant-expressionopt ]
195
direct-declarator ( parameter-type-list )
direct-declarator ( identifier-listopt )
pointer:
* type-qualifier-listopt
* type-qualifier-listopt pointer
type-qualifier-list:
type-qualifier
type-qualifier-list type-qualifier
parameter-type-list:
parameter-list
parameter-list , ...
parameter-list:
parameter-declaration
parameter-list , parameter-declaration
parameter-declaration:
declaration-specifiers declarator
declaration-specifiers abstract-declaratoropt
identifier-list:
identifier
identifier-list , identifier
initializer:
assignment-expression
{ initializer-list }
{ initializer-list , }
initializer-list:
initializer
initializer-list , initializer
type-name:
specifier-qualifier-list abstract-declaratoropt
abstract-declarator:
pointer
pointeropt direct-abstract-declarator
direct-abstract-declarator:
( abstract-declarator )
direct-abstract-declaratoropt [constant-expressionopt]
direct-abstract-declaratoropt (parameter-type-listopt)
typedef-name:
identifier
statement:
labeled-statement
expression-statement
compound-statement
selection-statement
196
iteration-statement
jump-statement
labeled-statement:
identifier : statement
case constant-expression : statement
default : statement
expression-statement:
expressionopt;
compound-statement:
{ declaration-listopt statement-listopt }
statement-list:
statement
statement-list statement
selection-statement:
if (expression) statement
if (expression) statement else statement
switch (expression) statement
iteration-statement:
while (expression) statement
do statement while (expression);
for (expressionopt; expressionopt; expressionopt) statement
jump-statement:
goto identifier;
continue;
break;
return expressionopt;
expression:
assignment-expression
expression , assignment-expression
assignment-expression:
conditional-expression
unary-expression assignment-operator assignment-expression
assignment-operator: one of
= *= /= %= += -= <<= >>= &= ^= |=
conditional-expression:
logical-OR-expression
logical-OR-expression ? expression : conditional-expression
constant-expression:
conditional-expression
logical-OR-expression:
logical-AND-expression
logical-OR-expression || logical-AND-expression
197
logical-AND-expression
:
inclusive-OR-expression
logical-AND-expression && inclusive-OR-expression
inclusive-OR-expression
:
exclusive-OR-expression
inclusive-OR-expression | exclusive-OR-expression
exclusive-OR-expression
:
AND-expression
exclusive-OR-expression
^ AND-expression
AND-expression
:
equality-expression
AND-expression
& equality-expression
equality-expression
:
relational-expression
equality-expression == relational-expression
equality-expression != relational-expression
relational-expression
:
shift-expression
relational-expression
< shift-expression
relational-expression
> shift-expression
relational-expression <= shift-expression
relational-expression >= shift-expression
shift-expression
:
additive-expression
shift-expression << additive-expression
shift-expression >> additive-expression
additive-expression
:
multiplicative-expression
additive-expression
+ multiplicative-expression
additive-expression
- multiplicative-expression
multiplicative-expression
:
multiplicative-expression
* cast-expression
multiplicative-expression
/ cast-expression
multiplicative-expression
% cast-expression
cast-expression
:
unary expression (type-name) cast-expression
unary-expression
:
postfix expression
++unary expression
--unary expression
unary-operator cast-expression
sizeof unary-expression
sizeof (type-name) 
198
unary operator: one of
& * + - ~ !
postfix-expression:
primary-expression
postfix-expression[expression]
postfix-expression(argument-expression-listopt)
postfix-expression.identifier
postfix-expression->+identifier
postfix-expression++
postfix-expression--
primary-expression:
identifier
constant
string
(expression)
argument-expression-list:
assignment-expression
assignment-expression-list , assignment-expression
constant:
integer-constant
character-constant
floating-constant
enumeration-constant
The following grammar for the preprocessor summarizes the structure of control lines, but is
not suitable for mechanized parsing. It includes the symbol text, which means ordinary program
text, non-conditional preprocessor control lines, or complete preprocessor conditional
instructions.
control-line:
# define identifier token-sequence
# define identifier(identifier, ... , identifier) token-sequence
# undef identifier
# include <filename>
# include "filename"
# line constant "filename"
# line constant
# error token-sequenceopt
# pragma token-sequenceopt
#
preprocessor-conditional
preprocessor-conditional:
if-line text elif-parts else-partopt #endif
if-line:
# if constant-expression
# ifdef identifier
# ifndef identifier
199
elif-parts
:
elif-line text
elif-partsopt
elif-line
:
# elif constant-expression
else-part
:
else-line text
else-line
:
#else
200
Appendix B - Standard Library
This appendix is a summary of the library defined by the ANSI standard. The standard library
is not part of the C language proper, but an environment that supports standard C will provide
the function declarations and type and macro definitions of this library. We have omitted a few
functions that are of limited utility or easily synthesized from others; we have omitted multibyte characters; and we have omitted discussion of locale issues; that is, properties that depend
on local language, nationality, or culture.
The functions, types and macros of the standard library are declared in standard headers:
 <assert.h> <float.h> <math.h> <stdarg.h> <stdlib.h>
 <ctype.h> <limits.h> <setjmp.h> <stddef.h> <string.h>
 <errno.h> <locale.h> <signal.h> <stdio.h> <time.h>
A header can be accessed by
#include <header>
Headers may be included in any order and any number of times. A header must be included
outside of any external declaration or definition and before any use of anything it declares. A
header need not be a source file.
External identifiers that begin with an underscore are reserved for use by the library, as are all
other identifiers that begin with an underscore and an upper-case letter or another underscore.
B.1 Input and Output: <stdio.h>
The input and output functions, types, and macros defined in <stdio.h> represent nearly one
third of the library.
A stream is a source or destination of data that may be associated with a disk or other
peripheral. The library supports text streams and binary streams, although on some systems,
notably UNIX, these are identical. A text stream is a sequence of lines; each line has zero or
more characters and is terminated by '\n'. An environment may need to convert a text stream
to or from some other representation (such as mapping '\n' to carriage return and linefeed).
A binary stream is a sequence of unprocessed bytes that record internal data, with the property
that if it is written, then read back on the same system, it will compare equal.
A stream is connected to a file or device by opening it; the connection is broken by closing the
stream. Opening a file returns a pointer to an object of type FILE, which records whatever
information is necessary to control the stream. We will use ``file pointer'' and ``stream''
interchangeably when there is no ambiguity.
When a program begins execution, the three streams stdin, stdout, and stderr are already
open.
B.1.1 File Operations
The following functions deal with operations on files. The type size_t is the unsigned integral
type produced by the sizeof operator.
FILE *fopen(const char *filename, const char *mode)
fopen opens the named file, and returns a stream, or NULL if the attempt fails. Legal
values for mode include:
"r" open text file for reading
"w" create text file for writing; discard previous contents if any
"a" append; open or create text file for writing at end of file
201
"r+" open text file for update (i.e., reading and writing)
"w+" create text file for update, discard previous contents if any
"a+" append; open or create text file for update, writing at end
Update mode permits reading and writing the same file; fflush or a file-positioning
function must be called between a read and a write or vice versa. If the mode includes b
after the initial letter, as in "rb" or "w+b", that indicates a binary file. Filenames are
limited to FILENAME_MAX characters. At most FOPEN_MAX files may be open at once.
FILE *freopen(const char *filename, const char *mode, FILE *stream)
freopen opens the file with the specified mode and associates the stream with it. It
returns stream, or NULL if an error occurs. freopen is normally used to change the
files associated with stdin, stdout, or stderr.
int fflush(FILE *stream)
On an output stream, fflush causes any buffered but unwritten data to be written; on
an input stream, the effect is undefined. It returns EOF for a write error, and zero
otherwise. fflush(NULL) flushes all output streams.
int fclose(FILE *stream)
fclose flushes any unwritten data for stream, discards any unread buffered input,
frees any automatically allocated buffer, then closes the stream. It returns EOF if any
errors occurred, and zero otherwise.
int remove(const char *filename)
remove removes the named file, so that a subsequent attempt to open it will fail. It
returns non-zero if the attempt fails.
int rename(const char *oldname, const char *newname)
rename changes the name of a file; it returns non-zero if the attempt fails.
FILE *tmpfile(void)
tmpfile creates a temporary file of mode "wb+" that will be automatically removed
when closed or when the program terminates normally. tmpfile returns a stream, or
NULL if it could not create the file.
char *tmpnam(char s[L_tmpnam])
tmpnam(NULL) creates a string that is not the name of an existing file, and returns a
pointer to an internal static array. tmpnam(s) stores the string in s as well as returning
it as the function value; s must have room for at least L_tmpnam characters. tmpnam
generates a different name each time it is called; at most TMP_MAX different names are
guaranteed during execution of the program. Note that tmpnam creates a name, not a
file.
int setvbuf(FILE *stream, char *buf, int mode, size_t size)
setvbuf controls buffering for the stream; it must be called before reading, writing or
any other operation. A mode of _IOFBF causes full buffering, _IOLBF line buffering of
text files, and _IONBF no buffering. If buf is not NULL, it will be used as the buffer,
otherwise a buffer will be allocated. size determines the buffer size. setvbuf returns
non-zero for any error.
void setbuf(FILE *stream, char *buf)
If buf is NULL, buffering is turned off for the stream. Otherwise, setbuf is equivalent
to (void) setvbuf(stream, buf, _IOFBF, BUFSIZ).
B.1.2 Formatted Output
The printf functions provide formatted output conversion.
 int fprintf(FILE *stream, const char *format, ...)
fprintf converts and writes output to stream under the control of format. The return value
is the number of characters written, or negative if an error occurred.
The format string contains two types of objects: ordinary characters, which are copied to the
output stream, and conversion specifications, each of which causes conversion and printing of
202
the next successive argument to fprintf. Each conversion specification begins with the
character % and ends with a conversion character. Between the % and the conversion character
there may be, in order:
• Flags (in any order), which modify the specification:
o -, which specifies left adjustment of the converted argument in its field.
o +, which specifies that the number will always be printed with a sign.
o space: if the first character is not a sign, a space will be prefixed.
o 0: for numeric conversions, specifies padding to the field width with leading
zeros.
o #, which specifies an alternate output form. For o, the first digit will become
zero. For x or X, 0x or 0X will be prefixed to a non-zero result. For e, E, f, g,
and G, the output will always have a decimal point; for g and G, trailing zeros
will not be removed.
• A number specifying a minimum field width. The converted argument will be printed in
a field at least this wide, and wider if necessary. If the converted argument has fewer
characters than the field width it will be padded on the left (or right, if left adjustment
has been requested) to make up the field width. The padding character is normally
space, but is 0 if the zero padding flag is present.
• A period, which separates the field width from the precision.
• A number, the precision, that specifies the maximum number of characters to be printed
from a string, or the number of digits to be printed after the decimal point for e, E, or f
conversions, or the number of significant digits for g or G conversion, or the number of
digits to be printed for an integer (leading 0s will be added to make up the necessary
width).
• A length modifier h, l (letter ell), or L. ``h'' indicates that the corresponding argument
is to be printed as a short or unsigned short; ``l'' indicates that the argument is a
long or unsigned long, ``L'' indicates that the argument is a long double.
Width or precision or both may be specified as *, in which case the value is computed by
converting the next argument(s), which must be int.
The conversion characters and their meanings are shown in Table B.1. If the character after the
% is not a conversion character, the behavior is undefined.
Table B.1 Printf Conversions
Character Argument type; Printed As
d,i int; signed decimal notation.
o int; unsigned octal notation (without a leading zero).
x,X unsigned int; unsigned hexadecimal notation (without a leading 0x or 0X),
using abcdef for 0x or ABCDEF for 0X.
u int; unsigned decimal notation.
c int; single character, after conversion to unsigned char
s
char *; characters from the string are printed until a '\0' is reached or until the
number of characters indicated by the precision have been printed.
203
f
double; decimal notation of the form [-]mmm.ddd, where the number of d's is
given by the precision. The default precision is 6; a precision of 0 suppresses the
decimal point.
e,E
double; decimal notation of the form [-]m.dddddde+/-xx or [-]m.ddddddE+/-
xx, where the number of d's is specified by the precision. The default precision is
6; a precision of 0 suppresses the decimal point.
g,G
double; %e or %E is used if the exponent is less than -4 or greater than or equal to
the precision; otherwise %f is used. Trailing zeros and a trailing decimal point are
not printed.
p void *; print as a pointer (implementation-dependent representation).
n
int *; the number of characters written so far by this call to printf is written
into the argument. No argument is converted.
% no argument is converted; print a %
int printf(const char *format, ...)
printf(...) is equivalent to fprintf(stdout, ...).
int sprintf(char *s, const char *format, ...)
sprintf is the same as printf except that the output is written into the string s,
terminated with '\0'. s must be big enough to hold the result. The return count does
not include the '\0'.
int vprintf(const char *format, va_list arg)
int vfprintf(FILE *stream, const char *format, va_list arg)
int vsprintf(char *s, const char *format, va_list arg)
The functions vprintf, vfprintf, and vsprintf are equivalent to the corresponding
printf functions, except that the variable argument list is replaced by arg, which has
been initialized by the va_start macro and perhaps va_arg calls. See the discussion of
<stdarg.h> in Section B.7.
B.1.3 Formatted Input
The scanf function deals with formatted input conversion.
int fscanf(FILE *stream, const char *format, ...)
fscanf reads from stream under control of format, and assigns converted values through
subsequent arguments, each of which must be a pointer. It returns when format is exhausted.
fscanf returns EOF if end of file or an error occurs before any conversion; otherwise it returns
the number of input items converted and assigned.
The format string usually contains conversion specifications, which are used to direct
interpretation of input. The format string may contain:
• Blanks or tabs, which are not ignored.
• Ordinary characters (not %), which are expected to match the next non-white space
character of the input stream.
• Conversion specifications, consisting of a %, an optional assignment suppression
character *, an optional number specifying a maximum field width, an optional h, l, or
L indicating the width of the target, and a conversion character.
A conversion specification determines the conversion of the next input field. Normally the
result is placed in the variable pointed to by the corresponding argument. If assignment
suppression is indicated by *, as in %*s, however, the input field is simply skipped; no
assignment is made. An input field is defined as a string of non-white space characters; it
extends either to the next white space character or until the field width, if specified, is
exhausted. This implies that scanf will read across line boundaries to find its input, since
204
newlines are white space. (White space characters are blank, tab, newline, carriage return,
vertical tab, and formfeed.)
The conversion character indicates the interpretation of the input field. The corresponding
argument must be a pointer. The legal conversion characters are shown in Table B.2.
The conversion characters d, i, n, o, u, and x may be preceded by h if the argument is a
pointer to short rather than int, or by l (letter ell) if the argument is a pointer to long. The
conversion characters e, f, and g may be preceded by l if a pointer to double rather than
float is in the argument list, and by L if a pointer to a long double.
Table B.2 Scanf Conversions
Character Input Data; Argument type
d decimal integer; int*
i
integer; int*. The integer may be in octal (leading 0) or hexadecimal (leading 0x
or 0X).
o octal integer (with or without leading zero); int *.
u unsigned decimal integer; unsigned int *.
x hexadecimal integer (with or without leading 0x or 0X); int*.
c
characters; char*. The next input characters are placed in the indicated array, up
to the number given by the width field; the default is 1. No '\0' is added. The
normal skip over white space characters is suppressed in this case; to read the
next non-white space character, use %1s.
s
string of non-white space characters (not quoted); char *, pointing to an array
of characters large enough to hold the string and a terminating '\0' that will be
added.
e,f,g
floating-point number; float *. The input format for float's is an optional sign,
a string of numbers possibly containing a decimal point, and an optional exponent
field containing an E or e followed by a possibly signed integer.
p pointer value as printed by printf("%p");, void *.
n
writes into the argument the number of characters read so far by this call; int *.
No input is read. The converted item count is not incremented.
[...] matches the longest non-empty string of input characters from the set between
brackets; char *. A '\0' is added. []...] includes ] in the set.
[^...] matches the longest non-empty string of input characters not from the set
between brackets; char *. A '\0' is added. [^]...] includes ] in the set.
% literal %; no assignment is made.
int scanf(const char *format, ...)
scanf(...) is identical to fscanf(stdin, ...).
int sscanf(const char *s, const char *format, ...)
sscanf(s, ...) is equivalent to scanf(...) except that the input characters are
taken from the string s.
B.1.4 Character Input and Output Functions
int fgetc(FILE *stream)
fgetc returns the next character of stream as an unsigned char (converted to an
int), or EOF if end of file or error occurs.
char *fgets(char *s, int n, FILE *stream)
fgets reads at most the next n-1 characters into the array s, stopping if a newline is
encountered; the newline is included in the array, which is terminated by '\0'. fgets
returns s, or NULL if end of file or error occurs. 
205
int fputc(int c, FILE *stream)
fputc writes the character c (converted to an unsigend char) on stream. It returns
the character written, or EOF for error.
int fputs(const char *s, FILE *stream)
fputs writes the string s (which need not contain \n) on stream; it returns nonnegative, or EOF for an error.
int getc(FILE *stream)
getc is equivalent to fgetc except that if it is a macro, it may evaluate stream more
than once.
int getchar(void)
getchar is equivalent to getc(stdin).
char *gets(char *s)
gets reads the next input line into the array s; it replaces the terminating newline with
'\0'. It returns s, or NULL if end of file or error occurs.
int putc(int c, FILE *stream)
putc is equivalent to fputc except that if it is a macro, it may evaluate stream more
than once.
int putchar(int c)
putchar(c) is equivalent to putc(c,stdout).
int puts(const char *s)
puts writes the string s and a newline to stdout. It returns EOF if an error occurs,
non-negative otherwise.
int ungetc(int c, FILE *stream)
ungetc pushes c (converted to an unsigned char) back onto stream, where it will be
returned on the next read. Only one character of pushback per stream is guaranteed.
EOF may not be pushed back. ungetc returns the character pushed back, or EOF for
error.
B.1.5 Direct Input and Output Functions
size_t fread(void *ptr, size_t size, size_t nobj, FILE *stream)
fread reads from stream into the array ptr at most nobj objects of size size. fread
returns the number of objects read; this may be less than the number requested. feof
and ferror must be used to determine status.
size_t fwrite(const void *ptr, size_t size, size_t nobj, FILE *stream)
fwrite writes, from the array ptr, nobj objects of size size on stream. It returns the
number of objects written, which is less than nobj on error.
B.1.6 File Positioning Functions
int fseek(FILE *stream, long offset, int origin)
fseek sets the file position for stream; a subsequent read or write will access data
beginning at the new position. For a binary file, the position is set to offset characters
from origin, which may be SEEK_SET (beginning), SEEK_CUR (current position), or
SEEK_END (end of file). For a text stream, offset must be zero, or a value returned by
ftell (in which case origin must be SEEK_SET). fseek returns non-zero on error.
long ftell(FILE *stream)
ftell returns the current file position for stream, or -1 on error.
void rewind(FILE *stream)
rewind(fp) is equivalent to fseek(fp, 0L, SEEK_SET); clearerr(fp).
int fgetpos(FILE *stream, fpos_t *ptr)
fgetpos records the current position in stream in *ptr, for subsequent use by
fsetpos. The type fpos_t is suitable for recording such values. fgetpos returns nonzero on error.
int fsetpos(FILE *stream, const fpos_t *ptr)
206
fsetpos positions stream at the position recorded by fgetpos in *ptr. fsetpos
returns non-zero on error.
B.1.7 Error Functions
Many of the functions in the library set status indicators when error or end of file occur. These
indicators may be set and tested explicitly. In addition, the integer expression errno (declared
in <errno.h>) may contain an error number that gives further information about the most
recent error.
void clearerr(FILE *stream)
clearerr clears the end of file and error indicators for stream.
int feof(FILE *stream)
feof returns non-zero if the end of file indicator for stream is set.
int ferror(FILE *stream)
ferror returns non-zero if the error indicator for stream is set.
void perror(const char *s)
perror(s) prints s and an implementation-defined error message corresponding to the
integer in errno, as if by
fprintf(stderr, "%s: %s\n", s, "error message");
See strerror in Section B.3.
B.2 Character Class Tests: <ctype.h>
The header <ctype.h> declares functions for testing characters. For each function, the
argument list is an int, whose value must be EOF or representable as an unsigned char, and
the return value is an int. The functions return non-zero (true) if the argument c satisfies the
condition described, and zero if not.
isalnum(c) isalpha(c) or isdigit(c) is true
isalpha(c) isupper(c) or islower(c) is true
iscntrl(c) control character
isdigit(c) decimal digit
isgraph(c) printing character except space
islower(c) lower-case letter
isprint(c) printing character including space
ispunct(c) printing character except space or letter or digit
isspace(c) space, formfeed, newline, carriage return, tab, vertical tab
isupper(c) upper-case letter
isxdigit(c) hexadecimal digit
In the seven-bit ASCII character set, the printing characters are 0x20 (' ') to 0x7E ('-');
the control characters are 0 NUL to 0x1F (US), and 0x7F (DEL).
In addition, there are two functions that convert the case of letters:
int tolower(c) convert c to lower case
int toupper(c) convert c to upper case
If c is an upper-case letter, tolower(c) returns the corresponding lower-case letter,
toupper(c) returns the corresponding upper-case letter; otherwise it returns c.
B.3 String Functions: <string.h>
There are two groups of string functions defined in the header <string.h>. The first have
names beginning with str; the second have names beginning with mem. Except for memmove,
the behavior is undefined if copying takes place between overlapping objects. Comparison
functions treat arguments as unsigned char arrays. 
207
In the following table, variables s and t are of type char *; cs and ct are of type const char
*; n is of type size_t; and c is an int converted to char.
char *strcpy(s,ct) copy string ct to string s, including '\0'; return s.
char
*strncpy(s,ct,n)
copy at most n characters of string ct to s; return s. Pad with '\0''s
if ct has fewer than n characters.
char *strcat(s,ct) concatenate string ct to end of string s; return s.
char
*strncat(s,ct,n)
concatenate at most n characters of string ct to string s, terminate s
with '\0'; return s.
int strcmp(cs,ct) compare string cs to string ct, return <0 if cs<ct, 0 if cs==ct, or >0
if cs>ct.
int
strncmp(cs,ct,n)
compare at most n characters of string cs to string ct; return <0 if
cs<ct, 0 if cs==ct, or >0 if cs>ct.
char *strchr(cs,c) return pointer to first occurrence of c in cs or NULL if not present.
char *strrchr(cs,c) return pointer to last occurrence of c in cs or NULL if not present.
size_t
strspn(cs,ct) return length of prefix of cs consisting of characters in ct.
size_t
strcspn(cs,ct) return length of prefix of cs consisting of characters not in ct.
char
*strpbrk(cs,ct)
return pointer to first occurrence in string cs of any character string
ct, or NULL if not present.
char *strstr(cs,ct) return pointer to first occurrence of string ct in cs, or NULL if not
present.
size_t strlen(cs) return length of cs.
char *strerror(n) return pointer to implementation-defined string corresponding to
error n.
char *strtok(s,ct) strtok searches s for tokens delimited by characters from ct; see
below.
A sequence of calls of strtok(s,ct) splits s into tokens, each delimited by a character from
ct. The first call in a sequence has a non-NULL s, it finds the first token in s consisting of
characters not in ct; it terminates that by overwriting the next character of s with '\0' and
returns a pointer to the token. Each subsequent call, indicated by a NULL value of s, returns the
next such token, searching from just past the end of the previous one. strtok returns NULL
when no further token is found. The string ct may be different on each call.
The mem... functions are meant for manipulating objects as character arrays; the intent is an
interface to efficient routines. In the following table, s and t are of type void *; cs and ct are
of type const void *; n is of type size_t; and c is an int converted to an unsigned char.
void
*memcpy(s,ct,n) copy n characters from ct to s, and return s.
void
*memmove(s,ct,n) same as memcpy except that it works even if the objects overlap.
int memcmp(cs,ct,n) compare the first n characters of cs with ct; return as with strcmp.
void
*memchr(cs,c,n)
return pointer to first occurrence of character c in cs, or NULL if not
present among the first n characters.
void *memset(s,c,n) place character c into first n characters of s, return s.
B.4 Mathematical Functions: <math.h>
The header <math.h> declares mathematical functions and macros.
The macros EDOM and ERANGE (found in <errno.h>) are non-zero integral constants that are
used to signal domain and range errors for the functions; HUGE_VAL is a positive double value.
A domain error occurs if an argument is outside the domain over which the function is
defined. On a domain error, errno is set to EDOM; the return value is implementation-defined.
A range error occurs if the result of the function cannot be represented as a double. If the
208
result overflows, the function returns HUGE_VAL with the right sign, and errno is set to
ERANGE. If the result underflows, the function returns zero; whether errno is set to ERANGE is
implementation-defined.
In the following table, x and y are of type double, n is an int, and all functions return double.
Angles for trigonometric functions are expressed in radians.
sin(x) sine of x
cos(x) cosine of x
tan(x) tangent of x
asin(x) sin-1(x) in range [-pi/2,pi/2], x in [-1,1].
acos(x) cos-1(x) in range [0,pi], x in [-1,1].
atan(x) tan-1(x) in range [-pi/2,pi/2].
atan2(y,x) tan-1(y/x) in range [-pi,pi].
sinh(x) hyperbolic sine of x
cosh(x) hyperbolic cosine of x
tanh(x) hyperbolic tangent of x
exp(x) exponential function e
x
log(x) natural logarithm ln(x), x>0.
log10(x) base 10 logarithm log10(x), x>0.
pow(x,y) x
y
. A domain error occurs if x=0 and y<=0, or if x<0 and y is not an
integer.
sqrt(x) sqare root of x, x>=0.
ceil(x) smallest integer not less than x, as a double.
floor(x) largest integer not greater than x, as a double.
fabs(x) absolute value |x|
ldexp(x,n) x*2n
frexp(x, int
*ip)
splits x into a normalized fraction in the interval [1/2,1) which is returned,
and a power of 2, which is stored in *exp. If x is zero, both parts of the
result are zero.
modf(x, double
*ip)
splits x into integral and fractional parts, each with the same sign as x. It
stores the integral part in *ip, and returns the fractional part.
fmod(x,y) floating-point remainder of x/y, with the same sign as x. If y is zero, the
result is implementation-defined.
B.5 Utility Functions: <stdlib.h>
The header <stdlib.h> declares functions for number conversion, storage allocation, and
similar tasks. double atof(const char *s)
atof converts s to double; it is equivalent to strtod(s, (char**)NULL).
int atoi(const char *s)
converts s to int; it is equivalent to (int)strtol(s, (char**)NULL, 10).
long atol(const char *s)
converts s to long; it is equivalent to strtol(s, (char**)NULL, 10).
double strtod(const char *s, char **endp)
strtod converts the prefix of s to double, ignoring leading white space; it stores a
pointer to any unconverted suffix in *endp unless endp is NULL. If the answer would
overflow, HUGE_VAL is returned with the proper sign; if the answer would underflow,
zero is returned. In either case errno is set to ERANGE.
long strtol(const char *s, char **endp, int base)
strtol converts the prefix of s to long, ignoring leading white space; it stores a
pointer to any unconverted suffix in *endp unless endp is NULL. If base is between 2
and 36, conversion is done assuming that the input is written in that base. If base is
zero, the base is 8, 10, or 16; leading 0 implies octal and leading 0x or 0X hexadecimal.
209
Letters in either case represent digits from 10 to base-1; a leading 0x or 0X is
permitted in base 16. If the answer would overflow, LONG_MAX or LONG_MIN is
returned, depending on the sign of the result, and errno is set to ERANGE.
unsigned long strtoul(const char *s, char **endp, int base)
strtoul is the same as strtol except that the result is unsigned long and the error
value is ULONG_MAX.
int rand(void)
rand returns a pseudo-random integer in the range 0 to RAND_MAX, which is at least
32767.
void srand(unsigned int seed)
srand uses seed as the seed for a new sequence of pseudo-random numbers. The
initial seed is 1.
void *calloc(size_t nobj, size_t size)
calloc returns a pointer to space for an array of nobj objects, each of size size, or
NULL if the request cannot be satisfied. The space is initialized to zero bytes.
void *malloc(size_t size)
malloc returns a pointer to space for an object of size size, or NULL if the request
cannot be satisfied. The space is uninitialized.
void *realloc(void *p, size_t size)
realloc changes the size of the object pointed to by p to size. The contents will be
unchanged up to the minimum of the old and new sizes. If the new size is larger, the
new space is uninitialized. realloc returns a pointer to the new space, or NULL if the
request cannot be satisfied, in which case *p is unchanged.
void free(void *p)
free deallocates the space pointed to by p; it does nothing if p is NULL. p must be a
pointer to space previously allocated by calloc, malloc, or realloc.
void abort(void)
abort causes the program to terminate abnormally, as if by raise(SIGABRT).
void exit(int status)
exit causes normal program termination. atexit functions are called in reverse order
of registration, open files are flushed, open streams are closed, and control is returned
to the environment. How status is returned to the environment is implementationdependent, but zero is taken as successful termination. The values EXIT_SUCCESS and
EXIT_FAILURE may also be used.
int atexit(void (*fcn)(void))
atexit registers the function fcn to be called when the program terminates normally;
it returns non-zero if the registration cannot be made.
int system(const char *s)
system passes the string s to the environment for execution. If s is NULL, system
returns non-zero if there is a command processor. If s is not NULL, the return value is
implementation-dependent.
char *getenv(const char *name)
getenv returns the environment string associated with name, or NULL if no string exists.
Details are implementation-dependent.
void *bsearch(const void *key, const void *base,
 size_t n, size_t size,
 int (*cmp)(const void *keyval, const void *datum))
bsearch searches base[0]...base[n-1] for an item that matches *key. The function
cmp must return negative if its first argument (the search key) is less than its second (a
table entry), zero if equal, and positive if greater. Items in the array base must be in
ascending order. bsearch returns a pointer to a matching item, or NULL if none exists.
void qsort(void *base, size_t n, size_t size,
210
 int (*cmp)(const void *, const void *))
qsort sorts into ascending order an array base[0]...base[n-1] of objects of size
size. The comparison function cmp is as in bsearch.
int abs(int n)
abs returns the absolute value of its int argument.
long labs(long n)
labs returns the absolute value of its long argument.
div_t div(int num, int denom)
div computes the quotient and remainder of num/denom. The results are stored in the
int members quot and rem of a structure of type div_t.
ldiv_t ldiv(long num, long denom)
ldiv computes the quotient and remainder of num/denom. The results are stored in the
long members quot and rem of a structure of type ldiv_t.
B.6 Diagnostics: <assert.h>
The assert macro is used to add diagnostics to programs:
void assert(int expression)
If expression is zero when
assert(expression)
is executed, the assert macro will print on stderr a message, such as
Assertion failed: expression, file filename, line nnn
It then calls abort to terminate execution. The source filename and line number come from the
preprocessor macros __FILE__ and __LINE__.
If NDEBUG is defined at the time <assert.h> is included, the assert macro is ignored.
B.7 Variable Argument Lists: <stdarg.h>
The header <stdarg.h> provides facilities for stepping through a list of function arguments of
unknown number and type.
Suppose lastarg is the last named parameter of a function f with a variable number of
arguments. Then declare within f a variable of type va_list that will point to each argument
in turn:
 va_list ap;
ap must be initialized once with the macro va_start before any unnamed argument is
accessed:
va_start(va_list ap, lastarg);
Thereafter, each execution of the macro va_arg will produce a value that has the type and
value of the next unnamed argument, and will also modify ap so the next use of va_arg returns
the next argument:
type va_arg(va_list ap, type);
The macro
 void va_end(va_list ap);
must be called once after the arguments have been processed but before f is exited. 
211
B.8 Non-local Jumps: <setjmp.h>
The declarations in <setjmp.h> provide a way to avoid the normal function call and return
sequence, typically to permit an immediate return from a deeply nested function call.
int setjmp(jmp_buf env)
The macro setjmp saves state information in env for use by longjmp. The return is
zero from a direct call of setjmp, and non-zero from a subsequent call of longjmp. A
call to setjmp can only occur in certain contexts, basically the test of if, switch, and
loops, and only in simple relational expressions.
 if (setjmp(env) == 0)
 /* get here on direct call */
 else
 /* get here by calling longjmp */
void longjmp(jmp_buf env, int val)
longjmp restores the state saved by the most recent call to setjmp, using the
information saved in env, and execution resumes as if the setjmp function had just
executed and returned the non-zero value val. The function containing the setjmp
must not have terminated. Accessible objects have the values they had at the time
longjmp was called, except that non-volatile automatic variables in the function
calling setjmp become undefined if they were changed after the setjmp call.
B.9 Signals: <signal.h>
The header <signal.h> provides facilities for handling exceptional conditions that arise during
execution, such as an interrupt signal from an external source or an error in execution.
void (*signal(int sig, void (*handler)(int)))(int)
signal determines how subsequent signals will be handled. If handler is SIG_DFL, the
implementation-defined default behavior is used, if it is SIG_IGN, the signal is ignored;
otherwise, the function pointed to by handler will be called, with the argument of the type of
signal. Valid signals include
SIGABRT abnormal termination, e.g., from abort
SIGFPE arithmetic error, e.g., zero divide or overflow
SIGILL illegal function image, e.g., illegal instruction
SIGINT interactive attention, e.g., interrupt
SIGSEGV illegal storage access, e.g., access outside memory limits
SIGTERM termination request sent to this program
signal returns the previous value of handler for the specific signal, or SIG_ERR if an error
occurs.
When a signal sig subsequently occurs, the signal is restored to its default behavior; then the
signal-handler function is called, as if by (*handler)(sig). If the handler returns, execution
will resume where it was when the signal occurred.
The initial state of signals is implementation-defined.
int raise(int sig)
raise sends the signal sig to the program; it returns non-zero if unsuccessful.
B.10 Date and Time Functions: <time.h>
The header <time.h> declares types and functions for manipulating date and time. Some
functions process local time, which may differ from calendar time, for example because of time
212
zone. clock_t and time_t are arithmetic types representing times, and struct tm holds the
components of a calendar time:
int tm_sec; seconds after the minute (0,61)
int tm_min; minutes after the hour (0,59)
int tm_hour; hours since midnight (0,23)
int tm_mday; day of the month (1,31)
int tm_mon; months since January (0,11)
int tm_year; years since 1900
int tm_wday; days since Sunday (0,6)
int tm_yday; days since January 1 (0,365)
int tm_isdst; Daylight Saving Time flag
tm_isdst is positive if Daylight Saving Time is in effect, zero if not, and negative if the
information is not available.
clock_t clock(void)
clock returns the processor time used by the program since the beginning of execution,
or -1 if unavailable. clock()/CLK_PER_SEC is a time in seconds.
time_t time(time_t *tp)
time returns the current calendar time or -1 if the time is not available. If tp is not
NULL, the return value is also assigned to *tp.
double difftime(time_t time2, time_t time1)
difftime returns time2-time1 expressed in seconds.
time_t mktime(struct tm *tp)
mktime converts the local time in the structure *tp into calendar time in the same
representation used by time. The components will have values in the ranges shown.
mktime returns the calendar time or -1 if it cannot be represented.
The next four functions return pointers to static objects that may be overwritten by other calls.
char *asctime(const struct tm *tp)
asctime</tt< converts the time in the structure *tp into a string of
the form
 Sun Jan 3 15:14:13 1988\n\0
char *ctime(const time_t *tp)
ctime converts the calendar time *tp to local time; it is equivalent
to
 asctime(localtime(tp))
struct tm *gmtime(const time_t *tp)
gmtime converts the calendar time *tp into Coordinated Universal Time
(UTC). It returns NULL if UTC is not available. The name gmtime has
historical significance.
struct tm *localtime(const time_t *tp)
localtime converts the calendar time *tp into local time.
size_t strftime(char *s, size_t smax, const char *fmt, const struct tm *tp)
strftime formats date and time information from *tp into s according
to fmt, which is analogous to a printf format. Ordinary characters
(including the terminating '\0') are copied into s. Each %c is
replaced as described below, using values appropriate for the local
environment. No more than smax characters are placed into s. strftime
returns the number of characters, excluding the '\0', or zero if more
than smax characters were produced.
%a abbreviated weekday name.
%A full weekday name.
%b abbreviated month name.
%B full month name.
%c local date and time representation.
%d day of the month (01-31).
%H hour (24-hour clock) (00-23).
213
%I hour (12-hour clock) (01-12).
%j day of the year (001-366).
%m month (01-12).
%M minute (00-59).
%p local equivalent of AM or PM.
%S second (00-61).
%U week number of the year (Sunday as 1st day of week) (00-53).
%w weekday (0-6, Sunday is 0).
%W week number of the year (Monday as 1st day of week) (00-53).
%x local date representation.
%X local time representation.
%y year without century (00-99).
%Y year with century.
%Z time zone name, if any.
%% %
B.11 Implementation-defined Limits:
<limits.h> and <float.h>
The header <limits.h> defines constants for the sizes of integral types.
The values below are acceptable minimum magnitudes; larger values may be
used.
CHAR_BIT 8 bits in a char
CHAR_MAX UCHAR_MAX or
SCHAR_MAX maximum value of char
CHAR_MIN 0 or SCHAR_MIN maximum value of char
INT_MAX 32767 maximum value of int
INT_MIN -32767 minimum value of int
LONG_MAX 2147483647 maximum value of long
LONG_MIN -2147483647 minimum value of long
SCHAR_MAX +127 maximum value of signed char
SCHAR_MIN -127 minimum value of signed char
SHRT_MAX +32767 maximum value of short
SHRT_MIN -32767 minimum value of short
UCHAR_MAX 255 maximum value of unsigned char
UINT_MAX 65535 maximum value of unsigned int
ULONG_MAX 4294967295 maximum value of unsigned long
USHRT_MAX 65535 maximum value of unsigned
short
The names in the table below, a subset of <float.h>, are constants related
to floating-point arithmetic. When a value is given, it represents the
minimum magnitude for the corresponding quantity. Each implementation
defines appropriate values.
FLT_RADIX 2 radix of exponent, representation, e.g., 2, 16
FLT_ROUNDS floating-point rounding mode for addition
FLT_DIG 6 decimal digits of precision
FLT_EPSILON 1E-5 smallest number x such that 1.0+x != 1.0
FLT_MANT_DIG number of base FLT_RADIX in mantissa
FLT_MAX 1E+37 maximum floating-point number
FLT_MAX_EXP maximum n such that FLT_RADIXn-1 is representable
FLT_MIN 1E-37 minimum normalized floating-point number
FLT_MIN_EXP minimum n such that 10n
 is a normalized number
DBL_DIG 10 decimal digits of precision
DBL_EPSILON 1E-9 smallest number x such that 1.0+x != 1.0
DBL_MANT_DIG number of base FLT_RADIX in mantissa
214
DBL_MAX 1E+37 maximum double floating-point number
DBL_MAX_EXP maximum n such that FLT_RADIXn-1 is representable
DBL_MIN 1E-37 minimum normalized double floating-point number
DBL_MIN_EXP minimum n such that 10n
 is a normalized number
215
Appendix C - Summary of Changes
Since the publication of the first edition of this book, the definition of the C language has
undergone changes. Almost all were extensions of the original language, and were carefully
designed to remain compatible with existing practice; some repaired ambiguities in the original
description; and some represent modifications that change existing practice. Many of the new
facilities were announced in the documents accompanying compilers available from AT&T,
and have subsequently been adopted by other suppliers of C compilers. More recently, the
ANSI committee standardizing the language incorporated most of the changes, and also
introduced other significant modifications. Their report was in part participated by some
commercial compilers even before issuance of the formal C standard.
This Appendix summarizes the differences between the language defined by the first edition of
this book, and that expected to be defined by the final standard. It treats only the language
itself, not its environment and library; although these are an important part of the standard,
there is little to compare with, because the first edition did not attempt to prescribe an
environment or library.
• Preprocessing is more carefully defined in the Standard than in the first edition, and is
extended: it is explicitly token based; there are new operators for concatenation of
tokens (##), and creation of strings (#); there are new control lines like #elif and
#pragma; redeclaration of macros by the same token sequence is explicitly permitted;
parameters inside strings are no longer replaced. Splicing of lines by \ is permitted
everywhere, not just in strings and macro definitions. See Par.A.12.
• The minimum significance of all internal identifiers increased to 31 characters; the
smallest mandated significance of identifiers with external linkage remains 6 monocase
letters. (Many implementations provide more.)
• Trigraph sequences introduced by ?? allow representation of characters lacking in
some character sets. Escapes for #\^[]{}|~ are defined, see Par.A.12.1. Observe that
the introduction of trigraphs may change the meaning of strings containing the
sequence ??.
• New keywords (void, const, volatile, signed, enum) are introduced. The
stillborn entry keyword is withdrawn.
• New escape sequences, for use within character constants and string literals, are
defined. The effect of following \ by a character not part of an approved escape
sequence is undefined. See Par.A.2.5.2.
• Everyone's favorite trivial change: 8 and 9 are not octal digits.
• The standard introduces a larger set of suffixes to make the type of constants explicit: U
or L for integers, F or L for floating. It also refines the rules for the type of unsiffixed
constants (Par.A.2.5).
• Adjacent string literals are concatenated.
• There is a notation for wide-character string literals and character constants; see
Par.A.2.6.
• Characters as well as other types, may be explicitly declared to carry, or not to carry, a
sign by using the keywords signed or unsigned. The locution long float as a
216
synonym for double is withdrawn, but long double may be used to declare an extraprecision floating quantity.
• For some time, type unsigned char has been available. The standard introduces the
signed keyword to make signedness explicit for char and other integral objects.
• The void type has been available in most implementations for some years. The
Standard introduces the use of the void * type as a generic pointer type; previously
char * played this role. At the same time, explicit rules are enacted against mixing
pointers and integers, and pointers of different type, without the use of casts.
• The Standard places explicit minima on the ranges of the arithmetic types, and
mandates headers (<limits.h> and <float.h>) giving the characteristics of each
particular implementation.
• Enumerations are new since the first edition of this book.
• The Standard adopts from C++ the notion of type qualifier, for example const
(Par.A.8.2).
• Strings are no longer modifiable, and so may be placed in read-only memory.
• The ``usual arithmetic conversions'' are changed, essentially from ``for integers,
unsigned always wins; for floating point, always use double'' to ``promote to the
smallest capacious-enough type.'' See Par.A.6.5.
• The old assignment operators like =+ are truly gone. Also, assignment operators are
now single tokens; in the first edition, they were pairs, and could be separated by white
space.
• A compiler's license to treat mathematically associative operators as computationally
associative is revoked.
• A unary + operator is introduced for symmetry with unary -.
• A pointer to a function may be used as a function designator without an explicit *
operator. See Par.A.7.3.2.
• Structures may be assigned, passed to functions, and returned by functions.
• Applying the address-of operator to arrays is permitted, and the result is a pointer to
the array.
• The sizeof operator, in the first edition, yielded type int; subsequently, many
implementations made it unsigned. The Standard makes its type explicitly
implementation-dependent, but requires the type, size_t, to be defined in a standard
header (<stddef.h>). A similar change occurs in the type (ptrdiff_t) of the
difference between pointers. See Par.A.7.4.8 and Par.A.7.7.
• The address-of operator & may not be applied to an object declared register, even if
the implementation chooses not to keep the object in a register.
• The type of a shift expression is that of the left operand; the right operand can't
promote the result. See Par.A.7.8.
• The Standard legalizes the creation of a pointer just beyond the end of an array, and
allows arithmetic and relations on it; see Par.A.7.7. 
217
• The Standard introduces (borrowing from C++) the notion of a function prototype
declaration that incorporates the types of the parameters, and includes an explicit
recognition of variadic functions together with an approved way of dealing with them.
See Pars. A.7.3.2, A.8.6.3, B.7. The older style is still accepted, with restrictions.
• Empty declarations, which have no declarators and don't declare at least a structure,
union, or enumeration, are forbidden by the Standard. On the other hand, a declaration
with just a structure or union tag redeclares that tag even if it was declared in an outer
scope.
• External data declarations without any specifiers or qualifiers (just a naked declarator)
are forbidden.
• Some implementations, when presented with an extern declaration in an inner block,
would export the declaration to the rest of the file. The Standard makes it clear that the
scope of such a declaration is just the block.
• The scope of parameters is injected into a function's compound statement, so that
variable declarations at the top level of the function cannot hide the parameters.
• The name spaces of identifiers are somewhat different. The Standard puts all tags in a
single name space, and also introduces a separate name space for labels; see
Par.A.11.1. Also, member names are associated with the structure or union of which
they are a part. (This has been common practice from some time.)
• Unions may be initialized; the initializer refers to the first member.
• Automatic structures, unions, and arrays may be initialized, albeit in a restricted way.
• Character arrays with an explicit size may be initialized by a string literal with exactly
that many characters (the \0 is quietly squeezed out).
• The controlling expression, and the case labels, of a switch may have any integral type. 

